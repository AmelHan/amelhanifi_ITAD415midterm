<html>
<head>
<title>test_distributed.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_distributed.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">__future__ </span><span class="s0">import </span><span class="s1">annotations</span>

<span class="s0">import </span><span class="s1">pytest</span>

<span class="s1">distributed = pytest.importorskip(</span><span class="s2">&quot;distributed&quot;</span><span class="s1">)</span>

<span class="s0">import </span><span class="s1">asyncio</span>
<span class="s0">import </span><span class="s1">os</span>
<span class="s0">import </span><span class="s1">sys</span>
<span class="s0">from </span><span class="s1">functools </span><span class="s0">import </span><span class="s1">partial</span>
<span class="s0">from </span><span class="s1">operator </span><span class="s0">import </span><span class="s1">add</span>

<span class="s0">from </span><span class="s1">distributed </span><span class="s0">import </span><span class="s1">Client</span><span class="s0">, </span><span class="s1">SchedulerPlugin</span><span class="s0">, </span><span class="s1">WorkerPlugin</span>
<span class="s0">from </span><span class="s1">distributed.utils_test </span><span class="s0">import </span><span class="s1">cleanup  </span><span class="s3"># noqa F401</span>
<span class="s0">from </span><span class="s1">distributed.utils_test </span><span class="s0">import </span><span class="s1">client </span><span class="s0">as </span><span class="s1">c  </span><span class="s3"># noqa F401</span>
<span class="s0">from </span><span class="s1">distributed.utils_test </span><span class="s0">import </span><span class="s1">(  </span><span class="s3"># noqa F401</span>
    <span class="s1">cluster</span><span class="s0">,</span>
    <span class="s1">cluster_fixture</span><span class="s0">,</span>
    <span class="s1">gen_cluster</span><span class="s0">,</span>
    <span class="s1">loop</span><span class="s0">,</span>
    <span class="s1">loop_in_thread</span><span class="s0">,</span>
    <span class="s1">popen</span><span class="s0">,</span>
    <span class="s1">varying</span><span class="s0">,</span>
<span class="s1">)</span>

<span class="s0">import </span><span class="s1">dask</span>
<span class="s0">import </span><span class="s1">dask.bag </span><span class="s0">as </span><span class="s1">db</span>
<span class="s0">from </span><span class="s1">dask </span><span class="s0">import </span><span class="s1">compute</span><span class="s0">, </span><span class="s1">delayed</span><span class="s0">, </span><span class="s1">persist</span>
<span class="s0">from </span><span class="s1">dask.base </span><span class="s0">import </span><span class="s1">compute_as_if_collection</span><span class="s0">, </span><span class="s1">get_scheduler</span>
<span class="s0">from </span><span class="s1">dask.blockwise </span><span class="s0">import </span><span class="s1">Blockwise</span>
<span class="s0">from </span><span class="s1">dask.delayed </span><span class="s0">import </span><span class="s1">Delayed</span>
<span class="s0">from </span><span class="s1">dask.distributed </span><span class="s0">import </span><span class="s1">futures_of</span><span class="s0">, </span><span class="s1">wait</span>
<span class="s0">from </span><span class="s1">dask.layers </span><span class="s0">import </span><span class="s1">ShuffleLayer</span><span class="s0">, </span><span class="s1">SimpleShuffleLayer</span>
<span class="s0">from </span><span class="s1">dask.utils </span><span class="s0">import </span><span class="s1">get_named_args</span><span class="s0">, </span><span class="s1">get_scheduler_lock</span><span class="s0">, </span><span class="s1">tmpdir</span><span class="s0">, </span><span class="s1">tmpfile</span>
<span class="s0">from </span><span class="s1">dask.utils_test </span><span class="s0">import </span><span class="s1">inc</span>

<span class="s0">if </span><span class="s2">&quot;should_check_state&quot; </span><span class="s0">in </span><span class="s1">get_named_args(gen_cluster):</span>
    <span class="s1">gen_cluster = partial(gen_cluster</span><span class="s0">, </span><span class="s1">should_check_state=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">cluster = partial(cluster</span><span class="s0">, </span><span class="s1">should_check_state=</span><span class="s0">False</span><span class="s1">)</span>


<span class="s3"># TODO: the fixture teardown for `cluster_fixture` is failing periodically with</span>
<span class="s3"># a PermissionError on windows only (in CI). Since this fixture lives in the</span>
<span class="s3"># distributed codebase and is nested within other fixtures we use, it's hard to</span>
<span class="s3"># patch it from the dask codebase. And since the error is during fixture</span>
<span class="s3"># teardown, an xfail won't cut it. As a hack, for now we skip all these tests</span>
<span class="s3"># on windows. See https://github.com/dask/dask/issues/8877.</span>
<span class="s1">pytestmark = pytest.mark.skipif(</span>
    <span class="s1">sys.platform == </span><span class="s2">&quot;win32&quot;</span><span class="s0">,</span>
    <span class="s1">reason=(</span>
        <span class="s2">&quot;The teardown of distributed.utils_test.cluster_fixture &quot;</span>
        <span class="s2">&quot;fails on windows CI currently&quot;</span>
    <span class="s1">)</span><span class="s0">,</span>
<span class="s1">)</span>

<span class="s1">ignore_sync_scheduler_warning = pytest.mark.filterwarnings(</span>
    <span class="s2">&quot;ignore:Running on a single-machine scheduler when a distributed client &quot;</span>
    <span class="s2">&quot;is active might lead to unexpected results.&quot;</span>
<span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_can_import_client():</span>
    <span class="s0">from </span><span class="s1">dask.distributed </span><span class="s0">import </span><span class="s1">Client  </span><span class="s3"># noqa: F401</span>


<span class="s0">def </span><span class="s1">test_can_import_nested_things():</span>
    <span class="s0">from </span><span class="s1">dask.distributed.protocol </span><span class="s0">import </span><span class="s1">dumps  </span><span class="s3"># noqa: F401</span>


<span class="s1">@gen_cluster(client=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_persist(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b):</span>
    <span class="s1">x = delayed(inc)(</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">(x2</span><span class="s0">,</span><span class="s1">) = persist(x)</span>

    <span class="s0">await </span><span class="s1">wait(x2)</span>
    <span class="s0">assert </span><span class="s1">x2.key </span><span class="s0">in </span><span class="s1">a.data </span><span class="s0">or </span><span class="s1">x2.key </span><span class="s0">in </span><span class="s1">b.data</span>

    <span class="s1">y = delayed(inc)(</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">y2</span><span class="s0">, </span><span class="s1">one = persist(y</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s0">await </span><span class="s1">wait(y2)</span>
    <span class="s0">assert </span><span class="s1">y2.key </span><span class="s0">in </span><span class="s1">a.data </span><span class="s0">or </span><span class="s1">y2.key </span><span class="s0">in </span><span class="s1">b.data</span>


<span class="s0">def </span><span class="s1">test_persist_nested(c):</span>
    <span class="s1">a = delayed(</span><span class="s4">1</span><span class="s1">) + </span><span class="s4">5</span>
    <span class="s1">b = a + </span><span class="s4">1</span>
    <span class="s1">c = a + </span><span class="s4">2</span>
    <span class="s1">result = persist({</span><span class="s2">&quot;a&quot;</span><span class="s1">: a</span><span class="s0">, </span><span class="s2">&quot;b&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s1">b]}</span><span class="s0">, </span><span class="s1">(c</span><span class="s0">, </span><span class="s4">2</span><span class="s1">)</span><span class="s0">, </span><span class="s4">4</span><span class="s0">, </span><span class="s1">[</span><span class="s4">5</span><span class="s1">])</span>
    <span class="s0">assert </span><span class="s1">isinstance(result[</span><span class="s4">0</span><span class="s1">][</span><span class="s2">&quot;a&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">Delayed)</span>
    <span class="s0">assert </span><span class="s1">isinstance(result[</span><span class="s4">0</span><span class="s1">][</span><span class="s2">&quot;b&quot;</span><span class="s1">][</span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">Delayed)</span>
    <span class="s0">assert </span><span class="s1">isinstance(result[</span><span class="s4">1</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">Delayed)</span>

    <span class="s1">sol = ({</span><span class="s2">&quot;a&quot;</span><span class="s1">: </span><span class="s4">6</span><span class="s0">, </span><span class="s2">&quot;b&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">7</span><span class="s1">]}</span><span class="s0">, </span><span class="s1">(</span><span class="s4">8</span><span class="s0">, </span><span class="s4">2</span><span class="s1">)</span><span class="s0">, </span><span class="s4">4</span><span class="s0">, </span><span class="s1">[</span><span class="s4">5</span><span class="s1">])</span>
    <span class="s0">assert </span><span class="s1">compute(*result) == sol</span>

    <span class="s1">res = persist([a</span><span class="s0">, </span><span class="s1">b]</span><span class="s0">, </span><span class="s1">c</span><span class="s0">, </span><span class="s4">4</span><span class="s0">, </span><span class="s1">[</span><span class="s4">5</span><span class="s1">]</span><span class="s0">, </span><span class="s1">traverse=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">res[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">] </span><span class="s0">is </span><span class="s1">a</span>
    <span class="s0">assert </span><span class="s1">res[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">1</span><span class="s1">] </span><span class="s0">is </span><span class="s1">b</span>
    <span class="s0">assert </span><span class="s1">res[</span><span class="s4">1</span><span class="s1">].compute() == </span><span class="s4">8</span>
    <span class="s0">assert </span><span class="s1">res[</span><span class="s4">2</span><span class="s1">:] == (</span><span class="s4">4</span><span class="s0">, </span><span class="s1">[</span><span class="s4">5</span><span class="s1">])</span>


<span class="s0">def </span><span class="s1">test_futures_to_delayed_dataframe(c):</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s1">df = pd.DataFrame({</span><span class="s2">&quot;x&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]})</span>

    <span class="s1">futures = c.scatter([df</span><span class="s0">, </span><span class="s1">df])</span>
    <span class="s1">ddf = dd.from_delayed(futures)</span>
    <span class="s1">dd.utils.assert_eq(ddf.compute()</span><span class="s0">, </span><span class="s1">pd.concat([df</span><span class="s0">, </span><span class="s1">df]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">))</span>

    <span class="s3"># Make sure from_delayed is Blockwise</span>
    <span class="s0">assert </span><span class="s1">isinstance(ddf.dask.layers[ddf._name]</span><span class="s0">, </span><span class="s1">Blockwise)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(TypeError):</span>
        <span class="s1">ddf = dd.from_delayed([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>


<span class="s0">def </span><span class="s1">test_from_delayed_dataframe(c):</span>
    <span class="s3"># Check that Delayed keys in the form of a tuple</span>
    <span class="s3"># are properly serialized in `from_delayed`</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s1">df = pd.DataFrame({</span><span class="s2">&quot;x&quot;</span><span class="s1">: range(</span><span class="s4">20</span><span class="s1">)})</span>
    <span class="s1">ddf = dd.from_pandas(df</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">ddf = dd.from_delayed(ddf.to_delayed())</span>
    <span class="s1">dd.utils.assert_eq(ddf</span><span class="s0">, </span><span class="s1">df</span><span class="s0">, </span><span class="s1">scheduler=c)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;fuse&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_fused_blockwise_dataframe_merge(c</span><span class="s0">, </span><span class="s1">fuse):</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s3"># Generate two DataFrames with more partitions than</span>
    <span class="s3"># the `max_branch` default used for shuffling (32).</span>
    <span class="s3"># We need a multi-stage shuffle to cover #7178 fix.</span>
    <span class="s1">size = </span><span class="s4">35</span>
    <span class="s1">df1 = pd.DataFrame({</span><span class="s2">&quot;x&quot;</span><span class="s1">: range(size)</span><span class="s0">, </span><span class="s2">&quot;y&quot;</span><span class="s1">: range(size)})</span>
    <span class="s1">df2 = pd.DataFrame({</span><span class="s2">&quot;x&quot;</span><span class="s1">: range(size)</span><span class="s0">, </span><span class="s2">&quot;z&quot;</span><span class="s1">: range(size)})</span>
    <span class="s1">ddf1 = dd.from_pandas(df1</span><span class="s0">, </span><span class="s1">npartitions=size) + </span><span class="s4">10</span>
    <span class="s1">ddf2 = dd.from_pandas(df2</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">5</span><span class="s1">) + </span><span class="s4">10</span>
    <span class="s1">df1 += </span><span class="s4">10</span>
    <span class="s1">df2 += </span><span class="s4">10</span>

    <span class="s0">with </span><span class="s1">dask.config.set({</span><span class="s2">&quot;optimization.fuse.active&quot;</span><span class="s1">: fuse}):</span>
        <span class="s1">ddfm = ddf1.merge(ddf2</span><span class="s0">, </span><span class="s1">on=[</span><span class="s2">&quot;x&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">how=</span><span class="s2">&quot;left&quot;</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s2">&quot;tasks&quot;</span><span class="s1">)</span>
        <span class="s1">ddfm.head()  </span><span class="s3"># https://github.com/dask/dask/issues/7178</span>
        <span class="s1">dfm = ddfm.compute().sort_values(</span><span class="s2">&quot;x&quot;</span><span class="s1">)</span>
        <span class="s3"># We call compute above since `sort_values` is not</span>
        <span class="s3"># supported in `dask.dataframe`</span>
    <span class="s1">dd.utils.assert_eq(</span>
        <span class="s1">dfm</span><span class="s0">, </span><span class="s1">df1.merge(df2</span><span class="s0">, </span><span class="s1">on=[</span><span class="s2">&quot;x&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">how=</span><span class="s2">&quot;left&quot;</span><span class="s1">).sort_values(</span><span class="s2">&quot;x&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s1">check_index=</span><span class="s0">False</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;on&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;a&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;a&quot;</span><span class="s1">]])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;broadcast&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_dataframe_broadcast_merge(c</span><span class="s0">, </span><span class="s1">on</span><span class="s0">, </span><span class="s1">broadcast):</span>
    <span class="s3"># See: https://github.com/dask/dask/issues/9870</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s1">pdfl = pd.DataFrame({</span><span class="s2">&quot;a&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">] * </span><span class="s4">2</span><span class="s0">, </span><span class="s2">&quot;b_left&quot;</span><span class="s1">: range(</span><span class="s4">4</span><span class="s1">)})</span>
    <span class="s1">pdfr = pd.DataFrame({</span><span class="s2">&quot;a&quot;</span><span class="s1">: [</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s2">&quot;b_right&quot;</span><span class="s1">: range(</span><span class="s4">2</span><span class="s1">)})</span>
    <span class="s1">dfl = dd.from_pandas(pdfl</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">4</span><span class="s1">)</span>
    <span class="s1">dfr = dd.from_pandas(pdfr</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s1">ddfm = dd.merge(dfl</span><span class="s0">, </span><span class="s1">dfr</span><span class="s0">, </span><span class="s1">on=on</span><span class="s0">, </span><span class="s1">broadcast=broadcast</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s2">&quot;tasks&quot;</span><span class="s1">)</span>
    <span class="s1">dfm = ddfm.compute()</span>
    <span class="s1">dd.utils.assert_eq(</span>
        <span class="s1">dfm.sort_values(</span><span class="s2">&quot;a&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">pd.merge(pdfl</span><span class="s0">, </span><span class="s1">pdfr</span><span class="s0">, </span><span class="s1">on=on).sort_values(</span><span class="s2">&quot;a&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">check_index=</span><span class="s0">False,</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;computation&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s0">None,</span>
        <span class="s2">&quot;compute_as_if_collection&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;dask.compute&quot;</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;scheduler, use_distributed&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s0">None, True</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s3"># If scheduler is explicitly provided, this takes precedence</span>
        <span class="s1">(</span><span class="s2">&quot;sync&quot;</span><span class="s0">, False</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_default_scheduler_on_worker(c</span><span class="s0">, </span><span class="s1">computation</span><span class="s0">, </span><span class="s1">use_distributed</span><span class="s0">, </span><span class="s1">scheduler):</span>
    <span class="s5">&quot;&quot;&quot;Should a collection use its default scheduler or the distributed 
    scheduler when being computed within a task? 
    &quot;&quot;&quot;</span>

    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s3"># Track how many submits/update-graph were received by the scheduler</span>
    <span class="s0">class </span><span class="s1">UpdateGraphCounter(SchedulerPlugin):</span>
        <span class="s0">async def </span><span class="s1">start(self</span><span class="s0">, </span><span class="s1">scheduler):</span>
            <span class="s1">scheduler._update_graph_count = </span><span class="s4">0</span>

        <span class="s0">def </span><span class="s1">update_graph(self</span><span class="s0">, </span><span class="s1">scheduler</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kwargs):</span>
            <span class="s1">scheduler._update_graph_count += </span><span class="s4">1</span>

    <span class="s1">c.register_plugin(UpdateGraphCounter())</span>

    <span class="s0">def </span><span class="s1">foo():</span>
        <span class="s1">size = </span><span class="s4">10</span>
        <span class="s1">df = pd.DataFrame({</span><span class="s2">&quot;x&quot;</span><span class="s1">: range(size)</span><span class="s0">, </span><span class="s2">&quot;y&quot;</span><span class="s1">: range(size)})</span>
        <span class="s1">ddf = dd.from_pandas(df</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">computation </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">ddf.compute(scheduler=scheduler)</span>
        <span class="s0">elif </span><span class="s1">computation == </span><span class="s2">&quot;dask.compute&quot;</span><span class="s1">:</span>
            <span class="s1">dask.compute(ddf</span><span class="s0">, </span><span class="s1">scheduler=scheduler)</span>
        <span class="s0">elif </span><span class="s1">computation == </span><span class="s2">&quot;compute_as_if_collection&quot;</span><span class="s1">:</span>
            <span class="s1">compute_as_if_collection(</span>
                <span class="s1">ddf.__class__</span><span class="s0">, </span><span class="s1">ddf.dask</span><span class="s0">, </span><span class="s1">list(ddf.dask)</span><span class="s0">, </span><span class="s1">scheduler=scheduler</span>
            <span class="s1">)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">assert False</span>

        <span class="s0">return True</span>

    <span class="s1">res = c.submit(foo)</span>
    <span class="s0">assert </span><span class="s1">res.result() </span><span class="s0">is True</span>

    <span class="s1">num_update_graphs = c.run_on_scheduler(</span>
        <span class="s0">lambda </span><span class="s1">dask_scheduler: dask_scheduler._update_graph_count</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">num_update_graphs == </span><span class="s4">2 </span><span class="s0">if </span><span class="s1">use_distributed </span><span class="s0">else </span><span class="s4">1</span><span class="s0">, </span><span class="s1">num_update_graphs</span>


<span class="s0">def </span><span class="s1">test_futures_to_delayed_bag(c):</span>
    <span class="s1">L = [</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span>

    <span class="s1">futures = c.scatter([L</span><span class="s0">, </span><span class="s1">L])</span>
    <span class="s1">b = db.from_delayed(futures)</span>
    <span class="s0">assert </span><span class="s1">list(b) == L + L</span>


<span class="s0">def </span><span class="s1">test_futures_to_delayed_array(c):</span>
    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s1">)</span>
    <span class="s0">from </span><span class="s1">dask.array.utils </span><span class="s0">import </span><span class="s1">assert_eq</span>

    <span class="s1">np = pytest.importorskip(</span><span class="s2">&quot;numpy&quot;</span><span class="s1">)</span>
    <span class="s1">x = np.arange(</span><span class="s4">5</span><span class="s1">)</span>

    <span class="s1">futures = c.scatter([x</span><span class="s0">, </span><span class="s1">x])</span>
    <span class="s1">A = da.concatenate(</span>
        <span class="s1">[da.from_delayed(f</span><span class="s0">, </span><span class="s1">shape=x.shape</span><span class="s0">, </span><span class="s1">dtype=x.dtype) </span><span class="s0">for </span><span class="s1">f </span><span class="s0">in </span><span class="s1">futures]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">assert_eq(A.compute()</span><span class="s0">, </span><span class="s1">np.concatenate([x</span><span class="s0">, </span><span class="s1">x]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">))</span>


<span class="s1">@ignore_sync_scheduler_warning</span>
<span class="s1">@gen_cluster(client=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_local_get_with_distributed_active(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b):</span>
    <span class="s0">with </span><span class="s1">dask.config.set(scheduler=</span><span class="s2">&quot;sync&quot;</span><span class="s1">):</span>
        <span class="s1">x = delayed(inc)(</span><span class="s4">1</span><span class="s1">).persist()</span>
    <span class="s0">await </span><span class="s1">asyncio.sleep(</span><span class="s4">0.01</span><span class="s1">)</span>
    <span class="s0">assert not </span><span class="s1">s.tasks  </span><span class="s3"># scheduler hasn't done anything</span>

    <span class="s1">x = delayed(inc)(</span><span class="s4">2</span><span class="s1">).persist(scheduler=</span><span class="s2">&quot;sync&quot;</span><span class="s1">)  </span><span class="s3"># noqa F841</span>
    <span class="s0">await </span><span class="s1">asyncio.sleep(</span><span class="s4">0.01</span><span class="s1">)</span>
    <span class="s0">assert not </span><span class="s1">s.tasks  </span><span class="s3"># scheduler hasn't done anything</span>


<span class="s1">@pytest.mark.xfail_with_pyarrow_strings</span>
<span class="s0">def </span><span class="s1">test_to_hdf_distributed(c):</span>
    <span class="s1">pytest.importorskip(</span><span class="s2">&quot;numpy&quot;</span><span class="s1">)</span>
    <span class="s1">pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>

    <span class="s0">from </span><span class="s1">dask.dataframe.io.tests.test_hdf </span><span class="s0">import </span><span class="s1">test_to_hdf</span>

    <span class="s1">test_to_hdf()</span>


<span class="s1">@ignore_sync_scheduler_warning</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;npartitions&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s4">1</span><span class="s0">,</span>
        <span class="s1">pytest.param(</span>
            <span class="s4">4</span><span class="s0">,</span>
            <span class="s1">marks=pytest.mark.xfail(reason=</span><span class="s2">&quot;HDF not multi-process safe&quot;</span><span class="s0">, </span><span class="s1">strict=</span><span class="s0">False</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">pytest.param(</span>
            <span class="s4">10</span><span class="s0">,</span>
            <span class="s1">marks=pytest.mark.xfail(reason=</span><span class="s2">&quot;HDF not multi-process safe&quot;</span><span class="s0">, </span><span class="s1">strict=</span><span class="s0">False</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.xfail_with_pyarrow_strings</span>
<span class="s0">def </span><span class="s1">test_to_hdf_scheduler_distributed(npartitions</span><span class="s0">, </span><span class="s1">c):</span>
    <span class="s1">pytest.importorskip(</span><span class="s2">&quot;numpy&quot;</span><span class="s1">)</span>
    <span class="s1">pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>

    <span class="s0">from </span><span class="s1">dask.dataframe.io.tests.test_hdf </span><span class="s0">import </span><span class="s1">test_to_hdf_schedulers</span>

    <span class="s1">test_to_hdf_schedulers(</span><span class="s0">None, </span><span class="s1">npartitions)</span>


<span class="s1">@gen_cluster(client=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_serializable_groupby_agg(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b):</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>
    <span class="s1">df = pd.DataFrame({</span><span class="s2">&quot;x&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">3</span><span class="s0">, </span><span class="s4">4</span><span class="s1">]</span><span class="s0">, </span><span class="s2">&quot;y&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]})</span>
    <span class="s1">ddf = dd.from_pandas(df</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s1">result = ddf.groupby(</span><span class="s2">&quot;y&quot;</span><span class="s0">, </span><span class="s1">sort=</span><span class="s0">False</span><span class="s1">).agg(</span><span class="s2">&quot;count&quot;</span><span class="s0">, </span><span class="s1">split_out=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s3"># Check Culling and Compute</span>
    <span class="s1">agg0 = </span><span class="s0">await </span><span class="s1">c.compute(result.partitions[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">agg1 = </span><span class="s0">await </span><span class="s1">c.compute(result.partitions[</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">dd.utils.assert_eq(</span>
        <span class="s1">pd.concat([agg0</span><span class="s0">, </span><span class="s1">agg1])</span><span class="s0">,</span>
        <span class="s1">pd.DataFrame({</span><span class="s2">&quot;x&quot;</span><span class="s1">: [</span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s2">&quot;y&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]}).set_index(</span><span class="s2">&quot;y&quot;</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_futures_in_graph(c):</span>
    <span class="s1">x</span><span class="s0">, </span><span class="s1">y = delayed(</span><span class="s4">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">delayed(</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">xx = delayed(add)(x</span><span class="s0">, </span><span class="s1">x)</span>
    <span class="s1">yy = delayed(add)(y</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">xxyy = delayed(add)(xx</span><span class="s0">, </span><span class="s1">yy)</span>

    <span class="s1">xxyy2 = c.persist(xxyy)</span>
    <span class="s1">xxyy3 = delayed(add)(xxyy2</span><span class="s0">, </span><span class="s4">10</span><span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">xxyy3.compute(scheduler=</span><span class="s2">&quot;dask.distributed&quot;</span><span class="s1">) == ((</span><span class="s4">1 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">) + (</span><span class="s4">2 </span><span class="s1">+ </span><span class="s4">2</span><span class="s1">)) + </span><span class="s4">10</span>


<span class="s0">def </span><span class="s1">test_zarr_distributed_roundtrip(c):</span>
    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s1">)</span>
    <span class="s1">pytest.importorskip(</span><span class="s2">&quot;zarr&quot;</span><span class="s1">)</span>

    <span class="s0">with </span><span class="s1">tmpdir() </span><span class="s0">as </span><span class="s1">d:</span>
        <span class="s1">a = da.zeros((</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">chunks=(</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">))</span>
        <span class="s1">a.to_zarr(d)</span>
        <span class="s1">a2 = da.from_zarr(d)</span>
        <span class="s1">da.assert_eq(a</span><span class="s0">, </span><span class="s1">a2</span><span class="s0">, </span><span class="s1">scheduler=c)</span>
        <span class="s0">assert </span><span class="s1">a2.chunks == a.chunks</span>


<span class="s0">def </span><span class="s1">test_zarr_in_memory_distributed_err(c):</span>
    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s1">)</span>
    <span class="s1">zarr = pytest.importorskip(</span><span class="s2">&quot;zarr&quot;</span><span class="s1">)</span>

    <span class="s1">chunks = (</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">a = da.ones((</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">chunks=chunks)</span>
    <span class="s1">z = zarr.zeros_like(a</span><span class="s0">, </span><span class="s1">chunks=chunks)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(RuntimeError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;distributed scheduler&quot;</span><span class="s1">):</span>
        <span class="s1">a.to_zarr(z)</span>


<span class="s0">def </span><span class="s1">test_scheduler_equals_client(c):</span>
    <span class="s1">x = delayed(</span><span class="s0">lambda</span><span class="s1">: </span><span class="s4">1</span><span class="s1">)()</span>
    <span class="s0">assert </span><span class="s1">x.compute(scheduler=c) == </span><span class="s4">1</span>
    <span class="s0">assert </span><span class="s1">c.run_on_scheduler(</span><span class="s0">lambda </span><span class="s1">dask_scheduler: dask_scheduler.story(x.key))</span>


<span class="s1">@gen_cluster(client=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_await(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b):</span>
    <span class="s1">x = dask.delayed(inc)(</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">x = </span><span class="s0">await </span><span class="s1">x.persist()</span>
    <span class="s0">assert </span><span class="s1">x.key </span><span class="s0">in </span><span class="s1">s.tasks</span>
    <span class="s0">assert </span><span class="s1">a.data </span><span class="s0">or </span><span class="s1">b.data</span>
    <span class="s0">assert </span><span class="s1">all(f.done() </span><span class="s0">for </span><span class="s1">f </span><span class="s0">in </span><span class="s1">futures_of(x))</span>


<span class="s0">def </span><span class="s1">test_local_scheduler():</span>
    <span class="s0">async def </span><span class="s1">f():</span>
        <span class="s1">x = dask.delayed(inc)(</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">y = x + </span><span class="s4">1</span>
        <span class="s1">z = </span><span class="s0">await </span><span class="s1">y.persist()</span>
        <span class="s0">assert </span><span class="s1">len(z.dask) == </span><span class="s4">1</span>

    <span class="s1">asyncio.run(f())</span>


<span class="s1">@gen_cluster(client=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_annotations_blockwise_unpack(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b):</span>
    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s1">)</span>
    <span class="s1">np = pytest.importorskip(</span><span class="s2">&quot;numpy&quot;</span><span class="s1">)</span>
    <span class="s0">from </span><span class="s1">dask.array.utils </span><span class="s0">import </span><span class="s1">assert_eq</span>

    <span class="s3"># A flaky doubling function -- need extra args because it is called before</span>
    <span class="s3"># application to establish dtype/meta.</span>
    <span class="s1">scale = varying([ZeroDivisionError(</span><span class="s2">&quot;one&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s1">ZeroDivisionError(</span><span class="s2">&quot;two&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>

    <span class="s0">def </span><span class="s1">flaky_double(x):</span>
        <span class="s0">return </span><span class="s1">scale() * x</span>

    <span class="s3"># A reliable double function.</span>
    <span class="s0">def </span><span class="s1">reliable_double(x):</span>
        <span class="s0">return </span><span class="s4">2 </span><span class="s1">* x</span>

    <span class="s1">x = da.ones(</span><span class="s4">10</span><span class="s0">, </span><span class="s1">chunks=(</span><span class="s4">5</span><span class="s0">,</span><span class="s1">))</span>

    <span class="s3"># The later annotations should not override the earlier annotations</span>
    <span class="s0">with </span><span class="s1">dask.annotate(retries=</span><span class="s4">2</span><span class="s1">):</span>
        <span class="s1">y = x.map_blocks(flaky_double</span><span class="s0">, </span><span class="s1">meta=np.array(()</span><span class="s0">, </span><span class="s1">dtype=np.float64))</span>
    <span class="s0">with </span><span class="s1">dask.annotate(retries=</span><span class="s4">0</span><span class="s1">):</span>
        <span class="s1">z = y.map_blocks(reliable_double</span><span class="s0">, </span><span class="s1">meta=np.array(()</span><span class="s0">, </span><span class="s1">dtype=np.float64))</span>

    <span class="s0">with </span><span class="s1">dask.config.set(optimization__fuse__active=</span><span class="s0">False</span><span class="s1">):</span>
        <span class="s1">z = </span><span class="s0">await </span><span class="s1">c.compute(z)</span>

    <span class="s1">assert_eq(z</span><span class="s0">, </span><span class="s1">np.ones(</span><span class="s4">10</span><span class="s1">) * </span><span class="s4">4.0</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;io&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s2">&quot;ones&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;zeros&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;full&quot;</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;fuse&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False, None</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_blockwise_array_creation(c</span><span class="s0">, </span><span class="s1">io</span><span class="s0">, </span><span class="s1">fuse):</span>
    <span class="s1">np = pytest.importorskip(</span><span class="s2">&quot;numpy&quot;</span><span class="s1">)</span>
    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s1">)</span>

    <span class="s1">chunks = (</span><span class="s4">5</span><span class="s0">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">shape = (</span><span class="s4">10</span><span class="s0">, </span><span class="s4">4</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">io == </span><span class="s2">&quot;ones&quot;</span><span class="s1">:</span>
        <span class="s1">darr = da.ones(shape</span><span class="s0">, </span><span class="s1">chunks=chunks)</span>
        <span class="s1">narr = np.ones(shape)</span>
    <span class="s0">elif </span><span class="s1">io == </span><span class="s2">&quot;zeros&quot;</span><span class="s1">:</span>
        <span class="s1">darr = da.zeros(shape</span><span class="s0">, </span><span class="s1">chunks=chunks)</span>
        <span class="s1">narr = np.zeros(shape)</span>
    <span class="s0">elif </span><span class="s1">io == </span><span class="s2">&quot;full&quot;</span><span class="s1">:</span>
        <span class="s1">darr = da.full(shape</span><span class="s0">, </span><span class="s4">10</span><span class="s0">, </span><span class="s1">chunks=chunks)</span>
        <span class="s1">narr = np.full(shape</span><span class="s0">, </span><span class="s4">10</span><span class="s1">)</span>

    <span class="s1">darr += </span><span class="s4">2</span>
    <span class="s1">narr += </span><span class="s4">2</span>
    <span class="s0">with </span><span class="s1">dask.config.set({</span><span class="s2">&quot;optimization.fuse.active&quot;</span><span class="s1">: fuse}):</span>
        <span class="s1">darr.compute()</span>
        <span class="s1">dsk = dask.array.optimize(darr.dask</span><span class="s0">, </span><span class="s1">darr.__dask_keys__())</span>
        <span class="s3"># dsk should be a dict unless fuse is explicitly False</span>
        <span class="s0">assert </span><span class="s1">isinstance(dsk</span><span class="s0">, </span><span class="s1">dict) == (fuse </span><span class="s0">is not False</span><span class="s1">)</span>
        <span class="s1">da.assert_eq(darr</span><span class="s0">, </span><span class="s1">narr</span><span class="s0">, </span><span class="s1">scheduler=c)</span>


<span class="s1">@ignore_sync_scheduler_warning</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;io&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s2">&quot;parquet-pyarrow&quot;</span><span class="s0">,</span>
        <span class="s1">pytest.param(</span>
            <span class="s2">&quot;parquet-fastparquet&quot;</span><span class="s0">, </span><span class="s1">marks=pytest.mark.skip_with_pyarrow_strings</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s2">&quot;csv&quot;</span><span class="s0">,</span>
        <span class="s3"># See https://github.com/dask/dask/issues/9793</span>
        <span class="s1">pytest.param(</span><span class="s2">&quot;hdf&quot;</span><span class="s0">, </span><span class="s1">marks=pytest.mark.flaky(reruns=</span><span class="s4">5</span><span class="s1">))</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;fuse&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False, None</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;from_futures&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_blockwise_dataframe_io(c</span><span class="s0">, </span><span class="s1">tmpdir</span><span class="s0">, </span><span class="s1">io</span><span class="s0">, </span><span class="s1">fuse</span><span class="s0">, </span><span class="s1">from_futures):</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s1">df = pd.DataFrame({</span><span class="s2">&quot;x&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">3</span><span class="s1">] * </span><span class="s4">5</span><span class="s0">, </span><span class="s2">&quot;y&quot;</span><span class="s1">: range(</span><span class="s4">15</span><span class="s1">)})</span>

    <span class="s0">if </span><span class="s1">from_futures:</span>
        <span class="s1">parts = [df.iloc[:</span><span class="s4">5</span><span class="s1">]</span><span class="s0">, </span><span class="s1">df.iloc[</span><span class="s4">5</span><span class="s1">:</span><span class="s4">10</span><span class="s1">]</span><span class="s0">, </span><span class="s1">df.iloc[</span><span class="s4">10</span><span class="s1">:</span><span class="s4">15</span><span class="s1">]]</span>
        <span class="s1">futs = c.scatter(parts)</span>
        <span class="s1">ddf0 = dd.from_delayed(futs</span><span class="s0">, </span><span class="s1">meta=parts[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">ddf0 = dd.from_pandas(df</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">3</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">io.startswith(</span><span class="s2">&quot;parquet&quot;</span><span class="s1">):</span>
        <span class="s0">if </span><span class="s1">io == </span><span class="s2">&quot;parquet-pyarrow&quot;</span><span class="s1">:</span>
            <span class="s1">pytest.importorskip(</span><span class="s2">&quot;pyarrow.parquet&quot;</span><span class="s1">)</span>
            <span class="s1">engine = </span><span class="s2">&quot;pyarrow&quot;</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">pytest.importorskip(</span><span class="s2">&quot;fastparquet&quot;</span><span class="s1">)</span>
            <span class="s1">engine = </span><span class="s2">&quot;fastparquet&quot;</span>
        <span class="s1">ddf0.to_parquet(str(tmpdir)</span><span class="s0">, </span><span class="s1">engine=engine)</span>
        <span class="s1">ddf = dd.read_parquet(str(tmpdir)</span><span class="s0">, </span><span class="s1">engine=engine)</span>
    <span class="s0">elif </span><span class="s1">io == </span><span class="s2">&quot;csv&quot;</span><span class="s1">:</span>
        <span class="s1">ddf0.to_csv(str(tmpdir)</span><span class="s0">, </span><span class="s1">index=</span><span class="s0">False</span><span class="s1">)</span>
        <span class="s1">ddf = dd.read_csv(os.path.join(str(tmpdir)</span><span class="s0">, </span><span class="s2">&quot;*&quot;</span><span class="s1">))</span>
    <span class="s0">elif </span><span class="s1">io == </span><span class="s2">&quot;hdf&quot;</span><span class="s1">:</span>
        <span class="s1">pytest.importorskip(</span><span class="s2">&quot;tables&quot;</span><span class="s1">)</span>
        <span class="s1">fn = str(tmpdir.join(</span><span class="s2">&quot;h5&quot;</span><span class="s1">))</span>
        <span class="s1">ddf0.to_hdf(fn</span><span class="s0">, </span><span class="s2">&quot;/data*&quot;</span><span class="s1">)</span>
        <span class="s1">ddf = dd.read_hdf(fn</span><span class="s0">, </span><span class="s2">&quot;/data*&quot;</span><span class="s1">)</span>

    <span class="s1">df = df[[</span><span class="s2">&quot;x&quot;</span><span class="s1">]] + </span><span class="s4">10</span>
    <span class="s1">ddf = ddf[[</span><span class="s2">&quot;x&quot;</span><span class="s1">]] + </span><span class="s4">10</span>
    <span class="s0">with </span><span class="s1">dask.config.set({</span><span class="s2">&quot;optimization.fuse.active&quot;</span><span class="s1">: fuse}):</span>
        <span class="s1">ddf.compute()</span>
        <span class="s1">dsk = dask.dataframe.optimize(ddf.dask</span><span class="s0">, </span><span class="s1">ddf.__dask_keys__())</span>
        <span class="s3"># dsk should not be a dict unless fuse is explicitly True</span>
        <span class="s0">assert </span><span class="s1">isinstance(dsk</span><span class="s0">, </span><span class="s1">dict) == bool(fuse)</span>

        <span class="s1">dd.assert_eq(ddf</span><span class="s0">, </span><span class="s1">df</span><span class="s0">, </span><span class="s1">check_index=</span><span class="s0">False</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_blockwise_fusion_after_compute(c):</span>
    <span class="s3"># See: https://github.com/dask/dask/issues/7720</span>

    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s3"># Simple sequence of Dask-Dataframe manipulations</span>
    <span class="s1">df = pd.DataFrame({</span><span class="s2">&quot;x&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">3</span><span class="s1">] * </span><span class="s4">5</span><span class="s1">})</span>
    <span class="s1">series = dd.from_pandas(df</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">2</span><span class="s1">)[</span><span class="s2">&quot;x&quot;</span><span class="s1">]</span>
    <span class="s1">result = series &lt; </span><span class="s4">3</span>

    <span class="s3"># Trigger an optimization of the `series` graph</span>
    <span class="s3"># (which `result` depends on), then compute `result`.</span>
    <span class="s3"># This is essentially a test of `rewrite_blockwise`.</span>
    <span class="s1">series_len = len(series)</span>
    <span class="s0">assert </span><span class="s1">series_len == </span><span class="s4">15</span>
    <span class="s0">assert </span><span class="s1">df.x[result.compute()].sum() == </span><span class="s4">15</span>


<span class="s1">@gen_cluster(client=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_blockwise_numpy_args(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b):</span>
    <span class="s5">&quot;&quot;&quot;Test pack/unpack of blockwise that includes a NumPy literal argument&quot;&quot;&quot;</span>
    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s1">)</span>
    <span class="s1">np = pytest.importorskip(</span><span class="s2">&quot;numpy&quot;</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">fn(x</span><span class="s0">, </span><span class="s1">dt):</span>
        <span class="s0">assert </span><span class="s1">type(dt) </span><span class="s0">is </span><span class="s1">np.uint16</span>
        <span class="s0">return </span><span class="s1">x.astype(dt)</span>

    <span class="s1">arr = da.blockwise(</span>
        <span class="s1">fn</span><span class="s0">, </span><span class="s2">&quot;x&quot;</span><span class="s0">, </span><span class="s1">da.ones(</span><span class="s4">1000</span><span class="s1">)</span><span class="s0">, </span><span class="s2">&quot;x&quot;</span><span class="s0">, </span><span class="s1">np.uint16(</span><span class="s4">42</span><span class="s1">)</span><span class="s0">, None, </span><span class="s1">dtype=np.uint16</span>
    <span class="s1">)</span>
    <span class="s1">res = </span><span class="s0">await </span><span class="s1">c.compute(arr.sum()</span><span class="s0">, </span><span class="s1">optimize_graph=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">res == </span><span class="s4">1000</span>


<span class="s1">@gen_cluster(client=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_blockwise_numpy_kwargs(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b):</span>
    <span class="s5">&quot;&quot;&quot;Test pack/unpack of blockwise that includes a NumPy literal keyword argument&quot;&quot;&quot;</span>
    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s1">)</span>
    <span class="s1">np = pytest.importorskip(</span><span class="s2">&quot;numpy&quot;</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">fn(x</span><span class="s0">, </span><span class="s1">dt=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s0">assert </span><span class="s1">type(dt) </span><span class="s0">is </span><span class="s1">np.uint16</span>
        <span class="s0">return </span><span class="s1">x.astype(dt)</span>

    <span class="s1">arr = da.blockwise(fn</span><span class="s0">, </span><span class="s2">&quot;x&quot;</span><span class="s0">, </span><span class="s1">da.ones(</span><span class="s4">1000</span><span class="s1">)</span><span class="s0">, </span><span class="s2">&quot;x&quot;</span><span class="s0">, </span><span class="s1">dtype=np.uint16</span><span class="s0">, </span><span class="s1">dt=np.uint16(</span><span class="s4">42</span><span class="s1">))</span>
    <span class="s1">res = </span><span class="s0">await </span><span class="s1">c.compute(arr.sum()</span><span class="s0">, </span><span class="s1">optimize_graph=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">res == </span><span class="s4">1000</span>


<span class="s0">def </span><span class="s1">test_blockwise_different_optimization(c):</span>
    <span class="s3"># Regression test for incorrect results due to SubgraphCallable.__eq__</span>
    <span class="s3"># not correctly handling subgraphs with the same outputs and arity but</span>
    <span class="s3"># different internals (GH-7632). The bug is triggered by distributed</span>
    <span class="s3"># because it uses a function cache.</span>
    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s1">)</span>
    <span class="s1">np = pytest.importorskip(</span><span class="s2">&quot;numpy&quot;</span><span class="s1">)</span>

    <span class="s1">u = da.from_array(np.arange(</span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">v = da.from_array(np.array([</span><span class="s4">10 </span><span class="s1">+ </span><span class="s4">2j</span><span class="s0">, </span><span class="s4">7 </span><span class="s1">- </span><span class="s4">3j</span><span class="s0">, </span><span class="s4">8 </span><span class="s1">+ </span><span class="s4">1j</span><span class="s1">]))</span>
    <span class="s1">cv = v.conj()</span>
    <span class="s1">x = u * cv</span>
    <span class="s1">(cv</span><span class="s0">,</span><span class="s1">) = dask.optimize(cv)</span>
    <span class="s1">y = u * cv</span>
    <span class="s1">expected = np.array([</span><span class="s4">0 </span><span class="s1">+ </span><span class="s4">0j</span><span class="s0">, </span><span class="s4">7 </span><span class="s1">+ </span><span class="s4">3j</span><span class="s0">, </span><span class="s4">16 </span><span class="s1">- </span><span class="s4">2j</span><span class="s1">])</span>
    <span class="s0">with </span><span class="s1">dask.config.set({</span><span class="s2">&quot;optimization.fuse.active&quot;</span><span class="s1">: </span><span class="s0">False</span><span class="s1">}):</span>
        <span class="s1">x_value = x.compute()</span>
        <span class="s1">y_value = y.compute()</span>
    <span class="s1">np.testing.assert_equal(x_value</span><span class="s0">, </span><span class="s1">expected)</span>
    <span class="s1">np.testing.assert_equal(y_value</span><span class="s0">, </span><span class="s1">expected)</span>


<span class="s1">@gen_cluster(client=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_combo_of_layer_types(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b):</span>
    <span class="s5">&quot;&quot;&quot;Check pack/unpack of a HLG that has every type of Layers!&quot;&quot;&quot;</span>

    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>
    <span class="s1">np = pytest.importorskip(</span><span class="s2">&quot;numpy&quot;</span><span class="s1">)</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">add(x</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">z</span><span class="s0">, </span><span class="s1">extra_arg):</span>
        <span class="s0">return </span><span class="s1">x + y + z + extra_arg</span>

    <span class="s1">y = c.submit(</span><span class="s0">lambda </span><span class="s1">x: x</span><span class="s0">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">z = c.submit(</span><span class="s0">lambda </span><span class="s1">x: x</span><span class="s0">, </span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">x = da.blockwise(</span>
        <span class="s1">add</span><span class="s0">,</span>
        <span class="s2">&quot;x&quot;</span><span class="s0">,</span>
        <span class="s1">da.zeros((</span><span class="s4">3</span><span class="s0">,</span><span class="s1">)</span><span class="s0">, </span><span class="s1">chunks=(</span><span class="s4">1</span><span class="s0">,</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s2">&quot;x&quot;</span><span class="s0">,</span>
        <span class="s1">da.ones((</span><span class="s4">3</span><span class="s0">,</span><span class="s1">)</span><span class="s0">, </span><span class="s1">chunks=(</span><span class="s4">1</span><span class="s0">,</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s2">&quot;x&quot;</span><span class="s0">,</span>
        <span class="s1">y</span><span class="s0">,</span>
        <span class="s0">None,</span>
        <span class="s1">concatenate=</span><span class="s0">False,</span>
        <span class="s1">dtype=int</span><span class="s0">,</span>
        <span class="s1">extra_arg=z</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">df = dd.from_pandas(pd.DataFrame({</span><span class="s2">&quot;a&quot;</span><span class="s1">: np.arange(</span><span class="s4">3</span><span class="s1">)})</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">df = df.shuffle(</span><span class="s2">&quot;a&quot;</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s2">&quot;tasks&quot;</span><span class="s1">)</span>
    <span class="s1">df = df[</span><span class="s2">&quot;a&quot;</span><span class="s1">].to_dask_array()</span>

    <span class="s1">res = x.sum() + df.sum()</span>
    <span class="s1">res = </span><span class="s0">await </span><span class="s1">c.compute(res</span><span class="s0">, </span><span class="s1">optimize_graph=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">res == </span><span class="s4">21</span>


<span class="s0">def </span><span class="s1">test_blockwise_concatenate(c):</span>
    <span class="s5">&quot;&quot;&quot;Test a blockwise operation with concatenated axes&quot;&quot;&quot;</span>
    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s1">)</span>
    <span class="s1">np = pytest.importorskip(</span><span class="s2">&quot;numpy&quot;</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">f(x</span><span class="s0">, </span><span class="s1">y):</span>
        <span class="s1">da.assert_eq(y</span><span class="s0">, </span><span class="s1">[[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]])</span>
        <span class="s0">return </span><span class="s1">x</span>

    <span class="s1">x = da.from_array(np.array([</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]))</span>
    <span class="s1">y = da.from_array(np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]]))</span>
    <span class="s1">z = da.blockwise(</span>
        <span class="s1">f</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;i&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">x</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;i&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">y</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;ij&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">dtype=x.dtype</span><span class="s0">,</span>
        <span class="s1">concatenate=</span><span class="s0">True,</span>
    <span class="s1">)</span>
    <span class="s1">c.compute(z</span><span class="s0">, </span><span class="s1">optimize_graph=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">da.assert_eq(z</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">scheduler=c)</span>


<span class="s1">@gen_cluster(client=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_map_partitions_partition_info(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b):</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>

    <span class="s1">ddf = dd.from_pandas(pd.DataFrame({</span><span class="s2">&quot;a&quot;</span><span class="s1">: range(</span><span class="s4">10</span><span class="s1">)})</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">res = </span><span class="s0">await </span><span class="s1">c.compute(</span>
        <span class="s1">ddf.map_partitions(</span><span class="s0">lambda </span><span class="s1">x</span><span class="s0">, </span><span class="s1">partition_info=</span><span class="s0">None</span><span class="s1">: partition_info)</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">res[</span><span class="s4">0</span><span class="s1">] == {</span><span class="s2">&quot;number&quot;</span><span class="s1">: </span><span class="s4">0</span><span class="s0">, </span><span class="s2">&quot;division&quot;</span><span class="s1">: </span><span class="s4">0</span><span class="s1">}</span>
    <span class="s0">assert </span><span class="s1">res[</span><span class="s4">1</span><span class="s1">] == {</span><span class="s2">&quot;number&quot;</span><span class="s1">: </span><span class="s4">1</span><span class="s0">, </span><span class="s2">&quot;division&quot;</span><span class="s1">: </span><span class="s4">5</span><span class="s1">}</span>


<span class="s1">@gen_cluster(client=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_futures_in_subgraphs(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b):</span>
    <span class="s5">&quot;&quot;&quot;Copied from distributed (tests/test_client.py)&quot;&quot;&quot;</span>

    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>

    <span class="s1">ddf = dd.from_pandas(</span>
        <span class="s1">pd.DataFrame(</span>
            <span class="s1">dict(</span>
                <span class="s1">uid=range(</span><span class="s4">50</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">enter_time=pd.date_range(</span>
                    <span class="s1">start=</span><span class="s2">&quot;2020-01-01&quot;</span><span class="s0">, </span><span class="s1">end=</span><span class="s2">&quot;2020-09-01&quot;</span><span class="s0">, </span><span class="s1">periods=</span><span class="s4">50</span><span class="s0">, </span><span class="s1">tz=</span><span class="s2">&quot;UTC&quot;</span>
                <span class="s1">)</span><span class="s0">,</span>
            <span class="s1">)</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">npartitions=</span><span class="s4">1</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">ddf = ddf[ddf.uid.isin(range(</span><span class="s4">29</span><span class="s1">))].persist()</span>
    <span class="s1">ddf[</span><span class="s2">&quot;day&quot;</span><span class="s1">] = ddf.enter_time.dt.day_name()</span>
    <span class="s1">ddf = </span><span class="s0">await </span><span class="s1">c.submit(dd.categorical.categorize</span><span class="s0">, </span><span class="s1">ddf</span><span class="s0">, </span><span class="s1">columns=[</span><span class="s2">&quot;day&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">index=</span><span class="s0">False</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;max_branch, expected_layer_type&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s4">32</span><span class="s0">, </span><span class="s1">SimpleShuffleLayer)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">2</span><span class="s0">, </span><span class="s1">ShuffleLayer)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@gen_cluster(client=</span><span class="s0">True, </span><span class="s1">nthreads=[(</span><span class="s2">&quot;&quot;</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)] * </span><span class="s4">2</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_shuffle_priority(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b</span><span class="s0">, </span><span class="s1">max_branch</span><span class="s0">, </span><span class="s1">expected_layer_type):</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s0">class </span><span class="s1">EnsureSplitsRunImmediatelyPlugin(WorkerPlugin):</span>
        <span class="s1">failure = </span><span class="s0">False</span>

        <span class="s0">def </span><span class="s1">setup(self</span><span class="s0">, </span><span class="s1">worker):</span>
            <span class="s1">self.worker = worker</span>

        <span class="s0">def </span><span class="s1">transition(self</span><span class="s0">, </span><span class="s1">key</span><span class="s0">, </span><span class="s1">start</span><span class="s0">, </span><span class="s1">finish</span><span class="s0">, </span><span class="s1">**kwargs):</span>
            <span class="s0">if </span><span class="s1">finish == </span><span class="s2">&quot;executing&quot; </span><span class="s0">and not </span><span class="s1">all(</span>
                <span class="s2">&quot;split&quot; </span><span class="s0">in </span><span class="s1">ts.key </span><span class="s0">for </span><span class="s1">ts </span><span class="s0">in </span><span class="s1">self.worker.state.executing</span>
            <span class="s1">):</span>
                <span class="s0">if </span><span class="s1">any(</span><span class="s2">&quot;split&quot; </span><span class="s0">in </span><span class="s1">ts.key </span><span class="s0">for </span><span class="s1">ts </span><span class="s0">in </span><span class="s1">list(self.worker.state.ready)):</span>
                    <span class="s1">EnsureSplitsRunImmediatelyPlugin.failure = </span><span class="s0">True</span>
                    <span class="s0">raise </span><span class="s1">RuntimeError(</span><span class="s2">&quot;Split tasks are not prioritized&quot;</span><span class="s1">)</span>

    <span class="s0">await </span><span class="s1">c.register_plugin(EnsureSplitsRunImmediatelyPlugin())</span>

    <span class="s3"># Test marked as &quot;flaky&quot; since the scheduling behavior</span>
    <span class="s3"># is not deterministic. Note that the test is still</span>
    <span class="s3"># very likely to fail every time if the &quot;split&quot; tasks</span>
    <span class="s3"># are not prioritized correctly</span>

    <span class="s1">df = pd.DataFrame({</span><span class="s2">&quot;a&quot;</span><span class="s1">: range(</span><span class="s4">1000</span><span class="s1">)})</span>
    <span class="s1">ddf = dd.from_pandas(df</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">10</span><span class="s1">)</span>

    <span class="s1">ddf2 = ddf.shuffle(</span><span class="s2">&quot;a&quot;</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s2">&quot;tasks&quot;</span><span class="s0">, </span><span class="s1">max_branch=max_branch)</span>

    <span class="s1">shuffle_layers = set(ddf2.dask.layers) - set(ddf.dask.layers)</span>
    <span class="s0">for </span><span class="s1">layer_name </span><span class="s0">in </span><span class="s1">shuffle_layers:</span>
        <span class="s0">assert </span><span class="s1">isinstance(ddf2.dask.layers[layer_name]</span><span class="s0">, </span><span class="s1">expected_layer_type)</span>
    <span class="s0">await </span><span class="s1">c.compute(ddf2)</span>
    <span class="s0">assert not </span><span class="s1">EnsureSplitsRunImmediatelyPlugin.failure</span>


<span class="s1">@gen_cluster(client=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_map_partitions_da_input(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b):</span>
    <span class="s5">&quot;&quot;&quot;Check that map_partitions can handle a dask array input&quot;&quot;&quot;</span>
    <span class="s1">np = pytest.importorskip(</span><span class="s2">&quot;numpy&quot;</span><span class="s1">)</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s1">)</span>
    <span class="s1">datasets = pytest.importorskip(</span><span class="s2">&quot;dask.datasets&quot;</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">f(d</span><span class="s0">, </span><span class="s1">a):</span>
        <span class="s0">assert </span><span class="s1">isinstance(d</span><span class="s0">, </span><span class="s1">pd.DataFrame)</span>
        <span class="s0">assert </span><span class="s1">isinstance(a</span><span class="s0">, </span><span class="s1">np.ndarray)</span>
        <span class="s0">return </span><span class="s1">d</span>

    <span class="s1">df = datasets.timeseries(freq=</span><span class="s2">&quot;1d&quot;</span><span class="s1">).persist()</span>
    <span class="s1">arr = da.ones((</span><span class="s4">1</span><span class="s0">,</span><span class="s1">)</span><span class="s0">, </span><span class="s1">chunks=</span><span class="s4">1</span><span class="s1">).persist()</span>
    <span class="s0">await </span><span class="s1">c.compute(df.map_partitions(f</span><span class="s0">, </span><span class="s1">arr</span><span class="s0">, </span><span class="s1">meta=df._meta))</span>


<span class="s0">def </span><span class="s1">test_map_partitions_df_input():</span>
    <span class="s5">&quot;&quot;&quot; 
    Check that map_partitions can handle a delayed 
    partition of a dataframe input 
    &quot;&quot;&quot;</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">f(d</span><span class="s0">, </span><span class="s1">a):</span>
        <span class="s0">assert </span><span class="s1">isinstance(d</span><span class="s0">, </span><span class="s1">pd.DataFrame)</span>
        <span class="s0">assert </span><span class="s1">isinstance(a</span><span class="s0">, </span><span class="s1">pd.DataFrame)</span>
        <span class="s0">return </span><span class="s1">d</span>

    <span class="s0">def </span><span class="s1">main():</span>
        <span class="s1">item_df = dd.from_pandas(pd.DataFrame({</span><span class="s2">&quot;a&quot;</span><span class="s1">: range(</span><span class="s4">10</span><span class="s1">)})</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">ddf = item_df.to_delayed()[</span><span class="s4">0</span><span class="s1">].persist()</span>
        <span class="s1">merged_df = dd.from_pandas(pd.DataFrame({</span><span class="s2">&quot;b&quot;</span><span class="s1">: range(</span><span class="s4">10</span><span class="s1">)})</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s3"># Notice, we include a shuffle in order to trigger a complex culling</span>
        <span class="s1">merged_df = merged_df.shuffle(on=</span><span class="s2">&quot;b&quot;</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s2">&quot;tasks&quot;</span><span class="s1">)</span>

        <span class="s1">merged_df.map_partitions(</span>
            <span class="s1">f</span><span class="s0">, </span><span class="s1">ddf</span><span class="s0">, </span><span class="s1">meta=merged_df</span><span class="s0">, </span><span class="s1">enforce_metadata=</span><span class="s0">False</span>
        <span class="s1">).compute()</span>

    <span class="s0">with </span><span class="s1">distributed.LocalCluster(</span>
        <span class="s1">scheduler_port=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s3"># Explicitly disabling dashboard to prevent related warnings being</span>
        <span class="s3"># elevated to errors until `bokeh=3` is fully supported.</span>
        <span class="s3"># See https://github.com/dask/dask/issues/9686 and</span>
        <span class="s3"># https://github.com/dask/distributed/issues/7173 for details.</span>
        <span class="s1">dashboard_address=</span><span class="s2">&quot;:0&quot;</span><span class="s0">,</span>
        <span class="s1">scheduler_kwargs={</span><span class="s2">&quot;dashboard&quot;</span><span class="s1">: </span><span class="s0">False</span><span class="s1">}</span><span class="s0">,</span>
        <span class="s1">asynchronous=</span><span class="s0">False,</span>
        <span class="s1">n_workers=</span><span class="s4">1</span><span class="s0">,</span>
        <span class="s1">nthreads=</span><span class="s4">1</span><span class="s0">,</span>
        <span class="s1">processes=</span><span class="s0">False,</span>
    <span class="s1">) </span><span class="s0">as </span><span class="s1">cluster:</span>
        <span class="s0">with </span><span class="s1">distributed.Client(cluster</span><span class="s0">, </span><span class="s1">asynchronous=</span><span class="s0">False</span><span class="s1">):</span>
            <span class="s1">main()</span>


<span class="s1">@pytest.mark.filterwarnings(</span>
    <span class="s2">&quot;ignore:Running on a single-machine scheduler when a distributed client &quot;</span>
    <span class="s2">&quot;is active might lead to unexpected results.&quot;</span>
<span class="s1">)</span>
<span class="s1">@gen_cluster(client=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_to_sql_engine_kwargs(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b):</span>
    <span class="s3"># https://github.com/dask/dask/issues/8738</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>
    <span class="s1">pytest.importorskip(</span><span class="s2">&quot;sqlalchemy&quot;</span><span class="s1">)</span>

    <span class="s1">df = pd.DataFrame({</span><span class="s2">&quot;a&quot;</span><span class="s1">: range(</span><span class="s4">10</span><span class="s1">)</span><span class="s0">, </span><span class="s2">&quot;b&quot;</span><span class="s1">: range(</span><span class="s4">10</span><span class="s1">)})</span>
    <span class="s1">df.index.name = </span><span class="s2">&quot;index&quot;</span>
    <span class="s1">ddf = dd.from_pandas(df</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">tmpfile() </span><span class="s0">as </span><span class="s1">f:</span>
        <span class="s1">uri = </span><span class="s2">f&quot;sqlite:///</span><span class="s0">{</span><span class="s1">f</span><span class="s0">}</span><span class="s2">&quot;</span>
        <span class="s1">result = ddf.to_sql(</span>
            <span class="s2">&quot;test&quot;</span><span class="s0">, </span><span class="s1">uri</span><span class="s0">, </span><span class="s1">index=</span><span class="s0">True, </span><span class="s1">engine_kwargs={</span><span class="s2">&quot;echo&quot;</span><span class="s1">: </span><span class="s0">False</span><span class="s1">}</span><span class="s0">, </span><span class="s1">compute=</span><span class="s0">False</span>
        <span class="s1">)</span>
        <span class="s0">await </span><span class="s1">c.compute(result)</span>

        <span class="s1">dd.utils.assert_eq(</span>
            <span class="s1">ddf</span><span class="s0">,</span>
            <span class="s1">dd.read_sql_table(</span><span class="s2">&quot;test&quot;</span><span class="s0">, </span><span class="s1">uri</span><span class="s0">, </span><span class="s2">&quot;index&quot;</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">check_divisions=</span><span class="s0">False,</span>
        <span class="s1">)</span>


<span class="s1">@gen_cluster(client=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_non_recursive_df_reduce(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b):</span>
    <span class="s3"># See https://github.com/dask/dask/issues/8773</span>

    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>

    <span class="s0">class </span><span class="s1">SomeObject:</span>
        <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">val):</span>
            <span class="s1">self.val = val</span>

    <span class="s1">N = </span><span class="s4">170</span>
    <span class="s1">series = pd.Series(data=[</span><span class="s4">1</span><span class="s1">] * N</span><span class="s0">, </span><span class="s1">index=range(</span><span class="s4">2</span><span class="s0">, </span><span class="s1">N + </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">dask_series = dd.from_pandas(series</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">34</span><span class="s1">)</span>
    <span class="s1">result = dask_series.reduction(</span>
        <span class="s1">chunk=</span><span class="s0">lambda </span><span class="s1">x: x</span><span class="s0">,</span>
        <span class="s1">aggregate=</span><span class="s0">lambda </span><span class="s1">x: SomeObject(x.sum().sum())</span><span class="s0">,</span>
        <span class="s1">split_every=</span><span class="s0">False,</span>
        <span class="s1">token=</span><span class="s2">&quot;commit-dataset&quot;</span><span class="s0">,</span>
        <span class="s1">meta=object</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">(</span><span class="s0">await </span><span class="s1">c.compute(result)).val == </span><span class="s4">170</span>


<span class="s0">def </span><span class="s1">test_set_index_no_resursion_error(c):</span>
    <span class="s3"># see: https://github.com/dask/dask/issues/8955</span>
    <span class="s1">pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">ddf = (</span>
            <span class="s1">dask.datasets.timeseries(start=</span><span class="s2">&quot;2000-01-01&quot;</span><span class="s0">, </span><span class="s1">end=</span><span class="s2">&quot;2000-07-01&quot;</span><span class="s0">, </span><span class="s1">freq=</span><span class="s2">&quot;12h&quot;</span><span class="s1">)</span>
            <span class="s1">.reset_index()</span>
            <span class="s1">.astype({</span><span class="s2">&quot;timestamp&quot;</span><span class="s1">: str})</span>
        <span class="s1">)</span>
        <span class="s1">ddf = ddf.set_index(</span><span class="s2">&quot;timestamp&quot;</span><span class="s0">, </span><span class="s1">sorted=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s1">ddf.compute()</span>
    <span class="s0">except </span><span class="s1">RecursionError:</span>
        <span class="s1">pytest.fail(</span><span class="s2">&quot;dd.set_index triggered a recursion error&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_get_scheduler_without_distributed_raises():</span>
    <span class="s1">msg = </span><span class="s2">&quot;no Client&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(RuntimeError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">get_scheduler(scheduler=</span><span class="s2">&quot;dask.distributed&quot;</span><span class="s1">)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(RuntimeError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">get_scheduler(scheduler=</span><span class="s2">&quot;distributed&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_get_scheduler_with_distributed_active(c):</span>
    <span class="s0">assert </span><span class="s1">get_scheduler() == c.get</span>
    <span class="s1">warning_message = (</span>
        <span class="s2">&quot;Running on a single-machine scheduler when a distributed client &quot;</span>
        <span class="s2">&quot;is active might lead to unexpected results.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=warning_message) </span><span class="s0">as </span><span class="s1">user_warnings_a:</span>
        <span class="s1">get_scheduler(scheduler=</span><span class="s2">&quot;threads&quot;</span><span class="s1">)</span>
        <span class="s1">get_scheduler(scheduler=</span><span class="s2">&quot;sync&quot;</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">len(user_warnings_a) == </span><span class="s4">2</span>


<span class="s0">def </span><span class="s1">test_get_scheduler_with_distributed_active_reset_config(c):</span>
    <span class="s0">assert </span><span class="s1">get_scheduler() == c.get</span>
    <span class="s0">with </span><span class="s1">dask.config.set(scheduler=</span><span class="s2">&quot;threads&quot;</span><span class="s1">):</span>
        <span class="s0">with </span><span class="s1">pytest.warns(UserWarning):</span>
            <span class="s0">assert </span><span class="s1">get_scheduler() != c.get</span>
        <span class="s0">with </span><span class="s1">dask.config.set(scheduler=</span><span class="s0">None</span><span class="s1">):</span>
            <span class="s0">assert </span><span class="s1">get_scheduler() == c.get</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;scheduler, expected_classes&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s0">None, </span><span class="s1">(</span><span class="s2">&quot;SerializableLock&quot;</span><span class="s0">, </span><span class="s2">&quot;SerializableLock&quot;</span><span class="s0">, </span><span class="s2">&quot;AcquirerProxy&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;threads&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s2">&quot;SerializableLock&quot;</span><span class="s0">, </span><span class="s2">&quot;SerializableLock&quot;</span><span class="s0">, </span><span class="s2">&quot;SerializableLock&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;processes&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s2">&quot;AcquirerProxy&quot;</span><span class="s0">, </span><span class="s2">&quot;AcquirerProxy&quot;</span><span class="s0">, </span><span class="s2">&quot;AcquirerProxy&quot;</span><span class="s1">))</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_get_scheduler_lock(scheduler</span><span class="s0">, </span><span class="s1">expected_classes):</span>
    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s0">, </span><span class="s1">reason=</span><span class="s2">&quot;Requires dask.array&quot;</span><span class="s1">)</span>
    <span class="s1">db = pytest.importorskip(</span><span class="s2">&quot;dask.bag&quot;</span><span class="s0">, </span><span class="s1">reason=</span><span class="s2">&quot;Requires dask.bag&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s0">, </span><span class="s1">reason=</span><span class="s2">&quot;Requires dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s1">darr = da.ones((</span><span class="s4">100</span><span class="s0">,</span><span class="s1">))</span>
    <span class="s1">ddf = dd.from_dask_array(darr</span><span class="s0">, </span><span class="s1">columns=[</span><span class="s2">&quot;x&quot;</span><span class="s1">])</span>
    <span class="s1">dbag = db.range(</span><span class="s4">100</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">collection</span><span class="s0">, </span><span class="s1">expected </span><span class="s0">in </span><span class="s1">zip((ddf</span><span class="s0">, </span><span class="s1">darr</span><span class="s0">, </span><span class="s1">dbag)</span><span class="s0">, </span><span class="s1">expected_classes):</span>
        <span class="s1">res = get_scheduler_lock(collection</span><span class="s0">, </span><span class="s1">scheduler=scheduler)</span>
        <span class="s0">assert </span><span class="s1">res.__class__.__name__ == expected</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;multiprocessing_method&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s2">&quot;spawn&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;fork&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;forkserver&quot;</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_get_scheduler_lock_distributed(c</span><span class="s0">, </span><span class="s1">multiprocessing_method):</span>
    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s0">, </span><span class="s1">reason=</span><span class="s2">&quot;Requires dask.array&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s0">, </span><span class="s1">reason=</span><span class="s2">&quot;Requires dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s1">darr = da.ones((</span><span class="s4">100</span><span class="s0">,</span><span class="s1">))</span>
    <span class="s1">ddf = dd.from_dask_array(darr</span><span class="s0">, </span><span class="s1">columns=[</span><span class="s2">&quot;x&quot;</span><span class="s1">])</span>
    <span class="s1">dbag = db.range(</span><span class="s4">100</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s0">with </span><span class="s1">dask.config.set(</span>
        <span class="s1">{</span><span class="s2">&quot;distributed.worker.multiprocessing-method&quot;</span><span class="s1">: multiprocessing_method}</span>
    <span class="s1">):</span>
        <span class="s0">for </span><span class="s1">collection </span><span class="s0">in </span><span class="s1">(ddf</span><span class="s0">, </span><span class="s1">darr</span><span class="s0">, </span><span class="s1">dbag):</span>
            <span class="s1">res = get_scheduler_lock(collection</span><span class="s0">, </span><span class="s1">scheduler=</span><span class="s2">&quot;distributed&quot;</span><span class="s1">)</span>
            <span class="s0">assert </span><span class="s1">isinstance(res</span><span class="s0">, </span><span class="s1">distributed.lock.Lock)</span>


<span class="s1">@pytest.mark.skip_with_pyarrow_strings  </span><span class="s3"># AttributeError: 'StringDtype' object has no attribute 'itemsize'</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;lock_param&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, </span><span class="s1">distributed.lock.Lock()])</span>
<span class="s0">def </span><span class="s1">test_write_single_hdf(c</span><span class="s0">, </span><span class="s1">lock_param):</span>
    <span class="s5">&quot;&quot;&quot;https://github.com/dask/dask/issues/9972 and 
    https://github.com/dask/dask/issues/10315 
    &quot;&quot;&quot;</span>
    <span class="s1">pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>
    <span class="s1">pytest.importorskip(</span><span class="s2">&quot;tables&quot;</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">tmpfile(extension=</span><span class="s2">&quot;hd5&quot;</span><span class="s1">) </span><span class="s0">as </span><span class="s1">f:</span>
        <span class="s1">ddf = dask.datasets.timeseries(start=</span><span class="s2">&quot;2000-01-01&quot;</span><span class="s0">, </span><span class="s1">end=</span><span class="s2">&quot;2000-07-01&quot;</span><span class="s0">, </span><span class="s1">freq=</span><span class="s2">&quot;12h&quot;</span><span class="s1">)</span>
        <span class="s1">ddf.to_hdf(str(f)</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;/ds_*&quot;</span><span class="s0">, </span><span class="s1">lock=lock_param)</span>


<span class="s1">@gen_cluster(config={</span><span class="s2">&quot;scheduler&quot;</span><span class="s1">: </span><span class="s2">&quot;sync&quot;</span><span class="s1">}</span><span class="s0">, </span><span class="s1">nthreads=[])</span>
<span class="s0">async def </span><span class="s1">test_get_scheduler_default_client_config_interleaving(s):</span>
    <span class="s3"># This test is using context managers intentionally. We should not refactor</span>
    <span class="s3"># this to use it in more places to make the client closing cleaner.</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning):</span>
        <span class="s0">assert </span><span class="s1">dask.base.get_scheduler() == dask.local.get_sync</span>
        <span class="s0">with </span><span class="s1">dask.config.set(scheduler=</span><span class="s2">&quot;threads&quot;</span><span class="s1">):</span>
            <span class="s0">assert </span><span class="s1">dask.base.get_scheduler() == dask.threaded.get</span>
            <span class="s1">client = </span><span class="s0">await </span><span class="s1">Client(s.address</span><span class="s0">, </span><span class="s1">set_as_default=</span><span class="s0">False, </span><span class="s1">asynchronous=</span><span class="s0">True</span><span class="s1">)</span>
            <span class="s0">try</span><span class="s1">:</span>
                <span class="s0">assert </span><span class="s1">dask.base.get_scheduler() == dask.threaded.get</span>
            <span class="s0">finally</span><span class="s1">:</span>
                <span class="s0">await </span><span class="s1">client.close()</span>

            <span class="s1">client = </span><span class="s0">await </span><span class="s1">Client(s.address</span><span class="s0">, </span><span class="s1">set_as_default=</span><span class="s0">True, </span><span class="s1">asynchronous=</span><span class="s0">True</span><span class="s1">)</span>
            <span class="s0">try</span><span class="s1">:</span>
                <span class="s0">assert </span><span class="s1">dask.base.get_scheduler() == client.get</span>
            <span class="s0">finally</span><span class="s1">:</span>
                <span class="s0">await </span><span class="s1">client.close()</span>
            <span class="s0">assert </span><span class="s1">dask.base.get_scheduler() == dask.threaded.get</span>

            <span class="s3"># FIXME: As soon as async with uses as_current this will be true as well</span>
            <span class="s3"># async with Client(s.address, set_as_default=False, asynchronous=True) as c:</span>
            <span class="s3">#     assert dask.base.get_scheduler() == c.get</span>
            <span class="s3"># assert dask.base.get_scheduler() == dask.threaded.get</span>

            <span class="s1">client = </span><span class="s0">await </span><span class="s1">Client(s.address</span><span class="s0">, </span><span class="s1">set_as_default=</span><span class="s0">False, </span><span class="s1">asynchronous=</span><span class="s0">True</span><span class="s1">)</span>
            <span class="s0">try</span><span class="s1">:</span>
                <span class="s0">assert </span><span class="s1">dask.base.get_scheduler() == dask.threaded.get</span>
                <span class="s0">with </span><span class="s1">client.as_current():</span>
                    <span class="s1">sc = dask.base.get_scheduler()</span>
                    <span class="s0">assert </span><span class="s1">sc == client.get</span>
                <span class="s0">assert </span><span class="s1">dask.base.get_scheduler() == dask.threaded.get</span>
            <span class="s0">finally</span><span class="s1">:</span>
                <span class="s0">await </span><span class="s1">client.close()</span>

            <span class="s3"># If it comes to a race between default and current, current wins</span>
            <span class="s1">client = </span><span class="s0">await </span><span class="s1">Client(s.address</span><span class="s0">, </span><span class="s1">set_as_default=</span><span class="s0">True, </span><span class="s1">asynchronous=</span><span class="s0">True</span><span class="s1">)</span>
            <span class="s1">client2 = </span><span class="s0">await </span><span class="s1">Client(s.address</span><span class="s0">, </span><span class="s1">set_as_default=</span><span class="s0">False, </span><span class="s1">asynchronous=</span><span class="s0">True</span><span class="s1">)</span>
            <span class="s0">try</span><span class="s1">:</span>
                <span class="s0">with </span><span class="s1">client2.as_current():</span>
                    <span class="s0">assert </span><span class="s1">dask.base.get_scheduler() == client2.get</span>
                <span class="s0">assert </span><span class="s1">dask.base.get_scheduler() == client.get</span>
            <span class="s0">finally</span><span class="s1">:</span>
                <span class="s0">await </span><span class="s1">client.close()</span>
                <span class="s0">await </span><span class="s1">client2.close()</span>

            <span class="s0">assert </span><span class="s1">dask.base.get_scheduler() == dask.threaded.get</span>

        <span class="s0">assert </span><span class="s1">dask.base.get_scheduler() == dask.local.get_sync</span>

        <span class="s1">client = </span><span class="s0">await </span><span class="s1">Client(s.address</span><span class="s0">, </span><span class="s1">set_as_default=</span><span class="s0">True, </span><span class="s1">asynchronous=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s0">try</span><span class="s1">:</span>
            <span class="s0">assert </span><span class="s1">dask.base.get_scheduler() == client.get</span>
            <span class="s0">with </span><span class="s1">dask.config.set(scheduler=</span><span class="s2">&quot;threads&quot;</span><span class="s1">):</span>
                <span class="s0">assert </span><span class="s1">dask.base.get_scheduler() == dask.threaded.get</span>
                <span class="s0">with </span><span class="s1">client.as_current():</span>
                    <span class="s0">assert </span><span class="s1">dask.base.get_scheduler() == client.get</span>
        <span class="s0">finally</span><span class="s1">:</span>
            <span class="s0">await </span><span class="s1">client.close()</span>


<span class="s1">@gen_cluster(client=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s0">async def </span><span class="s1">test_bag_groupby_default(c</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b):</span>
    <span class="s1">b = db.range(</span><span class="s4">100</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">b2 = b.groupby(</span><span class="s0">lambda </span><span class="s1">x: x % </span><span class="s4">13</span><span class="s1">)</span>
    <span class="s0">assert not </span><span class="s1">any(</span><span class="s2">&quot;partd&quot; </span><span class="s0">in </span><span class="s1">k[</span><span class="s4">0</span><span class="s1">] </span><span class="s0">for </span><span class="s1">k </span><span class="s0">in </span><span class="s1">b2.dask)</span>
</pre>
</body>
</html>