<html>
<head>
<title>test_kernel_pca.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_kernel_pca.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">warnings</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">import </span><span class="s1">scipy.sparse </span><span class="s0">as </span><span class="s1">sp</span>

<span class="s0">import </span><span class="s1">sklearn</span>
<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">load_iris</span><span class="s0">, </span><span class="s1">make_blobs</span><span class="s0">, </span><span class="s1">make_circles</span>
<span class="s0">from </span><span class="s1">sklearn.decomposition </span><span class="s0">import </span><span class="s1">PCA</span><span class="s0">, </span><span class="s1">KernelPCA</span>
<span class="s0">from </span><span class="s1">sklearn.exceptions </span><span class="s0">import </span><span class="s1">NotFittedError</span>
<span class="s0">from </span><span class="s1">sklearn.linear_model </span><span class="s0">import </span><span class="s1">Perceptron</span>
<span class="s0">from </span><span class="s1">sklearn.metrics.pairwise </span><span class="s0">import </span><span class="s1">rbf_kernel</span>
<span class="s0">from </span><span class="s1">sklearn.model_selection </span><span class="s0">import </span><span class="s1">GridSearchCV</span>
<span class="s0">from </span><span class="s1">sklearn.pipeline </span><span class="s0">import </span><span class="s1">Pipeline</span>
<span class="s0">from </span><span class="s1">sklearn.preprocessing </span><span class="s0">import </span><span class="s1">StandardScaler</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s0">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_equal</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.utils.validation </span><span class="s0">import </span><span class="s1">_check_psd_eigenvalues</span>


<span class="s0">def </span><span class="s1">test_kernel_pca():</span>
    <span class="s2">&quot;&quot;&quot;Nominal test for all solvers and all known kernels + a custom one 
 
    It tests 
     - that fit_transform is equivalent to fit+transform 
     - that the shapes of transforms and inverse transforms are correct 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X_fit = rng.random_sample((</span><span class="s3">5</span><span class="s0">, </span><span class="s3">4</span><span class="s1">))</span>
    <span class="s1">X_pred = rng.random_sample((</span><span class="s3">2</span><span class="s0">, </span><span class="s3">4</span><span class="s1">))</span>

    <span class="s0">def </span><span class="s1">histogram(x</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s4"># Histogram kernel implemented as a callable.</span>
        <span class="s0">assert </span><span class="s1">kwargs == {}  </span><span class="s4"># no kernel_params that we didn't ask for</span>
        <span class="s0">return </span><span class="s1">np.minimum(x</span><span class="s0">, </span><span class="s1">y).sum()</span>

    <span class="s0">for </span><span class="s1">eigen_solver </span><span class="s0">in </span><span class="s1">(</span><span class="s5">&quot;auto&quot;</span><span class="s0">, </span><span class="s5">&quot;dense&quot;</span><span class="s0">, </span><span class="s5">&quot;arpack&quot;</span><span class="s0">, </span><span class="s5">&quot;randomized&quot;</span><span class="s1">):</span>
        <span class="s0">for </span><span class="s1">kernel </span><span class="s0">in </span><span class="s1">(</span><span class="s5">&quot;linear&quot;</span><span class="s0">, </span><span class="s5">&quot;rbf&quot;</span><span class="s0">, </span><span class="s5">&quot;poly&quot;</span><span class="s0">, </span><span class="s1">histogram):</span>
            <span class="s4"># histogram kernel produces singular matrix inside linalg.solve</span>
            <span class="s4"># XXX use a least-squares approximation?</span>
            <span class="s1">inv = </span><span class="s0">not </span><span class="s1">callable(kernel)</span>

            <span class="s4"># transform fit data</span>
            <span class="s1">kpca = KernelPCA(</span>
                <span class="s3">4</span><span class="s0">, </span><span class="s1">kernel=kernel</span><span class="s0">, </span><span class="s1">eigen_solver=eigen_solver</span><span class="s0">, </span><span class="s1">fit_inverse_transform=inv</span>
            <span class="s1">)</span>
            <span class="s1">X_fit_transformed = kpca.fit_transform(X_fit)</span>
            <span class="s1">X_fit_transformed2 = kpca.fit(X_fit).transform(X_fit)</span>
            <span class="s1">assert_array_almost_equal(</span>
                <span class="s1">np.abs(X_fit_transformed)</span><span class="s0">, </span><span class="s1">np.abs(X_fit_transformed2)</span>
            <span class="s1">)</span>

            <span class="s4"># non-regression test: previously, gamma would be 0 by default,</span>
            <span class="s4"># forcing all eigenvalues to 0 under the poly kernel</span>
            <span class="s0">assert </span><span class="s1">X_fit_transformed.size != </span><span class="s3">0</span>

            <span class="s4"># transform new data</span>
            <span class="s1">X_pred_transformed = kpca.transform(X_pred)</span>
            <span class="s0">assert </span><span class="s1">X_pred_transformed.shape[</span><span class="s3">1</span><span class="s1">] == X_fit_transformed.shape[</span><span class="s3">1</span><span class="s1">]</span>

            <span class="s4"># inverse transform</span>
            <span class="s0">if </span><span class="s1">inv:</span>
                <span class="s1">X_pred2 = kpca.inverse_transform(X_pred_transformed)</span>
                <span class="s0">assert </span><span class="s1">X_pred2.shape == X_pred.shape</span>


<span class="s0">def </span><span class="s1">test_kernel_pca_invalid_parameters():</span>
    <span class="s2">&quot;&quot;&quot;Check that kPCA raises an error if the parameters are invalid 
 
    Tests fitting inverse transform with a precomputed kernel raises a 
    ValueError. 
    &quot;&quot;&quot;</span>
    <span class="s1">estimator = KernelPCA(</span>
        <span class="s1">n_components=</span><span class="s3">10</span><span class="s0">, </span><span class="s1">fit_inverse_transform=</span><span class="s0">True, </span><span class="s1">kernel=</span><span class="s5">&quot;precomputed&quot;</span>
    <span class="s1">)</span>
    <span class="s1">err_ms = </span><span class="s5">&quot;Cannot fit_inverse_transform with a precomputed kernel&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=err_ms):</span>
        <span class="s1">estimator.fit(np.random.randn(</span><span class="s3">10</span><span class="s0">, </span><span class="s3">10</span><span class="s1">))</span>


<span class="s0">def </span><span class="s1">test_kernel_pca_consistent_transform():</span>
    <span class="s2">&quot;&quot;&quot;Check robustness to mutations in the original training array 
 
    Test that after fitting a kPCA model, it stays independent of any 
    mutation of the values of the original data object by relying on an 
    internal copy. 
    &quot;&quot;&quot;</span>
    <span class="s4"># X_fit_ needs to retain the old, unmodified copy of X</span>
    <span class="s1">state = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = state.rand(</span><span class="s3">10</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">kpca = KernelPCA(random_state=state).fit(X)</span>
    <span class="s1">transformed1 = kpca.transform(X)</span>

    <span class="s1">X_copy = X.copy()</span>
    <span class="s1">X[:</span><span class="s0">, </span><span class="s3">0</span><span class="s1">] = </span><span class="s3">666</span>
    <span class="s1">transformed2 = kpca.transform(X_copy)</span>
    <span class="s1">assert_array_almost_equal(transformed1</span><span class="s0">, </span><span class="s1">transformed2)</span>


<span class="s0">def </span><span class="s1">test_kernel_pca_deterministic_output():</span>
    <span class="s2">&quot;&quot;&quot;Test that Kernel PCA produces deterministic output 
 
    Tests that the same inputs and random state produce the same output. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.rand(</span><span class="s3">10</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">eigen_solver = (</span><span class="s5">&quot;arpack&quot;</span><span class="s0">, </span><span class="s5">&quot;dense&quot;</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">solver </span><span class="s0">in </span><span class="s1">eigen_solver:</span>
        <span class="s1">transformed_X = np.zeros((</span><span class="s3">20</span><span class="s0">, </span><span class="s3">2</span><span class="s1">))</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">20</span><span class="s1">):</span>
            <span class="s1">kpca = KernelPCA(n_components=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">eigen_solver=solver</span><span class="s0">, </span><span class="s1">random_state=rng)</span>
            <span class="s1">transformed_X[i</span><span class="s0">, </span><span class="s1">:] = kpca.fit_transform(X)[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">assert_allclose(transformed_X</span><span class="s0">, </span><span class="s1">np.tile(transformed_X[</span><span class="s3">0</span><span class="s0">, </span><span class="s1">:]</span><span class="s0">, </span><span class="s3">20</span><span class="s1">).reshape(</span><span class="s3">20</span><span class="s0">, </span><span class="s3">2</span><span class="s1">))</span>


<span class="s0">def </span><span class="s1">test_kernel_pca_sparse():</span>
    <span class="s2">&quot;&quot;&quot;Test that kPCA works on a sparse data input. 
 
    Same test as ``test_kernel_pca except inverse_transform`` since it's not 
    implemented for sparse matrices. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X_fit = sp.csr_matrix(rng.random_sample((</span><span class="s3">5</span><span class="s0">, </span><span class="s3">4</span><span class="s1">)))</span>
    <span class="s1">X_pred = sp.csr_matrix(rng.random_sample((</span><span class="s3">2</span><span class="s0">, </span><span class="s3">4</span><span class="s1">)))</span>

    <span class="s0">for </span><span class="s1">eigen_solver </span><span class="s0">in </span><span class="s1">(</span><span class="s5">&quot;auto&quot;</span><span class="s0">, </span><span class="s5">&quot;arpack&quot;</span><span class="s0">, </span><span class="s5">&quot;randomized&quot;</span><span class="s1">):</span>
        <span class="s0">for </span><span class="s1">kernel </span><span class="s0">in </span><span class="s1">(</span><span class="s5">&quot;linear&quot;</span><span class="s0">, </span><span class="s5">&quot;rbf&quot;</span><span class="s0">, </span><span class="s5">&quot;poly&quot;</span><span class="s1">):</span>
            <span class="s4"># transform fit data</span>
            <span class="s1">kpca = KernelPCA(</span>
                <span class="s3">4</span><span class="s0">,</span>
                <span class="s1">kernel=kernel</span><span class="s0">,</span>
                <span class="s1">eigen_solver=eigen_solver</span><span class="s0">,</span>
                <span class="s1">fit_inverse_transform=</span><span class="s0">False,</span>
                <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
            <span class="s1">)</span>
            <span class="s1">X_fit_transformed = kpca.fit_transform(X_fit)</span>
            <span class="s1">X_fit_transformed2 = kpca.fit(X_fit).transform(X_fit)</span>
            <span class="s1">assert_array_almost_equal(</span>
                <span class="s1">np.abs(X_fit_transformed)</span><span class="s0">, </span><span class="s1">np.abs(X_fit_transformed2)</span>
            <span class="s1">)</span>

            <span class="s4"># transform new data</span>
            <span class="s1">X_pred_transformed = kpca.transform(X_pred)</span>
            <span class="s0">assert </span><span class="s1">X_pred_transformed.shape[</span><span class="s3">1</span><span class="s1">] == X_fit_transformed.shape[</span><span class="s3">1</span><span class="s1">]</span>

            <span class="s4"># inverse transform: not available for sparse matrices</span>
            <span class="s4"># XXX: should we raise another exception type here? For instance:</span>
            <span class="s4"># NotImplementedError.</span>
            <span class="s0">with </span><span class="s1">pytest.raises(NotFittedError):</span>
                <span class="s1">kpca.inverse_transform(X_pred_transformed)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s5">&quot;auto&quot;</span><span class="s0">, </span><span class="s5">&quot;dense&quot;</span><span class="s0">, </span><span class="s5">&quot;arpack&quot;</span><span class="s0">, </span><span class="s5">&quot;randomized&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;n_features&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">4</span><span class="s0">, </span><span class="s3">10</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_kernel_pca_linear_kernel(solver</span><span class="s0">, </span><span class="s1">n_features):</span>
    <span class="s2">&quot;&quot;&quot;Test that kPCA with linear kernel is equivalent to PCA for all solvers. 
 
    KernelPCA with linear kernel should produce the same output as PCA. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X_fit = rng.random_sample((</span><span class="s3">5</span><span class="s0">, </span><span class="s1">n_features))</span>
    <span class="s1">X_pred = rng.random_sample((</span><span class="s3">2</span><span class="s0">, </span><span class="s1">n_features))</span>

    <span class="s4"># for a linear kernel, kernel PCA should find the same projection as PCA</span>
    <span class="s4"># modulo the sign (direction)</span>
    <span class="s4"># fit only the first four components: fifth is near zero eigenvalue, so</span>
    <span class="s4"># can be trimmed due to roundoff error</span>
    <span class="s1">n_comps = </span><span class="s3">3 </span><span class="s0">if </span><span class="s1">solver == </span><span class="s5">&quot;arpack&quot; </span><span class="s0">else </span><span class="s3">4</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">np.abs(KernelPCA(n_comps</span><span class="s0">, </span><span class="s1">eigen_solver=solver).fit(X_fit).transform(X_pred))</span><span class="s0">,</span>
        <span class="s1">np.abs(</span>
            <span class="s1">PCA(n_comps</span><span class="s0">, </span><span class="s1">svd_solver=solver </span><span class="s0">if </span><span class="s1">solver != </span><span class="s5">&quot;dense&quot; </span><span class="s0">else </span><span class="s5">&quot;full&quot;</span><span class="s1">)</span>
            <span class="s1">.fit(X_fit)</span>
            <span class="s1">.transform(X_pred)</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_kernel_pca_n_components():</span>
    <span class="s2">&quot;&quot;&quot;Test that `n_components` is correctly taken into account for projections 
 
    For all solvers this tests that the output has the correct shape depending 
    on the selected number of components. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X_fit = rng.random_sample((</span><span class="s3">5</span><span class="s0">, </span><span class="s3">4</span><span class="s1">))</span>
    <span class="s1">X_pred = rng.random_sample((</span><span class="s3">2</span><span class="s0">, </span><span class="s3">4</span><span class="s1">))</span>

    <span class="s0">for </span><span class="s1">eigen_solver </span><span class="s0">in </span><span class="s1">(</span><span class="s5">&quot;dense&quot;</span><span class="s0">, </span><span class="s5">&quot;arpack&quot;</span><span class="s0">, </span><span class="s5">&quot;randomized&quot;</span><span class="s1">):</span>
        <span class="s0">for </span><span class="s1">c </span><span class="s0">in </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">4</span><span class="s1">]:</span>
            <span class="s1">kpca = KernelPCA(n_components=c</span><span class="s0">, </span><span class="s1">eigen_solver=eigen_solver)</span>
            <span class="s1">shape = kpca.fit(X_fit).transform(X_pred).shape</span>

            <span class="s0">assert </span><span class="s1">shape == (</span><span class="s3">2</span><span class="s0">, </span><span class="s1">c)</span>


<span class="s0">def </span><span class="s1">test_remove_zero_eig():</span>
    <span class="s2">&quot;&quot;&quot;Check that the ``remove_zero_eig`` parameter works correctly. 
 
    Tests that the null-space (Zero) eigenvalues are removed when 
    remove_zero_eig=True, whereas they are not by default. 
    &quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[</span><span class="s3">1 </span><span class="s1">- </span><span class="s3">1e-30</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1 </span><span class="s1">- </span><span class="s3">1e-20</span><span class="s1">]])</span>

    <span class="s4"># n_components=None (default) =&gt; remove_zero_eig is True</span>
    <span class="s1">kpca = KernelPCA()</span>
    <span class="s1">Xt = kpca.fit_transform(X)</span>
    <span class="s0">assert </span><span class="s1">Xt.shape == (</span><span class="s3">3</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">kpca = KernelPCA(n_components=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">Xt = kpca.fit_transform(X)</span>
    <span class="s0">assert </span><span class="s1">Xt.shape == (</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>

    <span class="s1">kpca = KernelPCA(n_components=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">remove_zero_eig=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">Xt = kpca.fit_transform(X)</span>
    <span class="s0">assert </span><span class="s1">Xt.shape == (</span><span class="s3">3</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_leave_zero_eig():</span>
    <span class="s2">&quot;&quot;&quot;Non-regression test for issue #12141 (PR #12143) 
 
    This test checks that fit().transform() returns the same result as 
    fit_transform() in case of non-removed zero eigenvalue. 
    &quot;&quot;&quot;</span>
    <span class="s1">X_fit = np.array([[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]])</span>

    <span class="s4"># Assert that even with all np warnings on, there is no div by zero warning</span>
    <span class="s0">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s4"># There might be warnings about the kernel being badly conditioned,</span>
        <span class="s4"># but there should not be warnings about division by zero.</span>
        <span class="s4"># (Numpy division by zero warning can have many message variants, but</span>
        <span class="s4"># at least we know that it is a RuntimeWarning so lets check only this)</span>
        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;error&quot;</span><span class="s0">, </span><span class="s1">RuntimeWarning)</span>
        <span class="s0">with </span><span class="s1">np.errstate(all=</span><span class="s5">&quot;warn&quot;</span><span class="s1">):</span>
            <span class="s1">k = KernelPCA(n_components=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">remove_zero_eig=</span><span class="s0">False, </span><span class="s1">eigen_solver=</span><span class="s5">&quot;dense&quot;</span><span class="s1">)</span>
            <span class="s4"># Fit, then transform</span>
            <span class="s1">A = k.fit(X_fit).transform(X_fit)</span>
            <span class="s4"># Do both at once</span>
            <span class="s1">B = k.fit_transform(X_fit)</span>
            <span class="s4"># Compare</span>
            <span class="s1">assert_array_almost_equal(np.abs(A)</span><span class="s0">, </span><span class="s1">np.abs(B))</span>


<span class="s0">def </span><span class="s1">test_kernel_pca_precomputed():</span>
    <span class="s2">&quot;&quot;&quot;Test that kPCA works with a precomputed kernel, for all solvers&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X_fit = rng.random_sample((</span><span class="s3">5</span><span class="s0">, </span><span class="s3">4</span><span class="s1">))</span>
    <span class="s1">X_pred = rng.random_sample((</span><span class="s3">2</span><span class="s0">, </span><span class="s3">4</span><span class="s1">))</span>

    <span class="s0">for </span><span class="s1">eigen_solver </span><span class="s0">in </span><span class="s1">(</span><span class="s5">&quot;dense&quot;</span><span class="s0">, </span><span class="s5">&quot;arpack&quot;</span><span class="s0">, </span><span class="s5">&quot;randomized&quot;</span><span class="s1">):</span>
        <span class="s1">X_kpca = (</span>
            <span class="s1">KernelPCA(</span><span class="s3">4</span><span class="s0">, </span><span class="s1">eigen_solver=eigen_solver</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
            <span class="s1">.fit(X_fit)</span>
            <span class="s1">.transform(X_pred)</span>
        <span class="s1">)</span>

        <span class="s1">X_kpca2 = (</span>
            <span class="s1">KernelPCA(</span>
                <span class="s3">4</span><span class="s0">, </span><span class="s1">eigen_solver=eigen_solver</span><span class="s0">, </span><span class="s1">kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span>
            <span class="s1">)</span>
            <span class="s1">.fit(np.dot(X_fit</span><span class="s0">, </span><span class="s1">X_fit.T))</span>
            <span class="s1">.transform(np.dot(X_pred</span><span class="s0">, </span><span class="s1">X_fit.T))</span>
        <span class="s1">)</span>

        <span class="s1">X_kpca_train = KernelPCA(</span>
            <span class="s3">4</span><span class="s0">, </span><span class="s1">eigen_solver=eigen_solver</span><span class="s0">, </span><span class="s1">kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span>
        <span class="s1">).fit_transform(np.dot(X_fit</span><span class="s0">, </span><span class="s1">X_fit.T))</span>

        <span class="s1">X_kpca_train2 = (</span>
            <span class="s1">KernelPCA(</span>
                <span class="s3">4</span><span class="s0">, </span><span class="s1">eigen_solver=eigen_solver</span><span class="s0">, </span><span class="s1">kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span>
            <span class="s1">)</span>
            <span class="s1">.fit(np.dot(X_fit</span><span class="s0">, </span><span class="s1">X_fit.T))</span>
            <span class="s1">.transform(np.dot(X_fit</span><span class="s0">, </span><span class="s1">X_fit.T))</span>
        <span class="s1">)</span>

        <span class="s1">assert_array_almost_equal(np.abs(X_kpca)</span><span class="s0">, </span><span class="s1">np.abs(X_kpca2))</span>

        <span class="s1">assert_array_almost_equal(np.abs(X_kpca_train)</span><span class="s0">, </span><span class="s1">np.abs(X_kpca_train2))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s5">&quot;auto&quot;</span><span class="s0">, </span><span class="s5">&quot;dense&quot;</span><span class="s0">, </span><span class="s5">&quot;arpack&quot;</span><span class="s0">, </span><span class="s5">&quot;randomized&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_kernel_pca_precomputed_non_symmetric(solver):</span>
    <span class="s2">&quot;&quot;&quot;Check that the kernel centerer works. 
 
    Tests that a non symmetric precomputed kernel is actually accepted 
    because the kernel centerer does its job correctly. 
    &quot;&quot;&quot;</span>

    <span class="s4"># a non symmetric gram matrix</span>
    <span class="s1">K = [[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">3</span><span class="s0">, </span><span class="s3">40</span><span class="s1">]]</span>
    <span class="s1">kpca = KernelPCA(</span>
        <span class="s1">kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s0">, </span><span class="s1">eigen_solver=solver</span><span class="s0">, </span><span class="s1">n_components=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span>
    <span class="s1">)</span>
    <span class="s1">kpca.fit(K)  </span><span class="s4"># no error</span>

    <span class="s4"># same test with centered kernel</span>
    <span class="s1">Kc = [[</span><span class="s3">9</span><span class="s0">, </span><span class="s1">-</span><span class="s3">9</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">9</span><span class="s0">, </span><span class="s3">9</span><span class="s1">]]</span>
    <span class="s1">kpca_c = KernelPCA(</span>
        <span class="s1">kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s0">, </span><span class="s1">eigen_solver=solver</span><span class="s0">, </span><span class="s1">n_components=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span>
    <span class="s1">)</span>
    <span class="s1">kpca_c.fit(Kc)</span>

    <span class="s4"># comparison between the non-centered and centered versions</span>
    <span class="s1">assert_array_equal(kpca.eigenvectors_</span><span class="s0">, </span><span class="s1">kpca_c.eigenvectors_)</span>
    <span class="s1">assert_array_equal(kpca.eigenvalues_</span><span class="s0">, </span><span class="s1">kpca_c.eigenvalues_)</span>


<span class="s0">def </span><span class="s1">test_gridsearch_pipeline():</span>
    <span class="s2">&quot;&quot;&quot;Check that kPCA works as expected in a grid search pipeline 
 
    Test if we can do a grid-search to find parameters to separate 
    circles with a perceptron model. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_circles(n_samples=</span><span class="s3">400</span><span class="s0">, </span><span class="s1">factor=</span><span class="s3">0.3</span><span class="s0">, </span><span class="s1">noise=</span><span class="s3">0.05</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">kpca = KernelPCA(kernel=</span><span class="s5">&quot;rbf&quot;</span><span class="s0">, </span><span class="s1">n_components=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">pipeline = Pipeline([(</span><span class="s5">&quot;kernel_pca&quot;</span><span class="s0">, </span><span class="s1">kpca)</span><span class="s0">, </span><span class="s1">(</span><span class="s5">&quot;Perceptron&quot;</span><span class="s0">, </span><span class="s1">Perceptron(max_iter=</span><span class="s3">5</span><span class="s1">))])</span>
    <span class="s1">param_grid = dict(kernel_pca__gamma=</span><span class="s3">2.0 </span><span class="s1">** np.arange(-</span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">))</span>
    <span class="s1">grid_search = GridSearchCV(pipeline</span><span class="s0">, </span><span class="s1">cv=</span><span class="s3">3</span><span class="s0">, </span><span class="s1">param_grid=param_grid)</span>
    <span class="s1">grid_search.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">grid_search.best_score_ == </span><span class="s3">1</span>


<span class="s0">def </span><span class="s1">test_gridsearch_pipeline_precomputed():</span>
    <span class="s2">&quot;&quot;&quot;Check that kPCA works as expected in a grid search pipeline (2) 
 
    Test if we can do a grid-search to find parameters to separate 
    circles with a perceptron model. This test uses a precomputed kernel. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_circles(n_samples=</span><span class="s3">400</span><span class="s0">, </span><span class="s1">factor=</span><span class="s3">0.3</span><span class="s0">, </span><span class="s1">noise=</span><span class="s3">0.05</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">kpca = KernelPCA(kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s0">, </span><span class="s1">n_components=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">pipeline = Pipeline([(</span><span class="s5">&quot;kernel_pca&quot;</span><span class="s0">, </span><span class="s1">kpca)</span><span class="s0">, </span><span class="s1">(</span><span class="s5">&quot;Perceptron&quot;</span><span class="s0">, </span><span class="s1">Perceptron(max_iter=</span><span class="s3">5</span><span class="s1">))])</span>
    <span class="s1">param_grid = dict(Perceptron__max_iter=np.arange(</span><span class="s3">1</span><span class="s0">, </span><span class="s3">5</span><span class="s1">))</span>
    <span class="s1">grid_search = GridSearchCV(pipeline</span><span class="s0">, </span><span class="s1">cv=</span><span class="s3">3</span><span class="s0">, </span><span class="s1">param_grid=param_grid)</span>
    <span class="s1">X_kernel = rbf_kernel(X</span><span class="s0">, </span><span class="s1">gamma=</span><span class="s3">2.0</span><span class="s1">)</span>
    <span class="s1">grid_search.fit(X_kernel</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">grid_search.best_score_ == </span><span class="s3">1</span>


<span class="s0">def </span><span class="s1">test_nested_circles():</span>
    <span class="s2">&quot;&quot;&quot;Check that kPCA projects in a space where nested circles are separable 
 
    Tests that 2D nested circles become separable with a perceptron when 
    projected in the first 2 kPCA using an RBF kernel, while raw samples 
    are not directly separable in the original space. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_circles(n_samples=</span><span class="s3">400</span><span class="s0">, </span><span class="s1">factor=</span><span class="s3">0.3</span><span class="s0">, </span><span class="s1">noise=</span><span class="s3">0.05</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s4"># 2D nested circles are not linearly separable</span>
    <span class="s1">train_score = Perceptron(max_iter=</span><span class="s3">5</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y).score(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">train_score &lt; </span><span class="s3">0.8</span>

    <span class="s4"># Project the circles data into the first 2 components of a RBF Kernel</span>
    <span class="s4"># PCA model.</span>
    <span class="s4"># Note that the gamma value is data dependent. If this test breaks</span>
    <span class="s4"># and the gamma value has to be updated, the Kernel PCA example will</span>
    <span class="s4"># have to be updated too.</span>
    <span class="s1">kpca = KernelPCA(</span>
        <span class="s1">kernel=</span><span class="s5">&quot;rbf&quot;</span><span class="s0">, </span><span class="s1">n_components=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">fit_inverse_transform=</span><span class="s0">True, </span><span class="s1">gamma=</span><span class="s3">2.0</span>
    <span class="s1">)</span>
    <span class="s1">X_kpca = kpca.fit_transform(X)</span>

    <span class="s4"># The data is perfectly linearly separable in that space</span>
    <span class="s1">train_score = Perceptron(max_iter=</span><span class="s3">5</span><span class="s1">).fit(X_kpca</span><span class="s0">, </span><span class="s1">y).score(X_kpca</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">train_score == </span><span class="s3">1.0</span>


<span class="s0">def </span><span class="s1">test_kernel_conditioning():</span>
    <span class="s2">&quot;&quot;&quot;Check that ``_check_psd_eigenvalues`` is correctly called in kPCA 
 
    Non-regression test for issue #12140 (PR #12145). 
    &quot;&quot;&quot;</span>

    <span class="s4"># create a pathological X leading to small non-zero eigenvalue</span>
    <span class="s1">X = [[</span><span class="s3">5</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">5 </span><span class="s1">+ </span><span class="s3">1e-8</span><span class="s0">, </span><span class="s3">1e-8</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">5 </span><span class="s1">+ </span><span class="s3">1e-8</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]]</span>
    <span class="s1">kpca = KernelPCA(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s0">, </span><span class="s1">n_components=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">fit_inverse_transform=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">kpca.fit(X)</span>

    <span class="s4"># check that the small non-zero eigenvalue was correctly set to zero</span>
    <span class="s0">assert </span><span class="s1">kpca.eigenvalues_.min() == </span><span class="s3">0</span>
    <span class="s0">assert </span><span class="s1">np.all(kpca.eigenvalues_ == _check_psd_eigenvalues(kpca.eigenvalues_))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s5">&quot;auto&quot;</span><span class="s0">, </span><span class="s5">&quot;dense&quot;</span><span class="s0">, </span><span class="s5">&quot;arpack&quot;</span><span class="s0">, </span><span class="s5">&quot;randomized&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_precomputed_kernel_not_psd(solver):</span>
    <span class="s2">&quot;&quot;&quot;Check how KernelPCA works with non-PSD kernels depending on n_components 
 
    Tests for all methods what happens with a non PSD gram matrix (this 
    can happen in an isomap scenario, or with custom kernel functions, or 
    maybe with ill-posed datasets). 
 
    When ``n_component`` is large enough to capture a negative eigenvalue, an 
    error should be raised. Otherwise, KernelPCA should run without error 
    since the negative eigenvalues are not selected. 
    &quot;&quot;&quot;</span>

    <span class="s4"># a non PSD kernel with large eigenvalues, already centered</span>
    <span class="s4"># it was captured from an isomap call and multiplied by 100 for compacity</span>
    <span class="s1">K = [</span>
        <span class="s1">[</span><span class="s3">4.48</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">8.07</span><span class="s0">, </span><span class="s3">2.33</span><span class="s0">, </span><span class="s3">2.33</span><span class="s0">, </span><span class="s3">2.33</span><span class="s0">, </span><span class="s1">-</span><span class="s3">5.76</span><span class="s0">, </span><span class="s1">-</span><span class="s3">12.78</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">6.48</span><span class="s0">, </span><span class="s3">4.5</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.24</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.24</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.24</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.81</span><span class="s0">, </span><span class="s3">7.49</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">8.07</span><span class="s0">, </span><span class="s3">4.5</span><span class="s0">, </span><span class="s3">15.48</span><span class="s0">, </span><span class="s3">2.09</span><span class="s0">, </span><span class="s3">2.09</span><span class="s0">, </span><span class="s3">2.09</span><span class="s0">, </span><span class="s1">-</span><span class="s3">11.1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">23.23</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">2.33</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.24</span><span class="s0">, </span><span class="s3">2.09</span><span class="s0">, </span><span class="s3">4.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3.65</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3.65</span><span class="s0">, </span><span class="s3">1.02</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.9</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">2.33</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.24</span><span class="s0">, </span><span class="s3">2.09</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3.65</span><span class="s0">, </span><span class="s3">4.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3.65</span><span class="s0">, </span><span class="s3">1.02</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.9</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">2.33</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.24</span><span class="s0">, </span><span class="s3">2.09</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3.65</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3.65</span><span class="s0">, </span><span class="s3">4.0</span><span class="s0">, </span><span class="s3">1.02</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.9</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[-</span><span class="s3">5.76</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.81</span><span class="s0">, </span><span class="s1">-</span><span class="s3">11.1</span><span class="s0">, </span><span class="s3">1.02</span><span class="s0">, </span><span class="s3">1.02</span><span class="s0">, </span><span class="s3">1.02</span><span class="s0">, </span><span class="s3">4.86</span><span class="s0">, </span><span class="s3">9.75</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[-</span><span class="s3">12.78</span><span class="s0">, </span><span class="s3">7.49</span><span class="s0">, </span><span class="s1">-</span><span class="s3">23.23</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.9</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.9</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.9</span><span class="s0">, </span><span class="s3">9.75</span><span class="s0">, </span><span class="s3">21.46</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s4"># this gram matrix has 5 positive eigenvalues and 3 negative ones</span>
    <span class="s4"># [ 52.72,   7.65,   7.65,   5.02,   0.  ,  -0.  ,  -6.13, -15.11]</span>

    <span class="s4"># 1. ask for enough components to get a significant negative one</span>
    <span class="s1">kpca = KernelPCA(kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s0">, </span><span class="s1">eigen_solver=solver</span><span class="s0">, </span><span class="s1">n_components=</span><span class="s3">7</span><span class="s1">)</span>
    <span class="s4"># make sure that the appropriate error is raised</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s5">&quot;There are significant negative eigenvalues&quot;</span><span class="s1">):</span>
        <span class="s1">kpca.fit(K)</span>

    <span class="s4"># 2. ask for a small enough n_components to get only positive ones</span>
    <span class="s1">kpca = KernelPCA(kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s0">, </span><span class="s1">eigen_solver=solver</span><span class="s0">, </span><span class="s1">n_components=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">solver == </span><span class="s5">&quot;randomized&quot;</span><span class="s1">:</span>
        <span class="s4"># the randomized method is still inconsistent with the others on this</span>
        <span class="s4"># since it selects the eigenvalues based on the largest 2 modules, not</span>
        <span class="s4"># on the largest 2 values.</span>
        <span class="s4">#</span>
        <span class="s4"># At least we can ensure that we return an error instead of returning</span>
        <span class="s4"># the wrong eigenvalues</span>
        <span class="s0">with </span><span class="s1">pytest.raises(</span>
            <span class="s1">ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s5">&quot;There are significant negative eigenvalues&quot;</span>
        <span class="s1">):</span>
            <span class="s1">kpca.fit(K)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s4"># general case: make sure that it works</span>
        <span class="s1">kpca.fit(K)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;n_components&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">4</span><span class="s0">, </span><span class="s3">10</span><span class="s0">, </span><span class="s3">20</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_kernel_pca_solvers_equivalence(n_components):</span>
    <span class="s2">&quot;&quot;&quot;Check that 'dense' 'arpack' &amp; 'randomized' solvers give similar results&quot;&quot;&quot;</span>

    <span class="s4"># Generate random data</span>
    <span class="s1">n_train</span><span class="s0">, </span><span class="s1">n_test = </span><span class="s3">1_000</span><span class="s0">, </span><span class="s3">100</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">_ = make_circles(</span>
        <span class="s1">n_samples=(n_train + n_test)</span><span class="s0">, </span><span class="s1">factor=</span><span class="s3">0.3</span><span class="s0">, </span><span class="s1">noise=</span><span class="s3">0.05</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span>
    <span class="s1">)</span>
    <span class="s1">X_fit</span><span class="s0">, </span><span class="s1">X_pred = X[:n_train</span><span class="s0">, </span><span class="s1">:]</span><span class="s0">, </span><span class="s1">X[n_train:</span><span class="s0">, </span><span class="s1">:]</span>

    <span class="s4"># reference (full)</span>
    <span class="s1">ref_pred = (</span>
        <span class="s1">KernelPCA(n_components</span><span class="s0">, </span><span class="s1">eigen_solver=</span><span class="s5">&quot;dense&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">.fit(X_fit)</span>
        <span class="s1">.transform(X_pred)</span>
    <span class="s1">)</span>

    <span class="s4"># arpack</span>
    <span class="s1">a_pred = (</span>
        <span class="s1">KernelPCA(n_components</span><span class="s0">, </span><span class="s1">eigen_solver=</span><span class="s5">&quot;arpack&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">.fit(X_fit)</span>
        <span class="s1">.transform(X_pred)</span>
    <span class="s1">)</span>
    <span class="s4"># check that the result is still correct despite the approx</span>
    <span class="s1">assert_array_almost_equal(np.abs(a_pred)</span><span class="s0">, </span><span class="s1">np.abs(ref_pred))</span>

    <span class="s4"># randomized</span>
    <span class="s1">r_pred = (</span>
        <span class="s1">KernelPCA(n_components</span><span class="s0">, </span><span class="s1">eigen_solver=</span><span class="s5">&quot;randomized&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">.fit(X_fit)</span>
        <span class="s1">.transform(X_pred)</span>
    <span class="s1">)</span>
    <span class="s4"># check that the result is still correct despite the approximation</span>
    <span class="s1">assert_array_almost_equal(np.abs(r_pred)</span><span class="s0">, </span><span class="s1">np.abs(ref_pred))</span>


<span class="s0">def </span><span class="s1">test_kernel_pca_inverse_transform_reconstruction():</span>
    <span class="s2">&quot;&quot;&quot;Test if the reconstruction is a good approximation. 
 
    Note that in general it is not possible to get an arbitrarily good 
    reconstruction because of kernel centering that does not 
    preserve all the information of the original data. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">*_ = make_blobs(n_samples=</span><span class="s3">100</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s3">4</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">kpca = KernelPCA(</span>
        <span class="s1">n_components=</span><span class="s3">20</span><span class="s0">, </span><span class="s1">kernel=</span><span class="s5">&quot;rbf&quot;</span><span class="s0">, </span><span class="s1">fit_inverse_transform=</span><span class="s0">True, </span><span class="s1">alpha=</span><span class="s3">1e-3</span>
    <span class="s1">)</span>
    <span class="s1">X_trans = kpca.fit_transform(X)</span>
    <span class="s1">X_reconst = kpca.inverse_transform(X_trans)</span>
    <span class="s0">assert </span><span class="s1">np.linalg.norm(X - X_reconst) / np.linalg.norm(X) &lt; </span><span class="s3">1e-1</span>


<span class="s0">def </span><span class="s1">test_kernel_pca_raise_not_fitted_error():</span>
    <span class="s1">X = np.random.randn(</span><span class="s3">15</span><span class="s1">).reshape(</span><span class="s3">5</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">kpca = KernelPCA()</span>
    <span class="s1">kpca.fit(X)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(NotFittedError):</span>
        <span class="s1">kpca.inverse_transform(X)</span>


<span class="s0">def </span><span class="s1">test_32_64_decomposition_shape():</span>
    <span class="s2">&quot;&quot;&quot;Test that the decomposition is similar for 32 and 64 bits data 
 
    Non regression test for 
    https://github.com/scikit-learn/scikit-learn/issues/18146 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s3">30</span><span class="s0">, </span><span class="s1">centers=[[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">cluster_std=</span><span class="s3">0.1</span>
    <span class="s1">)</span>
    <span class="s1">X = StandardScaler().fit_transform(X)</span>
    <span class="s1">X -= X.min()</span>

    <span class="s4"># Compare the shapes (corresponds to the number of non-zero eigenvalues)</span>
    <span class="s1">kpca = KernelPCA()</span>
    <span class="s0">assert </span><span class="s1">kpca.fit_transform(X).shape == kpca.fit_transform(X.astype(np.float32)).shape</span>


<span class="s0">def </span><span class="s1">test_kernel_pca_feature_names_out():</span>
    <span class="s2">&quot;&quot;&quot;Check feature names out for KernelPCA.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">*_ = make_blobs(n_samples=</span><span class="s3">100</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s3">4</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">kpca = KernelPCA(n_components=</span><span class="s3">2</span><span class="s1">).fit(X)</span>

    <span class="s1">names = kpca.get_feature_names_out()</span>
    <span class="s1">assert_array_equal([</span><span class="s5">f&quot;kernelpca</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s5">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">2</span><span class="s1">)]</span><span class="s0">, </span><span class="s1">names)</span>


<span class="s0">def </span><span class="s1">test_kernel_pca_inverse_correct_gamma():</span>
    <span class="s2">&quot;&quot;&quot;Check that gamma is set correctly when not provided. 
 
    Non-regression test for #26280 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.random_sample((</span><span class="s3">5</span><span class="s0">, </span><span class="s3">4</span><span class="s1">))</span>

    <span class="s1">kwargs = {</span>
        <span class="s5">&quot;n_components&quot;</span><span class="s1">: </span><span class="s3">2</span><span class="s0">,</span>
        <span class="s5">&quot;random_state&quot;</span><span class="s1">: rng</span><span class="s0">,</span>
        <span class="s5">&quot;fit_inverse_transform&quot;</span><span class="s1">: </span><span class="s0">True,</span>
        <span class="s5">&quot;kernel&quot;</span><span class="s1">: </span><span class="s5">&quot;rbf&quot;</span><span class="s0">,</span>
    <span class="s1">}</span>

    <span class="s1">expected_gamma = </span><span class="s3">1 </span><span class="s1">/ X.shape[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">kpca1 = KernelPCA(gamma=</span><span class="s0">None, </span><span class="s1">**kwargs).fit(X)</span>
    <span class="s1">kpca2 = KernelPCA(gamma=expected_gamma</span><span class="s0">, </span><span class="s1">**kwargs).fit(X)</span>

    <span class="s0">assert </span><span class="s1">kpca1.gamma_ == expected_gamma</span>
    <span class="s0">assert </span><span class="s1">kpca2.gamma_ == expected_gamma</span>

    <span class="s1">X1_recon = kpca1.inverse_transform(kpca1.transform(X))</span>
    <span class="s1">X2_recon = kpca2.inverse_transform(kpca1.transform(X))</span>

    <span class="s1">assert_allclose(X1_recon</span><span class="s0">, </span><span class="s1">X2_recon)</span>


<span class="s0">def </span><span class="s1">test_kernel_pca_pandas_output():</span>
    <span class="s2">&quot;&quot;&quot;Check that KernelPCA works with pandas output when the solver is arpack. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/27579 
    &quot;&quot;&quot;</span>
    <span class="s1">pytest.importorskip(</span><span class="s5">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">_ = load_iris(as_frame=</span><span class="s0">True, </span><span class="s1">return_X_y=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">sklearn.config_context(transform_output=</span><span class="s5">&quot;pandas&quot;</span><span class="s1">):</span>
        <span class="s1">KernelPCA(n_components=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">eigen_solver=</span><span class="s5">&quot;arpack&quot;</span><span class="s1">).fit_transform(X)</span>
</pre>
</body>
</html>