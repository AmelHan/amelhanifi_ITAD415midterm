<html>
<head>
<title>_rbm.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_rbm.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Restricted Boltzmann Machine 
&quot;&quot;&quot;</span>

<span class="s2"># Authors: Yann N. Dauphin &lt;dauphiya@iro.umontreal.ca&gt;</span>
<span class="s2">#          Vlad Niculae</span>
<span class="s2">#          Gabriel Synnaeve</span>
<span class="s2">#          Lars Buitinck</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">time</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">scipy.sparse </span><span class="s3">as </span><span class="s1">sp</span>
<span class="s3">from </span><span class="s1">scipy.special </span><span class="s3">import </span><span class="s1">expit  </span><span class="s2"># logistic function</span>

<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">BaseEstimator</span><span class="s3">,</span>
    <span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s3">,</span>
    <span class="s1">TransformerMixin</span><span class="s3">,</span>
    <span class="s1">_fit_context</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">check_random_state</span><span class="s3">, </span><span class="s1">gen_even_slices</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">Interval</span>
<span class="s3">from </span><span class="s1">..utils.extmath </span><span class="s3">import </span><span class="s1">log_logistic</span><span class="s3">, </span><span class="s1">safe_sparse_dot</span>
<span class="s3">from </span><span class="s1">..utils.validation </span><span class="s3">import </span><span class="s1">check_is_fitted</span>


<span class="s3">class </span><span class="s1">BernoulliRBM(ClassNamePrefixFeaturesOutMixin</span><span class="s3">, </span><span class="s1">TransformerMixin</span><span class="s3">, </span><span class="s1">BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Bernoulli Restricted Boltzmann Machine (RBM). 
 
    A Restricted Boltzmann Machine with binary visible units and 
    binary hidden units. Parameters are estimated using Stochastic Maximum 
    Likelihood (SML), also known as Persistent Contrastive Divergence (PCD) 
    [2]. 
 
    The time complexity of this implementation is ``O(d ** 2)`` assuming 
    d ~ n_features ~ n_components. 
 
    Read more in the :ref:`User Guide &lt;rbm&gt;`. 
 
    Parameters 
    ---------- 
    n_components : int, default=256 
        Number of binary hidden units. 
 
    learning_rate : float, default=0.1 
        The learning rate for weight updates. It is *highly* recommended 
        to tune this hyper-parameter. Reasonable values are in the 
        10**[0., -3.] range. 
 
    batch_size : int, default=10 
        Number of examples per minibatch. 
 
    n_iter : int, default=10 
        Number of iterations/sweeps over the training dataset to perform 
        during training. 
 
    verbose : int, default=0 
        The verbosity level. The default, zero, means silent mode. Range 
        of values is [0, inf]. 
 
    random_state : int, RandomState instance or None, default=None 
        Determines random number generation for: 
 
        - Gibbs sampling from visible and hidden layers. 
 
        - Initializing components, sampling from layers during fit. 
 
        - Corrupting the data when scoring samples. 
 
        Pass an int for reproducible results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    Attributes 
    ---------- 
    intercept_hidden_ : array-like of shape (n_components,) 
        Biases of the hidden units. 
 
    intercept_visible_ : array-like of shape (n_features,) 
        Biases of the visible units. 
 
    components_ : array-like of shape (n_components, n_features) 
        Weight matrix, where `n_features` is the number of 
        visible units and `n_components` is the number of hidden units. 
 
    h_samples_ : array-like of shape (batch_size, n_components) 
        Hidden Activation sampled from the model distribution, 
        where `batch_size` is the number of examples per minibatch and 
        `n_components` is the number of hidden units. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    sklearn.neural_network.MLPRegressor : Multi-layer Perceptron regressor. 
    sklearn.neural_network.MLPClassifier : Multi-layer Perceptron classifier. 
    sklearn.decomposition.PCA : An unsupervised linear dimensionality 
        reduction model. 
 
    References 
    ---------- 
 
    [1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for 
        deep belief nets. Neural Computation 18, pp 1527-1554. 
        https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf 
 
    [2] Tieleman, T. Training Restricted Boltzmann Machines using 
        Approximations to the Likelihood Gradient. International Conference 
        on Machine Learning (ICML) 2008 
 
    Examples 
    -------- 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.neural_network import BernoulliRBM 
    &gt;&gt;&gt; X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]]) 
    &gt;&gt;&gt; model = BernoulliRBM(n_components=2) 
    &gt;&gt;&gt; model.fit(X) 
    BernoulliRBM(n_components=2) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;n_components&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;learning_rate&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;batch_size&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;n_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s4">&quot;verbose&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">n_components=</span><span class="s5">256</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">learning_rate=</span><span class="s5">0.1</span><span class="s3">,</span>
        <span class="s1">batch_size=</span><span class="s5">10</span><span class="s3">,</span>
        <span class="s1">n_iter=</span><span class="s5">10</span><span class="s3">,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">):</span>
        <span class="s1">self.n_components = n_components</span>
        <span class="s1">self.learning_rate = learning_rate</span>
        <span class="s1">self.batch_size = batch_size</span>
        <span class="s1">self.n_iter = n_iter</span>
        <span class="s1">self.verbose = verbose</span>
        <span class="s1">self.random_state = random_state</span>

    <span class="s3">def </span><span class="s1">transform(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Compute the hidden layer activation probabilities, P(h=1|v=X). 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The data to be transformed. 
 
        Returns 
        ------- 
        h : ndarray of shape (n_samples, n_components) 
            Latent representations of the data. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False, </span><span class="s1">dtype=(np.float64</span><span class="s3">, </span><span class="s1">np.float32)</span>
        <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">self._mean_hiddens(X)</span>

    <span class="s3">def </span><span class="s1">_mean_hiddens(self</span><span class="s3">, </span><span class="s1">v):</span>
        <span class="s0">&quot;&quot;&quot;Computes the probabilities P(h=1|v). 
 
        Parameters 
        ---------- 
        v : ndarray of shape (n_samples, n_features) 
            Values of the visible layer. 
 
        Returns 
        ------- 
        h : ndarray of shape (n_samples, n_components) 
            Corresponding mean field values for the hidden layer. 
        &quot;&quot;&quot;</span>
        <span class="s1">p = safe_sparse_dot(v</span><span class="s3">, </span><span class="s1">self.components_.T)</span>
        <span class="s1">p += self.intercept_hidden_</span>
        <span class="s3">return </span><span class="s1">expit(p</span><span class="s3">, </span><span class="s1">out=p)</span>

    <span class="s3">def </span><span class="s1">_sample_hiddens(self</span><span class="s3">, </span><span class="s1">v</span><span class="s3">, </span><span class="s1">rng):</span>
        <span class="s0">&quot;&quot;&quot;Sample from the distribution P(h|v). 
 
        Parameters 
        ---------- 
        v : ndarray of shape (n_samples, n_features) 
            Values of the visible layer to sample from. 
 
        rng : RandomState instance 
            Random number generator to use. 
 
        Returns 
        ------- 
        h : ndarray of shape (n_samples, n_components) 
            Values of the hidden layer. 
        &quot;&quot;&quot;</span>
        <span class="s1">p = self._mean_hiddens(v)</span>
        <span class="s3">return </span><span class="s1">rng.uniform(size=p.shape) &lt; p</span>

    <span class="s3">def </span><span class="s1">_sample_visibles(self</span><span class="s3">, </span><span class="s1">h</span><span class="s3">, </span><span class="s1">rng):</span>
        <span class="s0">&quot;&quot;&quot;Sample from the distribution P(v|h). 
 
        Parameters 
        ---------- 
        h : ndarray of shape (n_samples, n_components) 
            Values of the hidden layer to sample from. 
 
        rng : RandomState instance 
            Random number generator to use. 
 
        Returns 
        ------- 
        v : ndarray of shape (n_samples, n_features) 
            Values of the visible layer. 
        &quot;&quot;&quot;</span>
        <span class="s1">p = np.dot(h</span><span class="s3">, </span><span class="s1">self.components_)</span>
        <span class="s1">p += self.intercept_visible_</span>
        <span class="s1">expit(p</span><span class="s3">, </span><span class="s1">out=p)</span>
        <span class="s3">return </span><span class="s1">rng.uniform(size=p.shape) &lt; p</span>

    <span class="s3">def </span><span class="s1">_free_energy(self</span><span class="s3">, </span><span class="s1">v):</span>
        <span class="s0">&quot;&quot;&quot;Computes the free energy F(v) = - log sum_h exp(-E(v,h)). 
 
        Parameters 
        ---------- 
        v : ndarray of shape (n_samples, n_features) 
            Values of the visible layer. 
 
        Returns 
        ------- 
        free_energy : ndarray of shape (n_samples,) 
            The value of the free energy. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">-safe_sparse_dot(v</span><span class="s3">, </span><span class="s1">self.intercept_visible_) - np.logaddexp(</span>
            <span class="s5">0</span><span class="s3">, </span><span class="s1">safe_sparse_dot(v</span><span class="s3">, </span><span class="s1">self.components_.T) + self.intercept_hidden_</span>
        <span class="s1">).sum(axis=</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">gibbs(self</span><span class="s3">, </span><span class="s1">v):</span>
        <span class="s0">&quot;&quot;&quot;Perform one Gibbs sampling step. 
 
        Parameters 
        ---------- 
        v : ndarray of shape (n_samples, n_features) 
            Values of the visible layer to start from. 
 
        Returns 
        ------- 
        v_new : ndarray of shape (n_samples, n_features) 
            Values of the visible layer after one Gibbs step. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;random_state_&quot;</span><span class="s1">):</span>
            <span class="s1">self.random_state_ = check_random_state(self.random_state)</span>
        <span class="s1">h_ = self._sample_hiddens(v</span><span class="s3">, </span><span class="s1">self.random_state_)</span>
        <span class="s1">v_ = self._sample_visibles(h_</span><span class="s3">, </span><span class="s1">self.random_state_)</span>

        <span class="s3">return </span><span class="s1">v_</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">partial_fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model to the partial segment of the data X. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples, n_features) 
            Training data. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None 
            Target values (None for unsupervised transformations). 
 
        Returns 
        ------- 
        self : BernoulliRBM 
            The fitted model. 
        &quot;&quot;&quot;</span>
        <span class="s1">first_pass = </span><span class="s3">not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;components_&quot;</span><span class="s1">)</span>
        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s1">dtype=np.float64</span><span class="s3">, </span><span class="s1">reset=first_pass</span>
        <span class="s1">)</span>
        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;random_state_&quot;</span><span class="s1">):</span>
            <span class="s1">self.random_state_ = check_random_state(self.random_state)</span>
        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;components_&quot;</span><span class="s1">):</span>
            <span class="s1">self.components_ = np.asarray(</span>
                <span class="s1">self.random_state_.normal(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0.01</span><span class="s3">, </span><span class="s1">(self.n_components</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">]))</span><span class="s3">,</span>
                <span class="s1">order=</span><span class="s4">&quot;F&quot;</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">self._n_features_out = self.components_.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;intercept_hidden_&quot;</span><span class="s1">):</span>
            <span class="s1">self.intercept_hidden_ = np.zeros(</span>
                <span class="s1">self.n_components</span><span class="s3">,</span>
            <span class="s1">)</span>
        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;intercept_visible_&quot;</span><span class="s1">):</span>
            <span class="s1">self.intercept_visible_ = np.zeros(</span>
                <span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">)</span>
        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;h_samples_&quot;</span><span class="s1">):</span>
            <span class="s1">self.h_samples_ = np.zeros((self.batch_size</span><span class="s3">, </span><span class="s1">self.n_components))</span>

        <span class="s1">self._fit(X</span><span class="s3">, </span><span class="s1">self.random_state_)</span>

    <span class="s3">def </span><span class="s1">_fit(self</span><span class="s3">, </span><span class="s1">v_pos</span><span class="s3">, </span><span class="s1">rng):</span>
        <span class="s0">&quot;&quot;&quot;Inner fit for one mini-batch. 
 
        Adjust the parameters to maximize the likelihood of v using 
        Stochastic Maximum Likelihood (SML). 
 
        Parameters 
        ---------- 
        v_pos : ndarray of shape (n_samples, n_features) 
            The data to use for training. 
 
        rng : RandomState instance 
            Random number generator to use for sampling. 
        &quot;&quot;&quot;</span>
        <span class="s1">h_pos = self._mean_hiddens(v_pos)</span>
        <span class="s1">v_neg = self._sample_visibles(self.h_samples_</span><span class="s3">, </span><span class="s1">rng)</span>
        <span class="s1">h_neg = self._mean_hiddens(v_neg)</span>

        <span class="s1">lr = float(self.learning_rate) / v_pos.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">update = safe_sparse_dot(v_pos.T</span><span class="s3">, </span><span class="s1">h_pos</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">).T</span>
        <span class="s1">update -= np.dot(h_neg.T</span><span class="s3">, </span><span class="s1">v_neg)</span>
        <span class="s1">self.components_ += lr * update</span>
        <span class="s1">self.intercept_hidden_ += lr * (h_pos.sum(axis=</span><span class="s5">0</span><span class="s1">) - h_neg.sum(axis=</span><span class="s5">0</span><span class="s1">))</span>
        <span class="s1">self.intercept_visible_ += lr * (</span>
            <span class="s1">np.asarray(v_pos.sum(axis=</span><span class="s5">0</span><span class="s1">)).squeeze() - v_neg.sum(axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">)</span>

        <span class="s1">h_neg[rng.uniform(size=h_neg.shape) &lt; h_neg] = </span><span class="s5">1.0  </span><span class="s2"># sample binomial</span>
        <span class="s1">self.h_samples_ = np.floor(h_neg</span><span class="s3">, </span><span class="s1">h_neg)</span>

    <span class="s3">def </span><span class="s1">score_samples(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Compute the pseudo-likelihood of X. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Values of the visible layer. Must be all-boolean (not checked). 
 
        Returns 
        ------- 
        pseudo_likelihood : ndarray of shape (n_samples,) 
            Value of the pseudo-likelihood (proxy for likelihood). 
 
        Notes 
        ----- 
        This method is not deterministic: it computes a quantity called the 
        free energy on X, then on a randomly corrupted version of X, and 
        returns the log of the logistic function of the difference. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s1">v = self._validate_data(X</span><span class="s3">, </span><span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">rng = check_random_state(self.random_state)</span>

        <span class="s2"># Randomly corrupt one feature in each sample in v.</span>
        <span class="s1">ind = (np.arange(v.shape[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">, </span><span class="s1">rng.randint(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">v.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">v.shape[</span><span class="s5">0</span><span class="s1">]))</span>
        <span class="s3">if </span><span class="s1">sp.issparse(v):</span>
            <span class="s1">data = -</span><span class="s5">2 </span><span class="s1">* v[ind] + </span><span class="s5">1</span>
            <span class="s1">v_ = v + sp.csr_matrix((data.A.ravel()</span><span class="s3">, </span><span class="s1">ind)</span><span class="s3">, </span><span class="s1">shape=v.shape)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">v_ = v.copy()</span>
            <span class="s1">v_[ind] = </span><span class="s5">1 </span><span class="s1">- v_[ind]</span>

        <span class="s1">fe = self._free_energy(v)</span>
        <span class="s1">fe_ = self._free_energy(v_)</span>
        <span class="s3">return </span><span class="s1">v.shape[</span><span class="s5">1</span><span class="s1">] * log_logistic(fe_ - fe)</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model to the data X. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training data. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None 
            Target values (None for unsupervised transformations). 
 
        Returns 
        ------- 
        self : BernoulliRBM 
            The fitted model. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s1">dtype=(np.float64</span><span class="s3">, </span><span class="s1">np.float32))</span>
        <span class="s1">n_samples = X.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">rng = check_random_state(self.random_state)</span>

        <span class="s1">self.components_ = np.asarray(</span>
            <span class="s1">rng.normal(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0.01</span><span class="s3">, </span><span class="s1">(self.n_components</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">]))</span><span class="s3">,</span>
            <span class="s1">order=</span><span class="s4">&quot;F&quot;</span><span class="s3">,</span>
            <span class="s1">dtype=X.dtype</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">self._n_features_out = self.components_.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">self.intercept_hidden_ = np.zeros(self.n_components</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s1">self.intercept_visible_ = np.zeros(X.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s1">self.h_samples_ = np.zeros((self.batch_size</span><span class="s3">, </span><span class="s1">self.n_components)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s1">n_batches = int(np.ceil(float(n_samples) / self.batch_size))</span>
        <span class="s1">batch_slices = list(</span>
            <span class="s1">gen_even_slices(n_batches * self.batch_size</span><span class="s3">, </span><span class="s1">n_batches</span><span class="s3">, </span><span class="s1">n_samples=n_samples)</span>
        <span class="s1">)</span>
        <span class="s1">verbose = self.verbose</span>
        <span class="s1">begin = time.time()</span>
        <span class="s3">for </span><span class="s1">iteration </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">self.n_iter + </span><span class="s5">1</span><span class="s1">):</span>
            <span class="s3">for </span><span class="s1">batch_slice </span><span class="s3">in </span><span class="s1">batch_slices:</span>
                <span class="s1">self._fit(X[batch_slice]</span><span class="s3">, </span><span class="s1">rng)</span>

            <span class="s3">if </span><span class="s1">verbose:</span>
                <span class="s1">end = time.time()</span>
                <span class="s1">print(</span>
                    <span class="s4">&quot;[%s] Iteration %d, pseudo-likelihood = %.2f, time = %.2fs&quot;</span>
                    <span class="s1">% (</span>
                        <span class="s1">type(self).__name__</span><span class="s3">,</span>
                        <span class="s1">iteration</span><span class="s3">,</span>
                        <span class="s1">self.score_samples(X).mean()</span><span class="s3">,</span>
                        <span class="s1">end - begin</span><span class="s3">,</span>
                    <span class="s1">)</span>
                <span class="s1">)</span>
                <span class="s1">begin = end</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span>
            <span class="s4">&quot;_xfail_checks&quot;</span><span class="s1">: {</span>
                <span class="s4">&quot;check_methods_subset_invariance&quot;</span><span class="s1">: (</span>
                    <span class="s4">&quot;fails for the decision_function method&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s4">&quot;check_methods_sample_order_invariance&quot;</span><span class="s1">: (</span>
                    <span class="s4">&quot;fails for the score_samples method&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">}</span><span class="s3">,</span>
            <span class="s4">&quot;preserves_dtype&quot;</span><span class="s1">: [np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">,</span>
        <span class="s1">}</span>
</pre>
</body>
</html>