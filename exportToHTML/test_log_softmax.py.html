<html>
<head>
<title>test_log_softmax.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_log_softmax.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">assert_allclose</span>

<span class="s0">import </span><span class="s1">pytest</span>

<span class="s0">import </span><span class="s1">scipy.special </span><span class="s0">as </span><span class="s1">sc</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">'x, expected'</span><span class="s0">, </span><span class="s1">[</span>
    <span class="s1">(np.array([</span><span class="s3">1000</span><span class="s0">, </span><span class="s3">1</span><span class="s1">])</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s3">0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">999</span><span class="s1">]))</span><span class="s0">,</span>

    <span class="s4"># Expected value computed using mpmath (with mpmath.mp.dps = 200) and then</span>
    <span class="s4"># converted to float.</span>
    <span class="s1">(np.arange(</span><span class="s3">4</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">3.4401896985611953</span><span class="s0">,</span>
                             <span class="s1">-</span><span class="s3">2.4401896985611953</span><span class="s0">,</span>
                             <span class="s1">-</span><span class="s3">1.4401896985611953</span><span class="s0">,</span>
                             <span class="s1">-</span><span class="s3">0.44018969856119533</span><span class="s1">]))</span>
<span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_log_softmax(x</span><span class="s0">, </span><span class="s1">expected):</span>
    <span class="s1">assert_allclose(sc.log_softmax(x)</span><span class="s0">, </span><span class="s1">expected</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-13</span><span class="s1">)</span>


<span class="s1">@pytest.fixture</span>
<span class="s0">def </span><span class="s1">log_softmax_x():</span>
    <span class="s1">x = np.arange(</span><span class="s3">4</span><span class="s1">)</span>
    <span class="s0">return </span><span class="s1">x</span>


<span class="s1">@pytest.fixture</span>
<span class="s0">def </span><span class="s1">log_softmax_expected():</span>
    <span class="s4"># Expected value computed using mpmath (with mpmath.mp.dps = 200) and then</span>
    <span class="s4"># converted to float.</span>
    <span class="s1">expected = np.array([-</span><span class="s3">3.4401896985611953</span><span class="s0">,</span>
                         <span class="s1">-</span><span class="s3">2.4401896985611953</span><span class="s0">,</span>
                         <span class="s1">-</span><span class="s3">1.4401896985611953</span><span class="s0">,</span>
                         <span class="s1">-</span><span class="s3">0.44018969856119533</span><span class="s1">])</span>
    <span class="s0">return </span><span class="s1">expected</span>


<span class="s0">def </span><span class="s1">test_log_softmax_translation(log_softmax_x</span><span class="s0">, </span><span class="s1">log_softmax_expected):</span>
    <span class="s4"># Translation property.  If all the values are changed by the same amount,</span>
    <span class="s4"># the softmax result does not change.</span>
    <span class="s1">x = log_softmax_x + </span><span class="s3">100</span>
    <span class="s1">expected = log_softmax_expected</span>
    <span class="s1">assert_allclose(sc.log_softmax(x)</span><span class="s0">, </span><span class="s1">expected</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-13</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_log_softmax_noneaxis(log_softmax_x</span><span class="s0">, </span><span class="s1">log_softmax_expected):</span>
    <span class="s4"># When axis=None, softmax operates on the entire array, and preserves</span>
    <span class="s4"># the shape.</span>
    <span class="s1">x = log_softmax_x.reshape(</span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">expected = log_softmax_expected.reshape(</span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">assert_allclose(sc.log_softmax(x)</span><span class="s0">, </span><span class="s1">expected</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-13</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">'axis_2d, expected_2d'</span><span class="s0">, </span><span class="s1">[</span>
    <span class="s1">(</span><span class="s3">0</span><span class="s0">, </span><span class="s1">np.log(</span><span class="s3">0.5</span><span class="s1">) * np.ones((</span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)))</span><span class="s0">,</span>
    <span class="s1">(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">np.array([[</span><span class="s3">0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">999</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">999</span><span class="s1">]]))</span>
<span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_axes(axis_2d</span><span class="s0">, </span><span class="s1">expected_2d):</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">sc.log_softmax([[</span><span class="s3">1000</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1000</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">axis=axis_2d)</span><span class="s0">,</span>
        <span class="s1">expected_2d</span><span class="s0">,</span>
        <span class="s1">rtol=</span><span class="s3">1e-13</span><span class="s0">,</span>
    <span class="s1">)</span>


<span class="s1">@pytest.fixture</span>
<span class="s0">def </span><span class="s1">log_softmax_2d_x():</span>
    <span class="s1">x = np.arange(</span><span class="s3">8</span><span class="s1">).reshape(</span><span class="s3">2</span><span class="s0">, </span><span class="s3">4</span><span class="s1">)</span>
    <span class="s0">return </span><span class="s1">x</span>


<span class="s1">@pytest.fixture</span>
<span class="s0">def </span><span class="s1">log_softmax_2d_expected():</span>
    <span class="s4"># Expected value computed using mpmath (with mpmath.mp.dps = 200) and then</span>
    <span class="s4"># converted to float.</span>
    <span class="s1">expected = np.array([[-</span><span class="s3">3.4401896985611953</span><span class="s0">,</span>
                         <span class="s1">-</span><span class="s3">2.4401896985611953</span><span class="s0">,</span>
                         <span class="s1">-</span><span class="s3">1.4401896985611953</span><span class="s0">,</span>
                         <span class="s1">-</span><span class="s3">0.44018969856119533</span><span class="s1">]</span><span class="s0">,</span>
                        <span class="s1">[-</span><span class="s3">3.4401896985611953</span><span class="s0">,</span>
                         <span class="s1">-</span><span class="s3">2.4401896985611953</span><span class="s0">,</span>
                         <span class="s1">-</span><span class="s3">1.4401896985611953</span><span class="s0">,</span>
                         <span class="s1">-</span><span class="s3">0.44018969856119533</span><span class="s1">]])</span>
    <span class="s0">return </span><span class="s1">expected</span>


<span class="s0">def </span><span class="s1">test_log_softmax_2d_axis1(log_softmax_2d_x</span><span class="s0">, </span><span class="s1">log_softmax_2d_expected):</span>
    <span class="s1">x = log_softmax_2d_x</span>
    <span class="s1">expected = log_softmax_2d_expected</span>
    <span class="s1">assert_allclose(sc.log_softmax(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">expected</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-13</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_log_softmax_2d_axis0(log_softmax_2d_x</span><span class="s0">, </span><span class="s1">log_softmax_2d_expected):</span>
    <span class="s1">x = log_softmax_2d_x.T</span>
    <span class="s1">expected = log_softmax_2d_expected.T</span>
    <span class="s1">assert_allclose(sc.log_softmax(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">expected</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-13</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_log_softmax_3d(log_softmax_2d_x</span><span class="s0">, </span><span class="s1">log_softmax_2d_expected):</span>
    <span class="s4"># 3-d input, with a tuple for the axis.</span>
    <span class="s1">x_3d = log_softmax_2d_x.reshape(</span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">expected_3d = log_softmax_2d_expected.reshape(</span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">assert_allclose(sc.log_softmax(x_3d</span><span class="s0">, </span><span class="s1">axis=(</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">))</span><span class="s0">, </span><span class="s1">expected_3d</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-13</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_log_softmax_scalar():</span>
    <span class="s1">assert_allclose(sc.log_softmax(</span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-13</span><span class="s1">)</span>
</pre>
</body>
</html>