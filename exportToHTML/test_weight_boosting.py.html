<html>
<head>
<title>test_weight_boosting.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_weight_boosting.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Testing for the boost module (sklearn.ensemble.boost).&quot;&quot;&quot;</span>

<span class="s2">import </span><span class="s1">re</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">scipy.sparse </span><span class="s2">import </span><span class="s1">coo_matrix</span><span class="s2">, </span><span class="s1">csc_matrix</span><span class="s2">, </span><span class="s1">csr_matrix</span><span class="s2">, </span><span class="s1">dok_matrix</span><span class="s2">, </span><span class="s1">lil_matrix</span>

<span class="s2">from </span><span class="s1">sklearn </span><span class="s2">import </span><span class="s1">datasets</span>
<span class="s2">from </span><span class="s1">sklearn.base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">clone</span>
<span class="s2">from </span><span class="s1">sklearn.dummy </span><span class="s2">import </span><span class="s1">DummyClassifier</span><span class="s2">, </span><span class="s1">DummyRegressor</span>
<span class="s2">from </span><span class="s1">sklearn.ensemble </span><span class="s2">import </span><span class="s1">AdaBoostClassifier</span><span class="s2">, </span><span class="s1">AdaBoostRegressor</span>
<span class="s2">from </span><span class="s1">sklearn.ensemble._weight_boosting </span><span class="s2">import </span><span class="s1">_samme_proba</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">LinearRegression</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">GridSearchCV</span><span class="s2">, </span><span class="s1">train_test_split</span>
<span class="s2">from </span><span class="s1">sklearn.svm </span><span class="s2">import </span><span class="s1">SVC</span><span class="s2">, </span><span class="s1">SVR</span>
<span class="s2">from </span><span class="s1">sklearn.tree </span><span class="s2">import </span><span class="s1">DecisionTreeClassifier</span><span class="s2">, </span><span class="s1">DecisionTreeRegressor</span>
<span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">shuffle</span>
<span class="s2">from </span><span class="s1">sklearn.utils._mocking </span><span class="s2">import </span><span class="s1">NoSampleWeightWrapper</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s2">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_less</span><span class="s2">,</span>
<span class="s1">)</span>

<span class="s3"># Common random state</span>
<span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

<span class="s3"># Toy sample</span>
<span class="s1">X = [[-</span><span class="s4">2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]]</span>
<span class="s1">y_class = [</span><span class="s5">&quot;foo&quot;</span><span class="s2">, </span><span class="s5">&quot;foo&quot;</span><span class="s2">, </span><span class="s5">&quot;foo&quot;</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]  </span><span class="s3"># test string class labels</span>
<span class="s1">y_regr = [-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>
<span class="s1">T = [[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]]</span>
<span class="s1">y_t_class = [</span><span class="s5">&quot;foo&quot;</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>
<span class="s1">y_t_regr = [-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>

<span class="s3"># Load the iris dataset and randomly permute it</span>
<span class="s1">iris = datasets.load_iris()</span>
<span class="s1">perm = rng.permutation(iris.target.size)</span>
<span class="s1">iris.data</span><span class="s2">, </span><span class="s1">iris.target = shuffle(iris.data</span><span class="s2">, </span><span class="s1">iris.target</span><span class="s2">, </span><span class="s1">random_state=rng)</span>

<span class="s3"># Load the diabetes dataset and randomly permute it</span>
<span class="s1">diabetes = datasets.load_diabetes()</span>
<span class="s1">diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target = shuffle(</span>
    <span class="s1">diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target</span><span class="s2">, </span><span class="s1">random_state=rng</span>
<span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_samme_proba():</span>
    <span class="s3"># Test the `_samme_proba` helper function.</span>

    <span class="s3"># Define some example (bad) `predict_proba` output.</span>
    <span class="s1">probs = np.array(</span>
        <span class="s1">[[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1e-6</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.19</span><span class="s2">, </span><span class="s4">0.6</span><span class="s2">, </span><span class="s4">0.2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">999</span><span class="s2">, </span><span class="s4">0.51</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1e-9</span><span class="s1">]]</span>
    <span class="s1">)</span>
    <span class="s1">probs /= np.abs(probs.sum(axis=</span><span class="s4">1</span><span class="s1">))[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>

    <span class="s3"># _samme_proba calls estimator.predict_proba.</span>
    <span class="s3"># Make a mock object so I can control what gets returned.</span>
    <span class="s2">class </span><span class="s1">MockEstimator:</span>
        <span class="s2">def </span><span class="s1">predict_proba(self</span><span class="s2">, </span><span class="s1">X):</span>
            <span class="s1">assert_array_equal(X.shape</span><span class="s2">, </span><span class="s1">probs.shape)</span>
            <span class="s2">return </span><span class="s1">probs</span>

    <span class="s1">mock = MockEstimator()</span>

    <span class="s1">samme_proba = _samme_proba(mock</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s1">np.ones_like(probs))</span>

    <span class="s1">assert_array_equal(samme_proba.shape</span><span class="s2">, </span><span class="s1">probs.shape)</span>
    <span class="s2">assert </span><span class="s1">np.isfinite(samme_proba).all()</span>

    <span class="s3"># Make sure that the correct elements come out as smallest --</span>
    <span class="s3"># `_samme_proba` should preserve the ordering in each example.</span>
    <span class="s1">assert_array_equal(np.argmin(samme_proba</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(np.argmax(samme_proba</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_oneclass_adaboost_proba():</span>
    <span class="s3"># Test predict_proba robustness for one class label input.</span>
    <span class="s3"># In response to issue #7501</span>
    <span class="s3"># https://github.com/scikit-learn/scikit-learn/issues/7501</span>
    <span class="s1">y_t = np.ones(len(X))</span>
    <span class="s1">clf = AdaBoostClassifier().fit(X</span><span class="s2">, </span><span class="s1">y_t)</span>
    <span class="s1">assert_array_almost_equal(clf.predict_proba(X)</span><span class="s2">, </span><span class="s1">np.ones((len(X)</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;algorithm&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">&quot;SAMME&quot;</span><span class="s2">, </span><span class="s5">&quot;SAMME.R&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_classification_toy(algorithm):</span>
    <span class="s3"># Check classification on a toy dataset.</span>
    <span class="s1">clf = AdaBoostClassifier(algorithm=algorithm</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y_class)</span>
    <span class="s1">assert_array_equal(clf.predict(T)</span><span class="s2">, </span><span class="s1">y_t_class)</span>
    <span class="s1">assert_array_equal(np.unique(np.asarray(y_t_class))</span><span class="s2">, </span><span class="s1">clf.classes_)</span>
    <span class="s2">assert </span><span class="s1">clf.predict_proba(T).shape == (len(T)</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">clf.decision_function(T).shape == (len(T)</span><span class="s2">,</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_regression_toy():</span>
    <span class="s3"># Check classification on a toy dataset.</span>
    <span class="s1">clf = AdaBoostRegressor(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y_regr)</span>
    <span class="s1">assert_array_equal(clf.predict(T)</span><span class="s2">, </span><span class="s1">y_t_regr)</span>


<span class="s2">def </span><span class="s1">test_iris():</span>
    <span class="s3"># Check consistency on dataset iris.</span>
    <span class="s1">classes = np.unique(iris.target)</span>
    <span class="s1">clf_samme = prob_samme = </span><span class="s2">None</span>

    <span class="s2">for </span><span class="s1">alg </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;SAMME&quot;</span><span class="s2">, </span><span class="s5">&quot;SAMME.R&quot;</span><span class="s1">]:</span>
        <span class="s1">clf = AdaBoostClassifier(algorithm=alg)</span>
        <span class="s1">clf.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>

        <span class="s1">assert_array_equal(classes</span><span class="s2">, </span><span class="s1">clf.classes_)</span>
        <span class="s1">proba = clf.predict_proba(iris.data)</span>
        <span class="s2">if </span><span class="s1">alg == </span><span class="s5">&quot;SAMME&quot;</span><span class="s1">:</span>
            <span class="s1">clf_samme = clf</span>
            <span class="s1">prob_samme = proba</span>
        <span class="s2">assert </span><span class="s1">proba.shape[</span><span class="s4">1</span><span class="s1">] == len(classes)</span>
        <span class="s2">assert </span><span class="s1">clf.decision_function(iris.data).shape[</span><span class="s4">1</span><span class="s1">] == len(classes)</span>

        <span class="s1">score = clf.score(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s2">assert </span><span class="s1">score &gt; </span><span class="s4">0.9</span><span class="s2">, </span><span class="s5">&quot;Failed with algorithm %s and score = %f&quot; </span><span class="s1">% (alg</span><span class="s2">, </span><span class="s1">score)</span>

        <span class="s3"># Check we used multiple estimators</span>
        <span class="s2">assert </span><span class="s1">len(clf.estimators_) &gt; </span><span class="s4">1</span>
        <span class="s3"># Check for distinct random states (see issue #7408)</span>
        <span class="s2">assert </span><span class="s1">len(set(est.random_state </span><span class="s2">for </span><span class="s1">est </span><span class="s2">in </span><span class="s1">clf.estimators_)) == len(</span>
            <span class="s1">clf.estimators_</span>
        <span class="s1">)</span>

    <span class="s3"># Somewhat hacky regression test: prior to</span>
    <span class="s3"># ae7adc880d624615a34bafdb1d75ef67051b8200,</span>
    <span class="s3"># predict_proba returned SAMME.R values for SAMME.</span>
    <span class="s1">clf_samme.algorithm = </span><span class="s5">&quot;SAMME.R&quot;</span>
    <span class="s1">assert_array_less(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">np.abs(clf_samme.predict_proba(iris.data) - prob_samme))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;loss&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s5">&quot;square&quot;</span><span class="s2">, </span><span class="s5">&quot;exponential&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_diabetes(loss):</span>
    <span class="s3"># Check consistency on dataset diabetes.</span>
    <span class="s1">reg = AdaBoostRegressor(loss=loss</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">reg.fit(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>
    <span class="s1">score = reg.score(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>
    <span class="s2">assert </span><span class="s1">score &gt; </span><span class="s4">0.55</span>

    <span class="s3"># Check we used multiple estimators</span>
    <span class="s2">assert </span><span class="s1">len(reg.estimators_) &gt; </span><span class="s4">1</span>
    <span class="s3"># Check for distinct random states (see issue #7408)</span>
    <span class="s2">assert </span><span class="s1">len(set(est.random_state </span><span class="s2">for </span><span class="s1">est </span><span class="s2">in </span><span class="s1">reg.estimators_)) == len(reg.estimators_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;algorithm&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">&quot;SAMME&quot;</span><span class="s2">, </span><span class="s5">&quot;SAMME.R&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_staged_predict(algorithm):</span>
    <span class="s3"># Check staged predictions.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">iris_weights = rng.randint(</span><span class="s4">10</span><span class="s2">, </span><span class="s1">size=iris.target.shape)</span>
    <span class="s1">diabetes_weights = rng.randint(</span><span class="s4">10</span><span class="s2">, </span><span class="s1">size=diabetes.target.shape)</span>

    <span class="s1">clf = AdaBoostClassifier(algorithm=algorithm</span><span class="s2">, </span><span class="s1">n_estimators=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">clf.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target</span><span class="s2">, </span><span class="s1">sample_weight=iris_weights)</span>

    <span class="s1">predictions = clf.predict(iris.data)</span>
    <span class="s1">staged_predictions = [p </span><span class="s2">for </span><span class="s1">p </span><span class="s2">in </span><span class="s1">clf.staged_predict(iris.data)]</span>
    <span class="s1">proba = clf.predict_proba(iris.data)</span>
    <span class="s1">staged_probas = [p </span><span class="s2">for </span><span class="s1">p </span><span class="s2">in </span><span class="s1">clf.staged_predict_proba(iris.data)]</span>
    <span class="s1">score = clf.score(iris.data</span><span class="s2">, </span><span class="s1">iris.target</span><span class="s2">, </span><span class="s1">sample_weight=iris_weights)</span>
    <span class="s1">staged_scores = [</span>
        <span class="s1">s </span><span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">clf.staged_score(iris.data</span><span class="s2">, </span><span class="s1">iris.target</span><span class="s2">, </span><span class="s1">sample_weight=iris_weights)</span>
    <span class="s1">]</span>

    <span class="s2">assert </span><span class="s1">len(staged_predictions) == </span><span class="s4">10</span>
    <span class="s1">assert_array_almost_equal(predictions</span><span class="s2">, </span><span class="s1">staged_predictions[-</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s2">assert </span><span class="s1">len(staged_probas) == </span><span class="s4">10</span>
    <span class="s1">assert_array_almost_equal(proba</span><span class="s2">, </span><span class="s1">staged_probas[-</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s2">assert </span><span class="s1">len(staged_scores) == </span><span class="s4">10</span>
    <span class="s1">assert_array_almost_equal(score</span><span class="s2">, </span><span class="s1">staged_scores[-</span><span class="s4">1</span><span class="s1">])</span>

    <span class="s3"># AdaBoost regression</span>
    <span class="s1">clf = AdaBoostRegressor(n_estimators=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target</span><span class="s2">, </span><span class="s1">sample_weight=diabetes_weights)</span>

    <span class="s1">predictions = clf.predict(diabetes.data)</span>
    <span class="s1">staged_predictions = [p </span><span class="s2">for </span><span class="s1">p </span><span class="s2">in </span><span class="s1">clf.staged_predict(diabetes.data)]</span>
    <span class="s1">score = clf.score(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target</span><span class="s2">, </span><span class="s1">sample_weight=diabetes_weights)</span>
    <span class="s1">staged_scores = [</span>
        <span class="s1">s</span>
        <span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">clf.staged_score(</span>
            <span class="s1">diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target</span><span class="s2">, </span><span class="s1">sample_weight=diabetes_weights</span>
        <span class="s1">)</span>
    <span class="s1">]</span>

    <span class="s2">assert </span><span class="s1">len(staged_predictions) == </span><span class="s4">10</span>
    <span class="s1">assert_array_almost_equal(predictions</span><span class="s2">, </span><span class="s1">staged_predictions[-</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s2">assert </span><span class="s1">len(staged_scores) == </span><span class="s4">10</span>
    <span class="s1">assert_array_almost_equal(score</span><span class="s2">, </span><span class="s1">staged_scores[-</span><span class="s4">1</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_gridsearch():</span>
    <span class="s3"># Check that base trees can be grid-searched.</span>
    <span class="s3"># AdaBoost classification</span>
    <span class="s1">boost = AdaBoostClassifier(estimator=DecisionTreeClassifier())</span>
    <span class="s1">parameters = {</span>
        <span class="s5">&quot;n_estimators&quot;</span><span class="s1">: (</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s5">&quot;estimator__max_depth&quot;</span><span class="s1">: (</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s5">&quot;algorithm&quot;</span><span class="s1">: (</span><span class="s5">&quot;SAMME&quot;</span><span class="s2">, </span><span class="s5">&quot;SAMME.R&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">}</span>
    <span class="s1">clf = GridSearchCV(boost</span><span class="s2">, </span><span class="s1">parameters)</span>
    <span class="s1">clf.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>

    <span class="s3"># AdaBoost regression</span>
    <span class="s1">boost = AdaBoostRegressor(estimator=DecisionTreeRegressor()</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">parameters = {</span><span class="s5">&quot;n_estimators&quot;</span><span class="s1">: (</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">, </span><span class="s5">&quot;estimator__max_depth&quot;</span><span class="s1">: (</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)}</span>
    <span class="s1">clf = GridSearchCV(boost</span><span class="s2">, </span><span class="s1">parameters)</span>
    <span class="s1">clf.fit(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>


<span class="s2">def </span><span class="s1">test_pickle():</span>
    <span class="s3"># Check pickability.</span>
    <span class="s2">import </span><span class="s1">pickle</span>

    <span class="s3"># Adaboost classifier</span>
    <span class="s2">for </span><span class="s1">alg </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;SAMME&quot;</span><span class="s2">, </span><span class="s5">&quot;SAMME.R&quot;</span><span class="s1">]:</span>
        <span class="s1">obj = AdaBoostClassifier(algorithm=alg)</span>
        <span class="s1">obj.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s1">score = obj.score(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s1">s = pickle.dumps(obj)</span>

        <span class="s1">obj2 = pickle.loads(s)</span>
        <span class="s2">assert </span><span class="s1">type(obj2) == obj.__class__</span>
        <span class="s1">score2 = obj2.score(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s2">assert </span><span class="s1">score == score2</span>

    <span class="s3"># Adaboost regressor</span>
    <span class="s1">obj = AdaBoostRegressor(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">obj.fit(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>
    <span class="s1">score = obj.score(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>
    <span class="s1">s = pickle.dumps(obj)</span>

    <span class="s1">obj2 = pickle.loads(s)</span>
    <span class="s2">assert </span><span class="s1">type(obj2) == obj.__class__</span>
    <span class="s1">score2 = obj2.score(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>
    <span class="s2">assert </span><span class="s1">score == score2</span>


<span class="s2">def </span><span class="s1">test_importances():</span>
    <span class="s3"># Check variable importances.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">2000</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">10</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">n_redundant=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">n_repeated=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s4">1</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s2">for </span><span class="s1">alg </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;SAMME&quot;</span><span class="s2">, </span><span class="s5">&quot;SAMME.R&quot;</span><span class="s1">]:</span>
        <span class="s1">clf = AdaBoostClassifier(algorithm=alg)</span>

        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">importances = clf.feature_importances_</span>

        <span class="s2">assert </span><span class="s1">importances.shape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">10</span>
        <span class="s2">assert </span><span class="s1">(importances[:</span><span class="s4">3</span><span class="s2">, </span><span class="s1">np.newaxis] &gt;= importances[</span><span class="s4">3</span><span class="s1">:]).all()</span>


<span class="s2">def </span><span class="s1">test_adaboost_classifier_sample_weight_error():</span>
    <span class="s3"># Test that it gives proper exception on incorrect sample weight.</span>
    <span class="s1">clf = AdaBoostClassifier()</span>
    <span class="s1">msg = re.escape(</span><span class="s5">&quot;sample_weight.shape == (1,), expected (6,)&quot;</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y_class</span><span class="s2">, </span><span class="s1">sample_weight=np.asarray([-</span><span class="s4">1</span><span class="s1">]))</span>


<span class="s2">def </span><span class="s1">test_estimator():</span>
    <span class="s3"># Test different estimators.</span>
    <span class="s2">from </span><span class="s1">sklearn.ensemble </span><span class="s2">import </span><span class="s1">RandomForestClassifier</span>

    <span class="s3"># XXX doesn't work with y_class because RF doesn't support classes_</span>
    <span class="s3"># Shouldn't AdaBoost run a LabelBinarizer?</span>
    <span class="s1">clf = AdaBoostClassifier(RandomForestClassifier())</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y_regr)</span>

    <span class="s1">clf = AdaBoostClassifier(SVC()</span><span class="s2">, </span><span class="s1">algorithm=</span><span class="s5">&quot;SAMME&quot;</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y_class)</span>

    <span class="s2">from </span><span class="s1">sklearn.ensemble </span><span class="s2">import </span><span class="s1">RandomForestRegressor</span>

    <span class="s1">clf = AdaBoostRegressor(RandomForestRegressor()</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y_regr)</span>

    <span class="s1">clf = AdaBoostRegressor(SVR()</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y_regr)</span>

    <span class="s3"># Check that an empty discrete ensemble fails in fit, not predict.</span>
    <span class="s1">X_fail = [[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]]</span>
    <span class="s1">y_fail = [</span><span class="s5">&quot;foo&quot;</span><span class="s2">, </span><span class="s5">&quot;bar&quot;</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span>
    <span class="s1">clf = AdaBoostClassifier(SVC()</span><span class="s2">, </span><span class="s1">algorithm=</span><span class="s5">&quot;SAMME&quot;</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;worse than random&quot;</span><span class="s1">):</span>
        <span class="s1">clf.fit(X_fail</span><span class="s2">, </span><span class="s1">y_fail)</span>


<span class="s2">def </span><span class="s1">test_sample_weights_infinite():</span>
    <span class="s1">msg = </span><span class="s5">&quot;Sample weights have reached infinite values&quot;</span>
    <span class="s1">clf = AdaBoostClassifier(n_estimators=</span><span class="s4">30</span><span class="s2">, </span><span class="s1">learning_rate=</span><span class="s4">23.0</span><span class="s2">, </span><span class="s1">algorithm=</span><span class="s5">&quot;SAMME&quot;</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>


<span class="s2">def </span><span class="s1">test_sparse_classification():</span>
    <span class="s3"># Check classification with sparse input.</span>

    <span class="s2">class </span><span class="s1">CustomSVC(SVC):</span>
        <span class="s0">&quot;&quot;&quot;SVC variant that records the nature of the training set.&quot;&quot;&quot;</span>

        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
            <span class="s0">&quot;&quot;&quot;Modification on fit caries data type for later verification.&quot;&quot;&quot;</span>
            <span class="s1">super().fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
            <span class="s1">self.data_type_ = type(X)</span>
            <span class="s2">return </span><span class="s1">self</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_multilabel_classification(</span>
        <span class="s1">n_classes=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">n_samples=</span><span class="s4">15</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span>
    <span class="s1">)</span>
    <span class="s3"># Flatten y to a 1d array</span>
    <span class="s1">y = np.ravel(y)</span>

    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test = train_test_split(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">sparse_format </span><span class="s2">in </span><span class="s1">[csc_matrix</span><span class="s2">, </span><span class="s1">csr_matrix</span><span class="s2">, </span><span class="s1">lil_matrix</span><span class="s2">, </span><span class="s1">coo_matrix</span><span class="s2">, </span><span class="s1">dok_matrix]:</span>
        <span class="s1">X_train_sparse = sparse_format(X_train)</span>
        <span class="s1">X_test_sparse = sparse_format(X_test)</span>

        <span class="s3"># Trained on sparse format</span>
        <span class="s1">sparse_classifier = AdaBoostClassifier(</span>
            <span class="s1">estimator=CustomSVC(probability=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">random_state=</span><span class="s4">1</span><span class="s2">,</span>
            <span class="s1">algorithm=</span><span class="s5">&quot;SAMME&quot;</span><span class="s2">,</span>
        <span class="s1">).fit(X_train_sparse</span><span class="s2">, </span><span class="s1">y_train)</span>

        <span class="s3"># Trained on dense format</span>
        <span class="s1">dense_classifier = AdaBoostClassifier(</span>
            <span class="s1">estimator=CustomSVC(probability=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">random_state=</span><span class="s4">1</span><span class="s2">,</span>
            <span class="s1">algorithm=</span><span class="s5">&quot;SAMME&quot;</span><span class="s2">,</span>
        <span class="s1">).fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>

        <span class="s3"># predict</span>
        <span class="s1">sparse_results = sparse_classifier.predict(X_test_sparse)</span>
        <span class="s1">dense_results = dense_classifier.predict(X_test)</span>
        <span class="s1">assert_array_equal(sparse_results</span><span class="s2">, </span><span class="s1">dense_results)</span>

        <span class="s3"># decision_function</span>
        <span class="s1">sparse_results = sparse_classifier.decision_function(X_test_sparse)</span>
        <span class="s1">dense_results = dense_classifier.decision_function(X_test)</span>
        <span class="s1">assert_array_almost_equal(sparse_results</span><span class="s2">, </span><span class="s1">dense_results)</span>

        <span class="s3"># predict_log_proba</span>
        <span class="s1">sparse_results = sparse_classifier.predict_log_proba(X_test_sparse)</span>
        <span class="s1">dense_results = dense_classifier.predict_log_proba(X_test)</span>
        <span class="s1">assert_array_almost_equal(sparse_results</span><span class="s2">, </span><span class="s1">dense_results)</span>

        <span class="s3"># predict_proba</span>
        <span class="s1">sparse_results = sparse_classifier.predict_proba(X_test_sparse)</span>
        <span class="s1">dense_results = dense_classifier.predict_proba(X_test)</span>
        <span class="s1">assert_array_almost_equal(sparse_results</span><span class="s2">, </span><span class="s1">dense_results)</span>

        <span class="s3"># score</span>
        <span class="s1">sparse_results = sparse_classifier.score(X_test_sparse</span><span class="s2">, </span><span class="s1">y_test)</span>
        <span class="s1">dense_results = dense_classifier.score(X_test</span><span class="s2">, </span><span class="s1">y_test)</span>
        <span class="s1">assert_array_almost_equal(sparse_results</span><span class="s2">, </span><span class="s1">dense_results)</span>

        <span class="s3"># staged_decision_function</span>
        <span class="s1">sparse_results = sparse_classifier.staged_decision_function(X_test_sparse)</span>
        <span class="s1">dense_results = dense_classifier.staged_decision_function(X_test)</span>
        <span class="s2">for </span><span class="s1">sprase_res</span><span class="s2">, </span><span class="s1">dense_res </span><span class="s2">in </span><span class="s1">zip(sparse_results</span><span class="s2">, </span><span class="s1">dense_results):</span>
            <span class="s1">assert_array_almost_equal(sprase_res</span><span class="s2">, </span><span class="s1">dense_res)</span>

        <span class="s3"># staged_predict</span>
        <span class="s1">sparse_results = sparse_classifier.staged_predict(X_test_sparse)</span>
        <span class="s1">dense_results = dense_classifier.staged_predict(X_test)</span>
        <span class="s2">for </span><span class="s1">sprase_res</span><span class="s2">, </span><span class="s1">dense_res </span><span class="s2">in </span><span class="s1">zip(sparse_results</span><span class="s2">, </span><span class="s1">dense_results):</span>
            <span class="s1">assert_array_equal(sprase_res</span><span class="s2">, </span><span class="s1">dense_res)</span>

        <span class="s3"># staged_predict_proba</span>
        <span class="s1">sparse_results = sparse_classifier.staged_predict_proba(X_test_sparse)</span>
        <span class="s1">dense_results = dense_classifier.staged_predict_proba(X_test)</span>
        <span class="s2">for </span><span class="s1">sprase_res</span><span class="s2">, </span><span class="s1">dense_res </span><span class="s2">in </span><span class="s1">zip(sparse_results</span><span class="s2">, </span><span class="s1">dense_results):</span>
            <span class="s1">assert_array_almost_equal(sprase_res</span><span class="s2">, </span><span class="s1">dense_res)</span>

        <span class="s3"># staged_score</span>
        <span class="s1">sparse_results = sparse_classifier.staged_score(X_test_sparse</span><span class="s2">, </span><span class="s1">y_test)</span>
        <span class="s1">dense_results = dense_classifier.staged_score(X_test</span><span class="s2">, </span><span class="s1">y_test)</span>
        <span class="s2">for </span><span class="s1">sprase_res</span><span class="s2">, </span><span class="s1">dense_res </span><span class="s2">in </span><span class="s1">zip(sparse_results</span><span class="s2">, </span><span class="s1">dense_results):</span>
            <span class="s1">assert_array_equal(sprase_res</span><span class="s2">, </span><span class="s1">dense_res)</span>

        <span class="s3"># Verify sparsity of data is maintained during training</span>
        <span class="s1">types = [i.data_type_ </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">sparse_classifier.estimators_]</span>

        <span class="s2">assert </span><span class="s1">all([(t == csc_matrix </span><span class="s2">or </span><span class="s1">t == csr_matrix) </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">types])</span>


<span class="s2">def </span><span class="s1">test_sparse_regression():</span>
    <span class="s3"># Check regression with sparse input.</span>

    <span class="s2">class </span><span class="s1">CustomSVR(SVR):</span>
        <span class="s0">&quot;&quot;&quot;SVR variant that records the nature of the training set.&quot;&quot;&quot;</span>

        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
            <span class="s0">&quot;&quot;&quot;Modification on fit caries data type for later verification.&quot;&quot;&quot;</span>
            <span class="s1">super().fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
            <span class="s1">self.data_type_ = type(X)</span>
            <span class="s2">return </span><span class="s1">self</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_regression(</span>
        <span class="s1">n_samples=</span><span class="s4">15</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">n_targets=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span>
    <span class="s1">)</span>

    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test = train_test_split(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">sparse_format </span><span class="s2">in </span><span class="s1">[csc_matrix</span><span class="s2">, </span><span class="s1">csr_matrix</span><span class="s2">, </span><span class="s1">lil_matrix</span><span class="s2">, </span><span class="s1">coo_matrix</span><span class="s2">, </span><span class="s1">dok_matrix]:</span>
        <span class="s1">X_train_sparse = sparse_format(X_train)</span>
        <span class="s1">X_test_sparse = sparse_format(X_test)</span>

        <span class="s3"># Trained on sparse format</span>
        <span class="s1">sparse_classifier = AdaBoostRegressor(</span>
            <span class="s1">estimator=CustomSVR()</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">1</span>
        <span class="s1">).fit(X_train_sparse</span><span class="s2">, </span><span class="s1">y_train)</span>

        <span class="s3"># Trained on dense format</span>
        <span class="s1">dense_classifier = dense_results = AdaBoostRegressor(</span>
            <span class="s1">estimator=CustomSVR()</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">1</span>
        <span class="s1">).fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>

        <span class="s3"># predict</span>
        <span class="s1">sparse_results = sparse_classifier.predict(X_test_sparse)</span>
        <span class="s1">dense_results = dense_classifier.predict(X_test)</span>
        <span class="s1">assert_array_almost_equal(sparse_results</span><span class="s2">, </span><span class="s1">dense_results)</span>

        <span class="s3"># staged_predict</span>
        <span class="s1">sparse_results = sparse_classifier.staged_predict(X_test_sparse)</span>
        <span class="s1">dense_results = dense_classifier.staged_predict(X_test)</span>
        <span class="s2">for </span><span class="s1">sprase_res</span><span class="s2">, </span><span class="s1">dense_res </span><span class="s2">in </span><span class="s1">zip(sparse_results</span><span class="s2">, </span><span class="s1">dense_results):</span>
            <span class="s1">assert_array_almost_equal(sprase_res</span><span class="s2">, </span><span class="s1">dense_res)</span>

        <span class="s1">types = [i.data_type_ </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">sparse_classifier.estimators_]</span>

        <span class="s2">assert </span><span class="s1">all([(t == csc_matrix </span><span class="s2">or </span><span class="s1">t == csr_matrix) </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">types])</span>


<span class="s2">def </span><span class="s1">test_sample_weight_adaboost_regressor():</span>
    <span class="s0">&quot;&quot;&quot; 
    AdaBoostRegressor should work without sample_weights in the base estimator 
    The random weighted sampling is done internally in the _boost method in 
    AdaBoostRegressor. 
    &quot;&quot;&quot;</span>

    <span class="s2">class </span><span class="s1">DummyEstimator(BaseEstimator):</span>
        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
            <span class="s2">pass</span>

        <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
            <span class="s2">return </span><span class="s1">np.zeros(X.shape[</span><span class="s4">0</span><span class="s1">])</span>

    <span class="s1">boost = AdaBoostRegressor(DummyEstimator()</span><span class="s2">, </span><span class="s1">n_estimators=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">boost.fit(X</span><span class="s2">, </span><span class="s1">y_regr)</span>
    <span class="s2">assert </span><span class="s1">len(boost.estimator_weights_) == len(boost.estimator_errors_)</span>


<span class="s2">def </span><span class="s1">test_multidimensional_X():</span>
    <span class="s0">&quot;&quot;&quot; 
    Check that the AdaBoost estimators can work with n-dimensional 
    data matrix 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">X = rng.randn(</span><span class="s4">51</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">yc = rng.choice([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s4">51</span><span class="s1">)</span>
    <span class="s1">yr = rng.randn(</span><span class="s4">51</span><span class="s1">)</span>

    <span class="s1">boost = AdaBoostClassifier(DummyClassifier(strategy=</span><span class="s5">&quot;most_frequent&quot;</span><span class="s1">))</span>
    <span class="s1">boost.fit(X</span><span class="s2">, </span><span class="s1">yc)</span>
    <span class="s1">boost.predict(X)</span>
    <span class="s1">boost.predict_proba(X)</span>

    <span class="s1">boost = AdaBoostRegressor(DummyRegressor())</span>
    <span class="s1">boost.fit(X</span><span class="s2">, </span><span class="s1">yr)</span>
    <span class="s1">boost.predict(X)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;algorithm&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">&quot;SAMME&quot;</span><span class="s2">, </span><span class="s5">&quot;SAMME.R&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_adaboostclassifier_without_sample_weight(algorithm):</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = iris.data</span><span class="s2">, </span><span class="s1">iris.target</span>
    <span class="s1">estimator = NoSampleWeightWrapper(DummyClassifier())</span>
    <span class="s1">clf = AdaBoostClassifier(estimator=estimator</span><span class="s2">, </span><span class="s1">algorithm=algorithm)</span>
    <span class="s1">err_msg = </span><span class="s5">&quot;{} doesn't support sample_weight&quot;</span><span class="s1">.format(estimator.__class__.__name__)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_adaboostregressor_sample_weight():</span>
    <span class="s3"># check that giving weight will have an influence on the error computed</span>
    <span class="s3"># for a weak learner</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">X = np.linspace(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">100</span><span class="s2">, </span><span class="s1">num=</span><span class="s4">1000</span><span class="s1">)</span>
    <span class="s1">y = (</span><span class="s4">0.8 </span><span class="s1">* X + </span><span class="s4">0.2</span><span class="s1">) + (rng.rand(X.shape[</span><span class="s4">0</span><span class="s1">]) * </span><span class="s4">0.0001</span><span class="s1">)</span>
    <span class="s1">X = X.reshape(-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s3"># add an arbitrary outlier</span>
    <span class="s1">X[-</span><span class="s4">1</span><span class="s1">] *= </span><span class="s4">10</span>
    <span class="s1">y[-</span><span class="s4">1</span><span class="s1">] = </span><span class="s4">10000</span>

    <span class="s3"># random_state=0 ensure that the underlying bootstrap will use the outlier</span>
    <span class="s1">regr_no_outlier = AdaBoostRegressor(</span>
        <span class="s1">estimator=LinearRegression()</span><span class="s2">, </span><span class="s1">n_estimators=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">regr_with_weight = clone(regr_no_outlier)</span>
    <span class="s1">regr_with_outlier = clone(regr_no_outlier)</span>

    <span class="s3"># fit 3 models:</span>
    <span class="s3"># - a model containing the outlier</span>
    <span class="s3"># - a model without the outlier</span>
    <span class="s3"># - a model containing the outlier but with a null sample-weight</span>
    <span class="s1">regr_with_outlier.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">regr_no_outlier.fit(X[:-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y[:-</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">sample_weight = np.ones_like(y)</span>
    <span class="s1">sample_weight[-</span><span class="s4">1</span><span class="s1">] = </span><span class="s4">0</span>
    <span class="s1">regr_with_weight.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">score_with_outlier = regr_with_outlier.score(X[:-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y[:-</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">score_no_outlier = regr_no_outlier.score(X[:-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y[:-</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">score_with_weight = regr_with_weight.score(X[:-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y[:-</span><span class="s4">1</span><span class="s1">])</span>

    <span class="s2">assert </span><span class="s1">score_with_outlier &lt; score_no_outlier</span>
    <span class="s2">assert </span><span class="s1">score_with_outlier &lt; score_with_weight</span>
    <span class="s2">assert </span><span class="s1">score_no_outlier == pytest.approx(score_with_weight)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;algorithm&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">&quot;SAMME&quot;</span><span class="s2">, </span><span class="s5">&quot;SAMME.R&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_adaboost_consistent_predict(algorithm):</span>
    <span class="s3"># check that predict_proba and predict give consistent results</span>
    <span class="s3"># regression test for:</span>
    <span class="s3"># https://github.com/scikit-learn/scikit-learn/issues/14084</span>
    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">*datasets.load_digits(return_X_y=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span>
    <span class="s1">)</span>
    <span class="s1">model = AdaBoostClassifier(algorithm=algorithm</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">model.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>

    <span class="s1">assert_array_equal(</span>
        <span class="s1">np.argmax(model.predict_proba(X_test)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">model.predict(X_test)</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;model, X, y&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(AdaBoostClassifier()</span><span class="s2">, </span><span class="s1">iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span><span class="s2">,</span>
        <span class="s1">(AdaBoostRegressor()</span><span class="s2">, </span><span class="s1">diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_adaboost_negative_weight_error(model</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
    <span class="s1">sample_weight = np.ones_like(y)</span>
    <span class="s1">sample_weight[-</span><span class="s4">1</span><span class="s1">] = -</span><span class="s4">10</span>

    <span class="s1">err_msg = </span><span class="s5">&quot;Negative values in data passed to `sample_weight`&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">model.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>


<span class="s2">def </span><span class="s1">test_adaboost_numerically_stable_feature_importance_with_small_weights():</span>
    <span class="s0">&quot;&quot;&quot;Check that we don't create NaN feature importance with numerically 
    instable inputs. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/20320 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">X = rng.normal(size=(</span><span class="s4">1000</span><span class="s2">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">y = rng.choice([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1000</span><span class="s1">)</span>
    <span class="s1">sample_weight = np.ones_like(y) * </span><span class="s4">1e-263</span>
    <span class="s1">tree = DecisionTreeClassifier(max_depth=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">12</span><span class="s1">)</span>
    <span class="s1">ada_model = AdaBoostClassifier(estimator=tree</span><span class="s2">, </span><span class="s1">n_estimators=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">12</span><span class="s1">)</span>
    <span class="s1">ada_model.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s2">assert </span><span class="s1">np.isnan(ada_model.feature_importances_).sum() == </span><span class="s4">0</span>


<span class="s3"># TODO(1.4): remove in 1.4</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;AdaBoost, Estimator&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(AdaBoostClassifier</span><span class="s2">, </span><span class="s1">DecisionTreeClassifier)</span><span class="s2">,</span>
        <span class="s1">(AdaBoostRegressor</span><span class="s2">, </span><span class="s1">DecisionTreeRegressor)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_base_estimator_argument_deprecated(AdaBoost</span><span class="s2">, </span><span class="s1">Estimator):</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">model = AdaBoost(base_estimator=Estimator())</span>

    <span class="s1">warn_msg = (</span>
        <span class="s5">&quot;`base_estimator` was renamed to `estimator` in version 1.2 and &quot;</span>
        <span class="s5">&quot;will be removed in 1.4.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s2">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">model.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s3"># TODO(1.4): remove in 1.4</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;AdaBoost&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">AdaBoostClassifier</span><span class="s2">,</span>
        <span class="s1">AdaBoostRegressor</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_base_estimator_argument_deprecated_none(AdaBoost):</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">model = AdaBoost(base_estimator=</span><span class="s2">None</span><span class="s1">)</span>

    <span class="s1">warn_msg = (</span>
        <span class="s5">&quot;`base_estimator` was renamed to `estimator` in version 1.2 and &quot;</span>
        <span class="s5">&quot;will be removed in 1.4.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s2">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">model.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s3"># TODO(1.4): remove in 1.4</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;AdaBoost&quot;</span><span class="s2">,</span>
    <span class="s1">[AdaBoostClassifier</span><span class="s2">, </span><span class="s1">AdaBoostRegressor]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_base_estimator_property_deprecated(AdaBoost):</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">model = AdaBoost()</span>
    <span class="s1">model.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">warn_msg = (</span>
        <span class="s5">&quot;Attribute `base_estimator_` was deprecated in version 1.2 and &quot;</span>
        <span class="s5">&quot;will be removed in 1.4. Use `estimator_` instead.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s2">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">model.base_estimator_</span>


<span class="s3"># TODO(1.4): remove in 1.4</span>
<span class="s2">def </span><span class="s1">test_deprecated_base_estimator_parameters_can_be_set():</span>
    <span class="s0">&quot;&quot;&quot;Check that setting base_estimator parameters works. 
 
    During the deprecation cycle setting &quot;base_estimator__*&quot; params should 
    work. 
 
    Non-regression test for https://github.com/scikit-learn/scikit-learn/issues/25470 
    &quot;&quot;&quot;</span>
    <span class="s3"># This implicitly sets &quot;estimator&quot;, it is how old code (pre v1.2) would</span>
    <span class="s3"># have instantiated AdaBoostClassifier and back then it would set</span>
    <span class="s3"># &quot;base_estimator&quot;.</span>
    <span class="s1">clf = AdaBoostClassifier(DecisionTreeClassifier())</span>

    <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;Parameter 'base_estimator' of&quot;</span><span class="s1">):</span>
        <span class="s1">clf.set_params(base_estimator__max_depth=</span><span class="s4">2</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;algorithm&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">&quot;SAMME&quot;</span><span class="s2">, </span><span class="s5">&quot;SAMME.R&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_adaboost_decision_function(algorithm</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Check that the decision function respects the symmetric constraint for weak 
    learners. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/26520 
    &quot;&quot;&quot;</span>
    <span class="s1">n_classes = </span><span class="s4">3</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_classification(</span>
        <span class="s1">n_classes=n_classes</span><span class="s2">, </span><span class="s1">n_clusters_per_class=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>
    <span class="s1">clf = AdaBoostClassifier(</span>
        <span class="s1">n_estimators=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span><span class="s2">, </span><span class="s1">algorithm=algorithm</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">y_score = clf.decision_function(X)</span>
    <span class="s1">assert_allclose(y_score.sum(axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-8</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">algorithm == </span><span class="s5">&quot;SAMME&quot;</span><span class="s1">:</span>
        <span class="s3"># With a single learner, we expect to have a decision function in</span>
        <span class="s3"># {1, - 1 / (n_classes - 1)}.</span>
        <span class="s2">assert </span><span class="s1">set(np.unique(y_score)) == {</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1 </span><span class="s1">/ (n_classes - </span><span class="s4">1</span><span class="s1">)}</span>

    <span class="s3"># We can assert the same for staged_decision_function since we have a single learner</span>
    <span class="s2">for </span><span class="s1">y_score </span><span class="s2">in </span><span class="s1">clf.staged_decision_function(X):</span>
        <span class="s1">assert_allclose(y_score.sum(axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-8</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">algorithm == </span><span class="s5">&quot;SAMME&quot;</span><span class="s1">:</span>
            <span class="s3"># With a single learner, we expect to have a decision function in</span>
            <span class="s3"># {1, - 1 / (n_classes - 1)}.</span>
            <span class="s2">assert </span><span class="s1">set(np.unique(y_score)) == {</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1 </span><span class="s1">/ (n_classes - </span><span class="s4">1</span><span class="s1">)}</span>

    <span class="s1">clf.set_params(n_estimators=</span><span class="s4">5</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">y_score = clf.decision_function(X)</span>
    <span class="s1">assert_allclose(y_score.sum(axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-8</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">y_score </span><span class="s2">in </span><span class="s1">clf.staged_decision_function(X):</span>
        <span class="s1">assert_allclose(y_score.sum(axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-8</span><span class="s1">)</span>
</pre>
</body>
</html>