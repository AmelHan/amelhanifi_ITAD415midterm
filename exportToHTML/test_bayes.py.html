<html>
<head>
<title>test_bayes.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_bayes.py</font>
</center></td></tr></table>
<pre><span class="s0"># Author: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr&gt;</span>
<span class="s0">#         Fabian Pedregosa &lt;fabian.pedregosa@inria.fr&gt;</span>
<span class="s0">#</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">from </span><span class="s1">math </span><span class="s2">import </span><span class="s1">log</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>

<span class="s2">from </span><span class="s1">sklearn </span><span class="s2">import </span><span class="s1">datasets</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">ARDRegression</span><span class="s2">, </span><span class="s1">BayesianRidge</span><span class="s2">, </span><span class="s1">Ridge</span>
<span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">check_random_state</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">assert_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_less</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.utils.extmath </span><span class="s2">import </span><span class="s1">fast_logdet</span>

<span class="s1">diabetes = datasets.load_diabetes()</span>


<span class="s2">def </span><span class="s1">test_bayesian_ridge_scores():</span>
    <span class="s3">&quot;&quot;&quot;Check scores attribute shape&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target</span>

    <span class="s1">clf = BayesianRidge(compute_score=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">assert </span><span class="s1">clf.scores_.shape == (clf.n_iter_ + </span><span class="s4">1</span><span class="s2">,</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_bayesian_ridge_score_values():</span>
    <span class="s3">&quot;&quot;&quot;Check value of score on toy example. 
 
    Compute log marginal likelihood with equation (36) in Sparse Bayesian 
    Learning and the Relevance Vector Machine (Tipping, 2001): 
 
    - 0.5 * (log |Id/alpha + X.X^T/lambda| + 
             y^T.(Id/alpha + X.X^T/lambda).y + n * log(2 * pi)) 
    + lambda_1 * log(lambda) - lambda_2 * lambda 
    + alpha_1 * log(alpha) - alpha_2 * alpha 
 
    and check equality with the score computed during training. 
    &quot;&quot;&quot;</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target</span>
    <span class="s1">n_samples = X.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s0"># check with initial values of alpha and lambda (see code for the values)</span>
    <span class="s1">eps = np.finfo(np.float64).eps</span>
    <span class="s1">alpha_ = </span><span class="s4">1.0 </span><span class="s1">/ (np.var(y) + eps)</span>
    <span class="s1">lambda_ = </span><span class="s4">1.0</span>

    <span class="s0"># value of the parameters of the Gamma hyperpriors</span>
    <span class="s1">alpha_1 = </span><span class="s4">0.1</span>
    <span class="s1">alpha_2 = </span><span class="s4">0.1</span>
    <span class="s1">lambda_1 = </span><span class="s4">0.1</span>
    <span class="s1">lambda_2 = </span><span class="s4">0.1</span>

    <span class="s0"># compute score using formula of docstring</span>
    <span class="s1">score = lambda_1 * log(lambda_) - lambda_2 * lambda_</span>
    <span class="s1">score += alpha_1 * log(alpha_) - alpha_2 * alpha_</span>
    <span class="s1">M = </span><span class="s4">1.0 </span><span class="s1">/ alpha_ * np.eye(n_samples) + </span><span class="s4">1.0 </span><span class="s1">/ lambda_ * np.dot(X</span><span class="s2">, </span><span class="s1">X.T)</span>
    <span class="s1">M_inv_dot_y = np.linalg.solve(M</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">score += -</span><span class="s4">0.5 </span><span class="s1">* (</span>
        <span class="s1">fast_logdet(M) + np.dot(y.T</span><span class="s2">, </span><span class="s1">M_inv_dot_y) + n_samples * log(</span><span class="s4">2 </span><span class="s1">* np.pi)</span>
    <span class="s1">)</span>

    <span class="s0"># compute score with BayesianRidge</span>
    <span class="s1">clf = BayesianRidge(</span>
        <span class="s1">alpha_1=alpha_1</span><span class="s2">,</span>
        <span class="s1">alpha_2=alpha_2</span><span class="s2">,</span>
        <span class="s1">lambda_1=lambda_1</span><span class="s2">,</span>
        <span class="s1">lambda_2=lambda_2</span><span class="s2">,</span>
        <span class="s1">max_iter=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">fit_intercept=</span><span class="s2">False,</span>
        <span class="s1">compute_score=</span><span class="s2">True,</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">assert_almost_equal(clf.scores_[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">9</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_bayesian_ridge_parameter():</span>
    <span class="s0"># Test correctness of lambda_ and alpha_ parameters (GitHub issue #8224)</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s2">, </span><span class="s4">7</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">4</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">6</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">5</span><span class="s1">]).T</span>

    <span class="s0"># A Ridge regression model using an alpha value equal to the ratio of</span>
    <span class="s0"># lambda_ and alpha_ from the Bayesian Ridge model must be identical</span>
    <span class="s1">br_model = BayesianRidge(compute_score=</span><span class="s2">True</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">rr_model = Ridge(alpha=br_model.lambda_ / br_model.alpha_).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(rr_model.coef_</span><span class="s2">, </span><span class="s1">br_model.coef_)</span>
    <span class="s1">assert_almost_equal(rr_model.intercept_</span><span class="s2">, </span><span class="s1">br_model.intercept_)</span>


<span class="s2">def </span><span class="s1">test_bayesian_sample_weights():</span>
    <span class="s0"># Test correctness of the sample_weights method</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s2">, </span><span class="s4">7</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">4</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">6</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">5</span><span class="s1">]).T</span>
    <span class="s1">w = np.array([</span><span class="s4">4</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]).T</span>

    <span class="s0"># A Ridge regression model using an alpha value equal to the ratio of</span>
    <span class="s0"># lambda_ and alpha_ from the Bayesian Ridge model must be identical</span>
    <span class="s1">br_model = BayesianRidge(compute_score=</span><span class="s2">True</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=w)</span>
    <span class="s1">rr_model = Ridge(alpha=br_model.lambda_ / br_model.alpha_).fit(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=w</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(rr_model.coef_</span><span class="s2">, </span><span class="s1">br_model.coef_)</span>
    <span class="s1">assert_almost_equal(rr_model.intercept_</span><span class="s2">, </span><span class="s1">br_model.intercept_)</span>


<span class="s2">def </span><span class="s1">test_toy_bayesian_ridge_object():</span>
    <span class="s0"># Test BayesianRidge on toy</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">6</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">8</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">10</span><span class="s1">]])</span>
    <span class="s1">Y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">8</span><span class="s2">, </span><span class="s4">10</span><span class="s1">])</span>
    <span class="s1">clf = BayesianRidge(compute_score=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>

    <span class="s0"># Check that the model could approximately learn the identity function</span>
    <span class="s1">test = [[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">4</span><span class="s1">]]</span>
    <span class="s1">assert_array_almost_equal(clf.predict(test)</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_bayesian_initial_params():</span>
    <span class="s0"># Test BayesianRidge with initial values (alpha_init, lambda_init)</span>
    <span class="s1">X = np.vander(np.linspace(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">5</span><span class="s1">)</span><span class="s2">, </span><span class="s4">4</span><span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.0</span><span class="s1">])  </span><span class="s0"># y = (x^3 - 6x^2 + 8x) / 3</span>

    <span class="s0"># In this case, starting from the default initial values will increase</span>
    <span class="s0"># the bias of the fitted curve. So, lambda_init should be small.</span>
    <span class="s1">reg = BayesianRidge(alpha_init=</span><span class="s4">1.0</span><span class="s2">, </span><span class="s1">lambda_init=</span><span class="s4">1e-3</span><span class="s1">)</span>
    <span class="s0"># Check the R2 score nearly equals to one.</span>
    <span class="s1">r2 = reg.fit(X</span><span class="s2">, </span><span class="s1">y).score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(r2</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_prediction_bayesian_ridge_ard_with_constant_input():</span>
    <span class="s0"># Test BayesianRidge and ARDRegression predictions for edge case of</span>
    <span class="s0"># constant target vectors</span>
    <span class="s1">n_samples = </span><span class="s4">4</span>
    <span class="s1">n_features = </span><span class="s4">5</span>
    <span class="s1">random_state = check_random_state(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">constant_value = random_state.rand()</span>
    <span class="s1">X = random_state.random_sample((n_samples</span><span class="s2">, </span><span class="s1">n_features))</span>
    <span class="s1">y = np.full(n_samples</span><span class="s2">, </span><span class="s1">constant_value</span><span class="s2">, </span><span class="s1">dtype=np.array(constant_value).dtype)</span>
    <span class="s1">expected = np.full(n_samples</span><span class="s2">, </span><span class="s1">constant_value</span><span class="s2">, </span><span class="s1">dtype=np.array(constant_value).dtype)</span>

    <span class="s2">for </span><span class="s1">clf </span><span class="s2">in </span><span class="s1">[BayesianRidge()</span><span class="s2">, </span><span class="s1">ARDRegression()]:</span>
        <span class="s1">y_pred = clf.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)</span>
        <span class="s1">assert_array_almost_equal(y_pred</span><span class="s2">, </span><span class="s1">expected)</span>


<span class="s2">def </span><span class="s1">test_std_bayesian_ridge_ard_with_constant_input():</span>
    <span class="s0"># Test BayesianRidge and ARDRegression standard dev. for edge case of</span>
    <span class="s0"># constant target vector</span>
    <span class="s0"># The standard dev. should be relatively small (&lt; 0.01 is tested here)</span>
    <span class="s1">n_samples = </span><span class="s4">10</span>
    <span class="s1">n_features = </span><span class="s4">5</span>
    <span class="s1">random_state = check_random_state(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">constant_value = random_state.rand()</span>
    <span class="s1">X = random_state.random_sample((n_samples</span><span class="s2">, </span><span class="s1">n_features))</span>
    <span class="s1">y = np.full(n_samples</span><span class="s2">, </span><span class="s1">constant_value</span><span class="s2">, </span><span class="s1">dtype=np.array(constant_value).dtype)</span>
    <span class="s1">expected_upper_boundary = </span><span class="s4">0.01</span>

    <span class="s2">for </span><span class="s1">clf </span><span class="s2">in </span><span class="s1">[BayesianRidge()</span><span class="s2">, </span><span class="s1">ARDRegression()]:</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">y_std = clf.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X</span><span class="s2">, </span><span class="s1">return_std=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">assert_array_less(y_std</span><span class="s2">, </span><span class="s1">expected_upper_boundary)</span>


<span class="s2">def </span><span class="s1">test_update_of_sigma_in_ard():</span>
    <span class="s0"># Checks that `sigma_` is updated correctly after the last iteration</span>
    <span class="s0"># of the ARDRegression algorithm. See issue #10128.</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">clf = ARDRegression(max_iter=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s0"># With the inputs above, ARDRegression prunes both of the two coefficients</span>
    <span class="s0"># in the first iteration. Hence, the expected shape of `sigma_` is (0, 0).</span>
    <span class="s2">assert </span><span class="s1">clf.sigma_.shape == (</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s0"># Ensure that no error is thrown at prediction stage</span>
    <span class="s1">clf.predict(X</span><span class="s2">, </span><span class="s1">return_std=</span><span class="s2">True</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_toy_ard_object():</span>
    <span class="s0"># Test BayesianRegression ARD classifier</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s1">]])</span>
    <span class="s1">Y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span>
    <span class="s1">clf = ARDRegression(compute_score=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>

    <span class="s0"># Check that the model could approximately learn the identity function</span>
    <span class="s1">test = [[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">4</span><span class="s1">]]</span>
    <span class="s1">assert_array_almost_equal(clf.predict(test)</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;n_samples, n_features&quot;</span><span class="s2">, </span><span class="s1">((</span><span class="s4">10</span><span class="s2">, </span><span class="s4">100</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s4">100</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)))</span>
<span class="s2">def </span><span class="s1">test_ard_accuracy_on_easy_problem(global_random_seed</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features):</span>
    <span class="s0"># Check that ARD converges with reasonable accuracy on an easy problem</span>
    <span class="s0"># (Github issue #14055)</span>
    <span class="s1">X = np.random.RandomState(global_random_seed).normal(size=(</span><span class="s4">250</span><span class="s2">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">y = X[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>

    <span class="s1">regressor = ARDRegression()</span>
    <span class="s1">regressor.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">abs_coef_error = np.abs(</span><span class="s4">1 </span><span class="s1">- regressor.coef_[</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s2">assert </span><span class="s1">abs_coef_error &lt; </span><span class="s4">1e-10</span>


<span class="s2">def </span><span class="s1">test_return_std():</span>
    <span class="s0"># Test return_std option for both Bayesian regressors</span>
    <span class="s2">def </span><span class="s1">f(X):</span>
        <span class="s2">return </span><span class="s1">np.dot(X</span><span class="s2">, </span><span class="s1">w) + b</span>

    <span class="s2">def </span><span class="s1">f_noise(X</span><span class="s2">, </span><span class="s1">noise_mult):</span>
        <span class="s2">return </span><span class="s1">f(X) + np.random.randn(X.shape[</span><span class="s4">0</span><span class="s1">]) * noise_mult</span>

    <span class="s1">d = </span><span class="s4">5</span>
    <span class="s1">n_train = </span><span class="s4">50</span>
    <span class="s1">n_test = </span><span class="s4">10</span>

    <span class="s1">w = np.array([</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.0</span><span class="s1">])</span>
    <span class="s1">b = </span><span class="s4">1.0</span>

    <span class="s1">X = np.random.random((n_train</span><span class="s2">, </span><span class="s1">d))</span>
    <span class="s1">X_test = np.random.random((n_test</span><span class="s2">, </span><span class="s1">d))</span>

    <span class="s2">for </span><span class="s1">decimal</span><span class="s2">, </span><span class="s1">noise_mult </span><span class="s2">in </span><span class="s1">enumerate([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.01</span><span class="s1">]):</span>
        <span class="s1">y = f_noise(X</span><span class="s2">, </span><span class="s1">noise_mult)</span>

        <span class="s1">m1 = BayesianRidge()</span>
        <span class="s1">m1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">y_mean1</span><span class="s2">, </span><span class="s1">y_std1 = m1.predict(X_test</span><span class="s2">, </span><span class="s1">return_std=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">assert_array_almost_equal(y_std1</span><span class="s2">, </span><span class="s1">noise_mult</span><span class="s2">, </span><span class="s1">decimal=decimal)</span>

        <span class="s1">m2 = ARDRegression()</span>
        <span class="s1">m2.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">y_mean2</span><span class="s2">, </span><span class="s1">y_std2 = m2.predict(X_test</span><span class="s2">, </span><span class="s1">return_std=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">assert_array_almost_equal(y_std2</span><span class="s2">, </span><span class="s1">noise_mult</span><span class="s2">, </span><span class="s1">decimal=decimal)</span>


<span class="s2">def </span><span class="s1">test_update_sigma(global_random_seed):</span>
    <span class="s0"># make sure the two update_sigma() helpers are equivalent. The woodbury</span>
    <span class="s0"># formula is used when n_samples &lt; n_features, and the other one is used</span>
    <span class="s0"># otherwise.</span>

    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>

    <span class="s0"># set n_samples == n_features to avoid instability issues when inverting</span>
    <span class="s0"># the matrices. Using the woodbury formula would be unstable when</span>
    <span class="s0"># n_samples &gt; n_features</span>
    <span class="s1">n_samples = n_features = </span><span class="s4">10</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">alpha = </span><span class="s4">1</span>
    <span class="s1">lmbda = np.arange(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">n_features + </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">keep_lambda = np.array([</span><span class="s2">True</span><span class="s1">] * n_features)</span>

    <span class="s1">reg = ARDRegression()</span>

    <span class="s1">sigma = reg._update_sigma(X</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">lmbda</span><span class="s2">, </span><span class="s1">keep_lambda)</span>
    <span class="s1">sigma_woodbury = reg._update_sigma_woodbury(X</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">lmbda</span><span class="s2">, </span><span class="s1">keep_lambda)</span>

    <span class="s1">np.testing.assert_allclose(sigma</span><span class="s2">, </span><span class="s1">sigma_woodbury)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;dtype&quot;</span><span class="s2">, </span><span class="s1">[np.float32</span><span class="s2">, </span><span class="s1">np.float64])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[BayesianRidge</span><span class="s2">, </span><span class="s1">ARDRegression])</span>
<span class="s2">def </span><span class="s1">test_dtype_match(dtype</span><span class="s2">, </span><span class="s1">Estimator):</span>
    <span class="s0"># Test that np.float32 input data is not cast to np.float64 when possible</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s2">, </span><span class="s4">7</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">4</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">6</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">5</span><span class="s1">]).T</span>

    <span class="s1">model = Estimator()</span>
    <span class="s0"># check type consistency</span>
    <span class="s1">model.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">attributes = [</span><span class="s5">&quot;coef_&quot;</span><span class="s2">, </span><span class="s5">&quot;sigma_&quot;</span><span class="s1">]</span>
    <span class="s2">for </span><span class="s1">attribute </span><span class="s2">in </span><span class="s1">attributes:</span>
        <span class="s2">assert </span><span class="s1">getattr(model</span><span class="s2">, </span><span class="s1">attribute).dtype == X.dtype</span>

    <span class="s1">y_mean</span><span class="s2">, </span><span class="s1">y_std = model.predict(X</span><span class="s2">, </span><span class="s1">return_std=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">y_mean.dtype == X.dtype</span>
    <span class="s2">assert </span><span class="s1">y_std.dtype == X.dtype</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[BayesianRidge</span><span class="s2">, </span><span class="s1">ARDRegression])</span>
<span class="s2">def </span><span class="s1">test_dtype_correctness(Estimator):</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s2">, </span><span class="s4">7</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">4</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">6</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">5</span><span class="s1">]).T</span>
    <span class="s1">model = Estimator()</span>
    <span class="s1">coef_32 = model.fit(X.astype(np.float32)</span><span class="s2">, </span><span class="s1">y).coef_</span>
    <span class="s1">coef_64 = model.fit(X.astype(np.float64)</span><span class="s2">, </span><span class="s1">y).coef_</span>
    <span class="s1">np.testing.assert_allclose(coef_32</span><span class="s2">, </span><span class="s1">coef_64</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-4</span><span class="s1">)</span>


<span class="s0"># TODO(1.5) remove</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[BayesianRidge</span><span class="s2">, </span><span class="s1">ARDRegression])</span>
<span class="s2">def </span><span class="s1">test_bayesian_ridge_ard_n_iter_deprecated(Estimator):</span>
    <span class="s3">&quot;&quot;&quot;Check the deprecation warning of `n_iter`.&quot;&quot;&quot;</span>
    <span class="s1">depr_msg = (</span>
        <span class="s5">&quot;'n_iter' was renamed to 'max_iter' in version 1.3 and will be removed in 1.5&quot;</span>
    <span class="s1">)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target</span>
    <span class="s1">model = Estimator(n_iter=</span><span class="s4">5</span><span class="s1">)</span>

    <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s2">, </span><span class="s1">match=depr_msg):</span>
        <span class="s1">model.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s0"># TODO(1.5) remove</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[BayesianRidge</span><span class="s2">, </span><span class="s1">ARDRegression])</span>
<span class="s2">def </span><span class="s1">test_bayesian_ridge_ard_max_iter_and_n_iter_both_set(Estimator):</span>
    <span class="s3">&quot;&quot;&quot;Check that a ValueError is raised when both `max_iter` and `n_iter` are set.&quot;&quot;&quot;</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">&quot;Both `n_iter` and `max_iter` attributes were set. Attribute&quot;</span>
        <span class="s5">&quot; `n_iter` was deprecated in version 1.3 and will be removed in&quot;</span>
        <span class="s5">&quot; 1.5. To avoid this error, only set the `max_iter` attribute.&quot;</span>
    <span class="s1">)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target</span>
    <span class="s1">model = Estimator(n_iter=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">5</span><span class="s1">)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">model.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
</pre>
</body>
</html>