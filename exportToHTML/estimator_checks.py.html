<html>
<head>
<title>estimator_checks.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #629755; font-style: italic;}
.s6 { color: #a5c261;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
estimator_checks.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">importlib</span>
<span class="s0">import </span><span class="s1">itertools</span>
<span class="s0">import </span><span class="s1">pickle</span>
<span class="s0">import </span><span class="s1">re</span>
<span class="s0">import </span><span class="s1">warnings</span>
<span class="s0">from </span><span class="s1">copy </span><span class="s0">import </span><span class="s1">deepcopy</span>
<span class="s0">from </span><span class="s1">functools </span><span class="s0">import </span><span class="s1">partial</span><span class="s0">, </span><span class="s1">wraps</span>
<span class="s0">from </span><span class="s1">inspect </span><span class="s0">import </span><span class="s1">signature</span>
<span class="s0">from </span><span class="s1">numbers </span><span class="s0">import </span><span class="s1">Integral</span><span class="s0">, </span><span class="s1">Real</span>

<span class="s0">import </span><span class="s1">joblib</span>
<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">from </span><span class="s1">scipy </span><span class="s0">import </span><span class="s1">sparse</span>
<span class="s0">from </span><span class="s1">scipy.stats </span><span class="s0">import </span><span class="s1">rankdata</span>

<span class="s0">from </span><span class="s1">.. </span><span class="s0">import </span><span class="s1">config_context</span>
<span class="s0">from </span><span class="s1">..base </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">ClusterMixin</span><span class="s0">,</span>
    <span class="s1">RegressorMixin</span><span class="s0">,</span>
    <span class="s1">clone</span><span class="s0">,</span>
    <span class="s1">is_classifier</span><span class="s0">,</span>
    <span class="s1">is_outlier_detector</span><span class="s0">,</span>
    <span class="s1">is_regressor</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">..datasets </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">load_iris</span><span class="s0">,</span>
    <span class="s1">make_blobs</span><span class="s0">,</span>
    <span class="s1">make_classification</span><span class="s0">,</span>
    <span class="s1">make_multilabel_classification</span><span class="s0">,</span>
    <span class="s1">make_regression</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">..exceptions </span><span class="s0">import </span><span class="s1">DataConversionWarning</span><span class="s0">, </span><span class="s1">NotFittedError</span><span class="s0">, </span><span class="s1">SkipTestWarning</span>
<span class="s0">from </span><span class="s1">..feature_selection </span><span class="s0">import </span><span class="s1">SelectFromModel</span><span class="s0">, </span><span class="s1">SelectKBest</span>
<span class="s0">from </span><span class="s1">..linear_model </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">LinearRegression</span><span class="s0">,</span>
    <span class="s1">LogisticRegression</span><span class="s0">,</span>
    <span class="s1">RANSACRegressor</span><span class="s0">,</span>
    <span class="s1">Ridge</span><span class="s0">,</span>
    <span class="s1">SGDRegressor</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">..metrics </span><span class="s0">import </span><span class="s1">accuracy_score</span><span class="s0">, </span><span class="s1">adjusted_rand_score</span><span class="s0">, </span><span class="s1">f1_score</span>
<span class="s0">from </span><span class="s1">..metrics.pairwise </span><span class="s0">import </span><span class="s1">linear_kernel</span><span class="s0">, </span><span class="s1">pairwise_distances</span><span class="s0">, </span><span class="s1">rbf_kernel</span>
<span class="s0">from </span><span class="s1">..model_selection </span><span class="s0">import </span><span class="s1">ShuffleSplit</span><span class="s0">, </span><span class="s1">train_test_split</span>
<span class="s0">from </span><span class="s1">..model_selection._validation </span><span class="s0">import </span><span class="s1">_safe_split</span>
<span class="s0">from </span><span class="s1">..pipeline </span><span class="s0">import </span><span class="s1">make_pipeline</span>
<span class="s0">from </span><span class="s1">..preprocessing </span><span class="s0">import </span><span class="s1">StandardScaler</span><span class="s0">, </span><span class="s1">scale</span>
<span class="s0">from </span><span class="s1">..random_projection </span><span class="s0">import </span><span class="s1">BaseRandomProjection</span>
<span class="s0">from </span><span class="s1">..utils._array_api </span><span class="s0">import </span><span class="s1">_convert_to_numpy</span><span class="s0">, </span><span class="s1">get_namespace</span>
<span class="s0">from </span><span class="s1">..utils._array_api </span><span class="s0">import </span><span class="s1">device </span><span class="s0">as </span><span class="s1">array_device</span>
<span class="s0">from </span><span class="s1">..utils._param_validation </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">InvalidParameterError</span><span class="s0">,</span>
    <span class="s1">generate_invalid_param_val</span><span class="s0">,</span>
    <span class="s1">make_constraint</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">..utils.fixes </span><span class="s0">import </span><span class="s1">parse_version</span><span class="s0">, </span><span class="s1">sp_version</span>
<span class="s0">from </span><span class="s1">..utils.validation </span><span class="s0">import </span><span class="s1">check_is_fitted</span>
<span class="s0">from </span><span class="s1">. </span><span class="s0">import </span><span class="s1">IS_PYPY</span><span class="s0">, </span><span class="s1">is_scalar_nan</span><span class="s0">, </span><span class="s1">shuffle</span>
<span class="s0">from </span><span class="s1">._param_validation </span><span class="s0">import </span><span class="s1">Interval</span>
<span class="s0">from </span><span class="s1">._tags </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">_DEFAULT_TAGS</span><span class="s0">,</span>
    <span class="s1">_safe_tags</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">._testing </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">SkipTest</span><span class="s0">,</span>
    <span class="s1">_get_args</span><span class="s0">,</span>
    <span class="s1">assert_allclose</span><span class="s0">,</span>
    <span class="s1">assert_allclose_dense_sparse</span><span class="s0">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_less</span><span class="s0">,</span>
    <span class="s1">assert_raise_message</span><span class="s0">,</span>
    <span class="s1">create_memmap_backed_data</span><span class="s0">,</span>
    <span class="s1">ignore_warnings</span><span class="s0">,</span>
    <span class="s1">raises</span><span class="s0">,</span>
    <span class="s1">set_random_state</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">.validation </span><span class="s0">import </span><span class="s1">_num_samples</span><span class="s0">, </span><span class="s1">has_fit_parameter</span>

<span class="s1">REGRESSION_DATASET = </span><span class="s0">None</span>
<span class="s1">CROSS_DECOMPOSITION = [</span><span class="s2">&quot;PLSCanonical&quot;</span><span class="s0">, </span><span class="s2">&quot;PLSRegression&quot;</span><span class="s0">, </span><span class="s2">&quot;CCA&quot;</span><span class="s0">, </span><span class="s2">&quot;PLSSVD&quot;</span><span class="s1">]</span>


<span class="s0">def </span><span class="s1">_yield_checks(estimator):</span>
    <span class="s1">name = estimator.__class__.__name__</span>
    <span class="s1">tags = _safe_tags(estimator)</span>

    <span class="s0">yield </span><span class="s1">check_no_attributes_set_in_init</span>
    <span class="s0">yield </span><span class="s1">check_estimators_dtypes</span>
    <span class="s0">yield </span><span class="s1">check_fit_score_takes_y</span>
    <span class="s0">if </span><span class="s1">has_fit_parameter(estimator</span><span class="s0">, </span><span class="s2">&quot;sample_weight&quot;</span><span class="s1">):</span>
        <span class="s0">yield </span><span class="s1">check_sample_weights_pandas_series</span>
        <span class="s0">yield </span><span class="s1">check_sample_weights_not_an_array</span>
        <span class="s0">yield </span><span class="s1">check_sample_weights_list</span>
        <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;pairwise&quot;</span><span class="s1">]:</span>
            <span class="s3"># We skip pairwise because the data is not pairwise</span>
            <span class="s0">yield </span><span class="s1">check_sample_weights_shape</span>
            <span class="s0">yield </span><span class="s1">check_sample_weights_not_overwritten</span>
            <span class="s0">yield </span><span class="s1">partial(check_sample_weights_invariance</span><span class="s0">, </span><span class="s1">kind=</span><span class="s2">&quot;ones&quot;</span><span class="s1">)</span>
            <span class="s0">yield </span><span class="s1">partial(check_sample_weights_invariance</span><span class="s0">, </span><span class="s1">kind=</span><span class="s2">&quot;zeros&quot;</span><span class="s1">)</span>
    <span class="s0">yield </span><span class="s1">check_estimators_fit_returns_self</span>
    <span class="s0">yield </span><span class="s1">partial(check_estimators_fit_returns_self</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">True</span><span class="s1">)</span>

    <span class="s3"># Check that all estimator yield informative messages when</span>
    <span class="s3"># trained on empty datasets</span>
    <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
        <span class="s0">yield </span><span class="s1">check_complex_data</span>
        <span class="s0">yield </span><span class="s1">check_dtype_object</span>
        <span class="s0">yield </span><span class="s1">check_estimators_empty_data_messages</span>

    <span class="s0">if </span><span class="s1">name </span><span class="s0">not in </span><span class="s1">CROSS_DECOMPOSITION:</span>
        <span class="s3"># cross-decomposition's &quot;transform&quot; returns X and Y</span>
        <span class="s0">yield </span><span class="s1">check_pipeline_consistency</span>

    <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;allow_nan&quot;</span><span class="s1">] </span><span class="s0">and not </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
        <span class="s3"># Test that all estimators check their input for NaN's and infs</span>
        <span class="s0">yield </span><span class="s1">check_estimators_nan_inf</span>

    <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;pairwise&quot;</span><span class="s1">]:</span>
        <span class="s3"># Check that pairwise estimator throws error on non-square input</span>
        <span class="s0">yield </span><span class="s1">check_nonsquare_error</span>

    <span class="s0">yield </span><span class="s1">check_estimators_overwrite_params</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;sparsify&quot;</span><span class="s1">):</span>
        <span class="s0">yield </span><span class="s1">check_sparsify_coefficients</span>

    <span class="s0">yield </span><span class="s1">check_estimator_sparse_data</span>

    <span class="s3"># Test that estimators can be pickled, and once pickled</span>
    <span class="s3"># give the same answer as before.</span>
    <span class="s0">yield </span><span class="s1">check_estimators_pickle</span>
    <span class="s0">yield </span><span class="s1">partial(check_estimators_pickle</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">True</span><span class="s1">)</span>

    <span class="s0">yield </span><span class="s1">check_estimator_get_tags_default_keys</span>

    <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;array_api_support&quot;</span><span class="s1">]:</span>
        <span class="s0">for </span><span class="s1">array_namespace </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;numpy.array_api&quot;</span><span class="s0">, </span><span class="s2">&quot;cupy.array_api&quot;</span><span class="s0">, </span><span class="s2">&quot;cupy&quot;</span><span class="s0">, </span><span class="s2">&quot;torch&quot;</span><span class="s1">]:</span>
            <span class="s0">if </span><span class="s1">array_namespace == </span><span class="s2">&quot;torch&quot;</span><span class="s1">:</span>
                <span class="s0">for </span><span class="s1">device</span><span class="s0">, </span><span class="s1">dtype </span><span class="s0">in </span><span class="s1">itertools.product(</span>
                    <span class="s1">(</span><span class="s2">&quot;cpu&quot;</span><span class="s0">, </span><span class="s2">&quot;cuda&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s2">&quot;float64&quot;</span><span class="s0">, </span><span class="s2">&quot;float32&quot;</span><span class="s1">)</span>
                <span class="s1">):</span>
                    <span class="s0">yield </span><span class="s1">partial(</span>
                        <span class="s1">check_array_api_input</span><span class="s0">,</span>
                        <span class="s1">array_namespace=array_namespace</span><span class="s0">,</span>
                        <span class="s1">dtype=dtype</span><span class="s0">,</span>
                        <span class="s1">device=device</span><span class="s0">,</span>
                    <span class="s1">)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">yield </span><span class="s1">partial(check_array_api_input</span><span class="s0">, </span><span class="s1">array_namespace=array_namespace)</span>


<span class="s0">def </span><span class="s1">_yield_classifier_checks(classifier):</span>
    <span class="s1">tags = _safe_tags(classifier)</span>

    <span class="s3"># test classifiers can handle non-array data and pandas objects</span>
    <span class="s0">yield </span><span class="s1">check_classifier_data_not_an_array</span>
    <span class="s3"># test classifiers trained on a single label always return this label</span>
    <span class="s0">yield </span><span class="s1">check_classifiers_one_label</span>
    <span class="s0">yield </span><span class="s1">check_classifiers_one_label_sample_weights</span>
    <span class="s0">yield </span><span class="s1">check_classifiers_classes</span>
    <span class="s0">yield </span><span class="s1">check_estimators_partial_fit_n_features</span>
    <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;multioutput&quot;</span><span class="s1">]:</span>
        <span class="s0">yield </span><span class="s1">check_classifier_multioutput</span>
    <span class="s3"># basic consistency testing</span>
    <span class="s0">yield </span><span class="s1">check_classifiers_train</span>
    <span class="s0">yield </span><span class="s1">partial(check_classifiers_train</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s0">yield </span><span class="s1">partial(check_classifiers_train</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">True, </span><span class="s1">X_dtype=</span><span class="s2">&quot;float32&quot;</span><span class="s1">)</span>
    <span class="s0">yield </span><span class="s1">check_classifiers_regression_target</span>
    <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;multilabel&quot;</span><span class="s1">]:</span>
        <span class="s0">yield </span><span class="s1">check_classifiers_multilabel_representation_invariance</span>
        <span class="s0">yield </span><span class="s1">check_classifiers_multilabel_output_format_predict</span>
        <span class="s0">yield </span><span class="s1">check_classifiers_multilabel_output_format_predict_proba</span>
        <span class="s0">yield </span><span class="s1">check_classifiers_multilabel_output_format_decision_function</span>
    <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
        <span class="s0">yield </span><span class="s1">check_supervised_y_no_nan</span>
        <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;multioutput_only&quot;</span><span class="s1">]:</span>
            <span class="s0">yield </span><span class="s1">check_supervised_y_2d</span>
    <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;requires_fit&quot;</span><span class="s1">]:</span>
        <span class="s0">yield </span><span class="s1">check_estimators_unfitted</span>
    <span class="s0">if </span><span class="s2">&quot;class_weight&quot; </span><span class="s0">in </span><span class="s1">classifier.get_params().keys():</span>
        <span class="s0">yield </span><span class="s1">check_class_weight_classifiers</span>

    <span class="s0">yield </span><span class="s1">check_non_transformer_estimators_n_iter</span>
    <span class="s3"># test if predict_proba is a monotonic transformation of decision_function</span>
    <span class="s0">yield </span><span class="s1">check_decision_proba_consistency</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_supervised_y_no_nan(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Checks that the Estimator targets are not NaN.</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">888</span><span class="s1">)</span>
    <span class="s1">X = rng.standard_normal(size=(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>

    <span class="s0">for </span><span class="s1">value </span><span class="s0">in </span><span class="s1">[np.nan</span><span class="s0">, </span><span class="s1">np.inf]:</span>
        <span class="s1">y = np.full(</span><span class="s4">10</span><span class="s0">, </span><span class="s1">value)</span>
        <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

        <span class="s1">module_name = estimator.__module__</span>
        <span class="s0">if </span><span class="s1">module_name.startswith(</span><span class="s2">&quot;sklearn.&quot;</span><span class="s1">) </span><span class="s0">and not </span><span class="s1">(</span>
            <span class="s2">&quot;test_&quot; </span><span class="s0">in </span><span class="s1">module_name </span><span class="s0">or </span><span class="s1">module_name.endswith(</span><span class="s2">&quot;_testing&quot;</span><span class="s1">)</span>
        <span class="s1">):</span>
            <span class="s3"># In scikit-learn we want the error message to mention the input</span>
            <span class="s3"># name and be specific about the kind of unexpected value.</span>
            <span class="s0">if </span><span class="s1">np.isinf(value):</span>
                <span class="s1">match = (</span>
                    <span class="s2">r&quot;Input (y|Y) contains infinity or a value too large for&quot;</span>
                    <span class="s2">r&quot; dtype\('float64'\).&quot;</span>
                <span class="s1">)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">match = </span><span class="s2">r&quot;Input (y|Y) contains NaN.&quot;</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s3"># Do not impose a particular error message to third-party libraries.</span>
            <span class="s1">match = </span><span class="s0">None</span>
        <span class="s1">err_msg = (</span>
            <span class="s2">f&quot;Estimator </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">should have raised error on fitting array y with inf&quot;</span>
            <span class="s2">&quot; value.&quot;</span>
        <span class="s1">)</span>
        <span class="s0">with </span><span class="s1">raises(ValueError</span><span class="s0">, </span><span class="s1">match=match</span><span class="s0">, </span><span class="s1">err_msg=err_msg):</span>
            <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">_yield_regressor_checks(regressor):</span>
    <span class="s1">tags = _safe_tags(regressor)</span>
    <span class="s3"># TODO: test with intercept</span>
    <span class="s3"># TODO: test with multiple responses</span>
    <span class="s3"># basic testing</span>
    <span class="s0">yield </span><span class="s1">check_regressors_train</span>
    <span class="s0">yield </span><span class="s1">partial(check_regressors_train</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s0">yield </span><span class="s1">partial(check_regressors_train</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">True, </span><span class="s1">X_dtype=</span><span class="s2">&quot;float32&quot;</span><span class="s1">)</span>
    <span class="s0">yield </span><span class="s1">check_regressor_data_not_an_array</span>
    <span class="s0">yield </span><span class="s1">check_estimators_partial_fit_n_features</span>
    <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;multioutput&quot;</span><span class="s1">]:</span>
        <span class="s0">yield </span><span class="s1">check_regressor_multioutput</span>
    <span class="s0">yield </span><span class="s1">check_regressors_no_decision_function</span>
    <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">] </span><span class="s0">and not </span><span class="s1">tags[</span><span class="s2">&quot;multioutput_only&quot;</span><span class="s1">]:</span>
        <span class="s0">yield </span><span class="s1">check_supervised_y_2d</span>
    <span class="s0">yield </span><span class="s1">check_supervised_y_no_nan</span>
    <span class="s1">name = regressor.__class__.__name__</span>
    <span class="s0">if </span><span class="s1">name != </span><span class="s2">&quot;CCA&quot;</span><span class="s1">:</span>
        <span class="s3"># check that the regressor handles int input</span>
        <span class="s0">yield </span><span class="s1">check_regressors_int</span>
    <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;requires_fit&quot;</span><span class="s1">]:</span>
        <span class="s0">yield </span><span class="s1">check_estimators_unfitted</span>
    <span class="s0">yield </span><span class="s1">check_non_transformer_estimators_n_iter</span>


<span class="s0">def </span><span class="s1">_yield_transformer_checks(transformer):</span>
    <span class="s1">tags = _safe_tags(transformer)</span>
    <span class="s3"># All transformers should either deal with sparse data or raise an</span>
    <span class="s3"># exception with type TypeError and an intelligible error message</span>
    <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
        <span class="s0">yield </span><span class="s1">check_transformer_data_not_an_array</span>
    <span class="s3"># these don't actually fit the data, so don't raise errors</span>
    <span class="s0">yield </span><span class="s1">check_transformer_general</span>
    <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;preserves_dtype&quot;</span><span class="s1">]:</span>
        <span class="s0">yield </span><span class="s1">check_transformer_preserve_dtypes</span>
    <span class="s0">yield </span><span class="s1">partial(check_transformer_general</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s0">if not </span><span class="s1">_safe_tags(transformer</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;stateless&quot;</span><span class="s1">):</span>
        <span class="s0">yield </span><span class="s1">check_transformers_unfitted</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">yield </span><span class="s1">check_transformers_unfitted_stateless</span>
    <span class="s3"># Dependent on external solvers and hence accessing the iter</span>
    <span class="s3"># param is non-trivial.</span>
    <span class="s1">external_solver = [</span>
        <span class="s2">&quot;Isomap&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;KernelPCA&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;LocallyLinearEmbedding&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;RandomizedLasso&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;LogisticRegressionCV&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;BisectingKMeans&quot;</span><span class="s0">,</span>
    <span class="s1">]</span>

    <span class="s1">name = transformer.__class__.__name__</span>
    <span class="s0">if </span><span class="s1">name </span><span class="s0">not in </span><span class="s1">external_solver:</span>
        <span class="s0">yield </span><span class="s1">check_transformer_n_iter</span>


<span class="s0">def </span><span class="s1">_yield_clustering_checks(clusterer):</span>
    <span class="s0">yield </span><span class="s1">check_clusterer_compute_labels_predict</span>
    <span class="s1">name = clusterer.__class__.__name__</span>
    <span class="s0">if </span><span class="s1">name </span><span class="s0">not in </span><span class="s1">(</span><span class="s2">&quot;WardAgglomeration&quot;</span><span class="s0">, </span><span class="s2">&quot;FeatureAgglomeration&quot;</span><span class="s1">):</span>
        <span class="s3"># this is clustering on the features</span>
        <span class="s3"># let's not test that here.</span>
        <span class="s0">yield </span><span class="s1">check_clustering</span>
        <span class="s0">yield </span><span class="s1">partial(check_clustering</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s0">yield </span><span class="s1">check_estimators_partial_fit_n_features</span>
    <span class="s0">if not </span><span class="s1">hasattr(clusterer</span><span class="s0">, </span><span class="s2">&quot;transform&quot;</span><span class="s1">):</span>
        <span class="s0">yield </span><span class="s1">check_non_transformer_estimators_n_iter</span>


<span class="s0">def </span><span class="s1">_yield_outliers_checks(estimator):</span>
    <span class="s3"># checks for the contamination parameter</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;contamination&quot;</span><span class="s1">):</span>
        <span class="s0">yield </span><span class="s1">check_outlier_contamination</span>

    <span class="s3"># checks for outlier detectors that have a fit_predict method</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;fit_predict&quot;</span><span class="s1">):</span>
        <span class="s0">yield </span><span class="s1">check_outliers_fit_predict</span>

    <span class="s3"># checks for estimators that can be used on a test set</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;predict&quot;</span><span class="s1">):</span>
        <span class="s0">yield </span><span class="s1">check_outliers_train</span>
        <span class="s0">yield </span><span class="s1">partial(check_outliers_train</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s3"># test outlier detectors can handle non-array data</span>
        <span class="s0">yield </span><span class="s1">check_classifier_data_not_an_array</span>
        <span class="s3"># test if NotFittedError is raised</span>
        <span class="s0">if </span><span class="s1">_safe_tags(estimator</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;requires_fit&quot;</span><span class="s1">):</span>
            <span class="s0">yield </span><span class="s1">check_estimators_unfitted</span>
    <span class="s0">yield </span><span class="s1">check_non_transformer_estimators_n_iter</span>


<span class="s0">def </span><span class="s1">_yield_all_checks(estimator):</span>
    <span class="s1">name = estimator.__class__.__name__</span>
    <span class="s1">tags = _safe_tags(estimator)</span>
    <span class="s0">if </span><span class="s2">&quot;2darray&quot; </span><span class="s0">not in </span><span class="s1">tags[</span><span class="s2">&quot;X_types&quot;</span><span class="s1">]:</span>
        <span class="s1">warnings.warn(</span>
            <span class="s2">&quot;Can't test estimator {} which requires input  of type {}&quot;</span><span class="s1">.format(</span>
                <span class="s1">name</span><span class="s0">, </span><span class="s1">tags[</span><span class="s2">&quot;X_types&quot;</span><span class="s1">]</span>
            <span class="s1">)</span><span class="s0">,</span>
            <span class="s1">SkipTestWarning</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s0">return</span>
    <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;_skip_test&quot;</span><span class="s1">]:</span>
        <span class="s1">warnings.warn(</span>
            <span class="s2">&quot;Explicit SKIP via _skip_test tag for estimator {}.&quot;</span><span class="s1">.format(name)</span><span class="s0">,</span>
            <span class="s1">SkipTestWarning</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s0">return</span>

    <span class="s0">for </span><span class="s1">check </span><span class="s0">in </span><span class="s1">_yield_checks(estimator):</span>
        <span class="s0">yield </span><span class="s1">check</span>
    <span class="s0">if </span><span class="s1">is_classifier(estimator):</span>
        <span class="s0">for </span><span class="s1">check </span><span class="s0">in </span><span class="s1">_yield_classifier_checks(estimator):</span>
            <span class="s0">yield </span><span class="s1">check</span>
    <span class="s0">if </span><span class="s1">is_regressor(estimator):</span>
        <span class="s0">for </span><span class="s1">check </span><span class="s0">in </span><span class="s1">_yield_regressor_checks(estimator):</span>
            <span class="s0">yield </span><span class="s1">check</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;transform&quot;</span><span class="s1">):</span>
        <span class="s0">for </span><span class="s1">check </span><span class="s0">in </span><span class="s1">_yield_transformer_checks(estimator):</span>
            <span class="s0">yield </span><span class="s1">check</span>
    <span class="s0">if </span><span class="s1">isinstance(estimator</span><span class="s0">, </span><span class="s1">ClusterMixin):</span>
        <span class="s0">for </span><span class="s1">check </span><span class="s0">in </span><span class="s1">_yield_clustering_checks(estimator):</span>
            <span class="s0">yield </span><span class="s1">check</span>
    <span class="s0">if </span><span class="s1">is_outlier_detector(estimator):</span>
        <span class="s0">for </span><span class="s1">check </span><span class="s0">in </span><span class="s1">_yield_outliers_checks(estimator):</span>
            <span class="s0">yield </span><span class="s1">check</span>
    <span class="s0">yield </span><span class="s1">check_parameters_default_constructible</span>
    <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;non_deterministic&quot;</span><span class="s1">]:</span>
        <span class="s0">yield </span><span class="s1">check_methods_sample_order_invariance</span>
        <span class="s0">yield </span><span class="s1">check_methods_subset_invariance</span>
    <span class="s0">yield </span><span class="s1">check_fit2d_1sample</span>
    <span class="s0">yield </span><span class="s1">check_fit2d_1feature</span>
    <span class="s0">yield </span><span class="s1">check_get_params_invariance</span>
    <span class="s0">yield </span><span class="s1">check_set_params</span>
    <span class="s0">yield </span><span class="s1">check_dict_unchanged</span>
    <span class="s0">yield </span><span class="s1">check_dont_overwrite_parameters</span>
    <span class="s0">yield </span><span class="s1">check_fit_idempotent</span>
    <span class="s0">yield </span><span class="s1">check_fit_check_is_fitted</span>
    <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
        <span class="s0">yield </span><span class="s1">check_n_features_in</span>
        <span class="s0">yield </span><span class="s1">check_fit1d</span>
        <span class="s0">yield </span><span class="s1">check_fit2d_predict1d</span>
        <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;requires_y&quot;</span><span class="s1">]:</span>
            <span class="s0">yield </span><span class="s1">check_requires_y_none</span>
    <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;requires_positive_X&quot;</span><span class="s1">]:</span>
        <span class="s0">yield </span><span class="s1">check_fit_non_negative</span>


<span class="s0">def </span><span class="s1">_get_check_estimator_ids(obj):</span>
    <span class="s5">&quot;&quot;&quot;Create pytest ids for checks. 
 
    When `obj` is an estimator, this returns the pprint version of the 
    estimator (with `print_changed_only=True`). When `obj` is a function, the 
    name of the function is returned with its keyword arguments. 
 
    `_get_check_estimator_ids` is designed to be used as the `id` in 
    `pytest.mark.parametrize` where `check_estimator(..., generate_only=True)` 
    is yielding estimators and checks. 
 
    Parameters 
    ---------- 
    obj : estimator or function 
        Items generated by `check_estimator`. 
 
    Returns 
    ------- 
    id : str or None 
 
    See Also 
    -------- 
    check_estimator 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">callable(obj):</span>
        <span class="s0">if not </span><span class="s1">isinstance(obj</span><span class="s0">, </span><span class="s1">partial):</span>
            <span class="s0">return </span><span class="s1">obj.__name__</span>

        <span class="s0">if not </span><span class="s1">obj.keywords:</span>
            <span class="s0">return </span><span class="s1">obj.func.__name__</span>

        <span class="s1">kwstring = </span><span class="s2">&quot;,&quot;</span><span class="s1">.join([</span><span class="s2">&quot;{}={}&quot;</span><span class="s1">.format(k</span><span class="s0">, </span><span class="s1">v) </span><span class="s0">for </span><span class="s1">k</span><span class="s0">, </span><span class="s1">v </span><span class="s0">in </span><span class="s1">obj.keywords.items()])</span>
        <span class="s0">return </span><span class="s2">&quot;{}({})&quot;</span><span class="s1">.format(obj.func.__name__</span><span class="s0">, </span><span class="s1">kwstring)</span>
    <span class="s0">if </span><span class="s1">hasattr(obj</span><span class="s0">, </span><span class="s2">&quot;get_params&quot;</span><span class="s1">):</span>
        <span class="s0">with </span><span class="s1">config_context(print_changed_only=</span><span class="s0">True</span><span class="s1">):</span>
            <span class="s0">return </span><span class="s1">re.sub(</span><span class="s2">r&quot;\s&quot;</span><span class="s0">, </span><span class="s2">&quot;&quot;</span><span class="s0">, </span><span class="s1">str(obj))</span>


<span class="s0">def </span><span class="s1">_construct_instance(Estimator):</span>
    <span class="s5">&quot;&quot;&quot;Construct Estimator instance if possible.&quot;&quot;&quot;</span>
    <span class="s1">required_parameters = getattr(Estimator</span><span class="s0">, </span><span class="s2">&quot;_required_parameters&quot;</span><span class="s0">, </span><span class="s1">[])</span>
    <span class="s0">if </span><span class="s1">len(required_parameters):</span>
        <span class="s0">if </span><span class="s1">required_parameters </span><span class="s0">in </span><span class="s1">([</span><span class="s2">&quot;estimator&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;base_estimator&quot;</span><span class="s1">]):</span>
            <span class="s3"># `RANSACRegressor` will raise an error with any model other</span>
            <span class="s3"># than `LinearRegression` if we don't fix `min_samples` parameter.</span>
            <span class="s3"># For common test, we can enforce using `LinearRegression` that</span>
            <span class="s3"># is the default estimator in `RANSACRegressor` instead of `Ridge`.</span>
            <span class="s0">if </span><span class="s1">issubclass(Estimator</span><span class="s0">, </span><span class="s1">RANSACRegressor):</span>
                <span class="s1">estimator = Estimator(LinearRegression())</span>
            <span class="s0">elif </span><span class="s1">issubclass(Estimator</span><span class="s0">, </span><span class="s1">RegressorMixin):</span>
                <span class="s1">estimator = Estimator(Ridge())</span>
            <span class="s0">elif </span><span class="s1">issubclass(Estimator</span><span class="s0">, </span><span class="s1">SelectFromModel):</span>
                <span class="s3"># Increases coverage because SGDRegressor has partial_fit</span>
                <span class="s1">estimator = Estimator(SGDRegressor(random_state=</span><span class="s4">0</span><span class="s1">))</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">estimator = Estimator(LogisticRegression(C=</span><span class="s4">1</span><span class="s1">))</span>
        <span class="s0">elif </span><span class="s1">required_parameters </span><span class="s0">in </span><span class="s1">([</span><span class="s2">&quot;estimators&quot;</span><span class="s1">]</span><span class="s0">,</span><span class="s1">):</span>
            <span class="s3"># Heterogeneous ensemble classes (i.e. stacking, voting)</span>
            <span class="s0">if </span><span class="s1">issubclass(Estimator</span><span class="s0">, </span><span class="s1">RegressorMixin):</span>
                <span class="s1">estimator = Estimator(</span>
                    <span class="s1">estimators=[(</span><span class="s2">&quot;est1&quot;</span><span class="s0">, </span><span class="s1">Ridge(alpha=</span><span class="s4">0.1</span><span class="s1">))</span><span class="s0">, </span><span class="s1">(</span><span class="s2">&quot;est2&quot;</span><span class="s0">, </span><span class="s1">Ridge(alpha=</span><span class="s4">1</span><span class="s1">))]</span>
                <span class="s1">)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">estimator = Estimator(</span>
                    <span class="s1">estimators=[</span>
                        <span class="s1">(</span><span class="s2">&quot;est1&quot;</span><span class="s0">, </span><span class="s1">LogisticRegression(C=</span><span class="s4">0.1</span><span class="s1">))</span><span class="s0">,</span>
                        <span class="s1">(</span><span class="s2">&quot;est2&quot;</span><span class="s0">, </span><span class="s1">LogisticRegression(C=</span><span class="s4">1</span><span class="s1">))</span><span class="s0">,</span>
                    <span class="s1">]</span>
                <span class="s1">)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">msg = (</span>
                <span class="s2">f&quot;Can't instantiate estimator </span><span class="s0">{</span><span class="s1">Estimator.__name__</span><span class="s0">} </span><span class="s2">&quot;</span>
                <span class="s2">f&quot;parameters </span><span class="s0">{</span><span class="s1">required_parameters</span><span class="s0">}</span><span class="s2">&quot;</span>
            <span class="s1">)</span>
            <span class="s3"># raise additional warning to be shown by pytest</span>
            <span class="s1">warnings.warn(msg</span><span class="s0">, </span><span class="s1">SkipTestWarning)</span>
            <span class="s0">raise </span><span class="s1">SkipTest(msg)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">estimator = Estimator()</span>
    <span class="s0">return </span><span class="s1">estimator</span>


<span class="s0">def </span><span class="s1">_maybe_mark_xfail(estimator</span><span class="s0">, </span><span class="s1">check</span><span class="s0">, </span><span class="s1">pytest):</span>
    <span class="s3"># Mark (estimator, check) pairs as XFAIL if needed (see conditions in</span>
    <span class="s3"># _should_be_skipped_or_marked())</span>
    <span class="s3"># This is similar to _maybe_skip(), but this one is used by</span>
    <span class="s3"># @parametrize_with_checks() instead of check_estimator()</span>

    <span class="s1">should_be_marked</span><span class="s0">, </span><span class="s1">reason = _should_be_skipped_or_marked(estimator</span><span class="s0">, </span><span class="s1">check)</span>
    <span class="s0">if not </span><span class="s1">should_be_marked:</span>
        <span class="s0">return </span><span class="s1">estimator</span><span class="s0">, </span><span class="s1">check</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">pytest.param(estimator</span><span class="s0">, </span><span class="s1">check</span><span class="s0">, </span><span class="s1">marks=pytest.mark.xfail(reason=reason))</span>


<span class="s0">def </span><span class="s1">_maybe_skip(estimator</span><span class="s0">, </span><span class="s1">check):</span>
    <span class="s3"># Wrap a check so that it's skipped if needed (see conditions in</span>
    <span class="s3"># _should_be_skipped_or_marked())</span>
    <span class="s3"># This is similar to _maybe_mark_xfail(), but this one is used by</span>
    <span class="s3"># check_estimator() instead of @parametrize_with_checks which requires</span>
    <span class="s3"># pytest</span>
    <span class="s1">should_be_skipped</span><span class="s0">, </span><span class="s1">reason = _should_be_skipped_or_marked(estimator</span><span class="s0">, </span><span class="s1">check)</span>
    <span class="s0">if not </span><span class="s1">should_be_skipped:</span>
        <span class="s0">return </span><span class="s1">check</span>

    <span class="s1">check_name = check.func.__name__ </span><span class="s0">if </span><span class="s1">isinstance(check</span><span class="s0">, </span><span class="s1">partial) </span><span class="s0">else </span><span class="s1">check.__name__</span>

    <span class="s1">@wraps(check)</span>
    <span class="s0">def </span><span class="s1">wrapped(*args</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s0">raise </span><span class="s1">SkipTest(</span>
            <span class="s2">f&quot;Skipping </span><span class="s0">{</span><span class="s1">check_name</span><span class="s0">} </span><span class="s2">for </span><span class="s0">{</span><span class="s1">estimator.__class__.__name__</span><span class="s0">}</span><span class="s2">: </span><span class="s0">{</span><span class="s1">reason</span><span class="s0">}</span><span class="s2">&quot;</span>
        <span class="s1">)</span>

    <span class="s0">return </span><span class="s1">wrapped</span>


<span class="s0">def </span><span class="s1">_should_be_skipped_or_marked(estimator</span><span class="s0">, </span><span class="s1">check):</span>
    <span class="s3"># Return whether a check should be skipped (when using check_estimator())</span>
    <span class="s3"># or marked as XFAIL (when using @parametrize_with_checks()), along with a</span>
    <span class="s3"># reason.</span>
    <span class="s3"># Currently, a check should be skipped or marked if</span>
    <span class="s3"># the check is in the _xfail_checks tag of the estimator</span>

    <span class="s1">check_name = check.func.__name__ </span><span class="s0">if </span><span class="s1">isinstance(check</span><span class="s0">, </span><span class="s1">partial) </span><span class="s0">else </span><span class="s1">check.__name__</span>

    <span class="s1">xfail_checks = _safe_tags(estimator</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;_xfail_checks&quot;</span><span class="s1">) </span><span class="s0">or </span><span class="s1">{}</span>
    <span class="s0">if </span><span class="s1">check_name </span><span class="s0">in </span><span class="s1">xfail_checks:</span>
        <span class="s0">return True, </span><span class="s1">xfail_checks[check_name]</span>

    <span class="s0">return False, </span><span class="s2">&quot;placeholder reason that will never be used&quot;</span>


<span class="s0">def </span><span class="s1">parametrize_with_checks(estimators):</span>
    <span class="s5">&quot;&quot;&quot;Pytest specific decorator for parametrizing estimator checks. 
 
    The `id` of each check is set to be a pprint version of the estimator 
    and the name of the check with its keyword arguments. 
    This allows to use `pytest -k` to specify which tests to run:: 
 
        pytest test_check_estimators.py -k check_estimators_fit_returns_self 
 
    Parameters 
    ---------- 
    estimators : list of estimators instances 
        Estimators to generated checks for. 
 
        .. versionchanged:: 0.24 
           Passing a class was deprecated in version 0.23, and support for 
           classes was removed in 0.24. Pass an instance instead. 
 
        .. versionadded:: 0.24 
 
    Returns 
    ------- 
    decorator : `pytest.mark.parametrize` 
 
    See Also 
    -------- 
    check_estimator : Check if estimator adheres to scikit-learn conventions. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.utils.estimator_checks import parametrize_with_checks 
    &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression 
    &gt;&gt;&gt; from sklearn.tree import DecisionTreeRegressor 
 
    &gt;&gt;&gt; @parametrize_with_checks([LogisticRegression(), 
    ...                           DecisionTreeRegressor()]) 
    ... def test_sklearn_compatible_estimator(estimator, check): 
    ...     check(estimator) 
 
    &quot;&quot;&quot;</span>
    <span class="s0">import </span><span class="s1">pytest</span>

    <span class="s0">if </span><span class="s1">any(isinstance(est</span><span class="s0">, </span><span class="s1">type) </span><span class="s0">for </span><span class="s1">est </span><span class="s0">in </span><span class="s1">estimators):</span>
        <span class="s1">msg = (</span>
            <span class="s2">&quot;Passing a class was deprecated in version 0.23 &quot;</span>
            <span class="s2">&quot;and isn't supported anymore from 0.24.&quot;</span>
            <span class="s2">&quot;Please pass an instance instead.&quot;</span>
        <span class="s1">)</span>
        <span class="s0">raise </span><span class="s1">TypeError(msg)</span>

    <span class="s0">def </span><span class="s1">checks_generator():</span>
        <span class="s0">for </span><span class="s1">estimator </span><span class="s0">in </span><span class="s1">estimators:</span>
            <span class="s1">name = type(estimator).__name__</span>
            <span class="s0">for </span><span class="s1">check </span><span class="s0">in </span><span class="s1">_yield_all_checks(estimator):</span>
                <span class="s1">check = partial(check</span><span class="s0">, </span><span class="s1">name)</span>
                <span class="s0">yield </span><span class="s1">_maybe_mark_xfail(estimator</span><span class="s0">, </span><span class="s1">check</span><span class="s0">, </span><span class="s1">pytest)</span>

    <span class="s0">return </span><span class="s1">pytest.mark.parametrize(</span>
        <span class="s2">&quot;estimator, check&quot;</span><span class="s0">, </span><span class="s1">checks_generator()</span><span class="s0">, </span><span class="s1">ids=_get_check_estimator_ids</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">check_estimator(estimator=</span><span class="s0">None, </span><span class="s1">generate_only=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot;Check if estimator adheres to scikit-learn conventions. 
 
    This function will run an extensive test-suite for input validation, 
    shapes, etc, making sure that the estimator complies with `scikit-learn` 
    conventions as detailed in :ref:`rolling_your_own_estimator`. 
    Additional tests for classifiers, regressors, clustering or transformers 
    will be run if the Estimator class inherits from the corresponding mixin 
    from sklearn.base. 
 
    Setting `generate_only=True` returns a generator that yields (estimator, 
    check) tuples where the check can be called independently from each 
    other, i.e. `check(estimator)`. This allows all checks to be run 
    independently and report the checks that are failing. 
 
    scikit-learn provides a pytest specific decorator, 
    :func:`~sklearn.utils.estimator_checks.parametrize_with_checks`, making it 
    easier to test multiple estimators. 
 
    Parameters 
    ---------- 
    estimator : estimator object 
        Estimator instance to check. 
 
        .. versionadded:: 1.1 
           Passing a class was deprecated in version 0.23, and support for 
           classes was removed in 0.24. 
 
    generate_only : bool, default=False 
        When `False`, checks are evaluated when `check_estimator` is called. 
        When `True`, `check_estimator` returns a generator that yields 
        (estimator, check) tuples. The check is run by calling 
        `check(estimator)`. 
 
        .. versionadded:: 0.22 
 
    Returns 
    ------- 
    checks_generator : generator 
        Generator that yields (estimator, check) tuples. Returned when 
        `generate_only=True`. 
 
    See Also 
    -------- 
    parametrize_with_checks : Pytest specific decorator for parametrizing estimator 
        checks. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">isinstance(estimator</span><span class="s0">, </span><span class="s1">type):</span>
        <span class="s1">msg = (</span>
            <span class="s2">&quot;Passing a class was deprecated in version 0.23 &quot;</span>
            <span class="s2">&quot;and isn't supported anymore from 0.24.&quot;</span>
            <span class="s2">&quot;Please pass an instance instead.&quot;</span>
        <span class="s1">)</span>
        <span class="s0">raise </span><span class="s1">TypeError(msg)</span>

    <span class="s1">name = type(estimator).__name__</span>

    <span class="s0">def </span><span class="s1">checks_generator():</span>
        <span class="s0">for </span><span class="s1">check </span><span class="s0">in </span><span class="s1">_yield_all_checks(estimator):</span>
            <span class="s1">check = _maybe_skip(estimator</span><span class="s0">, </span><span class="s1">check)</span>
            <span class="s0">yield </span><span class="s1">estimator</span><span class="s0">, </span><span class="s1">partial(check</span><span class="s0">, </span><span class="s1">name)</span>

    <span class="s0">if </span><span class="s1">generate_only:</span>
        <span class="s0">return </span><span class="s1">checks_generator()</span>

    <span class="s0">for </span><span class="s1">estimator</span><span class="s0">, </span><span class="s1">check </span><span class="s0">in </span><span class="s1">checks_generator():</span>
        <span class="s0">try</span><span class="s1">:</span>
            <span class="s1">check(estimator)</span>
        <span class="s0">except </span><span class="s1">SkipTest </span><span class="s0">as </span><span class="s1">exception:</span>
            <span class="s3"># SkipTest is thrown when pandas can't be imported, or by checks</span>
            <span class="s3"># that are in the xfail_checks tag</span>
            <span class="s1">warnings.warn(str(exception)</span><span class="s0">, </span><span class="s1">SkipTestWarning)</span>


<span class="s0">def </span><span class="s1">_regression_dataset():</span>
    <span class="s0">global </span><span class="s1">REGRESSION_DATASET</span>
    <span class="s0">if </span><span class="s1">REGRESSION_DATASET </span><span class="s0">is None</span><span class="s1">:</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(</span>
            <span class="s1">n_samples=</span><span class="s4">200</span><span class="s0">,</span>
            <span class="s1">n_features=</span><span class="s4">10</span><span class="s0">,</span>
            <span class="s1">n_informative=</span><span class="s4">1</span><span class="s0">,</span>
            <span class="s1">bias=</span><span class="s4">5.0</span><span class="s0">,</span>
            <span class="s1">noise=</span><span class="s4">20</span><span class="s0">,</span>
            <span class="s1">random_state=</span><span class="s4">42</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">X = StandardScaler().fit_transform(X)</span>
        <span class="s1">REGRESSION_DATASET = X</span><span class="s0">, </span><span class="s1">y</span>
    <span class="s0">return </span><span class="s1">REGRESSION_DATASET</span>


<span class="s0">def </span><span class="s1">_set_checking_parameters(estimator):</span>
    <span class="s3"># set parameters to speed up some estimators and</span>
    <span class="s3"># avoid deprecated behaviour</span>
    <span class="s1">params = estimator.get_params()</span>
    <span class="s1">name = estimator.__class__.__name__</span>
    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;TSNE&quot;</span><span class="s1">:</span>
        <span class="s1">estimator.set_params(perplexity=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s2">&quot;n_iter&quot; </span><span class="s0">in </span><span class="s1">params </span><span class="s0">and </span><span class="s1">name != </span><span class="s2">&quot;TSNE&quot;</span><span class="s1">:</span>
        <span class="s1">estimator.set_params(n_iter=</span><span class="s4">5</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s2">&quot;max_iter&quot; </span><span class="s0">in </span><span class="s1">params:</span>
        <span class="s0">if </span><span class="s1">estimator.max_iter </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">estimator.set_params(max_iter=min(</span><span class="s4">5</span><span class="s0">, </span><span class="s1">estimator.max_iter))</span>
        <span class="s3"># LinearSVR, LinearSVC</span>
        <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;LinearSVR&quot;</span><span class="s0">, </span><span class="s2">&quot;LinearSVC&quot;</span><span class="s1">]:</span>
            <span class="s1">estimator.set_params(max_iter=</span><span class="s4">20</span><span class="s1">)</span>
        <span class="s3"># NMF</span>
        <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;NMF&quot;</span><span class="s1">:</span>
            <span class="s1">estimator.set_params(max_iter=</span><span class="s4">500</span><span class="s1">)</span>
        <span class="s3"># DictionaryLearning</span>
        <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;DictionaryLearning&quot;</span><span class="s1">:</span>
            <span class="s1">estimator.set_params(max_iter=</span><span class="s4">20</span><span class="s0">, </span><span class="s1">transform_algorithm=</span><span class="s2">&quot;lasso_lars&quot;</span><span class="s1">)</span>
        <span class="s3"># MiniBatchNMF</span>
        <span class="s0">if </span><span class="s1">estimator.__class__.__name__ == </span><span class="s2">&quot;MiniBatchNMF&quot;</span><span class="s1">:</span>
            <span class="s1">estimator.set_params(max_iter=</span><span class="s4">20</span><span class="s0">, </span><span class="s1">fresh_restarts=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s3"># MLP</span>
        <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;MLPClassifier&quot;</span><span class="s0">, </span><span class="s2">&quot;MLPRegressor&quot;</span><span class="s1">]:</span>
            <span class="s1">estimator.set_params(max_iter=</span><span class="s4">100</span><span class="s1">)</span>
        <span class="s3"># MiniBatchDictionaryLearning</span>
        <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;MiniBatchDictionaryLearning&quot;</span><span class="s1">:</span>
            <span class="s1">estimator.set_params(max_iter=</span><span class="s4">5</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s2">&quot;n_resampling&quot; </span><span class="s0">in </span><span class="s1">params:</span>
        <span class="s3"># randomized lasso</span>
        <span class="s1">estimator.set_params(n_resampling=</span><span class="s4">5</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s2">&quot;n_estimators&quot; </span><span class="s0">in </span><span class="s1">params:</span>
        <span class="s1">estimator.set_params(n_estimators=min(</span><span class="s4">5</span><span class="s0">, </span><span class="s1">estimator.n_estimators))</span>
    <span class="s0">if </span><span class="s2">&quot;max_trials&quot; </span><span class="s0">in </span><span class="s1">params:</span>
        <span class="s3"># RANSAC</span>
        <span class="s1">estimator.set_params(max_trials=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s2">&quot;n_init&quot; </span><span class="s0">in </span><span class="s1">params:</span>
        <span class="s3"># K-Means</span>
        <span class="s1">estimator.set_params(n_init=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s2">&quot;batch_size&quot; </span><span class="s0">in </span><span class="s1">params </span><span class="s0">and not </span><span class="s1">name.startswith(</span><span class="s2">&quot;MLP&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.set_params(batch_size=</span><span class="s4">10</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;MeanShift&quot;</span><span class="s1">:</span>
        <span class="s3"># In the case of check_fit2d_1sample, bandwidth is set to None and</span>
        <span class="s3"># is thus estimated. De facto it is 0.0 as a single sample is provided</span>
        <span class="s3"># and this makes the test fails. Hence we give it a placeholder value.</span>
        <span class="s1">estimator.set_params(bandwidth=</span><span class="s4">1.0</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;TruncatedSVD&quot;</span><span class="s1">:</span>
        <span class="s3"># TruncatedSVD doesn't run with n_components = n_features</span>
        <span class="s3"># This is ugly :-/</span>
        <span class="s1">estimator.n_components = </span><span class="s4">1</span>

    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;LassoLarsIC&quot;</span><span class="s1">:</span>
        <span class="s3"># Noise variance estimation does not work when `n_samples &lt; n_features`.</span>
        <span class="s3"># We need to provide the noise variance explicitly.</span>
        <span class="s1">estimator.set_params(noise_variance=</span><span class="s4">1.0</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_clusters&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_clusters = min(estimator.n_clusters</span><span class="s0">, </span><span class="s4">2</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_best&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_best = </span><span class="s4">1</span>

    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;SelectFdr&quot;</span><span class="s1">:</span>
        <span class="s3"># be tolerant of noisy datasets (not actually speed)</span>
        <span class="s1">estimator.set_params(alpha=</span><span class="s4">0.5</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;TheilSenRegressor&quot;</span><span class="s1">:</span>
        <span class="s1">estimator.max_subpopulation = </span><span class="s4">100</span>

    <span class="s0">if </span><span class="s1">isinstance(estimator</span><span class="s0">, </span><span class="s1">BaseRandomProjection):</span>
        <span class="s3"># Due to the jl lemma and often very few samples, the number</span>
        <span class="s3"># of components of the random matrix projection will be probably</span>
        <span class="s3"># greater than the number of features.</span>
        <span class="s3"># So we impose a smaller number (avoid &quot;auto&quot; mode)</span>
        <span class="s1">estimator.set_params(n_components=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">isinstance(estimator</span><span class="s0">, </span><span class="s1">SelectKBest):</span>
        <span class="s3"># SelectKBest has a default of k=10</span>
        <span class="s3"># which is more feature than we have in most case.</span>
        <span class="s1">estimator.set_params(k=</span><span class="s4">1</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">(</span><span class="s2">&quot;HistGradientBoostingClassifier&quot;</span><span class="s0">, </span><span class="s2">&quot;HistGradientBoostingRegressor&quot;</span><span class="s1">):</span>
        <span class="s3"># The default min_samples_leaf (20) isn't appropriate for small</span>
        <span class="s3"># datasets (only very shallow trees are built) that the checks use.</span>
        <span class="s1">estimator.set_params(min_samples_leaf=</span><span class="s4">5</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;DummyClassifier&quot;</span><span class="s1">:</span>
        <span class="s3"># the default strategy prior would output constant predictions and fail</span>
        <span class="s3"># for check_classifiers_predictions</span>
        <span class="s1">estimator.set_params(strategy=</span><span class="s2">&quot;stratified&quot;</span><span class="s1">)</span>

    <span class="s3"># Speed-up by reducing the number of CV or splits for CV estimators</span>
    <span class="s1">loo_cv = [</span><span class="s2">&quot;RidgeCV&quot;</span><span class="s0">, </span><span class="s2">&quot;RidgeClassifierCV&quot;</span><span class="s1">]</span>
    <span class="s0">if </span><span class="s1">name </span><span class="s0">not in </span><span class="s1">loo_cv </span><span class="s0">and </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;cv&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.set_params(cv=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_splits&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.set_params(n_splits=</span><span class="s4">3</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;OneHotEncoder&quot;</span><span class="s1">:</span>
        <span class="s1">estimator.set_params(handle_unknown=</span><span class="s2">&quot;ignore&quot;</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;QuantileRegressor&quot;</span><span class="s1">:</span>
        <span class="s3"># Avoid warning due to Scipy deprecating interior-point solver</span>
        <span class="s1">solver = </span><span class="s2">&quot;highs&quot; </span><span class="s0">if </span><span class="s1">sp_version &gt;= parse_version(</span><span class="s2">&quot;1.6.0&quot;</span><span class="s1">) </span><span class="s0">else </span><span class="s2">&quot;interior-point&quot;</span>
        <span class="s1">estimator.set_params(solver=solver)</span>

    <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
        <span class="s1">estimator.set_params(n_components=</span><span class="s4">1</span><span class="s1">)</span>

    <span class="s3"># Default &quot;auto&quot; parameter can lead to different ordering of eigenvalues on</span>
    <span class="s3"># windows: #24105</span>
    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;SpectralEmbedding&quot;</span><span class="s1">:</span>
        <span class="s1">estimator.set_params(eigen_tol=</span><span class="s4">1e-5</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;HDBSCAN&quot;</span><span class="s1">:</span>
        <span class="s1">estimator.set_params(min_samples=</span><span class="s4">1</span><span class="s1">)</span>


<span class="s0">class </span><span class="s1">_NotAnArray:</span>
    <span class="s5">&quot;&quot;&quot;An object that is convertible to an array. 
 
    Parameters 
    ---------- 
    data : array-like 
        The data. 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">data):</span>
        <span class="s1">self.data = np.asarray(data)</span>

    <span class="s0">def </span><span class="s1">__array__(self</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s0">return </span><span class="s1">self.data</span>

    <span class="s0">def </span><span class="s1">__array_function__(self</span><span class="s0">, </span><span class="s1">func</span><span class="s0">, </span><span class="s1">types</span><span class="s0">, </span><span class="s1">args</span><span class="s0">, </span><span class="s1">kwargs):</span>
        <span class="s0">if </span><span class="s1">func.__name__ == </span><span class="s2">&quot;may_share_memory&quot;</span><span class="s1">:</span>
            <span class="s0">return True</span>
        <span class="s0">raise </span><span class="s1">TypeError(</span><span class="s2">&quot;Don't want to call array_function {}!&quot;</span><span class="s1">.format(func.__name__))</span>


<span class="s0">def </span><span class="s1">_is_pairwise_metric(estimator):</span>
    <span class="s5">&quot;&quot;&quot;Returns True if estimator accepts pairwise metric. 
 
    Parameters 
    ---------- 
    estimator : object 
        Estimator object to test. 
 
    Returns 
    ------- 
    out : bool 
        True if _pairwise is set to True and False otherwise. 
    &quot;&quot;&quot;</span>
    <span class="s1">metric = getattr(estimator</span><span class="s0">, </span><span class="s2">&quot;metric&quot;</span><span class="s0">, None</span><span class="s1">)</span>

    <span class="s0">return </span><span class="s1">bool(metric == </span><span class="s2">&quot;precomputed&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">_generate_sparse_matrix(X_csr):</span>
    <span class="s5">&quot;&quot;&quot;Generate sparse matrices with {32,64}bit indices of diverse format. 
 
    Parameters 
    ---------- 
    X_csr: CSR Matrix 
        Input matrix in CSR format. 
 
    Returns 
    ------- 
    out: iter(Matrices) 
        In format['dok', 'lil', 'dia', 'bsr', 'csr', 'csc', 'coo', 
        'coo_64', 'csc_64', 'csr_64'] 
    &quot;&quot;&quot;</span>

    <span class="s0">assert </span><span class="s1">X_csr.format == </span><span class="s2">&quot;csr&quot;</span>
    <span class="s0">yield </span><span class="s2">&quot;csr&quot;</span><span class="s0">, </span><span class="s1">X_csr.copy()</span>
    <span class="s0">for </span><span class="s1">sparse_format </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;dok&quot;</span><span class="s0">, </span><span class="s2">&quot;lil&quot;</span><span class="s0">, </span><span class="s2">&quot;dia&quot;</span><span class="s0">, </span><span class="s2">&quot;bsr&quot;</span><span class="s0">, </span><span class="s2">&quot;csc&quot;</span><span class="s0">, </span><span class="s2">&quot;coo&quot;</span><span class="s1">]:</span>
        <span class="s0">yield </span><span class="s1">sparse_format</span><span class="s0">, </span><span class="s1">X_csr.asformat(sparse_format)</span>

    <span class="s3"># Generate large indices matrix only if its supported by scipy</span>
    <span class="s1">X_coo = X_csr.asformat(</span><span class="s2">&quot;coo&quot;</span><span class="s1">)</span>
    <span class="s1">X_coo.row = X_coo.row.astype(</span><span class="s2">&quot;int64&quot;</span><span class="s1">)</span>
    <span class="s1">X_coo.col = X_coo.col.astype(</span><span class="s2">&quot;int64&quot;</span><span class="s1">)</span>
    <span class="s0">yield </span><span class="s2">&quot;coo_64&quot;</span><span class="s0">, </span><span class="s1">X_coo</span>

    <span class="s0">for </span><span class="s1">sparse_format </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;csc&quot;</span><span class="s0">, </span><span class="s2">&quot;csr&quot;</span><span class="s1">]:</span>
        <span class="s1">X = X_csr.asformat(sparse_format)</span>
        <span class="s1">X.indices = X.indices.astype(</span><span class="s2">&quot;int64&quot;</span><span class="s1">)</span>
        <span class="s1">X.indptr = X.indptr.astype(</span><span class="s2">&quot;int64&quot;</span><span class="s1">)</span>
        <span class="s0">yield </span><span class="s1">sparse_format + </span><span class="s2">&quot;_64&quot;</span><span class="s0">, </span><span class="s1">X</span>


<span class="s0">def </span><span class="s1">check_array_api_input(</span>
    <span class="s1">name</span><span class="s0">, </span><span class="s1">estimator_orig</span><span class="s0">, </span><span class="s1">*</span><span class="s0">, </span><span class="s1">array_namespace</span><span class="s0">, </span><span class="s1">device=</span><span class="s0">None, </span><span class="s1">dtype=</span><span class="s2">&quot;float64&quot;</span>
<span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot;Check that the array_api Array gives the same results as ndarrays.&quot;&quot;&quot;</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">array_mod = importlib.import_module(array_namespace)</span>
    <span class="s0">except </span><span class="s1">ModuleNotFoundError:</span>
        <span class="s0">raise </span><span class="s1">SkipTest(</span>
            <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">array_namespace</span><span class="s0">} </span><span class="s2">is not installed: not checking array_api input&quot;</span>
        <span class="s1">)</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s0">import </span><span class="s1">array_api_compat  </span><span class="s3"># noqa</span>
    <span class="s0">except </span><span class="s1">ImportError:</span>
        <span class="s0">raise </span><span class="s1">SkipTest(</span>
            <span class="s2">&quot;array_api_compat is not installed: not checking array_api input&quot;</span>
        <span class="s1">)</span>

    <span class="s3"># First create an array using the chosen array module and then get the</span>
    <span class="s3"># corresponding (compatibility wrapped) array namespace based on it.</span>
    <span class="s3"># This is because `cupy` is not the same as the compatibility wrapped</span>
    <span class="s3"># namespace of a CuPy array.</span>
    <span class="s1">xp = array_api_compat.get_namespace(array_mod.asarray(</span><span class="s4">1</span><span class="s1">))</span>

    <span class="s0">if </span><span class="s1">array_namespace == </span><span class="s2">&quot;torch&quot; </span><span class="s0">and </span><span class="s1">device == </span><span class="s2">&quot;cuda&quot; </span><span class="s0">and not </span><span class="s1">xp.has_cuda:</span>
        <span class="s0">raise </span><span class="s1">SkipTest(</span><span class="s2">&quot;PyTorch test requires cuda, which is not available&quot;</span><span class="s1">)</span>
    <span class="s0">elif </span><span class="s1">array_namespace </span><span class="s0">in </span><span class="s1">{</span><span class="s2">&quot;cupy&quot;</span><span class="s0">, </span><span class="s2">&quot;cupy.array_api&quot;</span><span class="s1">}:  </span><span class="s3"># pragma: nocover</span>
        <span class="s0">import </span><span class="s1">cupy</span>

        <span class="s0">if </span><span class="s1">cupy.cuda.runtime.getDeviceCount() == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s0">raise </span><span class="s1">SkipTest(</span><span class="s2">&quot;CuPy test requires cuda, which is not available&quot;</span><span class="s1">)</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">X = X.astype(dtype</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator_orig</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">est = clone(estimator_orig)</span>

    <span class="s1">X_xp = xp.asarray(X</span><span class="s0">, </span><span class="s1">device=device)</span>
    <span class="s1">y_xp = xp.asarray(y</span><span class="s0">, </span><span class="s1">device=device)</span>

    <span class="s1">est.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">array_attributes = {</span>
        <span class="s1">key: value </span><span class="s0">for </span><span class="s1">key</span><span class="s0">, </span><span class="s1">value </span><span class="s0">in </span><span class="s1">vars(est).items() </span><span class="s0">if </span><span class="s1">isinstance(value</span><span class="s0">, </span><span class="s1">np.ndarray)</span>
    <span class="s1">}</span>

    <span class="s1">est_xp = clone(est)</span>
    <span class="s0">with </span><span class="s1">config_context(array_api_dispatch=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s1">est_xp.fit(X_xp</span><span class="s0">, </span><span class="s1">y_xp)</span>

    <span class="s3"># Fitted attributes which are arrays must have the same</span>
    <span class="s3"># namespace as the one of the training data.</span>
    <span class="s0">for </span><span class="s1">key</span><span class="s0">, </span><span class="s1">attribute </span><span class="s0">in </span><span class="s1">array_attributes.items():</span>
        <span class="s1">est_xp_param = getattr(est_xp</span><span class="s0">, </span><span class="s1">key)</span>
        <span class="s0">assert </span><span class="s1">(</span>
            <span class="s1">get_namespace(est_xp_param)[</span><span class="s4">0</span><span class="s1">] == get_namespace(X_xp)[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">)</span><span class="s0">, </span><span class="s2">f&quot;'</span><span class="s0">{</span><span class="s1">key</span><span class="s0">}</span><span class="s2">' attribute is in wrong namespace&quot;</span>

        <span class="s0">assert </span><span class="s1">array_device(est_xp_param) == array_device(X_xp)</span>

        <span class="s1">est_xp_param_np = _convert_to_numpy(est_xp_param</span><span class="s0">, </span><span class="s1">xp=xp)</span>
        <span class="s1">assert_allclose(</span>
            <span class="s1">attribute</span><span class="s0">,</span>
            <span class="s1">est_xp_param_np</span><span class="s0">,</span>
            <span class="s1">err_msg=</span><span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">key</span><span class="s0">} </span><span class="s2">not the same&quot;</span><span class="s0">,</span>
            <span class="s1">atol=np.finfo(X.dtype).eps * </span><span class="s4">100</span><span class="s0">,</span>
        <span class="s1">)</span>

    <span class="s3"># Check estimator methods, if supported, give the same results</span>
    <span class="s1">methods = (</span>
        <span class="s2">&quot;decision_function&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;predict&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;predict_log_proba&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;predict_proba&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;transform&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;inverse_transform&quot;</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">for </span><span class="s1">method_name </span><span class="s0">in </span><span class="s1">methods:</span>
        <span class="s1">method = getattr(est</span><span class="s0">, </span><span class="s1">method_name</span><span class="s0">, None</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">method </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s0">continue</span>

        <span class="s1">result = method(X)</span>
        <span class="s0">with </span><span class="s1">config_context(array_api_dispatch=</span><span class="s0">True</span><span class="s1">):</span>
            <span class="s1">result_xp = getattr(est_xp</span><span class="s0">, </span><span class="s1">method_name)(X_xp)</span>

        <span class="s0">assert </span><span class="s1">(</span>
            <span class="s1">get_namespace(result_xp)[</span><span class="s4">0</span><span class="s1">] == get_namespace(X_xp)[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">)</span><span class="s0">, </span><span class="s2">f&quot;'</span><span class="s0">{</span><span class="s1">method</span><span class="s0">}</span><span class="s2">' output is in wrong namespace&quot;</span>

        <span class="s0">assert </span><span class="s1">array_device(result_xp) == array_device(X_xp)</span>

        <span class="s1">result_xp_np = _convert_to_numpy(result_xp</span><span class="s0">, </span><span class="s1">xp=xp)</span>

        <span class="s1">assert_allclose(</span>
            <span class="s1">result</span><span class="s0">,</span>
            <span class="s1">result_xp_np</span><span class="s0">,</span>
            <span class="s1">err_msg=</span><span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">method</span><span class="s0">} </span><span class="s2">did not the return the same result&quot;</span><span class="s0">,</span>
            <span class="s1">atol=np.finfo(X.dtype).eps * </span><span class="s4">100</span><span class="s0">,</span>
        <span class="s1">)</span>


<span class="s0">def </span><span class="s1">check_estimator_sparse_data(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = rng.uniform(size=(</span><span class="s4">40</span><span class="s0">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">X[X &lt; </span><span class="s4">0.8</span><span class="s1">] = </span><span class="s4">0</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">X_csr = sparse.csr_matrix(X)</span>
    <span class="s1">y = (</span><span class="s4">4 </span><span class="s1">* rng.uniform(size=</span><span class="s4">40</span><span class="s1">)).astype(int)</span>
    <span class="s3"># catch deprecation warnings</span>
    <span class="s0">with </span><span class="s1">ignore_warnings(category=FutureWarning):</span>
        <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">tags = _safe_tags(estimator_orig)</span>
    <span class="s0">for </span><span class="s1">matrix_format</span><span class="s0">, </span><span class="s1">X </span><span class="s0">in </span><span class="s1">_generate_sparse_matrix(X_csr):</span>
        <span class="s3"># catch deprecation warnings</span>
        <span class="s0">with </span><span class="s1">ignore_warnings(category=FutureWarning):</span>
            <span class="s1">estimator = clone(estimator_orig)</span>
            <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;Scaler&quot;</span><span class="s0">, </span><span class="s2">&quot;StandardScaler&quot;</span><span class="s1">]:</span>
                <span class="s1">estimator.set_params(with_mean=</span><span class="s0">False</span><span class="s1">)</span>
        <span class="s3"># fit and predict</span>
        <span class="s0">if </span><span class="s2">&quot;64&quot; </span><span class="s0">in </span><span class="s1">matrix_format:</span>
            <span class="s1">err_msg = (</span>
                <span class="s2">f&quot;Estimator </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">doesn't seem to support </span><span class="s0">{</span><span class="s1">matrix_format</span><span class="s0">} </span><span class="s2">&quot;</span>
                <span class="s2">&quot;matrix, and is not failing gracefully, e.g. by using &quot;</span>
                <span class="s2">&quot;check_array(X, accept_large_sparse=False)&quot;</span>
            <span class="s1">)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">err_msg = (</span>
                <span class="s2">f&quot;Estimator </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">doesn't seem to fail gracefully on sparse &quot;</span>
                <span class="s2">&quot;data: error message should state explicitly that sparse &quot;</span>
                <span class="s2">&quot;input is not supported if this is not the case.&quot;</span>
            <span class="s1">)</span>
        <span class="s0">with </span><span class="s1">raises(</span>
            <span class="s1">(TypeError</span><span class="s0">, </span><span class="s1">ValueError)</span><span class="s0">,</span>
            <span class="s1">match=[</span><span class="s2">&quot;sparse&quot;</span><span class="s0">, </span><span class="s2">&quot;Sparse&quot;</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">may_pass=</span><span class="s0">True,</span>
            <span class="s1">err_msg=err_msg</span><span class="s0">,</span>
        <span class="s1">):</span>
            <span class="s0">with </span><span class="s1">ignore_warnings(category=FutureWarning):</span>
                <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
            <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;predict&quot;</span><span class="s1">):</span>
                <span class="s1">pred = estimator.predict(X)</span>
                <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;multioutput_only&quot;</span><span class="s1">]:</span>
                    <span class="s0">assert </span><span class="s1">pred.shape == (X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>
                <span class="s0">else</span><span class="s1">:</span>
                    <span class="s0">assert </span><span class="s1">pred.shape == (X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">,</span><span class="s1">)</span>
            <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">):</span>
                <span class="s1">probs = estimator.predict_proba(X)</span>
                <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;binary_only&quot;</span><span class="s1">]:</span>
                    <span class="s1">expected_probs_shape = (X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s4">2</span><span class="s1">)</span>
                <span class="s0">else</span><span class="s1">:</span>
                    <span class="s1">expected_probs_shape = (X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s4">4</span><span class="s1">)</span>
                <span class="s0">assert </span><span class="s1">probs.shape == expected_probs_shape</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_sample_weights_pandas_series(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check that estimators will accept a 'sample_weight' parameter of</span>
    <span class="s3"># type pandas.Series in the 'fit' function.</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s0">import </span><span class="s1">pandas </span><span class="s0">as </span><span class="s1">pd</span>

        <span class="s1">X = np.array(</span>
            <span class="s1">[</span>
                <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">4</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">4</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">4</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">]</span>
        <span class="s1">)</span>
        <span class="s1">X = pd.DataFrame(_enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
        <span class="s1">y = pd.Series([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>
        <span class="s1">weights = pd.Series([</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">12</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">_safe_tags(estimator</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;multioutput_only&quot;</span><span class="s1">):</span>
            <span class="s1">y = pd.DataFrame(y</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
        <span class="s0">try</span><span class="s1">:</span>
            <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=weights)</span>
        <span class="s0">except </span><span class="s1">ValueError:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span>
                <span class="s2">&quot;Estimator {0} raises error if &quot;</span>
                <span class="s2">&quot;'sample_weight' parameter is of &quot;</span>
                <span class="s2">&quot;type pandas.Series&quot;</span><span class="s1">.format(name)</span>
            <span class="s1">)</span>
    <span class="s0">except </span><span class="s1">ImportError:</span>
        <span class="s0">raise </span><span class="s1">SkipTest(</span>
            <span class="s2">&quot;pandas is not installed: not testing for &quot;</span>
            <span class="s2">&quot;input of type pandas.Series to class weight.&quot;</span>
        <span class="s1">)</span>


<span class="s1">@ignore_warnings(category=(FutureWarning))</span>
<span class="s0">def </span><span class="s1">check_sample_weights_not_an_array(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check that estimators will accept a 'sample_weight' parameter of</span>
    <span class="s3"># type _NotAnArray in the 'fit' function.</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">X = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">4</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">4</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">4</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">X = _NotAnArray(_enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X))</span>
    <span class="s1">y = _NotAnArray([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">weights = _NotAnArray([</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">12</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">_safe_tags(estimator</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;multioutput_only&quot;</span><span class="s1">):</span>
        <span class="s1">y = _NotAnArray(y.data.reshape(-</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=weights)</span>


<span class="s1">@ignore_warnings(category=(FutureWarning))</span>
<span class="s0">def </span><span class="s1">check_sample_weights_list(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check that estimators will accept a 'sample_weight' parameter of</span>
    <span class="s3"># type list in the 'fit' function.</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s4">30</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">rnd.uniform(size=(n_samples</span><span class="s0">, </span><span class="s4">3</span><span class="s1">)))</span>
    <span class="s1">y = np.arange(n_samples) % </span><span class="s4">3</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">sample_weight = [</span><span class="s4">3</span><span class="s1">] * n_samples</span>
    <span class="s3"># Test that estimators don't raise any exception</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_sample_weights_shape(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check that estimators raise an error if sample_weight</span>
    <span class="s3"># shape mismatches the input</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">X = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=np.ones(len(y)))</span>

    <span class="s0">with </span><span class="s1">raises(ValueError):</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=np.ones(</span><span class="s4">2 </span><span class="s1">* len(y)))</span>

    <span class="s0">with </span><span class="s1">raises(ValueError):</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=np.ones((len(y)</span><span class="s0">, </span><span class="s4">2</span><span class="s1">)))</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_sample_weights_invariance(name</span><span class="s0">, </span><span class="s1">estimator_orig</span><span class="s0">, </span><span class="s1">kind=</span><span class="s2">&quot;ones&quot;</span><span class="s1">):</span>
    <span class="s3"># For kind=&quot;ones&quot; check that the estimators yield same results for</span>
    <span class="s3"># unit weights and no weights</span>
    <span class="s3"># For kind=&quot;zeros&quot; check that setting sample_weight to 0 is equivalent</span>
    <span class="s3"># to removing corresponding samples.</span>
    <span class="s1">estimator1 = clone(estimator_orig)</span>
    <span class="s1">estimator2 = clone(estimator_orig)</span>
    <span class="s1">set_random_state(estimator1</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">set_random_state(estimator2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">X1 = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span><span class="s0">,</span>
        <span class="s1">dtype=np.float64</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">y1 = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=int)</span>

    <span class="s0">if </span><span class="s1">kind == </span><span class="s2">&quot;ones&quot;</span><span class="s1">:</span>
        <span class="s1">X2 = X1</span>
        <span class="s1">y2 = y1</span>
        <span class="s1">sw2 = np.ones(shape=len(y1))</span>
        <span class="s1">err_msg = (</span>
            <span class="s2">f&quot;For </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">sample_weight=None is not equivalent to sample_weight=ones&quot;</span>
        <span class="s1">)</span>
    <span class="s0">elif </span><span class="s1">kind == </span><span class="s2">&quot;zeros&quot;</span><span class="s1">:</span>
        <span class="s3"># Construct a dataset that is very different to (X, y) if weights</span>
        <span class="s3"># are disregarded, but identical to (X, y) given weights.</span>
        <span class="s1">X2 = np.vstack([X1</span><span class="s0">, </span><span class="s1">X1 + </span><span class="s4">1</span><span class="s1">])</span>
        <span class="s1">y2 = np.hstack([y1</span><span class="s0">, </span><span class="s4">3 </span><span class="s1">- y1])</span>
        <span class="s1">sw2 = np.ones(shape=len(y1) * </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">sw2[len(y1) :] = </span><span class="s4">0</span>
        <span class="s1">X2</span><span class="s0">, </span><span class="s1">y2</span><span class="s0">, </span><span class="s1">sw2 = shuffle(X2</span><span class="s0">, </span><span class="s1">y2</span><span class="s0">, </span><span class="s1">sw2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

        <span class="s1">err_msg = (</span>
            <span class="s2">f&quot;For </span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">, a zero sample_weight is not equivalent to removing the sample&quot;</span>
        <span class="s1">)</span>
    <span class="s0">else</span><span class="s1">:  </span><span class="s3"># pragma: no cover</span>
        <span class="s0">raise </span><span class="s1">ValueError</span>

    <span class="s1">y1 = _enforce_estimator_tags_y(estimator1</span><span class="s0">, </span><span class="s1">y1)</span>
    <span class="s1">y2 = _enforce_estimator_tags_y(estimator2</span><span class="s0">, </span><span class="s1">y2)</span>

    <span class="s1">estimator1.fit(X1</span><span class="s0">, </span><span class="s1">y=y1</span><span class="s0">, </span><span class="s1">sample_weight=</span><span class="s0">None</span><span class="s1">)</span>
    <span class="s1">estimator2.fit(X2</span><span class="s0">, </span><span class="s1">y=y2</span><span class="s0">, </span><span class="s1">sample_weight=sw2)</span>

    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;predict&quot;</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s0">, </span><span class="s2">&quot;decision_function&quot;</span><span class="s0">, </span><span class="s2">&quot;transform&quot;</span><span class="s1">]:</span>
        <span class="s0">if </span><span class="s1">hasattr(estimator_orig</span><span class="s0">, </span><span class="s1">method):</span>
            <span class="s1">X_pred1 = getattr(estimator1</span><span class="s0">, </span><span class="s1">method)(X1)</span>
            <span class="s1">X_pred2 = getattr(estimator2</span><span class="s0">, </span><span class="s1">method)(X1)</span>
            <span class="s1">assert_allclose_dense_sparse(X_pred1</span><span class="s0">, </span><span class="s1">X_pred2</span><span class="s0">, </span><span class="s1">err_msg=err_msg)</span>


<span class="s0">def </span><span class="s1">check_sample_weights_not_overwritten(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check that estimators don't override the passed sample_weight parameter</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">set_random_state(estimator</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">X = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span><span class="s0">,</span>
        <span class="s1">dtype=np.float64</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=int)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">sample_weight_original = np.ones(y.shape[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">sample_weight_original[</span><span class="s4">0</span><span class="s1">] = </span><span class="s4">10.0</span>

    <span class="s1">sample_weight_fit = sample_weight_original.copy()</span>

    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight_fit)</span>

    <span class="s1">err_msg = </span><span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">overwrote the original `sample_weight` given during fit&quot;</span>
    <span class="s1">assert_allclose(sample_weight_fit</span><span class="s0">, </span><span class="s1">sample_weight_original</span><span class="s0">, </span><span class="s1">err_msg=err_msg)</span>


<span class="s1">@ignore_warnings(category=(FutureWarning</span><span class="s0">, </span><span class="s1">UserWarning))</span>
<span class="s0">def </span><span class="s1">check_dtype_object(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check that estimators treat dtype object as numeric if possible</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">rng.uniform(size=(</span><span class="s4">40</span><span class="s0">, </span><span class="s4">10</span><span class="s1">)))</span>
    <span class="s1">X = X.astype(object)</span>
    <span class="s1">tags = _safe_tags(estimator_orig)</span>
    <span class="s1">y = (X[:</span><span class="s0">, </span><span class="s4">0</span><span class="s1">] * </span><span class="s4">4</span><span class="s1">).astype(int)</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;predict&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.predict(X)</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;transform&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.transform(X)</span>

    <span class="s0">with </span><span class="s1">raises(Exception</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;Unknown label type&quot;</span><span class="s0">, </span><span class="s1">may_pass=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y.astype(object))</span>

    <span class="s0">if </span><span class="s2">&quot;string&quot; </span><span class="s0">not in </span><span class="s1">tags[</span><span class="s2">&quot;X_types&quot;</span><span class="s1">]:</span>
        <span class="s1">X[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">] = {</span><span class="s2">&quot;foo&quot;</span><span class="s1">: </span><span class="s2">&quot;bar&quot;</span><span class="s1">}</span>
        <span class="s3"># This error is raised by:</span>
        <span class="s3"># - `np.asarray` in `check_array`</span>
        <span class="s3"># - `_unique_python` for encoders</span>
        <span class="s1">msg = </span><span class="s2">&quot;argument must be .* string.* number&quot;</span>
        <span class="s0">with </span><span class="s1">raises(TypeError</span><span class="s0">, </span><span class="s1">match=msg):</span>
            <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s3"># Estimators supporting string will not call np.asarray to convert the</span>
        <span class="s3"># data to numeric and therefore, the error will not be raised.</span>
        <span class="s3"># Checking for each element dtype in the input array will be costly.</span>
        <span class="s3"># Refer to #11401 for full discussion.</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">check_complex_data(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s3"># check that estimators raise an exception on providing complex data</span>
    <span class="s1">X = rng.uniform(size=</span><span class="s4">10</span><span class="s1">) + </span><span class="s4">1j </span><span class="s1">* rng.uniform(size=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">X = X.reshape(-</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s3"># Something both valid for classification and regression</span>
    <span class="s1">y = rng.randint(low=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">high=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s4">10</span><span class="s1">) + </span><span class="s4">1j</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">set_random_state(estimator</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;Complex data not supported&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">check_dict_unchanged(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># this estimator raises</span>
    <span class="s3"># ValueError: Found array with 0 feature(s) (shape=(23, 0))</span>
    <span class="s3"># while a minimum of 1 is required.</span>
    <span class="s3"># error</span>
    <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;SpectralCoclustering&quot;</span><span class="s1">]:</span>
        <span class="s0">return</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;RANSACRegressor&quot;</span><span class="s1">]:</span>
        <span class="s1">X = </span><span class="s4">3 </span><span class="s1">* rnd.uniform(size=(</span><span class="s4">20</span><span class="s0">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">X = </span><span class="s4">2 </span><span class="s1">* rnd.uniform(size=(</span><span class="s4">20</span><span class="s0">, </span><span class="s4">3</span><span class="s1">))</span>

    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>

    <span class="s1">y = X[:</span><span class="s0">, </span><span class="s4">0</span><span class="s1">].astype(int)</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_components&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_components = </span><span class="s4">1</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_clusters&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_clusters = </span><span class="s4">1</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_best&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_best = </span><span class="s4">1</span>

    <span class="s1">set_random_state(estimator</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;predict&quot;</span><span class="s0">, </span><span class="s2">&quot;transform&quot;</span><span class="s0">, </span><span class="s2">&quot;decision_function&quot;</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">]:</span>
        <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s1">method):</span>
            <span class="s1">dict_before = estimator.__dict__.copy()</span>
            <span class="s1">getattr(estimator</span><span class="s0">, </span><span class="s1">method)(X)</span>
            <span class="s0">assert </span><span class="s1">estimator.__dict__ == dict_before</span><span class="s0">, </span><span class="s1">(</span>
                <span class="s2">&quot;Estimator changes __dict__ during %s&quot; </span><span class="s1">% method</span>
            <span class="s1">)</span>


<span class="s0">def </span><span class="s1">_is_public_parameter(attr):</span>
    <span class="s0">return not </span><span class="s1">(attr.startswith(</span><span class="s2">&quot;_&quot;</span><span class="s1">) </span><span class="s0">or </span><span class="s1">attr.endswith(</span><span class="s2">&quot;_&quot;</span><span class="s1">))</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_dont_overwrite_parameters(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check that fit method only changes or sets private attributes</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator_orig.__init__</span><span class="s0">, </span><span class="s2">&quot;deprecated_original&quot;</span><span class="s1">):</span>
        <span class="s3"># to not check deprecated classes</span>
        <span class="s0">return</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = </span><span class="s4">3 </span><span class="s1">* rnd.uniform(size=(</span><span class="s4">20</span><span class="s0">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = X[:</span><span class="s0">, </span><span class="s4">0</span><span class="s1">].astype(int)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_components&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_components = </span><span class="s4">1</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_clusters&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_clusters = </span><span class="s4">1</span>

    <span class="s1">set_random_state(estimator</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">dict_before_fit = estimator.__dict__.copy()</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">dict_after_fit = estimator.__dict__</span>

    <span class="s1">public_keys_after_fit = [</span>
        <span class="s1">key </span><span class="s0">for </span><span class="s1">key </span><span class="s0">in </span><span class="s1">dict_after_fit.keys() </span><span class="s0">if </span><span class="s1">_is_public_parameter(key)</span>
    <span class="s1">]</span>

    <span class="s1">attrs_added_by_fit = [</span>
        <span class="s1">key </span><span class="s0">for </span><span class="s1">key </span><span class="s0">in </span><span class="s1">public_keys_after_fit </span><span class="s0">if </span><span class="s1">key </span><span class="s0">not in </span><span class="s1">dict_before_fit.keys()</span>
    <span class="s1">]</span>

    <span class="s3"># check that fit doesn't add any public attribute</span>
    <span class="s0">assert not </span><span class="s1">attrs_added_by_fit</span><span class="s0">, </span><span class="s1">(</span>
        <span class="s2">&quot;Estimator adds public attribute(s) during&quot;</span>
        <span class="s2">&quot; the fit method.&quot;</span>
        <span class="s2">&quot; Estimators are only allowed to add private attributes&quot;</span>
        <span class="s2">&quot; either started with _ or ended&quot;</span>
        <span class="s2">&quot; with _ but %s added&quot;</span>
        <span class="s1">% </span><span class="s2">&quot;, &quot;</span><span class="s1">.join(attrs_added_by_fit)</span>
    <span class="s1">)</span>

    <span class="s3"># check that fit doesn't change any public attribute</span>
    <span class="s1">attrs_changed_by_fit = [</span>
        <span class="s1">key</span>
        <span class="s0">for </span><span class="s1">key </span><span class="s0">in </span><span class="s1">public_keys_after_fit</span>
        <span class="s0">if </span><span class="s1">(dict_before_fit[key] </span><span class="s0">is not </span><span class="s1">dict_after_fit[key])</span>
    <span class="s1">]</span>

    <span class="s0">assert not </span><span class="s1">attrs_changed_by_fit</span><span class="s0">, </span><span class="s1">(</span>
        <span class="s2">&quot;Estimator changes public attribute(s) during&quot;</span>
        <span class="s2">&quot; the fit method. Estimators are only allowed&quot;</span>
        <span class="s2">&quot; to change attributes started&quot;</span>
        <span class="s2">&quot; or ended with _, but&quot;</span>
        <span class="s2">&quot; %s changed&quot;</span>
        <span class="s1">% </span><span class="s2">&quot;, &quot;</span><span class="s1">.join(attrs_changed_by_fit)</span>
    <span class="s1">)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_fit2d_predict1d(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check by fitting a 2d array and predicting with a 1d array</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = </span><span class="s4">3 </span><span class="s1">* rnd.uniform(size=(</span><span class="s4">20</span><span class="s0">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = X[:</span><span class="s0">, </span><span class="s4">0</span><span class="s1">].astype(int)</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_components&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_components = </span><span class="s4">1</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_clusters&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_clusters = </span><span class="s4">1</span>

    <span class="s1">set_random_state(estimator</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;predict&quot;</span><span class="s0">, </span><span class="s2">&quot;transform&quot;</span><span class="s0">, </span><span class="s2">&quot;decision_function&quot;</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">]:</span>
        <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s1">method):</span>
            <span class="s1">assert_raise_message(</span>
                <span class="s1">ValueError</span><span class="s0">, </span><span class="s2">&quot;Reshape your data&quot;</span><span class="s0">, </span><span class="s1">getattr(estimator</span><span class="s0">, </span><span class="s1">method)</span><span class="s0">, </span><span class="s1">X[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">)</span>


<span class="s0">def </span><span class="s1">_apply_on_subsets(func</span><span class="s0">, </span><span class="s1">X):</span>
    <span class="s3"># apply function on the whole set and on mini batches</span>
    <span class="s1">result_full = func(X)</span>
    <span class="s1">n_features = X.shape[</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">result_by_batch = [func(batch.reshape(</span><span class="s4">1</span><span class="s0">, </span><span class="s1">n_features)) </span><span class="s0">for </span><span class="s1">batch </span><span class="s0">in </span><span class="s1">X]</span>

    <span class="s3"># func can output tuple (e.g. score_samples)</span>
    <span class="s0">if </span><span class="s1">type(result_full) == tuple:</span>
        <span class="s1">result_full = result_full[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">result_by_batch = list(map(</span><span class="s0">lambda </span><span class="s1">x: x[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">result_by_batch))</span>

    <span class="s0">if </span><span class="s1">sparse.issparse(result_full):</span>
        <span class="s1">result_full = result_full.A</span>
        <span class="s1">result_by_batch = [x.toarray() </span><span class="s0">for </span><span class="s1">x </span><span class="s0">in </span><span class="s1">result_by_batch]</span>

    <span class="s0">return </span><span class="s1">np.ravel(result_full)</span><span class="s0">, </span><span class="s1">np.ravel(result_by_batch)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_methods_subset_invariance(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check that method gives invariant results if applied</span>
    <span class="s3"># on mini batches or the whole set</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = </span><span class="s4">3 </span><span class="s1">* rnd.uniform(size=(</span><span class="s4">20</span><span class="s0">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = X[:</span><span class="s0">, </span><span class="s4">0</span><span class="s1">].astype(int)</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_components&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_components = </span><span class="s4">1</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_clusters&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_clusters = </span><span class="s4">1</span>

    <span class="s1">set_random_state(estimator</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">[</span>
        <span class="s2">&quot;predict&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;transform&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;decision_function&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;score_samples&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;predict_proba&quot;</span><span class="s0">,</span>
    <span class="s1">]:</span>
        <span class="s1">msg = (</span><span class="s2">&quot;{method} of {name} is not invariant when applied to a subset.&quot;</span><span class="s1">).format(</span>
            <span class="s1">method=method</span><span class="s0">, </span><span class="s1">name=name</span>
        <span class="s1">)</span>

        <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s1">method):</span>
            <span class="s1">result_full</span><span class="s0">, </span><span class="s1">result_by_batch = _apply_on_subsets(</span>
                <span class="s1">getattr(estimator</span><span class="s0">, </span><span class="s1">method)</span><span class="s0">, </span><span class="s1">X</span>
            <span class="s1">)</span>
            <span class="s1">assert_allclose(result_full</span><span class="s0">, </span><span class="s1">result_by_batch</span><span class="s0">, </span><span class="s1">atol=</span><span class="s4">1e-7</span><span class="s0">, </span><span class="s1">err_msg=msg)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_methods_sample_order_invariance(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check that method gives invariant results if applied</span>
    <span class="s3"># on a subset with different sample order</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = </span><span class="s4">3 </span><span class="s1">* rnd.uniform(size=(</span><span class="s4">20</span><span class="s0">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = X[:</span><span class="s0">, </span><span class="s4">0</span><span class="s1">].astype(np.int64)</span>
    <span class="s0">if </span><span class="s1">_safe_tags(estimator_orig</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;binary_only&quot;</span><span class="s1">):</span>
        <span class="s1">y[y == </span><span class="s4">2</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_components&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_components = </span><span class="s4">1</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_clusters&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_clusters = </span><span class="s4">2</span>

    <span class="s1">set_random_state(estimator</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">idx = np.random.permutation(X.shape[</span><span class="s4">0</span><span class="s1">])</span>

    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">[</span>
        <span class="s2">&quot;predict&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;transform&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;decision_function&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;score_samples&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;predict_proba&quot;</span><span class="s0">,</span>
    <span class="s1">]:</span>
        <span class="s1">msg = (</span>
            <span class="s2">&quot;{method} of {name} is not invariant when applied to a dataset&quot;</span>
            <span class="s2">&quot;with different sample order.&quot;</span>
        <span class="s1">).format(method=method</span><span class="s0">, </span><span class="s1">name=name)</span>

        <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s1">method):</span>
            <span class="s1">assert_allclose_dense_sparse(</span>
                <span class="s1">getattr(estimator</span><span class="s0">, </span><span class="s1">method)(X)[idx]</span><span class="s0">,</span>
                <span class="s1">getattr(estimator</span><span class="s0">, </span><span class="s1">method)(X[idx])</span><span class="s0">,</span>
                <span class="s1">atol=</span><span class="s4">1e-9</span><span class="s0">,</span>
                <span class="s1">err_msg=msg</span><span class="s0">,</span>
            <span class="s1">)</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">check_fit2d_1sample(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Check that fitting a 2d array with only one sample either works or</span>
    <span class="s3"># returns an informative message. The error message should either mention</span>
    <span class="s3"># the number of samples or the number of classes.</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = </span><span class="s4">3 </span><span class="s1">* rnd.uniform(size=(</span><span class="s4">1</span><span class="s0">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>

    <span class="s1">y = X[:</span><span class="s0">, </span><span class="s4">0</span><span class="s1">].astype(int)</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_components&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_components = </span><span class="s4">1</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_clusters&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_clusters = </span><span class="s4">1</span>

    <span class="s1">set_random_state(estimator</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s3"># min_cluster_size cannot be less than the data size for OPTICS.</span>
    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;OPTICS&quot;</span><span class="s1">:</span>
        <span class="s1">estimator.set_params(min_samples=</span><span class="s4">1.0</span><span class="s1">)</span>

    <span class="s3"># perplexity cannot be more than the number of samples for TSNE.</span>
    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;TSNE&quot;</span><span class="s1">:</span>
        <span class="s1">estimator.set_params(perplexity=</span><span class="s4">0.5</span><span class="s1">)</span>

    <span class="s1">msgs = [</span>
        <span class="s2">&quot;1 sample&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;n_samples = 1&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;n_samples=1&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;one sample&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;1 class&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;one class&quot;</span><span class="s0">,</span>
    <span class="s1">]</span>

    <span class="s0">with </span><span class="s1">raises(ValueError</span><span class="s0">, </span><span class="s1">match=msgs</span><span class="s0">, </span><span class="s1">may_pass=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">check_fit2d_1feature(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check fitting a 2d array with only 1 feature either works or returns</span>
    <span class="s3"># informative message</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = </span><span class="s4">3 </span><span class="s1">* rnd.uniform(size=(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = X[:</span><span class="s0">, </span><span class="s4">0</span><span class="s1">].astype(int)</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_components&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_components = </span><span class="s4">1</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_clusters&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_clusters = </span><span class="s4">1</span>
    <span class="s3"># ensure two labels in subsample for RandomizedLogisticRegression</span>
    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;RandomizedLogisticRegression&quot;</span><span class="s1">:</span>
        <span class="s1">estimator.sample_fraction = </span><span class="s4">1</span>
    <span class="s3"># ensure non skipped trials for RANSACRegressor</span>
    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;RANSACRegressor&quot;</span><span class="s1">:</span>
        <span class="s1">estimator.residual_threshold = </span><span class="s4">0.5</span>

    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">set_random_state(estimator</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s1">msgs = [</span><span class="s2">r&quot;1 feature\(s\)&quot;</span><span class="s0">, </span><span class="s2">&quot;n_features = 1&quot;</span><span class="s0">, </span><span class="s2">&quot;n_features=1&quot;</span><span class="s1">]</span>

    <span class="s0">with </span><span class="s1">raises(ValueError</span><span class="s0">, </span><span class="s1">match=msgs</span><span class="s0">, </span><span class="s1">may_pass=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">check_fit1d(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check fitting 1d X array raises a ValueError</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = </span><span class="s4">3 </span><span class="s1">* rnd.uniform(size=(</span><span class="s4">20</span><span class="s1">))</span>
    <span class="s1">y = X.astype(int)</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_components&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_components = </span><span class="s4">1</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_clusters&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.n_clusters = </span><span class="s4">1</span>

    <span class="s1">set_random_state(estimator</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">raises(ValueError):</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_transformer_general(name</span><span class="s0">, </span><span class="s1">transformer</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s4">30</span><span class="s0">,</span>
        <span class="s1">centers=[[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s4">2</span><span class="s0">,</span>
        <span class="s1">cluster_std=</span><span class="s4">0.1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X = StandardScaler().fit_transform(X)</span>
    <span class="s1">X = _enforce_estimator_tags_X(transformer</span><span class="s0">, </span><span class="s1">X)</span>

    <span class="s0">if </span><span class="s1">readonly_memmap:</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y = create_memmap_backed_data([X</span><span class="s0">, </span><span class="s1">y])</span>

    <span class="s1">_check_transformer(name</span><span class="s0">, </span><span class="s1">transformer</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_transformer_data_not_an_array(name</span><span class="s0">, </span><span class="s1">transformer):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s4">30</span><span class="s0">,</span>
        <span class="s1">centers=[[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s4">2</span><span class="s0">,</span>
        <span class="s1">cluster_std=</span><span class="s4">0.1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X = StandardScaler().fit_transform(X)</span>
    <span class="s1">X = _enforce_estimator_tags_X(transformer</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">this_X = _NotAnArray(X)</span>
    <span class="s1">this_y = _NotAnArray(np.asarray(y))</span>
    <span class="s1">_check_transformer(name</span><span class="s0">, </span><span class="s1">transformer</span><span class="s0">, </span><span class="s1">this_X</span><span class="s0">, </span><span class="s1">this_y)</span>
    <span class="s3"># try the same with some list</span>
    <span class="s1">_check_transformer(name</span><span class="s0">, </span><span class="s1">transformer</span><span class="s0">, </span><span class="s1">X.tolist()</span><span class="s0">, </span><span class="s1">y.tolist())</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_transformers_unfitted(name</span><span class="s0">, </span><span class="s1">transformer):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = _regression_dataset()</span>

    <span class="s1">transformer = clone(transformer)</span>
    <span class="s0">with </span><span class="s1">raises(</span>
        <span class="s1">(AttributeError</span><span class="s0">, </span><span class="s1">ValueError)</span><span class="s0">,</span>
        <span class="s1">err_msg=(</span>
            <span class="s2">&quot;The unfitted &quot;</span>
            <span class="s2">f&quot;transformer </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">does not raise an error when &quot;</span>
            <span class="s2">&quot;transform is called. Perhaps use &quot;</span>
            <span class="s2">&quot;check_is_fitted in transform.&quot;</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">):</span>
        <span class="s1">transformer.transform(X)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_transformers_unfitted_stateless(name</span><span class="s0">, </span><span class="s1">transformer):</span>
    <span class="s5">&quot;&quot;&quot;Check that using transform without prior fitting 
    doesn't raise a NotFittedError for stateless transformers. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = rng.uniform(size=(</span><span class="s4">20</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(transformer</span><span class="s0">, </span><span class="s1">X)</span>

    <span class="s1">transformer = clone(transformer)</span>
    <span class="s1">X_trans = transformer.transform(X)</span>

    <span class="s0">assert </span><span class="s1">X_trans.shape[</span><span class="s4">0</span><span class="s1">] == X.shape[</span><span class="s4">0</span><span class="s1">]</span>


<span class="s0">def </span><span class="s1">_check_transformer(name</span><span class="s0">, </span><span class="s1">transformer_orig</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = np.asarray(X).shape</span>
    <span class="s1">transformer = clone(transformer_orig)</span>
    <span class="s1">set_random_state(transformer)</span>

    <span class="s3"># fit</span>

    <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
        <span class="s1">y_ = np.c_[np.asarray(y)</span><span class="s0">, </span><span class="s1">np.asarray(y)]</span>
        <span class="s1">y_[::</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">] *= </span><span class="s4">2</span>
        <span class="s0">if </span><span class="s1">isinstance(X</span><span class="s0">, </span><span class="s1">_NotAnArray):</span>
            <span class="s1">y_ = _NotAnArray(y_)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y_ = y</span>

    <span class="s1">transformer.fit(X</span><span class="s0">, </span><span class="s1">y_)</span>
    <span class="s3"># fit_transform method should work on non fitted estimator</span>
    <span class="s1">transformer_clone = clone(transformer)</span>
    <span class="s1">X_pred = transformer_clone.fit_transform(X</span><span class="s0">, </span><span class="s1">y=y_)</span>

    <span class="s0">if </span><span class="s1">isinstance(X_pred</span><span class="s0">, </span><span class="s1">tuple):</span>
        <span class="s0">for </span><span class="s1">x_pred </span><span class="s0">in </span><span class="s1">X_pred:</span>
            <span class="s0">assert </span><span class="s1">x_pred.shape[</span><span class="s4">0</span><span class="s1">] == n_samples</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s3"># check for consistent n_samples</span>
        <span class="s0">assert </span><span class="s1">X_pred.shape[</span><span class="s4">0</span><span class="s1">] == n_samples</span>

    <span class="s0">if </span><span class="s1">hasattr(transformer</span><span class="s0">, </span><span class="s2">&quot;transform&quot;</span><span class="s1">):</span>
        <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
            <span class="s1">X_pred2 = transformer.transform(X</span><span class="s0">, </span><span class="s1">y_)</span>
            <span class="s1">X_pred3 = transformer.fit_transform(X</span><span class="s0">, </span><span class="s1">y=y_)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">X_pred2 = transformer.transform(X)</span>
            <span class="s1">X_pred3 = transformer.fit_transform(X</span><span class="s0">, </span><span class="s1">y=y_)</span>

        <span class="s0">if </span><span class="s1">_safe_tags(transformer_orig</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;non_deterministic&quot;</span><span class="s1">):</span>
            <span class="s1">msg = name + </span><span class="s2">&quot; is non deterministic&quot;</span>
            <span class="s0">raise </span><span class="s1">SkipTest(msg)</span>
        <span class="s0">if </span><span class="s1">isinstance(X_pred</span><span class="s0">, </span><span class="s1">tuple) </span><span class="s0">and </span><span class="s1">isinstance(X_pred2</span><span class="s0">, </span><span class="s1">tuple):</span>
            <span class="s0">for </span><span class="s1">x_pred</span><span class="s0">, </span><span class="s1">x_pred2</span><span class="s0">, </span><span class="s1">x_pred3 </span><span class="s0">in </span><span class="s1">zip(X_pred</span><span class="s0">, </span><span class="s1">X_pred2</span><span class="s0">, </span><span class="s1">X_pred3):</span>
                <span class="s1">assert_allclose_dense_sparse(</span>
                    <span class="s1">x_pred</span><span class="s0">,</span>
                    <span class="s1">x_pred2</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s4">1e-2</span><span class="s0">,</span>
                    <span class="s1">err_msg=</span><span class="s2">&quot;fit_transform and transform outcomes not consistent in %s&quot;</span>
                    <span class="s1">% transformer</span><span class="s0">,</span>
                <span class="s1">)</span>
                <span class="s1">assert_allclose_dense_sparse(</span>
                    <span class="s1">x_pred</span><span class="s0">,</span>
                    <span class="s1">x_pred3</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s4">1e-2</span><span class="s0">,</span>
                    <span class="s1">err_msg=</span><span class="s2">&quot;consecutive fit_transform outcomes not consistent in %s&quot;</span>
                    <span class="s1">% transformer</span><span class="s0">,</span>
                <span class="s1">)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">assert_allclose_dense_sparse(</span>
                <span class="s1">X_pred</span><span class="s0">,</span>
                <span class="s1">X_pred2</span><span class="s0">,</span>
                <span class="s1">err_msg=</span><span class="s2">&quot;fit_transform and transform outcomes not consistent in %s&quot;</span>
                <span class="s1">% transformer</span><span class="s0">,</span>
                <span class="s1">atol=</span><span class="s4">1e-2</span><span class="s0">,</span>
            <span class="s1">)</span>
            <span class="s1">assert_allclose_dense_sparse(</span>
                <span class="s1">X_pred</span><span class="s0">,</span>
                <span class="s1">X_pred3</span><span class="s0">,</span>
                <span class="s1">atol=</span><span class="s4">1e-2</span><span class="s0">,</span>
                <span class="s1">err_msg=</span><span class="s2">&quot;consecutive fit_transform outcomes not consistent in %s&quot;</span>
                <span class="s1">% transformer</span><span class="s0">,</span>
            <span class="s1">)</span>
            <span class="s0">assert </span><span class="s1">_num_samples(X_pred2) == n_samples</span>
            <span class="s0">assert </span><span class="s1">_num_samples(X_pred3) == n_samples</span>

        <span class="s3"># raises error on malformed input for transform</span>
        <span class="s0">if </span><span class="s1">(</span>
            <span class="s1">hasattr(X</span><span class="s0">, </span><span class="s2">&quot;shape&quot;</span><span class="s1">)</span>
            <span class="s0">and not </span><span class="s1">_safe_tags(transformer</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;stateless&quot;</span><span class="s1">)</span>
            <span class="s0">and </span><span class="s1">X.ndim == </span><span class="s4">2</span>
            <span class="s0">and </span><span class="s1">X.shape[</span><span class="s4">1</span><span class="s1">] &gt; </span><span class="s4">1</span>
        <span class="s1">):</span>
            <span class="s3"># If it's not an array, it does not have a 'T' property</span>
            <span class="s0">with </span><span class="s1">raises(</span>
                <span class="s1">ValueError</span><span class="s0">,</span>
                <span class="s1">err_msg=(</span>
                    <span class="s2">f&quot;The transformer </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">does not raise an error &quot;</span>
                    <span class="s2">&quot;when the number of features in transform is different from &quot;</span>
                    <span class="s2">&quot;the number of features in fit.&quot;</span>
                <span class="s1">)</span><span class="s0">,</span>
            <span class="s1">):</span>
                <span class="s1">transformer.transform(X[:</span><span class="s0">, </span><span class="s1">:-</span><span class="s4">1</span><span class="s1">])</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">check_pipeline_consistency(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s0">if </span><span class="s1">_safe_tags(estimator_orig</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;non_deterministic&quot;</span><span class="s1">):</span>
        <span class="s1">msg = name + </span><span class="s2">&quot; is non deterministic&quot;</span>
        <span class="s0">raise </span><span class="s1">SkipTest(msg)</span>

    <span class="s3"># check that make_pipeline(est) gives same score as est</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s4">30</span><span class="s0">,</span>
        <span class="s1">centers=[[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s4">2</span><span class="s0">,</span>
        <span class="s1">cluster_std=</span><span class="s4">0.1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">kernel=rbf_kernel)</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">set_random_state(estimator)</span>
    <span class="s1">pipeline = make_pipeline(estimator)</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">pipeline.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">funcs = [</span><span class="s2">&quot;score&quot;</span><span class="s0">, </span><span class="s2">&quot;fit_transform&quot;</span><span class="s1">]</span>

    <span class="s0">for </span><span class="s1">func_name </span><span class="s0">in </span><span class="s1">funcs:</span>
        <span class="s1">func = getattr(estimator</span><span class="s0">, </span><span class="s1">func_name</span><span class="s0">, None</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">func </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">func_pipeline = getattr(pipeline</span><span class="s0">, </span><span class="s1">func_name)</span>
            <span class="s1">result = func(X</span><span class="s0">, </span><span class="s1">y)</span>
            <span class="s1">result_pipe = func_pipeline(X</span><span class="s0">, </span><span class="s1">y)</span>
            <span class="s1">assert_allclose_dense_sparse(result</span><span class="s0">, </span><span class="s1">result_pipe)</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">check_fit_score_takes_y(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check that all estimators accept an optional y</span>
    <span class="s3"># in fit and score so they can be used in pipelines</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s4">30</span>
    <span class="s1">X = rnd.uniform(size=(n_samples</span><span class="s0">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = np.arange(n_samples) % </span><span class="s4">3</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">set_random_state(estimator)</span>

    <span class="s1">funcs = [</span><span class="s2">&quot;fit&quot;</span><span class="s0">, </span><span class="s2">&quot;score&quot;</span><span class="s0">, </span><span class="s2">&quot;partial_fit&quot;</span><span class="s0">, </span><span class="s2">&quot;fit_predict&quot;</span><span class="s0">, </span><span class="s2">&quot;fit_transform&quot;</span><span class="s1">]</span>
    <span class="s0">for </span><span class="s1">func_name </span><span class="s0">in </span><span class="s1">funcs:</span>
        <span class="s1">func = getattr(estimator</span><span class="s0">, </span><span class="s1">func_name</span><span class="s0">, None</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">func </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">func(X</span><span class="s0">, </span><span class="s1">y)</span>
            <span class="s1">args = [p.name </span><span class="s0">for </span><span class="s1">p </span><span class="s0">in </span><span class="s1">signature(func).parameters.values()]</span>
            <span class="s0">if </span><span class="s1">args[</span><span class="s4">0</span><span class="s1">] == </span><span class="s2">&quot;self&quot;</span><span class="s1">:</span>
                <span class="s3"># available_if makes methods into functions</span>
                <span class="s3"># with an explicit &quot;self&quot;, so need to shift arguments</span>
                <span class="s1">args = args[</span><span class="s4">1</span><span class="s1">:]</span>
            <span class="s0">assert </span><span class="s1">args[</span><span class="s4">1</span><span class="s1">] </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;y&quot;</span><span class="s0">, </span><span class="s2">&quot;Y&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">(</span>
                <span class="s2">&quot;Expected y or Y as second argument for method &quot;</span>
                <span class="s2">&quot;%s of %s. Got arguments: %r.&quot;</span>
                <span class="s1">% (func_name</span><span class="s0">, </span><span class="s1">type(estimator).__name__</span><span class="s0">, </span><span class="s1">args)</span>
            <span class="s1">)</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">check_estimators_dtypes(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train_32 = </span><span class="s4">3 </span><span class="s1">* rnd.uniform(size=(</span><span class="s4">20</span><span class="s0">, </span><span class="s4">5</span><span class="s1">)).astype(np.float32)</span>
    <span class="s1">X_train_32 = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X_train_32)</span>
    <span class="s1">X_train_64 = X_train_32.astype(np.float64)</span>
    <span class="s1">X_train_int_64 = X_train_32.astype(np.int64)</span>
    <span class="s1">X_train_int_32 = X_train_32.astype(np.int32)</span>
    <span class="s1">y = X_train_int_64[:</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator_orig</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">methods = [</span><span class="s2">&quot;predict&quot;</span><span class="s0">, </span><span class="s2">&quot;transform&quot;</span><span class="s0">, </span><span class="s2">&quot;decision_function&quot;</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">]</span>

    <span class="s0">for </span><span class="s1">X_train </span><span class="s0">in </span><span class="s1">[X_train_32</span><span class="s0">, </span><span class="s1">X_train_64</span><span class="s0">, </span><span class="s1">X_train_int_64</span><span class="s0">, </span><span class="s1">X_train_int_32]:</span>
        <span class="s1">estimator = clone(estimator_orig)</span>
        <span class="s1">set_random_state(estimator</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">estimator.fit(X_train</span><span class="s0">, </span><span class="s1">y)</span>

        <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">methods:</span>
            <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s1">method):</span>
                <span class="s1">getattr(estimator</span><span class="s0">, </span><span class="s1">method)(X_train)</span>


<span class="s0">def </span><span class="s1">check_transformer_preserve_dtypes(name</span><span class="s0">, </span><span class="s1">transformer_orig):</span>
    <span class="s3"># check that dtype are preserved meaning if input X is of some dtype</span>
    <span class="s3"># X_transformed should be from the same dtype.</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s4">30</span><span class="s0">,</span>
        <span class="s1">centers=[[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">cluster_std=</span><span class="s4">0.1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X = StandardScaler().fit_transform(X)</span>
    <span class="s1">X = _enforce_estimator_tags_X(transformer_orig</span><span class="s0">, </span><span class="s1">X)</span>

    <span class="s0">for </span><span class="s1">dtype </span><span class="s0">in </span><span class="s1">_safe_tags(transformer_orig</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;preserves_dtype&quot;</span><span class="s1">):</span>
        <span class="s1">X_cast = X.astype(dtype)</span>
        <span class="s1">transformer = clone(transformer_orig)</span>
        <span class="s1">set_random_state(transformer)</span>
        <span class="s1">X_trans1 = transformer.fit_transform(X_cast</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s1">X_trans2 = transformer.fit(X_cast</span><span class="s0">, </span><span class="s1">y).transform(X_cast)</span>

        <span class="s0">for </span><span class="s1">Xt</span><span class="s0">, </span><span class="s1">method </span><span class="s0">in </span><span class="s1">zip([X_trans1</span><span class="s0">, </span><span class="s1">X_trans2]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;fit_transform&quot;</span><span class="s0">, </span><span class="s2">&quot;transform&quot;</span><span class="s1">]):</span>
            <span class="s0">if </span><span class="s1">isinstance(Xt</span><span class="s0">, </span><span class="s1">tuple):</span>
                <span class="s3"># cross-decompostion returns a tuple of (x_scores, y_scores)</span>
                <span class="s3"># when given y with fit_transform; only check the first element</span>
                <span class="s1">Xt = Xt[</span><span class="s4">0</span><span class="s1">]</span>

            <span class="s3"># check that the output dtype is preserved</span>
            <span class="s0">assert </span><span class="s1">Xt.dtype == dtype</span><span class="s0">, </span><span class="s1">(</span>
                <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">(method=</span><span class="s0">{</span><span class="s1">method</span><span class="s0">}</span><span class="s2">) does not preserve dtype. &quot;</span>
                <span class="s2">f&quot;Original/Expected dtype=</span><span class="s0">{</span><span class="s1">dtype.__name__</span><span class="s0">}</span><span class="s2">, got dtype=</span><span class="s0">{</span><span class="s1">Xt.dtype</span><span class="s0">}</span><span class="s2">.&quot;</span>
            <span class="s1">)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_estimators_empty_data_messages(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s1">e = clone(estimator_orig)</span>
    <span class="s1">set_random_state(e</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s1">X_zero_samples = np.empty(</span><span class="s4">0</span><span class="s1">).reshape(</span><span class="s4">0</span><span class="s0">, </span><span class="s4">3</span><span class="s1">)</span>
    <span class="s3"># The precise message can change depending on whether X or y is</span>
    <span class="s3"># validated first. Let us test the type of exception only:</span>
    <span class="s1">err_msg = (</span>
        <span class="s2">f&quot;The estimator </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">does not raise a ValueError when an &quot;</span>
        <span class="s2">&quot;empty data is used to train. Perhaps use check_array in train.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">raises(ValueError</span><span class="s0">, </span><span class="s1">err_msg=err_msg):</span>
        <span class="s1">e.fit(X_zero_samples</span><span class="s0">, </span><span class="s1">[])</span>

    <span class="s1">X_zero_features = np.empty(</span><span class="s4">0</span><span class="s1">).reshape(</span><span class="s4">12</span><span class="s0">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s3"># the following y should be accepted by both classifiers and regressors</span>
    <span class="s3"># and ignored by unsupervised models</span>
    <span class="s1">y = _enforce_estimator_tags_y(e</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]))</span>
    <span class="s1">msg = </span><span class="s2">r&quot;0 feature\(s\) \(shape=\(\d*, 0\)\) while a minimum of \d* &quot; &quot;is required.&quot;</span>
    <span class="s0">with </span><span class="s1">raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">e.fit(X_zero_features</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_estimators_nan_inf(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Checks that Estimator X's do not contain NaN or inf.</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train_finite = _enforce_estimator_tags_X(</span>
        <span class="s1">estimator_orig</span><span class="s0">, </span><span class="s1">rnd.uniform(size=(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">)</span>
    <span class="s1">X_train_nan = rnd.uniform(size=(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">X_train_nan[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">] = np.nan</span>
    <span class="s1">X_train_inf = rnd.uniform(size=(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">X_train_inf[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">] = np.inf</span>
    <span class="s1">y = np.ones(</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">y[:</span><span class="s4">5</span><span class="s1">] = </span><span class="s4">0</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator_orig</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">error_string_fit = </span><span class="s2">f&quot;Estimator </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">doesn't check for NaN and inf in fit.&quot;</span>
    <span class="s1">error_string_predict = </span><span class="s2">f&quot;Estimator </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">doesn't check for NaN and inf in predict.&quot;</span>
    <span class="s1">error_string_transform = (</span>
        <span class="s2">f&quot;Estimator </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">doesn't check for NaN and inf in transform.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">for </span><span class="s1">X_train </span><span class="s0">in </span><span class="s1">[X_train_nan</span><span class="s0">, </span><span class="s1">X_train_inf]:</span>
        <span class="s3"># catch deprecation warnings</span>
        <span class="s0">with </span><span class="s1">ignore_warnings(category=FutureWarning):</span>
            <span class="s1">estimator = clone(estimator_orig)</span>
            <span class="s1">set_random_state(estimator</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>
            <span class="s3"># try to fit</span>
            <span class="s0">with </span><span class="s1">raises(ValueError</span><span class="s0">, </span><span class="s1">match=[</span><span class="s2">&quot;inf&quot;</span><span class="s0">, </span><span class="s2">&quot;NaN&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">err_msg=error_string_fit):</span>
                <span class="s1">estimator.fit(X_train</span><span class="s0">, </span><span class="s1">y)</span>
            <span class="s3"># actually fit</span>
            <span class="s1">estimator.fit(X_train_finite</span><span class="s0">, </span><span class="s1">y)</span>

            <span class="s3"># predict</span>
            <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;predict&quot;</span><span class="s1">):</span>
                <span class="s0">with </span><span class="s1">raises(</span>
                    <span class="s1">ValueError</span><span class="s0">,</span>
                    <span class="s1">match=[</span><span class="s2">&quot;inf&quot;</span><span class="s0">, </span><span class="s2">&quot;NaN&quot;</span><span class="s1">]</span><span class="s0">,</span>
                    <span class="s1">err_msg=error_string_predict</span><span class="s0">,</span>
                <span class="s1">):</span>
                    <span class="s1">estimator.predict(X_train)</span>

            <span class="s3"># transform</span>
            <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;transform&quot;</span><span class="s1">):</span>
                <span class="s0">with </span><span class="s1">raises(</span>
                    <span class="s1">ValueError</span><span class="s0">,</span>
                    <span class="s1">match=[</span><span class="s2">&quot;inf&quot;</span><span class="s0">, </span><span class="s2">&quot;NaN&quot;</span><span class="s1">]</span><span class="s0">,</span>
                    <span class="s1">err_msg=error_string_transform</span><span class="s0">,</span>
                <span class="s1">):</span>
                    <span class="s1">estimator.transform(X_train)</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">check_nonsquare_error(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s5">&quot;&quot;&quot;Test that error is thrown when non-square data provided.&quot;&quot;&quot;</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(n_samples=</span><span class="s4">20</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">estimator = clone(estimator_orig)</span>

    <span class="s0">with </span><span class="s1">raises(</span>
        <span class="s1">ValueError</span><span class="s0">,</span>
        <span class="s1">err_msg=(</span>
            <span class="s2">f&quot;The pairwise estimator </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">does not raise an error on non-square data&quot;</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">):</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">check_estimators_pickle(name</span><span class="s0">, </span><span class="s1">estimator_orig</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot;Test that we can pickle all estimators.&quot;&quot;&quot;</span>
    <span class="s1">check_methods = [</span><span class="s2">&quot;predict&quot;</span><span class="s0">, </span><span class="s2">&quot;transform&quot;</span><span class="s0">, </span><span class="s2">&quot;decision_function&quot;</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">]</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s4">30</span><span class="s0">,</span>
        <span class="s1">centers=[[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s4">2</span><span class="s0">,</span>
        <span class="s1">cluster_std=</span><span class="s4">0.1</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">kernel=rbf_kernel)</span>

    <span class="s1">tags = _safe_tags(estimator_orig)</span>
    <span class="s3"># include NaN values when the estimator should deal with them</span>
    <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;allow_nan&quot;</span><span class="s1">]:</span>
        <span class="s3"># set randomly 10 elements to np.nan</span>
        <span class="s1">rng = np.random.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
        <span class="s1">mask = rng.choice(X.size</span><span class="s0">, </span><span class="s4">10</span><span class="s0">, </span><span class="s1">replace=</span><span class="s0">False</span><span class="s1">)</span>
        <span class="s1">X.reshape(-</span><span class="s4">1</span><span class="s1">)[mask] = np.nan</span>

    <span class="s1">estimator = clone(estimator_orig)</span>

    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">set_random_state(estimator)</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">readonly_memmap:</span>
        <span class="s1">unpickled_estimator = create_memmap_backed_data(estimator)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s3"># pickle and unpickle!</span>
        <span class="s1">pickled_estimator = pickle.dumps(estimator)</span>
        <span class="s1">module_name = estimator.__module__</span>
        <span class="s0">if </span><span class="s1">module_name.startswith(</span><span class="s2">&quot;sklearn.&quot;</span><span class="s1">) </span><span class="s0">and not </span><span class="s1">(</span>
            <span class="s2">&quot;test_&quot; </span><span class="s0">in </span><span class="s1">module_name </span><span class="s0">or </span><span class="s1">module_name.endswith(</span><span class="s2">&quot;_testing&quot;</span><span class="s1">)</span>
        <span class="s1">):</span>
            <span class="s3"># strict check for sklearn estimators that are not implemented in test</span>
            <span class="s3"># modules.</span>
            <span class="s0">assert </span><span class="s6">b&quot;version&quot; </span><span class="s0">in </span><span class="s1">pickled_estimator</span>
        <span class="s1">unpickled_estimator = pickle.loads(pickled_estimator)</span>

    <span class="s1">result = dict()</span>
    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">check_methods:</span>
        <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s1">method):</span>
            <span class="s1">result[method] = getattr(estimator</span><span class="s0">, </span><span class="s1">method)(X)</span>

    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">result:</span>
        <span class="s1">unpickled_result = getattr(unpickled_estimator</span><span class="s0">, </span><span class="s1">method)(X)</span>
        <span class="s1">assert_allclose_dense_sparse(result[method]</span><span class="s0">, </span><span class="s1">unpickled_result)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_estimators_partial_fit_n_features(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check if number of features changes between calls to partial_fit.</span>
    <span class="s0">if not </span><span class="s1">hasattr(estimator_orig</span><span class="s0">, </span><span class="s2">&quot;partial_fit&quot;</span><span class="s1">):</span>
        <span class="s0">return</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(n_samples=</span><span class="s4">50</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator_orig</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">try</span><span class="s1">:</span>
        <span class="s0">if </span><span class="s1">is_classifier(estimator):</span>
            <span class="s1">classes = np.unique(y)</span>
            <span class="s1">estimator.partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=classes)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">estimator.partial_fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">except </span><span class="s1">NotImplementedError:</span>
        <span class="s0">return</span>

    <span class="s0">with </span><span class="s1">raises(</span>
        <span class="s1">ValueError</span><span class="s0">,</span>
        <span class="s1">err_msg=(</span>
            <span class="s2">f&quot;The estimator </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">does not raise an error when the &quot;</span>
            <span class="s2">&quot;number of features changes between calls to partial_fit.&quot;</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">):</span>
        <span class="s1">estimator.partial_fit(X[:</span><span class="s0">, </span><span class="s1">:-</span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_classifier_multioutput(name</span><span class="s0">, </span><span class="s1">estimator):</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_labels</span><span class="s0">, </span><span class="s1">n_classes = </span><span class="s4">42</span><span class="s0">, </span><span class="s4">5</span><span class="s0">, </span><span class="s4">3</span>
    <span class="s1">tags = _safe_tags(estimator)</span>
    <span class="s1">estimator = clone(estimator)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_multilabel_classification(</span>
        <span class="s1">random_state=</span><span class="s4">42</span><span class="s0">, </span><span class="s1">n_samples=n_samples</span><span class="s0">, </span><span class="s1">n_labels=n_labels</span><span class="s0">, </span><span class="s1">n_classes=n_classes</span>
    <span class="s1">)</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">y_pred = estimator.predict(X)</span>

    <span class="s0">assert </span><span class="s1">y_pred.shape == (n_samples</span><span class="s0">, </span><span class="s1">n_classes)</span><span class="s0">, </span><span class="s1">(</span>
        <span class="s2">&quot;The shape of the prediction for multioutput data is &quot;</span>
        <span class="s2">&quot;incorrect. Expected {}, got {}.&quot;</span><span class="s1">.format((n_samples</span><span class="s0">, </span><span class="s1">n_labels)</span><span class="s0">, </span><span class="s1">y_pred.shape)</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">y_pred.dtype.kind == </span><span class="s2">&quot;i&quot;</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;decision_function&quot;</span><span class="s1">):</span>
        <span class="s1">decision = estimator.decision_function(X)</span>
        <span class="s0">assert </span><span class="s1">isinstance(decision</span><span class="s0">, </span><span class="s1">np.ndarray)</span>
        <span class="s0">assert </span><span class="s1">decision.shape == (n_samples</span><span class="s0">, </span><span class="s1">n_classes)</span><span class="s0">, </span><span class="s1">(</span>
            <span class="s2">&quot;The shape of the decision function output for &quot;</span>
            <span class="s2">&quot;multioutput data is incorrect. Expected {}, got {}.&quot;</span><span class="s1">.format(</span>
                <span class="s1">(n_samples</span><span class="s0">, </span><span class="s1">n_classes)</span><span class="s0">, </span><span class="s1">decision.shape</span>
            <span class="s1">)</span>
        <span class="s1">)</span>

        <span class="s1">dec_pred = (decision &gt; </span><span class="s4">0</span><span class="s1">).astype(int)</span>
        <span class="s1">dec_exp = estimator.classes_[dec_pred]</span>
        <span class="s1">assert_array_equal(dec_exp</span><span class="s0">, </span><span class="s1">y_pred)</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">):</span>
        <span class="s1">y_prob = estimator.predict_proba(X)</span>

        <span class="s0">if </span><span class="s1">isinstance(y_prob</span><span class="s0">, </span><span class="s1">list) </span><span class="s0">and not </span><span class="s1">tags[</span><span class="s2">&quot;poor_score&quot;</span><span class="s1">]:</span>
            <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(n_classes):</span>
                <span class="s0">assert </span><span class="s1">y_prob[i].shape == (n_samples</span><span class="s0">, </span><span class="s4">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span>
                    <span class="s2">&quot;The shape of the probability for multioutput data is&quot;</span>
                    <span class="s2">&quot; incorrect. Expected {}, got {}.&quot;</span><span class="s1">.format(</span>
                        <span class="s1">(n_samples</span><span class="s0">, </span><span class="s4">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">y_prob[i].shape</span>
                    <span class="s1">)</span>
                <span class="s1">)</span>
                <span class="s1">assert_array_equal(</span>
                    <span class="s1">np.argmax(y_prob[i]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">).astype(int)</span><span class="s0">, </span><span class="s1">y_pred[:</span><span class="s0">, </span><span class="s1">i]</span>
                <span class="s1">)</span>
        <span class="s0">elif not </span><span class="s1">tags[</span><span class="s2">&quot;poor_score&quot;</span><span class="s1">]:</span>
            <span class="s0">assert </span><span class="s1">y_prob.shape == (n_samples</span><span class="s0">, </span><span class="s1">n_classes)</span><span class="s0">, </span><span class="s1">(</span>
                <span class="s2">&quot;The shape of the probability for multioutput data is&quot;</span>
                <span class="s2">&quot; incorrect. Expected {}, got {}.&quot;</span><span class="s1">.format(</span>
                    <span class="s1">(n_samples</span><span class="s0">, </span><span class="s1">n_classes)</span><span class="s0">, </span><span class="s1">y_prob.shape</span>
                <span class="s1">)</span>
            <span class="s1">)</span>
            <span class="s1">assert_array_equal(y_prob.round().astype(int)</span><span class="s0">, </span><span class="s1">y_pred)</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;decision_function&quot;</span><span class="s1">) </span><span class="s0">and </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">):</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(n_classes):</span>
            <span class="s1">y_proba = estimator.predict_proba(X)[:</span><span class="s0">, </span><span class="s1">i]</span>
            <span class="s1">y_decision = estimator.decision_function(X)</span>
            <span class="s1">assert_array_equal(rankdata(y_proba)</span><span class="s0">, </span><span class="s1">rankdata(y_decision[:</span><span class="s0">, </span><span class="s1">i]))</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_regressor_multioutput(name</span><span class="s0">, </span><span class="s1">estimator):</span>
    <span class="s1">estimator = clone(estimator)</span>
    <span class="s1">n_samples = n_features = </span><span class="s4">10</span>

    <span class="s0">if not </span><span class="s1">_is_pairwise_metric(estimator):</span>
        <span class="s1">n_samples = n_samples + </span><span class="s4">1</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">random_state=</span><span class="s4">42</span><span class="s0">, </span><span class="s1">n_targets=</span><span class="s4">5</span><span class="s0">, </span><span class="s1">n_samples=n_samples</span><span class="s0">, </span><span class="s1">n_features=n_features</span>
    <span class="s1">)</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator</span><span class="s0">, </span><span class="s1">X)</span>

    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">y_pred = estimator.predict(X)</span>

    <span class="s0">assert </span><span class="s1">y_pred.dtype == np.dtype(</span><span class="s2">&quot;float64&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span>
        <span class="s2">&quot;Multioutput predictions by a regressor are expected to be&quot;</span>
        <span class="s2">&quot; floating-point precision. Got {} instead&quot;</span><span class="s1">.format(y_pred.dtype)</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">y_pred.shape == y.shape</span><span class="s0">, </span><span class="s1">(</span>
        <span class="s2">&quot;The shape of the prediction for multioutput data is incorrect.&quot;</span>
        <span class="s2">&quot; Expected {}, got {}.&quot;</span>
    <span class="s1">)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_clustering(name</span><span class="s0">, </span><span class="s1">clusterer_orig</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s1">clusterer = clone(clusterer_orig)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(n_samples=</span><span class="s4">50</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = shuffle(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">7</span><span class="s1">)</span>
    <span class="s1">X = StandardScaler().fit_transform(X)</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">7</span><span class="s1">)</span>
    <span class="s1">X_noise = np.concatenate([X</span><span class="s0">, </span><span class="s1">rng.uniform(low=-</span><span class="s4">3</span><span class="s0">, </span><span class="s1">high=</span><span class="s4">3</span><span class="s0">, </span><span class="s1">size=(</span><span class="s4">5</span><span class="s0">, </span><span class="s4">2</span><span class="s1">))])</span>

    <span class="s0">if </span><span class="s1">readonly_memmap:</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">X_noise = create_memmap_backed_data([X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">X_noise])</span>

    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = X.shape</span>
    <span class="s3"># catch deprecation and neighbors warnings</span>
    <span class="s0">if </span><span class="s1">hasattr(clusterer</span><span class="s0">, </span><span class="s2">&quot;n_clusters&quot;</span><span class="s1">):</span>
        <span class="s1">clusterer.set_params(n_clusters=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">set_random_state(clusterer)</span>
    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;AffinityPropagation&quot;</span><span class="s1">:</span>
        <span class="s1">clusterer.set_params(preference=-</span><span class="s4">100</span><span class="s1">)</span>
        <span class="s1">clusterer.set_params(max_iter=</span><span class="s4">100</span><span class="s1">)</span>

    <span class="s3"># fit</span>
    <span class="s1">clusterer.fit(X)</span>
    <span class="s3"># with lists</span>
    <span class="s1">clusterer.fit(X.tolist())</span>

    <span class="s1">pred = clusterer.labels_</span>
    <span class="s0">assert </span><span class="s1">pred.shape == (n_samples</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">adjusted_rand_score(pred</span><span class="s0">, </span><span class="s1">y) &gt; </span><span class="s4">0.4</span>
    <span class="s0">if </span><span class="s1">_safe_tags(clusterer</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;non_deterministic&quot;</span><span class="s1">):</span>
        <span class="s0">return</span>
    <span class="s1">set_random_state(clusterer)</span>
    <span class="s0">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s1">pred2 = clusterer.fit_predict(X)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s0">, </span><span class="s1">pred2)</span>

    <span class="s3"># fit_predict(X) and labels_ should be of type int</span>
    <span class="s0">assert </span><span class="s1">pred.dtype </span><span class="s0">in </span><span class="s1">[np.dtype(</span><span class="s2">&quot;int32&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.dtype(</span><span class="s2">&quot;int64&quot;</span><span class="s1">)]</span>
    <span class="s0">assert </span><span class="s1">pred2.dtype </span><span class="s0">in </span><span class="s1">[np.dtype(</span><span class="s2">&quot;int32&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.dtype(</span><span class="s2">&quot;int64&quot;</span><span class="s1">)]</span>

    <span class="s3"># Add noise to X to test the possible values of the labels</span>
    <span class="s1">labels = clusterer.fit_predict(X_noise)</span>

    <span class="s3"># There should be at least one sample in every cluster. Equivalently</span>
    <span class="s3"># labels_ should contain all the consecutive values between its</span>
    <span class="s3"># min and its max.</span>
    <span class="s1">labels_sorted = np.unique(labels)</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">labels_sorted</span><span class="s0">, </span><span class="s1">np.arange(labels_sorted[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">labels_sorted[-</span><span class="s4">1</span><span class="s1">] + </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">)</span>

    <span class="s3"># Labels are expected to start at 0 (no noise) or -1 (if noise)</span>
    <span class="s0">assert </span><span class="s1">labels_sorted[</span><span class="s4">0</span><span class="s1">] </span><span class="s0">in </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s3"># Labels should be less than n_clusters - 1</span>
    <span class="s0">if </span><span class="s1">hasattr(clusterer</span><span class="s0">, </span><span class="s2">&quot;n_clusters&quot;</span><span class="s1">):</span>
        <span class="s1">n_clusters = getattr(clusterer</span><span class="s0">, </span><span class="s2">&quot;n_clusters&quot;</span><span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">n_clusters - </span><span class="s4">1 </span><span class="s1">&gt;= labels_sorted[-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s3"># else labels should be less than max(labels_) which is necessarily true</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_clusterer_compute_labels_predict(name</span><span class="s0">, </span><span class="s1">clusterer_orig):</span>
    <span class="s5">&quot;&quot;&quot;Check that predict is invariant of compute_labels.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(n_samples=</span><span class="s4">20</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clusterer = clone(clusterer_orig)</span>
    <span class="s1">set_random_state(clusterer)</span>

    <span class="s0">if </span><span class="s1">hasattr(clusterer</span><span class="s0">, </span><span class="s2">&quot;compute_labels&quot;</span><span class="s1">):</span>
        <span class="s3"># MiniBatchKMeans</span>
        <span class="s1">X_pred1 = clusterer.fit(X).predict(X)</span>
        <span class="s1">clusterer.set_params(compute_labels=</span><span class="s0">False</span><span class="s1">)</span>
        <span class="s1">X_pred2 = clusterer.fit(X).predict(X)</span>
        <span class="s1">assert_array_equal(X_pred1</span><span class="s0">, </span><span class="s1">X_pred2)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_classifiers_one_label(name</span><span class="s0">, </span><span class="s1">classifier_orig):</span>
    <span class="s1">error_string_fit = </span><span class="s2">&quot;Classifier can't train when only one class is present.&quot;</span>
    <span class="s1">error_string_predict = </span><span class="s2">&quot;Classifier can't predict when only one class is present.&quot;</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train = rnd.uniform(size=(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">X_test = rnd.uniform(size=(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">y = np.ones(</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s3"># catch deprecation warnings</span>
    <span class="s0">with </span><span class="s1">ignore_warnings(category=FutureWarning):</span>
        <span class="s1">classifier = clone(classifier_orig)</span>
        <span class="s0">with </span><span class="s1">raises(</span>
            <span class="s1">ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;class&quot;</span><span class="s0">, </span><span class="s1">may_pass=</span><span class="s0">True, </span><span class="s1">err_msg=error_string_fit</span>
        <span class="s1">) </span><span class="s0">as </span><span class="s1">cm:</span>
            <span class="s1">classifier.fit(X_train</span><span class="s0">, </span><span class="s1">y)</span>

        <span class="s0">if </span><span class="s1">cm.raised_and_matched:</span>
            <span class="s3"># ValueError was raised with proper error message</span>
            <span class="s0">return</span>

        <span class="s1">assert_array_equal(classifier.predict(X_test)</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">err_msg=error_string_predict)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_classifiers_one_label_sample_weights(name</span><span class="s0">, </span><span class="s1">classifier_orig):</span>
    <span class="s5">&quot;&quot;&quot;Check that classifiers accepting sample_weight fit or throws a ValueError with 
    an explicit message if the problem is reduced to one class. 
    &quot;&quot;&quot;</span>
    <span class="s1">error_fit = (</span>
        <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">failed when fitted on one label after sample_weight trimming. Error &quot;</span>
        <span class="s2">&quot;message is not explicit, it should have 'class'.&quot;</span>
    <span class="s1">)</span>
    <span class="s1">error_predict = </span><span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">prediction results should only output the remaining class.&quot;</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s3"># X should be square for test on SVC with precomputed kernel</span>
    <span class="s1">X_train = rnd.uniform(size=(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">X_test = rnd.uniform(size=(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">y = np.arange(</span><span class="s4">10</span><span class="s1">) % </span><span class="s4">2</span>
    <span class="s1">sample_weight = y.copy()  </span><span class="s3"># select a single class</span>
    <span class="s1">classifier = clone(classifier_orig)</span>

    <span class="s0">if </span><span class="s1">has_fit_parameter(classifier</span><span class="s0">, </span><span class="s2">&quot;sample_weight&quot;</span><span class="s1">):</span>
        <span class="s1">match = [</span><span class="s2">r&quot;\bclass(es)?\b&quot;</span><span class="s0">, </span><span class="s1">error_predict]</span>
        <span class="s1">err_type</span><span class="s0">, </span><span class="s1">err_msg = (AssertionError</span><span class="s0">, </span><span class="s1">ValueError)</span><span class="s0">, </span><span class="s1">error_fit</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">match = </span><span class="s2">r&quot;\bsample_weight\b&quot;</span>
        <span class="s1">err_type</span><span class="s0">, </span><span class="s1">err_msg = (TypeError</span><span class="s0">, </span><span class="s1">ValueError)</span><span class="s0">, None</span>

    <span class="s0">with </span><span class="s1">raises(err_type</span><span class="s0">, </span><span class="s1">match=match</span><span class="s0">, </span><span class="s1">may_pass=</span><span class="s0">True, </span><span class="s1">err_msg=err_msg) </span><span class="s0">as </span><span class="s1">cm:</span>
        <span class="s1">classifier.fit(X_train</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
        <span class="s0">if </span><span class="s1">cm.raised_and_matched:</span>
            <span class="s3"># raise the proper error type with the proper error message</span>
            <span class="s0">return</span>
        <span class="s3"># for estimators that do not fail, they should be able to predict the only</span>
        <span class="s3"># class remaining during fit</span>
        <span class="s1">assert_array_equal(</span>
            <span class="s1">classifier.predict(X_test)</span><span class="s0">, </span><span class="s1">np.ones(</span><span class="s4">10</span><span class="s1">)</span><span class="s0">, </span><span class="s1">err_msg=error_predict</span>
        <span class="s1">)</span>


<span class="s1">@ignore_warnings  </span><span class="s3"># Warnings are raised by decision function</span>
<span class="s0">def </span><span class="s1">check_classifiers_train(</span>
    <span class="s1">name</span><span class="s0">, </span><span class="s1">classifier_orig</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">False, </span><span class="s1">X_dtype=</span><span class="s2">&quot;float64&quot;</span>
<span class="s1">):</span>
    <span class="s1">X_m</span><span class="s0">, </span><span class="s1">y_m = make_blobs(n_samples=</span><span class="s4">300</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_m = X_m.astype(X_dtype)</span>
    <span class="s1">X_m</span><span class="s0">, </span><span class="s1">y_m = shuffle(X_m</span><span class="s0">, </span><span class="s1">y_m</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">7</span><span class="s1">)</span>
    <span class="s1">X_m = StandardScaler().fit_transform(X_m)</span>
    <span class="s3"># generate binary problem from multi-class one</span>
    <span class="s1">y_b = y_m[y_m != </span><span class="s4">2</span><span class="s1">]</span>
    <span class="s1">X_b = X_m[y_m != </span><span class="s4">2</span><span class="s1">]</span>

    <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;BernoulliNB&quot;</span><span class="s0">, </span><span class="s2">&quot;MultinomialNB&quot;</span><span class="s0">, </span><span class="s2">&quot;ComplementNB&quot;</span><span class="s0">, </span><span class="s2">&quot;CategoricalNB&quot;</span><span class="s1">]:</span>
        <span class="s1">X_m -= X_m.min()</span>
        <span class="s1">X_b -= X_b.min()</span>

    <span class="s0">if </span><span class="s1">readonly_memmap:</span>
        <span class="s1">X_m</span><span class="s0">, </span><span class="s1">y_m</span><span class="s0">, </span><span class="s1">X_b</span><span class="s0">, </span><span class="s1">y_b = create_memmap_backed_data([X_m</span><span class="s0">, </span><span class="s1">y_m</span><span class="s0">, </span><span class="s1">X_b</span><span class="s0">, </span><span class="s1">y_b])</span>

    <span class="s1">problems = [(X_b</span><span class="s0">, </span><span class="s1">y_b)]</span>
    <span class="s1">tags = _safe_tags(classifier_orig)</span>
    <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;binary_only&quot;</span><span class="s1">]:</span>
        <span class="s1">problems.append((X_m</span><span class="s0">, </span><span class="s1">y_m))</span>

    <span class="s0">for </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y </span><span class="s0">in </span><span class="s1">problems:</span>
        <span class="s1">classes = np.unique(y)</span>
        <span class="s1">n_classes = len(classes)</span>
        <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = X.shape</span>
        <span class="s1">classifier = clone(classifier_orig)</span>
        <span class="s1">X = _enforce_estimator_tags_X(classifier</span><span class="s0">, </span><span class="s1">X)</span>
        <span class="s1">y = _enforce_estimator_tags_y(classifier</span><span class="s0">, </span><span class="s1">y)</span>

        <span class="s1">set_random_state(classifier)</span>
        <span class="s3"># raises error on malformed input for fit</span>
        <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
            <span class="s0">with </span><span class="s1">raises(</span>
                <span class="s1">ValueError</span><span class="s0">,</span>
                <span class="s1">err_msg=(</span>
                    <span class="s2">f&quot;The classifier </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">does not raise an error when &quot;</span>
                    <span class="s2">&quot;incorrect/malformed input data for fit is passed. The number &quot;</span>
                    <span class="s2">&quot;of training examples is not the same as the number of &quot;</span>
                    <span class="s2">&quot;labels. Perhaps use check_X_y in fit.&quot;</span>
                <span class="s1">)</span><span class="s0">,</span>
            <span class="s1">):</span>
                <span class="s1">classifier.fit(X</span><span class="s0">, </span><span class="s1">y[:-</span><span class="s4">1</span><span class="s1">])</span>

        <span class="s3"># fit</span>
        <span class="s1">classifier.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s3"># with lists</span>
        <span class="s1">classifier.fit(X.tolist()</span><span class="s0">, </span><span class="s1">y.tolist())</span>
        <span class="s0">assert </span><span class="s1">hasattr(classifier</span><span class="s0">, </span><span class="s2">&quot;classes_&quot;</span><span class="s1">)</span>
        <span class="s1">y_pred = classifier.predict(X)</span>

        <span class="s0">assert </span><span class="s1">y_pred.shape == (n_samples</span><span class="s0">,</span><span class="s1">)</span>
        <span class="s3"># training set performance</span>
        <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;poor_score&quot;</span><span class="s1">]:</span>
            <span class="s0">assert </span><span class="s1">accuracy_score(y</span><span class="s0">, </span><span class="s1">y_pred) &gt; </span><span class="s4">0.83</span>

        <span class="s3"># raises error on malformed input for predict</span>
        <span class="s1">msg_pairwise = (</span>
            <span class="s2">&quot;The classifier {} does not raise an error when shape of X in &quot;</span>
            <span class="s2">&quot; {} is not equal to (n_test_samples, n_training_samples)&quot;</span>
        <span class="s1">)</span>
        <span class="s1">msg = (</span>
            <span class="s2">&quot;The classifier {} does not raise an error when the number of &quot;</span>
            <span class="s2">&quot;features in {} is different from the number of features in &quot;</span>
            <span class="s2">&quot;fit.&quot;</span>
        <span class="s1">)</span>

        <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
            <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;pairwise&quot;</span><span class="s1">]:</span>
                <span class="s0">with </span><span class="s1">raises(</span>
                    <span class="s1">ValueError</span><span class="s0">,</span>
                    <span class="s1">err_msg=msg_pairwise.format(name</span><span class="s0">, </span><span class="s2">&quot;predict&quot;</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">):</span>
                    <span class="s1">classifier.predict(X.reshape(-</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">))</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">with </span><span class="s1">raises(ValueError</span><span class="s0">, </span><span class="s1">err_msg=msg.format(name</span><span class="s0">, </span><span class="s2">&quot;predict&quot;</span><span class="s1">)):</span>
                    <span class="s1">classifier.predict(X.T)</span>
        <span class="s0">if </span><span class="s1">hasattr(classifier</span><span class="s0">, </span><span class="s2">&quot;decision_function&quot;</span><span class="s1">):</span>
            <span class="s0">try</span><span class="s1">:</span>
                <span class="s3"># decision_function agrees with predict</span>
                <span class="s1">decision = classifier.decision_function(X)</span>
                <span class="s0">if </span><span class="s1">n_classes == </span><span class="s4">2</span><span class="s1">:</span>
                    <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;multioutput_only&quot;</span><span class="s1">]:</span>
                        <span class="s0">assert </span><span class="s1">decision.shape == (n_samples</span><span class="s0">,</span><span class="s1">)</span>
                    <span class="s0">else</span><span class="s1">:</span>
                        <span class="s0">assert </span><span class="s1">decision.shape == (n_samples</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>
                    <span class="s1">dec_pred = (decision.ravel() &gt; </span><span class="s4">0</span><span class="s1">).astype(int)</span>
                    <span class="s1">assert_array_equal(dec_pred</span><span class="s0">, </span><span class="s1">y_pred)</span>
                <span class="s0">else</span><span class="s1">:</span>
                    <span class="s0">assert </span><span class="s1">decision.shape == (n_samples</span><span class="s0">, </span><span class="s1">n_classes)</span>
                    <span class="s1">assert_array_equal(np.argmax(decision</span><span class="s0">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">y_pred)</span>

                <span class="s3"># raises error on malformed input for decision_function</span>
                <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
                    <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;pairwise&quot;</span><span class="s1">]:</span>
                        <span class="s0">with </span><span class="s1">raises(</span>
                            <span class="s1">ValueError</span><span class="s0">,</span>
                            <span class="s1">err_msg=msg_pairwise.format(name</span><span class="s0">, </span><span class="s2">&quot;decision_function&quot;</span><span class="s1">)</span><span class="s0">,</span>
                        <span class="s1">):</span>
                            <span class="s1">classifier.decision_function(X.reshape(-</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">))</span>
                    <span class="s0">else</span><span class="s1">:</span>
                        <span class="s0">with </span><span class="s1">raises(</span>
                            <span class="s1">ValueError</span><span class="s0">,</span>
                            <span class="s1">err_msg=msg.format(name</span><span class="s0">, </span><span class="s2">&quot;decision_function&quot;</span><span class="s1">)</span><span class="s0">,</span>
                        <span class="s1">):</span>
                            <span class="s1">classifier.decision_function(X.T)</span>
            <span class="s0">except </span><span class="s1">NotImplementedError:</span>
                <span class="s0">pass</span>

        <span class="s0">if </span><span class="s1">hasattr(classifier</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">):</span>
            <span class="s3"># predict_proba agrees with predict</span>
            <span class="s1">y_prob = classifier.predict_proba(X)</span>
            <span class="s0">assert </span><span class="s1">y_prob.shape == (n_samples</span><span class="s0">, </span><span class="s1">n_classes)</span>
            <span class="s1">assert_array_equal(np.argmax(y_prob</span><span class="s0">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">y_pred)</span>
            <span class="s3"># check that probas for all classes sum to one</span>
            <span class="s1">assert_array_almost_equal(np.sum(y_prob</span><span class="s0">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.ones(n_samples))</span>
            <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
                <span class="s3"># raises error on malformed input for predict_proba</span>
                <span class="s0">if </span><span class="s1">tags[</span><span class="s2">&quot;pairwise&quot;</span><span class="s1">]:</span>
                    <span class="s0">with </span><span class="s1">raises(</span>
                        <span class="s1">ValueError</span><span class="s0">,</span>
                        <span class="s1">err_msg=msg_pairwise.format(name</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">)</span><span class="s0">,</span>
                    <span class="s1">):</span>
                        <span class="s1">classifier.predict_proba(X.reshape(-</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">))</span>
                <span class="s0">else</span><span class="s1">:</span>
                    <span class="s0">with </span><span class="s1">raises(</span>
                        <span class="s1">ValueError</span><span class="s0">,</span>
                        <span class="s1">err_msg=msg.format(name</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">)</span><span class="s0">,</span>
                    <span class="s1">):</span>
                        <span class="s1">classifier.predict_proba(X.T)</span>
            <span class="s0">if </span><span class="s1">hasattr(classifier</span><span class="s0">, </span><span class="s2">&quot;predict_log_proba&quot;</span><span class="s1">):</span>
                <span class="s3"># predict_log_proba is a transformation of predict_proba</span>
                <span class="s1">y_log_prob = classifier.predict_log_proba(X)</span>
                <span class="s1">assert_allclose(y_log_prob</span><span class="s0">, </span><span class="s1">np.log(y_prob)</span><span class="s0">, </span><span class="s4">8</span><span class="s0">, </span><span class="s1">atol=</span><span class="s4">1e-9</span><span class="s1">)</span>
                <span class="s1">assert_array_equal(np.argsort(y_log_prob)</span><span class="s0">, </span><span class="s1">np.argsort(y_prob))</span>


<span class="s0">def </span><span class="s1">check_outlier_corruption(num_outliers</span><span class="s0">, </span><span class="s1">expected_outliers</span><span class="s0">, </span><span class="s1">decision):</span>
    <span class="s3"># Check for deviation from the precise given contamination level that may</span>
    <span class="s3"># be due to ties in the anomaly scores.</span>
    <span class="s0">if </span><span class="s1">num_outliers &lt; expected_outliers:</span>
        <span class="s1">start = num_outliers</span>
        <span class="s1">end = expected_outliers + </span><span class="s4">1</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">start = expected_outliers</span>
        <span class="s1">end = num_outliers + </span><span class="s4">1</span>

    <span class="s3"># ensure that all values in the 'critical area' are tied,</span>
    <span class="s3"># leading to the observed discrepancy between provided</span>
    <span class="s3"># and actual contamination levels.</span>
    <span class="s1">sorted_decision = np.sort(decision)</span>
    <span class="s1">msg = (</span>
        <span class="s2">&quot;The number of predicted outliers is not equal to the expected &quot;</span>
        <span class="s2">&quot;number of outliers and this difference is not explained by the &quot;</span>
        <span class="s2">&quot;number of ties in the decision_function values&quot;</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">len(np.unique(sorted_decision[start:end])) == </span><span class="s4">1</span><span class="s0">, </span><span class="s1">msg</span>


<span class="s0">def </span><span class="s1">check_outliers_train(name</span><span class="s0">, </span><span class="s1">estimator_orig</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">True</span><span class="s1">):</span>
    <span class="s1">n_samples = </span><span class="s4">300</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">_ = make_blobs(n_samples=n_samples</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = shuffle(X</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">7</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">readonly_memmap:</span>
        <span class="s1">X = create_memmap_backed_data(X)</span>

    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">set_random_state(estimator)</span>

    <span class="s3"># fit</span>
    <span class="s1">estimator.fit(X)</span>
    <span class="s3"># with lists</span>
    <span class="s1">estimator.fit(X.tolist())</span>

    <span class="s1">y_pred = estimator.predict(X)</span>
    <span class="s0">assert </span><span class="s1">y_pred.shape == (n_samples</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">y_pred.dtype.kind == </span><span class="s2">&quot;i&quot;</span>
    <span class="s1">assert_array_equal(np.unique(y_pred)</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]))</span>

    <span class="s1">decision = estimator.decision_function(X)</span>
    <span class="s1">scores = estimator.score_samples(X)</span>
    <span class="s0">for </span><span class="s1">output </span><span class="s0">in </span><span class="s1">[decision</span><span class="s0">, </span><span class="s1">scores]:</span>
        <span class="s0">assert </span><span class="s1">output.dtype == np.dtype(</span><span class="s2">&quot;float&quot;</span><span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">output.shape == (n_samples</span><span class="s0">,</span><span class="s1">)</span>

    <span class="s3"># raises error on malformed input for predict</span>
    <span class="s0">with </span><span class="s1">raises(ValueError):</span>
        <span class="s1">estimator.predict(X.T)</span>

    <span class="s3"># decision_function agrees with predict</span>
    <span class="s1">dec_pred = (decision &gt;= </span><span class="s4">0</span><span class="s1">).astype(int)</span>
    <span class="s1">dec_pred[dec_pred == </span><span class="s4">0</span><span class="s1">] = -</span><span class="s4">1</span>
    <span class="s1">assert_array_equal(dec_pred</span><span class="s0">, </span><span class="s1">y_pred)</span>

    <span class="s3"># raises error on malformed input for decision_function</span>
    <span class="s0">with </span><span class="s1">raises(ValueError):</span>
        <span class="s1">estimator.decision_function(X.T)</span>

    <span class="s3"># decision_function is a translation of score_samples</span>
    <span class="s1">y_dec = scores - estimator.offset_</span>
    <span class="s1">assert_allclose(y_dec</span><span class="s0">, </span><span class="s1">decision)</span>

    <span class="s3"># raises error on malformed input for score_samples</span>
    <span class="s0">with </span><span class="s1">raises(ValueError):</span>
        <span class="s1">estimator.score_samples(X.T)</span>

    <span class="s3"># contamination parameter (not for OneClassSVM which has the nu parameter)</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;contamination&quot;</span><span class="s1">) </span><span class="s0">and not </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;novelty&quot;</span><span class="s1">):</span>
        <span class="s3"># proportion of outliers equal to contamination parameter when not</span>
        <span class="s3"># set to 'auto'. This is true for the training set and cannot thus be</span>
        <span class="s3"># checked as follows for estimators with a novelty parameter such as</span>
        <span class="s3"># LocalOutlierFactor (tested in check_outliers_fit_predict)</span>
        <span class="s1">expected_outliers = </span><span class="s4">30</span>
        <span class="s1">contamination = expected_outliers / n_samples</span>
        <span class="s1">estimator.set_params(contamination=contamination)</span>
        <span class="s1">estimator.fit(X)</span>
        <span class="s1">y_pred = estimator.predict(X)</span>

        <span class="s1">num_outliers = np.sum(y_pred != </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s3"># num_outliers should be equal to expected_outliers unless</span>
        <span class="s3"># there are ties in the decision_function values. this can</span>
        <span class="s3"># only be tested for estimators with a decision_function</span>
        <span class="s3"># method, i.e. all estimators except LOF which is already</span>
        <span class="s3"># excluded from this if branch.</span>
        <span class="s0">if </span><span class="s1">num_outliers != expected_outliers:</span>
            <span class="s1">decision = estimator.decision_function(X)</span>
            <span class="s1">check_outlier_corruption(num_outliers</span><span class="s0">, </span><span class="s1">expected_outliers</span><span class="s0">, </span><span class="s1">decision)</span>


<span class="s0">def </span><span class="s1">check_outlier_contamination(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Check that the contamination parameter is in (0.0, 0.5] when it is an</span>
    <span class="s3"># interval constraint.</span>

    <span class="s0">if not </span><span class="s1">hasattr(estimator_orig</span><span class="s0">, </span><span class="s2">&quot;_parameter_constraints&quot;</span><span class="s1">):</span>
        <span class="s3"># Only estimator implementing parameter constraints will be checked</span>
        <span class="s0">return</span>

    <span class="s0">if </span><span class="s2">&quot;contamination&quot; </span><span class="s0">not in </span><span class="s1">estimator_orig._parameter_constraints:</span>
        <span class="s0">return</span>

    <span class="s1">contamination_constraints = estimator_orig._parameter_constraints[</span><span class="s2">&quot;contamination&quot;</span><span class="s1">]</span>
    <span class="s0">if not </span><span class="s1">any([isinstance(c</span><span class="s0">, </span><span class="s1">Interval) </span><span class="s0">for </span><span class="s1">c </span><span class="s0">in </span><span class="s1">contamination_constraints]):</span>
        <span class="s0">raise </span><span class="s1">AssertionError(</span>
            <span class="s2">&quot;contamination constraints should contain a Real Interval constraint.&quot;</span>
        <span class="s1">)</span>

    <span class="s0">for </span><span class="s1">constraint </span><span class="s0">in </span><span class="s1">contamination_constraints:</span>
        <span class="s0">if </span><span class="s1">isinstance(constraint</span><span class="s0">, </span><span class="s1">Interval):</span>
            <span class="s0">assert </span><span class="s1">(</span>
                <span class="s1">constraint.type == Real</span>
                <span class="s0">and </span><span class="s1">constraint.left &gt;= </span><span class="s4">0.0</span>
                <span class="s0">and </span><span class="s1">constraint.right &lt;= </span><span class="s4">0.5</span>
                <span class="s0">and </span><span class="s1">(constraint.left &gt; </span><span class="s4">0 </span><span class="s0">or </span><span class="s1">constraint.closed </span><span class="s0">in </span><span class="s1">{</span><span class="s2">&quot;right&quot;</span><span class="s0">, </span><span class="s2">&quot;neither&quot;</span><span class="s1">})</span>
            <span class="s1">)</span><span class="s0">, </span><span class="s2">&quot;contamination constraint should be an interval in (0, 0.5]&quot;</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_classifiers_multilabel_representation_invariance(name</span><span class="s0">, </span><span class="s1">classifier_orig):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_multilabel_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">100</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s4">2</span><span class="s0">,</span>
        <span class="s1">n_classes=</span><span class="s4">5</span><span class="s0">,</span>
        <span class="s1">n_labels=</span><span class="s4">3</span><span class="s0">,</span>
        <span class="s1">length=</span><span class="s4">50</span><span class="s0">,</span>
        <span class="s1">allow_unlabeled=</span><span class="s0">True,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X = scale(X)</span>

    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">y_train = X[:</span><span class="s4">80</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y[:</span><span class="s4">80</span><span class="s1">]</span>
    <span class="s1">X_test = X[</span><span class="s4">80</span><span class="s1">:]</span>

    <span class="s1">y_train_list_of_lists = y_train.tolist()</span>
    <span class="s1">y_train_list_of_arrays = list(y_train)</span>

    <span class="s1">classifier = clone(classifier_orig)</span>
    <span class="s1">set_random_state(classifier)</span>

    <span class="s1">y_pred = classifier.fit(X_train</span><span class="s0">, </span><span class="s1">y_train).predict(X_test)</span>

    <span class="s1">y_pred_list_of_lists = classifier.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_list_of_lists).predict(</span>
        <span class="s1">X_test</span>
    <span class="s1">)</span>

    <span class="s1">y_pred_list_of_arrays = classifier.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_list_of_arrays).predict(</span>
        <span class="s1">X_test</span>
    <span class="s1">)</span>

    <span class="s1">assert_array_equal(y_pred</span><span class="s0">, </span><span class="s1">y_pred_list_of_arrays)</span>
    <span class="s1">assert_array_equal(y_pred</span><span class="s0">, </span><span class="s1">y_pred_list_of_lists)</span>

    <span class="s0">assert </span><span class="s1">y_pred.dtype == y_pred_list_of_arrays.dtype</span>
    <span class="s0">assert </span><span class="s1">y_pred.dtype == y_pred_list_of_lists.dtype</span>
    <span class="s0">assert </span><span class="s1">type(y_pred) == type(y_pred_list_of_arrays)</span>
    <span class="s0">assert </span><span class="s1">type(y_pred) == type(y_pred_list_of_lists)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_classifiers_multilabel_output_format_predict(name</span><span class="s0">, </span><span class="s1">classifier_orig):</span>
    <span class="s5">&quot;&quot;&quot;Check the output of the `predict` method for classifiers supporting 
    multilabel-indicator targets.&quot;&quot;&quot;</span>
    <span class="s1">classifier = clone(classifier_orig)</span>
    <span class="s1">set_random_state(classifier)</span>

    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">test_size</span><span class="s0">, </span><span class="s1">n_outputs = </span><span class="s4">100</span><span class="s0">, </span><span class="s4">25</span><span class="s0">, </span><span class="s4">5</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_multilabel_classification(</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s4">2</span><span class="s0">,</span>
        <span class="s1">n_classes=n_outputs</span><span class="s0">,</span>
        <span class="s1">n_labels=</span><span class="s4">3</span><span class="s0">,</span>
        <span class="s1">length=</span><span class="s4">50</span><span class="s0">,</span>
        <span class="s1">allow_unlabeled=</span><span class="s0">True,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X = scale(X)</span>

    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test = X[:-test_size]</span><span class="s0">, </span><span class="s1">X[-test_size:]</span>
    <span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = y[:-test_size]</span><span class="s0">, </span><span class="s1">y[-test_size:]</span>
    <span class="s1">classifier.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s1">response_method_name = </span><span class="s2">&quot;predict&quot;</span>
    <span class="s1">predict_method = getattr(classifier</span><span class="s0">, </span><span class="s1">response_method_name</span><span class="s0">, None</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">predict_method </span><span class="s0">is None</span><span class="s1">:</span>
        <span class="s0">raise </span><span class="s1">SkipTest(</span><span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">does not have a </span><span class="s0">{</span><span class="s1">response_method_name</span><span class="s0">} </span><span class="s2">method.&quot;</span><span class="s1">)</span>

    <span class="s1">y_pred = predict_method(X_test)</span>

    <span class="s3"># y_pred.shape -&gt; y_test.shape with the same dtype</span>
    <span class="s0">assert </span><span class="s1">isinstance(y_pred</span><span class="s0">, </span><span class="s1">np.ndarray)</span><span class="s0">, </span><span class="s1">(</span>
        <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.predict is expected to output a NumPy array. Got &quot;</span>
        <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">type(y_pred)</span><span class="s0">} </span><span class="s2">instead.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">y_pred.shape == y_test.shape</span><span class="s0">, </span><span class="s1">(</span>
        <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.predict outputs a NumPy array of shape </span><span class="s0">{</span><span class="s1">y_pred.shape</span><span class="s0">} </span><span class="s2">&quot;</span>
        <span class="s2">f&quot;instead of </span><span class="s0">{</span><span class="s1">y_test.shape</span><span class="s0">}</span><span class="s2">.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">y_pred.dtype == y_test.dtype</span><span class="s0">, </span><span class="s1">(</span>
        <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.predict does not output the same dtype than the targets. &quot;</span>
        <span class="s2">f&quot;Got </span><span class="s0">{</span><span class="s1">y_pred.dtype</span><span class="s0">} </span><span class="s2">instead of </span><span class="s0">{</span><span class="s1">y_test.dtype</span><span class="s0">}</span><span class="s2">.&quot;</span>
    <span class="s1">)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_classifiers_multilabel_output_format_predict_proba(name</span><span class="s0">, </span><span class="s1">classifier_orig):</span>
    <span class="s5">&quot;&quot;&quot;Check the output of the `predict_proba` method for classifiers supporting 
    multilabel-indicator targets.&quot;&quot;&quot;</span>
    <span class="s1">classifier = clone(classifier_orig)</span>
    <span class="s1">set_random_state(classifier)</span>

    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">test_size</span><span class="s0">, </span><span class="s1">n_outputs = </span><span class="s4">100</span><span class="s0">, </span><span class="s4">25</span><span class="s0">, </span><span class="s4">5</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_multilabel_classification(</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s4">2</span><span class="s0">,</span>
        <span class="s1">n_classes=n_outputs</span><span class="s0">,</span>
        <span class="s1">n_labels=</span><span class="s4">3</span><span class="s0">,</span>
        <span class="s1">length=</span><span class="s4">50</span><span class="s0">,</span>
        <span class="s1">allow_unlabeled=</span><span class="s0">True,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X = scale(X)</span>

    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test = X[:-test_size]</span><span class="s0">, </span><span class="s1">X[-test_size:]</span>
    <span class="s1">y_train = y[:-test_size]</span>
    <span class="s1">classifier.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s1">response_method_name = </span><span class="s2">&quot;predict_proba&quot;</span>
    <span class="s1">predict_proba_method = getattr(classifier</span><span class="s0">, </span><span class="s1">response_method_name</span><span class="s0">, None</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">predict_proba_method </span><span class="s0">is None</span><span class="s1">:</span>
        <span class="s0">raise </span><span class="s1">SkipTest(</span><span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">does not have a </span><span class="s0">{</span><span class="s1">response_method_name</span><span class="s0">} </span><span class="s2">method.&quot;</span><span class="s1">)</span>

    <span class="s1">y_pred = predict_proba_method(X_test)</span>

    <span class="s3"># y_pred.shape -&gt; 2 possibilities:</span>
    <span class="s3"># - list of length n_outputs of shape (n_samples, 2);</span>
    <span class="s3"># - ndarray of shape (n_samples, n_outputs).</span>
    <span class="s3"># dtype should be floating</span>
    <span class="s0">if </span><span class="s1">isinstance(y_pred</span><span class="s0">, </span><span class="s1">list):</span>
        <span class="s0">assert </span><span class="s1">len(y_pred) == n_outputs</span><span class="s0">, </span><span class="s1">(</span>
            <span class="s2">f&quot;When </span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.predict_proba returns a list, the list should &quot;</span>
            <span class="s2">&quot;be of length n_outputs and contain NumPy arrays. Got length &quot;</span>
            <span class="s2">f&quot;of </span><span class="s0">{</span><span class="s1">len(y_pred)</span><span class="s0">} </span><span class="s2">instead of </span><span class="s0">{</span><span class="s1">n_outputs</span><span class="s0">}</span><span class="s2">.&quot;</span>
        <span class="s1">)</span>
        <span class="s0">for </span><span class="s1">pred </span><span class="s0">in </span><span class="s1">y_pred:</span>
            <span class="s0">assert </span><span class="s1">pred.shape == (test_size</span><span class="s0">, </span><span class="s4">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span>
                <span class="s2">f&quot;When </span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.predict_proba returns a list, this list &quot;</span>
                <span class="s2">&quot;should contain NumPy arrays of shape (n_samples, 2). Got &quot;</span>
                <span class="s2">f&quot;NumPy arrays of shape </span><span class="s0">{</span><span class="s1">pred.shape</span><span class="s0">} </span><span class="s2">instead of &quot;</span>
                <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">(test_size</span><span class="s0">, </span><span class="s4">2</span><span class="s1">)</span><span class="s0">}</span><span class="s2">.&quot;</span>
            <span class="s1">)</span>
            <span class="s0">assert </span><span class="s1">pred.dtype.kind == </span><span class="s2">&quot;f&quot;</span><span class="s0">, </span><span class="s1">(</span>
                <span class="s2">f&quot;When </span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.predict_proba returns a list, it should &quot;</span>
                <span class="s2">&quot;contain NumPy arrays with floating dtype. Got &quot;</span>
                <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">pred.dtype</span><span class="s0">} </span><span class="s2">instead.&quot;</span>
            <span class="s1">)</span>
            <span class="s3"># check that we have the correct probabilities</span>
            <span class="s1">err_msg = (</span>
                <span class="s2">f&quot;When </span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.predict_proba returns a list, each NumPy &quot;</span>
                <span class="s2">&quot;array should contain probabilities for each class and &quot;</span>
                <span class="s2">&quot;thus each row should sum to 1 (or close to 1 due to &quot;</span>
                <span class="s2">&quot;numerical errors).&quot;</span>
            <span class="s1">)</span>
            <span class="s1">assert_allclose(pred.sum(axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s1">err_msg=err_msg)</span>
    <span class="s0">elif </span><span class="s1">isinstance(y_pred</span><span class="s0">, </span><span class="s1">np.ndarray):</span>
        <span class="s0">assert </span><span class="s1">y_pred.shape == (test_size</span><span class="s0">, </span><span class="s1">n_outputs)</span><span class="s0">, </span><span class="s1">(</span>
            <span class="s2">f&quot;When </span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.predict_proba returns a NumPy array, the &quot;</span>
            <span class="s2">f&quot;expected shape is (n_samples, n_outputs). Got </span><span class="s0">{</span><span class="s1">y_pred.shape</span><span class="s0">}</span><span class="s2">&quot;</span>
            <span class="s2">f&quot; instead of </span><span class="s0">{</span><span class="s1">(test_size</span><span class="s0">, </span><span class="s1">n_outputs)</span><span class="s0">}</span><span class="s2">.&quot;</span>
        <span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">y_pred.dtype.kind == </span><span class="s2">&quot;f&quot;</span><span class="s0">, </span><span class="s1">(</span>
            <span class="s2">f&quot;When </span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.predict_proba returns a NumPy array, the &quot;</span>
            <span class="s2">f&quot;expected data type is floating. Got </span><span class="s0">{</span><span class="s1">y_pred.dtype</span><span class="s0">} </span><span class="s2">instead.&quot;</span>
        <span class="s1">)</span>
        <span class="s1">err_msg = (</span>
            <span class="s2">f&quot;When </span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.predict_proba returns a NumPy array, this array &quot;</span>
            <span class="s2">&quot;is expected to provide probabilities of the positive class &quot;</span>
            <span class="s2">&quot;and should therefore contain values between 0 and 1.&quot;</span>
        <span class="s1">)</span>
        <span class="s1">assert_array_less(</span><span class="s4">0</span><span class="s0">, </span><span class="s1">y_pred</span><span class="s0">, </span><span class="s1">err_msg=err_msg)</span>
        <span class="s1">assert_array_less(y_pred</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s1">err_msg=err_msg)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span>
            <span class="s2">f&quot;Unknown returned type </span><span class="s0">{</span><span class="s1">type(y_pred)</span><span class="s0">} </span><span class="s2">by </span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.&quot;</span>
            <span class="s2">&quot;predict_proba. A list or a Numpy array is expected.&quot;</span>
        <span class="s1">)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_classifiers_multilabel_output_format_decision_function(name</span><span class="s0">, </span><span class="s1">classifier_orig):</span>
    <span class="s5">&quot;&quot;&quot;Check the output of the `decision_function` method for classifiers supporting 
    multilabel-indicator targets.&quot;&quot;&quot;</span>
    <span class="s1">classifier = clone(classifier_orig)</span>
    <span class="s1">set_random_state(classifier)</span>

    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">test_size</span><span class="s0">, </span><span class="s1">n_outputs = </span><span class="s4">100</span><span class="s0">, </span><span class="s4">25</span><span class="s0">, </span><span class="s4">5</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_multilabel_classification(</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s4">2</span><span class="s0">,</span>
        <span class="s1">n_classes=n_outputs</span><span class="s0">,</span>
        <span class="s1">n_labels=</span><span class="s4">3</span><span class="s0">,</span>
        <span class="s1">length=</span><span class="s4">50</span><span class="s0">,</span>
        <span class="s1">allow_unlabeled=</span><span class="s0">True,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X = scale(X)</span>

    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test = X[:-test_size]</span><span class="s0">, </span><span class="s1">X[-test_size:]</span>
    <span class="s1">y_train = y[:-test_size]</span>
    <span class="s1">classifier.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s1">response_method_name = </span><span class="s2">&quot;decision_function&quot;</span>
    <span class="s1">decision_function_method = getattr(classifier</span><span class="s0">, </span><span class="s1">response_method_name</span><span class="s0">, None</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">decision_function_method </span><span class="s0">is None</span><span class="s1">:</span>
        <span class="s0">raise </span><span class="s1">SkipTest(</span><span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">does not have a </span><span class="s0">{</span><span class="s1">response_method_name</span><span class="s0">} </span><span class="s2">method.&quot;</span><span class="s1">)</span>

    <span class="s1">y_pred = decision_function_method(X_test)</span>

    <span class="s3"># y_pred.shape -&gt; y_test.shape with floating dtype</span>
    <span class="s0">assert </span><span class="s1">isinstance(y_pred</span><span class="s0">, </span><span class="s1">np.ndarray)</span><span class="s0">, </span><span class="s1">(</span>
        <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.decision_function is expected to output a NumPy array.&quot;</span>
        <span class="s2">f&quot; Got </span><span class="s0">{</span><span class="s1">type(y_pred)</span><span class="s0">} </span><span class="s2">instead.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">y_pred.shape == (test_size</span><span class="s0">, </span><span class="s1">n_outputs)</span><span class="s0">, </span><span class="s1">(</span>
        <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.decision_function is expected to provide a NumPy array &quot;</span>
        <span class="s2">f&quot;of shape (n_samples, n_outputs). Got </span><span class="s0">{</span><span class="s1">y_pred.shape</span><span class="s0">} </span><span class="s2">instead of &quot;</span>
        <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">(test_size</span><span class="s0">, </span><span class="s1">n_outputs)</span><span class="s0">}</span><span class="s2">.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">y_pred.dtype.kind == </span><span class="s2">&quot;f&quot;</span><span class="s0">, </span><span class="s1">(</span>
        <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.decision_function is expected to output a floating dtype.&quot;</span>
        <span class="s2">f&quot; Got </span><span class="s0">{</span><span class="s1">y_pred.dtype</span><span class="s0">} </span><span class="s2">instead.&quot;</span>
    <span class="s1">)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_get_feature_names_out_error(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s5">&quot;&quot;&quot;Check the error raised by get_feature_names_out when called before fit. 
 
    Unfitted estimators with get_feature_names_out should raise a NotFittedError. 
    &quot;&quot;&quot;</span>

    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">err_msg = (</span>
        <span class="s2">f&quot;Estimator </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">should have raised a NotFitted error when fit is called&quot;</span>
        <span class="s2">&quot; before get_feature_names_out&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">raises(NotFittedError</span><span class="s0">, </span><span class="s1">err_msg=err_msg):</span>
        <span class="s1">estimator.get_feature_names_out()</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_estimators_fit_returns_self(name</span><span class="s0">, </span><span class="s1">estimator_orig</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot;Check if self is returned when calling fit.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(random_state=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">n_samples=</span><span class="s4">21</span><span class="s1">)</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>

    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">readonly_memmap:</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y = create_memmap_backed_data([X</span><span class="s0">, </span><span class="s1">y])</span>

    <span class="s1">set_random_state(estimator)</span>
    <span class="s0">assert </span><span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y) </span><span class="s0">is </span><span class="s1">estimator</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">check_estimators_unfitted(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s5">&quot;&quot;&quot;Check that predict raises an exception in an unfitted estimator. 
 
    Unfitted estimators should raise a NotFittedError. 
    &quot;&quot;&quot;</span>
    <span class="s3"># Common test for Regressors, Classifiers and Outlier detection estimators</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = _regression_dataset()</span>

    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">(</span>
        <span class="s2">&quot;decision_function&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;predict&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;predict_proba&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;predict_log_proba&quot;</span><span class="s0">,</span>
    <span class="s1">):</span>
        <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s1">method):</span>
            <span class="s0">with </span><span class="s1">raises(NotFittedError):</span>
                <span class="s1">getattr(estimator</span><span class="s0">, </span><span class="s1">method)(X)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_supervised_y_2d(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s1">tags = _safe_tags(estimator_orig)</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s4">30</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">rnd.uniform(size=(n_samples</span><span class="s0">, </span><span class="s4">3</span><span class="s1">)))</span>
    <span class="s1">y = np.arange(n_samples) % </span><span class="s4">3</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator_orig</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">set_random_state(estimator)</span>
    <span class="s3"># fit</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">y_pred = estimator.predict(X)</span>

    <span class="s1">set_random_state(estimator)</span>
    <span class="s3"># Check that when a 2D y is given, a DataConversionWarning is</span>
    <span class="s3"># raised</span>
    <span class="s0">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s0">True</span><span class="s1">) </span><span class="s0">as </span><span class="s1">w:</span>
        <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;always&quot;</span><span class="s0">, </span><span class="s1">DataConversionWarning)</span>
        <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;ignore&quot;</span><span class="s0">, </span><span class="s1">RuntimeWarning)</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y[:</span><span class="s0">, </span><span class="s1">np.newaxis])</span>
    <span class="s1">y_pred_2d = estimator.predict(X)</span>
    <span class="s1">msg = </span><span class="s2">&quot;expected 1 DataConversionWarning, got: %s&quot; </span><span class="s1">% </span><span class="s2">&quot;, &quot;</span><span class="s1">.join(</span>
        <span class="s1">[str(w_x) </span><span class="s0">for </span><span class="s1">w_x </span><span class="s0">in </span><span class="s1">w]</span>
    <span class="s1">)</span>
    <span class="s0">if not </span><span class="s1">tags[</span><span class="s2">&quot;multioutput&quot;</span><span class="s1">]:</span>
        <span class="s3"># check that we warned if we don't support multi-output</span>
        <span class="s0">assert </span><span class="s1">len(w) &gt; </span><span class="s4">0</span><span class="s0">, </span><span class="s1">msg</span>
        <span class="s0">assert </span><span class="s1">(</span>
            <span class="s2">&quot;DataConversionWarning('A column-vector y&quot;</span>
            <span class="s2">&quot; was passed when a 1d array was expected&quot;</span>
            <span class="s0">in </span><span class="s1">msg</span>
        <span class="s1">)</span>
    <span class="s1">assert_allclose(y_pred.ravel()</span><span class="s0">, </span><span class="s1">y_pred_2d.ravel())</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">check_classifiers_predictions(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">name</span><span class="s0">, </span><span class="s1">classifier_orig):</span>
    <span class="s1">classes = np.unique(y)</span>
    <span class="s1">classifier = clone(classifier_orig)</span>
    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;BernoulliNB&quot;</span><span class="s1">:</span>
        <span class="s1">X = X &gt; X.mean()</span>
    <span class="s1">set_random_state(classifier)</span>

    <span class="s1">classifier.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">y_pred = classifier.predict(X)</span>

    <span class="s0">if </span><span class="s1">hasattr(classifier</span><span class="s0">, </span><span class="s2">&quot;decision_function&quot;</span><span class="s1">):</span>
        <span class="s1">decision = classifier.decision_function(X)</span>
        <span class="s0">assert </span><span class="s1">isinstance(decision</span><span class="s0">, </span><span class="s1">np.ndarray)</span>
        <span class="s0">if </span><span class="s1">len(classes) == </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s1">dec_pred = (decision.ravel() &gt; </span><span class="s4">0</span><span class="s1">).astype(int)</span>
            <span class="s1">dec_exp = classifier.classes_[dec_pred]</span>
            <span class="s1">assert_array_equal(</span>
                <span class="s1">dec_exp</span><span class="s0">,</span>
                <span class="s1">y_pred</span><span class="s0">,</span>
                <span class="s1">err_msg=(</span>
                    <span class="s2">&quot;decision_function does not match &quot;</span>
                    <span class="s2">&quot;classifier for %r: expected '%s', got '%s'&quot;</span>
                <span class="s1">)</span>
                <span class="s1">% (</span>
                    <span class="s1">classifier</span><span class="s0">,</span>
                    <span class="s2">&quot;, &quot;</span><span class="s1">.join(map(str</span><span class="s0">, </span><span class="s1">dec_exp))</span><span class="s0">,</span>
                    <span class="s2">&quot;, &quot;</span><span class="s1">.join(map(str</span><span class="s0">, </span><span class="s1">y_pred))</span><span class="s0">,</span>
                <span class="s1">)</span><span class="s0">,</span>
            <span class="s1">)</span>
        <span class="s0">elif </span><span class="s1">getattr(classifier</span><span class="s0">, </span><span class="s2">&quot;decision_function_shape&quot;</span><span class="s0">, </span><span class="s2">&quot;ovr&quot;</span><span class="s1">) == </span><span class="s2">&quot;ovr&quot;</span><span class="s1">:</span>
            <span class="s1">decision_y = np.argmax(decision</span><span class="s0">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">).astype(int)</span>
            <span class="s1">y_exp = classifier.classes_[decision_y]</span>
            <span class="s1">assert_array_equal(</span>
                <span class="s1">y_exp</span><span class="s0">,</span>
                <span class="s1">y_pred</span><span class="s0">,</span>
                <span class="s1">err_msg=(</span>
                    <span class="s2">&quot;decision_function does not match &quot;</span>
                    <span class="s2">&quot;classifier for %r: expected '%s', got '%s'&quot;</span>
                <span class="s1">)</span>
                <span class="s1">% (</span>
                    <span class="s1">classifier</span><span class="s0">,</span>
                    <span class="s2">&quot;, &quot;</span><span class="s1">.join(map(str</span><span class="s0">, </span><span class="s1">y_exp))</span><span class="s0">,</span>
                    <span class="s2">&quot;, &quot;</span><span class="s1">.join(map(str</span><span class="s0">, </span><span class="s1">y_pred))</span><span class="s0">,</span>
                <span class="s1">)</span><span class="s0">,</span>
            <span class="s1">)</span>

    <span class="s3"># training set performance</span>
    <span class="s0">if </span><span class="s1">name != </span><span class="s2">&quot;ComplementNB&quot;</span><span class="s1">:</span>
        <span class="s3"># This is a pathological data set for ComplementNB.</span>
        <span class="s3"># For some specific cases 'ComplementNB' predicts less classes</span>
        <span class="s3"># than expected</span>
        <span class="s1">assert_array_equal(np.unique(y)</span><span class="s0">, </span><span class="s1">np.unique(y_pred))</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">classes</span><span class="s0">,</span>
        <span class="s1">classifier.classes_</span><span class="s0">,</span>
        <span class="s1">err_msg=</span><span class="s2">&quot;Unexpected classes_ attribute for %r: expected '%s', got '%s'&quot;</span>
        <span class="s1">% (</span>
            <span class="s1">classifier</span><span class="s0">,</span>
            <span class="s2">&quot;, &quot;</span><span class="s1">.join(map(str</span><span class="s0">, </span><span class="s1">classes))</span><span class="s0">,</span>
            <span class="s2">&quot;, &quot;</span><span class="s1">.join(map(str</span><span class="s0">, </span><span class="s1">classifier.classes_))</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">_choose_check_classifiers_labels(name</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">y_names):</span>
    <span class="s3"># Semisupervised classifiers use -1 as the indicator for an unlabeled</span>
    <span class="s3"># sample.</span>
    <span class="s0">return </span><span class="s1">(</span>
        <span class="s1">y</span>
        <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;LabelPropagation&quot;</span><span class="s0">, </span><span class="s2">&quot;LabelSpreading&quot;</span><span class="s0">, </span><span class="s2">&quot;SelfTrainingClassifier&quot;</span><span class="s1">]</span>
        <span class="s0">else </span><span class="s1">y_names</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">check_classifiers_classes(name</span><span class="s0">, </span><span class="s1">classifier_orig):</span>
    <span class="s1">X_multiclass</span><span class="s0">, </span><span class="s1">y_multiclass = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s4">30</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">cluster_std=</span><span class="s4">0.1</span>
    <span class="s1">)</span>
    <span class="s1">X_multiclass</span><span class="s0">, </span><span class="s1">y_multiclass = shuffle(X_multiclass</span><span class="s0">, </span><span class="s1">y_multiclass</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">7</span><span class="s1">)</span>
    <span class="s1">X_multiclass = StandardScaler().fit_transform(X_multiclass)</span>

    <span class="s1">X_binary = X_multiclass[y_multiclass != </span><span class="s4">2</span><span class="s1">]</span>
    <span class="s1">y_binary = y_multiclass[y_multiclass != </span><span class="s4">2</span><span class="s1">]</span>

    <span class="s1">X_multiclass = _enforce_estimator_tags_X(classifier_orig</span><span class="s0">, </span><span class="s1">X_multiclass)</span>
    <span class="s1">X_binary = _enforce_estimator_tags_X(classifier_orig</span><span class="s0">, </span><span class="s1">X_binary)</span>

    <span class="s1">labels_multiclass = [</span><span class="s2">&quot;one&quot;</span><span class="s0">, </span><span class="s2">&quot;two&quot;</span><span class="s0">, </span><span class="s2">&quot;three&quot;</span><span class="s1">]</span>
    <span class="s1">labels_binary = [</span><span class="s2">&quot;one&quot;</span><span class="s0">, </span><span class="s2">&quot;two&quot;</span><span class="s1">]</span>

    <span class="s1">y_names_multiclass = np.take(labels_multiclass</span><span class="s0">, </span><span class="s1">y_multiclass)</span>
    <span class="s1">y_names_binary = np.take(labels_binary</span><span class="s0">, </span><span class="s1">y_binary)</span>

    <span class="s1">problems = [(X_binary</span><span class="s0">, </span><span class="s1">y_binary</span><span class="s0">, </span><span class="s1">y_names_binary)]</span>
    <span class="s0">if not </span><span class="s1">_safe_tags(classifier_orig</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;binary_only&quot;</span><span class="s1">):</span>
        <span class="s1">problems.append((X_multiclass</span><span class="s0">, </span><span class="s1">y_multiclass</span><span class="s0">, </span><span class="s1">y_names_multiclass))</span>

    <span class="s0">for </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">y_names </span><span class="s0">in </span><span class="s1">problems:</span>
        <span class="s0">for </span><span class="s1">y_names_i </span><span class="s0">in </span><span class="s1">[y_names</span><span class="s0">, </span><span class="s1">y_names.astype(</span><span class="s2">&quot;O&quot;</span><span class="s1">)]:</span>
            <span class="s1">y_ = _choose_check_classifiers_labels(name</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">y_names_i)</span>
            <span class="s1">check_classifiers_predictions(X</span><span class="s0">, </span><span class="s1">y_</span><span class="s0">, </span><span class="s1">name</span><span class="s0">, </span><span class="s1">classifier_orig)</span>

    <span class="s1">labels_binary = [-</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">y_names_binary = np.take(labels_binary</span><span class="s0">, </span><span class="s1">y_binary)</span>
    <span class="s1">y_binary = _choose_check_classifiers_labels(name</span><span class="s0">, </span><span class="s1">y_binary</span><span class="s0">, </span><span class="s1">y_names_binary)</span>
    <span class="s1">check_classifiers_predictions(X_binary</span><span class="s0">, </span><span class="s1">y_binary</span><span class="s0">, </span><span class="s1">name</span><span class="s0">, </span><span class="s1">classifier_orig)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_regressors_int(name</span><span class="s0">, </span><span class="s1">regressor_orig):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">_ = _regression_dataset()</span>
    <span class="s1">X = _enforce_estimator_tags_X(regressor_orig</span><span class="s0">, </span><span class="s1">X[:</span><span class="s4">50</span><span class="s1">])</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">y = rnd.randint(</span><span class="s4">3</span><span class="s0">, </span><span class="s1">size=X.shape[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">y = _enforce_estimator_tags_y(regressor_orig</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s3"># separate estimators to control random seeds</span>
    <span class="s1">regressor_1 = clone(regressor_orig)</span>
    <span class="s1">regressor_2 = clone(regressor_orig)</span>
    <span class="s1">set_random_state(regressor_1)</span>
    <span class="s1">set_random_state(regressor_2)</span>

    <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
        <span class="s1">y_ = np.vstack([y</span><span class="s0">, </span><span class="s4">2 </span><span class="s1">* y + rnd.randint(</span><span class="s4">2</span><span class="s0">, </span><span class="s1">size=len(y))])</span>
        <span class="s1">y_ = y_.T</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y_ = y</span>

    <span class="s3"># fit</span>
    <span class="s1">regressor_1.fit(X</span><span class="s0">, </span><span class="s1">y_)</span>
    <span class="s1">pred1 = regressor_1.predict(X)</span>
    <span class="s1">regressor_2.fit(X</span><span class="s0">, </span><span class="s1">y_.astype(float))</span>
    <span class="s1">pred2 = regressor_2.predict(X)</span>
    <span class="s1">assert_allclose(pred1</span><span class="s0">, </span><span class="s1">pred2</span><span class="s0">, </span><span class="s1">atol=</span><span class="s4">1e-2</span><span class="s0">, </span><span class="s1">err_msg=name)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_regressors_train(</span>
    <span class="s1">name</span><span class="s0">, </span><span class="s1">regressor_orig</span><span class="s0">, </span><span class="s1">readonly_memmap=</span><span class="s0">False, </span><span class="s1">X_dtype=np.float64</span>
<span class="s1">):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = _regression_dataset()</span>
    <span class="s1">X = X.astype(X_dtype)</span>
    <span class="s1">y = scale(y)  </span><span class="s3"># X is already scaled</span>
    <span class="s1">regressor = clone(regressor_orig)</span>
    <span class="s1">X = _enforce_estimator_tags_X(regressor</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = _enforce_estimator_tags_y(regressor</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
        <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">y_ = np.vstack([y</span><span class="s0">, </span><span class="s4">2 </span><span class="s1">* y + rnd.randint(</span><span class="s4">2</span><span class="s0">, </span><span class="s1">size=len(y))])</span>
        <span class="s1">y_ = y_.T</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y_ = y</span>

    <span class="s0">if </span><span class="s1">readonly_memmap:</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">y_ = create_memmap_backed_data([X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">y_])</span>

    <span class="s0">if not </span><span class="s1">hasattr(regressor</span><span class="s0">, </span><span class="s2">&quot;alphas&quot;</span><span class="s1">) </span><span class="s0">and </span><span class="s1">hasattr(regressor</span><span class="s0">, </span><span class="s2">&quot;alpha&quot;</span><span class="s1">):</span>
        <span class="s3"># linear regressors need to set alpha, but not generalized CV ones</span>
        <span class="s1">regressor.alpha = </span><span class="s4">0.01</span>
    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;PassiveAggressiveRegressor&quot;</span><span class="s1">:</span>
        <span class="s1">regressor.C = </span><span class="s4">0.01</span>

    <span class="s3"># raises error on malformed input for fit</span>
    <span class="s0">with </span><span class="s1">raises(</span>
        <span class="s1">ValueError</span><span class="s0">,</span>
        <span class="s1">err_msg=(</span>
            <span class="s2">f&quot;The classifier </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">does not raise an error when &quot;</span>
            <span class="s2">&quot;incorrect/malformed input data for fit is passed. The number of &quot;</span>
            <span class="s2">&quot;training examples is not the same as the number of labels. Perhaps &quot;</span>
            <span class="s2">&quot;use check_X_y in fit.&quot;</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">):</span>
        <span class="s1">regressor.fit(X</span><span class="s0">, </span><span class="s1">y[:-</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s3"># fit</span>
    <span class="s1">set_random_state(regressor)</span>
    <span class="s1">regressor.fit(X</span><span class="s0">, </span><span class="s1">y_)</span>
    <span class="s1">regressor.fit(X.tolist()</span><span class="s0">, </span><span class="s1">y_.tolist())</span>
    <span class="s1">y_pred = regressor.predict(X)</span>
    <span class="s0">assert </span><span class="s1">y_pred.shape == y_.shape</span>

    <span class="s3"># TODO: find out why PLS and CCA fail. RANSAC is random</span>
    <span class="s3"># and furthermore assumes the presence of outliers, hence</span>
    <span class="s3"># skipped</span>
    <span class="s0">if not </span><span class="s1">_safe_tags(regressor</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;poor_score&quot;</span><span class="s1">):</span>
        <span class="s0">assert </span><span class="s1">regressor.score(X</span><span class="s0">, </span><span class="s1">y_) &gt; </span><span class="s4">0.5</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">check_regressors_no_decision_function(name</span><span class="s0">, </span><span class="s1">regressor_orig):</span>
    <span class="s3"># check that regressors don't have a decision_function, predict_proba, or</span>
    <span class="s3"># predict_log_proba method.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">regressor = clone(regressor_orig)</span>

    <span class="s1">X = rng.normal(size=(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">4</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(regressor_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = _enforce_estimator_tags_y(regressor</span><span class="s0">, </span><span class="s1">X[:</span><span class="s0">, </span><span class="s4">0</span><span class="s1">])</span>

    <span class="s1">regressor.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">funcs = [</span><span class="s2">&quot;decision_function&quot;</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s0">, </span><span class="s2">&quot;predict_log_proba&quot;</span><span class="s1">]</span>
    <span class="s0">for </span><span class="s1">func_name </span><span class="s0">in </span><span class="s1">funcs:</span>
        <span class="s0">assert not </span><span class="s1">hasattr(regressor</span><span class="s0">, </span><span class="s1">func_name)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_class_weight_classifiers(name</span><span class="s0">, </span><span class="s1">classifier_orig):</span>
    <span class="s0">if </span><span class="s1">_safe_tags(classifier_orig</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;binary_only&quot;</span><span class="s1">):</span>
        <span class="s1">problems = [</span><span class="s4">2</span><span class="s1">]</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">problems = [</span><span class="s4">2</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span>

    <span class="s0">for </span><span class="s1">n_centers </span><span class="s0">in </span><span class="s1">problems:</span>
        <span class="s3"># create a very noisy dataset</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(centers=n_centers</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">cluster_std=</span><span class="s4">20</span><span class="s1">)</span>
        <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(</span>
            <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">test_size=</span><span class="s4">0.5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span>
        <span class="s1">)</span>

        <span class="s3"># can't use gram_if_pairwise() here, setting up gram matrix manually</span>
        <span class="s0">if </span><span class="s1">_safe_tags(classifier_orig</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;pairwise&quot;</span><span class="s1">):</span>
            <span class="s1">X_test = rbf_kernel(X_test</span><span class="s0">, </span><span class="s1">X_train)</span>
            <span class="s1">X_train = rbf_kernel(X_train</span><span class="s0">, </span><span class="s1">X_train)</span>

        <span class="s1">n_centers = len(np.unique(y_train))</span>

        <span class="s0">if </span><span class="s1">n_centers == </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s1">class_weight = {</span><span class="s4">0</span><span class="s1">: </span><span class="s4">1000</span><span class="s0">, </span><span class="s4">1</span><span class="s1">: </span><span class="s4">0.0001</span><span class="s1">}</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">class_weight = {</span><span class="s4">0</span><span class="s1">: </span><span class="s4">1000</span><span class="s0">, </span><span class="s4">1</span><span class="s1">: </span><span class="s4">0.0001</span><span class="s0">, </span><span class="s4">2</span><span class="s1">: </span><span class="s4">0.0001</span><span class="s1">}</span>

        <span class="s1">classifier = clone(classifier_orig).set_params(class_weight=class_weight)</span>
        <span class="s0">if </span><span class="s1">hasattr(classifier</span><span class="s0">, </span><span class="s2">&quot;n_iter&quot;</span><span class="s1">):</span>
            <span class="s1">classifier.set_params(n_iter=</span><span class="s4">100</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">hasattr(classifier</span><span class="s0">, </span><span class="s2">&quot;max_iter&quot;</span><span class="s1">):</span>
            <span class="s1">classifier.set_params(max_iter=</span><span class="s4">1000</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">hasattr(classifier</span><span class="s0">, </span><span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="s1">):</span>
            <span class="s1">classifier.set_params(min_weight_fraction_leaf=</span><span class="s4">0.01</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">hasattr(classifier</span><span class="s0">, </span><span class="s2">&quot;n_iter_no_change&quot;</span><span class="s1">):</span>
            <span class="s1">classifier.set_params(n_iter_no_change=</span><span class="s4">20</span><span class="s1">)</span>

        <span class="s1">set_random_state(classifier)</span>
        <span class="s1">classifier.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
        <span class="s1">y_pred = classifier.predict(X_test)</span>
        <span class="s3"># XXX: Generally can use 0.89 here. On Windows, LinearSVC gets</span>
        <span class="s3">#      0.88 (Issue #9111)</span>
        <span class="s0">if not </span><span class="s1">_safe_tags(classifier_orig</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;poor_score&quot;</span><span class="s1">):</span>
            <span class="s0">assert </span><span class="s1">np.mean(y_pred == </span><span class="s4">0</span><span class="s1">) &gt; </span><span class="s4">0.87</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_class_weight_balanced_classifiers(</span>
    <span class="s1">name</span><span class="s0">, </span><span class="s1">classifier_orig</span><span class="s0">, </span><span class="s1">X_train</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test</span><span class="s0">, </span><span class="s1">weights</span>
<span class="s1">):</span>
    <span class="s1">classifier = clone(classifier_orig)</span>
    <span class="s0">if </span><span class="s1">hasattr(classifier</span><span class="s0">, </span><span class="s2">&quot;n_iter&quot;</span><span class="s1">):</span>
        <span class="s1">classifier.set_params(n_iter=</span><span class="s4">100</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">hasattr(classifier</span><span class="s0">, </span><span class="s2">&quot;max_iter&quot;</span><span class="s1">):</span>
        <span class="s1">classifier.set_params(max_iter=</span><span class="s4">1000</span><span class="s1">)</span>

    <span class="s1">set_random_state(classifier)</span>
    <span class="s1">classifier.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">y_pred = classifier.predict(X_test)</span>

    <span class="s1">classifier.set_params(class_weight=</span><span class="s2">&quot;balanced&quot;</span><span class="s1">)</span>
    <span class="s1">classifier.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">y_pred_balanced = classifier.predict(X_test)</span>
    <span class="s0">assert </span><span class="s1">f1_score(y_test</span><span class="s0">, </span><span class="s1">y_pred_balanced</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">) &gt; f1_score(</span>
        <span class="s1">y_test</span><span class="s0">, </span><span class="s1">y_pred</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span>
    <span class="s1">)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_class_weight_balanced_linear_classifier(name</span><span class="s0">, </span><span class="s1">Classifier):</span>
    <span class="s5">&quot;&quot;&quot;Test class weights with non-contiguous class labels.&quot;&quot;&quot;</span>
    <span class="s3"># this is run on classes, not instances, though this should be changed</span>
    <span class="s1">X = np.array([[-</span><span class="s4">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s4">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s4">0.8</span><span class="s0">, </span><span class="s1">-</span><span class="s4">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">0.0</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s1">-</span><span class="s4">1</span><span class="s0">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">])</span>

    <span class="s1">classifier = Classifier()</span>

    <span class="s0">if </span><span class="s1">hasattr(classifier</span><span class="s0">, </span><span class="s2">&quot;n_iter&quot;</span><span class="s1">):</span>
        <span class="s3"># This is a very small dataset, default n_iter are likely to prevent</span>
        <span class="s3"># convergence</span>
        <span class="s1">classifier.set_params(n_iter=</span><span class="s4">1000</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">hasattr(classifier</span><span class="s0">, </span><span class="s2">&quot;max_iter&quot;</span><span class="s1">):</span>
        <span class="s1">classifier.set_params(max_iter=</span><span class="s4">1000</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">hasattr(classifier</span><span class="s0">, </span><span class="s2">&quot;cv&quot;</span><span class="s1">):</span>
        <span class="s1">classifier.set_params(cv=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">set_random_state(classifier)</span>

    <span class="s3"># Let the model compute the class frequencies</span>
    <span class="s1">classifier.set_params(class_weight=</span><span class="s2">&quot;balanced&quot;</span><span class="s1">)</span>
    <span class="s1">coef_balanced = classifier.fit(X</span><span class="s0">, </span><span class="s1">y).coef_.copy()</span>

    <span class="s3"># Count each label occurrence to reweight manually</span>
    <span class="s1">n_samples = len(y)</span>
    <span class="s1">n_classes = float(len(np.unique(y)))</span>

    <span class="s1">class_weight = {</span>
        <span class="s4">1</span><span class="s1">: n_samples / (np.sum(y == </span><span class="s4">1</span><span class="s1">) * n_classes)</span><span class="s0">,</span>
        <span class="s1">-</span><span class="s4">1</span><span class="s1">: n_samples / (np.sum(y == -</span><span class="s4">1</span><span class="s1">) * n_classes)</span><span class="s0">,</span>
    <span class="s1">}</span>
    <span class="s1">classifier.set_params(class_weight=class_weight)</span>
    <span class="s1">coef_manual = classifier.fit(X</span><span class="s0">, </span><span class="s1">y).coef_.copy()</span>

    <span class="s1">assert_allclose(</span>
        <span class="s1">coef_balanced</span><span class="s0">,</span>
        <span class="s1">coef_manual</span><span class="s0">,</span>
        <span class="s1">err_msg=</span><span class="s2">&quot;Classifier %s is not computing class_weight=balanced properly.&quot; </span><span class="s1">% name</span><span class="s0">,</span>
    <span class="s1">)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_estimators_overwrite_params(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(random_state=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">n_samples=</span><span class="s4">21</span><span class="s1">)</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">kernel=rbf_kernel)</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">set_random_state(estimator)</span>

    <span class="s3"># Make a physical copy of the original estimator parameters before fitting.</span>
    <span class="s1">params = estimator.get_params()</span>
    <span class="s1">original_params = deepcopy(params)</span>

    <span class="s3"># Fit the model</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s3"># Compare the state of the model parameters with the original parameters</span>
    <span class="s1">new_params = estimator.get_params()</span>
    <span class="s0">for </span><span class="s1">param_name</span><span class="s0">, </span><span class="s1">original_value </span><span class="s0">in </span><span class="s1">original_params.items():</span>
        <span class="s1">new_value = new_params[param_name]</span>

        <span class="s3"># We should never change or mutate the internal state of input</span>
        <span class="s3"># parameters by default. To check this we use the joblib.hash function</span>
        <span class="s3"># that introspects recursively any subobjects to compute a checksum.</span>
        <span class="s3"># The only exception to this rule of immutable constructor parameters</span>
        <span class="s3"># is possible RandomState instance but in this check we explicitly</span>
        <span class="s3"># fixed the random_state params recursively to be integer seeds.</span>
        <span class="s0">assert </span><span class="s1">joblib.hash(new_value) == joblib.hash(original_value)</span><span class="s0">, </span><span class="s1">(</span>
            <span class="s2">&quot;Estimator %s should not change or mutate &quot;</span>
            <span class="s2">&quot; the parameter %s from %s to %s during fit.&quot;</span>
            <span class="s1">% (name</span><span class="s0">, </span><span class="s1">param_name</span><span class="s0">, </span><span class="s1">original_value</span><span class="s0">, </span><span class="s1">new_value)</span>
        <span class="s1">)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_no_attributes_set_in_init(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s5">&quot;&quot;&quot;Check setting during init.&quot;&quot;&quot;</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s3"># Clone fails if the estimator does not store</span>
        <span class="s3"># all parameters as an attribute during init</span>
        <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s0">except </span><span class="s1">AttributeError:</span>
        <span class="s0">raise </span><span class="s1">AttributeError(</span>
            <span class="s2">f&quot;Estimator </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">should store all parameters as an attribute during init.&quot;</span>
        <span class="s1">)</span>

    <span class="s0">if </span><span class="s1">hasattr(type(estimator).__init__</span><span class="s0">, </span><span class="s2">&quot;deprecated_original&quot;</span><span class="s1">):</span>
        <span class="s0">return</span>

    <span class="s1">init_params = _get_args(type(estimator).__init__)</span>
    <span class="s0">if </span><span class="s1">IS_PYPY:</span>
        <span class="s3"># __init__ signature has additional objects in PyPy</span>
        <span class="s0">for </span><span class="s1">key </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;obj&quot;</span><span class="s1">]:</span>
            <span class="s0">if </span><span class="s1">key </span><span class="s0">in </span><span class="s1">init_params:</span>
                <span class="s1">init_params.remove(key)</span>
    <span class="s1">parents_init_params = [</span>
        <span class="s1">param</span>
        <span class="s0">for </span><span class="s1">params_parent </span><span class="s0">in </span><span class="s1">(_get_args(parent) </span><span class="s0">for </span><span class="s1">parent </span><span class="s0">in </span><span class="s1">type(estimator).__mro__)</span>
        <span class="s0">for </span><span class="s1">param </span><span class="s0">in </span><span class="s1">params_parent</span>
    <span class="s1">]</span>

    <span class="s3"># Test for no setting apart from parameters during init</span>
    <span class="s1">invalid_attr = set(vars(estimator)) - set(init_params) - set(parents_init_params)</span>
    <span class="s3"># Ignore private attributes</span>
    <span class="s1">invalid_attr = set([attr </span><span class="s0">for </span><span class="s1">attr </span><span class="s0">in </span><span class="s1">invalid_attr </span><span class="s0">if not </span><span class="s1">attr.startswith(</span><span class="s2">&quot;_&quot;</span><span class="s1">)])</span>
    <span class="s0">assert not </span><span class="s1">invalid_attr</span><span class="s0">, </span><span class="s1">(</span>
        <span class="s2">&quot;Estimator %s should not set any attribute apart&quot;</span>
        <span class="s2">&quot; from parameters during init. Found attributes %s.&quot;</span>
        <span class="s1">% (name</span><span class="s0">, </span><span class="s1">sorted(invalid_attr))</span>
    <span class="s1">)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_sparsify_coefficients(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s1">X = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[-</span><span class="s4">2</span><span class="s0">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s4">1</span><span class="s0">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s4">1</span><span class="s0">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s4">1</span><span class="s0">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s4">2</span><span class="s0">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">])</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator_orig</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">est = clone(estimator_orig)</span>

    <span class="s1">est.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">pred_orig = est.predict(X)</span>

    <span class="s3"># test sparsify with dense inputs</span>
    <span class="s1">est.sparsify()</span>
    <span class="s0">assert </span><span class="s1">sparse.issparse(est.coef_)</span>
    <span class="s1">pred = est.predict(X)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s0">, </span><span class="s1">pred_orig)</span>

    <span class="s3"># pickle and unpickle with sparse coef_</span>
    <span class="s1">est = pickle.loads(pickle.dumps(est))</span>
    <span class="s0">assert </span><span class="s1">sparse.issparse(est.coef_)</span>
    <span class="s1">pred = est.predict(X)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s0">, </span><span class="s1">pred_orig)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_classifier_data_not_an_array(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s1">X = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">4</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator_orig</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">for </span><span class="s1">obj_type </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;NotAnArray&quot;</span><span class="s0">, </span><span class="s2">&quot;PandasDataframe&quot;</span><span class="s1">]:</span>
        <span class="s1">check_estimators_data_not_an_array(name</span><span class="s0">, </span><span class="s1">estimator_orig</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">obj_type)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_regressor_data_not_an_array(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = _regression_dataset()</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator_orig</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">for </span><span class="s1">obj_type </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;NotAnArray&quot;</span><span class="s0">, </span><span class="s2">&quot;PandasDataframe&quot;</span><span class="s1">]:</span>
        <span class="s1">check_estimators_data_not_an_array(name</span><span class="s0">, </span><span class="s1">estimator_orig</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">obj_type)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_estimators_data_not_an_array(name</span><span class="s0">, </span><span class="s1">estimator_orig</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">obj_type):</span>
    <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
        <span class="s0">raise </span><span class="s1">SkipTest(</span>
            <span class="s2">&quot;Skipping check_estimators_data_not_an_array &quot;</span>
            <span class="s2">&quot;for cross decomposition module as estimators &quot;</span>
            <span class="s2">&quot;are not deterministic.&quot;</span>
        <span class="s1">)</span>
    <span class="s3"># separate estimators to control random seeds</span>
    <span class="s1">estimator_1 = clone(estimator_orig)</span>
    <span class="s1">estimator_2 = clone(estimator_orig)</span>
    <span class="s1">set_random_state(estimator_1)</span>
    <span class="s1">set_random_state(estimator_2)</span>

    <span class="s0">if </span><span class="s1">obj_type </span><span class="s0">not in </span><span class="s1">[</span><span class="s2">&quot;NotAnArray&quot;</span><span class="s0">, </span><span class="s2">&quot;PandasDataframe&quot;</span><span class="s1">]:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;Data type {0} not supported&quot;</span><span class="s1">.format(obj_type))</span>

    <span class="s0">if </span><span class="s1">obj_type == </span><span class="s2">&quot;NotAnArray&quot;</span><span class="s1">:</span>
        <span class="s1">y_ = _NotAnArray(np.asarray(y))</span>
        <span class="s1">X_ = _NotAnArray(np.asarray(X))</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s3"># Here pandas objects (Series and DataFrame) are tested explicitly</span>
        <span class="s3"># because some estimators may handle them (especially their indexing)</span>
        <span class="s3"># specially.</span>
        <span class="s0">try</span><span class="s1">:</span>
            <span class="s0">import </span><span class="s1">pandas </span><span class="s0">as </span><span class="s1">pd</span>

            <span class="s1">y_ = np.asarray(y)</span>
            <span class="s0">if </span><span class="s1">y_.ndim == </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">y_ = pd.Series(y_</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">y_ = pd.DataFrame(y_</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
            <span class="s1">X_ = pd.DataFrame(np.asarray(X)</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>

        <span class="s0">except </span><span class="s1">ImportError:</span>
            <span class="s0">raise </span><span class="s1">SkipTest(</span>
                <span class="s2">&quot;pandas is not installed: not checking estimators for pandas objects.&quot;</span>
            <span class="s1">)</span>

    <span class="s3"># fit</span>
    <span class="s1">estimator_1.fit(X_</span><span class="s0">, </span><span class="s1">y_)</span>
    <span class="s1">pred1 = estimator_1.predict(X_)</span>
    <span class="s1">estimator_2.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">pred2 = estimator_2.predict(X)</span>
    <span class="s1">assert_allclose(pred1</span><span class="s0">, </span><span class="s1">pred2</span><span class="s0">, </span><span class="s1">atol=</span><span class="s4">1e-2</span><span class="s0">, </span><span class="s1">err_msg=name)</span>


<span class="s0">def </span><span class="s1">check_parameters_default_constructible(name</span><span class="s0">, </span><span class="s1">Estimator):</span>
    <span class="s3"># test default-constructibility</span>
    <span class="s3"># get rid of deprecation warnings</span>

    <span class="s1">Estimator = Estimator.__class__</span>

    <span class="s0">with </span><span class="s1">ignore_warnings(category=FutureWarning):</span>
        <span class="s1">estimator = _construct_instance(Estimator)</span>
        <span class="s3"># test cloning</span>
        <span class="s1">clone(estimator)</span>
        <span class="s3"># test __repr__</span>
        <span class="s1">repr(estimator)</span>
        <span class="s3"># test that set_params returns self</span>
        <span class="s0">assert </span><span class="s1">estimator.set_params() </span><span class="s0">is </span><span class="s1">estimator</span>

        <span class="s3"># test if init does nothing but set parameters</span>
        <span class="s3"># this is important for grid_search etc.</span>
        <span class="s3"># We get the default parameters from init and then</span>
        <span class="s3"># compare these against the actual values of the attributes.</span>

        <span class="s3"># this comes from getattr. Gets rid of deprecation decorator.</span>
        <span class="s1">init = getattr(estimator.__init__</span><span class="s0">, </span><span class="s2">&quot;deprecated_original&quot;</span><span class="s0">, </span><span class="s1">estimator.__init__)</span>

        <span class="s0">try</span><span class="s1">:</span>

            <span class="s0">def </span><span class="s1">param_filter(p):</span>
                <span class="s5">&quot;&quot;&quot;Identify hyper parameters of an estimator.&quot;&quot;&quot;</span>
                <span class="s0">return </span><span class="s1">(</span>
                    <span class="s1">p.name != </span><span class="s2">&quot;self&quot;</span>
                    <span class="s0">and </span><span class="s1">p.kind != p.VAR_KEYWORD</span>
                    <span class="s0">and </span><span class="s1">p.kind != p.VAR_POSITIONAL</span>
                <span class="s1">)</span>

            <span class="s1">init_params = [</span>
                <span class="s1">p </span><span class="s0">for </span><span class="s1">p </span><span class="s0">in </span><span class="s1">signature(init).parameters.values() </span><span class="s0">if </span><span class="s1">param_filter(p)</span>
            <span class="s1">]</span>

        <span class="s0">except </span><span class="s1">(TypeError</span><span class="s0">, </span><span class="s1">ValueError):</span>
            <span class="s3"># init is not a python function.</span>
            <span class="s3"># true for mixins</span>
            <span class="s0">return</span>
        <span class="s1">params = estimator.get_params()</span>
        <span class="s3"># they can need a non-default argument</span>
        <span class="s1">init_params = init_params[len(getattr(estimator</span><span class="s0">, </span><span class="s2">&quot;_required_parameters&quot;</span><span class="s0">, </span><span class="s1">[])) :]</span>

        <span class="s0">for </span><span class="s1">init_param </span><span class="s0">in </span><span class="s1">init_params:</span>
            <span class="s0">assert </span><span class="s1">(</span>
                <span class="s1">init_param.default != init_param.empty</span>
            <span class="s1">)</span><span class="s0">, </span><span class="s2">&quot;parameter %s for %s has no default value&quot; </span><span class="s1">% (</span>
                <span class="s1">init_param.name</span><span class="s0">,</span>
                <span class="s1">type(estimator).__name__</span><span class="s0">,</span>
            <span class="s1">)</span>
            <span class="s1">allowed_types = {</span>
                <span class="s1">str</span><span class="s0">,</span>
                <span class="s1">int</span><span class="s0">,</span>
                <span class="s1">float</span><span class="s0">,</span>
                <span class="s1">bool</span><span class="s0">,</span>
                <span class="s1">tuple</span><span class="s0">,</span>
                <span class="s1">type(</span><span class="s0">None</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">type</span><span class="s0">,</span>
            <span class="s1">}</span>
            <span class="s3"># Any numpy numeric such as np.int32.</span>
            <span class="s1">allowed_types.update(np.core.numerictypes.allTypes.values())</span>

            <span class="s1">allowed_value = (</span>
                <span class="s1">type(init_param.default) </span><span class="s0">in </span><span class="s1">allowed_types</span>
                <span class="s0">or</span>
                <span class="s3"># Although callables are mutable, we accept them as argument</span>
                <span class="s3"># default value and trust that neither the implementation of</span>
                <span class="s3"># the callable nor of the estimator changes the state of the</span>
                <span class="s3"># callable.</span>
                <span class="s1">callable(init_param.default)</span>
            <span class="s1">)</span>

            <span class="s0">assert </span><span class="s1">allowed_value</span><span class="s0">, </span><span class="s1">(</span>
                <span class="s2">f&quot;Parameter '</span><span class="s0">{</span><span class="s1">init_param.name</span><span class="s0">}</span><span class="s2">' of estimator &quot;</span>
                <span class="s2">f&quot;'</span><span class="s0">{</span><span class="s1">Estimator.__name__</span><span class="s0">}</span><span class="s2">' is of type &quot;</span>
                <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">type(init_param.default).__name__</span><span class="s0">} </span><span class="s2">which is not allowed. &quot;</span>
                <span class="s2">f&quot;'</span><span class="s0">{</span><span class="s1">init_param.name</span><span class="s0">}</span><span class="s2">' must be a callable or must be of type &quot;</span>
                <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">set(type.__name__ </span><span class="s0">for </span><span class="s1">type </span><span class="s0">in </span><span class="s1">allowed_types)</span><span class="s0">}</span><span class="s2">.&quot;</span>
            <span class="s1">)</span>
            <span class="s0">if </span><span class="s1">init_param.name </span><span class="s0">not in </span><span class="s1">params.keys():</span>
                <span class="s3"># deprecated parameter, not in get_params</span>
                <span class="s0">assert </span><span class="s1">init_param.default </span><span class="s0">is None, </span><span class="s1">(</span>
                    <span class="s2">f&quot;Estimator parameter '</span><span class="s0">{</span><span class="s1">init_param.name</span><span class="s0">}</span><span class="s2">' of estimator &quot;</span>
                    <span class="s2">f&quot;'</span><span class="s0">{</span><span class="s1">Estimator.__name__</span><span class="s0">}</span><span class="s2">' is not returned by get_params. &quot;</span>
                    <span class="s2">&quot;If it is deprecated, set its default value to None.&quot;</span>
                <span class="s1">)</span>
                <span class="s0">continue</span>

            <span class="s1">param_value = params[init_param.name]</span>
            <span class="s0">if </span><span class="s1">isinstance(param_value</span><span class="s0">, </span><span class="s1">np.ndarray):</span>
                <span class="s1">assert_array_equal(param_value</span><span class="s0">, </span><span class="s1">init_param.default)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">failure_text = (</span>
                    <span class="s2">f&quot;Parameter </span><span class="s0">{</span><span class="s1">init_param.name</span><span class="s0">} </span><span class="s2">was mutated on init. All &quot;</span>
                    <span class="s2">&quot;parameters must be stored unchanged.&quot;</span>
                <span class="s1">)</span>
                <span class="s0">if </span><span class="s1">is_scalar_nan(param_value):</span>
                    <span class="s3"># Allows to set default parameters to np.nan</span>
                    <span class="s0">assert </span><span class="s1">param_value </span><span class="s0">is </span><span class="s1">init_param.default</span><span class="s0">, </span><span class="s1">failure_text</span>
                <span class="s0">else</span><span class="s1">:</span>
                    <span class="s0">assert </span><span class="s1">param_value == init_param.default</span><span class="s0">, </span><span class="s1">failure_text</span>


<span class="s0">def </span><span class="s1">_enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s3"># Estimators with a `requires_positive_y` tag only accept strictly positive</span>
    <span class="s3"># data</span>
    <span class="s0">if </span><span class="s1">_safe_tags(estimator</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;requires_positive_y&quot;</span><span class="s1">):</span>
        <span class="s3"># Create strictly positive y. The minimal increment above 0 is 1, as</span>
        <span class="s3"># y could be of integer dtype.</span>
        <span class="s1">y += </span><span class="s4">1 </span><span class="s1">+ abs(y.min())</span>
    <span class="s0">if </span><span class="s1">_safe_tags(estimator</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;binary_only&quot;</span><span class="s1">) </span><span class="s0">and </span><span class="s1">y.size &gt; </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s1">y = np.where(y == y.flat[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">y.flat[</span><span class="s4">0</span><span class="s1">] + </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3"># Estimators in mono_output_task_error raise ValueError if y is of 1-D</span>
    <span class="s3"># Convert into a 2-D y for those estimators.</span>
    <span class="s0">if </span><span class="s1">_safe_tags(estimator</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;multioutput_only&quot;</span><span class="s1">):</span>
        <span class="s0">return </span><span class="s1">np.reshape(y</span><span class="s0">, </span><span class="s1">(-</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s0">return </span><span class="s1">y</span>


<span class="s0">def </span><span class="s1">_enforce_estimator_tags_X(estimator</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">kernel=linear_kernel):</span>
    <span class="s3"># Estimators with `1darray` in `X_types` tag only accept</span>
    <span class="s3"># X of shape (`n_samples`,)</span>
    <span class="s0">if </span><span class="s2">&quot;1darray&quot; </span><span class="s0">in </span><span class="s1">_safe_tags(estimator</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;X_types&quot;</span><span class="s1">):</span>
        <span class="s1">X = X[:</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span>
    <span class="s3"># Estimators with a `requires_positive_X` tag only accept</span>
    <span class="s3"># strictly positive data</span>
    <span class="s0">if </span><span class="s1">_safe_tags(estimator</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;requires_positive_X&quot;</span><span class="s1">):</span>
        <span class="s1">X = X - X.min()</span>
    <span class="s0">if </span><span class="s2">&quot;categorical&quot; </span><span class="s0">in </span><span class="s1">_safe_tags(estimator</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;X_types&quot;</span><span class="s1">):</span>
        <span class="s1">dtype = np.float64 </span><span class="s0">if </span><span class="s1">_safe_tags(estimator</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;allow_nan&quot;</span><span class="s1">) </span><span class="s0">else </span><span class="s1">np.int32</span>
        <span class="s1">X = np.round((X - X.min())).astype(dtype)</span>

    <span class="s0">if </span><span class="s1">estimator.__class__.__name__ == </span><span class="s2">&quot;SkewedChi2Sampler&quot;</span><span class="s1">:</span>
        <span class="s3"># SkewedChi2Sampler requires X &gt; -skewdness in transform</span>
        <span class="s1">X = X - X.min()</span>

    <span class="s3"># Pairwise estimators only accept</span>
    <span class="s3"># X of shape (`n_samples`, `n_samples`)</span>
    <span class="s0">if </span><span class="s1">_is_pairwise_metric(estimator):</span>
        <span class="s1">X = pairwise_distances(X</span><span class="s0">, </span><span class="s1">metric=</span><span class="s2">&quot;euclidean&quot;</span><span class="s1">)</span>
    <span class="s0">elif </span><span class="s1">_safe_tags(estimator</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;pairwise&quot;</span><span class="s1">):</span>
        <span class="s1">X = kernel(X</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s0">return </span><span class="s1">X</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_non_transformer_estimators_n_iter(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Test that estimators that are not transformers with a parameter</span>
    <span class="s3"># max_iter, return the attribute of n_iter_ at least 1.</span>

    <span class="s3"># These models are dependent on external solvers like</span>
    <span class="s3"># libsvm and accessing the iter parameter is non-trivial.</span>
    <span class="s3"># SelfTrainingClassifier does not perform an iteration if all samples are</span>
    <span class="s3"># labeled, hence n_iter_ = 0 is valid.</span>
    <span class="s1">not_run_check_n_iter = [</span>
        <span class="s2">&quot;Ridge&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;RidgeClassifier&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;RandomizedLasso&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;LogisticRegressionCV&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;LinearSVC&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;LogisticRegression&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;SelfTrainingClassifier&quot;</span><span class="s0">,</span>
    <span class="s1">]</span>

    <span class="s3"># Tested in test_transformer_n_iter</span>
    <span class="s1">not_run_check_n_iter += CROSS_DECOMPOSITION</span>
    <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">not_run_check_n_iter:</span>
        <span class="s0">return</span>

    <span class="s3"># LassoLars stops early for the default alpha=1.0 the iris dataset.</span>
    <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;LassoLars&quot;</span><span class="s1">:</span>
        <span class="s1">estimator = clone(estimator_orig).set_params(alpha=</span><span class="s4">0.0</span><span class="s1">)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;max_iter&quot;</span><span class="s1">):</span>
        <span class="s1">iris = load_iris()</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y_ = iris.data</span><span class="s0">, </span><span class="s1">iris.target</span>
        <span class="s1">y_ = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y_)</span>

        <span class="s1">set_random_state(estimator</span><span class="s0">, </span><span class="s4">0</span><span class="s1">)</span>

        <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>

        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y_)</span>

        <span class="s0">assert </span><span class="s1">np.all(estimator.n_iter_ &gt;= </span><span class="s4">1</span><span class="s1">)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_transformer_n_iter(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Test that transformers with a parameter max_iter, return the</span>
    <span class="s3"># attribute of n_iter_ at least 1.</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;max_iter&quot;</span><span class="s1">):</span>
        <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
            <span class="s3"># Check using default data</span>
            <span class="s1">X = [[</span><span class="s4">0.0</span><span class="s0">, </span><span class="s4">0.0</span><span class="s0">, </span><span class="s4">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">0.0</span><span class="s0">, </span><span class="s4">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">2.0</span><span class="s0">, </span><span class="s4">2.0</span><span class="s0">, </span><span class="s4">2.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">2.0</span><span class="s0">, </span><span class="s4">5.0</span><span class="s0">, </span><span class="s4">4.0</span><span class="s1">]]</span>
            <span class="s1">y_ = [[</span><span class="s4">0.1</span><span class="s0">, </span><span class="s1">-</span><span class="s4">0.2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0.9</span><span class="s0">, </span><span class="s4">1.1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0.1</span><span class="s0">, </span><span class="s1">-</span><span class="s4">0.5</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0.3</span><span class="s0">, </span><span class="s1">-</span><span class="s4">0.2</span><span class="s1">]]</span>

        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">X</span><span class="s0">, </span><span class="s1">y_ = make_blobs(</span>
                <span class="s1">n_samples=</span><span class="s4">30</span><span class="s0">,</span>
                <span class="s1">centers=[[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s0">,</span>
                <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
                <span class="s1">n_features=</span><span class="s4">2</span><span class="s0">,</span>
                <span class="s1">cluster_std=</span><span class="s4">0.1</span><span class="s0">,</span>
            <span class="s1">)</span>
            <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>
        <span class="s1">set_random_state(estimator</span><span class="s0">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y_)</span>

        <span class="s3"># These return a n_iter per component.</span>
        <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
            <span class="s0">for </span><span class="s1">iter_ </span><span class="s0">in </span><span class="s1">estimator.n_iter_:</span>
                <span class="s0">assert </span><span class="s1">iter_ &gt;= </span><span class="s4">1</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">assert </span><span class="s1">estimator.n_iter_ &gt;= </span><span class="s4">1</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_get_params_invariance(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Checks if get_params(deep=False) is a subset of get_params(deep=True)</span>
    <span class="s1">e = clone(estimator_orig)</span>

    <span class="s1">shallow_params = e.get_params(deep=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">deep_params = e.get_params(deep=</span><span class="s0">True</span><span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">all(item </span><span class="s0">in </span><span class="s1">deep_params.items() </span><span class="s0">for </span><span class="s1">item </span><span class="s0">in </span><span class="s1">shallow_params.items())</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_set_params(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Check that get_params() returns the same thing</span>
    <span class="s3"># before and after set_params() with some fuzz</span>
    <span class="s1">estimator = clone(estimator_orig)</span>

    <span class="s1">orig_params = estimator.get_params(deep=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">msg = </span><span class="s2">&quot;get_params result does not match what was passed to set_params&quot;</span>

    <span class="s1">estimator.set_params(**orig_params)</span>
    <span class="s1">curr_params = estimator.get_params(deep=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">set(orig_params.keys()) == set(curr_params.keys())</span><span class="s0">, </span><span class="s1">msg</span>
    <span class="s0">for </span><span class="s1">k</span><span class="s0">, </span><span class="s1">v </span><span class="s0">in </span><span class="s1">curr_params.items():</span>
        <span class="s0">assert </span><span class="s1">orig_params[k] </span><span class="s0">is </span><span class="s1">v</span><span class="s0">, </span><span class="s1">msg</span>

    <span class="s3"># some fuzz values</span>
    <span class="s1">test_values = [-np.inf</span><span class="s0">, </span><span class="s1">np.inf</span><span class="s0">, None</span><span class="s1">]</span>

    <span class="s1">test_params = deepcopy(orig_params)</span>
    <span class="s0">for </span><span class="s1">param_name </span><span class="s0">in </span><span class="s1">orig_params.keys():</span>
        <span class="s1">default_value = orig_params[param_name]</span>
        <span class="s0">for </span><span class="s1">value </span><span class="s0">in </span><span class="s1">test_values:</span>
            <span class="s1">test_params[param_name] = value</span>
            <span class="s0">try</span><span class="s1">:</span>
                <span class="s1">estimator.set_params(**test_params)</span>
            <span class="s0">except </span><span class="s1">(TypeError</span><span class="s0">, </span><span class="s1">ValueError) </span><span class="s0">as </span><span class="s1">e:</span>
                <span class="s1">e_type = e.__class__.__name__</span>
                <span class="s3"># Exception occurred, possibly parameter validation</span>
                <span class="s1">warnings.warn(</span>
                    <span class="s2">&quot;{0} occurred during set_params of param {1} on &quot;</span>
                    <span class="s2">&quot;{2}. It is recommended to delay parameter &quot;</span>
                    <span class="s2">&quot;validation until fit.&quot;</span><span class="s1">.format(e_type</span><span class="s0">, </span><span class="s1">param_name</span><span class="s0">, </span><span class="s1">name)</span>
                <span class="s1">)</span>

                <span class="s1">change_warning_msg = (</span>
                    <span class="s2">&quot;Estimator's parameters changed after set_params raised {}&quot;</span><span class="s1">.format(</span>
                        <span class="s1">e_type</span>
                    <span class="s1">)</span>
                <span class="s1">)</span>
                <span class="s1">params_before_exception = curr_params</span>
                <span class="s1">curr_params = estimator.get_params(deep=</span><span class="s0">False</span><span class="s1">)</span>
                <span class="s0">try</span><span class="s1">:</span>
                    <span class="s0">assert </span><span class="s1">set(params_before_exception.keys()) == set(</span>
                        <span class="s1">curr_params.keys()</span>
                    <span class="s1">)</span>
                    <span class="s0">for </span><span class="s1">k</span><span class="s0">, </span><span class="s1">v </span><span class="s0">in </span><span class="s1">curr_params.items():</span>
                        <span class="s0">assert </span><span class="s1">params_before_exception[k] </span><span class="s0">is </span><span class="s1">v</span>
                <span class="s0">except </span><span class="s1">AssertionError:</span>
                    <span class="s1">warnings.warn(change_warning_msg)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">curr_params = estimator.get_params(deep=</span><span class="s0">False</span><span class="s1">)</span>
                <span class="s0">assert </span><span class="s1">set(test_params.keys()) == set(curr_params.keys())</span><span class="s0">, </span><span class="s1">msg</span>
                <span class="s0">for </span><span class="s1">k</span><span class="s0">, </span><span class="s1">v </span><span class="s0">in </span><span class="s1">curr_params.items():</span>
                    <span class="s0">assert </span><span class="s1">test_params[k] </span><span class="s0">is </span><span class="s1">v</span><span class="s0">, </span><span class="s1">msg</span>
        <span class="s1">test_params[param_name] = default_value</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_classifiers_regression_target(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Check if classifier throws an exception when fed regression targets</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = _regression_dataset()</span>

    <span class="s1">X = _enforce_estimator_tags_X(estimator_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">e = clone(estimator_orig)</span>
    <span class="s1">msg = </span><span class="s2">&quot;Unknown label type: &quot;</span>
    <span class="s0">if not </span><span class="s1">_safe_tags(e</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">):</span>
        <span class="s0">with </span><span class="s1">raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
            <span class="s1">e.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_decision_proba_consistency(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Check whether an estimator having both decision_function and</span>
    <span class="s3"># predict_proba methods has outputs with perfect rank correlation.</span>

    <span class="s1">centers = [(</span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s4">4</span><span class="s0">, </span><span class="s4">4</span><span class="s1">)]</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s4">100</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s4">4</span><span class="s0">,</span>
        <span class="s1">centers=centers</span><span class="s0">,</span>
        <span class="s1">cluster_std=</span><span class="s4">1.0</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">True,</span>
    <span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">test_size=</span><span class="s4">0.2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">estimator = clone(estimator_orig)</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;decision_function&quot;</span><span class="s1">) </span><span class="s0">and </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
        <span class="s3"># Since the link function from decision_function() to predict_proba()</span>
        <span class="s3"># is sometimes not precise enough (typically expit), we round to the</span>
        <span class="s3"># 10th decimal to avoid numerical issues: we compare the rank</span>
        <span class="s3"># with deterministic ties rather than get platform specific rank</span>
        <span class="s3"># inversions in case of machine level differences.</span>
        <span class="s1">a = estimator.predict_proba(X_test)[:</span><span class="s0">, </span><span class="s4">1</span><span class="s1">].round(decimals=</span><span class="s4">10</span><span class="s1">)</span>
        <span class="s1">b = estimator.decision_function(X_test).round(decimals=</span><span class="s4">10</span><span class="s1">)</span>

        <span class="s1">rank_proba</span><span class="s0">, </span><span class="s1">rank_score = rankdata(a)</span><span class="s0">, </span><span class="s1">rankdata(b)</span>
        <span class="s0">try</span><span class="s1">:</span>
            <span class="s1">assert_array_almost_equal(rank_proba</span><span class="s0">, </span><span class="s1">rank_score)</span>
        <span class="s0">except </span><span class="s1">AssertionError:</span>
            <span class="s3"># Sometimes, the rounding applied on the probabilities will have</span>
            <span class="s3"># ties that are not present in the scores because it is</span>
            <span class="s3"># numerically more precise. In this case, we relax the test by</span>
            <span class="s3"># grouping the decision function scores based on the probability</span>
            <span class="s3"># rank and check that the score is monotonically increasing.</span>
            <span class="s1">grouped_y_score = np.array(</span>
                <span class="s1">[b[rank_proba == group].mean() </span><span class="s0">for </span><span class="s1">group </span><span class="s0">in </span><span class="s1">np.unique(rank_proba)]</span>
            <span class="s1">)</span>
            <span class="s1">sorted_idx = np.argsort(grouped_y_score)</span>
            <span class="s1">assert_array_equal(sorted_idx</span><span class="s0">, </span><span class="s1">np.arange(len(sorted_idx)))</span>


<span class="s0">def </span><span class="s1">check_outliers_fit_predict(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Check fit_predict for outlier detectors.</span>

    <span class="s1">n_samples = </span><span class="s4">300</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">_ = make_blobs(n_samples=n_samples</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = shuffle(X</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">7</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">estimator = clone(estimator_orig)</span>

    <span class="s1">set_random_state(estimator)</span>

    <span class="s1">y_pred = estimator.fit_predict(X)</span>
    <span class="s0">assert </span><span class="s1">y_pred.shape == (n_samples</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">y_pred.dtype.kind == </span><span class="s2">&quot;i&quot;</span>
    <span class="s1">assert_array_equal(np.unique(y_pred)</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]))</span>

    <span class="s3"># check fit_predict = fit.predict when the estimator has both a predict and</span>
    <span class="s3"># a fit_predict method. recall that it is already assumed here that the</span>
    <span class="s3"># estimator has a fit_predict method</span>
    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;predict&quot;</span><span class="s1">):</span>
        <span class="s1">y_pred_2 = estimator.fit(X).predict(X)</span>
        <span class="s1">assert_array_equal(y_pred</span><span class="s0">, </span><span class="s1">y_pred_2)</span>

    <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;contamination&quot;</span><span class="s1">):</span>
        <span class="s3"># proportion of outliers equal to contamination parameter when not</span>
        <span class="s3"># set to 'auto'</span>
        <span class="s1">expected_outliers = </span><span class="s4">30</span>
        <span class="s1">contamination = float(expected_outliers) / n_samples</span>
        <span class="s1">estimator.set_params(contamination=contamination)</span>
        <span class="s1">y_pred = estimator.fit_predict(X)</span>

        <span class="s1">num_outliers = np.sum(y_pred != </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s3"># num_outliers should be equal to expected_outliers unless</span>
        <span class="s3"># there are ties in the decision_function values. this can</span>
        <span class="s3"># only be tested for estimators with a decision_function</span>
        <span class="s3"># method</span>
        <span class="s0">if </span><span class="s1">num_outliers != expected_outliers </span><span class="s0">and </span><span class="s1">hasattr(</span>
            <span class="s1">estimator</span><span class="s0">, </span><span class="s2">&quot;decision_function&quot;</span>
        <span class="s1">):</span>
            <span class="s1">decision = estimator.decision_function(X)</span>
            <span class="s1">check_outlier_corruption(num_outliers</span><span class="s0">, </span><span class="s1">expected_outliers</span><span class="s0">, </span><span class="s1">decision)</span>


<span class="s0">def </span><span class="s1">check_fit_non_negative(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Check that proper warning is raised for non-negative X</span>
    <span class="s3"># when tag requires_positive_X is present</span>
    <span class="s1">X = np.array([[-</span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s0">with </span><span class="s1">raises(ValueError):</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">check_fit_idempotent(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Check that est.fit(X) is the same as est.fit(X).fit(X). Ideally we would</span>
    <span class="s3"># check that the estimated parameters during training (e.g. coefs_) are</span>
    <span class="s3"># the same, but having a universal comparison function for those</span>
    <span class="s3"># attributes is difficult and full of edge cases. So instead we check that</span>
    <span class="s3"># predict(), predict_proba(), decision_function() and transform() return</span>
    <span class="s3"># the same results.</span>

    <span class="s1">check_methods = [</span><span class="s2">&quot;predict&quot;</span><span class="s0">, </span><span class="s2">&quot;transform&quot;</span><span class="s0">, </span><span class="s2">&quot;decision_function&quot;</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">]</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">set_random_state(estimator)</span>
    <span class="s0">if </span><span class="s2">&quot;warm_start&quot; </span><span class="s0">in </span><span class="s1">estimator.get_params().keys():</span>
        <span class="s1">estimator.set_params(warm_start=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s1">n_samples = </span><span class="s4">100</span>
    <span class="s1">X = rng.normal(loc=</span><span class="s4">100</span><span class="s0">, </span><span class="s1">size=(n_samples</span><span class="s0">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s0">if </span><span class="s1">is_regressor(estimator_orig):</span>
        <span class="s1">y = rng.normal(size=n_samples)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y = rng.randint(low=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">high=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">train</span><span class="s0">, </span><span class="s1">test = next(ShuffleSplit(test_size=</span><span class="s4">0.2</span><span class="s0">, </span><span class="s1">random_state=rng).split(X))</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">y_train = _safe_split(estimator</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">train)</span>
    <span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test = _safe_split(estimator</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">test</span><span class="s0">, </span><span class="s1">train)</span>

    <span class="s3"># Fit for the first time</span>
    <span class="s1">estimator.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s1">result = {</span>
        <span class="s1">method: getattr(estimator</span><span class="s0">, </span><span class="s1">method)(X_test)</span>
        <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">check_methods</span>
        <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s1">method)</span>
    <span class="s1">}</span>

    <span class="s3"># Fit again</span>
    <span class="s1">set_random_state(estimator)</span>
    <span class="s1">estimator.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">check_methods:</span>
        <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s1">method):</span>
            <span class="s1">new_result = getattr(estimator</span><span class="s0">, </span><span class="s1">method)(X_test)</span>
            <span class="s0">if </span><span class="s1">np.issubdtype(new_result.dtype</span><span class="s0">, </span><span class="s1">np.floating):</span>
                <span class="s1">tol = </span><span class="s4">2 </span><span class="s1">* np.finfo(new_result.dtype).eps</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">tol = </span><span class="s4">2 </span><span class="s1">* np.finfo(np.float64).eps</span>
            <span class="s1">assert_allclose_dense_sparse(</span>
                <span class="s1">result[method]</span><span class="s0">,</span>
                <span class="s1">new_result</span><span class="s0">,</span>
                <span class="s1">atol=max(tol</span><span class="s0">, </span><span class="s4">1e-9</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">rtol=max(tol</span><span class="s0">, </span><span class="s4">1e-7</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">err_msg=</span><span class="s2">&quot;Idempotency check failed for method {}&quot;</span><span class="s1">.format(method)</span><span class="s0">,</span>
            <span class="s1">)</span>


<span class="s0">def </span><span class="s1">check_fit_check_is_fitted(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Make sure that estimator doesn't pass check_is_fitted before calling fit</span>
    <span class="s3"># and that passes check_is_fitted once it's fit.</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s4">42</span><span class="s1">)</span>

    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">set_random_state(estimator)</span>
    <span class="s0">if </span><span class="s2">&quot;warm_start&quot; </span><span class="s0">in </span><span class="s1">estimator.get_params():</span>
        <span class="s1">estimator.set_params(warm_start=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s1">n_samples = </span><span class="s4">100</span>
    <span class="s1">X = rng.normal(loc=</span><span class="s4">100</span><span class="s0">, </span><span class="s1">size=(n_samples</span><span class="s0">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s0">if </span><span class="s1">is_regressor(estimator_orig):</span>
        <span class="s1">y = rng.normal(size=n_samples)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y = rng.randint(low=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">high=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if not </span><span class="s1">_safe_tags(estimator).get(</span><span class="s2">&quot;stateless&quot;</span><span class="s0">, False</span><span class="s1">):</span>
        <span class="s3"># stateless estimators (such as FunctionTransformer) are always &quot;fit&quot;!</span>
        <span class="s0">try</span><span class="s1">:</span>
            <span class="s1">check_is_fitted(estimator)</span>
            <span class="s0">raise </span><span class="s1">AssertionError(</span>
                <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">estimator.__class__.__name__</span><span class="s0">} </span><span class="s2">passes check_is_fitted before being&quot;</span>
                <span class="s2">&quot; fit!&quot;</span>
            <span class="s1">)</span>
        <span class="s0">except </span><span class="s1">NotFittedError:</span>
            <span class="s0">pass</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">check_is_fitted(estimator)</span>
    <span class="s0">except </span><span class="s1">NotFittedError </span><span class="s0">as </span><span class="s1">e:</span>
        <span class="s0">raise </span><span class="s1">NotFittedError(</span>
            <span class="s2">&quot;Estimator fails to pass `check_is_fitted` even though it has been fit.&quot;</span>
        <span class="s1">) </span><span class="s0">from </span><span class="s1">e</span>


<span class="s0">def </span><span class="s1">check_n_features_in(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Make sure that n_features_in_ attribute doesn't exist until fit is</span>
    <span class="s3"># called, and that its value is correct.</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">set_random_state(estimator)</span>
    <span class="s0">if </span><span class="s2">&quot;warm_start&quot; </span><span class="s0">in </span><span class="s1">estimator.get_params():</span>
        <span class="s1">estimator.set_params(warm_start=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s1">n_samples = </span><span class="s4">100</span>
    <span class="s1">X = rng.normal(loc=</span><span class="s4">100</span><span class="s0">, </span><span class="s1">size=(n_samples</span><span class="s0">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s0">if </span><span class="s1">is_regressor(estimator_orig):</span>
        <span class="s1">y = rng.normal(size=n_samples)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y = rng.randint(low=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">high=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">assert not </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_features_in_&quot;</span><span class="s1">)</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;n_features_in_&quot;</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">estimator.n_features_in_ == X.shape[</span><span class="s4">1</span><span class="s1">]</span>


<span class="s0">def </span><span class="s1">check_requires_y_none(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Make sure that an estimator with requires_y=True fails gracefully when</span>
    <span class="s3"># given y=None</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">set_random_state(estimator)</span>

    <span class="s1">n_samples = </span><span class="s4">100</span>
    <span class="s1">X = rng.normal(loc=</span><span class="s4">100</span><span class="s0">, </span><span class="s1">size=(n_samples</span><span class="s0">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator</span><span class="s0">, </span><span class="s1">X)</span>

    <span class="s1">expected_err_msgs = (</span>
        <span class="s2">&quot;requires y to be passed, but the target y is None&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;Expected array-like (array or non-string sequence), got None&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;y should be a 1d array&quot;</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, None</span><span class="s1">)</span>
    <span class="s0">except </span><span class="s1">ValueError </span><span class="s0">as </span><span class="s1">ve:</span>
        <span class="s0">if not </span><span class="s1">any(msg </span><span class="s0">in </span><span class="s1">str(ve) </span><span class="s0">for </span><span class="s1">msg </span><span class="s0">in </span><span class="s1">expected_err_msgs):</span>
            <span class="s0">raise </span><span class="s1">ve</span>


<span class="s1">@ignore_warnings(category=FutureWarning)</span>
<span class="s0">def </span><span class="s1">check_n_features_in_after_fitting(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Make sure that n_features_in are checked after fitting</span>
    <span class="s1">tags = _safe_tags(estimator_orig)</span>

    <span class="s1">is_supported_X_types = (</span>
        <span class="s2">&quot;2darray&quot; </span><span class="s0">in </span><span class="s1">tags[</span><span class="s2">&quot;X_types&quot;</span><span class="s1">] </span><span class="s0">or </span><span class="s2">&quot;categorical&quot; </span><span class="s0">in </span><span class="s1">tags[</span><span class="s2">&quot;X_types&quot;</span><span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s0">if not </span><span class="s1">is_supported_X_types </span><span class="s0">or </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
        <span class="s0">return</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">set_random_state(estimator)</span>
    <span class="s0">if </span><span class="s2">&quot;warm_start&quot; </span><span class="s0">in </span><span class="s1">estimator.get_params():</span>
        <span class="s1">estimator.set_params(warm_start=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s1">n_samples = </span><span class="s4">150</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s0">, </span><span class="s4">8</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(estimator</span><span class="s0">, </span><span class="s1">X)</span>

    <span class="s0">if </span><span class="s1">is_regressor(estimator):</span>
        <span class="s1">y = rng.normal(size=n_samples)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y = rng.randint(low=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">high=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">estimator.n_features_in_ == X.shape[</span><span class="s4">1</span><span class="s1">]</span>

    <span class="s3"># check methods will check n_features_in_</span>
    <span class="s1">check_methods = [</span>
        <span class="s2">&quot;predict&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;transform&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;decision_function&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;predict_proba&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;score&quot;</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">X_bad = X[:</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">]]</span>

    <span class="s1">msg = </span><span class="s2">f&quot;X has 1 features, but </span><span class="s0">\\</span><span class="s2">w+ is expecting </span><span class="s0">{</span><span class="s1">X.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s0">} </span><span class="s2">features as input&quot;</span>
    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">check_methods:</span>
        <span class="s0">if not </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s1">method):</span>
            <span class="s0">continue</span>

        <span class="s1">callable_method = getattr(estimator</span><span class="s0">, </span><span class="s1">method)</span>
        <span class="s0">if </span><span class="s1">method == </span><span class="s2">&quot;score&quot;</span><span class="s1">:</span>
            <span class="s1">callable_method = partial(callable_method</span><span class="s0">, </span><span class="s1">y=y)</span>

        <span class="s0">with </span><span class="s1">raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
            <span class="s1">callable_method(X_bad)</span>

    <span class="s3"># partial_fit will check in the second call</span>
    <span class="s0">if not </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;partial_fit&quot;</span><span class="s1">):</span>
        <span class="s0">return</span>

    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s0">if </span><span class="s1">is_classifier(estimator):</span>
        <span class="s1">estimator.partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=np.unique(y))</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">estimator.partial_fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">estimator.n_features_in_ == X.shape[</span><span class="s4">1</span><span class="s1">]</span>

    <span class="s0">with </span><span class="s1">raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">estimator.partial_fit(X_bad</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">check_estimator_get_tags_default_keys(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># check that if _get_tags is implemented, it contains all keys from</span>
    <span class="s3"># _DEFAULT_KEYS</span>
    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s0">if not </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;_get_tags&quot;</span><span class="s1">):</span>
        <span class="s0">return</span>

    <span class="s1">tags_keys = set(estimator._get_tags().keys())</span>
    <span class="s1">default_tags_keys = set(_DEFAULT_TAGS.keys())</span>
    <span class="s0">assert </span><span class="s1">tags_keys.intersection(default_tags_keys) == default_tags_keys</span><span class="s0">, </span><span class="s1">(</span>
        <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">._get_tags() is missing entries for the following default tags&quot;</span>
        <span class="s2">f&quot;: </span><span class="s0">{</span><span class="s1">default_tags_keys - tags_keys.intersection(default_tags_keys)</span><span class="s0">}</span><span class="s2">&quot;</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">check_dataframe_column_names_consistency(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s0">import </span><span class="s1">pandas </span><span class="s0">as </span><span class="s1">pd</span>
    <span class="s0">except </span><span class="s1">ImportError:</span>
        <span class="s0">raise </span><span class="s1">SkipTest(</span>
            <span class="s2">&quot;pandas is not installed: not checking column name consistency for pandas&quot;</span>
        <span class="s1">)</span>

    <span class="s1">tags = _safe_tags(estimator_orig)</span>
    <span class="s1">is_supported_X_types = (</span>
        <span class="s2">&quot;2darray&quot; </span><span class="s0">in </span><span class="s1">tags[</span><span class="s2">&quot;X_types&quot;</span><span class="s1">] </span><span class="s0">or </span><span class="s2">&quot;categorical&quot; </span><span class="s0">in </span><span class="s1">tags[</span><span class="s2">&quot;X_types&quot;</span><span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s0">if not </span><span class="s1">is_supported_X_types </span><span class="s0">or </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
        <span class="s0">return</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">estimator = clone(estimator_orig)</span>
    <span class="s1">set_random_state(estimator)</span>

    <span class="s1">X_orig = rng.normal(size=(</span><span class="s4">150</span><span class="s0">, </span><span class="s4">8</span><span class="s1">))</span>

    <span class="s1">X_orig = _enforce_estimator_tags_X(estimator</span><span class="s0">, </span><span class="s1">X_orig)</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = X_orig.shape</span>

    <span class="s1">names = np.array([</span><span class="s2">f&quot;col_</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s2">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(n_features)])</span>
    <span class="s1">X = pd.DataFrame(X_orig</span><span class="s0">, </span><span class="s1">columns=names</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">is_regressor(estimator):</span>
        <span class="s1">y = rng.normal(size=n_samples)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y = rng.randint(low=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">high=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s3"># Check that calling `fit` does not raise any warnings about feature names.</span>
    <span class="s0">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.filterwarnings(</span>
            <span class="s2">&quot;error&quot;</span><span class="s0">,</span>
            <span class="s1">message=</span><span class="s2">&quot;X does not have valid feature names&quot;</span><span class="s0">,</span>
            <span class="s1">category=UserWarning</span><span class="s0">,</span>
            <span class="s1">module=</span><span class="s2">&quot;sklearn&quot;</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if not </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;feature_names_in_&quot;</span><span class="s1">):</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span>
            <span class="s2">&quot;Estimator does not have a feature_names_in_ &quot;</span>
            <span class="s2">&quot;attribute after fitting with a dataframe&quot;</span>
        <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">isinstance(estimator.feature_names_in_</span><span class="s0">, </span><span class="s1">np.ndarray)</span>
    <span class="s0">assert </span><span class="s1">estimator.feature_names_in_.dtype == object</span>
    <span class="s1">assert_array_equal(estimator.feature_names_in_</span><span class="s0">, </span><span class="s1">names)</span>

    <span class="s3"># Only check sklearn estimators for feature_names_in_ in docstring</span>
    <span class="s1">module_name = estimator_orig.__module__</span>
    <span class="s0">if </span><span class="s1">(</span>
        <span class="s1">module_name.startswith(</span><span class="s2">&quot;sklearn.&quot;</span><span class="s1">)</span>
        <span class="s0">and not </span><span class="s1">(</span><span class="s2">&quot;test_&quot; </span><span class="s0">in </span><span class="s1">module_name </span><span class="s0">or </span><span class="s1">module_name.endswith(</span><span class="s2">&quot;_testing&quot;</span><span class="s1">))</span>
        <span class="s0">and </span><span class="s1">(</span><span class="s2">&quot;feature_names_in_&quot; </span><span class="s0">not in </span><span class="s1">(estimator_orig.__doc__))</span>
    <span class="s1">):</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span>
            <span class="s2">f&quot;Estimator </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">does not document its feature_names_in_ attribute&quot;</span>
        <span class="s1">)</span>

    <span class="s1">check_methods = []</span>
    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">(</span>
        <span class="s2">&quot;predict&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;transform&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;decision_function&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;predict_proba&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;score&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;score_samples&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;predict_log_proba&quot;</span><span class="s0">,</span>
    <span class="s1">):</span>
        <span class="s0">if not </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s1">method):</span>
            <span class="s0">continue</span>

        <span class="s1">callable_method = getattr(estimator</span><span class="s0">, </span><span class="s1">method)</span>
        <span class="s0">if </span><span class="s1">method == </span><span class="s2">&quot;score&quot;</span><span class="s1">:</span>
            <span class="s1">callable_method = partial(callable_method</span><span class="s0">, </span><span class="s1">y=y)</span>
        <span class="s1">check_methods.append((method</span><span class="s0">, </span><span class="s1">callable_method))</span>

    <span class="s0">for </span><span class="s1">_</span><span class="s0">, </span><span class="s1">method </span><span class="s0">in </span><span class="s1">check_methods:</span>
        <span class="s0">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.filterwarnings(</span>
                <span class="s2">&quot;error&quot;</span><span class="s0">,</span>
                <span class="s1">message=</span><span class="s2">&quot;X does not have valid feature names&quot;</span><span class="s0">,</span>
                <span class="s1">category=UserWarning</span><span class="s0">,</span>
                <span class="s1">module=</span><span class="s2">&quot;sklearn&quot;</span><span class="s0">,</span>
            <span class="s1">)</span>
            <span class="s1">method(X)  </span><span class="s3"># works without UserWarning for valid features</span>

    <span class="s1">invalid_names = [</span>
        <span class="s1">(names[::-</span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s2">&quot;Feature names must be in the same order as they were in fit.&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">[</span><span class="s2">f&quot;another_prefix_</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s2">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(n_features)]</span><span class="s0">,</span>
            <span class="s1">(</span>
                <span class="s2">&quot;Feature names unseen at fit time:</span><span class="s0">\n</span><span class="s2">- another_prefix_0</span><span class="s0">\n</span><span class="s2">-&quot;</span>
                <span class="s2">&quot; another_prefix_1</span><span class="s0">\n</span><span class="s2">&quot;</span>
            <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">names[:</span><span class="s4">3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s2">f&quot;Feature names seen at fit time, yet now missing:</span><span class="s0">\n</span><span class="s2">- </span><span class="s0">{</span><span class="s1">min(names[</span><span class="s4">3</span><span class="s1">:])</span><span class="s0">}\n</span><span class="s2">&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">params = {</span>
        <span class="s1">key: value</span>
        <span class="s0">for </span><span class="s1">key</span><span class="s0">, </span><span class="s1">value </span><span class="s0">in </span><span class="s1">estimator.get_params().items()</span>
        <span class="s0">if </span><span class="s2">&quot;early_stopping&quot; </span><span class="s0">in </span><span class="s1">key</span>
    <span class="s1">}</span>
    <span class="s1">early_stopping_enabled = any(value </span><span class="s0">is True for </span><span class="s1">value </span><span class="s0">in </span><span class="s1">params.values())</span>

    <span class="s0">for </span><span class="s1">invalid_name</span><span class="s0">, </span><span class="s1">additional_message </span><span class="s0">in </span><span class="s1">invalid_names:</span>
        <span class="s1">X_bad = pd.DataFrame(X</span><span class="s0">, </span><span class="s1">columns=invalid_name</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>

        <span class="s1">expected_msg = re.escape(</span>
            <span class="s2">&quot;The feature names should match those that were passed during fit.</span><span class="s0">\n</span><span class="s2">&quot;</span>
            <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">additional_message</span><span class="s0">}</span><span class="s2">&quot;</span>
        <span class="s1">)</span>
        <span class="s0">for </span><span class="s1">name</span><span class="s0">, </span><span class="s1">method </span><span class="s0">in </span><span class="s1">check_methods:</span>
            <span class="s0">with </span><span class="s1">raises(</span>
                <span class="s1">ValueError</span><span class="s0">, </span><span class="s1">match=expected_msg</span><span class="s0">, </span><span class="s1">err_msg=</span><span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">did not raise&quot;</span>
            <span class="s1">):</span>
                <span class="s1">method(X_bad)</span>

        <span class="s3"># partial_fit checks on second call</span>
        <span class="s3"># Do not call partial fit if early_stopping is on</span>
        <span class="s0">if not </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s2">&quot;partial_fit&quot;</span><span class="s1">) </span><span class="s0">or </span><span class="s1">early_stopping_enabled:</span>
            <span class="s0">continue</span>

        <span class="s1">estimator = clone(estimator_orig)</span>
        <span class="s0">if </span><span class="s1">is_classifier(estimator):</span>
            <span class="s1">classes = np.unique(y)</span>
            <span class="s1">estimator.partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=classes)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">estimator.partial_fit(X</span><span class="s0">, </span><span class="s1">y)</span>

        <span class="s0">with </span><span class="s1">raises(ValueError</span><span class="s0">, </span><span class="s1">match=expected_msg):</span>
            <span class="s1">estimator.partial_fit(X_bad</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">check_transformer_get_feature_names_out(name</span><span class="s0">, </span><span class="s1">transformer_orig):</span>
    <span class="s1">tags = transformer_orig._get_tags()</span>
    <span class="s0">if </span><span class="s2">&quot;2darray&quot; </span><span class="s0">not in </span><span class="s1">tags[</span><span class="s2">&quot;X_types&quot;</span><span class="s1">] </span><span class="s0">or </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
        <span class="s0">return</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s4">30</span><span class="s0">,</span>
        <span class="s1">centers=[[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s4">2</span><span class="s0">,</span>
        <span class="s1">cluster_std=</span><span class="s4">0.1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X = StandardScaler().fit_transform(X)</span>

    <span class="s1">transformer = clone(transformer_orig)</span>
    <span class="s1">X = _enforce_estimator_tags_X(transformer</span><span class="s0">, </span><span class="s1">X)</span>

    <span class="s1">n_features = X.shape[</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">set_random_state(transformer)</span>

    <span class="s1">y_ = y</span>
    <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
        <span class="s1">y_ = np.c_[np.asarray(y)</span><span class="s0">, </span><span class="s1">np.asarray(y)]</span>
        <span class="s1">y_[::</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">] *= </span><span class="s4">2</span>

    <span class="s1">X_transform = transformer.fit_transform(X</span><span class="s0">, </span><span class="s1">y=y_)</span>
    <span class="s1">input_features = [</span><span class="s2">f&quot;feature</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s2">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(n_features)]</span>

    <span class="s3"># input_features names is not the same length as n_features_in_</span>
    <span class="s0">with </span><span class="s1">raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;input_features should have length equal&quot;</span><span class="s1">):</span>
        <span class="s1">transformer.get_feature_names_out(input_features[::</span><span class="s4">2</span><span class="s1">])</span>

    <span class="s1">feature_names_out = transformer.get_feature_names_out(input_features)</span>
    <span class="s0">assert </span><span class="s1">feature_names_out </span><span class="s0">is not None</span>
    <span class="s0">assert </span><span class="s1">isinstance(feature_names_out</span><span class="s0">, </span><span class="s1">np.ndarray)</span>
    <span class="s0">assert </span><span class="s1">feature_names_out.dtype == object</span>
    <span class="s0">assert </span><span class="s1">all(isinstance(name</span><span class="s0">, </span><span class="s1">str) </span><span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">feature_names_out)</span>

    <span class="s0">if </span><span class="s1">isinstance(X_transform</span><span class="s0">, </span><span class="s1">tuple):</span>
        <span class="s1">n_features_out = X_transform[</span><span class="s4">0</span><span class="s1">].shape[</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">n_features_out = X_transform.shape[</span><span class="s4">1</span><span class="s1">]</span>

    <span class="s0">assert </span><span class="s1">(</span>
        <span class="s1">len(feature_names_out) == n_features_out</span>
    <span class="s1">)</span><span class="s0">, </span><span class="s2">f&quot;Expected </span><span class="s0">{</span><span class="s1">n_features_out</span><span class="s0">} </span><span class="s2">feature names, got </span><span class="s0">{</span><span class="s1">len(feature_names_out)</span><span class="s0">}</span><span class="s2">&quot;</span>


<span class="s0">def </span><span class="s1">check_transformer_get_feature_names_out_pandas(name</span><span class="s0">, </span><span class="s1">transformer_orig):</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s0">import </span><span class="s1">pandas </span><span class="s0">as </span><span class="s1">pd</span>
    <span class="s0">except </span><span class="s1">ImportError:</span>
        <span class="s0">raise </span><span class="s1">SkipTest(</span>
            <span class="s2">&quot;pandas is not installed: not checking column name consistency for pandas&quot;</span>
        <span class="s1">)</span>

    <span class="s1">tags = transformer_orig._get_tags()</span>
    <span class="s0">if </span><span class="s2">&quot;2darray&quot; </span><span class="s0">not in </span><span class="s1">tags[</span><span class="s2">&quot;X_types&quot;</span><span class="s1">] </span><span class="s0">or </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
        <span class="s0">return</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s4">30</span><span class="s0">,</span>
        <span class="s1">centers=[[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s4">2</span><span class="s0">,</span>
        <span class="s1">cluster_std=</span><span class="s4">0.1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X = StandardScaler().fit_transform(X)</span>

    <span class="s1">transformer = clone(transformer_orig)</span>
    <span class="s1">X = _enforce_estimator_tags_X(transformer</span><span class="s0">, </span><span class="s1">X)</span>

    <span class="s1">n_features = X.shape[</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">set_random_state(transformer)</span>

    <span class="s1">y_ = y</span>
    <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
        <span class="s1">y_ = np.c_[np.asarray(y)</span><span class="s0">, </span><span class="s1">np.asarray(y)]</span>
        <span class="s1">y_[::</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">] *= </span><span class="s4">2</span>

    <span class="s1">feature_names_in = [</span><span class="s2">f&quot;col</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s2">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(n_features)]</span>
    <span class="s1">df = pd.DataFrame(X</span><span class="s0">, </span><span class="s1">columns=feature_names_in</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">X_transform = transformer.fit_transform(df</span><span class="s0">, </span><span class="s1">y=y_)</span>

    <span class="s3"># error is raised when `input_features` do not match feature_names_in</span>
    <span class="s1">invalid_feature_names = [</span><span class="s2">f&quot;bad</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s2">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(n_features)]</span>
    <span class="s0">with </span><span class="s1">raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;input_features is not equal to feature_names_in_&quot;</span><span class="s1">):</span>
        <span class="s1">transformer.get_feature_names_out(invalid_feature_names)</span>

    <span class="s1">feature_names_out_default = transformer.get_feature_names_out()</span>
    <span class="s1">feature_names_in_explicit_names = transformer.get_feature_names_out(</span>
        <span class="s1">feature_names_in</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(feature_names_out_default</span><span class="s0">, </span><span class="s1">feature_names_in_explicit_names)</span>

    <span class="s0">if </span><span class="s1">isinstance(X_transform</span><span class="s0">, </span><span class="s1">tuple):</span>
        <span class="s1">n_features_out = X_transform[</span><span class="s4">0</span><span class="s1">].shape[</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">n_features_out = X_transform.shape[</span><span class="s4">1</span><span class="s1">]</span>

    <span class="s0">assert </span><span class="s1">(</span>
        <span class="s1">len(feature_names_out_default) == n_features_out</span>
    <span class="s1">)</span><span class="s0">, </span><span class="s2">f&quot;Expected </span><span class="s0">{</span><span class="s1">n_features_out</span><span class="s0">} </span><span class="s2">feature names, got </span><span class="s0">{</span><span class="s1">len(feature_names_out_default)</span><span class="s0">}</span><span class="s2">&quot;</span>


<span class="s0">def </span><span class="s1">check_param_validation(name</span><span class="s0">, </span><span class="s1">estimator_orig):</span>
    <span class="s3"># Check that an informative error is raised when the value of a constructor</span>
    <span class="s3"># parameter does not have an appropriate type or value.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = rng.uniform(size=(</span><span class="s4">20</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">y = rng.randint(</span><span class="s4">0</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">y = _enforce_estimator_tags_y(estimator_orig</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">estimator_params = estimator_orig.get_params(deep=</span><span class="s0">False</span><span class="s1">).keys()</span>

    <span class="s3"># check that there is a constraint for each parameter</span>
    <span class="s0">if </span><span class="s1">estimator_params:</span>
        <span class="s1">validation_params = estimator_orig._parameter_constraints.keys()</span>
        <span class="s1">unexpected_params = set(validation_params) - set(estimator_params)</span>
        <span class="s1">missing_params = set(estimator_params) - set(validation_params)</span>
        <span class="s1">err_msg = (</span>
            <span class="s2">f&quot;Mismatch between _parameter_constraints and the parameters of </span><span class="s0">{</span><span class="s1">name</span><span class="s0">}</span><span class="s2">.&quot;</span>
            <span class="s2">f&quot;</span><span class="s0">\n</span><span class="s2">Consider the unexpected parameters </span><span class="s0">{</span><span class="s1">unexpected_params</span><span class="s0">} </span><span class="s2">and expected but&quot;</span>
            <span class="s2">f&quot; missing parameters </span><span class="s0">{</span><span class="s1">missing_params</span><span class="s0">}</span><span class="s2">&quot;</span>
        <span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">validation_params == estimator_params</span><span class="s0">, </span><span class="s1">err_msg</span>

    <span class="s3"># this object does not have a valid type for sure for all params</span>
    <span class="s1">param_with_bad_type = type(</span><span class="s2">&quot;BadType&quot;</span><span class="s0">, </span><span class="s1">()</span><span class="s0">, </span><span class="s1">{})()</span>

    <span class="s1">fit_methods = [</span><span class="s2">&quot;fit&quot;</span><span class="s0">, </span><span class="s2">&quot;partial_fit&quot;</span><span class="s0">, </span><span class="s2">&quot;fit_transform&quot;</span><span class="s0">, </span><span class="s2">&quot;fit_predict&quot;</span><span class="s1">]</span>

    <span class="s0">for </span><span class="s1">param_name </span><span class="s0">in </span><span class="s1">estimator_params:</span>
        <span class="s1">constraints = estimator_orig._parameter_constraints[param_name]</span>

        <span class="s0">if </span><span class="s1">constraints == </span><span class="s2">&quot;no_validation&quot;</span><span class="s1">:</span>
            <span class="s3"># This parameter is not validated</span>
            <span class="s0">continue</span>

        <span class="s3"># Mixing an interval of reals and an interval of integers must be avoided.</span>
        <span class="s0">if </span><span class="s1">any(</span>
            <span class="s1">isinstance(constraint</span><span class="s0">, </span><span class="s1">Interval) </span><span class="s0">and </span><span class="s1">constraint.type == Integral</span>
            <span class="s0">for </span><span class="s1">constraint </span><span class="s0">in </span><span class="s1">constraints</span>
        <span class="s1">) </span><span class="s0">and </span><span class="s1">any(</span>
            <span class="s1">isinstance(constraint</span><span class="s0">, </span><span class="s1">Interval) </span><span class="s0">and </span><span class="s1">constraint.type == Real</span>
            <span class="s0">for </span><span class="s1">constraint </span><span class="s0">in </span><span class="s1">constraints</span>
        <span class="s1">):</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span>
                <span class="s2">f&quot;The constraint for parameter </span><span class="s0">{</span><span class="s1">param_name</span><span class="s0">} </span><span class="s2">of </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">can't have a mix&quot;</span>
                <span class="s2">&quot; of intervals of Integral and Real types. Use the type RealNotInt&quot;</span>
                <span class="s2">&quot; instead of Real.&quot;</span>
            <span class="s1">)</span>

        <span class="s1">match = </span><span class="s2">rf&quot;The '</span><span class="s0">{</span><span class="s1">param_name</span><span class="s0">}</span><span class="s2">' parameter of </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">must be .* Got .* instead.&quot;</span>
        <span class="s1">err_msg = (</span>
            <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">does not raise an informative error message when the &quot;</span>
            <span class="s2">f&quot;parameter </span><span class="s0">{</span><span class="s1">param_name</span><span class="s0">} </span><span class="s2">does not have a valid type or value.&quot;</span>
        <span class="s1">)</span>

        <span class="s1">estimator = clone(estimator_orig)</span>

        <span class="s3"># First, check that the error is raised if param doesn't match any valid type.</span>
        <span class="s1">estimator.set_params(**{param_name: param_with_bad_type})</span>

        <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">fit_methods:</span>
            <span class="s0">if not </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s1">method):</span>
                <span class="s3"># the method is not accessible with the current set of parameters</span>
                <span class="s0">continue</span>

            <span class="s0">with </span><span class="s1">raises(InvalidParameterError</span><span class="s0">, </span><span class="s1">match=match</span><span class="s0">, </span><span class="s1">err_msg=err_msg):</span>
                <span class="s0">if </span><span class="s1">any(</span>
                    <span class="s1">isinstance(X_type</span><span class="s0">, </span><span class="s1">str) </span><span class="s0">and </span><span class="s1">X_type.endswith(</span><span class="s2">&quot;labels&quot;</span><span class="s1">)</span>
                    <span class="s0">for </span><span class="s1">X_type </span><span class="s0">in </span><span class="s1">_safe_tags(estimator</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;X_types&quot;</span><span class="s1">)</span>
                <span class="s1">):</span>
                    <span class="s3"># The estimator is a label transformer and take only `y`</span>
                    <span class="s1">getattr(estimator</span><span class="s0">, </span><span class="s1">method)(y)</span>
                <span class="s0">else</span><span class="s1">:</span>
                    <span class="s1">getattr(estimator</span><span class="s0">, </span><span class="s1">method)(X</span><span class="s0">, </span><span class="s1">y)</span>

        <span class="s3"># Then, for constraints that are more than a type constraint, check that the</span>
        <span class="s3"># error is raised if param does match a valid type but does not match any valid</span>
        <span class="s3"># value for this type.</span>
        <span class="s1">constraints = [make_constraint(constraint) </span><span class="s0">for </span><span class="s1">constraint </span><span class="s0">in </span><span class="s1">constraints]</span>

        <span class="s0">for </span><span class="s1">constraint </span><span class="s0">in </span><span class="s1">constraints:</span>
            <span class="s0">try</span><span class="s1">:</span>
                <span class="s1">bad_value = generate_invalid_param_val(constraint)</span>
            <span class="s0">except </span><span class="s1">NotImplementedError:</span>
                <span class="s0">continue</span>

            <span class="s1">estimator.set_params(**{param_name: bad_value})</span>

            <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">fit_methods:</span>
                <span class="s0">if not </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s1">method):</span>
                    <span class="s3"># the method is not accessible with the current set of parameters</span>
                    <span class="s0">continue</span>

                <span class="s0">with </span><span class="s1">raises(InvalidParameterError</span><span class="s0">, </span><span class="s1">match=match</span><span class="s0">, </span><span class="s1">err_msg=err_msg):</span>
                    <span class="s0">if </span><span class="s1">any(</span>
                        <span class="s1">X_type.endswith(</span><span class="s2">&quot;labels&quot;</span><span class="s1">)</span>
                        <span class="s0">for </span><span class="s1">X_type </span><span class="s0">in </span><span class="s1">_safe_tags(estimator</span><span class="s0">, </span><span class="s1">key=</span><span class="s2">&quot;X_types&quot;</span><span class="s1">)</span>
                    <span class="s1">):</span>
                        <span class="s3"># The estimator is a label transformer and take only `y`</span>
                        <span class="s1">getattr(estimator</span><span class="s0">, </span><span class="s1">method)(y)</span>
                    <span class="s0">else</span><span class="s1">:</span>
                        <span class="s1">getattr(estimator</span><span class="s0">, </span><span class="s1">method)(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">check_set_output_transform(name</span><span class="s0">, </span><span class="s1">transformer_orig):</span>
    <span class="s3"># Check transformer.set_output with the default configuration does not</span>
    <span class="s3"># change the transform output.</span>
    <span class="s1">tags = transformer_orig._get_tags()</span>
    <span class="s0">if </span><span class="s2">&quot;2darray&quot; </span><span class="s0">not in </span><span class="s1">tags[</span><span class="s2">&quot;X_types&quot;</span><span class="s1">] </span><span class="s0">or </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
        <span class="s0">return</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">transformer = clone(transformer_orig)</span>

    <span class="s1">X = rng.uniform(size=(</span><span class="s4">20</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(transformer_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = rng.randint(</span><span class="s4">0</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">y = _enforce_estimator_tags_y(transformer_orig</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">set_random_state(transformer)</span>

    <span class="s0">def </span><span class="s1">fit_then_transform(est):</span>
        <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
            <span class="s0">return </span><span class="s1">est.fit(X</span><span class="s0">, </span><span class="s1">y).transform(X</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s0">return </span><span class="s1">est.fit(X</span><span class="s0">, </span><span class="s1">y).transform(X)</span>

    <span class="s0">def </span><span class="s1">fit_transform(est):</span>
        <span class="s0">return </span><span class="s1">est.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">transform_methods = {</span>
        <span class="s2">&quot;transform&quot;</span><span class="s1">: fit_then_transform</span><span class="s0">,</span>
        <span class="s2">&quot;fit_transform&quot;</span><span class="s1">: fit_transform</span><span class="s0">,</span>
    <span class="s1">}</span>
    <span class="s0">for </span><span class="s1">name</span><span class="s0">, </span><span class="s1">transform_method </span><span class="s0">in </span><span class="s1">transform_methods.items():</span>
        <span class="s1">transformer = clone(transformer)</span>
        <span class="s0">if not </span><span class="s1">hasattr(transformer</span><span class="s0">, </span><span class="s1">name):</span>
            <span class="s0">continue</span>
        <span class="s1">X_trans_no_setting = transform_method(transformer)</span>

        <span class="s3"># Auto wrapping only wraps the first array</span>
        <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
            <span class="s1">X_trans_no_setting = X_trans_no_setting[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s1">transformer.set_output(transform=</span><span class="s2">&quot;default&quot;</span><span class="s1">)</span>
        <span class="s1">X_trans_default = transform_method(transformer)</span>

        <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
            <span class="s1">X_trans_default = X_trans_default[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s3"># Default and no setting -&gt; returns the same transformation</span>
        <span class="s1">assert_allclose_dense_sparse(X_trans_no_setting</span><span class="s0">, </span><span class="s1">X_trans_default)</span>


<span class="s0">def </span><span class="s1">_output_from_fit_transform(transformer</span><span class="s0">, </span><span class="s1">name</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">df</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s5">&quot;&quot;&quot;Generate output to test `set_output` for different configuration: 
 
    - calling either `fit.transform` or `fit_transform`; 
    - passing either a dataframe or a numpy array to fit; 
    - passing either a dataframe or a numpy array to transform. 
    &quot;&quot;&quot;</span>
    <span class="s1">outputs = {}</span>

    <span class="s3"># fit then transform case:</span>
    <span class="s1">cases = [</span>
        <span class="s1">(</span><span class="s2">&quot;fit.transform/df/df&quot;</span><span class="s0">, </span><span class="s1">df</span><span class="s0">, </span><span class="s1">df)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;fit.transform/df/array&quot;</span><span class="s0">, </span><span class="s1">df</span><span class="s0">, </span><span class="s1">X)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;fit.transform/array/df&quot;</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">df)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;fit.transform/array/array&quot;</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">X)</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s0">if </span><span class="s1">all(hasattr(transformer</span><span class="s0">, </span><span class="s1">meth) </span><span class="s0">for </span><span class="s1">meth </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;fit&quot;</span><span class="s0">, </span><span class="s2">&quot;transform&quot;</span><span class="s1">]):</span>
        <span class="s0">for </span><span class="s1">(</span>
            <span class="s1">case</span><span class="s0">,</span>
            <span class="s1">data_fit</span><span class="s0">,</span>
            <span class="s1">data_transform</span><span class="s0">,</span>
        <span class="s1">) </span><span class="s0">in </span><span class="s1">cases:</span>
            <span class="s1">transformer.fit(data_fit</span><span class="s0">, </span><span class="s1">y)</span>
            <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
                <span class="s1">X_trans</span><span class="s0">, </span><span class="s1">_ = transformer.transform(data_transform</span><span class="s0">, </span><span class="s1">y)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">X_trans = transformer.transform(data_transform)</span>
            <span class="s1">outputs[case] = (X_trans</span><span class="s0">, </span><span class="s1">transformer.get_feature_names_out())</span>

    <span class="s3"># fit_transform case:</span>
    <span class="s1">cases = [</span>
        <span class="s1">(</span><span class="s2">&quot;fit_transform/df&quot;</span><span class="s0">, </span><span class="s1">df)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;fit_transform/array&quot;</span><span class="s0">, </span><span class="s1">X)</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s0">if </span><span class="s1">hasattr(transformer</span><span class="s0">, </span><span class="s2">&quot;fit_transform&quot;</span><span class="s1">):</span>
        <span class="s0">for </span><span class="s1">case</span><span class="s0">, </span><span class="s1">data </span><span class="s0">in </span><span class="s1">cases:</span>
            <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CROSS_DECOMPOSITION:</span>
                <span class="s1">X_trans</span><span class="s0">, </span><span class="s1">_ = transformer.fit_transform(data</span><span class="s0">, </span><span class="s1">y)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">X_trans = transformer.fit_transform(data</span><span class="s0">, </span><span class="s1">y)</span>
            <span class="s1">outputs[case] = (X_trans</span><span class="s0">, </span><span class="s1">transformer.get_feature_names_out())</span>

    <span class="s0">return </span><span class="s1">outputs</span>


<span class="s0">def </span><span class="s1">_check_generated_dataframe(name</span><span class="s0">, </span><span class="s1">case</span><span class="s0">, </span><span class="s1">index</span><span class="s0">, </span><span class="s1">outputs_default</span><span class="s0">, </span><span class="s1">outputs_pandas):</span>
    <span class="s0">import </span><span class="s1">pandas </span><span class="s0">as </span><span class="s1">pd</span>

    <span class="s1">X_trans</span><span class="s0">, </span><span class="s1">feature_names_default = outputs_default</span>
    <span class="s1">df_trans</span><span class="s0">, </span><span class="s1">feature_names_pandas = outputs_pandas</span>

    <span class="s0">assert </span><span class="s1">isinstance(df_trans</span><span class="s0">, </span><span class="s1">pd.DataFrame)</span>
    <span class="s3"># We always rely on the output of `get_feature_names_out` of the</span>
    <span class="s3"># transformer used to generate the dataframe as a ground-truth of the</span>
    <span class="s3"># columns.</span>
    <span class="s3"># If a dataframe is passed into transform, then the output should have the same</span>
    <span class="s3"># index</span>
    <span class="s1">expected_index = index </span><span class="s0">if </span><span class="s1">case.endswith(</span><span class="s2">&quot;df&quot;</span><span class="s1">) </span><span class="s0">else None</span>
    <span class="s1">expected_dataframe = pd.DataFrame(</span>
        <span class="s1">X_trans</span><span class="s0">, </span><span class="s1">columns=feature_names_pandas</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False, </span><span class="s1">index=expected_index</span>
    <span class="s1">)</span>

    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">pd.testing.assert_frame_equal(df_trans</span><span class="s0">, </span><span class="s1">expected_dataframe)</span>
    <span class="s0">except </span><span class="s1">AssertionError </span><span class="s0">as </span><span class="s1">e:</span>
        <span class="s0">raise </span><span class="s1">AssertionError(</span>
            <span class="s2">f&quot;</span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">does not generate a valid dataframe in the </span><span class="s0">{</span><span class="s1">case</span><span class="s0">} </span><span class="s2">&quot;</span>
            <span class="s2">&quot;case. The generated dataframe is not equal to the expected &quot;</span>
            <span class="s2">f&quot;dataframe. The error message is: </span><span class="s0">{</span><span class="s1">e</span><span class="s0">}</span><span class="s2">&quot;</span>
        <span class="s1">) </span><span class="s0">from </span><span class="s1">e</span>


<span class="s0">def </span><span class="s1">check_set_output_transform_pandas(name</span><span class="s0">, </span><span class="s1">transformer_orig):</span>
    <span class="s3"># Check transformer.set_output configures the output of transform=&quot;pandas&quot;.</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s0">import </span><span class="s1">pandas </span><span class="s0">as </span><span class="s1">pd</span>
    <span class="s0">except </span><span class="s1">ImportError:</span>
        <span class="s0">raise </span><span class="s1">SkipTest(</span>
            <span class="s2">&quot;pandas is not installed: not checking column name consistency for pandas&quot;</span>
        <span class="s1">)</span>

    <span class="s1">tags = transformer_orig._get_tags()</span>
    <span class="s0">if </span><span class="s2">&quot;2darray&quot; </span><span class="s0">not in </span><span class="s1">tags[</span><span class="s2">&quot;X_types&quot;</span><span class="s1">] </span><span class="s0">or </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
        <span class="s0">return</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">transformer = clone(transformer_orig)</span>

    <span class="s1">X = rng.uniform(size=(</span><span class="s4">20</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(transformer_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = rng.randint(</span><span class="s4">0</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">y = _enforce_estimator_tags_y(transformer_orig</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">set_random_state(transformer)</span>

    <span class="s1">feature_names_in = [</span><span class="s2">f&quot;col</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s2">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(X.shape[</span><span class="s4">1</span><span class="s1">])]</span>
    <span class="s1">index = [</span><span class="s2">f&quot;index</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s2">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(X.shape[</span><span class="s4">0</span><span class="s1">])]</span>
    <span class="s1">df = pd.DataFrame(X</span><span class="s0">, </span><span class="s1">columns=feature_names_in</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False, </span><span class="s1">index=index)</span>

    <span class="s1">transformer_default = clone(transformer).set_output(transform=</span><span class="s2">&quot;default&quot;</span><span class="s1">)</span>
    <span class="s1">outputs_default = _output_from_fit_transform(transformer_default</span><span class="s0">, </span><span class="s1">name</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">df</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">transformer_pandas = clone(transformer).set_output(transform=</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">outputs_pandas = _output_from_fit_transform(transformer_pandas</span><span class="s0">, </span><span class="s1">name</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">df</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">except </span><span class="s1">ValueError </span><span class="s0">as </span><span class="s1">e:</span>
        <span class="s3"># transformer does not support sparse data</span>
        <span class="s0">assert </span><span class="s2">&quot;Pandas output does not support sparse data.&quot; </span><span class="s0">in </span><span class="s1">str(e)</span><span class="s0">, </span><span class="s1">e</span>
        <span class="s0">return</span>

    <span class="s0">for </span><span class="s1">case </span><span class="s0">in </span><span class="s1">outputs_default:</span>
        <span class="s1">_check_generated_dataframe(</span>
            <span class="s1">name</span><span class="s0">, </span><span class="s1">case</span><span class="s0">, </span><span class="s1">index</span><span class="s0">, </span><span class="s1">outputs_default[case]</span><span class="s0">, </span><span class="s1">outputs_pandas[case]</span>
        <span class="s1">)</span>


<span class="s0">def </span><span class="s1">check_global_output_transform_pandas(name</span><span class="s0">, </span><span class="s1">transformer_orig):</span>
    <span class="s5">&quot;&quot;&quot;Check that setting globally the output of a transformer to pandas lead to the 
    right results.&quot;&quot;&quot;</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s0">import </span><span class="s1">pandas </span><span class="s0">as </span><span class="s1">pd</span>
    <span class="s0">except </span><span class="s1">ImportError:</span>
        <span class="s0">raise </span><span class="s1">SkipTest(</span>
            <span class="s2">&quot;pandas is not installed: not checking column name consistency for pandas&quot;</span>
        <span class="s1">)</span>

    <span class="s1">tags = transformer_orig._get_tags()</span>
    <span class="s0">if </span><span class="s2">&quot;2darray&quot; </span><span class="s0">not in </span><span class="s1">tags[</span><span class="s2">&quot;X_types&quot;</span><span class="s1">] </span><span class="s0">or </span><span class="s1">tags[</span><span class="s2">&quot;no_validation&quot;</span><span class="s1">]:</span>
        <span class="s0">return</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">transformer = clone(transformer_orig)</span>

    <span class="s1">X = rng.uniform(size=(</span><span class="s4">20</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">X = _enforce_estimator_tags_X(transformer_orig</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">y = rng.randint(</span><span class="s4">0</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">y = _enforce_estimator_tags_y(transformer_orig</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">set_random_state(transformer)</span>

    <span class="s1">feature_names_in = [</span><span class="s2">f&quot;col</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s2">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(X.shape[</span><span class="s4">1</span><span class="s1">])]</span>
    <span class="s1">index = [</span><span class="s2">f&quot;index</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s2">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(X.shape[</span><span class="s4">0</span><span class="s1">])]</span>
    <span class="s1">df = pd.DataFrame(X</span><span class="s0">, </span><span class="s1">columns=feature_names_in</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False, </span><span class="s1">index=index)</span>

    <span class="s1">transformer_default = clone(transformer).set_output(transform=</span><span class="s2">&quot;default&quot;</span><span class="s1">)</span>
    <span class="s1">outputs_default = _output_from_fit_transform(transformer_default</span><span class="s0">, </span><span class="s1">name</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">df</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">transformer_pandas = clone(transformer)</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s0">with </span><span class="s1">config_context(transform_output=</span><span class="s2">&quot;pandas&quot;</span><span class="s1">):</span>
            <span class="s1">outputs_pandas = _output_from_fit_transform(</span>
                <span class="s1">transformer_pandas</span><span class="s0">, </span><span class="s1">name</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">df</span><span class="s0">, </span><span class="s1">y</span>
            <span class="s1">)</span>
    <span class="s0">except </span><span class="s1">ValueError </span><span class="s0">as </span><span class="s1">e:</span>
        <span class="s3"># transformer does not support sparse data</span>
        <span class="s0">assert </span><span class="s2">&quot;Pandas output does not support sparse data.&quot; </span><span class="s0">in </span><span class="s1">str(e)</span><span class="s0">, </span><span class="s1">e</span>
        <span class="s0">return</span>

    <span class="s0">for </span><span class="s1">case </span><span class="s0">in </span><span class="s1">outputs_default:</span>
        <span class="s1">_check_generated_dataframe(</span>
            <span class="s1">name</span><span class="s0">, </span><span class="s1">case</span><span class="s0">, </span><span class="s1">index</span><span class="s0">, </span><span class="s1">outputs_default[case]</span><span class="s0">, </span><span class="s1">outputs_pandas[case]</span>
        <span class="s1">)</span>
</pre>
</body>
</html>