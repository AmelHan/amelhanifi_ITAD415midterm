<html>
<head>
<title>test_distributed_estimation.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_distributed_estimation.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">assert_equal</span><span class="s0">, </span><span class="s1">assert_</span><span class="s0">, </span><span class="s1">assert_allclose</span>
<span class="s0">from </span><span class="s1">statsmodels.regression.linear_model </span><span class="s0">import </span><span class="s1">OLS</span>
<span class="s0">from </span><span class="s1">statsmodels.genmod.generalized_linear_model </span><span class="s0">import </span><span class="s1">GLM</span>
<span class="s0">from </span><span class="s1">statsmodels.genmod.families </span><span class="s0">import </span><span class="s1">Binomial</span>
<span class="s0">from </span><span class="s1">statsmodels.base.distributed_estimation </span><span class="s0">import </span><span class="s1">_calc_grad</span><span class="s0">, </span><span class="s1">\</span>
    <span class="s1">_calc_wdesign_mat</span><span class="s0">, </span><span class="s1">_est_regularized_debiased</span><span class="s0">, </span><span class="s1">_join_debiased</span><span class="s0">, </span><span class="s1">\</span>
    <span class="s1">_est_regularized_naive</span><span class="s0">, </span><span class="s1">_est_unregularized_naive</span><span class="s0">, </span><span class="s1">_join_naive</span><span class="s0">, </span><span class="s1">\</span>
    <span class="s1">DistributedModel</span>


<span class="s0">def </span><span class="s1">_data_gen(endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">partitions):</span>
    <span class="s2">&quot;&quot;&quot;partitions data&quot;&quot;&quot;</span>

    <span class="s1">n_exog = exog.shape[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">n_part = np.ceil(n_exog / partitions)</span>

    <span class="s1">n_part = np.floor(n_exog / partitions)</span>
    <span class="s1">rem = n_exog - n_part * partitions</span>

    <span class="s1">stp = </span><span class="s3">0</span>

    <span class="s0">while </span><span class="s1">stp &lt; (partitions - </span><span class="s3">1</span><span class="s1">):</span>
        <span class="s1">ii = int(n_part * stp)</span>
        <span class="s1">jj = int(n_part * (stp + </span><span class="s3">1</span><span class="s1">))</span>
        <span class="s0">yield </span><span class="s1">endog[ii:jj]</span><span class="s0">, </span><span class="s1">exog[ii:jj</span><span class="s0">, </span><span class="s1">:]</span>
        <span class="s1">stp += </span><span class="s3">1</span>

    <span class="s1">ii = int(n_part * stp)</span>
    <span class="s1">jj = int(n_part * (stp + </span><span class="s3">1</span><span class="s1">) + rem)</span>
    <span class="s0">yield </span><span class="s1">endog[ii:jj]</span><span class="s0">, </span><span class="s1">exog[ii:jj</span><span class="s0">, </span><span class="s1">:]</span>


<span class="s0">def </span><span class="s1">test_calc_grad():</span>

    <span class="s4"># separately tests that _calc_grad returns</span>
    <span class="s4"># sensible results</span>
    <span class="s4">#</span>
    <span class="s4"># regression test</span>

    <span class="s1">np.random.seed(</span><span class="s3">435265</span><span class="s1">)</span>
    <span class="s1">X = np.random.normal(size=(</span><span class="s3">50</span><span class="s0">, </span><span class="s3">3</span><span class="s1">))</span>
    <span class="s1">y = np.random.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s3">50</span><span class="s1">)</span>
    <span class="s1">beta = np.random.normal(size=</span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">mod = OLS(y</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">grad = _calc_grad(mod</span><span class="s0">, </span><span class="s1">beta</span><span class="s0">, </span><span class="s3">0.01</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">{})</span>
    <span class="s1">assert_allclose(grad</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s3">19.75816</span><span class="s0">, </span><span class="s1">-</span><span class="s3">6.62307</span><span class="s0">, </span><span class="s3">7.324644</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_calc_wdesign_mat():</span>

    <span class="s4"># separately tests that _calc_wdesign_mat</span>
    <span class="s4"># returns sensible results</span>
    <span class="s4">#</span>
    <span class="s4"># regression test</span>

    <span class="s1">np.random.seed(</span><span class="s3">435265</span><span class="s1">)</span>
    <span class="s1">X = np.random.normal(size=(</span><span class="s3">3</span><span class="s0">, </span><span class="s3">3</span><span class="s1">))</span>
    <span class="s1">y = np.random.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">beta = np.random.normal(size=</span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">mod = OLS(y</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">dmat = _calc_wdesign_mat(mod</span><span class="s0">, </span><span class="s1">beta</span><span class="s0">, </span><span class="s1">{})</span>
    <span class="s1">assert_allclose(dmat</span><span class="s0">, </span><span class="s1">np.array([[</span><span class="s3">1.306314</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.024897</span><span class="s0">, </span><span class="s3">1.326498</span><span class="s1">]</span><span class="s0">,</span>
                                    <span class="s1">[-</span><span class="s3">0.539219</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.483028</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.703503</span><span class="s1">]</span><span class="s0">,</span>
                                    <span class="s1">[-</span><span class="s3">3.327987</span><span class="s0">, </span><span class="s3">0.524541</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.139761</span><span class="s1">]])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">mod = GLM(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">family=Binomial())</span>
    <span class="s1">dmat = _calc_wdesign_mat(mod</span><span class="s0">, </span><span class="s1">beta</span><span class="s0">, </span><span class="s1">{})</span>
    <span class="s1">assert_allclose(dmat</span><span class="s0">, </span><span class="s1">np.array([[</span><span class="s3">0.408616</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.007788</span><span class="s0">, </span><span class="s3">0.41493</span><span class="s1">]</span><span class="s0">,</span>
                                    <span class="s1">[-</span><span class="s3">0.263292</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.235854</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.343509</span><span class="s1">]</span><span class="s0">,</span>
                                    <span class="s1">[-</span><span class="s3">0.11241</span><span class="s0">, </span><span class="s3">0.017718</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.004721</span><span class="s1">]])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_est_regularized_debiased():</span>

    <span class="s4"># tests that the shape of all the intermediate steps</span>
    <span class="s4"># remains correct for regularized debiased estimation,</span>
    <span class="s4"># does this for OLS and GLM</span>

    <span class="s1">np.random.seed(</span><span class="s3">435265</span><span class="s1">)</span>
    <span class="s1">X = np.random.normal(size=(</span><span class="s3">50</span><span class="s0">, </span><span class="s3">3</span><span class="s1">))</span>
    <span class="s1">y = np.random.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s3">50</span><span class="s1">)</span>
    <span class="s1">beta = np.random.normal(size=</span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">mod = OLS(y</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">res = _est_regularized_debiased(mod</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">bhat = res[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">grad = res[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">ghat_l = res[</span><span class="s3">2</span><span class="s1">]</span>
    <span class="s1">that_l = res[</span><span class="s3">3</span><span class="s1">]</span>

    <span class="s1">assert_(isinstance(res</span><span class="s0">, </span><span class="s1">tuple))</span>
    <span class="s1">assert_equal(bhat.shape</span><span class="s0">, </span><span class="s1">beta.shape)</span>
    <span class="s1">assert_equal(grad.shape</span><span class="s0">, </span><span class="s1">beta.shape)</span>
    <span class="s1">assert_(isinstance(ghat_l</span><span class="s0">, </span><span class="s1">list))</span>
    <span class="s1">assert_(isinstance(that_l</span><span class="s0">, </span><span class="s1">list))</span>
    <span class="s1">assert_equal(len(ghat_l)</span><span class="s0">, </span><span class="s1">len(that_l))</span>
    <span class="s1">assert_equal(ghat_l[</span><span class="s3">0</span><span class="s1">].shape</span><span class="s0">, </span><span class="s1">(</span><span class="s3">2</span><span class="s0">,</span><span class="s1">))</span>
    <span class="s1">assert_(isinstance(that_l[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">float))</span>

    <span class="s1">mod = GLM(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">family=Binomial())</span>
    <span class="s1">res = _est_regularized_debiased(mod</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">bhat = res[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">grad = res[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">ghat_l = res[</span><span class="s3">2</span><span class="s1">]</span>
    <span class="s1">that_l = res[</span><span class="s3">3</span><span class="s1">]</span>

    <span class="s1">assert_(isinstance(res</span><span class="s0">, </span><span class="s1">tuple))</span>
    <span class="s1">assert_equal(bhat.shape</span><span class="s0">, </span><span class="s1">beta.shape)</span>
    <span class="s1">assert_equal(grad.shape</span><span class="s0">, </span><span class="s1">beta.shape)</span>
    <span class="s1">assert_(isinstance(ghat_l</span><span class="s0">, </span><span class="s1">list))</span>
    <span class="s1">assert_(isinstance(that_l</span><span class="s0">, </span><span class="s1">list))</span>
    <span class="s1">assert_equal(len(ghat_l)</span><span class="s0">, </span><span class="s1">len(that_l))</span>
    <span class="s1">assert_equal(ghat_l[</span><span class="s3">0</span><span class="s1">].shape</span><span class="s0">, </span><span class="s1">(</span><span class="s3">2</span><span class="s0">,</span><span class="s1">))</span>
    <span class="s1">assert_(isinstance(that_l[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">float))</span>


<span class="s0">def </span><span class="s1">test_est_regularized_naive():</span>

    <span class="s4"># tests that the shape of all the intermediate steps</span>
    <span class="s4"># remains correct for regularized naive estimation,</span>
    <span class="s4"># does this for OLS and GLM</span>

    <span class="s1">np.random.seed(</span><span class="s3">435265</span><span class="s1">)</span>
    <span class="s1">X = np.random.normal(size=(</span><span class="s3">50</span><span class="s0">, </span><span class="s3">3</span><span class="s1">))</span>
    <span class="s1">y = np.random.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s3">50</span><span class="s1">)</span>
    <span class="s1">beta = np.random.normal(size=</span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">mod = OLS(y</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">res = _est_regularized_naive(mod</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>

    <span class="s1">assert_equal(res.shape</span><span class="s0">, </span><span class="s1">beta.shape)</span>

    <span class="s1">mod = GLM(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">family=Binomial())</span>
    <span class="s1">res = _est_regularized_naive(mod</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>

    <span class="s1">assert_equal(res.shape</span><span class="s0">, </span><span class="s1">beta.shape)</span>


<span class="s0">def </span><span class="s1">test_est_unregularized_naive():</span>

    <span class="s4"># tests that the shape of all the intermediate steps</span>
    <span class="s4"># remains correct for unregularized naive estimation,</span>
    <span class="s4"># does this for OLS and GLM</span>

    <span class="s1">np.random.seed(</span><span class="s3">435265</span><span class="s1">)</span>
    <span class="s1">X = np.random.normal(size=(</span><span class="s3">50</span><span class="s0">, </span><span class="s3">3</span><span class="s1">))</span>
    <span class="s1">y = np.random.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s3">50</span><span class="s1">)</span>
    <span class="s1">beta = np.random.normal(size=</span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">mod = OLS(y</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">res = _est_unregularized_naive(mod</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>

    <span class="s1">assert_equal(res.shape</span><span class="s0">, </span><span class="s1">beta.shape)</span>

    <span class="s1">mod = GLM(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">family=Binomial())</span>
    <span class="s1">res = _est_unregularized_naive(mod</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>

    <span class="s1">assert_equal(res.shape</span><span class="s0">, </span><span class="s1">beta.shape)</span>


<span class="s0">def </span><span class="s1">test_join_debiased():</span>

    <span class="s4"># tests that the results of all the intermediate steps</span>
    <span class="s4"># remains correct for debiased join, does this for OLS and GLM</span>
    <span class="s4">#</span>
    <span class="s4"># regression test</span>

    <span class="s1">np.random.seed(</span><span class="s3">435265</span><span class="s1">)</span>
    <span class="s1">X = np.random.normal(size=(</span><span class="s3">50</span><span class="s0">, </span><span class="s3">3</span><span class="s1">))</span>
    <span class="s1">y = np.random.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s3">50</span><span class="s1">)</span>
    <span class="s1">mod = OLS(y</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">res_l = []</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">2</span><span class="s1">):</span>
        <span class="s1">res = _est_regularized_debiased(mod</span><span class="s0">, </span><span class="s1">i</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.1</span><span class="s1">})</span>
        <span class="s1">res_l.append(res)</span>
    <span class="s1">joined = _join_debiased(res_l)</span>
    <span class="s1">assert_allclose(joined</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.167548</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.016567</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.34414</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">mod = GLM(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">family=Binomial())</span>
    <span class="s1">res_l = []</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">2</span><span class="s1">):</span>
        <span class="s1">res = _est_regularized_debiased(mod</span><span class="s0">, </span><span class="s1">i</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.1</span><span class="s1">})</span>
        <span class="s1">res_l.append(res)</span>
    <span class="s1">joined = _join_debiased(res_l)</span>
    <span class="s1">assert_allclose(joined</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.164515</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.412854</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.223955</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_join_naive():</span>

    <span class="s4"># tests that the results of all the intermediate steps</span>
    <span class="s4"># remains correct for naive join, does this for OLS and GLM</span>
    <span class="s4">#</span>
    <span class="s4"># regression test</span>

    <span class="s1">np.random.seed(</span><span class="s3">435265</span><span class="s1">)</span>
    <span class="s1">X = np.random.normal(size=(</span><span class="s3">50</span><span class="s0">, </span><span class="s3">3</span><span class="s1">))</span>
    <span class="s1">y = np.random.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s3">50</span><span class="s1">)</span>
    <span class="s1">mod = OLS(y</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">res_l = []</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">2</span><span class="s1">):</span>
        <span class="s1">res = _est_regularized_naive(mod</span><span class="s0">, </span><span class="s1">i</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.1</span><span class="s1">})</span>
        <span class="s1">res_l.append(res)</span>
    <span class="s1">joined = _join_naive(res_l)</span>
    <span class="s1">assert_allclose(joined</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.020757</span><span class="s0">, </span><span class="s3">0.</span><span class="s0">, </span><span class="s3">0.</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">mod = GLM(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">family=Binomial())</span>
    <span class="s1">res_l = []</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">2</span><span class="s1">):</span>
        <span class="s1">res = _est_regularized_naive(mod</span><span class="s0">, </span><span class="s1">i</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.1</span><span class="s1">})</span>
        <span class="s1">res_l.append(res)</span>
    <span class="s1">joined = _join_naive(res_l)</span>
    <span class="s1">assert_allclose(joined</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s3">0.</span><span class="s0">, </span><span class="s3">0.</span><span class="s0">, </span><span class="s3">0.</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_fit_sequential():</span>

    <span class="s4"># tests that the results of all the intermediate steps</span>
    <span class="s4"># remains correct for sequential fit, does this for OLS and GLM</span>
    <span class="s4"># and a variety of model sizes</span>
    <span class="s4">#</span>
    <span class="s4"># regression test</span>

    <span class="s1">np.random.seed(</span><span class="s3">435265</span><span class="s1">)</span>
    <span class="s1">X = np.random.normal(size=(</span><span class="s3">50</span><span class="s0">, </span><span class="s3">3</span><span class="s1">))</span>
    <span class="s1">y = np.random.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s3">50</span><span class="s1">)</span>

    <span class="s1">mod = DistributedModel(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">model_class=OLS)</span>
    <span class="s1">fit = mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">parallel_method=</span><span class="s5">&quot;sequential&quot;</span><span class="s0">,</span>
                  <span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">assert_allclose(fit.params</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.191606</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.012565</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.351398</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">mod = DistributedModel(</span><span class="s3">2</span><span class="s0">, </span><span class="s1">model_class=OLS)</span>
    <span class="s1">fit = mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">parallel_method=</span><span class="s5">&quot;sequential&quot;</span><span class="s0">,</span>
                  <span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">assert_allclose(fit.params</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.157416</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.029643</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.471653</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">mod = DistributedModel(</span><span class="s3">3</span><span class="s0">, </span><span class="s1">model_class=OLS)</span>
    <span class="s1">fit = mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">parallel_method=</span><span class="s5">&quot;sequential&quot;</span><span class="s0">,</span>
                  <span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">assert_allclose(fit.params</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.124891</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.050934</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.403354</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">mod = DistributedModel(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">model_class=GLM</span><span class="s0">,</span>
                           <span class="s1">init_kwds={</span><span class="s5">&quot;family&quot;</span><span class="s1">: Binomial()})</span>
    <span class="s1">fit = mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">parallel_method=</span><span class="s5">&quot;sequential&quot;</span><span class="s0">,</span>
                  <span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">assert_allclose(fit.params</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.164515</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.412854</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.223955</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">mod = DistributedModel(</span><span class="s3">2</span><span class="s0">, </span><span class="s1">model_class=GLM</span><span class="s0">,</span>
                           <span class="s1">init_kwds={</span><span class="s5">&quot;family&quot;</span><span class="s1">: Binomial()})</span>
    <span class="s1">fit = mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">parallel_method=</span><span class="s5">&quot;sequential&quot;</span><span class="s0">,</span>
                  <span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">assert_allclose(fit.params</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.142513</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.360324</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.295485</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">mod = DistributedModel(</span><span class="s3">3</span><span class="s0">, </span><span class="s1">model_class=GLM</span><span class="s0">,</span>
                           <span class="s1">init_kwds={</span><span class="s5">&quot;family&quot;</span><span class="s1">: Binomial()})</span>
    <span class="s1">fit = mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">parallel_method=</span><span class="s5">&quot;sequential&quot;</span><span class="s0">,</span>
                  <span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">assert_allclose(fit.params</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.110487</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.306431</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.243921</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_fit_joblib():</span>

    <span class="s4"># tests that the results of all the intermediate steps</span>
    <span class="s4"># remains correct for joblib fit, does this for OLS and GLM</span>
    <span class="s4"># and a variety of model sizes</span>
    <span class="s4">#</span>
    <span class="s4"># regression test</span>

    <span class="s1">np.random.seed(</span><span class="s3">435265</span><span class="s1">)</span>
    <span class="s1">X = np.random.normal(size=(</span><span class="s3">50</span><span class="s0">, </span><span class="s3">3</span><span class="s1">))</span>
    <span class="s1">y = np.random.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s3">50</span><span class="s1">)</span>

    <span class="s1">mod = DistributedModel(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">model_class=OLS)</span>
    <span class="s1">fit = mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">parallel_method=</span><span class="s5">&quot;joblib&quot;</span><span class="s0">,</span>
                  <span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">assert_allclose(fit.params</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.191606</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.012565</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.351398</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">mod = DistributedModel(</span><span class="s3">2</span><span class="s0">, </span><span class="s1">model_class=OLS)</span>
    <span class="s1">fit = mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">parallel_method=</span><span class="s5">&quot;joblib&quot;</span><span class="s0">,</span>
                  <span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">assert_allclose(fit.params</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.157416</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.029643</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.471653</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">mod = DistributedModel(</span><span class="s3">3</span><span class="s0">, </span><span class="s1">model_class=OLS)</span>
    <span class="s1">fit = mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">parallel_method=</span><span class="s5">&quot;joblib&quot;</span><span class="s0">,</span>
                  <span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">assert_allclose(fit.params</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.124891</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.050934</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.403354</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">mod = DistributedModel(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">model_class=GLM</span><span class="s0">,</span>
                           <span class="s1">init_kwds={</span><span class="s5">&quot;family&quot;</span><span class="s1">: Binomial()})</span>
    <span class="s1">fit = mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">parallel_method=</span><span class="s5">&quot;joblib&quot;</span><span class="s0">,</span>
                  <span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">assert_allclose(fit.params</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.164515</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.412854</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.223955</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">mod = DistributedModel(</span><span class="s3">2</span><span class="s0">, </span><span class="s1">model_class=GLM</span><span class="s0">,</span>
                           <span class="s1">init_kwds={</span><span class="s5">&quot;family&quot;</span><span class="s1">: Binomial()})</span>
    <span class="s1">fit = mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">parallel_method=</span><span class="s5">&quot;joblib&quot;</span><span class="s0">,</span>
                  <span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">assert_allclose(fit.params</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.142513</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.360324</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.295485</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">mod = DistributedModel(</span><span class="s3">3</span><span class="s0">, </span><span class="s1">model_class=GLM</span><span class="s0">,</span>
                           <span class="s1">init_kwds={</span><span class="s5">&quot;family&quot;</span><span class="s1">: Binomial()})</span>
    <span class="s1">fit = mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">parallel_method=</span><span class="s5">&quot;joblib&quot;</span><span class="s0">,</span>
                  <span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">assert_allclose(fit.params</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.110487</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.306431</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.243921</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_single_partition():</span>

    <span class="s4"># tests that the results make sense if we have a single partition</span>

    <span class="s1">np.random.seed(</span><span class="s3">435265</span><span class="s1">)</span>
    <span class="s1">N = </span><span class="s3">200</span>
    <span class="s1">p = </span><span class="s3">10</span>
    <span class="s1">m = </span><span class="s3">1</span>

    <span class="s1">beta = np.random.normal(size=p)</span>
    <span class="s1">beta = beta * np.random.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">p)</span>
    <span class="s1">X = np.random.normal(size=(N</span><span class="s0">, </span><span class="s1">p))</span>
    <span class="s1">y = X.dot(beta) + np.random.normal(size=N)</span>

    <span class="s4"># test regularized OLS v. naive</span>
    <span class="s1">db_mod = DistributedModel(m)</span>
    <span class="s1">fitOLSdb = db_mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">m)</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0</span><span class="s1">})</span>

    <span class="s1">nv_mod = DistributedModel(m</span><span class="s0">, </span><span class="s1">estimation_method=_est_regularized_naive</span><span class="s0">,</span>
                              <span class="s1">join_method=_join_naive)</span>
    <span class="s1">fitOLSnv = nv_mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">m)</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0</span><span class="s1">})</span>

    <span class="s1">ols_mod = OLS(y</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">fitOLS = ols_mod.fit(alpha=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">assert_allclose(fitOLSdb.params</span><span class="s0">, </span><span class="s1">fitOLS.params)</span>
    <span class="s1">assert_allclose(fitOLSnv.params</span><span class="s0">, </span><span class="s1">fitOLS.params)</span>

    <span class="s4"># test regularized</span>
    <span class="s1">nv_mod = DistributedModel(m</span><span class="s0">, </span><span class="s1">estimation_method=_est_regularized_naive</span><span class="s0">,</span>
                              <span class="s1">join_method=_join_naive)</span>
    <span class="s1">fitOLSnv = nv_mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">m)</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.1</span><span class="s1">})</span>

    <span class="s1">ols_mod = OLS(y</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">fitOLS = ols_mod.fit_regularized(alpha=</span><span class="s3">0.1</span><span class="s1">)</span>

    <span class="s1">assert_allclose(fitOLSnv.params</span><span class="s0">, </span><span class="s1">fitOLS.params)</span>


<span class="s0">def </span><span class="s1">test_larger_p():</span>

    <span class="s4"># tests when p &gt; N / m for the debiased and naive case</span>

    <span class="s1">np.random.seed(</span><span class="s3">435265</span><span class="s1">)</span>
    <span class="s1">N = </span><span class="s3">40</span>
    <span class="s1">p = </span><span class="s3">40</span>
    <span class="s1">m = </span><span class="s3">5</span>

    <span class="s1">beta = np.random.normal(size=p)</span>
    <span class="s1">beta = beta * np.random.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">p)</span>
    <span class="s1">X = np.random.normal(size=(N</span><span class="s0">, </span><span class="s1">p))</span>
    <span class="s1">y = X.dot(beta) + np.random.normal(size=N)</span>

    <span class="s1">db_mod = DistributedModel(m)</span>
    <span class="s1">fitOLSdb = db_mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">m)</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.1</span><span class="s1">})</span>
    <span class="s1">assert_equal(np.sum(np.isnan(fitOLSdb.params))</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">nv_mod = DistributedModel(m</span><span class="s0">, </span><span class="s1">estimation_method=_est_regularized_naive</span><span class="s0">,</span>
                              <span class="s1">join_method=_join_naive)</span>
    <span class="s1">fitOLSnv = nv_mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">m)</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.1</span><span class="s1">})</span>
    <span class="s1">assert_equal(np.sum(np.isnan(fitOLSnv.params))</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_non_zero_params():</span>

    <span class="s4"># tests that the thresholding does not cause any issues</span>

    <span class="s1">np.random.seed(</span><span class="s3">435265</span><span class="s1">)</span>
    <span class="s1">N = </span><span class="s3">200</span>
    <span class="s1">p = </span><span class="s3">10</span>
    <span class="s1">m = </span><span class="s3">5</span>

    <span class="s1">beta = np.random.normal(size=p)</span>
    <span class="s1">beta = beta * np.random.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">p)</span>
    <span class="s1">X = np.random.normal(size=(N</span><span class="s0">, </span><span class="s1">p))</span>
    <span class="s1">y = X.dot(beta) + np.random.normal(size=N)</span>

    <span class="s1">db_mod = DistributedModel(m</span><span class="s0">, </span><span class="s1">join_kwds={</span><span class="s5">&quot;threshold&quot;</span><span class="s1">: </span><span class="s3">0.13</span><span class="s1">})</span>
    <span class="s1">fitOLSdb = db_mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">m)</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.1</span><span class="s1">})</span>
    <span class="s1">ols_mod = OLS(y</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">fitOLS = ols_mod.fit_regularized(alpha=</span><span class="s3">0.1</span><span class="s1">)</span>

    <span class="s1">nz_params_db = </span><span class="s3">1 </span><span class="s1">* (fitOLSdb.params != </span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">nz_params_ols = </span><span class="s3">1 </span><span class="s1">* (fitOLS.params != </span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">assert_allclose(nz_params_db</span><span class="s0">, </span><span class="s1">nz_params_ols)</span>


<span class="s0">def </span><span class="s1">test_repeat_partition():</span>

    <span class="s4"># tests that if we use identical partitions the average is the same</span>
    <span class="s4"># as the estimate for the full data</span>

    <span class="s1">np.random.seed(</span><span class="s3">435265</span><span class="s1">)</span>
    <span class="s1">N = </span><span class="s3">200</span>
    <span class="s1">p = </span><span class="s3">10</span>
    <span class="s1">m = </span><span class="s3">1</span>

    <span class="s1">beta = np.random.normal(size=p)</span>
    <span class="s1">beta = beta * np.random.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">p)</span>
    <span class="s1">X = np.random.normal(size=(N</span><span class="s0">, </span><span class="s1">p))</span>
    <span class="s1">y = X.dot(beta) + np.random.normal(size=N)</span>

    <span class="s0">def </span><span class="s1">_rep_data_gen(endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">partitions):</span>
        <span class="s2">&quot;&quot;&quot;partitions data&quot;&quot;&quot;</span>

        <span class="s1">n_exog = exog.shape[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">n_part = np.ceil(n_exog / partitions)</span>

        <span class="s1">ii = </span><span class="s3">0</span>
        <span class="s0">while </span><span class="s1">ii &lt; n_exog:</span>
            <span class="s0">yield </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span>
            <span class="s1">ii += int(n_part)</span>

    <span class="s1">nv_mod = DistributedModel(m</span><span class="s0">, </span><span class="s1">estimation_method=_est_regularized_naive</span><span class="s0">,</span>
                              <span class="s1">join_method=_join_naive)</span>
    <span class="s1">fitOLSnv = nv_mod.fit(_rep_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">m)</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.1</span><span class="s1">})</span>

    <span class="s1">ols_mod = OLS(y</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">fitOLS = ols_mod.fit_regularized(alpha=</span><span class="s3">0.1</span><span class="s1">)</span>

    <span class="s1">assert_allclose(fitOLSnv.params</span><span class="s0">, </span><span class="s1">fitOLS.params)</span>


<span class="s0">def </span><span class="s1">test_debiased_v_average():</span>

    <span class="s4"># tests that the debiased method performs better than the standard</span>
    <span class="s4"># average.  Does this for both OLS and GLM.</span>

    <span class="s1">np.random.seed(</span><span class="s3">435265</span><span class="s1">)</span>
    <span class="s1">N = </span><span class="s3">200</span>
    <span class="s1">p = </span><span class="s3">10</span>
    <span class="s1">m = </span><span class="s3">4</span>

    <span class="s1">beta = np.random.normal(size=p)</span>
    <span class="s1">beta = beta * np.random.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">p)</span>
    <span class="s1">X = np.random.normal(size=(N</span><span class="s0">, </span><span class="s1">p))</span>
    <span class="s1">y = X.dot(beta) + np.random.normal(size=N)</span>

    <span class="s1">db_mod = DistributedModel(m)</span>
    <span class="s1">fitOLSdb = db_mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">m)</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.2</span><span class="s1">})</span>
    <span class="s1">olsdb = np.linalg.norm(fitOLSdb.params - beta)</span>
    <span class="s1">n_mod = DistributedModel(m</span><span class="s0">, </span><span class="s1">estimation_method=_est_regularized_naive</span><span class="s0">,</span>
                             <span class="s1">join_method=_join_naive)</span>
    <span class="s1">fitOLSn = n_mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">m)</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.2</span><span class="s1">})</span>
    <span class="s1">olsn = np.linalg.norm(fitOLSn.params - beta)</span>

    <span class="s1">assert_(olsdb &lt; olsn)</span>

    <span class="s1">prob = </span><span class="s3">1 </span><span class="s1">/ (</span><span class="s3">1 </span><span class="s1">+ np.exp(-X.dot(beta) + np.random.normal(size=N)))</span>
    <span class="s1">y = </span><span class="s3">1. </span><span class="s1">* (prob &gt; </span><span class="s3">0.5</span><span class="s1">)</span>

    <span class="s1">db_mod = DistributedModel(m</span><span class="s0">, </span><span class="s1">model_class=GLM</span><span class="s0">,</span>
                              <span class="s1">init_kwds={</span><span class="s5">&quot;family&quot;</span><span class="s1">: Binomial()})</span>
    <span class="s1">fitGLMdb = db_mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">m)</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.2</span><span class="s1">})</span>
    <span class="s1">glmdb = np.linalg.norm(fitGLMdb.params - beta)</span>
    <span class="s1">n_mod = DistributedModel(m</span><span class="s0">, </span><span class="s1">model_class=GLM</span><span class="s0">,</span>
                             <span class="s1">init_kwds={</span><span class="s5">&quot;family&quot;</span><span class="s1">: Binomial()}</span><span class="s0">,</span>
                             <span class="s1">estimation_method=_est_regularized_naive</span><span class="s0">,</span>
                             <span class="s1">join_method=_join_naive)</span>
    <span class="s1">fitGLMn = n_mod.fit(_data_gen(y</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">m)</span><span class="s0">, </span><span class="s1">fit_kwds={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0.2</span><span class="s1">})</span>
    <span class="s1">glmn = np.linalg.norm(fitGLMn.params - beta)</span>

    <span class="s1">assert_(glmdb &lt; glmn)</span>
</pre>
</body>
</html>