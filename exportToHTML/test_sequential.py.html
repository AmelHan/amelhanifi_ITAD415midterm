<html>
<head>
<title>test_sequential.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6897bb;}
.s3 { color: #6a8759;}
.s4 { color: #808080;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_sequential.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">import </span><span class="s1">scipy</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">assert_array_equal</span>

<span class="s0">from </span><span class="s1">sklearn.cluster </span><span class="s0">import </span><span class="s1">KMeans</span>
<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">make_blobs</span><span class="s0">, </span><span class="s1">make_classification</span><span class="s0">, </span><span class="s1">make_regression</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble </span><span class="s0">import </span><span class="s1">HistGradientBoostingRegressor</span>
<span class="s0">from </span><span class="s1">sklearn.feature_selection </span><span class="s0">import </span><span class="s1">SequentialFeatureSelector</span>
<span class="s0">from </span><span class="s1">sklearn.linear_model </span><span class="s0">import </span><span class="s1">LinearRegression</span>
<span class="s0">from </span><span class="s1">sklearn.model_selection </span><span class="s0">import </span><span class="s1">LeaveOneGroupOut</span><span class="s0">, </span><span class="s1">cross_val_score</span>
<span class="s0">from </span><span class="s1">sklearn.neighbors </span><span class="s0">import </span><span class="s1">KNeighborsClassifier</span>
<span class="s0">from </span><span class="s1">sklearn.pipeline </span><span class="s0">import </span><span class="s1">make_pipeline</span>
<span class="s0">from </span><span class="s1">sklearn.preprocessing </span><span class="s0">import </span><span class="s1">StandardScaler</span>


<span class="s0">def </span><span class="s1">test_bad_n_features_to_select():</span>
    <span class="s1">n_features = </span><span class="s2">5</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_features=n_features)</span>
    <span class="s1">sfs = SequentialFeatureSelector(LinearRegression()</span><span class="s0">, </span><span class="s1">n_features_to_select=n_features)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s3">&quot;n_features_to_select must be &lt; n_features&quot;</span><span class="s1">):</span>
        <span class="s1">sfs.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;direction&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s3">&quot;forward&quot;</span><span class="s0">, </span><span class="s3">&quot;backward&quot;</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;n_features_to_select&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s2">1</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">9</span><span class="s0">, </span><span class="s3">&quot;auto&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_n_features_to_select(direction</span><span class="s0">, </span><span class="s1">n_features_to_select):</span>
    <span class="s4"># Make sure n_features_to_select is respected</span>

    <span class="s1">n_features = </span><span class="s2">10</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_features=n_features</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">sfs = SequentialFeatureSelector(</span>
        <span class="s1">LinearRegression()</span><span class="s0">,</span>
        <span class="s1">n_features_to_select=n_features_to_select</span><span class="s0">,</span>
        <span class="s1">direction=direction</span><span class="s0">,</span>
        <span class="s1">cv=</span><span class="s2">2</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">sfs.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">n_features_to_select == </span><span class="s3">&quot;auto&quot;</span><span class="s1">:</span>
        <span class="s1">n_features_to_select = n_features // </span><span class="s2">2</span>

    <span class="s0">assert </span><span class="s1">sfs.get_support(indices=</span><span class="s0">True</span><span class="s1">).shape[</span><span class="s2">0</span><span class="s1">] == n_features_to_select</span>
    <span class="s0">assert </span><span class="s1">sfs.n_features_to_select_ == n_features_to_select</span>
    <span class="s0">assert </span><span class="s1">sfs.transform(X).shape[</span><span class="s2">1</span><span class="s1">] == n_features_to_select</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;direction&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s3">&quot;forward&quot;</span><span class="s0">, </span><span class="s3">&quot;backward&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_n_features_to_select_auto(direction):</span>
    <span class="s5">&quot;&quot;&quot;Check the behaviour of `n_features_to_select=&quot;auto&quot;` with different 
    values for the parameter `tol`. 
    &quot;&quot;&quot;</span>

    <span class="s1">n_features = </span><span class="s2">10</span>
    <span class="s1">tol = </span><span class="s2">1e-3</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_features=n_features</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">sfs = SequentialFeatureSelector(</span>
        <span class="s1">LinearRegression()</span><span class="s0">,</span>
        <span class="s1">n_features_to_select=</span><span class="s3">&quot;auto&quot;</span><span class="s0">,</span>
        <span class="s1">tol=tol</span><span class="s0">,</span>
        <span class="s1">direction=direction</span><span class="s0">,</span>
        <span class="s1">cv=</span><span class="s2">2</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">sfs.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">max_features_to_select = n_features - </span><span class="s2">1</span>

    <span class="s0">assert </span><span class="s1">sfs.get_support(indices=</span><span class="s0">True</span><span class="s1">).shape[</span><span class="s2">0</span><span class="s1">] &lt;= max_features_to_select</span>
    <span class="s0">assert </span><span class="s1">sfs.n_features_to_select_ &lt;= max_features_to_select</span>
    <span class="s0">assert </span><span class="s1">sfs.transform(X).shape[</span><span class="s2">1</span><span class="s1">] &lt;= max_features_to_select</span>
    <span class="s0">assert </span><span class="s1">sfs.get_support(indices=</span><span class="s0">True</span><span class="s1">).shape[</span><span class="s2">0</span><span class="s1">] == sfs.n_features_to_select_</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;direction&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s3">&quot;forward&quot;</span><span class="s0">, </span><span class="s3">&quot;backward&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_n_features_to_select_stopping_criterion(direction):</span>
    <span class="s5">&quot;&quot;&quot;Check the behaviour stopping criterion for feature selection 
    depending on the values of `n_features_to_select` and `tol`. 
 
    When `direction` is `'forward'`, select a new features at random 
    among those not currently selected in selector.support_, 
    build a new version of the data that includes all the features 
    in selector.support_ + this newly selected feature. 
    And check that the cross-validation score of the model trained on 
    this new dataset variant is lower than the model with 
    the selected forward selected features or at least does not improve 
    by more than the tol margin. 
 
    When `direction` is `'backward'`, instead of adding a new feature 
    to selector.support_, try to remove one of those selected features at random 
    And check that the cross-validation score is either decreasing or 
    not improving by more than the tol margin. 
    &quot;&quot;&quot;</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_features=</span><span class="s2">50</span><span class="s0">, </span><span class="s1">n_informative=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s1">tol = </span><span class="s2">1e-3</span>

    <span class="s1">sfs = SequentialFeatureSelector(</span>
        <span class="s1">LinearRegression()</span><span class="s0">,</span>
        <span class="s1">n_features_to_select=</span><span class="s3">&quot;auto&quot;</span><span class="s0">,</span>
        <span class="s1">tol=tol</span><span class="s0">,</span>
        <span class="s1">direction=direction</span><span class="s0">,</span>
        <span class="s1">cv=</span><span class="s2">2</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">sfs.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">selected_X = sfs.transform(X)</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s1">added_candidates = list(set(range(X.shape[</span><span class="s2">1</span><span class="s1">])) - set(sfs.get_support(indices=</span><span class="s0">True</span><span class="s1">)))</span>
    <span class="s1">added_X = np.hstack(</span>
        <span class="s1">[</span>
            <span class="s1">selected_X</span><span class="s0">,</span>
            <span class="s1">(X[:</span><span class="s0">, </span><span class="s1">rng.choice(added_candidates)])[:</span><span class="s0">, </span><span class="s1">np.newaxis]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">removed_candidate = rng.choice(list(range(sfs.n_features_to_select_)))</span>
    <span class="s1">removed_X = np.delete(selected_X</span><span class="s0">, </span><span class="s1">removed_candidate</span><span class="s0">, </span><span class="s1">axis=</span><span class="s2">1</span><span class="s1">)</span>

    <span class="s1">plain_cv_score = cross_val_score(LinearRegression()</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">cv=</span><span class="s2">2</span><span class="s1">).mean()</span>
    <span class="s1">sfs_cv_score = cross_val_score(LinearRegression()</span><span class="s0">, </span><span class="s1">selected_X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">cv=</span><span class="s2">2</span><span class="s1">).mean()</span>
    <span class="s1">added_cv_score = cross_val_score(LinearRegression()</span><span class="s0">, </span><span class="s1">added_X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">cv=</span><span class="s2">2</span><span class="s1">).mean()</span>
    <span class="s1">removed_cv_score = cross_val_score(LinearRegression()</span><span class="s0">, </span><span class="s1">removed_X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">cv=</span><span class="s2">2</span><span class="s1">).mean()</span>

    <span class="s0">assert </span><span class="s1">sfs_cv_score &gt;= plain_cv_score</span>

    <span class="s0">if </span><span class="s1">direction == </span><span class="s3">&quot;forward&quot;</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">(sfs_cv_score - added_cv_score) &lt;= tol</span>
        <span class="s0">assert </span><span class="s1">(sfs_cv_score - removed_cv_score) &gt;= tol</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">(added_cv_score - sfs_cv_score) &lt;= tol</span>
        <span class="s0">assert </span><span class="s1">(removed_cv_score - sfs_cv_score) &lt;= tol</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;direction&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s3">&quot;forward&quot;</span><span class="s0">, </span><span class="s3">&quot;backward&quot;</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;n_features_to_select, expected&quot;</span><span class="s0">,</span>
    <span class="s1">(</span>
        <span class="s1">(</span><span class="s2">0.1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">1.0</span><span class="s0">, </span><span class="s2">10</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">0.5</span><span class="s0">, </span><span class="s2">5</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">)</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_n_features_to_select_float(direction</span><span class="s0">, </span><span class="s1">n_features_to_select</span><span class="s0">, </span><span class="s1">expected):</span>
    <span class="s4"># Test passing a float as n_features_to_select</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_features=</span><span class="s2">10</span><span class="s1">)</span>
    <span class="s1">sfs = SequentialFeatureSelector(</span>
        <span class="s1">LinearRegression()</span><span class="s0">,</span>
        <span class="s1">n_features_to_select=n_features_to_select</span><span class="s0">,</span>
        <span class="s1">direction=direction</span><span class="s0">,</span>
        <span class="s1">cv=</span><span class="s2">2</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">sfs.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">sfs.n_features_to_select_ == expected</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;seed&quot;</span><span class="s0">, </span><span class="s1">range(</span><span class="s2">10</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;direction&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s3">&quot;forward&quot;</span><span class="s0">, </span><span class="s3">&quot;backward&quot;</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;n_features_to_select, expected_selected_features&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s2">2</span><span class="s0">, </span><span class="s1">[</span><span class="s2">0</span><span class="s0">, </span><span class="s2">2</span><span class="s1">])</span><span class="s0">,  </span><span class="s4"># f1 is dropped since it has no predictive power</span>
        <span class="s1">(</span><span class="s2">1</span><span class="s0">, </span><span class="s1">[</span><span class="s2">2</span><span class="s1">])</span><span class="s0">,  </span><span class="s4"># f2 is more predictive than f0 so it's kept</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_sanity(seed</span><span class="s0">, </span><span class="s1">direction</span><span class="s0">, </span><span class="s1">n_features_to_select</span><span class="s0">, </span><span class="s1">expected_selected_features):</span>
    <span class="s4"># Basic sanity check: 3 features, only f0 and f2 are correlated with the</span>
    <span class="s4"># target, f2 having a stronger correlation than f0. We expect f1 to be</span>
    <span class="s4"># dropped, and f2 to always be selected.</span>

    <span class="s1">rng = np.random.RandomState(seed)</span>
    <span class="s1">n_samples = </span><span class="s2">100</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s2">3</span><span class="s1">)</span>
    <span class="s1">y = </span><span class="s2">3 </span><span class="s1">* X[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] - </span><span class="s2">10 </span><span class="s1">* X[:</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span>

    <span class="s1">sfs = SequentialFeatureSelector(</span>
        <span class="s1">LinearRegression()</span><span class="s0">,</span>
        <span class="s1">n_features_to_select=n_features_to_select</span><span class="s0">,</span>
        <span class="s1">direction=direction</span><span class="s0">,</span>
        <span class="s1">cv=</span><span class="s2">2</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">sfs.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(sfs.get_support(indices=</span><span class="s0">True</span><span class="s1">)</span><span class="s0">, </span><span class="s1">expected_selected_features)</span>


<span class="s0">def </span><span class="s1">test_sparse_support():</span>
    <span class="s4"># Make sure sparse data is supported</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_features=</span><span class="s2">10</span><span class="s1">)</span>
    <span class="s1">X = scipy.sparse.csr_matrix(X)</span>
    <span class="s1">sfs = SequentialFeatureSelector(</span>
        <span class="s1">LinearRegression()</span><span class="s0">, </span><span class="s1">n_features_to_select=</span><span class="s3">&quot;auto&quot;</span><span class="s0">, </span><span class="s1">cv=</span><span class="s2">2</span>
    <span class="s1">)</span>
    <span class="s1">sfs.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">sfs.transform(X)</span>


<span class="s0">def </span><span class="s1">test_nan_support():</span>
    <span class="s4"># Make sure nans are OK if the underlying estimator supports nans</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s2">40</span><span class="s0">, </span><span class="s2">4</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_samples</span><span class="s0">, </span><span class="s1">n_features</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">nan_mask = rng.randint(</span><span class="s2">0</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s1">size=(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span><span class="s0">, </span><span class="s1">dtype=bool)</span>
    <span class="s1">X[nan_mask] = np.nan</span>
    <span class="s1">sfs = SequentialFeatureSelector(</span>
        <span class="s1">HistGradientBoostingRegressor()</span><span class="s0">, </span><span class="s1">n_features_to_select=</span><span class="s3">&quot;auto&quot;</span><span class="s0">, </span><span class="s1">cv=</span><span class="s2">2</span>
    <span class="s1">)</span>
    <span class="s1">sfs.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">sfs.transform(X)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s3">&quot;Input X contains NaN&quot;</span><span class="s1">):</span>
        <span class="s4"># LinearRegression does not support nans</span>
        <span class="s1">SequentialFeatureSelector(</span>
            <span class="s1">LinearRegression()</span><span class="s0">, </span><span class="s1">n_features_to_select=</span><span class="s3">&quot;auto&quot;</span><span class="s0">, </span><span class="s1">cv=</span><span class="s2">2</span>
        <span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_pipeline_support():</span>
    <span class="s4"># Make sure that pipelines can be passed into SFS and that SFS can be</span>
    <span class="s4"># passed into a pipeline</span>

    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s2">50</span><span class="s0">, </span><span class="s2">3</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_samples</span><span class="s0">, </span><span class="s1">n_features</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s4"># pipeline in SFS</span>
    <span class="s1">pipe = make_pipeline(StandardScaler()</span><span class="s0">, </span><span class="s1">LinearRegression())</span>
    <span class="s1">sfs = SequentialFeatureSelector(pipe</span><span class="s0">, </span><span class="s1">n_features_to_select=</span><span class="s3">&quot;auto&quot;</span><span class="s0">, </span><span class="s1">cv=</span><span class="s2">2</span><span class="s1">)</span>
    <span class="s1">sfs.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">sfs.transform(X)</span>

    <span class="s4"># SFS in pipeline</span>
    <span class="s1">sfs = SequentialFeatureSelector(</span>
        <span class="s1">LinearRegression()</span><span class="s0">, </span><span class="s1">n_features_to_select=</span><span class="s3">&quot;auto&quot;</span><span class="s0">, </span><span class="s1">cv=</span><span class="s2">2</span>
    <span class="s1">)</span>
    <span class="s1">pipe = make_pipeline(StandardScaler()</span><span class="s0">, </span><span class="s1">sfs)</span>
    <span class="s1">pipe.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">pipe.transform(X)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;n_features_to_select&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_unsupervised_model_fit(n_features_to_select):</span>
    <span class="s4"># Make sure that models without classification labels are not being</span>
    <span class="s4"># validated</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(n_features=</span><span class="s2">4</span><span class="s1">)</span>
    <span class="s1">sfs = SequentialFeatureSelector(</span>
        <span class="s1">KMeans(n_init=</span><span class="s2">1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">n_features_to_select=n_features_to_select</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">sfs.fit(X)</span>
    <span class="s0">assert </span><span class="s1">sfs.transform(X).shape[</span><span class="s2">1</span><span class="s1">] == n_features_to_select</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;y&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s3">&quot;no_validation&quot;</span><span class="s0">, </span><span class="s2">1j</span><span class="s0">, </span><span class="s2">99.9</span><span class="s0">, </span><span class="s1">np.nan</span><span class="s0">, </span><span class="s2">3</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_no_y_validation_model_fit(y):</span>
    <span class="s4"># Make sure that other non-conventional y labels are not accepted</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">clusters = make_blobs(n_features=</span><span class="s2">6</span><span class="s1">)</span>
    <span class="s1">sfs = SequentialFeatureSelector(</span>
        <span class="s1">KMeans()</span><span class="s0">,</span>
        <span class="s1">n_features_to_select=</span><span class="s2">3</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">with </span><span class="s1">pytest.raises((TypeError</span><span class="s0">, </span><span class="s1">ValueError)):</span>
        <span class="s1">sfs.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_forward_neg_tol_error():</span>
    <span class="s5">&quot;&quot;&quot;Check that we raise an error when tol&lt;0 and direction='forward'&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_features=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">sfs = SequentialFeatureSelector(</span>
        <span class="s1">LinearRegression()</span><span class="s0">,</span>
        <span class="s1">n_features_to_select=</span><span class="s3">&quot;auto&quot;</span><span class="s0">,</span>
        <span class="s1">direction=</span><span class="s3">&quot;forward&quot;</span><span class="s0">,</span>
        <span class="s1">tol=-</span><span class="s2">1e-3</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s3">&quot;tol must be positive&quot;</span><span class="s1">):</span>
        <span class="s1">sfs.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_backward_neg_tol():</span>
    <span class="s5">&quot;&quot;&quot;Check that SequentialFeatureSelector works negative tol 
 
    non-regression test for #25525 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_features=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">lr = LinearRegression()</span>
    <span class="s1">initial_score = lr.fit(X</span><span class="s0">, </span><span class="s1">y).score(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">sfs = SequentialFeatureSelector(</span>
        <span class="s1">lr</span><span class="s0">,</span>
        <span class="s1">n_features_to_select=</span><span class="s3">&quot;auto&quot;</span><span class="s0">,</span>
        <span class="s1">direction=</span><span class="s3">&quot;backward&quot;</span><span class="s0">,</span>
        <span class="s1">tol=-</span><span class="s2">1e-3</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">Xr = sfs.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">new_score = lr.fit(Xr</span><span class="s0">, </span><span class="s1">y).score(Xr</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">assert </span><span class="s2">0 </span><span class="s1">&lt; sfs.get_support().sum() &lt; X.shape[</span><span class="s2">1</span><span class="s1">]</span>
    <span class="s0">assert </span><span class="s1">new_score &lt; initial_score</span>


<span class="s0">def </span><span class="s1">test_cv_generator_support():</span>
    <span class="s5">&quot;&quot;&quot;Check that no exception raised when cv is generator 
 
    non-regression test for #25957 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(random_state=</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s1">groups = np.zeros_like(y</span><span class="s0">, </span><span class="s1">dtype=int)</span>
    <span class="s1">groups[y.size // </span><span class="s2">2 </span><span class="s1">:] = </span><span class="s2">1</span>

    <span class="s1">cv = LeaveOneGroupOut()</span>
    <span class="s1">splits = cv.split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">groups=groups)</span>

    <span class="s1">knc = KNeighborsClassifier(n_neighbors=</span><span class="s2">5</span><span class="s1">)</span>

    <span class="s1">sfs = SequentialFeatureSelector(knc</span><span class="s0">, </span><span class="s1">n_features_to_select=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">cv=splits)</span>
    <span class="s1">sfs.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
</pre>
</body>
</html>