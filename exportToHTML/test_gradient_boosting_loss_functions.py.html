<html>
<head>
<title>test_gradient_boosting_loss_functions.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_gradient_boosting_loss_functions.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Testing for the gradient boosting loss functions and initial estimators. 
&quot;&quot;&quot;</span>
<span class="s2">from </span><span class="s1">itertools </span><span class="s2">import </span><span class="s1">product</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">numpy.testing </span><span class="s2">import </span><span class="s1">assert_allclose</span>
<span class="s2">from </span><span class="s1">pytest </span><span class="s2">import </span><span class="s1">approx</span>

<span class="s2">from </span><span class="s1">sklearn.ensemble._gb_losses </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">LOSS_FUNCTIONS</span><span class="s2">,</span>
    <span class="s1">BinomialDeviance</span><span class="s2">,</span>
    <span class="s1">ExponentialLoss</span><span class="s2">,</span>
    <span class="s1">HuberLossFunction</span><span class="s2">,</span>
    <span class="s1">LeastAbsoluteError</span><span class="s2">,</span>
    <span class="s1">LeastSquaresError</span><span class="s2">,</span>
    <span class="s1">MultinomialDeviance</span><span class="s2">,</span>
    <span class="s1">QuantileLossFunction</span><span class="s2">,</span>
    <span class="s1">RegressionLossFunction</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">mean_pinball_loss</span>
<span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">check_random_state</span>


<span class="s2">def </span><span class="s1">test_binomial_deviance():</span>
    <span class="s3"># Check binomial deviance loss.</span>
    <span class="s3"># Check against alternative definitions in ESLII.</span>
    <span class="s1">bd = BinomialDeviance(</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s3"># pred has the same BD for y in {0, 1}</span>
    <span class="s2">assert </span><span class="s1">bd(np.array([</span><span class="s4">0.0</span><span class="s1">])</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s4">0.0</span><span class="s1">])) == bd(np.array([</span><span class="s4">1.0</span><span class="s1">])</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s4">0.0</span><span class="s1">]))</span>

    <span class="s2">assert </span><span class="s1">bd(np.array([</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s4">100.0</span><span class="s2">, </span><span class="s4">100</span><span class="s2">, </span><span class="s4">100</span><span class="s1">])) == approx(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">bd(np.array([</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s4">100.0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">100</span><span class="s2">, </span><span class="s1">-</span><span class="s4">100</span><span class="s1">])) == approx(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s3"># check if same results as alternative definition of deviance, from ESLII</span>
    <span class="s3"># Eq. (10.18): -loglike = log(1 + exp(-2*z*f))</span>
    <span class="s3"># Note:</span>
    <span class="s3"># - We use y = {0, 1}, ESL (10.18) uses z in {-1, 1}, hence y=2*y-1</span>
    <span class="s3"># - ESL 2*f = pred_raw, hence the factor 2 of ESL disappears.</span>
    <span class="s3"># - Deviance = -2*loglike + .., hence a factor of 2 in front.</span>
    <span class="s2">def </span><span class="s1">alt_dev(y</span><span class="s2">, </span><span class="s1">raw_pred):</span>
        <span class="s1">z = </span><span class="s4">2 </span><span class="s1">* y - </span><span class="s4">1</span>
        <span class="s2">return </span><span class="s4">2 </span><span class="s1">* np.mean(np.log(</span><span class="s4">1 </span><span class="s1">+ np.exp(-z * raw_pred)))</span>

    <span class="s1">test_data = product(</span>
        <span class="s1">(np.array([</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]))</span><span class="s2">,</span>
        <span class="s1">(np.array([-</span><span class="s4">5.0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">5</span><span class="s2">, </span><span class="s1">-</span><span class="s4">5</span><span class="s1">])</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s4">3.0</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]))</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s2">for </span><span class="s1">datum </span><span class="s2">in </span><span class="s1">test_data:</span>
        <span class="s2">assert </span><span class="s1">bd(*datum) == approx(alt_dev(*datum))</span>

    <span class="s3"># check the negative gradient against alternative formula from ESLII</span>
    <span class="s3"># Note: negative_gradient is half the negative gradient.</span>
    <span class="s2">def </span><span class="s1">alt_ng(y</span><span class="s2">, </span><span class="s1">raw_pred):</span>
        <span class="s1">z = </span><span class="s4">2 </span><span class="s1">* y - </span><span class="s4">1</span>
        <span class="s2">return </span><span class="s1">z / (</span><span class="s4">1 </span><span class="s1">+ np.exp(z * raw_pred))</span>

    <span class="s2">for </span><span class="s1">datum </span><span class="s2">in </span><span class="s1">test_data:</span>
        <span class="s2">assert </span><span class="s1">bd.negative_gradient(*datum) == approx(alt_ng(*datum))</span>


<span class="s2">def </span><span class="s1">test_sample_weight_smoke():</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">13</span><span class="s1">)</span>
    <span class="s1">y = rng.rand(</span><span class="s4">100</span><span class="s1">)</span>
    <span class="s1">pred = rng.rand(</span><span class="s4">100</span><span class="s1">)</span>

    <span class="s3"># least squares</span>
    <span class="s1">loss = LeastSquaresError()</span>
    <span class="s1">loss_wo_sw = loss(y</span><span class="s2">, </span><span class="s1">pred)</span>
    <span class="s1">loss_w_sw = loss(y</span><span class="s2">, </span><span class="s1">pred</span><span class="s2">, </span><span class="s1">np.ones(pred.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.float32))</span>
    <span class="s2">assert </span><span class="s1">loss_wo_sw == approx(loss_w_sw)</span>


<span class="s2">def </span><span class="s1">test_sample_weight_init_estimators():</span>
    <span class="s3"># Smoke test for init estimators with sample weights.</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">13</span><span class="s1">)</span>
    <span class="s1">X = rng.rand(</span><span class="s4">100</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">sample_weight = np.ones(</span><span class="s4">100</span><span class="s1">)</span>
    <span class="s1">reg_y = rng.rand(</span><span class="s4">100</span><span class="s1">)</span>

    <span class="s1">clf_y = rng.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">100</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">Loss </span><span class="s2">in </span><span class="s1">LOSS_FUNCTIONS.values():</span>
        <span class="s2">if </span><span class="s1">Loss </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">continue</span>
        <span class="s2">if </span><span class="s1">issubclass(Loss</span><span class="s2">, </span><span class="s1">RegressionLossFunction):</span>
            <span class="s1">y = reg_y</span>
            <span class="s1">loss = Loss()</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">k = </span><span class="s4">2</span>
            <span class="s1">y = clf_y</span>
            <span class="s2">if </span><span class="s1">Loss.is_multi_class:</span>
                <span class="s3"># skip multiclass</span>
                <span class="s2">continue</span>
            <span class="s1">loss = Loss(k)</span>

        <span class="s1">init_est = loss.init_estimator()</span>
        <span class="s1">init_est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">out = loss.get_init_raw_predictions(X</span><span class="s2">, </span><span class="s1">init_est)</span>
        <span class="s2">assert </span><span class="s1">out.shape == (y.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s1">sw_init_est = loss.init_estimator()</span>
        <span class="s1">sw_init_est.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
        <span class="s1">sw_out = loss.get_init_raw_predictions(X</span><span class="s2">, </span><span class="s1">sw_init_est)</span>
        <span class="s2">assert </span><span class="s1">sw_out.shape == (y.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s3"># check if predictions match</span>
        <span class="s1">assert_allclose(out</span><span class="s2">, </span><span class="s1">sw_out</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_quantile_loss_function():</span>
    <span class="s3"># Non regression test for the QuantileLossFunction object</span>
    <span class="s3"># There was a sign problem when evaluating the function</span>
    <span class="s3"># for negative values of 'ytrue - ypred'</span>
    <span class="s1">x = np.asarray([-</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">])</span>
    <span class="s1">y_found = QuantileLossFunction(</span><span class="s4">0.9</span><span class="s1">)(x</span><span class="s2">, </span><span class="s1">np.zeros_like(x))</span>
    <span class="s1">y_expected = np.asarray([</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">0.9</span><span class="s1">]).mean()</span>
    <span class="s1">np.testing.assert_allclose(y_found</span><span class="s2">, </span><span class="s1">y_expected)</span>
    <span class="s1">y_found_p = mean_pinball_loss(x</span><span class="s2">, </span><span class="s1">np.zeros_like(x)</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s4">0.9</span><span class="s1">)</span>
    <span class="s1">np.testing.assert_allclose(y_found</span><span class="s2">, </span><span class="s1">y_found_p)</span>


<span class="s2">def </span><span class="s1">test_sample_weight_deviance():</span>
    <span class="s3"># Test if deviance supports sample weights.</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">13</span><span class="s1">)</span>
    <span class="s1">sample_weight = np.ones(</span><span class="s4">100</span><span class="s1">)</span>
    <span class="s1">reg_y = rng.rand(</span><span class="s4">100</span><span class="s1">)</span>
    <span class="s1">clf_y = rng.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">100</span><span class="s1">)</span>
    <span class="s1">mclf_y = rng.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">100</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">Loss </span><span class="s2">in </span><span class="s1">LOSS_FUNCTIONS.values():</span>
        <span class="s2">if </span><span class="s1">Loss </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">continue</span>
        <span class="s2">if </span><span class="s1">issubclass(Loss</span><span class="s2">, </span><span class="s1">RegressionLossFunction):</span>
            <span class="s1">y = reg_y</span>
            <span class="s1">p = reg_y</span>
            <span class="s1">loss = Loss()</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">k = </span><span class="s4">2</span>
            <span class="s1">y = clf_y</span>
            <span class="s1">p = clf_y</span>
            <span class="s2">if </span><span class="s1">Loss.is_multi_class:</span>
                <span class="s1">k = </span><span class="s4">3</span>
                <span class="s1">y = mclf_y</span>
                <span class="s3"># one-hot encoding</span>
                <span class="s1">p = np.zeros((y.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">k)</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
                <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(k):</span>
                    <span class="s1">p[:</span><span class="s2">, </span><span class="s1">i] = y == i</span>
            <span class="s1">loss = Loss(k)</span>

        <span class="s1">deviance_w_w = loss(y</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">sample_weight)</span>
        <span class="s1">deviance_wo_w = loss(y</span><span class="s2">, </span><span class="s1">p)</span>
        <span class="s1">assert_allclose(deviance_wo_w</span><span class="s2">, </span><span class="s1">deviance_w_w)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;n_classes, n_samples&quot;</span><span class="s2">, </span><span class="s1">[(</span><span class="s4">3</span><span class="s2">, </span><span class="s4">100</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s4">5</span><span class="s2">, </span><span class="s4">57</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s4">7</span><span class="s2">, </span><span class="s4">13</span><span class="s1">)])</span>
<span class="s2">def </span><span class="s1">test_multinomial_deviance(n_classes</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check multinomial deviance with and without sample weights.</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">sample_weight = np.ones(n_samples)</span>
    <span class="s1">y_true = rng.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">n_classes</span><span class="s2">, </span><span class="s1">size=n_samples)</span>
    <span class="s1">y_pred = np.zeros((n_samples</span><span class="s2">, </span><span class="s1">n_classes)</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s2">for </span><span class="s1">klass </span><span class="s2">in </span><span class="s1">range(y_pred.shape[</span><span class="s4">1</span><span class="s1">]):</span>
        <span class="s1">y_pred[:</span><span class="s2">, </span><span class="s1">klass] = y_true == klass</span>

    <span class="s1">loss = MultinomialDeviance(n_classes)</span>
    <span class="s1">loss_wo_sw = loss(y_true</span><span class="s2">, </span><span class="s1">y_pred)</span>
    <span class="s2">assert </span><span class="s1">loss_wo_sw &gt; </span><span class="s4">0</span>
    <span class="s1">loss_w_sw = loss(y_true</span><span class="s2">, </span><span class="s1">y_pred</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s2">assert </span><span class="s1">loss_wo_sw == approx(loss_w_sw)</span>

    <span class="s3"># Multinomial deviance uses weighted average loss rather than</span>
    <span class="s3"># weighted sum loss, so we make sure that the value remains the same</span>
    <span class="s3"># when we device the weight by 2.</span>
    <span class="s1">loss_w_sw = loss(y_true</span><span class="s2">, </span><span class="s1">y_pred</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s4">0.5 </span><span class="s1">* sample_weight)</span>
    <span class="s2">assert </span><span class="s1">loss_wo_sw == approx(loss_w_sw)</span>


<span class="s2">def </span><span class="s1">test_mdl_computation_weighted():</span>
    <span class="s1">raw_predictions = np.array([[</span><span class="s4">1.0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">2.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">2.0</span><span class="s1">]])</span>
    <span class="s1">y_true = np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">weights = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span>
    <span class="s1">expected_loss = </span><span class="s4">1.0909323</span>
    <span class="s3"># MultinomialDeviance loss computation with weights.</span>
    <span class="s1">loss = MultinomialDeviance(</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">loss(y_true</span><span class="s2">, </span><span class="s1">raw_predictions</span><span class="s2">, </span><span class="s1">weights) == approx(expected_loss)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;n&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_mdl_exception(n):</span>
    <span class="s3"># Check that MultinomialDeviance throws an exception when n_classes &lt;= 2</span>
    <span class="s1">err_msg = </span><span class="s5">&quot;MultinomialDeviance requires more than 2 classes.&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">MultinomialDeviance(n)</span>


<span class="s2">def </span><span class="s1">test_init_raw_predictions_shapes():</span>
    <span class="s3"># Make sure get_init_raw_predictions returns float64 arrays with shape</span>
    <span class="s3"># (n_samples, K) where K is 1 for binary classification and regression, and</span>
    <span class="s3"># K = n_classes for multiclass classification</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">n_samples = </span><span class="s4">100</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s2">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">y = rng.normal(size=n_samples)</span>
    <span class="s2">for </span><span class="s1">loss </span><span class="s2">in </span><span class="s1">(</span>
        <span class="s1">LeastSquaresError()</span><span class="s2">,</span>
        <span class="s1">LeastAbsoluteError()</span><span class="s2">,</span>
        <span class="s1">QuantileLossFunction()</span><span class="s2">,</span>
        <span class="s1">HuberLossFunction()</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">init_estimator = loss.init_estimator().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">raw_predictions = loss.get_init_raw_predictions(y</span><span class="s2">, </span><span class="s1">init_estimator)</span>
        <span class="s2">assert </span><span class="s1">raw_predictions.shape == (n_samples</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">raw_predictions.dtype == np.float64</span>

    <span class="s1">y = rng.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s1">size=n_samples)</span>
    <span class="s2">for </span><span class="s1">loss </span><span class="s2">in </span><span class="s1">(BinomialDeviance(n_classes=</span><span class="s4">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">ExponentialLoss(n_classes=</span><span class="s4">2</span><span class="s1">)):</span>
        <span class="s1">init_estimator = loss.init_estimator().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">raw_predictions = loss.get_init_raw_predictions(y</span><span class="s2">, </span><span class="s1">init_estimator)</span>
        <span class="s2">assert </span><span class="s1">raw_predictions.shape == (n_samples</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">raw_predictions.dtype == np.float64</span>

    <span class="s2">for </span><span class="s1">n_classes </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">3</span><span class="s2">, </span><span class="s4">5</span><span class="s1">):</span>
        <span class="s1">y = rng.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">n_classes</span><span class="s2">, </span><span class="s1">size=n_samples)</span>
        <span class="s1">loss = MultinomialDeviance(n_classes=n_classes)</span>
        <span class="s1">init_estimator = loss.init_estimator().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">raw_predictions = loss.get_init_raw_predictions(y</span><span class="s2">, </span><span class="s1">init_estimator)</span>
        <span class="s2">assert </span><span class="s1">raw_predictions.shape == (n_samples</span><span class="s2">, </span><span class="s1">n_classes)</span>
        <span class="s2">assert </span><span class="s1">raw_predictions.dtype == np.float64</span>


<span class="s2">def </span><span class="s1">test_init_raw_predictions_values(global_random_seed):</span>
    <span class="s3"># Make sure the get_init_raw_predictions() returns the expected values for</span>
    <span class="s3"># each loss.</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>

    <span class="s1">n_samples = </span><span class="s4">100</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s2">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">y = rng.normal(size=n_samples)</span>

    <span class="s3"># Least squares loss</span>
    <span class="s1">loss = LeastSquaresError()</span>
    <span class="s1">init_estimator = loss.init_estimator().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">raw_predictions = loss.get_init_raw_predictions(y</span><span class="s2">, </span><span class="s1">init_estimator)</span>
    <span class="s3"># Make sure baseline prediction is the mean of all targets</span>
    <span class="s1">assert_allclose(raw_predictions</span><span class="s2">, </span><span class="s1">y.mean())</span>

    <span class="s3"># Least absolute and huber loss</span>
    <span class="s2">for </span><span class="s1">Loss </span><span class="s2">in </span><span class="s1">(LeastAbsoluteError</span><span class="s2">, </span><span class="s1">HuberLossFunction):</span>
        <span class="s1">loss = Loss()</span>
        <span class="s1">init_estimator = loss.init_estimator().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">raw_predictions = loss.get_init_raw_predictions(y</span><span class="s2">, </span><span class="s1">init_estimator)</span>
        <span class="s3"># Make sure baseline prediction is the median of all targets</span>
        <span class="s1">assert_allclose(raw_predictions</span><span class="s2">, </span><span class="s1">np.median(y))</span>

    <span class="s3"># Quantile loss</span>
    <span class="s2">for </span><span class="s1">alpha </span><span class="s2">in </span><span class="s1">(</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0.9</span><span class="s1">):</span>
        <span class="s1">loss = QuantileLossFunction(alpha=alpha)</span>
        <span class="s1">init_estimator = loss.init_estimator().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">raw_predictions = loss.get_init_raw_predictions(y</span><span class="s2">, </span><span class="s1">init_estimator)</span>
        <span class="s3"># Make sure baseline prediction is the alpha-quantile of all targets</span>
        <span class="s1">assert_allclose(raw_predictions</span><span class="s2">, </span><span class="s1">np.percentile(y</span><span class="s2">, </span><span class="s1">alpha * </span><span class="s4">100</span><span class="s1">))</span>

    <span class="s1">y = rng.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s1">size=n_samples)</span>

    <span class="s3"># Binomial deviance</span>
    <span class="s1">loss = BinomialDeviance(n_classes=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">init_estimator = loss.init_estimator().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s3"># Make sure baseline prediction is equal to link_function(p), where p</span>
    <span class="s3"># is the proba of the positive class. We want predict_proba() to return p,</span>
    <span class="s3"># and by definition</span>
    <span class="s3"># p = inverse_link_function(raw_prediction) = sigmoid(raw_prediction)</span>
    <span class="s3"># So we want raw_prediction = link_function(p) = log(p / (1 - p))</span>
    <span class="s1">raw_predictions = loss.get_init_raw_predictions(y</span><span class="s2">, </span><span class="s1">init_estimator)</span>
    <span class="s1">p = y.mean()</span>
    <span class="s1">assert_allclose(raw_predictions</span><span class="s2">, </span><span class="s1">np.log(p / (</span><span class="s4">1 </span><span class="s1">- p)))</span>

    <span class="s3"># Exponential loss</span>
    <span class="s1">loss = ExponentialLoss(n_classes=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">init_estimator = loss.init_estimator().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">raw_predictions = loss.get_init_raw_predictions(y</span><span class="s2">, </span><span class="s1">init_estimator)</span>
    <span class="s1">p = y.mean()</span>
    <span class="s1">assert_allclose(raw_predictions</span><span class="s2">, </span><span class="s4">0.5 </span><span class="s1">* np.log(p / (</span><span class="s4">1 </span><span class="s1">- p)))</span>

    <span class="s3"># Multinomial deviance loss</span>
    <span class="s2">for </span><span class="s1">n_classes </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">3</span><span class="s2">, </span><span class="s4">5</span><span class="s1">):</span>
        <span class="s1">y = rng.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">n_classes</span><span class="s2">, </span><span class="s1">size=n_samples)</span>
        <span class="s1">loss = MultinomialDeviance(n_classes=n_classes)</span>
        <span class="s1">init_estimator = loss.init_estimator().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">raw_predictions = loss.get_init_raw_predictions(y</span><span class="s2">, </span><span class="s1">init_estimator)</span>
        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(n_classes):</span>
            <span class="s1">p = (y == k).mean()</span>
            <span class="s1">assert_allclose(raw_predictions[:</span><span class="s2">, </span><span class="s1">k]</span><span class="s2">, </span><span class="s1">np.log(p))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;alpha&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.4</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0.6</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_lad_equals_quantiles(global_random_seed</span><span class="s2">, </span><span class="s1">alpha):</span>
    <span class="s3"># Make sure quantile loss with alpha = .5 is equivalent to LAD</span>
    <span class="s1">lad = LeastAbsoluteError()</span>
    <span class="s1">ql = QuantileLossFunction(alpha=alpha)</span>

    <span class="s1">n_samples = </span><span class="s4">50</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">raw_predictions = rng.normal(size=(n_samples))</span>
    <span class="s1">y_true = rng.normal(size=(n_samples))</span>

    <span class="s1">lad_loss = lad(y_true</span><span class="s2">, </span><span class="s1">raw_predictions)</span>
    <span class="s1">ql_loss = ql(y_true</span><span class="s2">, </span><span class="s1">raw_predictions)</span>
    <span class="s2">if </span><span class="s1">alpha == </span><span class="s4">0.5</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s1">lad_loss == approx(</span><span class="s4">2 </span><span class="s1">* ql_loss)</span>

    <span class="s1">weights = np.linspace(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">n_samples) ** </span><span class="s4">2</span>
    <span class="s1">lad_weighted_loss = lad(y_true</span><span class="s2">, </span><span class="s1">raw_predictions</span><span class="s2">, </span><span class="s1">sample_weight=weights)</span>
    <span class="s1">ql_weighted_loss = ql(y_true</span><span class="s2">, </span><span class="s1">raw_predictions</span><span class="s2">, </span><span class="s1">sample_weight=weights)</span>
    <span class="s2">if </span><span class="s1">alpha == </span><span class="s4">0.5</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s1">lad_weighted_loss == approx(</span><span class="s4">2 </span><span class="s1">* ql_weighted_loss)</span>
    <span class="s1">pbl_weighted_loss = mean_pinball_loss(</span>
        <span class="s1">y_true</span><span class="s2">, </span><span class="s1">raw_predictions</span><span class="s2">, </span><span class="s1">sample_weight=weights</span><span class="s2">, </span><span class="s1">alpha=alpha</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">pbl_weighted_loss == approx(ql_weighted_loss)</span>


<span class="s2">def </span><span class="s1">test_exponential_loss():</span>
    <span class="s0">&quot;&quot;&quot;Check that we compute the negative gradient of the exponential loss. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/9666 
    &quot;&quot;&quot;</span>
    <span class="s1">loss = ExponentialLoss(n_classes=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">y_true = np.array([</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">y_pred = np.array([</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s3"># we expect to have loss = exp(0) = 1</span>
    <span class="s2">assert </span><span class="s1">loss(y_true</span><span class="s2">, </span><span class="s1">y_pred) == pytest.approx(</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3"># we expect to have negative gradient = -1 * (1 * exp(0)) = -1</span>
    <span class="s1">assert_allclose(loss.negative_gradient(y_true</span><span class="s2">, </span><span class="s1">y_pred)</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
</pre>
</body>
</html>