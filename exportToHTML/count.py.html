<html>
<head>
<title>count.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
count.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot; 
Created on Mon Jul 26 08:34:59 2010 
 
Author: josef-pktd 
 
changes: 
added offset and zero-inflated version of Poisson 
 - kind of ok, need better test cases, 
 - a nan in ZIP bse, need to check hessian calculations 
 - found error in ZIP loglike 
 - all tests pass with 
 
Issues 
------ 
* If true model is not zero-inflated then numerical Hessian for ZIP has zeros 
  for the inflation probability and is not invertible. 
  -&gt; hessian inverts and bse look ok if row and column are dropped, pinv also works 
* GenericMLE: still get somewhere (where?) 
   &quot;CacheWriteWarning: The attribute 'bse' cannot be overwritten&quot; 
* bfgs is too fragile, does not come back 
* `nm` is slow but seems to work 
* need good start_params and their use in genericmle needs to be checked for 
  consistency, set as attribute or method (called as attribute) 
* numerical hessian needs better scaling 
 
* check taking parts out of the loop, e.g. factorial(endog) could be precalculated 
 
 
&quot;&quot;&quot;</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">stats</span>
<span class="s3">from </span><span class="s1">scipy.special </span><span class="s3">import </span><span class="s1">factorial</span>
<span class="s3">from </span><span class="s1">statsmodels.base.model </span><span class="s3">import </span><span class="s1">GenericLikelihoodModel</span>


<span class="s3">def </span><span class="s1">maxabs(arr1</span><span class="s3">, </span><span class="s1">arr2):</span>
    <span class="s3">return </span><span class="s1">np.max(np.abs(arr1 - arr2))</span>

<span class="s3">def </span><span class="s1">maxabsrel(arr1</span><span class="s3">, </span><span class="s1">arr2):</span>
    <span class="s3">return </span><span class="s1">np.max(np.abs(arr2 / arr1 - </span><span class="s4">1</span><span class="s1">))</span>



<span class="s3">class </span><span class="s1">PoissonGMLE(GenericLikelihoodModel):</span>
    <span class="s2">'''Maximum Likelihood Estimation of Poisson Model 
 
    This is an example for generic MLE which has the same 
    statistical model as discretemod.Poisson. 
 
    Except for defining the negative log-likelihood method, all 
    methods and results are generic. Gradients and Hessian 
    and all resulting statistics are based on numerical 
    differentiation. 
 
    '''</span>

    <span class="s0"># copied from discretemod.Poisson</span>
    <span class="s3">def </span><span class="s1">nloglikeobs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot; 
        Loglikelihood of Poisson model 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
 
        Returns 
        ------- 
        The log likelihood of the model evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\ln L=\\sum_{i=1}^{n}\\left[-\\lambda_{i}+y_{i}x_{i}^{\\prime}\\beta-\\ln y_{i}!\\right] 
        &quot;&quot;&quot;</span>
        <span class="s1">XB = np.dot(self.exog</span><span class="s3">, </span><span class="s1">params)</span>
        <span class="s1">endog = self.endog</span>
        <span class="s3">return </span><span class="s1">np.exp(XB) -  endog*XB + np.log(factorial(endog))</span>

    <span class="s3">def </span><span class="s1">predict_distribution(self</span><span class="s3">, </span><span class="s1">exog):</span>
        <span class="s2">'''return frozen scipy.stats distribution with mu at estimated prediction 
        '''</span>
        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s5">&quot;result&quot;</span><span class="s1">):</span>
            <span class="s0"># TODO: why would this be ValueError instead of AttributeError?</span>
            <span class="s0"># TODO: Why even make this a Model attribute in the first place?</span>
            <span class="s0">#  It belongs on the Results class</span>
            <span class="s3">raise </span><span class="s1">ValueError</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">result = self.result</span>
            <span class="s1">params = result.params</span>
            <span class="s1">mu = np.exp(np.dot(exog</span><span class="s3">, </span><span class="s1">params))</span>
            <span class="s3">return </span><span class="s1">stats.poisson(mu</span><span class="s3">, </span><span class="s1">loc=</span><span class="s4">0</span><span class="s1">)</span>



<span class="s3">class </span><span class="s1">PoissonOffsetGMLE(GenericLikelihoodModel):</span>
    <span class="s2">'''Maximum Likelihood Estimation of Poisson Model 
 
    This is an example for generic MLE which has the same 
    statistical model as discretemod.Poisson but adds offset 
 
    Except for defining the negative log-likelihood method, all 
    methods and results are generic. Gradients and Hessian 
    and all resulting statistics are based on numerical 
    differentiation. 
 
    '''</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None, </span><span class="s1">missing=</span><span class="s5">'none'</span><span class="s3">, </span><span class="s1">**kwds):</span>
        <span class="s0"># let them be none in case user wants to use inheritance</span>
        <span class="s3">if </span><span class="s1">offset </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">offset.ndim == </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">offset = offset[:</span><span class="s3">,None</span><span class="s1">] </span><span class="s0">#need column</span>
            <span class="s1">self.offset = offset.ravel()</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.offset = </span><span class="s4">0.</span>
        <span class="s1">super(PoissonOffsetGMLE</span><span class="s3">, </span><span class="s1">self).__init__(endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">missing=missing</span><span class="s3">,</span>
                <span class="s1">**kwds)</span>

<span class="s0">#this was added temporarily for bug-hunting, but should not be needed</span>
<span class="s0">#    def loglike(self, params):</span>
<span class="s0">#        return -self.nloglikeobs(params).sum(0)</span>

    <span class="s0"># original copied from discretemod.Poisson</span>
    <span class="s3">def </span><span class="s1">nloglikeobs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot; 
        Loglikelihood of Poisson model 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
 
        Returns 
        ------- 
        The log likelihood of the model evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\ln L=\\sum_{i=1}^{n}\\left[-\\lambda_{i}+y_{i}x_{i}^{\\prime}\\beta-\\ln y_{i}!\\right] 
        &quot;&quot;&quot;</span>

        <span class="s1">XB = self.offset + np.dot(self.exog</span><span class="s3">, </span><span class="s1">params)</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">nloglik = np.exp(XB) -  endog*XB + np.log(factorial(endog))</span>
        <span class="s3">return </span><span class="s1">nloglik</span>

<span class="s3">class </span><span class="s1">PoissonZiGMLE(GenericLikelihoodModel):</span>
    <span class="s2">'''Maximum Likelihood Estimation of Poisson Model 
 
    This is an example for generic MLE which has the same statistical model 
    as discretemod.Poisson but adds offset and zero-inflation. 
 
    Except for defining the negative log-likelihood method, all 
    methods and results are generic. Gradients and Hessian 
    and all resulting statistics are based on numerical 
    differentiation. 
 
    There are numerical problems if there is no zero-inflation. 
 
    '''</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None, </span><span class="s1">missing=</span><span class="s5">'none'</span><span class="s3">, </span><span class="s1">**kwds):</span>
        <span class="s0"># let them be none in case user wants to use inheritance</span>
        <span class="s1">self.k_extra = </span><span class="s4">1</span>
        <span class="s1">super(PoissonZiGMLE</span><span class="s3">, </span><span class="s1">self).__init__(endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">missing=missing</span><span class="s3">,</span>
                <span class="s1">extra_params_names=[</span><span class="s5">&quot;zi&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">**kwds)</span>
        <span class="s3">if </span><span class="s1">offset </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">offset.ndim == </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">offset = offset[:</span><span class="s3">,None</span><span class="s1">] </span><span class="s0">#need column</span>
            <span class="s1">self.offset = offset.ravel()  </span><span class="s0">#which way?</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.offset = </span><span class="s4">0.</span>

        <span class="s0">#TODO: it's not standard pattern to use default exog</span>
        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self.exog = np.ones((self.nobs</span><span class="s3">,</span><span class="s4">1</span><span class="s1">))</span>
        <span class="s1">self.nparams = self.exog.shape[</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s0">#what's the shape in regression for exog if only constant</span>
        <span class="s1">self.start_params = np.hstack((np.ones(self.nparams)</span><span class="s3">, </span><span class="s4">0</span><span class="s1">))</span>
        <span class="s0"># need to add zi params to nparams</span>
        <span class="s1">self.nparams += </span><span class="s4">1</span>
        <span class="s1">self.cloneattr = [</span><span class="s5">'start_params'</span><span class="s1">]</span>
        <span class="s0"># needed for t_test and summary</span>
        <span class="s0"># Note: no added to super __init__ which also adjusts df_resid</span>
        <span class="s0"># self.exog_names.append('zi')</span>


    <span class="s0"># original copied from discretemod.Poisson</span>
    <span class="s3">def </span><span class="s1">nloglikeobs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot; 
        Loglikelihood of Poisson model 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
 
        Returns 
        ------- 
        The log likelihood of the model evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\ln L=\\sum_{i=1}^{n}\\left[-\\lambda_{i}+y_{i}x_{i}^{\\prime}\\beta-\\ln y_{i}!\\right] 
        &quot;&quot;&quot;</span>
        <span class="s1">beta = params[:-</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">gamm = </span><span class="s4">1 </span><span class="s1">/ (</span><span class="s4">1 </span><span class="s1">+ np.exp(params[-</span><span class="s4">1</span><span class="s1">]))  </span><span class="s0">#check this</span>
        <span class="s0"># replace with np.dot(self.exogZ, gamma)</span>
        <span class="s0">#print(np.shape(self.offset), self.exog.shape, beta.shape</span>
        <span class="s1">XB = self.offset + np.dot(self.exog</span><span class="s3">, </span><span class="s1">beta)</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">nloglik = -np.log(</span><span class="s4">1</span><span class="s1">-gamm) + np.exp(XB) -  endog*XB + np.log(factorial(endog))</span>
        <span class="s1">nloglik[endog==</span><span class="s4">0</span><span class="s1">] = - np.log(gamm + np.exp(-nloglik[endog==</span><span class="s4">0</span><span class="s1">]))</span>

        <span class="s3">return </span><span class="s1">nloglik</span>
</pre>
</body>
</html>