<html>
<head>
<title>test_partial_dependence.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_partial_dependence.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Testing for the partial dependence module. 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">warnings</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>

<span class="s2">import </span><span class="s1">sklearn</span>
<span class="s2">from </span><span class="s1">sklearn.base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">ClassifierMixin</span><span class="s2">, </span><span class="s1">clone</span><span class="s2">, </span><span class="s1">is_regressor</span>
<span class="s2">from </span><span class="s1">sklearn.cluster </span><span class="s2">import </span><span class="s1">KMeans</span>
<span class="s2">from </span><span class="s1">sklearn.compose </span><span class="s2">import </span><span class="s1">make_column_transformer</span>
<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">load_iris</span><span class="s2">, </span><span class="s1">make_classification</span><span class="s2">, </span><span class="s1">make_regression</span>
<span class="s2">from </span><span class="s1">sklearn.dummy </span><span class="s2">import </span><span class="s1">DummyClassifier</span>
<span class="s2">from </span><span class="s1">sklearn.ensemble </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">GradientBoostingClassifier</span><span class="s2">,</span>
    <span class="s1">GradientBoostingRegressor</span><span class="s2">,</span>
    <span class="s1">HistGradientBoostingClassifier</span><span class="s2">,</span>
    <span class="s1">HistGradientBoostingRegressor</span><span class="s2">,</span>
    <span class="s1">RandomForestRegressor</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.exceptions </span><span class="s2">import </span><span class="s1">NotFittedError</span>
<span class="s2">from </span><span class="s1">sklearn.inspection </span><span class="s2">import </span><span class="s1">partial_dependence</span>
<span class="s2">from </span><span class="s1">sklearn.inspection._partial_dependence </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">_grid_from_X</span><span class="s2">,</span>
    <span class="s1">_partial_dependence_brute</span><span class="s2">,</span>
    <span class="s1">_partial_dependence_recursion</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">LinearRegression</span><span class="s2">, </span><span class="s1">LogisticRegression</span><span class="s2">, </span><span class="s1">MultiTaskLasso</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">r2_score</span>
<span class="s2">from </span><span class="s1">sklearn.pipeline </span><span class="s2">import </span><span class="s1">make_pipeline</span>
<span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">PolynomialFeatures</span><span class="s2">,</span>
    <span class="s1">RobustScaler</span><span class="s2">,</span>
    <span class="s1">StandardScaler</span><span class="s2">,</span>
    <span class="s1">scale</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.tree </span><span class="s2">import </span><span class="s1">DecisionTreeRegressor</span>
<span class="s2">from </span><span class="s1">sklearn.tree.tests.test_tree </span><span class="s2">import </span><span class="s1">assert_is_subtree</span>
<span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">_IS_32BIT</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">assert_allclose</span><span class="s2">, </span><span class="s1">assert_array_equal</span>
<span class="s2">from </span><span class="s1">sklearn.utils.validation </span><span class="s2">import </span><span class="s1">check_random_state</span>

<span class="s3"># toy sample</span>
<span class="s1">X = [[-</span><span class="s4">2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]]</span>
<span class="s1">y = [-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>


<span class="s3"># (X, y), n_targets  &lt;-- as expected in the output of partial_dep()</span>
<span class="s1">binary_classification_data = (make_classification(n_samples=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">multiclass_classification_data = (</span>
    <span class="s1">make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">n_classes=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">n_clusters_per_class=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span><span class="s2">,</span>
    <span class="s4">3</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">regression_data = (make_regression(n_samples=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">multioutput_regression_data = (</span>
    <span class="s1">make_regression(n_samples=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">n_targets=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s4">2</span><span class="s2">,</span>
<span class="s1">)</span>

<span class="s3"># iris</span>
<span class="s1">iris = load_iris()</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;Estimator, method, data&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(GradientBoostingClassifier</span><span class="s2">, </span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">binary_classification_data)</span><span class="s2">,</span>
        <span class="s1">(GradientBoostingClassifier</span><span class="s2">, </span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">multiclass_classification_data)</span><span class="s2">,</span>
        <span class="s1">(GradientBoostingClassifier</span><span class="s2">, </span><span class="s5">&quot;brute&quot;</span><span class="s2">, </span><span class="s1">binary_classification_data)</span><span class="s2">,</span>
        <span class="s1">(GradientBoostingClassifier</span><span class="s2">, </span><span class="s5">&quot;brute&quot;</span><span class="s2">, </span><span class="s1">multiclass_classification_data)</span><span class="s2">,</span>
        <span class="s1">(GradientBoostingRegressor</span><span class="s2">, </span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">regression_data)</span><span class="s2">,</span>
        <span class="s1">(GradientBoostingRegressor</span><span class="s2">, </span><span class="s5">&quot;brute&quot;</span><span class="s2">, </span><span class="s1">regression_data)</span><span class="s2">,</span>
        <span class="s1">(DecisionTreeRegressor</span><span class="s2">, </span><span class="s5">&quot;brute&quot;</span><span class="s2">, </span><span class="s1">regression_data)</span><span class="s2">,</span>
        <span class="s1">(LinearRegression</span><span class="s2">, </span><span class="s5">&quot;brute&quot;</span><span class="s2">, </span><span class="s1">regression_data)</span><span class="s2">,</span>
        <span class="s1">(LinearRegression</span><span class="s2">, </span><span class="s5">&quot;brute&quot;</span><span class="s2">, </span><span class="s1">multioutput_regression_data)</span><span class="s2">,</span>
        <span class="s1">(LogisticRegression</span><span class="s2">, </span><span class="s5">&quot;brute&quot;</span><span class="s2">, </span><span class="s1">binary_classification_data)</span><span class="s2">,</span>
        <span class="s1">(LogisticRegression</span><span class="s2">, </span><span class="s5">&quot;brute&quot;</span><span class="s2">, </span><span class="s1">multiclass_classification_data)</span><span class="s2">,</span>
        <span class="s1">(MultiTaskLasso</span><span class="s2">, </span><span class="s5">&quot;brute&quot;</span><span class="s2">, </span><span class="s1">multioutput_regression_data)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;grid_resolution&quot;</span><span class="s2">, </span><span class="s1">(</span><span class="s4">5</span><span class="s2">, </span><span class="s4">10</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;features&quot;</span><span class="s2">, </span><span class="s1">([</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kind&quot;</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;average&quot;</span><span class="s2">, </span><span class="s5">&quot;individual&quot;</span><span class="s2">, </span><span class="s5">&quot;both&quot;</span><span class="s1">))</span>
<span class="s2">def </span><span class="s1">test_output_shape(Estimator</span><span class="s2">, </span><span class="s1">method</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">grid_resolution</span><span class="s2">, </span><span class="s1">features</span><span class="s2">, </span><span class="s1">kind):</span>
    <span class="s3"># Check that partial_dependence has consistent output shape for different</span>
    <span class="s3"># kinds of estimators:</span>
    <span class="s3"># - classifiers with binary and multiclass settings</span>
    <span class="s3"># - regressors</span>
    <span class="s3"># - multi-task regressors</span>

    <span class="s1">est = Estimator()</span>
    <span class="s2">if </span><span class="s1">hasattr(est</span><span class="s2">, </span><span class="s5">&quot;n_estimators&quot;</span><span class="s1">):</span>
        <span class="s1">est.set_params(n_estimators=</span><span class="s4">2</span><span class="s1">)  </span><span class="s3"># speed-up computations</span>

    <span class="s3"># n_target corresponds to the number of classes (1 for binary classif) or</span>
    <span class="s3"># the number of tasks / outputs in multi task settings. It's equal to 1 for</span>
    <span class="s3"># classical regression_data.</span>
    <span class="s1">(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">, </span><span class="s1">n_targets = data</span>
    <span class="s1">n_instances = X.shape[</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">result = partial_dependence(</span>
        <span class="s1">est</span><span class="s2">,</span>
        <span class="s1">X=X</span><span class="s2">,</span>
        <span class="s1">features=features</span><span class="s2">,</span>
        <span class="s1">method=method</span><span class="s2">,</span>
        <span class="s1">kind=kind</span><span class="s2">,</span>
        <span class="s1">grid_resolution=grid_resolution</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">pdp</span><span class="s2">, </span><span class="s1">axes = result</span><span class="s2">, </span><span class="s1">result[</span><span class="s5">&quot;grid_values&quot;</span><span class="s1">]</span>

    <span class="s1">expected_pdp_shape = (n_targets</span><span class="s2">, </span><span class="s1">*[grid_resolution </span><span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(len(features))])</span>
    <span class="s1">expected_ice_shape = (</span>
        <span class="s1">n_targets</span><span class="s2">,</span>
        <span class="s1">n_instances</span><span class="s2">,</span>
        <span class="s1">*[grid_resolution </span><span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(len(features))]</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">if </span><span class="s1">kind == </span><span class="s5">&quot;average&quot;</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s1">pdp.average.shape == expected_pdp_shape</span>
    <span class="s2">elif </span><span class="s1">kind == </span><span class="s5">&quot;individual&quot;</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s1">pdp.individual.shape == expected_ice_shape</span>
    <span class="s2">else</span><span class="s1">:  </span><span class="s3"># 'both'</span>
        <span class="s2">assert </span><span class="s1">pdp.average.shape == expected_pdp_shape</span>
        <span class="s2">assert </span><span class="s1">pdp.individual.shape == expected_ice_shape</span>

    <span class="s1">expected_axes_shape = (len(features)</span><span class="s2">, </span><span class="s1">grid_resolution)</span>
    <span class="s2">assert </span><span class="s1">axes </span><span class="s2">is not None</span>
    <span class="s2">assert </span><span class="s1">np.asarray(axes).shape == expected_axes_shape</span>


<span class="s2">def </span><span class="s1">test_grid_from_X():</span>
    <span class="s3"># tests for _grid_from_X: sanity check for output, and for shapes.</span>

    <span class="s3"># Make sure that the grid is a cartesian product of the input (it will use</span>
    <span class="s3"># the unique values instead of the percentiles)</span>
    <span class="s1">percentiles = (</span><span class="s4">0.05</span><span class="s2">, </span><span class="s4">0.95</span><span class="s1">)</span>
    <span class="s1">grid_resolution = </span><span class="s4">100</span>
    <span class="s1">is_categorical = [</span><span class="s2">False, False</span><span class="s1">]</span>
    <span class="s1">X = np.asarray([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span>
    <span class="s1">grid</span><span class="s2">, </span><span class="s1">axes = _grid_from_X(X</span><span class="s2">, </span><span class="s1">percentiles</span><span class="s2">, </span><span class="s1">is_categorical</span><span class="s2">, </span><span class="s1">grid_resolution)</span>
    <span class="s1">assert_array_equal(grid</span><span class="s2">, </span><span class="s1">[[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span>
    <span class="s1">assert_array_equal(axes</span><span class="s2">, </span><span class="s1">X.T)</span>

    <span class="s3"># test shapes of returned objects depending on the number of unique values</span>
    <span class="s3"># for a feature.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">grid_resolution = </span><span class="s4">15</span>

    <span class="s3"># n_unique_values &gt; grid_resolution</span>
    <span class="s1">X = rng.normal(size=(</span><span class="s4">20</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">grid</span><span class="s2">, </span><span class="s1">axes = _grid_from_X(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">percentiles</span><span class="s2">, </span><span class="s1">is_categorical</span><span class="s2">, </span><span class="s1">grid_resolution=grid_resolution</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">grid.shape == (grid_resolution * grid_resolution</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s2">assert </span><span class="s1">np.asarray(axes).shape == (</span><span class="s4">2</span><span class="s2">, </span><span class="s1">grid_resolution)</span>

    <span class="s3"># n_unique_values &lt; grid_resolution, will use actual values</span>
    <span class="s1">n_unique_values = </span><span class="s4">12</span>
    <span class="s1">X[n_unique_values - </span><span class="s4">1 </span><span class="s1">:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">12345</span>
    <span class="s1">rng.shuffle(X)  </span><span class="s3"># just to make sure the order is irrelevant</span>
    <span class="s1">grid</span><span class="s2">, </span><span class="s1">axes = _grid_from_X(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">percentiles</span><span class="s2">, </span><span class="s1">is_categorical</span><span class="s2">, </span><span class="s1">grid_resolution=grid_resolution</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">grid.shape == (n_unique_values * grid_resolution</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s3"># axes is a list of arrays of different shapes</span>
    <span class="s2">assert </span><span class="s1">axes[</span><span class="s4">0</span><span class="s1">].shape == (n_unique_values</span><span class="s2">,</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">axes[</span><span class="s4">1</span><span class="s1">].shape == (grid_resolution</span><span class="s2">,</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;grid_resolution&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s4">2</span><span class="s2">,  </span><span class="s3"># since n_categories &gt; 2, we should not use quantiles resampling</span>
        <span class="s4">100</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_grid_from_X_with_categorical(grid_resolution):</span>
    <span class="s0">&quot;&quot;&quot;Check that `_grid_from_X` always sample from categories and does not 
    depend from the percentiles. 
    &quot;&quot;&quot;</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s5">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">percentiles = (</span><span class="s4">0.05</span><span class="s2">, </span><span class="s4">0.95</span><span class="s1">)</span>
    <span class="s1">is_categorical = [</span><span class="s2">True</span><span class="s1">]</span>
    <span class="s1">X = pd.DataFrame({</span><span class="s5">&quot;cat_feature&quot;</span><span class="s1">: [</span><span class="s5">&quot;A&quot;</span><span class="s2">, </span><span class="s5">&quot;B&quot;</span><span class="s2">, </span><span class="s5">&quot;C&quot;</span><span class="s2">, </span><span class="s5">&quot;A&quot;</span><span class="s2">, </span><span class="s5">&quot;B&quot;</span><span class="s2">, </span><span class="s5">&quot;D&quot;</span><span class="s2">, </span><span class="s5">&quot;E&quot;</span><span class="s1">]})</span>
    <span class="s1">grid</span><span class="s2">, </span><span class="s1">axes = _grid_from_X(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">percentiles</span><span class="s2">, </span><span class="s1">is_categorical</span><span class="s2">, </span><span class="s1">grid_resolution=grid_resolution</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">grid.shape == (</span><span class="s4">5</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s2">assert </span><span class="s1">axes[</span><span class="s4">0</span><span class="s1">].shape == (</span><span class="s4">5</span><span class="s2">,</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;grid_resolution&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">100</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_grid_from_X_heterogeneous_type(grid_resolution):</span>
    <span class="s0">&quot;&quot;&quot;Check that `_grid_from_X` always sample from categories and does not 
    depend from the percentiles. 
    &quot;&quot;&quot;</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s5">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">percentiles = (</span><span class="s4">0.05</span><span class="s2">, </span><span class="s4">0.95</span><span class="s1">)</span>
    <span class="s1">is_categorical = [</span><span class="s2">True, False</span><span class="s1">]</span>
    <span class="s1">X = pd.DataFrame(</span>
        <span class="s1">{</span>
            <span class="s5">&quot;cat&quot;</span><span class="s1">: [</span><span class="s5">&quot;A&quot;</span><span class="s2">, </span><span class="s5">&quot;B&quot;</span><span class="s2">, </span><span class="s5">&quot;C&quot;</span><span class="s2">, </span><span class="s5">&quot;A&quot;</span><span class="s2">, </span><span class="s5">&quot;B&quot;</span><span class="s2">, </span><span class="s5">&quot;D&quot;</span><span class="s2">, </span><span class="s5">&quot;E&quot;</span><span class="s2">, </span><span class="s5">&quot;A&quot;</span><span class="s2">, </span><span class="s5">&quot;B&quot;</span><span class="s2">, </span><span class="s5">&quot;D&quot;</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s5">&quot;num&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">8</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">}</span>
    <span class="s1">)</span>
    <span class="s1">nunique = X.nunique()</span>

    <span class="s1">grid</span><span class="s2">, </span><span class="s1">axes = _grid_from_X(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">percentiles</span><span class="s2">, </span><span class="s1">is_categorical</span><span class="s2">, </span><span class="s1">grid_resolution=grid_resolution</span>
    <span class="s1">)</span>
    <span class="s2">if </span><span class="s1">grid_resolution == </span><span class="s4">3</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s1">grid.shape == (</span><span class="s4">15</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">axes[</span><span class="s4">0</span><span class="s1">].shape[</span><span class="s4">0</span><span class="s1">] == nunique[</span><span class="s5">&quot;num&quot;</span><span class="s1">]</span>
        <span class="s2">assert </span><span class="s1">axes[</span><span class="s4">1</span><span class="s1">].shape[</span><span class="s4">0</span><span class="s1">] == grid_resolution</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s1">grid.shape == (</span><span class="s4">25</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">axes[</span><span class="s4">0</span><span class="s1">].shape[</span><span class="s4">0</span><span class="s1">] == nunique[</span><span class="s5">&quot;cat&quot;</span><span class="s1">]</span>
        <span class="s2">assert </span><span class="s1">axes[</span><span class="s4">1</span><span class="s1">].shape[</span><span class="s4">0</span><span class="s1">] == nunique[</span><span class="s5">&quot;cat&quot;</span><span class="s1">]</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;grid_resolution, percentiles, err_msg&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s4">2</span><span class="s2">, </span><span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0.0001</span><span class="s1">)</span><span class="s2">, </span><span class="s5">&quot;percentiles are too close&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s4">100</span><span class="s2">, </span><span class="s1">(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">)</span><span class="s2">, </span><span class="s5">&quot;'percentiles' must be a sequence of 2 elements&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s4">100</span><span class="s2">, </span><span class="s4">12345</span><span class="s2">, </span><span class="s5">&quot;'percentiles' must be a sequence of 2 elements&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s4">100</span><span class="s2">, </span><span class="s1">(-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0.95</span><span class="s1">)</span><span class="s2">, </span><span class="s5">r&quot;'percentiles' values must be in \[0, 1\]&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s4">100</span><span class="s2">, </span><span class="s1">(</span><span class="s4">0.05</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">, </span><span class="s5">r&quot;'percentiles' values must be in \[0, 1\]&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s4">100</span><span class="s2">, </span><span class="s1">(</span><span class="s4">0.9</span><span class="s2">, </span><span class="s4">0.1</span><span class="s1">)</span><span class="s2">, </span><span class="s5">r&quot;percentiles\[0\] must be strictly less than&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">(</span><span class="s4">0.05</span><span class="s2">, </span><span class="s4">0.95</span><span class="s1">)</span><span class="s2">, </span><span class="s5">&quot;'grid_resolution' must be strictly greater than 1&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_grid_from_X_error(grid_resolution</span><span class="s2">, </span><span class="s1">percentiles</span><span class="s2">, </span><span class="s1">err_msg):</span>
    <span class="s1">X = np.asarray([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span>
    <span class="s1">is_categorical = [</span><span class="s2">False</span><span class="s1">]</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">_grid_from_X(X</span><span class="s2">, </span><span class="s1">percentiles</span><span class="s2">, </span><span class="s1">is_categorical</span><span class="s2">, </span><span class="s1">grid_resolution)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;target_feature&quot;</span><span class="s2">, </span><span class="s1">range(</span><span class="s4">5</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;est, method&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(LinearRegression()</span><span class="s2">, </span><span class="s5">&quot;brute&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(GradientBoostingRegressor(random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s5">&quot;brute&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(GradientBoostingRegressor(random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s5">&quot;recursion&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(HistGradientBoostingRegressor(random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s5">&quot;brute&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(HistGradientBoostingRegressor(random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s5">&quot;recursion&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_partial_dependence_helpers(est</span><span class="s2">, </span><span class="s1">method</span><span class="s2">, </span><span class="s1">target_feature):</span>
    <span class="s3"># Check that what is returned by _partial_dependence_brute or</span>
    <span class="s3"># _partial_dependence_recursion is equivalent to manually setting a target</span>
    <span class="s3"># feature to a given value, and computing the average prediction over all</span>
    <span class="s3"># samples.</span>
    <span class="s3"># This also checks that the brute and recursion methods give the same</span>
    <span class="s3"># output.</span>
    <span class="s3"># Note that even on the trainset, the brute and the recursion methods</span>
    <span class="s3"># aren't always strictly equivalent, in particular when the slow method</span>
    <span class="s3"># generates unrealistic samples that have low mass in the joint</span>
    <span class="s3"># distribution of the input features, and when some of the features are</span>
    <span class="s3"># dependent. Hence the high tolerance on the checks.</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">n_informative=</span><span class="s4">5</span><span class="s1">)</span>
    <span class="s3"># The 'init' estimator for GBDT (here the average prediction) isn't taken</span>
    <span class="s3"># into account with the recursion method, for technical reasons. We set</span>
    <span class="s3"># the mean to 0 to that this 'bug' doesn't have any effect.</span>
    <span class="s1">y = y - y.mean()</span>
    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s3"># target feature will be set to .5 and then to 123</span>
    <span class="s1">features = np.array([target_feature]</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span>
    <span class="s1">grid = np.array([[</span><span class="s4">0.5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">123</span><span class="s1">]])</span>

    <span class="s2">if </span><span class="s1">method == </span><span class="s5">&quot;brute&quot;</span><span class="s1">:</span>
        <span class="s1">pdp</span><span class="s2">, </span><span class="s1">predictions = _partial_dependence_brute(</span>
            <span class="s1">est</span><span class="s2">, </span><span class="s1">grid</span><span class="s2">, </span><span class="s1">features</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">response_method=</span><span class="s5">&quot;auto&quot;</span>
        <span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">pdp = _partial_dependence_recursion(est</span><span class="s2">, </span><span class="s1">grid</span><span class="s2">, </span><span class="s1">features)</span>

    <span class="s1">mean_predictions = []</span>
    <span class="s2">for </span><span class="s1">val </span><span class="s2">in </span><span class="s1">(</span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">123</span><span class="s1">):</span>
        <span class="s1">X_ = X.copy()</span>
        <span class="s1">X_[:</span><span class="s2">, </span><span class="s1">target_feature] = val</span>
        <span class="s1">mean_predictions.append(est.predict(X_).mean())</span>

    <span class="s1">pdp = pdp[</span><span class="s4">0</span><span class="s1">]  </span><span class="s3"># (shape is (1, 2) so make it (2,))</span>

    <span class="s3"># allow for greater margin for error with recursion method</span>
    <span class="s1">rtol = </span><span class="s4">1e-1 </span><span class="s2">if </span><span class="s1">method == </span><span class="s5">&quot;recursion&quot; </span><span class="s2">else </span><span class="s4">1e-3</span>
    <span class="s2">assert </span><span class="s1">np.allclose(pdp</span><span class="s2">, </span><span class="s1">mean_predictions</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;seed&quot;</span><span class="s2">, </span><span class="s1">range(</span><span class="s4">1</span><span class="s1">))</span>
<span class="s2">def </span><span class="s1">test_recursion_decision_tree_vs_forest_and_gbdt(seed):</span>
    <span class="s3"># Make sure that the recursion method gives the same results on a</span>
    <span class="s3"># DecisionTreeRegressor and a GradientBoostingRegressor or a</span>
    <span class="s3"># RandomForestRegressor with 1 tree and equivalent parameters.</span>

    <span class="s1">rng = np.random.RandomState(seed)</span>

    <span class="s3"># Purely random dataset to avoid correlated features</span>
    <span class="s1">n_samples = </span><span class="s4">1000</span>
    <span class="s1">n_features = </span><span class="s4">5</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">y = rng.randn(n_samples) * </span><span class="s4">10</span>

    <span class="s3"># The 'init' estimator for GBDT (here the average prediction) isn't taken</span>
    <span class="s3"># into account with the recursion method, for technical reasons. We set</span>
    <span class="s3"># the mean to 0 to that this 'bug' doesn't have any effect.</span>
    <span class="s1">y = y - y.mean()</span>

    <span class="s3"># set max_depth not too high to avoid splits with same gain but different</span>
    <span class="s3"># features</span>
    <span class="s1">max_depth = </span><span class="s4">5</span>

    <span class="s1">tree_seed = </span><span class="s4">0</span>
    <span class="s1">forest = RandomForestRegressor(</span>
        <span class="s1">n_estimators=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">max_features=</span><span class="s2">None,</span>
        <span class="s1">bootstrap=</span><span class="s2">False,</span>
        <span class="s1">max_depth=max_depth</span><span class="s2">,</span>
        <span class="s1">random_state=tree_seed</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s3"># The forest will use ensemble.base._set_random_states to set the</span>
    <span class="s3"># random_state of the tree sub-estimator. We simulate this here to have</span>
    <span class="s3"># equivalent estimators.</span>
    <span class="s1">equiv_random_state = check_random_state(tree_seed).randint(np.iinfo(np.int32).max)</span>
    <span class="s1">gbdt = GradientBoostingRegressor(</span>
        <span class="s1">n_estimators=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">learning_rate=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">criterion=</span><span class="s5">&quot;squared_error&quot;</span><span class="s2">,</span>
        <span class="s1">max_depth=max_depth</span><span class="s2">,</span>
        <span class="s1">random_state=equiv_random_state</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">tree = DecisionTreeRegressor(max_depth=max_depth</span><span class="s2">, </span><span class="s1">random_state=equiv_random_state)</span>

    <span class="s1">forest.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">gbdt.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">tree.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s3"># sanity check: if the trees aren't the same, the PD values won't be equal</span>
    <span class="s2">try</span><span class="s1">:</span>
        <span class="s1">assert_is_subtree(tree.tree_</span><span class="s2">, </span><span class="s1">gbdt[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">].tree_)</span>
        <span class="s1">assert_is_subtree(tree.tree_</span><span class="s2">, </span><span class="s1">forest[</span><span class="s4">0</span><span class="s1">].tree_)</span>
    <span class="s2">except </span><span class="s1">AssertionError:</span>
        <span class="s3"># For some reason the trees aren't exactly equal on 32bits, so the PDs</span>
        <span class="s3"># cannot be equal either. See</span>
        <span class="s3"># https://github.com/scikit-learn/scikit-learn/issues/8853</span>
        <span class="s2">assert </span><span class="s1">_IS_32BIT</span><span class="s2">, </span><span class="s5">&quot;this should only fail on 32 bit platforms&quot;</span>
        <span class="s2">return</span>

    <span class="s1">grid = rng.randn(</span><span class="s4">50</span><span class="s1">).reshape(-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s2">for </span><span class="s1">f </span><span class="s2">in </span><span class="s1">range(n_features):</span>
        <span class="s1">features = np.array([f]</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span>

        <span class="s1">pdp_forest = _partial_dependence_recursion(forest</span><span class="s2">, </span><span class="s1">grid</span><span class="s2">, </span><span class="s1">features)</span>
        <span class="s1">pdp_gbdt = _partial_dependence_recursion(gbdt</span><span class="s2">, </span><span class="s1">grid</span><span class="s2">, </span><span class="s1">features)</span>
        <span class="s1">pdp_tree = _partial_dependence_recursion(tree</span><span class="s2">, </span><span class="s1">grid</span><span class="s2">, </span><span class="s1">features)</span>

        <span class="s1">np.testing.assert_allclose(pdp_gbdt</span><span class="s2">, </span><span class="s1">pdp_tree)</span>
        <span class="s1">np.testing.assert_allclose(pdp_forest</span><span class="s2">, </span><span class="s1">pdp_tree)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;est&quot;</span><span class="s2">,</span>
    <span class="s1">(</span>
        <span class="s1">GradientBoostingClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">HistGradientBoostingClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">)</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;target_feature&quot;</span><span class="s2">, </span><span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">5</span><span class="s1">))</span>
<span class="s2">def </span><span class="s1">test_recursion_decision_function(est</span><span class="s2">, </span><span class="s1">target_feature):</span>
    <span class="s3"># Make sure the recursion method (implicitly uses decision_function) has</span>
    <span class="s3"># the same result as using brute method with</span>
    <span class="s3"># response_method=decision_function</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_classes=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">n_clusters_per_class=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">np.mean(y) == </span><span class="s4">0.5  </span><span class="s3"># make sure the init estimator predicts 0 anyway</span>

    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">preds_1 = partial_dependence(</span>
        <span class="s1">est</span><span class="s2">,</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">[target_feature]</span><span class="s2">,</span>
        <span class="s1">response_method=</span><span class="s5">&quot;decision_function&quot;</span><span class="s2">,</span>
        <span class="s1">method=</span><span class="s5">&quot;recursion&quot;</span><span class="s2">,</span>
        <span class="s1">kind=</span><span class="s5">&quot;average&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">preds_2 = partial_dependence(</span>
        <span class="s1">est</span><span class="s2">,</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">[target_feature]</span><span class="s2">,</span>
        <span class="s1">response_method=</span><span class="s5">&quot;decision_function&quot;</span><span class="s2">,</span>
        <span class="s1">method=</span><span class="s5">&quot;brute&quot;</span><span class="s2">,</span>
        <span class="s1">kind=</span><span class="s5">&quot;average&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">assert_allclose(preds_1[</span><span class="s5">&quot;average&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">preds_2[</span><span class="s5">&quot;average&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-7</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;est&quot;</span><span class="s2">,</span>
    <span class="s1">(</span>
        <span class="s1">LinearRegression()</span><span class="s2">,</span>
        <span class="s1">GradientBoostingRegressor(random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">HistGradientBoostingRegressor(</span>
            <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">min_samples_leaf=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">max_leaf_nodes=</span><span class="s2">None, </span><span class="s1">max_iter=</span><span class="s4">1</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">DecisionTreeRegressor(random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">)</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;power&quot;</span><span class="s2">, </span><span class="s1">(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))</span>
<span class="s2">def </span><span class="s1">test_partial_dependence_easy_target(est</span><span class="s2">, </span><span class="s1">power):</span>
    <span class="s3"># If the target y only depends on one feature in an obvious way (linear or</span>
    <span class="s3"># quadratic) then the partial dependence for that feature should reflect</span>
    <span class="s3"># it.</span>
    <span class="s3"># We here fit a linear regression_data model (with polynomial features if</span>
    <span class="s3"># needed) and compute r_squared to check that the partial dependence</span>
    <span class="s3"># correctly reflects the target.</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s4">200</span>
    <span class="s1">target_variable = </span><span class="s4">2</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s2">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">y = X[:</span><span class="s2">, </span><span class="s1">target_variable] ** power</span>

    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">pdp = partial_dependence(</span>
        <span class="s1">est</span><span class="s2">, </span><span class="s1">features=[target_variable]</span><span class="s2">, </span><span class="s1">X=X</span><span class="s2">, </span><span class="s1">grid_resolution=</span><span class="s4">1000</span><span class="s2">, </span><span class="s1">kind=</span><span class="s5">&quot;average&quot;</span>
    <span class="s1">)</span>

    <span class="s1">new_X = pdp[</span><span class="s5">&quot;grid_values&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">].reshape(-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">new_y = pdp[</span><span class="s5">&quot;average&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s3"># add polynomial features if needed</span>
    <span class="s1">new_X = PolynomialFeatures(degree=power).fit_transform(new_X)</span>

    <span class="s1">lr = LinearRegression().fit(new_X</span><span class="s2">, </span><span class="s1">new_y)</span>
    <span class="s1">r2 = r2_score(new_y</span><span class="s2">, </span><span class="s1">lr.predict(new_X))</span>

    <span class="s2">assert </span><span class="s1">r2 &gt; </span><span class="s4">0.99</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;Estimator&quot;</span><span class="s2">,</span>
    <span class="s1">(</span>
        <span class="s1">sklearn.tree.DecisionTreeClassifier</span><span class="s2">,</span>
        <span class="s1">sklearn.tree.ExtraTreeClassifier</span><span class="s2">,</span>
        <span class="s1">sklearn.ensemble.ExtraTreesClassifier</span><span class="s2">,</span>
        <span class="s1">sklearn.neighbors.KNeighborsClassifier</span><span class="s2">,</span>
        <span class="s1">sklearn.neighbors.RadiusNeighborsClassifier</span><span class="s2">,</span>
        <span class="s1">sklearn.ensemble.RandomForestClassifier</span><span class="s2">,</span>
    <span class="s1">)</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_multiclass_multioutput(Estimator):</span>
    <span class="s3"># Make sure error is raised for multiclass-multioutput classifiers</span>

    <span class="s3"># make multiclass-multioutput dataset</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_classes=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">n_clusters_per_class=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">y = np.array([y</span><span class="s2">, </span><span class="s1">y]).T</span>

    <span class="s1">est = Estimator()</span>
    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;Multiclass-multioutput estimators are not supported&quot;</span>
    <span class="s1">):</span>
        <span class="s1">partial_dependence(est</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">])</span>


<span class="s2">class </span><span class="s1">NoPredictProbaNoDecisionFunction(ClassifierMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s3"># simulate that we have some classes</span>
        <span class="s1">self.classes_ = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>
        <span class="s2">return </span><span class="s1">self</span>


<span class="s1">@pytest.mark.filterwarnings(</span><span class="s5">&quot;ignore:A Bunch will be returned&quot;</span><span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;estimator, params, err_msg&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">KMeans(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">&quot;auto&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">{</span><span class="s5">&quot;features&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s1">]}</span><span class="s2">,</span>
            <span class="s5">&quot;'estimator' must be a fitted regressor or classifier&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">LinearRegression()</span><span class="s2">,</span>
            <span class="s1">{</span><span class="s5">&quot;features&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;response_method&quot;</span><span class="s1">: </span><span class="s5">&quot;predict_proba&quot;</span><span class="s1">}</span><span class="s2">,</span>
            <span class="s5">&quot;The response_method parameter is ignored for regressors&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">GradientBoostingClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">{</span>
                <span class="s5">&quot;features&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
                <span class="s5">&quot;response_method&quot;</span><span class="s1">: </span><span class="s5">&quot;predict_proba&quot;</span><span class="s2">,</span>
                <span class="s5">&quot;method&quot;</span><span class="s1">: </span><span class="s5">&quot;recursion&quot;</span><span class="s2">,</span>
            <span class="s1">}</span><span class="s2">,</span>
            <span class="s5">&quot;'recursion' method, the response_method must be 'decision_function'&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">GradientBoostingClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">{</span><span class="s5">&quot;features&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;response_method&quot;</span><span class="s1">: </span><span class="s5">&quot;predict_proba&quot;</span><span class="s2">, </span><span class="s5">&quot;method&quot;</span><span class="s1">: </span><span class="s5">&quot;auto&quot;</span><span class="s1">}</span><span class="s2">,</span>
            <span class="s5">&quot;'recursion' method, the response_method must be 'decision_function'&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">LinearRegression()</span><span class="s2">,</span>
            <span class="s1">{</span><span class="s5">&quot;features&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;method&quot;</span><span class="s1">: </span><span class="s5">&quot;recursion&quot;</span><span class="s2">, </span><span class="s5">&quot;kind&quot;</span><span class="s1">: </span><span class="s5">&quot;individual&quot;</span><span class="s1">}</span><span class="s2">,</span>
            <span class="s5">&quot;The 'recursion' method only applies when 'kind' is set to 'average'&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">LinearRegression()</span><span class="s2">,</span>
            <span class="s1">{</span><span class="s5">&quot;features&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;method&quot;</span><span class="s1">: </span><span class="s5">&quot;recursion&quot;</span><span class="s2">, </span><span class="s5">&quot;kind&quot;</span><span class="s1">: </span><span class="s5">&quot;both&quot;</span><span class="s1">}</span><span class="s2">,</span>
            <span class="s5">&quot;The 'recursion' method only applies when 'kind' is set to 'average'&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">LinearRegression()</span><span class="s2">,</span>
            <span class="s1">{</span><span class="s5">&quot;features&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;method&quot;</span><span class="s1">: </span><span class="s5">&quot;recursion&quot;</span><span class="s1">}</span><span class="s2">,</span>
            <span class="s5">&quot;Only the following estimators support the 'recursion' method:&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_partial_dependence_error(estimator</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">err_msg):</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">estimator.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">partial_dependence(estimator</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">**params)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;estimator&quot;</span><span class="s2">, </span><span class="s1">[LinearRegression()</span><span class="s2">, </span><span class="s1">GradientBoostingClassifier(random_state=</span><span class="s4">0</span><span class="s1">)]</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;features&quot;</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">10000</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_partial_dependence_unknown_feature_indices(estimator</span><span class="s2">, </span><span class="s1">features):</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">estimator.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">err_msg = </span><span class="s5">&quot;all features must be in&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">partial_dependence(estimator</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">[features])</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;estimator&quot;</span><span class="s2">, </span><span class="s1">[LinearRegression()</span><span class="s2">, </span><span class="s1">GradientBoostingClassifier(random_state=</span><span class="s4">0</span><span class="s1">)]</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_partial_dependence_unknown_feature_string(estimator):</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s5">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">df = pd.DataFrame(X)</span>
    <span class="s1">estimator.fit(df</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">features = [</span><span class="s5">&quot;random&quot;</span><span class="s1">]</span>
    <span class="s1">err_msg = </span><span class="s5">&quot;A given column is not a column of the dataframe&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">partial_dependence(estimator</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">features)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;estimator&quot;</span><span class="s2">, </span><span class="s1">[LinearRegression()</span><span class="s2">, </span><span class="s1">GradientBoostingClassifier(random_state=</span><span class="s4">0</span><span class="s1">)]</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_partial_dependence_X_list(estimator):</span>
    <span class="s3"># check that array-like objects are accepted</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">estimator.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">partial_dependence(estimator</span><span class="s2">, </span><span class="s1">list(X)</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">kind=</span><span class="s5">&quot;average&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_warning_recursion_non_constant_init():</span>
    <span class="s3"># make sure that passing a non-constant init parameter to a GBDT and using</span>
    <span class="s3"># recursion method yields a warning.</span>

    <span class="s1">gbc = GradientBoostingClassifier(init=DummyClassifier()</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">gbc.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">with </span><span class="s1">pytest.warns(</span>
        <span class="s1">UserWarning</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;Using recursion method with a non-constant init predictor&quot;</span>
    <span class="s1">):</span>
        <span class="s1">partial_dependence(gbc</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">method=</span><span class="s5">&quot;recursion&quot;</span><span class="s2">, </span><span class="s1">kind=</span><span class="s5">&quot;average&quot;</span><span class="s1">)</span>

    <span class="s2">with </span><span class="s1">pytest.warns(</span>
        <span class="s1">UserWarning</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;Using recursion method with a non-constant init predictor&quot;</span>
    <span class="s1">):</span>
        <span class="s1">partial_dependence(gbc</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">method=</span><span class="s5">&quot;recursion&quot;</span><span class="s2">, </span><span class="s1">kind=</span><span class="s5">&quot;average&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_partial_dependence_sample_weight_of_fitted_estimator():</span>
    <span class="s3"># Test near perfect correlation between partial dependence and diagonal</span>
    <span class="s3"># when sample weights emphasize y = x predictions</span>
    <span class="s3"># non-regression test for #13193</span>
    <span class="s3"># TODO: extend to HistGradientBoosting once sample_weight is supported</span>
    <span class="s1">N = </span><span class="s4">1000</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">123456</span><span class="s1">)</span>
    <span class="s1">mask = rng.randint(</span><span class="s4">2</span><span class="s2">, </span><span class="s1">size=N</span><span class="s2">, </span><span class="s1">dtype=bool)</span>

    <span class="s1">x = rng.rand(N)</span>
    <span class="s3"># set y = x on mask and y = -x outside</span>
    <span class="s1">y = x.copy()</span>
    <span class="s1">y[~mask] = -y[~mask]</span>
    <span class="s1">X = np.c_[mask</span><span class="s2">, </span><span class="s1">x]</span>
    <span class="s3"># sample weights to emphasize data points where y = x</span>
    <span class="s1">sample_weight = np.ones(N)</span>
    <span class="s1">sample_weight[mask] = </span><span class="s4">1000.0</span>

    <span class="s1">clf = GradientBoostingRegressor(n_estimators=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">pdp = partial_dependence(clf</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">features=[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">kind=</span><span class="s5">&quot;average&quot;</span><span class="s1">)</span>

    <span class="s2">assert </span><span class="s1">np.corrcoef(pdp[</span><span class="s5">&quot;average&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">pdp[</span><span class="s5">&quot;grid_values&quot;</span><span class="s1">])[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">] &gt; </span><span class="s4">0.99</span>


<span class="s2">def </span><span class="s1">test_hist_gbdt_sw_not_supported():</span>
    <span class="s3"># TODO: remove/fix when PDP supports HGBT with sample weights</span>
    <span class="s1">clf = HistGradientBoostingRegressor(random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=np.ones(len(X)))</span>

    <span class="s2">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">NotImplementedError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;does not support partial dependence&quot;</span>
    <span class="s1">):</span>
        <span class="s1">partial_dependence(clf</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">features=[</span><span class="s4">1</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_partial_dependence_pipeline():</span>
    <span class="s3"># check that the partial dependence support pipeline</span>
    <span class="s1">iris = load_iris()</span>

    <span class="s1">scaler = StandardScaler()</span>
    <span class="s1">clf = DummyClassifier(random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">pipe = make_pipeline(scaler</span><span class="s2">, </span><span class="s1">clf)</span>

    <span class="s1">clf.fit(scaler.fit_transform(iris.data)</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">pipe.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>

    <span class="s1">features = </span><span class="s4">0</span>
    <span class="s1">pdp_pipe = partial_dependence(</span>
        <span class="s1">pipe</span><span class="s2">, </span><span class="s1">iris.data</span><span class="s2">, </span><span class="s1">features=[features]</span><span class="s2">, </span><span class="s1">grid_resolution=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">kind=</span><span class="s5">&quot;average&quot;</span>
    <span class="s1">)</span>
    <span class="s1">pdp_clf = partial_dependence(</span>
        <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">scaler.transform(iris.data)</span><span class="s2">,</span>
        <span class="s1">features=[features]</span><span class="s2">,</span>
        <span class="s1">grid_resolution=</span><span class="s4">10</span><span class="s2">,</span>
        <span class="s1">kind=</span><span class="s5">&quot;average&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(pdp_pipe[</span><span class="s5">&quot;average&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">pdp_clf[</span><span class="s5">&quot;average&quot;</span><span class="s1">])</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">pdp_pipe[</span><span class="s5">&quot;grid_values&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">pdp_clf[</span><span class="s5">&quot;grid_values&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">] * scaler.scale_[features] + scaler.mean_[features]</span><span class="s2">,</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;estimator&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">LogisticRegression(max_iter=</span><span class="s4">1000</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">GradientBoostingClassifier(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">n_estimators=</span><span class="s4">5</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
    <span class="s1">ids=[</span><span class="s5">&quot;estimator-brute&quot;</span><span class="s2">, </span><span class="s5">&quot;estimator-recursion&quot;</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;preprocessor&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s2">None,</span>
        <span class="s1">make_column_transformer(</span>
            <span class="s1">(StandardScaler()</span><span class="s2">, </span><span class="s1">[iris.feature_names[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)])</span><span class="s2">,</span>
            <span class="s1">(RobustScaler()</span><span class="s2">, </span><span class="s1">[iris.feature_names[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">)])</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">make_column_transformer(</span>
            <span class="s1">(StandardScaler()</span><span class="s2">, </span><span class="s1">[iris.feature_names[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)])</span><span class="s2">,</span>
            <span class="s1">remainder=</span><span class="s5">&quot;passthrough&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
    <span class="s1">ids=[</span><span class="s5">&quot;None&quot;</span><span class="s2">, </span><span class="s5">&quot;column-transformer&quot;</span><span class="s2">, </span><span class="s5">&quot;column-transformer-passthrough&quot;</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;features&quot;</span><span class="s2">,</span>
    <span class="s1">[[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[iris.feature_names[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)]]</span><span class="s2">,</span>
    <span class="s1">ids=[</span><span class="s5">&quot;features-integer&quot;</span><span class="s2">, </span><span class="s5">&quot;features-string&quot;</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_partial_dependence_dataframe(estimator</span><span class="s2">, </span><span class="s1">preprocessor</span><span class="s2">, </span><span class="s1">features):</span>
    <span class="s3"># check that the partial dependence support dataframe and pipeline</span>
    <span class="s3"># including a column transformer</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s5">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">df = pd.DataFrame(scale(iris.data)</span><span class="s2">, </span><span class="s1">columns=iris.feature_names)</span>

    <span class="s1">pipe = make_pipeline(preprocessor</span><span class="s2">, </span><span class="s1">estimator)</span>
    <span class="s1">pipe.fit(df</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">pdp_pipe = partial_dependence(</span>
        <span class="s1">pipe</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">features=features</span><span class="s2">, </span><span class="s1">grid_resolution=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">kind=</span><span class="s5">&quot;average&quot;</span>
    <span class="s1">)</span>

    <span class="s3"># the column transformer will reorder the column when transforming</span>
    <span class="s3"># we mixed the index to be sure that we are computing the partial</span>
    <span class="s3"># dependence of the right columns</span>
    <span class="s2">if </span><span class="s1">preprocessor </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">X_proc = clone(preprocessor).fit_transform(df)</span>
        <span class="s1">features_clf = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">X_proc = df</span>
        <span class="s1">features_clf = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span>

    <span class="s1">clf = clone(estimator).fit(X_proc</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">pdp_clf = partial_dependence(</span>
        <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">X_proc</span><span class="s2">,</span>
        <span class="s1">features=features_clf</span><span class="s2">,</span>
        <span class="s1">method=</span><span class="s5">&quot;brute&quot;</span><span class="s2">,</span>
        <span class="s1">grid_resolution=</span><span class="s4">10</span><span class="s2">,</span>
        <span class="s1">kind=</span><span class="s5">&quot;average&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">assert_allclose(pdp_pipe[</span><span class="s5">&quot;average&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">pdp_clf[</span><span class="s5">&quot;average&quot;</span><span class="s1">])</span>
    <span class="s2">if </span><span class="s1">preprocessor </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">scaler = preprocessor.named_transformers_[</span><span class="s5">&quot;standardscaler&quot;</span><span class="s1">]</span>
        <span class="s1">assert_allclose(</span>
            <span class="s1">pdp_pipe[</span><span class="s5">&quot;grid_values&quot;</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">pdp_clf[</span><span class="s5">&quot;grid_values&quot;</span><span class="s1">][</span><span class="s4">1</span><span class="s1">] * scaler.scale_[</span><span class="s4">1</span><span class="s1">] + scaler.mean_[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">assert_allclose(pdp_pipe[</span><span class="s5">&quot;grid_values&quot;</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">pdp_clf[</span><span class="s5">&quot;grid_values&quot;</span><span class="s1">][</span><span class="s4">1</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;features, expected_pd_shape&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">(</span><span class="s4">3</span><span class="s2">, </span><span class="s4">10</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">(iris.feature_names[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">(</span><span class="s4">3</span><span class="s2">, </span><span class="s4">10</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">(</span><span class="s4">3</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">10</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">([iris.feature_names[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)]</span><span class="s2">, </span><span class="s1">(</span><span class="s4">3</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">10</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">([</span><span class="s2">True, False, True, False</span><span class="s1">]</span><span class="s2">, </span><span class="s1">(</span><span class="s4">3</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">10</span><span class="s1">))</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
    <span class="s1">ids=[</span><span class="s5">&quot;scalar-int&quot;</span><span class="s2">, </span><span class="s5">&quot;scalar-str&quot;</span><span class="s2">, </span><span class="s5">&quot;list-int&quot;</span><span class="s2">, </span><span class="s5">&quot;list-str&quot;</span><span class="s2">, </span><span class="s5">&quot;mask&quot;</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_partial_dependence_feature_type(features</span><span class="s2">, </span><span class="s1">expected_pd_shape):</span>
    <span class="s3"># check all possible features type supported in PDP</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s5">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">df = pd.DataFrame(iris.data</span><span class="s2">, </span><span class="s1">columns=iris.feature_names)</span>

    <span class="s1">preprocessor = make_column_transformer(</span>
        <span class="s1">(StandardScaler()</span><span class="s2">, </span><span class="s1">[iris.feature_names[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)])</span><span class="s2">,</span>
        <span class="s1">(RobustScaler()</span><span class="s2">, </span><span class="s1">[iris.feature_names[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">)])</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">pipe = make_pipeline(</span>
        <span class="s1">preprocessor</span><span class="s2">, </span><span class="s1">LogisticRegression(max_iter=</span><span class="s4">1000</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">)</span>
    <span class="s1">pipe.fit(df</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">pdp_pipe = partial_dependence(</span>
        <span class="s1">pipe</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">features=features</span><span class="s2">, </span><span class="s1">grid_resolution=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">kind=</span><span class="s5">&quot;average&quot;</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">pdp_pipe[</span><span class="s5">&quot;average&quot;</span><span class="s1">].shape == expected_pd_shape</span>
    <span class="s2">assert </span><span class="s1">len(pdp_pipe[</span><span class="s5">&quot;grid_values&quot;</span><span class="s1">]) == len(pdp_pipe[</span><span class="s5">&quot;average&quot;</span><span class="s1">].shape) - </span><span class="s4">1</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;estimator&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">LinearRegression()</span><span class="s2">,</span>
        <span class="s1">LogisticRegression()</span><span class="s2">,</span>
        <span class="s1">GradientBoostingRegressor()</span><span class="s2">,</span>
        <span class="s1">GradientBoostingClassifier()</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_partial_dependence_unfitted(estimator):</span>
    <span class="s1">X = iris.data</span>
    <span class="s1">preprocessor = make_column_transformer(</span>
        <span class="s1">(StandardScaler()</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span><span class="s2">, </span><span class="s1">(RobustScaler()</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span>
    <span class="s1">)</span>
    <span class="s1">pipe = make_pipeline(preprocessor</span><span class="s2">, </span><span class="s1">estimator)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;is not fitted yet&quot;</span><span class="s1">):</span>
        <span class="s1">partial_dependence(pipe</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">features=[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">grid_resolution=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;is not fitted yet&quot;</span><span class="s1">):</span>
        <span class="s1">partial_dependence(estimator</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">features=[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">grid_resolution=</span><span class="s4">10</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;Estimator, data&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(LinearRegression</span><span class="s2">, </span><span class="s1">multioutput_regression_data)</span><span class="s2">,</span>
        <span class="s1">(LogisticRegression</span><span class="s2">, </span><span class="s1">binary_classification_data)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_kind_average_and_average_of_individual(Estimator</span><span class="s2">, </span><span class="s1">data):</span>
    <span class="s1">est = Estimator()</span>
    <span class="s1">(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">, </span><span class="s1">n_targets = data</span>
    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">pdp_avg = partial_dependence(est</span><span class="s2">, </span><span class="s1">X=X</span><span class="s2">, </span><span class="s1">features=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">kind=</span><span class="s5">&quot;average&quot;</span><span class="s1">)</span>
    <span class="s1">pdp_ind = partial_dependence(est</span><span class="s2">, </span><span class="s1">X=X</span><span class="s2">, </span><span class="s1">features=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">kind=</span><span class="s5">&quot;individual&quot;</span><span class="s1">)</span>
    <span class="s1">avg_ind = np.mean(pdp_ind[</span><span class="s5">&quot;individual&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">assert_allclose(avg_ind</span><span class="s2">, </span><span class="s1">pdp_avg[</span><span class="s5">&quot;average&quot;</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;Estimator, data&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(LinearRegression</span><span class="s2">, </span><span class="s1">multioutput_regression_data)</span><span class="s2">,</span>
        <span class="s1">(LogisticRegression</span><span class="s2">, </span><span class="s1">binary_classification_data)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_partial_dependence_kind_individual_ignores_sample_weight(Estimator</span><span class="s2">, </span><span class="s1">data):</span>
    <span class="s0">&quot;&quot;&quot;Check that `sample_weight` does not have any effect on reported ICE.&quot;&quot;&quot;</span>
    <span class="s1">est = Estimator()</span>
    <span class="s1">(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">, </span><span class="s1">n_targets = data</span>
    <span class="s1">sample_weight = np.arange(X.shape[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">pdp_nsw = partial_dependence(est</span><span class="s2">, </span><span class="s1">X=X</span><span class="s2">, </span><span class="s1">features=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">kind=</span><span class="s5">&quot;individual&quot;</span><span class="s1">)</span>
    <span class="s1">pdp_sw = partial_dependence(</span>
        <span class="s1">est</span><span class="s2">, </span><span class="s1">X=X</span><span class="s2">, </span><span class="s1">features=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">kind=</span><span class="s5">&quot;individual&quot;</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(pdp_nsw[</span><span class="s5">&quot;individual&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">pdp_sw[</span><span class="s5">&quot;individual&quot;</span><span class="s1">])</span>
    <span class="s1">assert_allclose(pdp_nsw[</span><span class="s5">&quot;grid_values&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">pdp_sw[</span><span class="s5">&quot;grid_values&quot;</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;estimator&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">LinearRegression()</span><span class="s2">,</span>
        <span class="s1">LogisticRegression()</span><span class="s2">,</span>
        <span class="s1">RandomForestRegressor()</span><span class="s2">,</span>
        <span class="s1">GradientBoostingClassifier()</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;non_null_weight_idx&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_partial_dependence_non_null_weight_idx(estimator</span><span class="s2">, </span><span class="s1">non_null_weight_idx):</span>
    <span class="s0">&quot;&quot;&quot;Check that if we pass a `sample_weight` of zeros with only one index with 
    sample weight equals one, then the average `partial_dependence` with this 
    `sample_weight` is equal to the individual `partial_dependence` of the 
    corresponding index. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = iris.data</span><span class="s2">, </span><span class="s1">iris.target</span>
    <span class="s1">preprocessor = make_column_transformer(</span>
        <span class="s1">(StandardScaler()</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span><span class="s2">, </span><span class="s1">(RobustScaler()</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span>
    <span class="s1">)</span>
    <span class="s1">pipe = make_pipeline(preprocessor</span><span class="s2">, </span><span class="s1">estimator).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">sample_weight = np.zeros_like(y)</span>
    <span class="s1">sample_weight[non_null_weight_idx] = </span><span class="s4">1</span>
    <span class="s1">pdp_sw = partial_dependence(</span>
        <span class="s1">pipe</span><span class="s2">,</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">kind=</span><span class="s5">&quot;average&quot;</span><span class="s2">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s2">,</span>
        <span class="s1">grid_resolution=</span><span class="s4">10</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">pdp_ind = partial_dependence(pipe</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">kind=</span><span class="s5">&quot;individual&quot;</span><span class="s2">, </span><span class="s1">grid_resolution=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">output_dim = </span><span class="s4">1 </span><span class="s2">if </span><span class="s1">is_regressor(pipe) </span><span class="s2">else </span><span class="s1">len(np.unique(y))</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(output_dim):</span>
        <span class="s1">assert_allclose(</span>
            <span class="s1">pdp_ind[</span><span class="s5">&quot;individual&quot;</span><span class="s1">][i][non_null_weight_idx]</span><span class="s2">,</span>
            <span class="s1">pdp_sw[</span><span class="s5">&quot;average&quot;</span><span class="s1">][i]</span><span class="s2">,</span>
        <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;Estimator, data&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(LinearRegression</span><span class="s2">, </span><span class="s1">multioutput_regression_data)</span><span class="s2">,</span>
        <span class="s1">(LogisticRegression</span><span class="s2">, </span><span class="s1">binary_classification_data)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_partial_dependence_equivalence_equal_sample_weight(Estimator</span><span class="s2">, </span><span class="s1">data):</span>
    <span class="s0">&quot;&quot;&quot;Check that `sample_weight=None` is equivalent to having equal weights.&quot;&quot;&quot;</span>

    <span class="s1">est = Estimator()</span>
    <span class="s1">(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">, </span><span class="s1">n_targets = data</span>
    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">params = </span><span class="s2">None, </span><span class="s1">{</span><span class="s5">&quot;X&quot;</span><span class="s1">: X</span><span class="s2">, </span><span class="s5">&quot;features&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;kind&quot;</span><span class="s1">: </span><span class="s5">&quot;average&quot;</span><span class="s1">}</span>
    <span class="s1">pdp_sw_none = partial_dependence(est</span><span class="s2">, </span><span class="s1">**params</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">sample_weight = np.ones(len(y))</span>
    <span class="s1">pdp_sw_unit = partial_dependence(est</span><span class="s2">, </span><span class="s1">**params</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_allclose(pdp_sw_none[</span><span class="s5">&quot;average&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">pdp_sw_unit[</span><span class="s5">&quot;average&quot;</span><span class="s1">])</span>
    <span class="s1">sample_weight = </span><span class="s4">2 </span><span class="s1">* np.ones(len(y))</span>
    <span class="s1">pdp_sw_doubling = partial_dependence(est</span><span class="s2">, </span><span class="s1">**params</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_allclose(pdp_sw_none[</span><span class="s5">&quot;average&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">pdp_sw_doubling[</span><span class="s5">&quot;average&quot;</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_partial_dependence_sample_weight_size_error():</span>
    <span class="s0">&quot;&quot;&quot;Check that we raise an error when the size of `sample_weight` is not 
    consistent with `X` and `y`. 
    &quot;&quot;&quot;</span>
    <span class="s1">est = LogisticRegression()</span>
    <span class="s1">(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">, </span><span class="s1">n_targets = binary_classification_data</span>
    <span class="s1">sample_weight = np.ones_like(y)</span>
    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;sample_weight.shape ==&quot;</span><span class="s1">):</span>
        <span class="s1">partial_dependence(</span>
            <span class="s1">est</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">features=[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight[</span><span class="s4">1</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">grid_resolution=</span><span class="s4">10</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_partial_dependence_sample_weight_with_recursion():</span>
    <span class="s0">&quot;&quot;&quot;Check that we raise an error when `sample_weight` is provided with 
    `&quot;recursion&quot;` method. 
    &quot;&quot;&quot;</span>
    <span class="s1">est = RandomForestRegressor()</span>
    <span class="s1">(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">, </span><span class="s1">n_targets = regression_data</span>
    <span class="s1">sample_weight = np.ones_like(y)</span>
    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;'recursion' method can only be applied when&quot;</span><span class="s1">):</span>
        <span class="s1">partial_dependence(</span>
            <span class="s1">est</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">features=[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">method=</span><span class="s5">&quot;recursion&quot;</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight</span>
        <span class="s1">)</span>


<span class="s3"># TODO(1.5): Remove when bunch values is deprecated in 1.5</span>
<span class="s2">def </span><span class="s1">test_partial_dependence_bunch_values_deprecated():</span>
    <span class="s0">&quot;&quot;&quot;Test that deprecation warning is raised when values is accessed.&quot;&quot;&quot;</span>

    <span class="s1">est = LogisticRegression()</span>
    <span class="s1">(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">, </span><span class="s1">_ = binary_classification_data</span>
    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">pdp_avg = partial_dependence(est</span><span class="s2">, </span><span class="s1">X=X</span><span class="s2">, </span><span class="s1">features=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">kind=</span><span class="s5">&quot;average&quot;</span><span class="s1">)</span>

    <span class="s1">msg = (</span>
        <span class="s5">&quot;Key: 'values', is deprecated in 1.3 and will be &quot;</span>
        <span class="s5">&quot;removed in 1.5. Please use 'grid_values' instead&quot;</span>
    <span class="s1">)</span>

    <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s3"># Does not raise warnings with &quot;grid_values&quot;</span>
        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;error&quot;</span><span class="s2">, </span><span class="s1">FutureWarning)</span>
        <span class="s1">grid_values = pdp_avg[</span><span class="s5">&quot;grid_values&quot;</span><span class="s1">]</span>

    <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s3"># Warns for &quot;values&quot;</span>
        <span class="s1">values = pdp_avg[</span><span class="s5">&quot;values&quot;</span><span class="s1">]</span>

    <span class="s3"># &quot;values&quot; and &quot;grid_values&quot; are the same object</span>
    <span class="s2">assert </span><span class="s1">values </span><span class="s2">is </span><span class="s1">grid_values</span>


<span class="s2">def </span><span class="s1">test_mixed_type_categorical():</span>
    <span class="s0">&quot;&quot;&quot;Check that we raise a proper error when a column has mixed types and 
    the sorting of `np.unique` will fail.&quot;&quot;&quot;</span>
    <span class="s1">X = np.array([</span><span class="s5">&quot;A&quot;</span><span class="s2">, </span><span class="s5">&quot;B&quot;</span><span class="s2">, </span><span class="s5">&quot;C&quot;</span><span class="s2">, </span><span class="s1">np.nan]</span><span class="s2">, </span><span class="s1">dtype=object).reshape(-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>

    <span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">OrdinalEncoder</span>

    <span class="s1">clf = make_pipeline(</span>
        <span class="s1">OrdinalEncoder(encoded_missing_value=-</span><span class="s4">1</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">LogisticRegression()</span><span class="s2">,</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;The column #0 contains mixed data types&quot;</span><span class="s1">):</span>
        <span class="s1">partial_dependence(clf</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">features=[</span><span class="s4">0</span><span class="s1">])</span>
</pre>
</body>
</html>