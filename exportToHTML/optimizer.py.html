<html>
<head>
<title>optimizer.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
optimizer.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Functions that are general enough to use for any model fitting. The idea is 
to untie these from LikelihoodModel so that they may be re-used generally. 
&quot;&quot;&quot;</span>
<span class="s2">from </span><span class="s1">__future__ </span><span class="s2">import </span><span class="s1">annotations</span>

<span class="s2">from </span><span class="s1">typing </span><span class="s2">import </span><span class="s1">Any</span><span class="s2">, </span><span class="s1">Sequence</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">optimize</span>
<span class="s2">from </span><span class="s1">statsmodels.compat.scipy </span><span class="s2">import </span><span class="s1">SP_LT_15</span><span class="s2">, </span><span class="s1">SP_LT_17</span>


<span class="s2">def </span><span class="s1">check_kwargs(kwargs: dict[str</span><span class="s2">, </span><span class="s1">Any]</span><span class="s2">, </span><span class="s1">allowed: Sequence[str]</span><span class="s2">, </span><span class="s1">method: str):</span>
    <span class="s1">extra = set(list(kwargs.keys())).difference(list(allowed))</span>
    <span class="s2">if </span><span class="s1">extra:</span>
        <span class="s2">import </span><span class="s1">warnings</span>

        <span class="s1">warnings.warn(</span>
            <span class="s3">&quot;Keyword arguments have been passed to the optimizer that have &quot;</span>
            <span class="s3">&quot;no effect. The list of allowed keyword arguments for method &quot;</span>
            <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">method</span><span class="s2">} </span><span class="s3">is: </span><span class="s2">{</span><span class="s3">', '</span><span class="s1">.join(allowed)</span><span class="s2">}</span><span class="s3">. The list of unsupported &quot;</span>
            <span class="s3">f&quot;keyword arguments passed include: </span><span class="s2">{</span><span class="s3">', '</span><span class="s1">.join(extra)</span><span class="s2">}</span><span class="s3">. After &quot;</span>
            <span class="s3">&quot;release 0.14, this will raise.&quot;</span><span class="s2">,</span>
            <span class="s1">FutureWarning</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">_check_method(method</span><span class="s2">, </span><span class="s1">methods):</span>
    <span class="s2">if </span><span class="s1">method </span><span class="s2">not in </span><span class="s1">methods:</span>
        <span class="s1">message = </span><span class="s3">&quot;Unknown fit method %s&quot; </span><span class="s1">% method</span>
        <span class="s2">raise </span><span class="s1">ValueError(message)</span>


<span class="s2">class </span><span class="s1">Optimizer:</span>
    <span class="s2">def </span><span class="s1">_fit(self</span><span class="s2">, </span><span class="s1">objective</span><span class="s2">, </span><span class="s1">gradient</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">fargs</span><span class="s2">, </span><span class="s1">kwargs</span><span class="s2">,</span>
             <span class="s1">hessian=</span><span class="s2">None, </span><span class="s1">method=</span><span class="s3">'newton'</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">full_output=</span><span class="s2">True,</span>
             <span class="s1">disp=</span><span class="s2">True, </span><span class="s1">callback=</span><span class="s2">None, </span><span class="s1">retall=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fit function for any model with an objective function. 
 
        Parameters 
        ---------- 
        objective : function 
            Objective function to be minimized. 
        gradient : function 
            The gradient of the objective function. 
        start_params : array_like, optional 
            Initial guess of the solution for the loglikelihood maximization. 
            The default is an array of zeros. 
        fargs : tuple 
            Extra arguments passed to the objective function, i.e. 
            objective(x,*args) 
        kwargs : dict[str, Any] 
            Extra keyword arguments passed to the objective function, i.e. 
            objective(x,**kwargs) 
        hessian : str, optional 
            Method for computing the Hessian matrix, if applicable. 
        method : str {'newton','nm','bfgs','powell','cg','ncg','basinhopping', 
            'minimize'} 
            Method can be 'newton' for Newton-Raphson, 'nm' for Nelder-Mead, 
            'bfgs' for Broyden-Fletcher-Goldfarb-Shanno, 'powell' for modified 
            Powell's method, 'cg' for conjugate gradient, 'ncg' for Newton- 
            conjugate gradient, 'basinhopping' for global basin-hopping 
            solver, if available or a generic 'minimize' which is a wrapper for 
            scipy.optimize.minimize. `method` determines which solver from 
            scipy.optimize is used. The explicit arguments in `fit` are passed 
            to the solver, with the exception of the basin-hopping solver. Each 
            solver has several optional arguments that are not the same across 
            solvers. See the notes section below (or scipy.optimize) for the 
            available arguments and for the list of explicit arguments that the 
            basin-hopping solver supports.. 
        maxiter : int 
            The maximum number of iterations to perform. 
        full_output : bool 
            Set to True to have all available output in the Results object's 
            mle_retvals attribute. The output is dependent on the solver. 
            See LikelihoodModelResults notes section for more information. 
        disp : bool 
            Set to True to print convergence messages. 
        callback : callable callback(xk) 
            Called after each iteration, as callback(xk), where xk is the 
            current parameter vector. 
        retall : bool 
            Set to True to return list of solutions at each iteration. 
            Available in Results object's mle_retvals attribute. 
 
        Returns 
        ------- 
        xopt : ndarray 
            The solution to the objective function 
        retvals : dict, None 
            If `full_output` is True then this is a dictionary which holds 
            information returned from the solver used. If it is False, this is 
            None. 
        optim_settings : dict 
            A dictionary that contains the parameters passed to the solver. 
 
        Notes 
        ----- 
        The 'basinhopping' solver ignores `maxiter`, `retall`, `full_output` 
        explicit arguments. 
 
        Optional arguments for the solvers (available in Results.mle_settings):: 
 
            'newton' 
                tol : float 
                    Relative error in params acceptable for convergence. 
            'nm' -- Nelder Mead 
                xtol : float 
                    Relative error in params acceptable for convergence 
                ftol : float 
                    Relative error in loglike(params) acceptable for 
                    convergence 
                maxfun : int 
                    Maximum number of function evaluations to make. 
            'bfgs' 
                gtol : float 
                    Stop when norm of gradient is less than gtol. 
                norm : float 
                    Order of norm (np.Inf is max, -np.Inf is min) 
                epsilon 
                    If fprime is approximated, use this value for the step 
                    size. Only relevant if LikelihoodModel.score is None. 
            'lbfgs' 
                m : int 
                    The maximum number of variable metric corrections used to 
                    define the limited memory matrix. (The limited memory BFGS 
                    method does not store the full hessian but uses this many 
                    terms in an approximation to it.) 
                pgtol : float 
                    The iteration will stop when 
                    ``max{|proj g_i | i = 1, ..., n} &lt;= pgtol`` where pg_i is 
                    the i-th component of the projected gradient. 
                factr : float 
                    The iteration stops when 
                    ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} &lt;= factr * eps``, 
                    where eps is the machine precision, which is automatically 
                    generated by the code. Typical values for factr are: 1e12 
                    for low accuracy; 1e7 for moderate accuracy; 10.0 for 
                    extremely high accuracy. See Notes for relationship to 
                    ftol, which is exposed (instead of factr) by the 
                    scipy.optimize.minimize interface to L-BFGS-B. 
                maxfun : int 
                    Maximum number of iterations. 
                epsilon : float 
                    Step size used when approx_grad is True, for numerically 
                    calculating the gradient 
                approx_grad : bool 
                    Whether to approximate the gradient numerically (in which 
                    case func returns only the function value). 
            'cg' 
                gtol : float 
                    Stop when norm of gradient is less than gtol. 
                norm : float 
                    Order of norm (np.Inf is max, -np.Inf is min) 
                epsilon : float 
                    If fprime is approximated, use this value for the step 
                    size. Can be scalar or vector.  Only relevant if 
                    Likelihoodmodel.score is None. 
            'ncg' 
                fhess_p : callable f'(x,*args) 
                    Function which computes the Hessian of f times an arbitrary 
                    vector, p.  Should only be supplied if 
                    LikelihoodModel.hessian is None. 
                avextol : float 
                    Stop when the average relative error in the minimizer 
                    falls below this amount. 
                epsilon : float or ndarray 
                    If fhess is approximated, use this value for the step size. 
                    Only relevant if Likelihoodmodel.hessian is None. 
            'powell' 
                xtol : float 
                    Line-search error tolerance 
                ftol : float 
                    Relative error in loglike(params) for acceptable for 
                    convergence. 
                maxfun : int 
                    Maximum number of function evaluations to make. 
                start_direc : ndarray 
                    Initial direction set. 
            'basinhopping' 
                niter : int 
                    The number of basin hopping iterations. 
                niter_success : int 
                    Stop the run if the global minimum candidate remains the 
                    same for this number of iterations. 
                T : float 
                    The &quot;temperature&quot; parameter for the accept or reject 
                    criterion. Higher &quot;temperatures&quot; mean that larger jumps 
                    in function value will be accepted. For best results 
                    `T` should be comparable to the separation (in function 
                    value) between local minima. 
                stepsize : float 
                    Initial step size for use in the random displacement. 
                interval : int 
                    The interval for how often to update the `stepsize`. 
                minimizer : dict 
                    Extra keyword arguments to be passed to the minimizer 
                    `scipy.optimize.minimize()`, for example 'method' - the 
                    minimization method (e.g. 'L-BFGS-B'), or 'tol' - the 
                    tolerance for termination. Other arguments are mapped from 
                    explicit argument of `fit`: 
                    - `args` &lt;- `fargs` 
                    - `jac` &lt;- `score` 
                    - `hess` &lt;- `hess` 
            'minimize' 
                min_method : str, optional 
                    Name of minimization method to use. 
                    Any method specific arguments can be passed directly. 
                    For a list of methods and their arguments, see 
                    documentation of `scipy.optimize.minimize`. 
                    If no method is specified, then BFGS is used. 
        &quot;&quot;&quot;</span>
        <span class="s5"># TODO: generalize the regularization stuff</span>
        <span class="s5"># Extract kwargs specific to fit_regularized calling fit</span>
        <span class="s1">extra_fit_funcs = kwargs.get(</span><span class="s3">'extra_fit_funcs'</span><span class="s2">, </span><span class="s1">dict())</span>

        <span class="s1">methods = [</span><span class="s3">'newton'</span><span class="s2">, </span><span class="s3">'nm'</span><span class="s2">, </span><span class="s3">'bfgs'</span><span class="s2">, </span><span class="s3">'lbfgs'</span><span class="s2">, </span><span class="s3">'powell'</span><span class="s2">, </span><span class="s3">'cg'</span><span class="s2">, </span><span class="s3">'ncg'</span><span class="s2">,</span>
                   <span class="s3">'basinhopping'</span><span class="s2">, </span><span class="s3">'minimize'</span><span class="s1">]</span>
        <span class="s1">methods += extra_fit_funcs.keys()</span>
        <span class="s1">method = method.lower()</span>
        <span class="s1">_check_method(method</span><span class="s2">, </span><span class="s1">methods)</span>

        <span class="s1">fit_funcs = {</span>
            <span class="s3">'newton'</span><span class="s1">: _fit_newton</span><span class="s2">,</span>
            <span class="s3">'nm'</span><span class="s1">: _fit_nm</span><span class="s2">,  </span><span class="s5"># Nelder-Mead</span>
            <span class="s3">'bfgs'</span><span class="s1">: _fit_bfgs</span><span class="s2">,</span>
            <span class="s3">'lbfgs'</span><span class="s1">: _fit_lbfgs</span><span class="s2">,</span>
            <span class="s3">'cg'</span><span class="s1">: _fit_cg</span><span class="s2">,</span>
            <span class="s3">'ncg'</span><span class="s1">: _fit_ncg</span><span class="s2">,</span>
            <span class="s3">'powell'</span><span class="s1">: _fit_powell</span><span class="s2">,</span>
            <span class="s3">'basinhopping'</span><span class="s1">: _fit_basinhopping</span><span class="s2">,</span>
            <span class="s3">'minimize'</span><span class="s1">: _fit_minimize  </span><span class="s5"># wrapper for scipy.optimize.minimize</span>
        <span class="s1">}</span>

        <span class="s5"># NOTE: fit_regularized checks the methods for these but it should be</span>
        <span class="s5">#      moved up probably</span>
        <span class="s2">if </span><span class="s1">extra_fit_funcs:</span>
            <span class="s1">fit_funcs.update(extra_fit_funcs)</span>

        <span class="s1">func = fit_funcs[method]</span>
        <span class="s1">xopt</span><span class="s2">, </span><span class="s1">retvals = func(objective</span><span class="s2">, </span><span class="s1">gradient</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">fargs</span><span class="s2">, </span><span class="s1">kwargs</span><span class="s2">,</span>
                             <span class="s1">disp=disp</span><span class="s2">, </span><span class="s1">maxiter=maxiter</span><span class="s2">, </span><span class="s1">callback=callback</span><span class="s2">,</span>
                             <span class="s1">retall=retall</span><span class="s2">, </span><span class="s1">full_output=full_output</span><span class="s2">,</span>
                             <span class="s1">hess=hessian)</span>

        <span class="s1">optim_settings = {</span><span class="s3">'optimizer'</span><span class="s1">: method</span><span class="s2">, </span><span class="s3">'start_params'</span><span class="s1">: start_params</span><span class="s2">,</span>
                          <span class="s3">'maxiter'</span><span class="s1">: maxiter</span><span class="s2">, </span><span class="s3">'full_output'</span><span class="s1">: full_output</span><span class="s2">,</span>
                          <span class="s3">'disp'</span><span class="s1">: disp</span><span class="s2">, </span><span class="s3">'fargs'</span><span class="s1">: fargs</span><span class="s2">, </span><span class="s3">'callback'</span><span class="s1">: callback</span><span class="s2">,</span>
                          <span class="s3">'retall'</span><span class="s1">: retall</span><span class="s2">, </span><span class="s3">&quot;extra_fit_funcs&quot;</span><span class="s1">: extra_fit_funcs}</span>
        <span class="s1">optim_settings.update(kwargs)</span>
        <span class="s5"># set as attributes or return?</span>
        <span class="s2">return </span><span class="s1">xopt</span><span class="s2">, </span><span class="s1">retvals</span><span class="s2">, </span><span class="s1">optim_settings</span>

    <span class="s2">def </span><span class="s1">_fit_constrained(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        TODO: how to add constraints? 
 
        Something like 
        sm.add_constraint(Model, func) 
 
        or 
 
        model_instance.add_constraint(func) 
        model_instance.add_constraint(&quot;x1 + x2 = 2&quot;) 
        result = model_instance.fit() 
        &quot;&quot;&quot;</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError</span>

    <span class="s2">def </span><span class="s1">_fit_regularized(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s5"># TODO: code will not necessarily be general here. 3 options.</span>
        <span class="s5"># 1) setup for scipy.optimize.fmin_sqlsqp</span>
        <span class="s5"># 2) setup for cvxopt</span>
        <span class="s5"># 3) setup for openopt</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError</span>


<span class="s5">########################################</span>
<span class="s5"># Helper functions to fit</span>


<span class="s2">def </span><span class="s1">_fit_minimize(f</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">fargs</span><span class="s2">, </span><span class="s1">kwargs</span><span class="s2">, </span><span class="s1">disp=</span><span class="s2">True,</span>
                  <span class="s1">maxiter=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">callback=</span><span class="s2">None, </span><span class="s1">retall=</span><span class="s2">False,</span>
                  <span class="s1">full_output=</span><span class="s2">True, </span><span class="s1">hess=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Fit using scipy minimize, where kwarg `min_method` defines the algorithm. 
 
    Parameters 
    ---------- 
    f : function 
        Returns negative log likelihood given parameters. 
    score : function 
        Returns gradient of negative log likelihood with respect to params. 
    start_params : array_like, optional 
        Initial guess of the solution for the loglikelihood maximization. 
        The default is an array of zeros. 
    fargs : tuple 
        Extra arguments passed to the objective function, i.e. 
        objective(x,*args) 
    kwargs : dict[str, Any] 
        Extra keyword arguments passed to the objective function, i.e. 
        objective(x,**kwargs) 
    disp : bool 
        Set to True to print convergence messages. 
    maxiter : int 
        The maximum number of iterations to perform. 
    callback : callable callback(xk) 
        Called after each iteration, as callback(xk), where xk is the 
        current parameter vector. 
    retall : bool 
        Set to True to return list of solutions at each iteration. 
        Available in Results object's mle_retvals attribute. 
    full_output : bool 
        Set to True to have all available output in the Results object's 
        mle_retvals attribute. The output is dependent on the solver. 
        See LikelihoodModelResults notes section for more information. 
    hess : str, optional 
        Method for computing the Hessian matrix, if applicable. 
 
    Returns 
    ------- 
    xopt : ndarray 
        The solution to the objective function 
    retvals : dict, None 
        If `full_output` is True then this is a dictionary which holds 
        information returned from the solver used. If it is False, this is 
        None. 
    &quot;&quot;&quot;</span>
    <span class="s1">kwargs.setdefault(</span><span class="s3">'min_method'</span><span class="s2">, </span><span class="s3">'BFGS'</span><span class="s1">)</span>

    <span class="s5"># prepare options dict for minimize</span>
    <span class="s1">filter_opts = [</span><span class="s3">'extra_fit_funcs'</span><span class="s2">, </span><span class="s3">'niter'</span><span class="s2">, </span><span class="s3">'min_method'</span><span class="s2">, </span><span class="s3">'tol'</span><span class="s2">, </span><span class="s3">'bounds'</span><span class="s2">, </span><span class="s3">'constraints'</span><span class="s1">]</span>
    <span class="s1">options = {k: v </span><span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">v </span><span class="s2">in </span><span class="s1">kwargs.items() </span><span class="s2">if </span><span class="s1">k </span><span class="s2">not in </span><span class="s1">filter_opts}</span>
    <span class="s1">options[</span><span class="s3">'disp'</span><span class="s1">] = disp</span>
    <span class="s1">options[</span><span class="s3">'maxiter'</span><span class="s1">] = maxiter</span>

    <span class="s5"># Use Hessian/Jacobian only if they're required by the method</span>
    <span class="s1">no_hess = [</span><span class="s3">'Nelder-Mead'</span><span class="s2">, </span><span class="s3">'Powell'</span><span class="s2">, </span><span class="s3">'CG'</span><span class="s2">, </span><span class="s3">'BFGS'</span><span class="s2">, </span><span class="s3">'COBYLA'</span><span class="s2">, </span><span class="s3">'SLSQP'</span><span class="s1">]</span>
    <span class="s1">no_jac = [</span><span class="s3">'Nelder-Mead'</span><span class="s2">, </span><span class="s3">'Powell'</span><span class="s2">, </span><span class="s3">'COBYLA'</span><span class="s1">]</span>
    <span class="s2">if </span><span class="s1">kwargs[</span><span class="s3">'min_method'</span><span class="s1">] </span><span class="s2">in </span><span class="s1">no_hess:</span>
        <span class="s1">hess = </span><span class="s2">None</span>
    <span class="s2">if </span><span class="s1">kwargs[</span><span class="s3">'min_method'</span><span class="s1">] </span><span class="s2">in </span><span class="s1">no_jac:</span>
        <span class="s1">score = </span><span class="s2">None</span>

    <span class="s5"># Use bounds/constraints only if they're allowed by the method</span>
    <span class="s1">has_bounds = [</span><span class="s3">'L-BFGS-B'</span><span class="s2">, </span><span class="s3">'TNC'</span><span class="s2">, </span><span class="s3">'SLSQP'</span><span class="s2">, </span><span class="s3">'trust-constr'</span><span class="s1">]</span>
    <span class="s5"># Added in SP 1.5</span>
    <span class="s2">if not </span><span class="s1">SP_LT_15:</span>
        <span class="s1">has_bounds += [</span><span class="s3">'Powell'</span><span class="s1">]</span>
    <span class="s5"># Added in SP 1.7</span>
    <span class="s2">if not </span><span class="s1">SP_LT_17:</span>
        <span class="s1">has_bounds += [</span><span class="s3">'Nelder-Mead'</span><span class="s1">]</span>
    <span class="s1">has_constraints = [</span><span class="s3">'COBYLA'</span><span class="s2">, </span><span class="s3">'SLSQP'</span><span class="s2">, </span><span class="s3">'trust-constr'</span><span class="s1">]</span>

    <span class="s2">if </span><span class="s3">'bounds' </span><span class="s2">in </span><span class="s1">kwargs.keys() </span><span class="s2">and </span><span class="s1">kwargs[</span><span class="s3">'min_method'</span><span class="s1">] </span><span class="s2">in </span><span class="s1">has_bounds:</span>
        <span class="s1">bounds = kwargs[</span><span class="s3">'bounds'</span><span class="s1">]</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">bounds = </span><span class="s2">None</span>

    <span class="s2">if </span><span class="s3">'constraints' </span><span class="s2">in </span><span class="s1">kwargs.keys() </span><span class="s2">and </span><span class="s1">kwargs[</span><span class="s3">'min_method'</span><span class="s1">] </span><span class="s2">in </span><span class="s1">has_constraints:</span>
        <span class="s1">constraints = kwargs[</span><span class="s3">'constraints'</span><span class="s1">]</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">constraints = ()</span>

    <span class="s1">res = optimize.minimize(f</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">args=fargs</span><span class="s2">, </span><span class="s1">method=kwargs[</span><span class="s3">'min_method'</span><span class="s1">]</span><span class="s2">,</span>
                            <span class="s1">jac=score</span><span class="s2">, </span><span class="s1">hess=hess</span><span class="s2">, </span><span class="s1">bounds=bounds</span><span class="s2">, </span><span class="s1">constraints=constraints</span><span class="s2">,</span>
                            <span class="s1">callback=callback</span><span class="s2">, </span><span class="s1">options=options)</span>

    <span class="s1">xopt = res.x</span>
    <span class="s1">retvals = </span><span class="s2">None</span>
    <span class="s2">if </span><span class="s1">full_output:</span>
        <span class="s1">nit = getattr(res</span><span class="s2">, </span><span class="s3">'nit'</span><span class="s2">, </span><span class="s1">np.nan)  </span><span class="s5"># scipy 0.14 compat</span>
        <span class="s1">retvals = {</span><span class="s3">'fopt'</span><span class="s1">: res.fun</span><span class="s2">, </span><span class="s3">'iterations'</span><span class="s1">: nit</span><span class="s2">,</span>
                   <span class="s3">'fcalls'</span><span class="s1">: res.nfev</span><span class="s2">, </span><span class="s3">'warnflag'</span><span class="s1">: res.status</span><span class="s2">,</span>
                   <span class="s3">'converged'</span><span class="s1">: res.success}</span>
        <span class="s2">if </span><span class="s1">retall:</span>
            <span class="s1">retvals.update({</span><span class="s3">'allvecs'</span><span class="s1">: res.values()})</span>

    <span class="s2">return </span><span class="s1">xopt</span><span class="s2">, </span><span class="s1">retvals</span>


<span class="s2">def </span><span class="s1">_fit_newton(f</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">fargs</span><span class="s2">, </span><span class="s1">kwargs</span><span class="s2">, </span><span class="s1">disp=</span><span class="s2">True,</span>
                <span class="s1">maxiter=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">callback=</span><span class="s2">None, </span><span class="s1">retall=</span><span class="s2">False,</span>
                <span class="s1">full_output=</span><span class="s2">True, </span><span class="s1">hess=</span><span class="s2">None, </span><span class="s1">ridge_factor=</span><span class="s4">1e-10</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Fit using Newton-Raphson algorithm. 
 
    Parameters 
    ---------- 
    f : function 
        Returns negative log likelihood given parameters. 
    score : function 
        Returns gradient of negative log likelihood with respect to params. 
    start_params : array_like, optional 
        Initial guess of the solution for the loglikelihood maximization. 
        The default is an array of zeros. 
    fargs : tuple 
        Extra arguments passed to the objective function, i.e. 
        objective(x,*args) 
    kwargs : dict[str, Any] 
        Extra keyword arguments passed to the objective function, i.e. 
        objective(x,**kwargs) 
    disp : bool 
        Set to True to print convergence messages. 
    maxiter : int 
        The maximum number of iterations to perform. 
    callback : callable callback(xk) 
        Called after each iteration, as callback(xk), where xk is the 
        current parameter vector. 
    retall : bool 
        Set to True to return list of solutions at each iteration. 
        Available in Results object's mle_retvals attribute. 
    full_output : bool 
        Set to True to have all available output in the Results object's 
        mle_retvals attribute. The output is dependent on the solver. 
        See LikelihoodModelResults notes section for more information. 
    hess : str, optional 
        Method for computing the Hessian matrix, if applicable. 
    ridge_factor : float 
        Regularization factor for Hessian matrix. 
 
    Returns 
    ------- 
    xopt : ndarray 
        The solution to the objective function 
    retvals : dict, None 
        If `full_output` is True then this is a dictionary which holds 
        information returned from the solver used. If it is False, this is 
        None. 
    &quot;&quot;&quot;</span>
    <span class="s1">check_kwargs(kwargs</span><span class="s2">, </span><span class="s1">(</span><span class="s3">&quot;tol&quot;</span><span class="s2">, </span><span class="s3">&quot;ridge_factor&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;newton&quot;</span><span class="s1">)</span>
    <span class="s1">tol = kwargs.setdefault(</span><span class="s3">'tol'</span><span class="s2">, </span><span class="s4">1e-8</span><span class="s1">)</span>
    <span class="s1">ridge_factor = kwargs.setdefault(</span><span class="s3">'ridge_factor'</span><span class="s2">, </span><span class="s4">1e-10</span><span class="s1">)</span>
    <span class="s1">iterations = </span><span class="s4">0</span>
    <span class="s1">oldparams = np.inf</span>
    <span class="s1">newparams = np.asarray(start_params)</span>
    <span class="s2">if </span><span class="s1">retall:</span>
        <span class="s1">history = [oldparams</span><span class="s2">, </span><span class="s1">newparams]</span>
    <span class="s2">while </span><span class="s1">(iterations &lt; maxiter </span><span class="s2">and </span><span class="s1">np.any(np.abs(newparams -</span>
                                                  <span class="s1">oldparams) &gt; tol)):</span>
        <span class="s1">H = np.asarray(hess(newparams))</span>
        <span class="s5"># regularize Hessian, not clear what ridge factor should be</span>
        <span class="s5"># keyword option with absolute default 1e-10, see #1847</span>
        <span class="s2">if not </span><span class="s1">np.all(ridge_factor == </span><span class="s4">0</span><span class="s1">):</span>
            <span class="s1">H[np.diag_indices(H.shape[</span><span class="s4">0</span><span class="s1">])] += ridge_factor</span>
        <span class="s1">oldparams = newparams</span>
        <span class="s1">newparams = oldparams - np.linalg.solve(H</span><span class="s2">, </span><span class="s1">score(oldparams))</span>
        <span class="s2">if </span><span class="s1">retall:</span>
            <span class="s1">history.append(newparams)</span>
        <span class="s2">if </span><span class="s1">callback </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">callback(newparams)</span>
        <span class="s1">iterations += </span><span class="s4">1</span>
    <span class="s1">fval = f(newparams</span><span class="s2">, </span><span class="s1">*fargs)  </span><span class="s5"># this is the negative likelihood</span>
    <span class="s2">if </span><span class="s1">iterations == maxiter:</span>
        <span class="s1">warnflag = </span><span class="s4">1</span>
        <span class="s2">if </span><span class="s1">disp:</span>
            <span class="s1">print(</span><span class="s3">&quot;Warning: Maximum number of iterations has been &quot;</span>
                  <span class="s3">&quot;exceeded.&quot;</span><span class="s1">)</span>
            <span class="s1">print(</span><span class="s3">&quot;         Current function value: %f&quot; </span><span class="s1">% fval)</span>
            <span class="s1">print(</span><span class="s3">&quot;         Iterations: %d&quot; </span><span class="s1">% iterations)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">warnflag = </span><span class="s4">0</span>
        <span class="s2">if </span><span class="s1">disp:</span>
            <span class="s1">print(</span><span class="s3">&quot;Optimization terminated successfully.&quot;</span><span class="s1">)</span>
            <span class="s1">print(</span><span class="s3">&quot;         Current function value: %f&quot; </span><span class="s1">% fval)</span>
            <span class="s1">print(</span><span class="s3">&quot;         Iterations %d&quot; </span><span class="s1">% iterations)</span>
    <span class="s2">if </span><span class="s1">full_output:</span>
        <span class="s1">(xopt</span><span class="s2">, </span><span class="s1">fopt</span><span class="s2">, </span><span class="s1">niter</span><span class="s2">,</span>
         <span class="s1">gopt</span><span class="s2">, </span><span class="s1">hopt) = (newparams</span><span class="s2">, </span><span class="s1">f(newparams</span><span class="s2">, </span><span class="s1">*fargs)</span><span class="s2">,</span>
                        <span class="s1">iterations</span><span class="s2">, </span><span class="s1">score(newparams)</span><span class="s2">,</span>
                        <span class="s1">hess(newparams))</span>
        <span class="s1">converged = </span><span class="s2">not </span><span class="s1">warnflag</span>
        <span class="s1">retvals = {</span><span class="s3">'fopt'</span><span class="s1">: fopt</span><span class="s2">, </span><span class="s3">'iterations'</span><span class="s1">: niter</span><span class="s2">, </span><span class="s3">'score'</span><span class="s1">: gopt</span><span class="s2">,</span>
                   <span class="s3">'Hessian'</span><span class="s1">: hopt</span><span class="s2">, </span><span class="s3">'warnflag'</span><span class="s1">: warnflag</span><span class="s2">,</span>
                   <span class="s3">'converged'</span><span class="s1">: converged}</span>
        <span class="s2">if </span><span class="s1">retall:</span>
            <span class="s1">retvals.update({</span><span class="s3">'allvecs'</span><span class="s1">: history})</span>

    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">xopt = newparams</span>
        <span class="s1">retvals = </span><span class="s2">None</span>

    <span class="s2">return </span><span class="s1">xopt</span><span class="s2">, </span><span class="s1">retvals</span>


<span class="s2">def </span><span class="s1">_fit_bfgs(f</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">fargs</span><span class="s2">, </span><span class="s1">kwargs</span><span class="s2">, </span><span class="s1">disp=</span><span class="s2">True,</span>
              <span class="s1">maxiter=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">callback=</span><span class="s2">None, </span><span class="s1">retall=</span><span class="s2">False,</span>
              <span class="s1">full_output=</span><span class="s2">True, </span><span class="s1">hess=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Fit using Broyden-Fletcher-Goldfarb-Shannon algorithm. 
 
    Parameters 
    ---------- 
    f : function 
        Returns negative log likelihood given parameters. 
    score : function 
        Returns gradient of negative log likelihood with respect to params. 
    start_params : array_like, optional 
        Initial guess of the solution for the loglikelihood maximization. 
        The default is an array of zeros. 
    fargs : tuple 
        Extra arguments passed to the objective function, i.e. 
        objective(x,*args) 
    kwargs : dict[str, Any] 
        Extra keyword arguments passed to the objective function, i.e. 
        objective(x,**kwargs) 
    disp : bool 
        Set to True to print convergence messages. 
    maxiter : int 
        The maximum number of iterations to perform. 
    callback : callable callback(xk) 
        Called after each iteration, as callback(xk), where xk is the 
        current parameter vector. 
    retall : bool 
        Set to True to return list of solutions at each iteration. 
        Available in Results object's mle_retvals attribute. 
    full_output : bool 
        Set to True to have all available output in the Results object's 
        mle_retvals attribute. The output is dependent on the solver. 
        See LikelihoodModelResults notes section for more information. 
    hess : str, optional 
        Method for computing the Hessian matrix, if applicable. 
 
    Returns 
    ------- 
    xopt : ndarray 
        The solution to the objective function 
    retvals : dict, None 
        If `full_output` is True then this is a dictionary which holds 
        information returned from the solver used. If it is False, this is 
        None. 
    &quot;&quot;&quot;</span>
    <span class="s1">check_kwargs(kwargs</span><span class="s2">, </span><span class="s1">(</span><span class="s3">&quot;gtol&quot;</span><span class="s2">, </span><span class="s3">&quot;norm&quot;</span><span class="s2">, </span><span class="s3">&quot;epsilon&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;bfgs&quot;</span><span class="s1">)</span>
    <span class="s1">gtol = kwargs.setdefault(</span><span class="s3">'gtol'</span><span class="s2">, </span><span class="s4">1.0000000000000001e-05</span><span class="s1">)</span>
    <span class="s1">norm = kwargs.setdefault(</span><span class="s3">'norm'</span><span class="s2">, </span><span class="s1">np.Inf)</span>
    <span class="s1">epsilon = kwargs.setdefault(</span><span class="s3">'epsilon'</span><span class="s2">, </span><span class="s4">1.4901161193847656e-08</span><span class="s1">)</span>
    <span class="s1">retvals = optimize.fmin_bfgs(f</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">args=fargs</span><span class="s2">,</span>
                                 <span class="s1">gtol=gtol</span><span class="s2">, </span><span class="s1">norm=norm</span><span class="s2">, </span><span class="s1">epsilon=epsilon</span><span class="s2">,</span>
                                 <span class="s1">maxiter=maxiter</span><span class="s2">, </span><span class="s1">full_output=full_output</span><span class="s2">,</span>
                                 <span class="s1">disp=disp</span><span class="s2">, </span><span class="s1">retall=retall</span><span class="s2">, </span><span class="s1">callback=callback)</span>
    <span class="s2">if </span><span class="s1">full_output:</span>
        <span class="s2">if not </span><span class="s1">retall:</span>
            <span class="s1">xopt</span><span class="s2">, </span><span class="s1">fopt</span><span class="s2">, </span><span class="s1">gopt</span><span class="s2">, </span><span class="s1">Hinv</span><span class="s2">, </span><span class="s1">fcalls</span><span class="s2">, </span><span class="s1">gcalls</span><span class="s2">, </span><span class="s1">warnflag = retvals</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">(xopt</span><span class="s2">, </span><span class="s1">fopt</span><span class="s2">, </span><span class="s1">gopt</span><span class="s2">, </span><span class="s1">Hinv</span><span class="s2">, </span><span class="s1">fcalls</span><span class="s2">,</span>
             <span class="s1">gcalls</span><span class="s2">, </span><span class="s1">warnflag</span><span class="s2">, </span><span class="s1">allvecs) = retvals</span>
        <span class="s1">converged = </span><span class="s2">not </span><span class="s1">warnflag</span>
        <span class="s1">retvals = {</span><span class="s3">'fopt'</span><span class="s1">: fopt</span><span class="s2">, </span><span class="s3">'gopt'</span><span class="s1">: gopt</span><span class="s2">, </span><span class="s3">'Hinv'</span><span class="s1">: Hinv</span><span class="s2">,</span>
                   <span class="s3">'fcalls'</span><span class="s1">: fcalls</span><span class="s2">, </span><span class="s3">'gcalls'</span><span class="s1">: gcalls</span><span class="s2">, </span><span class="s3">'warnflag'</span><span class="s1">:</span>
                       <span class="s1">warnflag</span><span class="s2">, </span><span class="s3">'converged'</span><span class="s1">: converged}</span>
        <span class="s2">if </span><span class="s1">retall:</span>
            <span class="s1">retvals.update({</span><span class="s3">'allvecs'</span><span class="s1">: allvecs})</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">xopt = retvals</span>
        <span class="s1">retvals = </span><span class="s2">None</span>

    <span class="s2">return </span><span class="s1">xopt</span><span class="s2">, </span><span class="s1">retvals</span>


<span class="s2">def </span><span class="s1">_fit_lbfgs(f</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">fargs</span><span class="s2">, </span><span class="s1">kwargs</span><span class="s2">, </span><span class="s1">disp=</span><span class="s2">True, </span><span class="s1">maxiter=</span><span class="s4">100</span><span class="s2">,</span>
               <span class="s1">callback=</span><span class="s2">None, </span><span class="s1">retall=</span><span class="s2">False, </span><span class="s1">full_output=</span><span class="s2">True, </span><span class="s1">hess=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Fit using Limited-memory Broyden-Fletcher-Goldfarb-Shannon algorithm. 
 
    Parameters 
    ---------- 
    f : function 
        Returns negative log likelihood given parameters. 
    score : function 
        Returns gradient of negative log likelihood with respect to params. 
    start_params : array_like, optional 
        Initial guess of the solution for the loglikelihood maximization. 
        The default is an array of zeros. 
    fargs : tuple 
        Extra arguments passed to the objective function, i.e. 
        objective(x,*args) 
    kwargs : dict[str, Any] 
        Extra keyword arguments passed to the objective function, i.e. 
        objective(x,**kwargs) 
    disp : bool 
        Set to True to print convergence messages. 
    maxiter : int 
        The maximum number of iterations to perform. 
    callback : callable callback(xk) 
        Called after each iteration, as callback(xk), where xk is the 
        current parameter vector. 
    retall : bool 
        Set to True to return list of solutions at each iteration. 
        Available in Results object's mle_retvals attribute. 
    full_output : bool 
        Set to True to have all available output in the Results object's 
        mle_retvals attribute. The output is dependent on the solver. 
        See LikelihoodModelResults notes section for more information. 
    hess : str, optional 
        Method for computing the Hessian matrix, if applicable. 
 
    Returns 
    ------- 
    xopt : ndarray 
        The solution to the objective function 
    retvals : dict, None 
        If `full_output` is True then this is a dictionary which holds 
        information returned from the solver used. If it is False, this is 
        None. 
 
    Notes 
    ----- 
    Within the mle part of statsmodels, the log likelihood function and 
    its gradient with respect to the parameters do not have notationally 
    consistent sign. 
    &quot;&quot;&quot;</span>
    <span class="s1">check_kwargs(</span>
        <span class="s1">kwargs</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s3">&quot;m&quot;</span><span class="s2">, </span><span class="s3">&quot;pgtol&quot;</span><span class="s2">, </span><span class="s3">&quot;factr&quot;</span><span class="s2">, </span><span class="s3">&quot;maxfun&quot;</span><span class="s2">, </span><span class="s3">&quot;epsilon&quot;</span><span class="s2">, </span><span class="s3">&quot;approx_grad&quot;</span><span class="s2">, </span><span class="s3">&quot;bounds&quot;</span><span class="s2">, </span><span class="s3">&quot;loglike_and_score&quot;</span><span class="s2">, </span><span class="s3">&quot;iprint&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s3">&quot;lbfgs&quot;</span>
    <span class="s1">)</span>
    <span class="s5"># Use unconstrained optimization by default.</span>
    <span class="s1">bounds = kwargs.setdefault(</span><span class="s3">'bounds'</span><span class="s2">, </span><span class="s1">[(</span><span class="s2">None, None</span><span class="s1">)] * len(start_params))</span>
    <span class="s1">kwargs.setdefault(</span><span class="s3">'iprint'</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s5"># Pass the following keyword argument names through to fmin_l_bfgs_b</span>
    <span class="s5"># if they are present in kwargs, otherwise use the fmin_l_bfgs_b</span>
    <span class="s5"># default values.</span>
    <span class="s1">names = (</span><span class="s3">'m'</span><span class="s2">, </span><span class="s3">'pgtol'</span><span class="s2">, </span><span class="s3">'factr'</span><span class="s2">, </span><span class="s3">'maxfun'</span><span class="s2">, </span><span class="s3">'epsilon'</span><span class="s2">, </span><span class="s3">'approx_grad'</span><span class="s1">)</span>
    <span class="s1">extra_kwargs = dict((x</span><span class="s2">, </span><span class="s1">kwargs[x]) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">names </span><span class="s2">if </span><span class="s1">x </span><span class="s2">in </span><span class="s1">kwargs)</span>

    <span class="s5"># Extract values for the options related to the gradient.</span>
    <span class="s1">approx_grad = kwargs.get(</span><span class="s3">'approx_grad'</span><span class="s2">, False</span><span class="s1">)</span>
    <span class="s1">loglike_and_score = kwargs.get(</span><span class="s3">'loglike_and_score'</span><span class="s2">, None</span><span class="s1">)</span>
    <span class="s1">epsilon = kwargs.get(</span><span class="s3">'epsilon'</span><span class="s2">, None</span><span class="s1">)</span>

    <span class="s5"># The approx_grad flag has superpowers nullifying the score function arg.</span>
    <span class="s2">if </span><span class="s1">approx_grad:</span>
        <span class="s1">score = </span><span class="s2">None</span>

    <span class="s5"># Choose among three options for dealing with the gradient (the gradient</span>
    <span class="s5"># of a log likelihood function with respect to its parameters</span>
    <span class="s5"># is more specifically called the score in statistics terminology).</span>
    <span class="s5"># The first option is to use the finite-differences</span>
    <span class="s5"># approximation that is built into the fmin_l_bfgs_b optimizer.</span>
    <span class="s5"># The second option is to use the provided score function.</span>
    <span class="s5"># The third option is to use the score component of a provided</span>
    <span class="s5"># function that simultaneously evaluates the log likelihood and score.</span>
    <span class="s2">if </span><span class="s1">epsilon </span><span class="s2">and not </span><span class="s1">approx_grad:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">'a finite-differences epsilon was provided '</span>
                         <span class="s3">'even though we are not using approx_grad'</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">approx_grad </span><span class="s2">and </span><span class="s1">loglike_and_score:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">'gradient approximation was requested '</span>
                         <span class="s3">'even though an analytic loglike_and_score function '</span>
                         <span class="s3">'was given'</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">loglike_and_score:</span>
        <span class="s1">func = </span><span class="s2">lambda </span><span class="s1">p</span><span class="s2">, </span><span class="s1">*a: tuple(-x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">loglike_and_score(p</span><span class="s2">, </span><span class="s1">*a))</span>
    <span class="s2">elif </span><span class="s1">score:</span>
        <span class="s1">func = f</span>
        <span class="s1">extra_kwargs[</span><span class="s3">'fprime'</span><span class="s1">] = score</span>
    <span class="s2">elif </span><span class="s1">approx_grad:</span>
        <span class="s1">func = f</span>

    <span class="s1">retvals = optimize.fmin_l_bfgs_b(func</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">maxiter=maxiter</span><span class="s2">,</span>
                                     <span class="s1">callback=callback</span><span class="s2">, </span><span class="s1">args=fargs</span><span class="s2">,</span>
                                     <span class="s1">bounds=bounds</span><span class="s2">, </span><span class="s1">disp=disp</span><span class="s2">,</span>
                                     <span class="s1">**extra_kwargs)</span>

    <span class="s2">if </span><span class="s1">full_output:</span>
        <span class="s1">xopt</span><span class="s2">, </span><span class="s1">fopt</span><span class="s2">, </span><span class="s1">d = retvals</span>
        <span class="s5"># The warnflag is</span>
        <span class="s5"># 0 if converged</span>
        <span class="s5"># 1 if too many function evaluations or too many iterations</span>
        <span class="s5"># 2 if stopped for another reason, given in d['task']</span>
        <span class="s1">warnflag = d[</span><span class="s3">'warnflag'</span><span class="s1">]</span>
        <span class="s1">converged = (warnflag == </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">gopt = d[</span><span class="s3">'grad'</span><span class="s1">]</span>
        <span class="s1">fcalls = d[</span><span class="s3">'funcalls'</span><span class="s1">]</span>
        <span class="s1">iterations = d[</span><span class="s3">'nit'</span><span class="s1">]</span>
        <span class="s1">retvals = {</span><span class="s3">'fopt'</span><span class="s1">: fopt</span><span class="s2">, </span><span class="s3">'gopt'</span><span class="s1">: gopt</span><span class="s2">, </span><span class="s3">'fcalls'</span><span class="s1">: fcalls</span><span class="s2">,</span>
                   <span class="s3">'warnflag'</span><span class="s1">: warnflag</span><span class="s2">, </span><span class="s3">'converged'</span><span class="s1">: converged</span><span class="s2">,</span>
                   <span class="s3">'iterations'</span><span class="s1">: iterations}</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">xopt = retvals[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">retvals = </span><span class="s2">None</span>

    <span class="s2">return </span><span class="s1">xopt</span><span class="s2">, </span><span class="s1">retvals</span>


<span class="s2">def </span><span class="s1">_fit_nm(f</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">fargs</span><span class="s2">, </span><span class="s1">kwargs</span><span class="s2">, </span><span class="s1">disp=</span><span class="s2">True,</span>
            <span class="s1">maxiter=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">callback=</span><span class="s2">None, </span><span class="s1">retall=</span><span class="s2">False,</span>
            <span class="s1">full_output=</span><span class="s2">True, </span><span class="s1">hess=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Fit using Nelder-Mead algorithm. 
 
    Parameters 
    ---------- 
    f : function 
        Returns negative log likelihood given parameters. 
    score : function 
        Returns gradient of negative log likelihood with respect to params. 
    start_params : array_like, optional 
        Initial guess of the solution for the loglikelihood maximization. 
        The default is an array of zeros. 
    fargs : tuple 
        Extra arguments passed to the objective function, i.e. 
        objective(x,*args) 
    kwargs : dict[str, Any] 
        Extra keyword arguments passed to the objective function, i.e. 
        objective(x,**kwargs) 
    disp : bool 
        Set to True to print convergence messages. 
    maxiter : int 
        The maximum number of iterations to perform. 
    callback : callable callback(xk) 
        Called after each iteration, as callback(xk), where xk is the 
        current parameter vector. 
    retall : bool 
        Set to True to return list of solutions at each iteration. 
        Available in Results object's mle_retvals attribute. 
    full_output : bool 
        Set to True to have all available output in the Results object's 
        mle_retvals attribute. The output is dependent on the solver. 
        See LikelihoodModelResults notes section for more information. 
    hess : str, optional 
        Method for computing the Hessian matrix, if applicable. 
 
    Returns 
    ------- 
    xopt : ndarray 
        The solution to the objective function 
    retvals : dict, None 
        If `full_output` is True then this is a dictionary which holds 
        information returned from the solver used. If it is False, this is 
        None. 
    &quot;&quot;&quot;</span>
    <span class="s1">check_kwargs(kwargs</span><span class="s2">, </span><span class="s1">(</span><span class="s3">&quot;xtol&quot;</span><span class="s2">, </span><span class="s3">&quot;ftol&quot;</span><span class="s2">, </span><span class="s3">&quot;maxfun&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;nm&quot;</span><span class="s1">)</span>
    <span class="s1">xtol = kwargs.setdefault(</span><span class="s3">'xtol'</span><span class="s2">, </span><span class="s4">0.0001</span><span class="s1">)</span>
    <span class="s1">ftol = kwargs.setdefault(</span><span class="s3">'ftol'</span><span class="s2">, </span><span class="s4">0.0001</span><span class="s1">)</span>
    <span class="s1">maxfun = kwargs.setdefault(</span><span class="s3">'maxfun'</span><span class="s2">, None</span><span class="s1">)</span>
    <span class="s1">retvals = optimize.fmin(f</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">args=fargs</span><span class="s2">, </span><span class="s1">xtol=xtol</span><span class="s2">,</span>
                            <span class="s1">ftol=ftol</span><span class="s2">, </span><span class="s1">maxiter=maxiter</span><span class="s2">, </span><span class="s1">maxfun=maxfun</span><span class="s2">,</span>
                            <span class="s1">full_output=full_output</span><span class="s2">, </span><span class="s1">disp=disp</span><span class="s2">, </span><span class="s1">retall=retall</span><span class="s2">,</span>
                            <span class="s1">callback=callback)</span>
    <span class="s2">if </span><span class="s1">full_output:</span>
        <span class="s2">if not </span><span class="s1">retall:</span>
            <span class="s1">xopt</span><span class="s2">, </span><span class="s1">fopt</span><span class="s2">, </span><span class="s1">niter</span><span class="s2">, </span><span class="s1">fcalls</span><span class="s2">, </span><span class="s1">warnflag = retvals</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">xopt</span><span class="s2">, </span><span class="s1">fopt</span><span class="s2">, </span><span class="s1">niter</span><span class="s2">, </span><span class="s1">fcalls</span><span class="s2">, </span><span class="s1">warnflag</span><span class="s2">, </span><span class="s1">allvecs = retvals</span>
        <span class="s1">converged = </span><span class="s2">not </span><span class="s1">warnflag</span>
        <span class="s1">retvals = {</span><span class="s3">'fopt'</span><span class="s1">: fopt</span><span class="s2">, </span><span class="s3">'iterations'</span><span class="s1">: niter</span><span class="s2">,</span>
                   <span class="s3">'fcalls'</span><span class="s1">: fcalls</span><span class="s2">, </span><span class="s3">'warnflag'</span><span class="s1">: warnflag</span><span class="s2">,</span>
                   <span class="s3">'converged'</span><span class="s1">: converged}</span>
        <span class="s2">if </span><span class="s1">retall:</span>
            <span class="s1">retvals.update({</span><span class="s3">'allvecs'</span><span class="s1">: allvecs})</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">xopt = retvals</span>
        <span class="s1">retvals = </span><span class="s2">None</span>

    <span class="s2">return </span><span class="s1">xopt</span><span class="s2">, </span><span class="s1">retvals</span>


<span class="s2">def </span><span class="s1">_fit_cg(f</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">fargs</span><span class="s2">, </span><span class="s1">kwargs</span><span class="s2">, </span><span class="s1">disp=</span><span class="s2">True,</span>
            <span class="s1">maxiter=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">callback=</span><span class="s2">None, </span><span class="s1">retall=</span><span class="s2">False,</span>
            <span class="s1">full_output=</span><span class="s2">True, </span><span class="s1">hess=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Fit using Conjugate Gradient algorithm. 
 
    Parameters 
    ---------- 
    f : function 
        Returns negative log likelihood given parameters. 
    score : function 
        Returns gradient of negative log likelihood with respect to params. 
    start_params : array_like, optional 
        Initial guess of the solution for the loglikelihood maximization. 
        The default is an array of zeros. 
    fargs : tuple 
        Extra arguments passed to the objective function, i.e. 
        objective(x,*args) 
    kwargs : dict[str, Any] 
        Extra keyword arguments passed to the objective function, i.e. 
        objective(x,**kwargs) 
    disp : bool 
        Set to True to print convergence messages. 
    maxiter : int 
        The maximum number of iterations to perform. 
    callback : callable callback(xk) 
        Called after each iteration, as callback(xk), where xk is the 
        current parameter vector. 
    retall : bool 
        Set to True to return list of solutions at each iteration. 
        Available in Results object's mle_retvals attribute. 
    full_output : bool 
        Set to True to have all available output in the Results object's 
        mle_retvals attribute. The output is dependent on the solver. 
        See LikelihoodModelResults notes section for more information. 
    hess : str, optional 
        Method for computing the Hessian matrix, if applicable. 
 
    Returns 
    ------- 
    xopt : ndarray 
        The solution to the objective function 
    retvals : dict, None 
        If `full_output` is True then this is a dictionary which holds 
        information returned from the solver used. If it is False, this is 
        None. 
    &quot;&quot;&quot;</span>
    <span class="s1">check_kwargs(kwargs</span><span class="s2">, </span><span class="s1">(</span><span class="s3">&quot;gtol&quot;</span><span class="s2">, </span><span class="s3">&quot;norm&quot;</span><span class="s2">, </span><span class="s3">&quot;epsilon&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;cg&quot;</span><span class="s1">)</span>
    <span class="s1">gtol = kwargs.setdefault(</span><span class="s3">'gtol'</span><span class="s2">, </span><span class="s4">1.0000000000000001e-05</span><span class="s1">)</span>
    <span class="s1">norm = kwargs.setdefault(</span><span class="s3">'norm'</span><span class="s2">, </span><span class="s1">np.Inf)</span>
    <span class="s1">epsilon = kwargs.setdefault(</span><span class="s3">'epsilon'</span><span class="s2">, </span><span class="s4">1.4901161193847656e-08</span><span class="s1">)</span>
    <span class="s1">retvals = optimize.fmin_cg(f</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">gtol=gtol</span><span class="s2">, </span><span class="s1">norm=norm</span><span class="s2">,</span>
                               <span class="s1">epsilon=epsilon</span><span class="s2">, </span><span class="s1">maxiter=maxiter</span><span class="s2">,</span>
                               <span class="s1">full_output=full_output</span><span class="s2">, </span><span class="s1">disp=disp</span><span class="s2">,</span>
                               <span class="s1">retall=retall</span><span class="s2">, </span><span class="s1">callback=callback)</span>
    <span class="s2">if </span><span class="s1">full_output:</span>
        <span class="s2">if not </span><span class="s1">retall:</span>
            <span class="s1">xopt</span><span class="s2">, </span><span class="s1">fopt</span><span class="s2">, </span><span class="s1">fcalls</span><span class="s2">, </span><span class="s1">gcalls</span><span class="s2">, </span><span class="s1">warnflag = retvals</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">xopt</span><span class="s2">, </span><span class="s1">fopt</span><span class="s2">, </span><span class="s1">fcalls</span><span class="s2">, </span><span class="s1">gcalls</span><span class="s2">, </span><span class="s1">warnflag</span><span class="s2">, </span><span class="s1">allvecs = retvals</span>
        <span class="s1">converged = </span><span class="s2">not </span><span class="s1">warnflag</span>
        <span class="s1">retvals = {</span><span class="s3">'fopt'</span><span class="s1">: fopt</span><span class="s2">, </span><span class="s3">'fcalls'</span><span class="s1">: fcalls</span><span class="s2">, </span><span class="s3">'gcalls'</span><span class="s1">: gcalls</span><span class="s2">,</span>
                   <span class="s3">'warnflag'</span><span class="s1">: warnflag</span><span class="s2">, </span><span class="s3">'converged'</span><span class="s1">: converged}</span>
        <span class="s2">if </span><span class="s1">retall:</span>
            <span class="s1">retvals.update({</span><span class="s3">'allvecs'</span><span class="s1">: allvecs})</span>

    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">xopt = retvals</span>
        <span class="s1">retvals = </span><span class="s2">None</span>

    <span class="s2">return </span><span class="s1">xopt</span><span class="s2">, </span><span class="s1">retvals</span>


<span class="s2">def </span><span class="s1">_fit_ncg(f</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">fargs</span><span class="s2">, </span><span class="s1">kwargs</span><span class="s2">, </span><span class="s1">disp=</span><span class="s2">True,</span>
             <span class="s1">maxiter=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">callback=</span><span class="s2">None, </span><span class="s1">retall=</span><span class="s2">False,</span>
             <span class="s1">full_output=</span><span class="s2">True, </span><span class="s1">hess=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Fit using Newton Conjugate Gradient algorithm. 
 
    Parameters 
    ---------- 
    f : function 
        Returns negative log likelihood given parameters. 
    score : function 
        Returns gradient of negative log likelihood with respect to params. 
    start_params : array_like, optional 
        Initial guess of the solution for the loglikelihood maximization. 
        The default is an array of zeros. 
    fargs : tuple 
        Extra arguments passed to the objective function, i.e. 
        objective(x,*args) 
    kwargs : dict[str, Any] 
        Extra keyword arguments passed to the objective function, i.e. 
        objective(x,**kwargs) 
    disp : bool 
        Set to True to print convergence messages. 
    maxiter : int 
        The maximum number of iterations to perform. 
    callback : callable callback(xk) 
        Called after each iteration, as callback(xk), where xk is the 
        current parameter vector. 
    retall : bool 
        Set to True to return list of solutions at each iteration. 
        Available in Results object's mle_retvals attribute. 
    full_output : bool 
        Set to True to have all available output in the Results object's 
        mle_retvals attribute. The output is dependent on the solver. 
        See LikelihoodModelResults notes section for more information. 
    hess : str, optional 
        Method for computing the Hessian matrix, if applicable. 
 
    Returns 
    ------- 
    xopt : ndarray 
        The solution to the objective function 
    retvals : dict, None 
        If `full_output` is True then this is a dictionary which holds 
        information returned from the solver used. If it is False, this is 
        None. 
    &quot;&quot;&quot;</span>
    <span class="s1">check_kwargs(kwargs</span><span class="s2">, </span><span class="s1">(</span><span class="s3">&quot;fhess_p&quot;</span><span class="s2">, </span><span class="s3">&quot;avextol&quot;</span><span class="s2">, </span><span class="s3">&quot;epsilon&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;ncg&quot;</span><span class="s1">)</span>
    <span class="s1">fhess_p = kwargs.setdefault(</span><span class="s3">'fhess_p'</span><span class="s2">, None</span><span class="s1">)</span>
    <span class="s1">avextol = kwargs.setdefault(</span><span class="s3">'avextol'</span><span class="s2">, </span><span class="s4">1.0000000000000001e-05</span><span class="s1">)</span>
    <span class="s1">epsilon = kwargs.setdefault(</span><span class="s3">'epsilon'</span><span class="s2">, </span><span class="s4">1.4901161193847656e-08</span><span class="s1">)</span>
    <span class="s1">retvals = optimize.fmin_ncg(f</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">fhess_p=fhess_p</span><span class="s2">,</span>
                                <span class="s1">fhess=hess</span><span class="s2">, </span><span class="s1">args=fargs</span><span class="s2">, </span><span class="s1">avextol=avextol</span><span class="s2">,</span>
                                <span class="s1">epsilon=epsilon</span><span class="s2">, </span><span class="s1">maxiter=maxiter</span><span class="s2">,</span>
                                <span class="s1">full_output=full_output</span><span class="s2">, </span><span class="s1">disp=disp</span><span class="s2">,</span>
                                <span class="s1">retall=retall</span><span class="s2">, </span><span class="s1">callback=callback)</span>
    <span class="s2">if </span><span class="s1">full_output:</span>
        <span class="s2">if not </span><span class="s1">retall:</span>
            <span class="s1">xopt</span><span class="s2">, </span><span class="s1">fopt</span><span class="s2">, </span><span class="s1">fcalls</span><span class="s2">, </span><span class="s1">gcalls</span><span class="s2">, </span><span class="s1">hcalls</span><span class="s2">, </span><span class="s1">warnflag = retvals</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">xopt</span><span class="s2">, </span><span class="s1">fopt</span><span class="s2">, </span><span class="s1">fcalls</span><span class="s2">, </span><span class="s1">gcalls</span><span class="s2">, </span><span class="s1">hcalls</span><span class="s2">, </span><span class="s1">warnflag</span><span class="s2">, </span><span class="s1">allvecs = \</span>
                <span class="s1">retvals</span>
        <span class="s1">converged = </span><span class="s2">not </span><span class="s1">warnflag</span>
        <span class="s1">retvals = {</span><span class="s3">'fopt'</span><span class="s1">: fopt</span><span class="s2">, </span><span class="s3">'fcalls'</span><span class="s1">: fcalls</span><span class="s2">, </span><span class="s3">'gcalls'</span><span class="s1">: gcalls</span><span class="s2">,</span>
                   <span class="s3">'hcalls'</span><span class="s1">: hcalls</span><span class="s2">, </span><span class="s3">'warnflag'</span><span class="s1">: warnflag</span><span class="s2">,</span>
                   <span class="s3">'converged'</span><span class="s1">: converged}</span>
        <span class="s2">if </span><span class="s1">retall:</span>
            <span class="s1">retvals.update({</span><span class="s3">'allvecs'</span><span class="s1">: allvecs})</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">xopt = retvals</span>
        <span class="s1">retvals = </span><span class="s2">None</span>

    <span class="s2">return </span><span class="s1">xopt</span><span class="s2">, </span><span class="s1">retvals</span>


<span class="s2">def </span><span class="s1">_fit_powell(f</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">fargs</span><span class="s2">, </span><span class="s1">kwargs</span><span class="s2">, </span><span class="s1">disp=</span><span class="s2">True,</span>
                <span class="s1">maxiter=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">callback=</span><span class="s2">None, </span><span class="s1">retall=</span><span class="s2">False,</span>
                <span class="s1">full_output=</span><span class="s2">True, </span><span class="s1">hess=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Fit using Powell's conjugate direction algorithm. 
 
    Parameters 
    ---------- 
    f : function 
        Returns negative log likelihood given parameters. 
    score : function 
        Returns gradient of negative log likelihood with respect to params. 
    start_params : array_like, optional 
        Initial guess of the solution for the loglikelihood maximization. 
        The default is an array of zeros. 
    fargs : tuple 
        Extra arguments passed to the objective function, i.e. 
        objective(x,*args) 
    kwargs : dict[str, Any] 
        Extra keyword arguments passed to the objective function, i.e. 
        objective(x,**kwargs) 
    disp : bool 
        Set to True to print convergence messages. 
    maxiter : int 
        The maximum number of iterations to perform. 
    callback : callable callback(xk) 
        Called after each iteration, as callback(xk), where xk is the 
        current parameter vector. 
    retall : bool 
        Set to True to return list of solutions at each iteration. 
        Available in Results object's mle_retvals attribute. 
    full_output : bool 
        Set to True to have all available output in the Results object's 
        mle_retvals attribute. The output is dependent on the solver. 
        See LikelihoodModelResults notes section for more information. 
    hess : str, optional 
        Method for computing the Hessian matrix, if applicable. 
 
    Returns 
    ------- 
    xopt : ndarray 
        The solution to the objective function 
    retvals : dict, None 
        If `full_output` is True then this is a dictionary which holds 
        information returned from the solver used. If it is False, this is 
        None. 
    &quot;&quot;&quot;</span>
    <span class="s1">check_kwargs(kwargs</span><span class="s2">, </span><span class="s1">(</span><span class="s3">&quot;xtol&quot;</span><span class="s2">, </span><span class="s3">&quot;ftol&quot;</span><span class="s2">, </span><span class="s3">&quot;maxfun&quot;</span><span class="s2">, </span><span class="s3">&quot;start_direc&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;powell&quot;</span><span class="s1">)</span>
    <span class="s1">xtol = kwargs.setdefault(</span><span class="s3">'xtol'</span><span class="s2">, </span><span class="s4">0.0001</span><span class="s1">)</span>
    <span class="s1">ftol = kwargs.setdefault(</span><span class="s3">'ftol'</span><span class="s2">, </span><span class="s4">0.0001</span><span class="s1">)</span>
    <span class="s1">maxfun = kwargs.setdefault(</span><span class="s3">'maxfun'</span><span class="s2">, None</span><span class="s1">)</span>
    <span class="s1">start_direc = kwargs.setdefault(</span><span class="s3">'start_direc'</span><span class="s2">, None</span><span class="s1">)</span>
    <span class="s1">retvals = optimize.fmin_powell(f</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">args=fargs</span><span class="s2">, </span><span class="s1">xtol=xtol</span><span class="s2">,</span>
                                   <span class="s1">ftol=ftol</span><span class="s2">, </span><span class="s1">maxiter=maxiter</span><span class="s2">, </span><span class="s1">maxfun=maxfun</span><span class="s2">,</span>
                                   <span class="s1">full_output=full_output</span><span class="s2">, </span><span class="s1">disp=disp</span><span class="s2">,</span>
                                   <span class="s1">retall=retall</span><span class="s2">, </span><span class="s1">callback=callback</span><span class="s2">,</span>
                                   <span class="s1">direc=start_direc)</span>
    <span class="s2">if </span><span class="s1">full_output:</span>
        <span class="s2">if not </span><span class="s1">retall:</span>
            <span class="s1">xopt</span><span class="s2">, </span><span class="s1">fopt</span><span class="s2">, </span><span class="s1">direc</span><span class="s2">, </span><span class="s1">niter</span><span class="s2">, </span><span class="s1">fcalls</span><span class="s2">, </span><span class="s1">warnflag = retvals</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">xopt</span><span class="s2">, </span><span class="s1">fopt</span><span class="s2">, </span><span class="s1">direc</span><span class="s2">, </span><span class="s1">niter</span><span class="s2">, </span><span class="s1">fcalls</span><span class="s2">, </span><span class="s1">warnflag</span><span class="s2">, </span><span class="s1">allvecs = \</span>
                <span class="s1">retvals</span>
        <span class="s1">converged = </span><span class="s2">not </span><span class="s1">warnflag</span>
        <span class="s1">retvals = {</span><span class="s3">'fopt'</span><span class="s1">: fopt</span><span class="s2">, </span><span class="s3">'direc'</span><span class="s1">: direc</span><span class="s2">, </span><span class="s3">'iterations'</span><span class="s1">: niter</span><span class="s2">,</span>
                   <span class="s3">'fcalls'</span><span class="s1">: fcalls</span><span class="s2">, </span><span class="s3">'warnflag'</span><span class="s1">: warnflag</span><span class="s2">,</span>
                   <span class="s3">'converged'</span><span class="s1">: converged}</span>
        <span class="s2">if </span><span class="s1">retall:</span>
            <span class="s1">retvals.update({</span><span class="s3">'allvecs'</span><span class="s1">: allvecs})</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">xopt = retvals</span>
        <span class="s1">retvals = </span><span class="s2">None</span>

    <span class="s2">return </span><span class="s1">xopt</span><span class="s2">, </span><span class="s1">retvals</span>


<span class="s2">def </span><span class="s1">_fit_basinhopping(f</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">fargs</span><span class="s2">, </span><span class="s1">kwargs</span><span class="s2">, </span><span class="s1">disp=</span><span class="s2">True,</span>
                      <span class="s1">maxiter=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">callback=</span><span class="s2">None, </span><span class="s1">retall=</span><span class="s2">False,</span>
                      <span class="s1">full_output=</span><span class="s2">True, </span><span class="s1">hess=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Fit using Basin-hopping algorithm. 
 
    Parameters 
    ---------- 
    f : function 
        Returns negative log likelihood given parameters. 
    score : function 
        Returns gradient of negative log likelihood with respect to params. 
    start_params : array_like, optional 
        Initial guess of the solution for the loglikelihood maximization. 
        The default is an array of zeros. 
    fargs : tuple 
        Extra arguments passed to the objective function, i.e. 
        objective(x,*args) 
    kwargs : dict[str, Any] 
        Extra keyword arguments passed to the objective function, i.e. 
        objective(x,**kwargs) 
    disp : bool 
        Set to True to print convergence messages. 
    maxiter : int 
        The maximum number of iterations to perform. 
    callback : callable callback(xk) 
        Called after each iteration, as callback(xk), where xk is the 
        current parameter vector. 
    retall : bool 
        Set to True to return list of solutions at each iteration. 
        Available in Results object's mle_retvals attribute. 
    full_output : bool 
        Set to True to have all available output in the Results object's 
        mle_retvals attribute. The output is dependent on the solver. 
        See LikelihoodModelResults notes section for more information. 
    hess : str, optional 
        Method for computing the Hessian matrix, if applicable. 
 
    Returns 
    ------- 
    xopt : ndarray 
        The solution to the objective function 
    retvals : dict, None 
        If `full_output` is True then this is a dictionary which holds 
        information returned from the solver used. If it is False, this is 
        None. 
    &quot;&quot;&quot;</span>
    <span class="s1">check_kwargs(</span>
        <span class="s1">kwargs</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s3">&quot;niter&quot;</span><span class="s2">, </span><span class="s3">&quot;niter_success&quot;</span><span class="s2">, </span><span class="s3">&quot;T&quot;</span><span class="s2">, </span><span class="s3">&quot;stepsize&quot;</span><span class="s2">, </span><span class="s3">&quot;interval&quot;</span><span class="s2">, </span><span class="s3">&quot;minimizer&quot;</span><span class="s2">, </span><span class="s3">&quot;seed&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s3">&quot;basinhopping&quot;</span>
    <span class="s1">)</span>
    <span class="s1">kwargs = {k: v </span><span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">v </span><span class="s2">in </span><span class="s1">kwargs.items()}</span>
    <span class="s1">niter = kwargs.setdefault(</span><span class="s3">'niter'</span><span class="s2">, </span><span class="s4">100</span><span class="s1">)</span>
    <span class="s1">niter_success = kwargs.setdefault(</span><span class="s3">'niter_success'</span><span class="s2">, None</span><span class="s1">)</span>
    <span class="s1">T = kwargs.setdefault(</span><span class="s3">'T'</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">)</span>
    <span class="s1">stepsize = kwargs.setdefault(</span><span class="s3">'stepsize'</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">)</span>
    <span class="s1">interval = kwargs.setdefault(</span><span class="s3">'interval'</span><span class="s2">, </span><span class="s4">50</span><span class="s1">)</span>
    <span class="s1">seed = kwargs.get(</span><span class="s3">&quot;seed&quot;</span><span class="s1">)</span>
    <span class="s1">minimizer_kwargs = kwargs.get(</span><span class="s3">'minimizer'</span><span class="s2">, </span><span class="s1">{})</span>
    <span class="s1">minimizer_kwargs[</span><span class="s3">'args'</span><span class="s1">] = fargs</span>
    <span class="s1">minimizer_kwargs[</span><span class="s3">'jac'</span><span class="s1">] = score</span>
    <span class="s1">method = minimizer_kwargs.get(</span><span class="s3">'method'</span><span class="s2">, None</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">method </span><span class="s2">and </span><span class="s1">method != </span><span class="s3">'L-BFGS-B'</span><span class="s1">:  </span><span class="s5"># l_bfgs_b does not take a hessian</span>
        <span class="s1">minimizer_kwargs[</span><span class="s3">'hess'</span><span class="s1">] = hess</span>

    <span class="s1">retvals = optimize.basinhopping(f</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">,</span>
                                    <span class="s1">minimizer_kwargs=minimizer_kwargs</span><span class="s2">,</span>
                                    <span class="s1">niter=niter</span><span class="s2">, </span><span class="s1">niter_success=niter_success</span><span class="s2">,</span>
                                    <span class="s1">T=T</span><span class="s2">, </span><span class="s1">stepsize=stepsize</span><span class="s2">, </span><span class="s1">disp=disp</span><span class="s2">,</span>
                                    <span class="s1">callback=callback</span><span class="s2">, </span><span class="s1">interval=interval</span><span class="s2">,</span>
                                    <span class="s1">seed=seed)</span>
    <span class="s1">xopt = retvals.x</span>
    <span class="s2">if </span><span class="s1">full_output:</span>
        <span class="s1">retvals = {</span>
            <span class="s3">'fopt'</span><span class="s1">: retvals.fun</span><span class="s2">,</span>
            <span class="s3">'iterations'</span><span class="s1">: retvals.nit</span><span class="s2">,</span>
            <span class="s3">'fcalls'</span><span class="s1">: retvals.nfev</span><span class="s2">,</span>
            <span class="s3">'converged'</span><span class="s1">: </span><span class="s3">'completed successfully' </span><span class="s2">in </span><span class="s1">retvals.message[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">}</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">retvals = </span><span class="s2">None</span>

    <span class="s2">return </span><span class="s1">xopt</span><span class="s2">, </span><span class="s1">retvals</span>
</pre>
</body>
</html>