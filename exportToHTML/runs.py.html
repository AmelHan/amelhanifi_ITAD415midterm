<html>
<head>
<title>runs.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
runs.py</font>
</center></td></tr></table>
<pre><span class="s0">'''runstest 
 
formulas for mean and var of runs taken from SAS manual NPAR tests, also idea 
for runstest_1samp and runstest_2samp 
 
Description in NIST handbook and dataplot does not explain their expected 
values, or variance 
 
Note: 
There are (at least) two definitions of runs used in literature. The classical 
definition which is also used here, is that runs are sequences of identical 
observations separated by observations with different realizations. 
The second definition allows for overlapping runs, or runs where counting a 
run is also started after a run of a fixed length of the same kind. 
 
 
TODO 
* add one-sided tests where possible or where it makes sense 
 
'''</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">stats</span>
<span class="s2">from </span><span class="s1">scipy.special </span><span class="s2">import </span><span class="s1">comb</span>
<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.validation </span><span class="s2">import </span><span class="s1">array_like</span>

<span class="s2">class </span><span class="s1">Runs:</span>
    <span class="s0">'''class for runs in a binary sequence 
 
 
    Parameters 
    ---------- 
    x : array_like, 1d 
        data array, 
 
 
    Notes 
    ----- 
    This was written as a more general class for runs. This has some redundant 
    calculations when only the runs_test is used. 
 
    TODO: make it lazy 
 
    The runs test could be generalized to more than 1d if there is a use case 
    for it. 
 
    This should be extended once I figure out what the distribution of runs 
    of any length k is. 
 
    The exact distribution for the runs test is also available but not yet 
    verified. 
 
    '''</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s1">self.x = np.asarray(x)</span>

        <span class="s1">self.runstart = runstart = np.nonzero(np.diff(np.r_[[-np.inf]</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">[np.inf]]))[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">self.runs = runs = np.diff(runstart)</span>
        <span class="s1">self.runs_sign = runs_sign = x[runstart[:-</span><span class="s3">1</span><span class="s1">]]</span>
        <span class="s1">self.runs_pos = runs[runs_sign==</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">self.runs_neg = runs[runs_sign==</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">self.runs_freqs = np.bincount(runs)</span>
        <span class="s1">self.n_runs = len(self.runs)</span>
        <span class="s1">self.n_pos = (x==</span><span class="s3">1</span><span class="s1">).sum()</span>

    <span class="s2">def </span><span class="s1">runs_test(self</span><span class="s2">, </span><span class="s1">correction=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s0">'''basic version of runs test 
 
        Parameters 
        ---------- 
        correction : bool 
            Following the SAS manual, for samplesize below 50, the test 
            statistic is corrected by 0.5. This can be turned off with 
            correction=False, and was included to match R, tseries, which 
            does not use any correction. 
 
        pvalue based on normal distribution, with integer correction 
 
        '''</span>
        <span class="s1">self.npo = npo = (self.runs_pos).sum()</span>
        <span class="s1">self.nne = nne = (self.runs_neg).sum()</span>

        <span class="s4">#n_r = self.n_runs</span>
        <span class="s1">n = npo + nne</span>
        <span class="s1">npn = npo * nne</span>
        <span class="s1">rmean = </span><span class="s3">2. </span><span class="s1">* npn / n + </span><span class="s3">1</span>
        <span class="s1">rvar = </span><span class="s3">2. </span><span class="s1">* npn * (</span><span class="s3">2.</span><span class="s1">*npn - n) / n**</span><span class="s3">2. </span><span class="s1">/ (n-</span><span class="s3">1.</span><span class="s1">)</span>
        <span class="s1">rstd = np.sqrt(rvar)</span>
        <span class="s1">rdemean = self.n_runs - rmean</span>
        <span class="s2">if </span><span class="s1">n &gt;= </span><span class="s3">50 </span><span class="s2">or not </span><span class="s1">correction:</span>
            <span class="s1">z = rdemean</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">rdemean &gt; </span><span class="s3">0.5</span><span class="s1">:</span>
                <span class="s1">z = rdemean - </span><span class="s3">0.5</span>
            <span class="s2">elif </span><span class="s1">rdemean &lt; </span><span class="s3">0.5</span><span class="s1">:</span>
                <span class="s1">z = rdemean + </span><span class="s3">0.5</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">z = </span><span class="s3">0.</span>

        <span class="s1">z /= rstd</span>
        <span class="s1">pval = </span><span class="s3">2 </span><span class="s1">* stats.norm.sf(np.abs(z))</span>
        <span class="s2">return </span><span class="s1">z</span><span class="s2">, </span><span class="s1">pval</span>

<span class="s2">def </span><span class="s1">runstest_1samp(x</span><span class="s2">, </span><span class="s1">cutoff=</span><span class="s5">'mean'</span><span class="s2">, </span><span class="s1">correction=</span><span class="s2">True</span><span class="s1">):</span>
    <span class="s0">'''use runs test on binary discretized data above/below cutoff 
 
    Parameters 
    ---------- 
    x : array_like 
        data, numeric 
    cutoff : {'mean', 'median'} or number 
        This specifies the cutoff to split the data into large and small 
        values. 
    correction : bool 
        Following the SAS manual, for samplesize below 50, the test 
        statistic is corrected by 0.5. This can be turned off with 
        correction=False, and was included to match R, tseries, which 
        does not use any correction. 
 
    Returns 
    ------- 
    z_stat : float 
        test statistic, asymptotically normally distributed 
    p-value : float 
        p-value, reject the null hypothesis if it is below an type 1 error 
        level, alpha . 
 
    '''</span>

    <span class="s1">x = array_like(x</span><span class="s2">, </span><span class="s5">&quot;x&quot;</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">cutoff == </span><span class="s5">'mean'</span><span class="s1">:</span>
        <span class="s1">cutoff = np.mean(x)</span>
    <span class="s2">elif </span><span class="s1">cutoff == </span><span class="s5">'median'</span><span class="s1">:</span>
        <span class="s1">cutoff = np.median(x)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">cutoff = float(cutoff)</span>
    <span class="s1">xindicator = (x &gt;= cutoff).astype(int)</span>
    <span class="s2">return </span><span class="s1">Runs(xindicator).runs_test(correction=correction)</span>

<span class="s2">def </span><span class="s1">runstest_2samp(x</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None, </span><span class="s1">groups=</span><span class="s2">None, </span><span class="s1">correction=</span><span class="s2">True</span><span class="s1">):</span>
    <span class="s0">'''Wald-Wolfowitz runstest for two samples 
 
    This tests whether two samples come from the same distribution. 
 
    Parameters 
    ---------- 
    x : array_like 
        data, numeric, contains either one group, if y is also given, or 
        both groups, if additionally a group indicator is provided 
    y : array_like (optional) 
        data, numeric 
    groups : array_like 
        group labels or indicator the data for both groups is given in a 
        single 1-dimensional array, x. If group labels are not [0,1], then 
    correction : bool 
        Following the SAS manual, for samplesize below 50, the test 
        statistic is corrected by 0.5. This can be turned off with 
        correction=False, and was included to match R, tseries, which 
        does not use any correction. 
 
    Returns 
    ------- 
    z_stat : float 
        test statistic, asymptotically normally distributed 
    p-value : float 
        p-value, reject the null hypothesis if it is below an type 1 error 
        level, alpha . 
 
 
    Notes 
    ----- 
    Wald-Wolfowitz runs test. 
 
    If there are ties, then then the test statistic and p-value that is 
    reported, is based on the higher p-value between sorting all tied 
    observations of the same group 
 
 
    This test is intended for continuous distributions 
    SAS has treatment for ties, but not clear, and sounds more complicated 
    (minimum and maximum possible runs prevent use of argsort) 
    (maybe it's not so difficult, idea: add small positive noise to first 
    one, run test, then to the other, run test, take max(?) p-value - DONE 
    This gives not the minimum and maximum of the number of runs, but should 
    be close. Not true, this is close to minimum but far away from maximum. 
    maximum number of runs would use alternating groups in the ties.) 
    Maybe adding random noise would be the better approach. 
 
    SAS has exact distribution for sample size &lt;=30, does not look standard 
    but should be easy to add. 
 
    currently two-sided test only 
 
    This has not been verified against a reference implementation. In a short 
    Monte Carlo simulation where both samples are normally distribute, the test 
    seems to be correctly sized for larger number of observations (30 or 
    larger), but conservative (i.e. reject less often than nominal) with a 
    sample size of 10 in each group. 
 
    See Also 
    -------- 
    runs_test_1samp 
    Runs 
    RunsProb 
 
    '''</span>
    <span class="s1">x = np.asarray(x)</span>
    <span class="s2">if </span><span class="s1">y </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">y = np.asarray(y)</span>
        <span class="s1">groups = np.concatenate((np.zeros(len(x))</span><span class="s2">, </span><span class="s1">np.ones(len(y))))</span>
        <span class="s4"># note reassigning x</span>
        <span class="s1">x = np.concatenate((x</span><span class="s2">, </span><span class="s1">y))</span>
        <span class="s1">gruni = np.arange(</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s2">elif </span><span class="s1">groups </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">gruni = np.unique(groups)</span>
        <span class="s2">if </span><span class="s1">gruni.size != </span><span class="s3">2</span><span class="s1">:  </span><span class="s4"># pylint: disable=E1103</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'not exactly two groups specified'</span><span class="s1">)</span>
        <span class="s4">#require groups to be numeric ???</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'either y or groups is necessary'</span><span class="s1">)</span>

    <span class="s1">xargsort = np.argsort(x)</span>
    <span class="s4">#check for ties</span>
    <span class="s1">x_sorted = x[xargsort]</span>
    <span class="s1">x_diff = np.diff(x_sorted)  </span><span class="s4"># used for detecting and handling ties</span>
    <span class="s2">if </span><span class="s1">x_diff.min() == </span><span class="s3">0</span><span class="s1">:</span>
        <span class="s1">print(</span><span class="s5">'ties detected'</span><span class="s1">)   </span><span class="s4">#replace with warning</span>
        <span class="s1">x_mindiff = x_diff[x_diff &gt; </span><span class="s3">0</span><span class="s1">].min()</span>
        <span class="s1">eps = x_mindiff/</span><span class="s3">2.</span>
        <span class="s1">xx = x.copy()  </span><span class="s4">#do not change original, just in case</span>

        <span class="s1">xx[groups==gruni[</span><span class="s3">0</span><span class="s1">]] += eps</span>
        <span class="s1">xargsort = np.argsort(xx)</span>
        <span class="s1">xindicator = groups[xargsort]</span>
        <span class="s1">z0</span><span class="s2">, </span><span class="s1">p0 = Runs(xindicator).runs_test(correction=correction)</span>

        <span class="s1">xx[groups==gruni[</span><span class="s3">0</span><span class="s1">]] -= eps   </span><span class="s4">#restore xx = x</span>
        <span class="s1">xx[groups==gruni[</span><span class="s3">1</span><span class="s1">]] += eps</span>
        <span class="s1">xargsort = np.argsort(xx)</span>
        <span class="s1">xindicator = groups[xargsort]</span>
        <span class="s1">z1</span><span class="s2">, </span><span class="s1">p1 = Runs(xindicator).runs_test(correction=correction)</span>

        <span class="s1">idx = np.argmax([p0</span><span class="s2">,</span><span class="s1">p1])</span>
        <span class="s2">return </span><span class="s1">[z0</span><span class="s2">, </span><span class="s1">z1][idx]</span><span class="s2">, </span><span class="s1">[p0</span><span class="s2">, </span><span class="s1">p1][idx]</span>

    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">xindicator = groups[xargsort]</span>
        <span class="s2">return </span><span class="s1">Runs(xindicator).runs_test(correction=correction)</span>


<span class="s2">class </span><span class="s1">TotalRunsProb:</span>
    <span class="s0">'''class for the probability distribution of total runs 
 
    This is the exact probability distribution for the (Wald-Wolfowitz) 
    runs test. The random variable is the total number of runs if the 
    sample has (n0, n1) observations of groups 0 and 1. 
 
 
    Notes 
    ----- 
    Written as a class so I can store temporary calculations, but I do not 
    think it matters much. 
 
    Formulas taken from SAS manual for one-sided significance level. 
 
    Could be converted to a full univariate distribution, subclassing 
    scipy.stats.distributions. 
 
    *Status* 
    Not verified yet except for mean. 
 
 
 
    '''</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">n0</span><span class="s2">, </span><span class="s1">n1):</span>
        <span class="s1">self.n0 = n0</span>
        <span class="s1">self.n1 = n1</span>
        <span class="s1">self.n = n = n0 + n1</span>
        <span class="s1">self.comball = comb(n</span><span class="s2">, </span><span class="s1">n1)</span>

    <span class="s2">def </span><span class="s1">runs_prob_even(self</span><span class="s2">, </span><span class="s1">r):</span>
        <span class="s1">n0</span><span class="s2">, </span><span class="s1">n1 = self.n0</span><span class="s2">, </span><span class="s1">self.n1</span>
        <span class="s1">tmp0 = comb(n0-</span><span class="s3">1</span><span class="s2">, </span><span class="s1">r//</span><span class="s3">2</span><span class="s1">-</span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">tmp1 = comb(n1-</span><span class="s3">1</span><span class="s2">, </span><span class="s1">r//</span><span class="s3">2</span><span class="s1">-</span><span class="s3">1</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">tmp0 * tmp1 * </span><span class="s3">2. </span><span class="s1">/ self.comball</span>

    <span class="s2">def </span><span class="s1">runs_prob_odd(self</span><span class="s2">, </span><span class="s1">r):</span>
        <span class="s1">n0</span><span class="s2">, </span><span class="s1">n1 = self.n0</span><span class="s2">, </span><span class="s1">self.n1</span>
        <span class="s1">k = (r+</span><span class="s3">1</span><span class="s1">)//</span><span class="s3">2</span>
        <span class="s1">tmp0 = comb(n0-</span><span class="s3">1</span><span class="s2">, </span><span class="s1">k-</span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">tmp1 = comb(n1-</span><span class="s3">1</span><span class="s2">, </span><span class="s1">k-</span><span class="s3">2</span><span class="s1">)</span>
        <span class="s1">tmp3 = comb(n0-</span><span class="s3">1</span><span class="s2">, </span><span class="s1">k-</span><span class="s3">2</span><span class="s1">)</span>
        <span class="s1">tmp4 = comb(n1-</span><span class="s3">1</span><span class="s2">, </span><span class="s1">k-</span><span class="s3">1</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">(tmp0 * tmp1 + tmp3 * tmp4)  / self.comball</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">r):</span>
        <span class="s1">r = np.asarray(r)</span>
        <span class="s1">r_isodd = np.mod(r</span><span class="s2">, </span><span class="s3">2</span><span class="s1">) &gt; </span><span class="s3">0</span>
        <span class="s1">r_odd = r[r_isodd]</span>
        <span class="s1">r_even = r[~r_isodd]</span>
        <span class="s1">runs_pdf = np.zeros(r.shape)</span>
        <span class="s1">runs_pdf[r_isodd] = self.runs_prob_odd(r_odd)</span>
        <span class="s1">runs_pdf[~r_isodd] = self.runs_prob_even(r_even)</span>
        <span class="s2">return </span><span class="s1">runs_pdf</span>


    <span class="s2">def </span><span class="s1">cdf(self</span><span class="s2">, </span><span class="s1">r):</span>
        <span class="s1">r_ = np.arange(</span><span class="s3">2</span><span class="s2">,</span><span class="s1">r+</span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">cdfval = self.runs_prob_even(r_[::</span><span class="s3">2</span><span class="s1">]).sum()</span>
        <span class="s1">cdfval += self.runs_prob_odd(r_[</span><span class="s3">1</span><span class="s1">::</span><span class="s3">2</span><span class="s1">]).sum()</span>
        <span class="s2">return </span><span class="s1">cdfval</span>


<span class="s2">class </span><span class="s1">RunsProb:</span>
    <span class="s0">'''distribution of success runs of length k or more (classical definition) 
 
    The underlying process is assumed to be a sequence of Bernoulli trials 
    of a given length n. 
 
    not sure yet, how to interpret or use the distribution for runs 
    of length k or more. 
 
    Musseli also has longest success run, and waiting time distribution 
    negative binomial of order k and geometric of order k 
 
    need to compare with Godpole 
 
    need a MonteCarlo function to do some quick tests before doing more 
 
 
    '''</span>



    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p):</span>
        <span class="s0">'''distribution of success runs of length k or more 
 
        Parameters 
        ---------- 
        x : float 
            count of runs of length n 
        k : int 
            length of runs 
        n : int 
            total number of observations or trials 
        p : float 
            probability of success in each Bernoulli trial 
 
        Returns 
        ------- 
        pdf : float 
            probability that x runs of length of k are observed 
 
        Notes 
        ----- 
        not yet vectorized 
 
        References 
        ---------- 
        Muselli 1996, theorem 3 
        '''</span>

        <span class="s1">q = </span><span class="s3">1</span><span class="s1">-p</span>
        <span class="s1">m = np.arange(x</span><span class="s2">, </span><span class="s1">(n+</span><span class="s3">1</span><span class="s1">)//(k+</span><span class="s3">1</span><span class="s1">)+</span><span class="s3">1</span><span class="s1">)[:</span><span class="s2">,None</span><span class="s1">]</span>
        <span class="s1">terms = (-</span><span class="s3">1</span><span class="s1">)**(m-x) * comb(m</span><span class="s2">, </span><span class="s1">x) * p**(m*k) * q**(m-</span><span class="s3">1</span><span class="s1">) \</span>
                <span class="s1">* (comb(n - m*k</span><span class="s2">, </span><span class="s1">m - </span><span class="s3">1</span><span class="s1">) + q * comb(n - m*k</span><span class="s2">, </span><span class="s1">m))</span>
        <span class="s2">return </span><span class="s1">terms.sum(</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">pdf_nb(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p):</span>
        <span class="s2">pass</span>
        <span class="s4">#y = np.arange(m-1, n-mk+1</span>

<span class="s5">''' 
&gt;&gt;&gt; [np.sum([RunsProb().pdf(xi, k, 16, 10/16.) for xi in range(0,16)]) for k in range(16)] 
[0.99999332193894064, 0.99999999999999367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] 
&gt;&gt;&gt; [(np.arange(0,16) * [RunsProb().pdf(xi, k, 16, 10/16.) for xi in range(0,16)]).sum() for k in range(16)] 
[6.9998931510341809, 4.1406249999999929, 2.4414062500000075, 1.4343261718749996, 0.83923339843749856, 0.48875808715820324, 0.28312206268310569, 0.1629814505577086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] 
&gt;&gt;&gt; np.array([(np.arange(0,16) * [RunsProb().pdf(xi, k, 16, 10/16.) for xi in range(0,16)]).sum() for k in range(16)])/11 
array([ 0.63635392,  0.37642045,  0.22194602,  0.13039329,  0.07629395, 
        0.04443255,  0.02573837,  0.0148165 ,  0.        ,  0.        , 
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ]) 
&gt;&gt;&gt; np.diff([(np.arange(0,16) * [RunsProb().pdf(xi, k, 16, 10/16.) for xi in range(0,16)]).sum() for k in range(16)][::-1]) 
array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        , 
        0.        ,  0.        ,  0.16298145,  0.12014061,  0.20563602, 
        0.35047531,  0.59509277,  1.00708008,  1.69921875,  2.85926815]) 
'''</span>



<span class="s2">def </span><span class="s1">median_test_ksample(x</span><span class="s2">, </span><span class="s1">groups):</span>
    <span class="s0">'''chisquare test for equality of median/location 
 
    This tests whether all groups have the same fraction of observations 
    above the median. 
 
    Parameters 
    ---------- 
    x : array_like 
        data values stacked for all groups 
    groups : array_like 
        group labels or indicator 
 
    Returns 
    ------- 
    stat : float 
       test statistic 
    pvalue : float 
       pvalue from the chisquare distribution 
    others ???? 
       currently some test output, table and expected 
 
    '''</span>
    <span class="s1">x = np.asarray(x)</span>
    <span class="s1">gruni = np.unique(groups)</span>
    <span class="s1">xli = [x[groups==group] </span><span class="s2">for </span><span class="s1">group </span><span class="s2">in </span><span class="s1">gruni]</span>
    <span class="s1">xmedian = np.median(x)</span>
    <span class="s1">counts_larger = np.array([(xg &gt; xmedian).sum() </span><span class="s2">for </span><span class="s1">xg </span><span class="s2">in </span><span class="s1">xli])</span>
    <span class="s1">counts = np.array([len(xg) </span><span class="s2">for </span><span class="s1">xg </span><span class="s2">in </span><span class="s1">xli])</span>
    <span class="s1">counts_smaller = counts - counts_larger</span>
    <span class="s1">nobs = counts.sum()</span>
    <span class="s1">n_larger = (x &gt; xmedian).sum()</span>
    <span class="s1">n_smaller = nobs - n_larger</span>
    <span class="s1">table = np.vstack((counts_smaller</span><span class="s2">, </span><span class="s1">counts_larger))</span>

    <span class="s4">#the following should be replaced by chisquare_contingency table</span>
    <span class="s1">expected = np.vstack((counts * </span><span class="s3">1. </span><span class="s1">/ nobs * n_smaller</span><span class="s2">,</span>
                          <span class="s1">counts * </span><span class="s3">1. </span><span class="s1">/ nobs * n_larger))</span>

    <span class="s2">if </span><span class="s1">(expected &lt; </span><span class="s3">5</span><span class="s1">).any():</span>
        <span class="s1">print(</span><span class="s5">'Warning: There are cells with less than 5 expected' </span><span class="s1">\</span>
        <span class="s5">'observations. The chisquare distribution might not be a good' </span><span class="s1">\</span>
        <span class="s5">'approximation for the true distribution.'</span><span class="s1">)</span>

    <span class="s4">#check ddof</span>
    <span class="s2">return </span><span class="s1">stats.chisquare(table.ravel()</span><span class="s2">, </span><span class="s1">expected.ravel()</span><span class="s2">, </span><span class="s1">ddof=</span><span class="s3">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">table</span><span class="s2">, </span><span class="s1">expected</span>




<span class="s2">def </span><span class="s1">cochrans_q(x):</span>
    <span class="s0">'''Cochran's Q test for identical effect of k treatments 
 
    Cochran's Q is a k-sample extension of the McNemar test. If there are only 
    two treatments, then Cochran's Q test and McNemar test are equivalent. 
 
    Test that the probability of success is the same for each treatment. 
    The alternative is that at least two treatments have a different 
    probability of success. 
 
    Parameters 
    ---------- 
    x : array_like, 2d (N,k) 
        data with N cases and k variables 
 
    Returns 
    ------- 
    q_stat : float 
       test statistic 
    pvalue : float 
       pvalue from the chisquare distribution 
 
    Notes 
    ----- 
    In Wikipedia terminology, rows are blocks and columns are treatments. 
    The number of rows N, should be large for the chisquare distribution to be 
    a good approximation. 
    The Null hypothesis of the test is that all treatments have the 
    same effect. 
 
    References 
    ---------- 
    https://en.wikipedia.org/wiki/Cochran_test 
    SAS Manual for NPAR TESTS 
 
    '''</span>

    <span class="s1">warnings.warn(</span><span class="s5">&quot;Deprecated, use stats.cochrans_q instead&quot;</span><span class="s2">, </span><span class="s1">FutureWarning)</span>

    <span class="s1">x = np.asarray(x)</span>
    <span class="s1">gruni = np.unique(x)</span>
    <span class="s1">N</span><span class="s2">, </span><span class="s1">k = x.shape</span>
    <span class="s1">count_row_success = (x==gruni[-</span><span class="s3">1</span><span class="s1">]).sum(</span><span class="s3">1</span><span class="s2">, </span><span class="s1">float)</span>
    <span class="s1">count_col_success = (x==gruni[-</span><span class="s3">1</span><span class="s1">]).sum(</span><span class="s3">0</span><span class="s2">, </span><span class="s1">float)</span>
    <span class="s1">count_row_ss = count_row_success.sum()</span>
    <span class="s1">count_col_ss = count_col_success.sum()</span>
    <span class="s2">assert </span><span class="s1">count_row_ss == count_col_ss  </span><span class="s4">#just a calculation check</span>


    <span class="s4">#this is SAS manual</span>
    <span class="s1">q_stat = (k-</span><span class="s3">1</span><span class="s1">) * (k *  np.sum(count_col_success**</span><span class="s3">2</span><span class="s1">) - count_col_ss**</span><span class="s3">2</span><span class="s1">) \</span>
             <span class="s1">/ (k * count_row_ss - np.sum(count_row_success**</span><span class="s3">2</span><span class="s1">))</span>

    <span class="s4">#Note: the denominator looks just like k times the variance of the</span>
    <span class="s4">#columns</span>

    <span class="s4">#Wikipedia uses a different, but equivalent expression</span>
<span class="s4">##    q_stat = (k-1) * (k *  np.sum(count_row_success**2) - count_row_ss**2) \</span>
<span class="s4">##             / (k * count_col_ss - np.sum(count_col_success**2))</span>

    <span class="s2">return </span><span class="s1">q_stat</span><span class="s2">, </span><span class="s1">stats.chi2.sf(q_stat</span><span class="s2">, </span><span class="s1">k-</span><span class="s3">1</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">mcnemar(x</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None, </span><span class="s1">exact=</span><span class="s2">True, </span><span class="s1">correction=</span><span class="s2">True</span><span class="s1">):</span>
    <span class="s0">'''McNemar test 
 
    Parameters 
    ---------- 
    x, y : array_like 
        two paired data samples. If y is None, then x can be a 2 by 2 
        contingency table. x and y can have more than one dimension, then 
        the results are calculated under the assumption that axis zero 
        contains the observation for the samples. 
    exact : bool 
        If exact is true, then the binomial distribution will be used. 
        If exact is false, then the chisquare distribution will be used, which 
        is the approximation to the distribution of the test statistic for 
        large sample sizes. 
    correction : bool 
        If true, then a continuity correction is used for the chisquare 
        distribution (if exact is false.) 
 
    Returns 
    ------- 
    stat : float or int, array 
        The test statistic is the chisquare statistic if exact is false. If the 
        exact binomial distribution is used, then this contains the min(n1, n2), 
        where n1, n2 are cases that are zero in one sample but one in the other 
        sample. 
 
    pvalue : float or array 
        p-value of the null hypothesis of equal effects. 
 
    Notes 
    ----- 
    This is a special case of Cochran's Q test. The results when the chisquare 
    distribution is used are identical, except for continuity correction. 
 
    '''</span>

    <span class="s1">warnings.warn(</span><span class="s5">&quot;Deprecated, use stats.TableSymmetry instead&quot;</span><span class="s2">, </span><span class="s1">FutureWarning)</span>

    <span class="s1">x = np.asarray(x)</span>
    <span class="s2">if </span><span class="s1">y </span><span class="s2">is None and </span><span class="s1">x.shape[</span><span class="s3">0</span><span class="s1">] == x.shape[</span><span class="s3">1</span><span class="s1">]:</span>
        <span class="s2">if </span><span class="s1">x.shape[</span><span class="s3">0</span><span class="s1">] != </span><span class="s3">2</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'table needs to be 2 by 2'</span><span class="s1">)</span>
        <span class="s1">n1</span><span class="s2">, </span><span class="s1">n2 = x[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">x[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s4"># I'm not checking here whether x and y are binary,</span>
        <span class="s4"># is not this also paired sign test</span>
        <span class="s1">n1 = np.sum(x &lt; y</span><span class="s2">, </span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">n2 = np.sum(x &gt; y</span><span class="s2">, </span><span class="s3">0</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">exact:</span>
        <span class="s1">stat = np.minimum(n1</span><span class="s2">, </span><span class="s1">n2)</span>
        <span class="s4"># binom is symmetric with p=0.5</span>
        <span class="s1">pval = stats.binom.cdf(stat</span><span class="s2">, </span><span class="s1">n1 + n2</span><span class="s2">, </span><span class="s3">0.5</span><span class="s1">) * </span><span class="s3">2</span>
        <span class="s1">pval = np.minimum(pval</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)  </span><span class="s4"># limit to 1 if n1==n2</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">corr = int(correction) </span><span class="s4"># convert bool to 0 or 1</span>
        <span class="s1">stat = (np.abs(n1 - n2) - corr)**</span><span class="s3">2 </span><span class="s1">/ (</span><span class="s3">1. </span><span class="s1">* (n1 + n2))</span>
        <span class="s1">df = </span><span class="s3">1</span>
        <span class="s1">pval = stats.chi2.sf(stat</span><span class="s2">, </span><span class="s1">df)</span>
    <span class="s2">return </span><span class="s1">stat</span><span class="s2">, </span><span class="s1">pval</span>


<span class="s2">def </span><span class="s1">symmetry_bowker(table):</span>
    <span class="s0">'''Test for symmetry of a (k, k) square contingency table 
 
    This is an extension of the McNemar test to test the Null hypothesis 
    that the contingency table is symmetric around the main diagonal, that is 
 
    n_{i, j} = n_{j, i}  for all i, j 
 
    Parameters 
    ---------- 
    table : array_like, 2d, (k, k) 
        a square contingency table that contains the count for k categories 
        in rows and columns. 
 
    Returns 
    ------- 
    statistic : float 
        chisquare test statistic 
    p-value : float 
        p-value of the test statistic based on chisquare distribution 
    df : int 
        degrees of freedom of the chisquare distribution 
 
    Notes 
    ----- 
    Implementation is based on the SAS documentation, R includes it in 
    `mcnemar.test` if the table is not 2 by 2. 
 
    The pvalue is based on the chisquare distribution which requires that the 
    sample size is not very small to be a good approximation of the true 
    distribution. For 2x2 contingency tables exact distribution can be 
    obtained with `mcnemar` 
 
    See Also 
    -------- 
    mcnemar 
 
 
    '''</span>

    <span class="s1">warnings.warn(</span><span class="s5">&quot;Deprecated, use stats.TableSymmetry instead&quot;</span><span class="s2">, </span><span class="s1">FutureWarning)</span>

    <span class="s1">table = np.asarray(table)</span>
    <span class="s1">k</span><span class="s2">, </span><span class="s1">k2 = table.shape</span>
    <span class="s2">if </span><span class="s1">k != k2:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'table needs to be square'</span><span class="s1">)</span>

    <span class="s4">#low_idx = np.tril_indices(k, -1)  # this does not have Fortran order</span>
    <span class="s1">upp_idx = np.triu_indices(k</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">tril = table.T[upp_idx]   </span><span class="s4"># lower triangle in column order</span>
    <span class="s1">triu = table[upp_idx]     </span><span class="s4"># upper triangle in row order</span>

    <span class="s1">stat = ((tril - triu)**</span><span class="s3">2 </span><span class="s1">/ (tril + triu + </span><span class="s3">1e-20</span><span class="s1">)).sum()</span>
    <span class="s1">df = k * (k-</span><span class="s3">1</span><span class="s1">) / </span><span class="s3">2.</span>
    <span class="s1">pval = stats.chi2.sf(stat</span><span class="s2">, </span><span class="s1">df)</span>

    <span class="s2">return </span><span class="s1">stat</span><span class="s2">, </span><span class="s1">pval</span><span class="s2">, </span><span class="s1">df</span>


<span class="s2">if </span><span class="s1">__name__ == </span><span class="s5">'__main__'</span><span class="s1">:</span>

    <span class="s1">x1 = np.array([</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s1">])</span>

    <span class="s1">print(Runs(x1).runs_test())</span>
    <span class="s1">print(runstest_1samp(x1</span><span class="s2">, </span><span class="s1">cutoff=</span><span class="s5">'mean'</span><span class="s1">))</span>
    <span class="s1">print(runstest_2samp(np.arange(</span><span class="s3">16</span><span class="s2">,</span><span class="s3">0</span><span class="s2">,</span><span class="s1">-</span><span class="s3">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">groups=x1))</span>
    <span class="s1">print(TotalRunsProb(</span><span class="s3">7</span><span class="s2">,</span><span class="s3">9</span><span class="s1">).cdf(</span><span class="s3">11</span><span class="s1">))</span>
    <span class="s1">print(median_test_ksample(np.random.randn(</span><span class="s3">100</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.random.randint(</span><span class="s3">0</span><span class="s2">,</span><span class="s3">2</span><span class="s2">,</span><span class="s3">100</span><span class="s1">)))</span>
    <span class="s1">print(cochrans_q(np.random.randint(</span><span class="s3">0</span><span class="s2">,</span><span class="s3">2</span><span class="s2">,</span><span class="s1">(</span><span class="s3">100</span><span class="s2">,</span><span class="s3">8</span><span class="s1">))))</span>
</pre>
</body>
</html>