<html>
<head>
<title>dummy.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
dummy.py</font>
</center></td></tr></table>
<pre><span class="s0"># Author: Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span>
<span class="s0">#         Arnaud Joly &lt;a.joly@ulg.ac.be&gt;</span>
<span class="s0">#         Maheshakya Wijewardena &lt;maheshakya.10@cse.mrt.ac.lk&gt;</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">numbers </span><span class="s2">import </span><span class="s1">Integral</span><span class="s2">, </span><span class="s1">Real</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">scipy.sparse </span><span class="s2">as </span><span class="s1">sp</span>

<span class="s2">from </span><span class="s1">.base </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">BaseEstimator</span><span class="s2">,</span>
    <span class="s1">ClassifierMixin</span><span class="s2">,</span>
    <span class="s1">MultiOutputMixin</span><span class="s2">,</span>
    <span class="s1">RegressorMixin</span><span class="s2">,</span>
    <span class="s1">_fit_context</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">.utils </span><span class="s2">import </span><span class="s1">check_random_state</span>
<span class="s2">from </span><span class="s1">.utils._param_validation </span><span class="s2">import </span><span class="s1">Interval</span><span class="s2">, </span><span class="s1">StrOptions</span>
<span class="s2">from </span><span class="s1">.utils.multiclass </span><span class="s2">import </span><span class="s1">class_distribution</span>
<span class="s2">from </span><span class="s1">.utils.random </span><span class="s2">import </span><span class="s1">_random_choice_csc</span>
<span class="s2">from </span><span class="s1">.utils.stats </span><span class="s2">import </span><span class="s1">_weighted_percentile</span>
<span class="s2">from </span><span class="s1">.utils.validation </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">_check_sample_weight</span><span class="s2">,</span>
    <span class="s1">_num_samples</span><span class="s2">,</span>
    <span class="s1">check_array</span><span class="s2">,</span>
    <span class="s1">check_consistent_length</span><span class="s2">,</span>
    <span class="s1">check_is_fitted</span><span class="s2">,</span>
<span class="s1">)</span>


<span class="s2">class </span><span class="s1">DummyClassifier(MultiOutputMixin</span><span class="s2">, </span><span class="s1">ClassifierMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
    <span class="s3">&quot;&quot;&quot;DummyClassifier makes predictions that ignore the input features. 
 
    This classifier serves as a simple baseline to compare against other more 
    complex classifiers. 
 
    The specific behavior of the baseline is selected with the `strategy` 
    parameter. 
 
    All strategies make predictions that ignore the input feature values passed 
    as the `X` argument to `fit` and `predict`. The predictions, however, 
    typically depend on values observed in the `y` parameter passed to `fit`. 
 
    Note that the &quot;stratified&quot; and &quot;uniform&quot; strategies lead to 
    non-deterministic predictions that can be rendered deterministic by setting 
    the `random_state` parameter if needed. The other strategies are naturally 
    deterministic and, once fit, always return the same constant prediction 
    for any value of `X`. 
 
    Read more in the :ref:`User Guide &lt;dummy_estimators&gt;`. 
 
    .. versionadded:: 0.13 
 
    Parameters 
    ---------- 
    strategy : {&quot;most_frequent&quot;, &quot;prior&quot;, &quot;stratified&quot;, &quot;uniform&quot;, \ 
            &quot;constant&quot;}, default=&quot;prior&quot; 
        Strategy to use to generate predictions. 
 
        * &quot;most_frequent&quot;: the `predict` method always returns the most 
          frequent class label in the observed `y` argument passed to `fit`. 
          The `predict_proba` method returns the matching one-hot encoded 
          vector. 
        * &quot;prior&quot;: the `predict` method always returns the most frequent 
          class label in the observed `y` argument passed to `fit` (like 
          &quot;most_frequent&quot;). ``predict_proba`` always returns the empirical 
          class distribution of `y` also known as the empirical class prior 
          distribution. 
        * &quot;stratified&quot;: the `predict_proba` method randomly samples one-hot 
          vectors from a multinomial distribution parametrized by the empirical 
          class prior probabilities. 
          The `predict` method returns the class label which got probability 
          one in the one-hot vector of `predict_proba`. 
          Each sampled row of both methods is therefore independent and 
          identically distributed. 
        * &quot;uniform&quot;: generates predictions uniformly at random from the list 
          of unique classes observed in `y`, i.e. each class has equal 
          probability. 
        * &quot;constant&quot;: always predicts a constant label that is provided by 
          the user. This is useful for metrics that evaluate a non-majority 
          class. 
 
          .. versionchanged:: 0.24 
             The default value of `strategy` has changed to &quot;prior&quot; in version 
             0.24. 
 
    random_state : int, RandomState instance or None, default=None 
        Controls the randomness to generate the predictions when 
        ``strategy='stratified'`` or ``strategy='uniform'``. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    constant : int or str or array-like of shape (n_outputs,), default=None 
        The explicit constant as predicted by the &quot;constant&quot; strategy. This 
        parameter is useful only for the &quot;constant&quot; strategy. 
 
    Attributes 
    ---------- 
    classes_ : ndarray of shape (n_classes,) or list of such arrays 
        Unique class labels observed in `y`. For multi-output classification 
        problems, this attribute is a list of arrays as each output has an 
        independent set of possible classes. 
 
    n_classes_ : int or list of int 
        Number of label for each output. 
 
    class_prior_ : ndarray of shape (n_classes,) or list of such arrays 
        Frequency of each class observed in `y`. For multioutput classification 
        problems, this is computed independently for each output. 
 
    n_outputs_ : int 
        Number of outputs. 
 
    sparse_output_ : bool 
        True if the array returned from predict is to be in sparse CSC format. 
        Is automatically set to True if the input `y` is passed in sparse 
        format. 
 
    See Also 
    -------- 
    DummyRegressor : Regressor that makes predictions using simple rules. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.dummy import DummyClassifier 
    &gt;&gt;&gt; X = np.array([-1, 1, 1, 1]) 
    &gt;&gt;&gt; y = np.array([0, 1, 1, 1]) 
    &gt;&gt;&gt; dummy_clf = DummyClassifier(strategy=&quot;most_frequent&quot;) 
    &gt;&gt;&gt; dummy_clf.fit(X, y) 
    DummyClassifier(strategy='most_frequent') 
    &gt;&gt;&gt; dummy_clf.predict(X) 
    array([1, 1, 1, 1]) 
    &gt;&gt;&gt; dummy_clf.score(X, y) 
    0.75 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;strategy&quot;</span><span class="s1">: [</span>
            <span class="s1">StrOptions({</span><span class="s4">&quot;most_frequent&quot;</span><span class="s2">, </span><span class="s4">&quot;prior&quot;</span><span class="s2">, </span><span class="s4">&quot;stratified&quot;</span><span class="s2">, </span><span class="s4">&quot;uniform&quot;</span><span class="s2">, </span><span class="s4">&quot;constant&quot;</span><span class="s1">})</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s4">&quot;constant&quot;</span><span class="s1">: [Integral</span><span class="s2">, </span><span class="s1">str</span><span class="s2">, </span><span class="s4">&quot;array-like&quot;</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">strategy=</span><span class="s4">&quot;prior&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None, </span><span class="s1">constant=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self.strategy = strategy</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.constant = constant</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s3">&quot;&quot;&quot;Fit the baseline classifier. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training data. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_outputs) 
            Target values. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">self._strategy = self.strategy</span>

        <span class="s2">if </span><span class="s1">self._strategy == </span><span class="s4">&quot;uniform&quot; </span><span class="s2">and </span><span class="s1">sp.issparse(y):</span>
            <span class="s1">y = y.toarray()</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s4">&quot;A local copy of the target data has been converted &quot;</span>
                    <span class="s4">&quot;to a numpy array. Predicting on sparse target data &quot;</span>
                    <span class="s4">&quot;with the uniform strategy would not save memory &quot;</span>
                    <span class="s4">&quot;and would be slower.&quot;</span>
                <span class="s1">)</span><span class="s2">,</span>
                <span class="s1">UserWarning</span><span class="s2">,</span>
            <span class="s1">)</span>

        <span class="s1">self.sparse_output_ = sp.issparse(y)</span>

        <span class="s2">if not </span><span class="s1">self.sparse_output_:</span>
            <span class="s1">y = np.asarray(y)</span>
            <span class="s1">y = np.atleast_1d(y)</span>

        <span class="s2">if </span><span class="s1">y.ndim == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">y = np.reshape(y</span><span class="s2">, </span><span class="s1">(-</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">))</span>

        <span class="s1">self.n_outputs_ = y.shape[</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">check_consistent_length(X</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s2">, </span><span class="s1">X)</span>

        <span class="s2">if </span><span class="s1">self._strategy == </span><span class="s4">&quot;constant&quot;</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">self.constant </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;Constant target value has to be specified &quot;</span>
                    <span class="s4">&quot;when the constant strategy is used.&quot;</span>
                <span class="s1">)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">constant = np.reshape(np.atleast_1d(self.constant)</span><span class="s2">, </span><span class="s1">(-</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">))</span>
                <span class="s2">if </span><span class="s1">constant.shape[</span><span class="s5">0</span><span class="s1">] != self.n_outputs_:</span>
                    <span class="s2">raise </span><span class="s1">ValueError(</span>
                        <span class="s4">&quot;Constant target value should have shape (%d, 1).&quot;</span>
                        <span class="s1">% self.n_outputs_</span>
                    <span class="s1">)</span>

        <span class="s1">(self.classes_</span><span class="s2">, </span><span class="s1">self.n_classes_</span><span class="s2">, </span><span class="s1">self.class_prior_) = class_distribution(</span>
            <span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight</span>
        <span class="s1">)</span>

        <span class="s2">if </span><span class="s1">self._strategy == </span><span class="s4">&quot;constant&quot;</span><span class="s1">:</span>
            <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(self.n_outputs_):</span>
                <span class="s2">if not </span><span class="s1">any(constant[k][</span><span class="s5">0</span><span class="s1">] == c </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">self.classes_[k]):</span>
                    <span class="s0"># Checking in case of constant strategy if the constant</span>
                    <span class="s0"># provided by the user is in y.</span>
                    <span class="s1">err_msg = (</span>
                        <span class="s4">&quot;The constant target value must be present in &quot;</span>
                        <span class="s4">&quot;the training data. You provided constant={}. &quot;</span>
                        <span class="s4">&quot;Possible values are: {}.&quot;</span><span class="s1">.format(</span>
                            <span class="s1">self.constant</span><span class="s2">, </span><span class="s1">self.classes_[k].tolist()</span>
                        <span class="s1">)</span>
                    <span class="s1">)</span>
                    <span class="s2">raise </span><span class="s1">ValueError(err_msg)</span>

        <span class="s2">if </span><span class="s1">self.n_outputs_ == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">self.n_classes_ = self.n_classes_[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">self.classes_ = self.classes_[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">self.class_prior_ = self.class_prior_[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s3">&quot;&quot;&quot;Perform classification on test vectors X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Test data. 
 
        Returns 
        ------- 
        y : array-like of shape (n_samples,) or (n_samples, n_outputs) 
            Predicted target values for X. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s0"># numpy random_state expects Python int and not long as size argument</span>
        <span class="s0"># under Windows</span>
        <span class="s1">n_samples = _num_samples(X)</span>
        <span class="s1">rs = check_random_state(self.random_state)</span>

        <span class="s1">n_classes_ = self.n_classes_</span>
        <span class="s1">classes_ = self.classes_</span>
        <span class="s1">class_prior_ = self.class_prior_</span>
        <span class="s1">constant = self.constant</span>
        <span class="s2">if </span><span class="s1">self.n_outputs_ == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s0"># Get same type even for self.n_outputs_ == 1</span>
            <span class="s1">n_classes_ = [n_classes_]</span>
            <span class="s1">classes_ = [classes_]</span>
            <span class="s1">class_prior_ = [class_prior_]</span>
            <span class="s1">constant = [constant]</span>
        <span class="s0"># Compute probability only once</span>
        <span class="s2">if </span><span class="s1">self._strategy == </span><span class="s4">&quot;stratified&quot;</span><span class="s1">:</span>
            <span class="s1">proba = self.predict_proba(X)</span>
            <span class="s2">if </span><span class="s1">self.n_outputs_ == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">proba = [proba]</span>

        <span class="s2">if </span><span class="s1">self.sparse_output_:</span>
            <span class="s1">class_prob = </span><span class="s2">None</span>
            <span class="s2">if </span><span class="s1">self._strategy </span><span class="s2">in </span><span class="s1">(</span><span class="s4">&quot;most_frequent&quot;</span><span class="s2">, </span><span class="s4">&quot;prior&quot;</span><span class="s1">):</span>
                <span class="s1">classes_ = [np.array([cp.argmax()]) </span><span class="s2">for </span><span class="s1">cp </span><span class="s2">in </span><span class="s1">class_prior_]</span>

            <span class="s2">elif </span><span class="s1">self._strategy == </span><span class="s4">&quot;stratified&quot;</span><span class="s1">:</span>
                <span class="s1">class_prob = class_prior_</span>

            <span class="s2">elif </span><span class="s1">self._strategy == </span><span class="s4">&quot;uniform&quot;</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;Sparse target prediction is not &quot;</span>
                    <span class="s4">&quot;supported with the uniform strategy&quot;</span>
                <span class="s1">)</span>

            <span class="s2">elif </span><span class="s1">self._strategy == </span><span class="s4">&quot;constant&quot;</span><span class="s1">:</span>
                <span class="s1">classes_ = [np.array([c]) </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">constant]</span>

            <span class="s1">y = _random_choice_csc(n_samples</span><span class="s2">, </span><span class="s1">classes_</span><span class="s2">, </span><span class="s1">class_prob</span><span class="s2">, </span><span class="s1">self.random_state)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">self._strategy </span><span class="s2">in </span><span class="s1">(</span><span class="s4">&quot;most_frequent&quot;</span><span class="s2">, </span><span class="s4">&quot;prior&quot;</span><span class="s1">):</span>
                <span class="s1">y = np.tile(</span>
                    <span class="s1">[</span>
                        <span class="s1">classes_[k][class_prior_[k].argmax()]</span>
                        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(self.n_outputs_)</span>
                    <span class="s1">]</span><span class="s2">,</span>
                    <span class="s1">[n_samples</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]</span><span class="s2">,</span>
                <span class="s1">)</span>

            <span class="s2">elif </span><span class="s1">self._strategy == </span><span class="s4">&quot;stratified&quot;</span><span class="s1">:</span>
                <span class="s1">y = np.vstack(</span>
                    <span class="s1">[</span>
                        <span class="s1">classes_[k][proba[k].argmax(axis=</span><span class="s5">1</span><span class="s1">)]</span>
                        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(self.n_outputs_)</span>
                    <span class="s1">]</span>
                <span class="s1">).T</span>

            <span class="s2">elif </span><span class="s1">self._strategy == </span><span class="s4">&quot;uniform&quot;</span><span class="s1">:</span>
                <span class="s1">ret = [</span>
                    <span class="s1">classes_[k][rs.randint(n_classes_[k]</span><span class="s2">, </span><span class="s1">size=n_samples)]</span>
                    <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(self.n_outputs_)</span>
                <span class="s1">]</span>
                <span class="s1">y = np.vstack(ret).T</span>

            <span class="s2">elif </span><span class="s1">self._strategy == </span><span class="s4">&quot;constant&quot;</span><span class="s1">:</span>
                <span class="s1">y = np.tile(self.constant</span><span class="s2">, </span><span class="s1">(n_samples</span><span class="s2">, </span><span class="s5">1</span><span class="s1">))</span>

            <span class="s2">if </span><span class="s1">self.n_outputs_ == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">y = np.ravel(y)</span>

        <span class="s2">return </span><span class="s1">y</span>

    <span class="s2">def </span><span class="s1">predict_proba(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s3">&quot;&quot;&quot; 
        Return probability estimates for the test vectors X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Test data. 
 
        Returns 
        ------- 
        P : ndarray of shape (n_samples, n_classes) or list of such arrays 
            Returns the probability of the sample for each class in 
            the model, where classes are ordered arithmetically, for each 
            output. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s0"># numpy random_state expects Python int and not long as size argument</span>
        <span class="s0"># under Windows</span>
        <span class="s1">n_samples = _num_samples(X)</span>
        <span class="s1">rs = check_random_state(self.random_state)</span>

        <span class="s1">n_classes_ = self.n_classes_</span>
        <span class="s1">classes_ = self.classes_</span>
        <span class="s1">class_prior_ = self.class_prior_</span>
        <span class="s1">constant = self.constant</span>
        <span class="s2">if </span><span class="s1">self.n_outputs_ == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s0"># Get same type even for self.n_outputs_ == 1</span>
            <span class="s1">n_classes_ = [n_classes_]</span>
            <span class="s1">classes_ = [classes_]</span>
            <span class="s1">class_prior_ = [class_prior_]</span>
            <span class="s1">constant = [constant]</span>

        <span class="s1">P = []</span>
        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(self.n_outputs_):</span>
            <span class="s2">if </span><span class="s1">self._strategy == </span><span class="s4">&quot;most_frequent&quot;</span><span class="s1">:</span>
                <span class="s1">ind = class_prior_[k].argmax()</span>
                <span class="s1">out = np.zeros((n_samples</span><span class="s2">, </span><span class="s1">n_classes_[k])</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
                <span class="s1">out[:</span><span class="s2">, </span><span class="s1">ind] = </span><span class="s5">1.0</span>
            <span class="s2">elif </span><span class="s1">self._strategy == </span><span class="s4">&quot;prior&quot;</span><span class="s1">:</span>
                <span class="s1">out = np.ones((n_samples</span><span class="s2">, </span><span class="s5">1</span><span class="s1">)) * class_prior_[k]</span>

            <span class="s2">elif </span><span class="s1">self._strategy == </span><span class="s4">&quot;stratified&quot;</span><span class="s1">:</span>
                <span class="s1">out = rs.multinomial(</span><span class="s5">1</span><span class="s2">, </span><span class="s1">class_prior_[k]</span><span class="s2">, </span><span class="s1">size=n_samples)</span>
                <span class="s1">out = out.astype(np.float64)</span>

            <span class="s2">elif </span><span class="s1">self._strategy == </span><span class="s4">&quot;uniform&quot;</span><span class="s1">:</span>
                <span class="s1">out = np.ones((n_samples</span><span class="s2">, </span><span class="s1">n_classes_[k])</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
                <span class="s1">out /= n_classes_[k]</span>

            <span class="s2">elif </span><span class="s1">self._strategy == </span><span class="s4">&quot;constant&quot;</span><span class="s1">:</span>
                <span class="s1">ind = np.where(classes_[k] == constant[k])</span>
                <span class="s1">out = np.zeros((n_samples</span><span class="s2">, </span><span class="s1">n_classes_[k])</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
                <span class="s1">out[:</span><span class="s2">, </span><span class="s1">ind] = </span><span class="s5">1.0</span>

            <span class="s1">P.append(out)</span>

        <span class="s2">if </span><span class="s1">self.n_outputs_ == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">P = P[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s2">return </span><span class="s1">P</span>

    <span class="s2">def </span><span class="s1">predict_log_proba(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s3">&quot;&quot;&quot; 
        Return log probability estimates for the test vectors X. 
 
        Parameters 
        ---------- 
        X : {array-like, object with finite length or shape} 
            Training data. 
 
        Returns 
        ------- 
        P : ndarray of shape (n_samples, n_classes) or list of such arrays 
            Returns the log probability of the sample for each class in 
            the model, where classes are ordered arithmetically for each 
            output. 
        &quot;&quot;&quot;</span>
        <span class="s1">proba = self.predict_proba(X)</span>
        <span class="s2">if </span><span class="s1">self.n_outputs_ == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">np.log(proba)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">[np.log(p) </span><span class="s2">for </span><span class="s1">p </span><span class="s2">in </span><span class="s1">proba]</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span>
            <span class="s4">&quot;poor_score&quot;</span><span class="s1">: </span><span class="s2">True,</span>
            <span class="s4">&quot;no_validation&quot;</span><span class="s1">: </span><span class="s2">True,</span>
            <span class="s4">&quot;_xfail_checks&quot;</span><span class="s1">: {</span>
                <span class="s4">&quot;check_methods_subset_invariance&quot;</span><span class="s1">: </span><span class="s4">&quot;fails for the predict method&quot;</span><span class="s2">,</span>
                <span class="s4">&quot;check_methods_sample_order_invariance&quot;</span><span class="s1">: </span><span class="s4">&quot;fails for the predict method&quot;</span><span class="s2">,</span>
            <span class="s1">}</span><span class="s2">,</span>
        <span class="s1">}</span>

    <span class="s2">def </span><span class="s1">score(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s3">&quot;&quot;&quot;Return the mean accuracy on the given test data and labels. 
 
        In multi-label classification, this is the subset accuracy 
        which is a harsh metric since you require for each sample that 
        each label set be correctly predicted. 
 
        Parameters 
        ---------- 
        X : None or array-like of shape (n_samples, n_features) 
            Test samples. Passing None as test samples gives the same result 
            as passing real test samples, since DummyClassifier 
            operates independently of the sampled observations. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_outputs) 
            True labels for X. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. 
 
        Returns 
        ------- 
        score : float 
            Mean accuracy of self.predict(X) w.r.t. y. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">X </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">X = np.zeros(shape=(len(y)</span><span class="s2">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s2">return </span><span class="s1">super().score(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight)</span>


<span class="s2">class </span><span class="s1">DummyRegressor(MultiOutputMixin</span><span class="s2">, </span><span class="s1">RegressorMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
    <span class="s3">&quot;&quot;&quot;Regressor that makes predictions using simple rules. 
 
    This regressor is useful as a simple baseline to compare with other 
    (real) regressors. Do not use it for real problems. 
 
    Read more in the :ref:`User Guide &lt;dummy_estimators&gt;`. 
 
    .. versionadded:: 0.13 
 
    Parameters 
    ---------- 
    strategy : {&quot;mean&quot;, &quot;median&quot;, &quot;quantile&quot;, &quot;constant&quot;}, default=&quot;mean&quot; 
        Strategy to use to generate predictions. 
 
        * &quot;mean&quot;: always predicts the mean of the training set 
        * &quot;median&quot;: always predicts the median of the training set 
        * &quot;quantile&quot;: always predicts a specified quantile of the training set, 
          provided with the quantile parameter. 
        * &quot;constant&quot;: always predicts a constant value that is provided by 
          the user. 
 
    constant : int or float or array-like of shape (n_outputs,), default=None 
        The explicit constant as predicted by the &quot;constant&quot; strategy. This 
        parameter is useful only for the &quot;constant&quot; strategy. 
 
    quantile : float in [0.0, 1.0], default=None 
        The quantile to predict using the &quot;quantile&quot; strategy. A quantile of 
        0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the 
        maximum. 
 
    Attributes 
    ---------- 
    constant_ : ndarray of shape (1, n_outputs) 
        Mean or median or quantile of the training targets or constant value 
        given by the user. 
 
    n_outputs_ : int 
        Number of outputs. 
 
    See Also 
    -------- 
    DummyClassifier: Classifier that makes predictions using simple rules. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.dummy import DummyRegressor 
    &gt;&gt;&gt; X = np.array([1.0, 2.0, 3.0, 4.0]) 
    &gt;&gt;&gt; y = np.array([2.0, 3.0, 5.0, 10.0]) 
    &gt;&gt;&gt; dummy_regr = DummyRegressor(strategy=&quot;mean&quot;) 
    &gt;&gt;&gt; dummy_regr.fit(X, y) 
    DummyRegressor() 
    &gt;&gt;&gt; dummy_regr.predict(X) 
    array([5., 5., 5., 5.]) 
    &gt;&gt;&gt; dummy_regr.score(X, y) 
    0.0 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;strategy&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;mean&quot;</span><span class="s2">, </span><span class="s4">&quot;median&quot;</span><span class="s2">, </span><span class="s4">&quot;quantile&quot;</span><span class="s2">, </span><span class="s4">&quot;constant&quot;</span><span class="s1">})]</span><span class="s2">,</span>
        <span class="s4">&quot;quantile&quot;</span><span class="s1">: [Interval(Real</span><span class="s2">, </span><span class="s5">0.0</span><span class="s2">, </span><span class="s5">1.0</span><span class="s2">, </span><span class="s1">closed=</span><span class="s4">&quot;both&quot;</span><span class="s1">)</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s4">&quot;constant&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Real</span><span class="s2">, None, None, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s4">&quot;array-like&quot;</span><span class="s2">,</span>
            <span class="s2">None,</span>
        <span class="s1">]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">strategy=</span><span class="s4">&quot;mean&quot;</span><span class="s2">, </span><span class="s1">constant=</span><span class="s2">None, </span><span class="s1">quantile=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self.strategy = strategy</span>
        <span class="s1">self.constant = constant</span>
        <span class="s1">self.quantile = quantile</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s3">&quot;&quot;&quot;Fit the random regressor. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training data. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_outputs) 
            Target values. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">y = check_array(y</span><span class="s2">, </span><span class="s1">ensure_2d=</span><span class="s2">False, </span><span class="s1">input_name=</span><span class="s4">&quot;y&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">len(y) == </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;y must not be empty.&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">y.ndim == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">y = np.reshape(y</span><span class="s2">, </span><span class="s1">(-</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">self.n_outputs_ = y.shape[</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">check_consistent_length(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight)</span>

        <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s2">, </span><span class="s1">X)</span>

        <span class="s2">if </span><span class="s1">self.strategy == </span><span class="s4">&quot;mean&quot;</span><span class="s1">:</span>
            <span class="s1">self.constant_ = np.average(y</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">weights=sample_weight)</span>

        <span class="s2">elif </span><span class="s1">self.strategy == </span><span class="s4">&quot;median&quot;</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s1">self.constant_ = np.median(y</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">self.constant_ = [</span>
                    <span class="s1">_weighted_percentile(y[:</span><span class="s2">, </span><span class="s1">k]</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">percentile=</span><span class="s5">50.0</span><span class="s1">)</span>
                    <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(self.n_outputs_)</span>
                <span class="s1">]</span>

        <span class="s2">elif </span><span class="s1">self.strategy == </span><span class="s4">&quot;quantile&quot;</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">self.quantile </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;When using `strategy='quantile', you have to specify the desired &quot;</span>
                    <span class="s4">&quot;quantile in the range [0, 1].&quot;</span>
                <span class="s1">)</span>
            <span class="s1">percentile = self.quantile * </span><span class="s5">100.0</span>
            <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s1">self.constant_ = np.percentile(y</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">q=percentile)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">self.constant_ = [</span>
                    <span class="s1">_weighted_percentile(y[:</span><span class="s2">, </span><span class="s1">k]</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">percentile=percentile)</span>
                    <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(self.n_outputs_)</span>
                <span class="s1">]</span>

        <span class="s2">elif </span><span class="s1">self.strategy == </span><span class="s4">&quot;constant&quot;</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">self.constant </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">TypeError(</span>
                    <span class="s4">&quot;Constant target value has to be specified &quot;</span>
                    <span class="s4">&quot;when the constant strategy is used.&quot;</span>
                <span class="s1">)</span>

            <span class="s1">self.constant_ = check_array(</span>
                <span class="s1">self.constant</span><span class="s2">,</span>
                <span class="s1">accept_sparse=[</span><span class="s4">&quot;csr&quot;</span><span class="s2">, </span><span class="s4">&quot;csc&quot;</span><span class="s2">, </span><span class="s4">&quot;coo&quot;</span><span class="s1">]</span><span class="s2">,</span>
                <span class="s1">ensure_2d=</span><span class="s2">False,</span>
                <span class="s1">ensure_min_samples=</span><span class="s5">0</span><span class="s2">,</span>
            <span class="s1">)</span>

            <span class="s2">if </span><span class="s1">self.n_outputs_ != </span><span class="s5">1 </span><span class="s2">and </span><span class="s1">self.constant_.shape[</span><span class="s5">0</span><span class="s1">] != y.shape[</span><span class="s5">1</span><span class="s1">]:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;Constant target value should have shape (%d, 1).&quot; </span><span class="s1">% y.shape[</span><span class="s5">1</span><span class="s1">]</span>
                <span class="s1">)</span>

        <span class="s1">self.constant_ = np.reshape(self.constant_</span><span class="s2">, </span><span class="s1">(</span><span class="s5">1</span><span class="s2">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">))</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">return_std=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s3">&quot;&quot;&quot;Perform classification on test vectors X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Test data. 
 
        return_std : bool, default=False 
            Whether to return the standard deviation of posterior prediction. 
            All zeros in this case. 
 
            .. versionadded:: 0.20 
 
        Returns 
        ------- 
        y : array-like of shape (n_samples,) or (n_samples, n_outputs) 
            Predicted target values for X. 
 
        y_std : array-like of shape (n_samples,) or (n_samples, n_outputs) 
            Standard deviation of predictive distribution of query points. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">n_samples = _num_samples(X)</span>

        <span class="s1">y = np.full(</span>
            <span class="s1">(n_samples</span><span class="s2">, </span><span class="s1">self.n_outputs_)</span><span class="s2">,</span>
            <span class="s1">self.constant_</span><span class="s2">,</span>
            <span class="s1">dtype=np.array(self.constant_).dtype</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">y_std = np.zeros((n_samples</span><span class="s2">, </span><span class="s1">self.n_outputs_))</span>

        <span class="s2">if </span><span class="s1">self.n_outputs_ == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">y = np.ravel(y)</span>
            <span class="s1">y_std = np.ravel(y_std)</span>

        <span class="s2">return </span><span class="s1">(y</span><span class="s2">, </span><span class="s1">y_std) </span><span class="s2">if </span><span class="s1">return_std </span><span class="s2">else </span><span class="s1">y</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s4">&quot;poor_score&quot;</span><span class="s1">: </span><span class="s2">True, </span><span class="s4">&quot;no_validation&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>

    <span class="s2">def </span><span class="s1">score(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s3">&quot;&quot;&quot;Return the coefficient of determination R^2 of the prediction. 
 
        The coefficient R^2 is defined as `(1 - u/v)`, where `u` is the 
        residual sum of squares `((y_true - y_pred) ** 2).sum()` and `v` is the 
        total sum of squares `((y_true - y_true.mean()) ** 2).sum()`. The best 
        possible score is 1.0 and it can be negative (because the model can be 
        arbitrarily worse). A constant model that always predicts the expected 
        value of y, disregarding the input features, would get a R^2 score of 
        0.0. 
 
        Parameters 
        ---------- 
        X : None or array-like of shape (n_samples, n_features) 
            Test samples. Passing None as test samples gives the same result 
            as passing real test samples, since `DummyRegressor` 
            operates independently of the sampled observations. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_outputs) 
            True values for X. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. 
 
        Returns 
        ------- 
        score : float 
            R^2 of `self.predict(X)` w.r.t. y. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">X </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">X = np.zeros(shape=(len(y)</span><span class="s2">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s2">return </span><span class="s1">super().score(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight)</span>
</pre>
</body>
</html>