<html>
<head>
<title>test_multioutput.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6897bb;}
.s3 { color: #6a8759;}
.s4 { color: #808080;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_multioutput.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">re</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">import </span><span class="s1">scipy.sparse </span><span class="s0">as </span><span class="s1">sp</span>
<span class="s0">from </span><span class="s1">joblib </span><span class="s0">import </span><span class="s1">cpu_count</span>

<span class="s0">from </span><span class="s1">sklearn </span><span class="s0">import </span><span class="s1">datasets</span>
<span class="s0">from </span><span class="s1">sklearn.base </span><span class="s0">import </span><span class="s1">ClassifierMixin</span><span class="s0">, </span><span class="s1">clone</span>
<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">load_linnerud</span><span class="s0">,</span>
    <span class="s1">make_classification</span><span class="s0">,</span>
    <span class="s1">make_multilabel_classification</span><span class="s0">,</span>
    <span class="s1">make_regression</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.dummy </span><span class="s0">import </span><span class="s1">DummyClassifier</span><span class="s0">, </span><span class="s1">DummyRegressor</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">GradientBoostingRegressor</span><span class="s0">,</span>
    <span class="s1">RandomForestClassifier</span><span class="s0">,</span>
    <span class="s1">StackingRegressor</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.exceptions </span><span class="s0">import </span><span class="s1">NotFittedError</span>
<span class="s0">from </span><span class="s1">sklearn.impute </span><span class="s0">import </span><span class="s1">SimpleImputer</span>
<span class="s0">from </span><span class="s1">sklearn.linear_model </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">Lasso</span><span class="s0">,</span>
    <span class="s1">LinearRegression</span><span class="s0">,</span>
    <span class="s1">LogisticRegression</span><span class="s0">,</span>
    <span class="s1">OrthogonalMatchingPursuit</span><span class="s0">,</span>
    <span class="s1">PassiveAggressiveClassifier</span><span class="s0">,</span>
    <span class="s1">Ridge</span><span class="s0">,</span>
    <span class="s1">SGDClassifier</span><span class="s0">,</span>
    <span class="s1">SGDRegressor</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.metrics </span><span class="s0">import </span><span class="s1">jaccard_score</span><span class="s0">, </span><span class="s1">mean_squared_error</span>
<span class="s0">from </span><span class="s1">sklearn.model_selection </span><span class="s0">import </span><span class="s1">GridSearchCV</span><span class="s0">, </span><span class="s1">train_test_split</span>
<span class="s0">from </span><span class="s1">sklearn.multiclass </span><span class="s0">import </span><span class="s1">OneVsRestClassifier</span>
<span class="s0">from </span><span class="s1">sklearn.multioutput </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">ClassifierChain</span><span class="s0">,</span>
    <span class="s1">MultiOutputClassifier</span><span class="s0">,</span>
    <span class="s1">MultiOutputRegressor</span><span class="s0">,</span>
    <span class="s1">RegressorChain</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.pipeline </span><span class="s0">import </span><span class="s1">make_pipeline</span>
<span class="s0">from </span><span class="s1">sklearn.svm </span><span class="s0">import </span><span class="s1">LinearSVC</span>
<span class="s0">from </span><span class="s1">sklearn.tree </span><span class="s0">import </span><span class="s1">DecisionTreeClassifier</span>
<span class="s0">from </span><span class="s1">sklearn.utils </span><span class="s0">import </span><span class="s1">shuffle</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">assert_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_equal</span><span class="s0">,</span>
<span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_multi_target_regression():</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_regression(n_targets=</span><span class="s2">3</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">y_train = X[:</span><span class="s2">50</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y[:</span><span class="s2">50</span><span class="s1">]</span>
    <span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test = X[</span><span class="s2">50</span><span class="s1">:]</span><span class="s0">, </span><span class="s1">y[</span><span class="s2">50</span><span class="s1">:]</span>

    <span class="s1">references = np.zeros_like(y_test)</span>
    <span class="s0">for </span><span class="s1">n </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">3</span><span class="s1">):</span>
        <span class="s1">rgr = GradientBoostingRegressor(random_state=</span><span class="s2">0</span><span class="s1">)</span>
        <span class="s1">rgr.fit(X_train</span><span class="s0">, </span><span class="s1">y_train[:</span><span class="s0">, </span><span class="s1">n])</span>
        <span class="s1">references[:</span><span class="s0">, </span><span class="s1">n] = rgr.predict(X_test)</span>

    <span class="s1">rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=</span><span class="s2">0</span><span class="s1">))</span>
    <span class="s1">rgr.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">y_pred = rgr.predict(X_test)</span>

    <span class="s1">assert_almost_equal(references</span><span class="s0">, </span><span class="s1">y_pred)</span>


<span class="s0">def </span><span class="s1">test_multi_target_regression_partial_fit():</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_regression(n_targets=</span><span class="s2">3</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">y_train = X[:</span><span class="s2">50</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y[:</span><span class="s2">50</span><span class="s1">]</span>
    <span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test = X[</span><span class="s2">50</span><span class="s1">:]</span><span class="s0">, </span><span class="s1">y[</span><span class="s2">50</span><span class="s1">:]</span>

    <span class="s1">references = np.zeros_like(y_test)</span>
    <span class="s1">half_index = </span><span class="s2">25</span>
    <span class="s0">for </span><span class="s1">n </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">3</span><span class="s1">):</span>
        <span class="s1">sgr = SGDRegressor(random_state=</span><span class="s2">0</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">5</span><span class="s1">)</span>
        <span class="s1">sgr.partial_fit(X_train[:half_index]</span><span class="s0">, </span><span class="s1">y_train[:half_index</span><span class="s0">, </span><span class="s1">n])</span>
        <span class="s1">sgr.partial_fit(X_train[half_index:]</span><span class="s0">, </span><span class="s1">y_train[half_index:</span><span class="s0">, </span><span class="s1">n])</span>
        <span class="s1">references[:</span><span class="s0">, </span><span class="s1">n] = sgr.predict(X_test)</span>

    <span class="s1">sgr = MultiOutputRegressor(SGDRegressor(random_state=</span><span class="s2">0</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">5</span><span class="s1">))</span>

    <span class="s1">sgr.partial_fit(X_train[:half_index]</span><span class="s0">, </span><span class="s1">y_train[:half_index])</span>
    <span class="s1">sgr.partial_fit(X_train[half_index:]</span><span class="s0">, </span><span class="s1">y_train[half_index:])</span>

    <span class="s1">y_pred = sgr.predict(X_test)</span>
    <span class="s1">assert_almost_equal(references</span><span class="s0">, </span><span class="s1">y_pred)</span>
    <span class="s0">assert not </span><span class="s1">hasattr(MultiOutputRegressor(Lasso)</span><span class="s0">, </span><span class="s3">&quot;partial_fit&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_multi_target_regression_one_target():</span>
    <span class="s4"># Test multi target regression raises</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_regression(n_targets=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=</span><span class="s2">0</span><span class="s1">))</span>
    <span class="s1">msg = </span><span class="s3">&quot;at least two dimensions&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">rgr.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_multi_target_sparse_regression():</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_regression(n_targets=</span><span class="s2">3</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">y_train = X[:</span><span class="s2">50</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y[:</span><span class="s2">50</span><span class="s1">]</span>
    <span class="s1">X_test = X[</span><span class="s2">50</span><span class="s1">:]</span>

    <span class="s0">for </span><span class="s1">sparse </span><span class="s0">in </span><span class="s1">[</span>
        <span class="s1">sp.csr_matrix</span><span class="s0">,</span>
        <span class="s1">sp.csc_matrix</span><span class="s0">,</span>
        <span class="s1">sp.coo_matrix</span><span class="s0">,</span>
        <span class="s1">sp.dok_matrix</span><span class="s0">,</span>
        <span class="s1">sp.lil_matrix</span><span class="s0">,</span>
    <span class="s1">]:</span>
        <span class="s1">rgr = MultiOutputRegressor(Lasso(random_state=</span><span class="s2">0</span><span class="s1">))</span>
        <span class="s1">rgr_sparse = MultiOutputRegressor(Lasso(random_state=</span><span class="s2">0</span><span class="s1">))</span>

        <span class="s1">rgr.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
        <span class="s1">rgr_sparse.fit(sparse(X_train)</span><span class="s0">, </span><span class="s1">y_train)</span>

        <span class="s1">assert_almost_equal(rgr.predict(X_test)</span><span class="s0">, </span><span class="s1">rgr_sparse.predict(sparse(X_test)))</span>


<span class="s0">def </span><span class="s1">test_multi_target_sample_weights_api():</span>
    <span class="s1">X = [[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">6</span><span class="s1">]]</span>
    <span class="s1">y = [[</span><span class="s2">3.141</span><span class="s0">, </span><span class="s2">2.718</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">2.718</span><span class="s0">, </span><span class="s2">3.141</span><span class="s1">]]</span>
    <span class="s1">w = [</span><span class="s2">0.8</span><span class="s0">, </span><span class="s2">0.6</span><span class="s1">]</span>

    <span class="s1">rgr = MultiOutputRegressor(OrthogonalMatchingPursuit())</span>
    <span class="s1">msg = </span><span class="s3">&quot;does not support sample weights&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">rgr.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">w)</span>

    <span class="s4"># no exception should be raised if the base estimator supports weights</span>
    <span class="s1">rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=</span><span class="s2">0</span><span class="s1">))</span>
    <span class="s1">rgr.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">w)</span>


<span class="s0">def </span><span class="s1">test_multi_target_sample_weight_partial_fit():</span>
    <span class="s4"># weighted regressor</span>
    <span class="s1">X = [[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">6</span><span class="s1">]]</span>
    <span class="s1">y = [[</span><span class="s2">3.141</span><span class="s0">, </span><span class="s2">2.718</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">2.718</span><span class="s0">, </span><span class="s2">3.141</span><span class="s1">]]</span>
    <span class="s1">w = [</span><span class="s2">2.0</span><span class="s0">, </span><span class="s2">1.0</span><span class="s1">]</span>
    <span class="s1">rgr_w = MultiOutputRegressor(SGDRegressor(random_state=</span><span class="s2">0</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">5</span><span class="s1">))</span>
    <span class="s1">rgr_w.partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">w)</span>

    <span class="s4"># weighted with different weights</span>
    <span class="s1">w = [</span><span class="s2">2.0</span><span class="s0">, </span><span class="s2">2.0</span><span class="s1">]</span>
    <span class="s1">rgr = MultiOutputRegressor(SGDRegressor(random_state=</span><span class="s2">0</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">5</span><span class="s1">))</span>
    <span class="s1">rgr.partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">w)</span>

    <span class="s0">assert </span><span class="s1">rgr.predict(X)[</span><span class="s2">0</span><span class="s1">][</span><span class="s2">0</span><span class="s1">] != rgr_w.predict(X)[</span><span class="s2">0</span><span class="s1">][</span><span class="s2">0</span><span class="s1">]</span>


<span class="s0">def </span><span class="s1">test_multi_target_sample_weights():</span>
    <span class="s4"># weighted regressor</span>
    <span class="s1">Xw = [[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">6</span><span class="s1">]]</span>
    <span class="s1">yw = [[</span><span class="s2">3.141</span><span class="s0">, </span><span class="s2">2.718</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">2.718</span><span class="s0">, </span><span class="s2">3.141</span><span class="s1">]]</span>
    <span class="s1">w = [</span><span class="s2">2.0</span><span class="s0">, </span><span class="s2">1.0</span><span class="s1">]</span>
    <span class="s1">rgr_w = MultiOutputRegressor(GradientBoostingRegressor(random_state=</span><span class="s2">0</span><span class="s1">))</span>
    <span class="s1">rgr_w.fit(Xw</span><span class="s0">, </span><span class="s1">yw</span><span class="s0">, </span><span class="s1">w)</span>

    <span class="s4"># unweighted, but with repeated samples</span>
    <span class="s1">X = [[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">6</span><span class="s1">]]</span>
    <span class="s1">y = [[</span><span class="s2">3.141</span><span class="s0">, </span><span class="s2">2.718</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">3.141</span><span class="s0">, </span><span class="s2">2.718</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">2.718</span><span class="s0">, </span><span class="s2">3.141</span><span class="s1">]]</span>
    <span class="s1">rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=</span><span class="s2">0</span><span class="s1">))</span>
    <span class="s1">rgr.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">X_test = [[</span><span class="s2">1.5</span><span class="s0">, </span><span class="s2">2.5</span><span class="s0">, </span><span class="s2">3.5</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">3.5</span><span class="s0">, </span><span class="s2">4.5</span><span class="s0">, </span><span class="s2">5.5</span><span class="s1">]]</span>
    <span class="s1">assert_almost_equal(rgr.predict(X_test)</span><span class="s0">, </span><span class="s1">rgr_w.predict(X_test))</span>


<span class="s4"># Import the data</span>
<span class="s1">iris = datasets.load_iris()</span>
<span class="s4"># create a multiple targets by randomized shuffling and concatenating y.</span>
<span class="s1">X = iris.data</span>
<span class="s1">y1 = iris.target</span>
<span class="s1">y2 = shuffle(y1</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">1</span><span class="s1">)</span>
<span class="s1">y3 = shuffle(y1</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">2</span><span class="s1">)</span>
<span class="s1">y = np.column_stack((y1</span><span class="s0">, </span><span class="s1">y2</span><span class="s0">, </span><span class="s1">y3))</span>
<span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = X.shape</span>
<span class="s1">n_outputs = y.shape[</span><span class="s2">1</span><span class="s1">]</span>
<span class="s1">n_classes = len(np.unique(y1))</span>
<span class="s1">classes = list(map(np.unique</span><span class="s0">, </span><span class="s1">(y1</span><span class="s0">, </span><span class="s1">y2</span><span class="s0">, </span><span class="s1">y3)))</span>


<span class="s0">def </span><span class="s1">test_multi_output_classification_partial_fit_parallelism():</span>
    <span class="s1">sgd_linear_clf = SGDClassifier(loss=</span><span class="s3">&quot;log_loss&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">5</span><span class="s1">)</span>
    <span class="s1">mor = MultiOutputClassifier(sgd_linear_clf</span><span class="s0">, </span><span class="s1">n_jobs=</span><span class="s2">4</span><span class="s1">)</span>
    <span class="s1">mor.partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes)</span>
    <span class="s1">est1 = mor.estimators_[</span><span class="s2">0</span><span class="s1">]</span>
    <span class="s1">mor.partial_fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">est2 = mor.estimators_[</span><span class="s2">0</span><span class="s1">]</span>
    <span class="s0">if </span><span class="s1">cpu_count() &gt; </span><span class="s2">1</span><span class="s1">:</span>
        <span class="s4"># parallelism requires this to be the case for a sane implementation</span>
        <span class="s0">assert </span><span class="s1">est1 </span><span class="s0">is not </span><span class="s1">est2</span>


<span class="s4"># check multioutput has predict_proba</span>
<span class="s0">def </span><span class="s1">test_hasattr_multi_output_predict_proba():</span>
    <span class="s4"># default SGDClassifier has loss='hinge'</span>
    <span class="s4"># which does not expose a predict_proba method</span>
    <span class="s1">sgd_linear_clf = SGDClassifier(random_state=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">5</span><span class="s1">)</span>
    <span class="s1">multi_target_linear = MultiOutputClassifier(sgd_linear_clf)</span>
    <span class="s1">multi_target_linear.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert not </span><span class="s1">hasattr(multi_target_linear</span><span class="s0">, </span><span class="s3">&quot;predict_proba&quot;</span><span class="s1">)</span>

    <span class="s4"># case where predict_proba attribute exists</span>
    <span class="s1">sgd_linear_clf = SGDClassifier(loss=</span><span class="s3">&quot;log_loss&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">5</span><span class="s1">)</span>
    <span class="s1">multi_target_linear = MultiOutputClassifier(sgd_linear_clf)</span>
    <span class="s1">multi_target_linear.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">hasattr(multi_target_linear</span><span class="s0">, </span><span class="s3">&quot;predict_proba&quot;</span><span class="s1">)</span>


<span class="s4"># check predict_proba passes</span>
<span class="s0">def </span><span class="s1">test_multi_output_predict_proba():</span>
    <span class="s1">sgd_linear_clf = SGDClassifier(random_state=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">5</span><span class="s1">)</span>
    <span class="s1">param = {</span><span class="s3">&quot;loss&quot;</span><span class="s1">: (</span><span class="s3">&quot;hinge&quot;</span><span class="s0">, </span><span class="s3">&quot;log_loss&quot;</span><span class="s0">, </span><span class="s3">&quot;modified_huber&quot;</span><span class="s1">)}</span>

    <span class="s4"># inner function for custom scoring</span>
    <span class="s0">def </span><span class="s1">custom_scorer(estimator</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
        <span class="s0">if </span><span class="s1">hasattr(estimator</span><span class="s0">, </span><span class="s3">&quot;predict_proba&quot;</span><span class="s1">):</span>
            <span class="s0">return </span><span class="s2">1.0</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s2">0.0</span>

    <span class="s1">grid_clf = GridSearchCV(</span>
        <span class="s1">sgd_linear_clf</span><span class="s0">,</span>
        <span class="s1">param_grid=param</span><span class="s0">,</span>
        <span class="s1">scoring=custom_scorer</span><span class="s0">,</span>
        <span class="s1">cv=</span><span class="s2">3</span><span class="s0">,</span>
        <span class="s1">error_score=</span><span class="s3">&quot;raise&quot;</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">multi_target_linear = MultiOutputClassifier(grid_clf)</span>
    <span class="s1">multi_target_linear.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">multi_target_linear.predict_proba(X)</span>

    <span class="s4"># SGDClassifier defaults to loss='hinge' which is not a probabilistic</span>
    <span class="s4"># loss function; therefore it does not expose a predict_proba method</span>
    <span class="s1">sgd_linear_clf = SGDClassifier(random_state=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">5</span><span class="s1">)</span>
    <span class="s1">multi_target_linear = MultiOutputClassifier(sgd_linear_clf)</span>
    <span class="s1">multi_target_linear.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">err_msg = </span><span class="s3">&quot;probability estimates are not available for loss='hinge'&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(AttributeError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">multi_target_linear.predict_proba(X)</span>


<span class="s0">def </span><span class="s1">test_multi_output_classification_partial_fit():</span>
    <span class="s4"># test if multi_target initializes correctly with base estimator and fit</span>
    <span class="s4"># assert predictions work as expected for predict</span>

    <span class="s1">sgd_linear_clf = SGDClassifier(loss=</span><span class="s3">&quot;log_loss&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">5</span><span class="s1">)</span>
    <span class="s1">multi_target_linear = MultiOutputClassifier(sgd_linear_clf)</span>

    <span class="s4"># train the multi_target_linear and also get the predictions.</span>
    <span class="s1">half_index = X.shape[</span><span class="s2">0</span><span class="s1">] // </span><span class="s2">2</span>
    <span class="s1">multi_target_linear.partial_fit(X[:half_index]</span><span class="s0">, </span><span class="s1">y[:half_index]</span><span class="s0">, </span><span class="s1">classes=classes)</span>

    <span class="s1">first_predictions = multi_target_linear.predict(X)</span>
    <span class="s0">assert </span><span class="s1">(n_samples</span><span class="s0">, </span><span class="s1">n_outputs) == first_predictions.shape</span>

    <span class="s1">multi_target_linear.partial_fit(X[half_index:]</span><span class="s0">, </span><span class="s1">y[half_index:])</span>
    <span class="s1">second_predictions = multi_target_linear.predict(X)</span>
    <span class="s0">assert </span><span class="s1">(n_samples</span><span class="s0">, </span><span class="s1">n_outputs) == second_predictions.shape</span>

    <span class="s4"># train the linear classification with each column and assert that</span>
    <span class="s4"># predictions are equal after first partial_fit and second partial_fit</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">3</span><span class="s1">):</span>
        <span class="s4"># create a clone with the same state</span>
        <span class="s1">sgd_linear_clf = clone(sgd_linear_clf)</span>
        <span class="s1">sgd_linear_clf.partial_fit(</span>
            <span class="s1">X[:half_index]</span><span class="s0">, </span><span class="s1">y[:half_index</span><span class="s0">, </span><span class="s1">i]</span><span class="s0">, </span><span class="s1">classes=classes[i]</span>
        <span class="s1">)</span>
        <span class="s1">assert_array_equal(sgd_linear_clf.predict(X)</span><span class="s0">, </span><span class="s1">first_predictions[:</span><span class="s0">, </span><span class="s1">i])</span>
        <span class="s1">sgd_linear_clf.partial_fit(X[half_index:]</span><span class="s0">, </span><span class="s1">y[half_index:</span><span class="s0">, </span><span class="s1">i])</span>
        <span class="s1">assert_array_equal(sgd_linear_clf.predict(X)</span><span class="s0">, </span><span class="s1">second_predictions[:</span><span class="s0">, </span><span class="s1">i])</span>


<span class="s0">def </span><span class="s1">test_multi_output_classification_partial_fit_no_first_classes_exception():</span>
    <span class="s1">sgd_linear_clf = SGDClassifier(loss=</span><span class="s3">&quot;log_loss&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">5</span><span class="s1">)</span>
    <span class="s1">multi_target_linear = MultiOutputClassifier(sgd_linear_clf)</span>
    <span class="s1">msg = </span><span class="s3">&quot;classes must be passed on the first call to partial_fit.&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">multi_target_linear.partial_fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_multi_output_classification():</span>
    <span class="s4"># test if multi_target initializes correctly with base estimator and fit</span>
    <span class="s4"># assert predictions work as expected for predict, prodict_proba and score</span>

    <span class="s1">forest = RandomForestClassifier(n_estimators=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">1</span><span class="s1">)</span>
    <span class="s1">multi_target_forest = MultiOutputClassifier(forest)</span>

    <span class="s4"># train the multi_target_forest and also get the predictions.</span>
    <span class="s1">multi_target_forest.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">predictions = multi_target_forest.predict(X)</span>
    <span class="s0">assert </span><span class="s1">(n_samples</span><span class="s0">, </span><span class="s1">n_outputs) == predictions.shape</span>

    <span class="s1">predict_proba = multi_target_forest.predict_proba(X)</span>

    <span class="s0">assert </span><span class="s1">len(predict_proba) == n_outputs</span>
    <span class="s0">for </span><span class="s1">class_probabilities </span><span class="s0">in </span><span class="s1">predict_proba:</span>
        <span class="s0">assert </span><span class="s1">(n_samples</span><span class="s0">, </span><span class="s1">n_classes) == class_probabilities.shape</span>

    <span class="s1">assert_array_equal(np.argmax(np.dstack(predict_proba)</span><span class="s0">, </span><span class="s1">axis=</span><span class="s2">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">predictions)</span>

    <span class="s4"># train the forest with each column and assert that predictions are equal</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">3</span><span class="s1">):</span>
        <span class="s1">forest_ = clone(forest)  </span><span class="s4"># create a clone with the same state</span>
        <span class="s1">forest_.fit(X</span><span class="s0">, </span><span class="s1">y[:</span><span class="s0">, </span><span class="s1">i])</span>
        <span class="s0">assert </span><span class="s1">list(forest_.predict(X)) == list(predictions[:</span><span class="s0">, </span><span class="s1">i])</span>
        <span class="s1">assert_array_equal(list(forest_.predict_proba(X))</span><span class="s0">, </span><span class="s1">list(predict_proba[i]))</span>


<span class="s0">def </span><span class="s1">test_multiclass_multioutput_estimator():</span>
    <span class="s4"># test to check meta of meta estimators</span>
    <span class="s1">svc = LinearSVC(dual=</span><span class="s3">&quot;auto&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">multi_class_svc = OneVsRestClassifier(svc)</span>
    <span class="s1">multi_target_svc = MultiOutputClassifier(multi_class_svc)</span>

    <span class="s1">multi_target_svc.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">predictions = multi_target_svc.predict(X)</span>
    <span class="s0">assert </span><span class="s1">(n_samples</span><span class="s0">, </span><span class="s1">n_outputs) == predictions.shape</span>

    <span class="s4"># train the forest with each column and assert that predictions are equal</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">3</span><span class="s1">):</span>
        <span class="s1">multi_class_svc_ = clone(multi_class_svc)  </span><span class="s4"># create a clone</span>
        <span class="s1">multi_class_svc_.fit(X</span><span class="s0">, </span><span class="s1">y[:</span><span class="s0">, </span><span class="s1">i])</span>
        <span class="s0">assert </span><span class="s1">list(multi_class_svc_.predict(X)) == list(predictions[:</span><span class="s0">, </span><span class="s1">i])</span>


<span class="s0">def </span><span class="s1">test_multiclass_multioutput_estimator_predict_proba():</span>
    <span class="s1">seed = </span><span class="s2">542</span>

    <span class="s4"># make test deterministic</span>
    <span class="s1">rng = np.random.RandomState(seed)</span>

    <span class="s4"># random features</span>
    <span class="s1">X = rng.normal(size=(</span><span class="s2">5</span><span class="s0">, </span><span class="s2">5</span><span class="s1">))</span>

    <span class="s4"># random labels</span>
    <span class="s1">y1 = np.array([</span><span class="s3">&quot;b&quot;</span><span class="s0">, </span><span class="s3">&quot;a&quot;</span><span class="s0">, </span><span class="s3">&quot;a&quot;</span><span class="s0">, </span><span class="s3">&quot;b&quot;</span><span class="s0">, </span><span class="s3">&quot;a&quot;</span><span class="s1">]).reshape(</span><span class="s2">5</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)  </span><span class="s4"># 2 classes</span>
    <span class="s1">y2 = np.array([</span><span class="s3">&quot;d&quot;</span><span class="s0">, </span><span class="s3">&quot;e&quot;</span><span class="s0">, </span><span class="s3">&quot;f&quot;</span><span class="s0">, </span><span class="s3">&quot;e&quot;</span><span class="s0">, </span><span class="s3">&quot;d&quot;</span><span class="s1">]).reshape(</span><span class="s2">5</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)  </span><span class="s4"># 3 classes</span>

    <span class="s1">Y = np.concatenate([y1</span><span class="s0">, </span><span class="s1">y2]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s2">1</span><span class="s1">)</span>

    <span class="s1">clf = MultiOutputClassifier(</span>
        <span class="s1">LogisticRegression(solver=</span><span class="s3">&quot;liblinear&quot;</span><span class="s0">, </span><span class="s1">random_state=seed)</span>
    <span class="s1">)</span>

    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">y_result = clf.predict_proba(X)</span>
    <span class="s1">y_actual = [</span>
        <span class="s1">np.array(</span>
            <span class="s1">[</span>
                <span class="s1">[</span><span class="s2">0.23481764</span><span class="s0">, </span><span class="s2">0.76518236</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s2">0.67196072</span><span class="s0">, </span><span class="s2">0.32803928</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s2">0.54681448</span><span class="s0">, </span><span class="s2">0.45318552</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s2">0.34883923</span><span class="s0">, </span><span class="s2">0.65116077</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s2">0.73687069</span><span class="s0">, </span><span class="s2">0.26312931</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">]</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">np.array(</span>
            <span class="s1">[</span>
                <span class="s1">[</span><span class="s2">0.5171785</span><span class="s0">, </span><span class="s2">0.23878628</span><span class="s0">, </span><span class="s2">0.24403522</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s2">0.22141451</span><span class="s0">, </span><span class="s2">0.64102704</span><span class="s0">, </span><span class="s2">0.13755846</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s2">0.16751315</span><span class="s0">, </span><span class="s2">0.18256843</span><span class="s0">, </span><span class="s2">0.64991843</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s2">0.27357372</span><span class="s0">, </span><span class="s2">0.55201592</span><span class="s0">, </span><span class="s2">0.17441036</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s1">[</span><span class="s2">0.65745193</span><span class="s0">, </span><span class="s2">0.26062899</span><span class="s0">, </span><span class="s2">0.08191907</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">]</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span>

    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(len(y_actual)):</span>
        <span class="s1">assert_almost_equal(y_result[i]</span><span class="s0">, </span><span class="s1">y_actual[i])</span>


<span class="s0">def </span><span class="s1">test_multi_output_classification_sample_weights():</span>
    <span class="s4"># weighted classifier</span>
    <span class="s1">Xw = [[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">6</span><span class="s1">]]</span>
    <span class="s1">yw = [[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]]</span>
    <span class="s1">w = np.asarray([</span><span class="s2">2.0</span><span class="s0">, </span><span class="s2">1.0</span><span class="s1">])</span>
    <span class="s1">forest = RandomForestClassifier(n_estimators=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">1</span><span class="s1">)</span>
    <span class="s1">clf_w = MultiOutputClassifier(forest)</span>
    <span class="s1">clf_w.fit(Xw</span><span class="s0">, </span><span class="s1">yw</span><span class="s0">, </span><span class="s1">w)</span>

    <span class="s4"># unweighted, but with repeated samples</span>
    <span class="s1">X = [[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">6</span><span class="s1">]]</span>
    <span class="s1">y = [[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]]</span>
    <span class="s1">forest = RandomForestClassifier(n_estimators=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">1</span><span class="s1">)</span>
    <span class="s1">clf = MultiOutputClassifier(forest)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">X_test = [[</span><span class="s2">1.5</span><span class="s0">, </span><span class="s2">2.5</span><span class="s0">, </span><span class="s2">3.5</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">3.5</span><span class="s0">, </span><span class="s2">4.5</span><span class="s0">, </span><span class="s2">5.5</span><span class="s1">]]</span>
    <span class="s1">assert_almost_equal(clf.predict(X_test)</span><span class="s0">, </span><span class="s1">clf_w.predict(X_test))</span>


<span class="s0">def </span><span class="s1">test_multi_output_classification_partial_fit_sample_weights():</span>
    <span class="s4"># weighted classifier</span>
    <span class="s1">Xw = [[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">6</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1.5</span><span class="s0">, </span><span class="s2">2.5</span><span class="s0">, </span><span class="s2">3.5</span><span class="s1">]]</span>
    <span class="s1">yw = [[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]]</span>
    <span class="s1">w = np.asarray([</span><span class="s2">2.0</span><span class="s0">, </span><span class="s2">1.0</span><span class="s0">, </span><span class="s2">1.0</span><span class="s1">])</span>
    <span class="s1">sgd_linear_clf = SGDClassifier(random_state=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">20</span><span class="s1">)</span>
    <span class="s1">clf_w = MultiOutputClassifier(sgd_linear_clf)</span>
    <span class="s1">clf_w.fit(Xw</span><span class="s0">, </span><span class="s1">yw</span><span class="s0">, </span><span class="s1">w)</span>

    <span class="s4"># unweighted, but with repeated samples</span>
    <span class="s1">X = [[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">6</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1.5</span><span class="s0">, </span><span class="s2">2.5</span><span class="s0">, </span><span class="s2">3.5</span><span class="s1">]]</span>
    <span class="s1">y = [[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]]</span>
    <span class="s1">sgd_linear_clf = SGDClassifier(random_state=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">20</span><span class="s1">)</span>
    <span class="s1">clf = MultiOutputClassifier(sgd_linear_clf)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">X_test = [[</span><span class="s2">1.5</span><span class="s0">, </span><span class="s2">2.5</span><span class="s0">, </span><span class="s2">3.5</span><span class="s1">]]</span>
    <span class="s1">assert_array_almost_equal(clf.predict(X_test)</span><span class="s0">, </span><span class="s1">clf_w.predict(X_test))</span>


<span class="s0">def </span><span class="s1">test_multi_output_exceptions():</span>
    <span class="s4"># NotFittedError when fit is not done but score, predict and</span>
    <span class="s4"># and predict_proba are called</span>
    <span class="s1">moc = MultiOutputClassifier(LinearSVC(dual=</span><span class="s3">&quot;auto&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">))</span>
    <span class="s0">with </span><span class="s1">pytest.raises(NotFittedError):</span>
        <span class="s1">moc.score(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s4"># ValueError when number of outputs is different</span>
    <span class="s4"># for fit and score</span>
    <span class="s1">y_new = np.column_stack((y1</span><span class="s0">, </span><span class="s1">y2))</span>
    <span class="s1">moc.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">moc.score(X</span><span class="s0">, </span><span class="s1">y_new)</span>

    <span class="s4"># ValueError when y is continuous</span>
    <span class="s1">msg = </span><span class="s3">&quot;Unknown label type&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">moc.fit(X</span><span class="s0">, </span><span class="s1">X[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;response_method&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">&quot;predict_proba&quot;</span><span class="s0">, </span><span class="s3">&quot;predict&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_multi_output_not_fitted_error(response_method):</span>
    <span class="s5">&quot;&quot;&quot;Check that we raise the proper error when the estimator is not fitted&quot;&quot;&quot;</span>
    <span class="s1">moc = MultiOutputClassifier(LogisticRegression())</span>
    <span class="s0">with </span><span class="s1">pytest.raises(NotFittedError):</span>
        <span class="s1">getattr(moc</span><span class="s0">, </span><span class="s1">response_method)(X)</span>


<span class="s0">def </span><span class="s1">test_multi_output_delegate_predict_proba():</span>
    <span class="s5">&quot;&quot;&quot;Check the behavior for the delegation of predict_proba to the underlying 
    estimator&quot;&quot;&quot;</span>

    <span class="s4"># A base estimator with `predict_proba`should expose the method even before fit</span>
    <span class="s1">moc = MultiOutputClassifier(LogisticRegression())</span>
    <span class="s0">assert </span><span class="s1">hasattr(moc</span><span class="s0">, </span><span class="s3">&quot;predict_proba&quot;</span><span class="s1">)</span>
    <span class="s1">moc.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">hasattr(moc</span><span class="s0">, </span><span class="s3">&quot;predict_proba&quot;</span><span class="s1">)</span>

    <span class="s4"># A base estimator without `predict_proba` should raise an AttributeError</span>
    <span class="s1">moc = MultiOutputClassifier(LinearSVC(dual=</span><span class="s3">&quot;auto&quot;</span><span class="s1">))</span>
    <span class="s0">assert not </span><span class="s1">hasattr(moc</span><span class="s0">, </span><span class="s3">&quot;predict_proba&quot;</span><span class="s1">)</span>
    <span class="s1">msg = </span><span class="s3">&quot;'LinearSVC' object has no attribute 'predict_proba'&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(AttributeError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">moc.predict_proba(X)</span>
    <span class="s1">moc.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert not </span><span class="s1">hasattr(moc</span><span class="s0">, </span><span class="s3">&quot;predict_proba&quot;</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(AttributeError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">moc.predict_proba(X)</span>


<span class="s0">def </span><span class="s1">generate_multilabel_dataset_with_correlations():</span>
    <span class="s4"># Generate a multilabel data set from a multiclass dataset as a way of</span>
    <span class="s4"># by representing the integer number of the original class using a binary</span>
    <span class="s4"># encoding.</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s2">1000</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s2">100</span><span class="s0">, </span><span class="s1">n_classes=</span><span class="s2">16</span><span class="s0">, </span><span class="s1">n_informative=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span>
    <span class="s1">)</span>

    <span class="s1">Y_multi = np.array([[int(yyy) </span><span class="s0">for </span><span class="s1">yyy </span><span class="s0">in </span><span class="s1">format(yy</span><span class="s0">, </span><span class="s3">&quot;#06b&quot;</span><span class="s1">)[</span><span class="s2">2</span><span class="s1">:]] </span><span class="s0">for </span><span class="s1">yy </span><span class="s0">in </span><span class="s1">y])</span>
    <span class="s0">return </span><span class="s1">X</span><span class="s0">, </span><span class="s1">Y_multi</span>


<span class="s0">def </span><span class="s1">test_classifier_chain_fit_and_predict_with_linear_svc():</span>
    <span class="s4"># Fit classifier chain and verify predict performance using LinearSVC</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = generate_multilabel_dataset_with_correlations()</span>
    <span class="s1">classifier_chain = ClassifierChain(LinearSVC(dual=</span><span class="s3">&quot;auto&quot;</span><span class="s1">))</span>
    <span class="s1">classifier_chain.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">Y_pred = classifier_chain.predict(X)</span>
    <span class="s0">assert </span><span class="s1">Y_pred.shape == Y.shape</span>

    <span class="s1">Y_decision = classifier_chain.decision_function(X)</span>

    <span class="s1">Y_binary = Y_decision &gt;= </span><span class="s2">0</span>
    <span class="s1">assert_array_equal(Y_binary</span><span class="s0">, </span><span class="s1">Y_pred)</span>
    <span class="s0">assert not </span><span class="s1">hasattr(classifier_chain</span><span class="s0">, </span><span class="s3">&quot;predict_proba&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_classifier_chain_fit_and_predict_with_sparse_data():</span>
    <span class="s4"># Fit classifier chain with sparse data</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = generate_multilabel_dataset_with_correlations()</span>
    <span class="s1">X_sparse = sp.csr_matrix(X)</span>

    <span class="s1">classifier_chain = ClassifierChain(LogisticRegression())</span>
    <span class="s1">classifier_chain.fit(X_sparse</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">Y_pred_sparse = classifier_chain.predict(X_sparse)</span>

    <span class="s1">classifier_chain = ClassifierChain(LogisticRegression())</span>
    <span class="s1">classifier_chain.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">Y_pred_dense = classifier_chain.predict(X)</span>

    <span class="s1">assert_array_equal(Y_pred_sparse</span><span class="s0">, </span><span class="s1">Y_pred_dense)</span>


<span class="s0">def </span><span class="s1">test_classifier_chain_vs_independent_models():</span>
    <span class="s4"># Verify that an ensemble of classifier chains (each of length</span>
    <span class="s4"># N) can achieve a higher Jaccard similarity score than N independent</span>
    <span class="s4"># models</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = generate_multilabel_dataset_with_correlations()</span>
    <span class="s1">X_train = X[:</span><span class="s2">600</span><span class="s0">, </span><span class="s1">:]</span>
    <span class="s1">X_test = X[</span><span class="s2">600</span><span class="s1">:</span><span class="s0">, </span><span class="s1">:]</span>
    <span class="s1">Y_train = Y[:</span><span class="s2">600</span><span class="s0">, </span><span class="s1">:]</span>
    <span class="s1">Y_test = Y[</span><span class="s2">600</span><span class="s1">:</span><span class="s0">, </span><span class="s1">:]</span>

    <span class="s1">ovr = OneVsRestClassifier(LogisticRegression())</span>
    <span class="s1">ovr.fit(X_train</span><span class="s0">, </span><span class="s1">Y_train)</span>
    <span class="s1">Y_pred_ovr = ovr.predict(X_test)</span>

    <span class="s1">chain = ClassifierChain(LogisticRegression())</span>
    <span class="s1">chain.fit(X_train</span><span class="s0">, </span><span class="s1">Y_train)</span>
    <span class="s1">Y_pred_chain = chain.predict(X_test)</span>

    <span class="s0">assert </span><span class="s1">jaccard_score(Y_test</span><span class="s0">, </span><span class="s1">Y_pred_chain</span><span class="s0">, </span><span class="s1">average=</span><span class="s3">&quot;samples&quot;</span><span class="s1">) &gt; jaccard_score(</span>
        <span class="s1">Y_test</span><span class="s0">, </span><span class="s1">Y_pred_ovr</span><span class="s0">, </span><span class="s1">average=</span><span class="s3">&quot;samples&quot;</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_base_chain_fit_and_predict():</span>
    <span class="s4"># Fit base chain and verify predict performance</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = generate_multilabel_dataset_with_correlations()</span>
    <span class="s1">chains = [RegressorChain(Ridge())</span><span class="s0">, </span><span class="s1">ClassifierChain(LogisticRegression())]</span>
    <span class="s0">for </span><span class="s1">chain </span><span class="s0">in </span><span class="s1">chains:</span>
        <span class="s1">chain.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
        <span class="s1">Y_pred = chain.predict(X)</span>
        <span class="s0">assert </span><span class="s1">Y_pred.shape == Y.shape</span>
        <span class="s0">assert </span><span class="s1">[c.coef_.size </span><span class="s0">for </span><span class="s1">c </span><span class="s0">in </span><span class="s1">chain.estimators_] == list(</span>
            <span class="s1">range(X.shape[</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">X.shape[</span><span class="s2">1</span><span class="s1">] + Y.shape[</span><span class="s2">1</span><span class="s1">])</span>
        <span class="s1">)</span>

    <span class="s1">Y_prob = chains[</span><span class="s2">1</span><span class="s1">].predict_proba(X)</span>
    <span class="s1">Y_binary = Y_prob &gt;= </span><span class="s2">0.5</span>
    <span class="s1">assert_array_equal(Y_binary</span><span class="s0">, </span><span class="s1">Y_pred)</span>

    <span class="s0">assert </span><span class="s1">isinstance(chains[</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">ClassifierMixin)</span>


<span class="s0">def </span><span class="s1">test_base_chain_fit_and_predict_with_sparse_data_and_cv():</span>
    <span class="s4"># Fit base chain with sparse data cross_val_predict</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = generate_multilabel_dataset_with_correlations()</span>
    <span class="s1">X_sparse = sp.csr_matrix(X)</span>
    <span class="s1">base_chains = [</span>
        <span class="s1">ClassifierChain(LogisticRegression()</span><span class="s0">, </span><span class="s1">cv=</span><span class="s2">3</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">RegressorChain(Ridge()</span><span class="s0">, </span><span class="s1">cv=</span><span class="s2">3</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s0">for </span><span class="s1">chain </span><span class="s0">in </span><span class="s1">base_chains:</span>
        <span class="s1">chain.fit(X_sparse</span><span class="s0">, </span><span class="s1">Y)</span>
        <span class="s1">Y_pred = chain.predict(X_sparse)</span>
        <span class="s0">assert </span><span class="s1">Y_pred.shape == Y.shape</span>


<span class="s0">def </span><span class="s1">test_base_chain_random_order():</span>
    <span class="s4"># Fit base chain with random order</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = generate_multilabel_dataset_with_correlations()</span>
    <span class="s0">for </span><span class="s1">chain </span><span class="s0">in </span><span class="s1">[ClassifierChain(LogisticRegression())</span><span class="s0">, </span><span class="s1">RegressorChain(Ridge())]:</span>
        <span class="s1">chain_random = clone(chain).set_params(order=</span><span class="s3">&quot;random&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">42</span><span class="s1">)</span>
        <span class="s1">chain_random.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
        <span class="s1">chain_fixed = clone(chain).set_params(order=chain_random.order_)</span>
        <span class="s1">chain_fixed.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
        <span class="s1">assert_array_equal(chain_fixed.order_</span><span class="s0">, </span><span class="s1">chain_random.order_)</span>
        <span class="s0">assert </span><span class="s1">list(chain_random.order) != list(range(</span><span class="s2">4</span><span class="s1">))</span>
        <span class="s0">assert </span><span class="s1">len(chain_random.order_) == </span><span class="s2">4</span>
        <span class="s0">assert </span><span class="s1">len(set(chain_random.order_)) == </span><span class="s2">4</span>
        <span class="s4"># Randomly ordered chain should behave identically to a fixed order</span>
        <span class="s4"># chain with the same order.</span>
        <span class="s0">for </span><span class="s1">est1</span><span class="s0">, </span><span class="s1">est2 </span><span class="s0">in </span><span class="s1">zip(chain_random.estimators_</span><span class="s0">, </span><span class="s1">chain_fixed.estimators_):</span>
            <span class="s1">assert_array_almost_equal(est1.coef_</span><span class="s0">, </span><span class="s1">est2.coef_)</span>


<span class="s0">def </span><span class="s1">test_base_chain_crossval_fit_and_predict():</span>
    <span class="s4"># Fit chain with cross_val_predict and verify predict</span>
    <span class="s4"># performance</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = generate_multilabel_dataset_with_correlations()</span>

    <span class="s0">for </span><span class="s1">chain </span><span class="s0">in </span><span class="s1">[ClassifierChain(LogisticRegression())</span><span class="s0">, </span><span class="s1">RegressorChain(Ridge())]:</span>
        <span class="s1">chain.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
        <span class="s1">chain_cv = clone(chain).set_params(cv=</span><span class="s2">3</span><span class="s1">)</span>
        <span class="s1">chain_cv.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
        <span class="s1">Y_pred_cv = chain_cv.predict(X)</span>
        <span class="s1">Y_pred = chain.predict(X)</span>

        <span class="s0">assert </span><span class="s1">Y_pred_cv.shape == Y_pred.shape</span>
        <span class="s0">assert not </span><span class="s1">np.all(Y_pred == Y_pred_cv)</span>
        <span class="s0">if </span><span class="s1">isinstance(chain</span><span class="s0">, </span><span class="s1">ClassifierChain):</span>
            <span class="s0">assert </span><span class="s1">jaccard_score(Y</span><span class="s0">, </span><span class="s1">Y_pred_cv</span><span class="s0">, </span><span class="s1">average=</span><span class="s3">&quot;samples&quot;</span><span class="s1">) &gt; </span><span class="s2">0.4</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">assert </span><span class="s1">mean_squared_error(Y</span><span class="s0">, </span><span class="s1">Y_pred_cv) &lt; </span><span class="s2">0.25</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;estimator&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">RandomForestClassifier(n_estimators=</span><span class="s2">2</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">MultiOutputClassifier(RandomForestClassifier(n_estimators=</span><span class="s2">2</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">ClassifierChain(RandomForestClassifier(n_estimators=</span><span class="s2">2</span><span class="s1">))</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_multi_output_classes_(estimator):</span>
    <span class="s4"># Tests classes_ attribute of multioutput classifiers</span>
    <span class="s4"># RandomForestClassifier supports multioutput out-of-the-box</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">isinstance(estimator.classes_</span><span class="s0">, </span><span class="s1">list)</span>
    <span class="s0">assert </span><span class="s1">len(estimator.classes_) == n_outputs</span>
    <span class="s0">for </span><span class="s1">estimator_classes</span><span class="s0">, </span><span class="s1">expected_classes </span><span class="s0">in </span><span class="s1">zip(classes</span><span class="s0">, </span><span class="s1">estimator.classes_):</span>
        <span class="s1">assert_array_equal(estimator_classes</span><span class="s0">, </span><span class="s1">expected_classes)</span>


<span class="s0">class </span><span class="s1">DummyRegressorWithFitParams(DummyRegressor):</span>
    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=</span><span class="s0">None, </span><span class="s1">**fit_params):</span>
        <span class="s1">self._fit_params = fit_params</span>
        <span class="s0">return </span><span class="s1">super().fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight)</span>


<span class="s0">class </span><span class="s1">DummyClassifierWithFitParams(DummyClassifier):</span>
    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=</span><span class="s0">None, </span><span class="s1">**fit_params):</span>
        <span class="s1">self._fit_params = fit_params</span>
        <span class="s0">return </span><span class="s1">super().fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight)</span>


<span class="s1">@pytest.mark.filterwarnings(</span><span class="s3">&quot;ignore:`n_features_in_` is deprecated&quot;</span><span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;estimator, dataset&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">MultiOutputClassifier(DummyClassifierWithFitParams(strategy=</span><span class="s3">&quot;prior&quot;</span><span class="s1">))</span><span class="s0">,</span>
            <span class="s1">datasets.make_multilabel_classification()</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">MultiOutputRegressor(DummyRegressorWithFitParams())</span><span class="s0">,</span>
            <span class="s1">datasets.make_regression(n_targets=</span><span class="s2">3</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_multioutput_estimator_with_fit_params(estimator</span><span class="s0">, </span><span class="s1">dataset):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = dataset</span>
    <span class="s1">some_param = np.zeros_like(X)</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">some_param=some_param)</span>
    <span class="s0">for </span><span class="s1">dummy_estimator </span><span class="s0">in </span><span class="s1">estimator.estimators_:</span>
        <span class="s0">assert </span><span class="s3">&quot;some_param&quot; </span><span class="s0">in </span><span class="s1">dummy_estimator._fit_params</span>


<span class="s0">def </span><span class="s1">test_regressor_chain_w_fit_params():</span>
    <span class="s4"># Make sure fit_params are properly propagated to the sub-estimators</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_regression(n_targets=</span><span class="s2">3</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">weight = rng.rand(y.shape[</span><span class="s2">0</span><span class="s1">])</span>

    <span class="s0">class </span><span class="s1">MySGD(SGDRegressor):</span>
        <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">**fit_params):</span>
            <span class="s1">self.sample_weight_ = fit_params[</span><span class="s3">&quot;sample_weight&quot;</span><span class="s1">]</span>
            <span class="s1">super().fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">**fit_params)</span>

    <span class="s1">model = RegressorChain(MySGD())</span>

    <span class="s4"># Fitting with params</span>
    <span class="s1">fit_param = {</span><span class="s3">&quot;sample_weight&quot;</span><span class="s1">: weight}</span>
    <span class="s1">model.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">**fit_param)</span>

    <span class="s0">for </span><span class="s1">est </span><span class="s0">in </span><span class="s1">model.estimators_:</span>
        <span class="s0">assert </span><span class="s1">est.sample_weight_ </span><span class="s0">is </span><span class="s1">weight</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;MultiOutputEstimator, Estimator&quot;</span><span class="s0">,</span>
    <span class="s1">[(MultiOutputClassifier</span><span class="s0">, </span><span class="s1">LogisticRegression)</span><span class="s0">, </span><span class="s1">(MultiOutputRegressor</span><span class="s0">, </span><span class="s1">Ridge)]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s4"># FIXME: we should move this test in `estimator_checks` once we are able</span>
<span class="s4"># to construct meta-estimator instances</span>
<span class="s0">def </span><span class="s1">test_support_missing_values(MultiOutputEstimator</span><span class="s0">, </span><span class="s1">Estimator):</span>
    <span class="s4"># smoke test to check that pipeline MultioutputEstimators are letting</span>
    <span class="s4"># the validation of missing values to</span>
    <span class="s4"># the underlying pipeline, regressor or classifier</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = rng.randn(</span><span class="s2">50</span><span class="s0">, </span><span class="s2">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">rng.binomial(</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0.5</span><span class="s0">, </span><span class="s1">(</span><span class="s2">50</span><span class="s0">, </span><span class="s2">3</span><span class="s1">))</span>
    <span class="s1">mask = rng.choice([</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">X.shape</span><span class="s0">, </span><span class="s1">p=[</span><span class="s2">0.01</span><span class="s0">, </span><span class="s2">0.99</span><span class="s1">]).astype(bool)</span>
    <span class="s1">X[mask] = np.nan</span>

    <span class="s1">pipe = make_pipeline(SimpleImputer()</span><span class="s0">, </span><span class="s1">Estimator())</span>
    <span class="s1">MultiOutputEstimator(pipe).fit(X</span><span class="s0">, </span><span class="s1">y).score(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;order_type&quot;</span><span class="s0">, </span><span class="s1">[list</span><span class="s0">, </span><span class="s1">np.array</span><span class="s0">, </span><span class="s1">tuple])</span>
<span class="s0">def </span><span class="s1">test_classifier_chain_tuple_order(order_type):</span>
    <span class="s1">X = [[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">6</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1.5</span><span class="s0">, </span><span class="s2">2.5</span><span class="s0">, </span><span class="s2">3.5</span><span class="s1">]]</span>
    <span class="s1">y = [[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]]</span>
    <span class="s1">order = order_type([</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">])</span>

    <span class="s1">chain = ClassifierChain(RandomForestClassifier()</span><span class="s0">, </span><span class="s1">order=order)</span>

    <span class="s1">chain.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">X_test = [[</span><span class="s2">1.5</span><span class="s0">, </span><span class="s2">2.5</span><span class="s0">, </span><span class="s2">3.5</span><span class="s1">]]</span>
    <span class="s1">y_test = [[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]]</span>
    <span class="s1">assert_array_almost_equal(chain.predict(X_test)</span><span class="s0">, </span><span class="s1">y_test)</span>


<span class="s0">def </span><span class="s1">test_classifier_chain_tuple_invalid_order():</span>
    <span class="s1">X = [[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">6</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1.5</span><span class="s0">, </span><span class="s2">2.5</span><span class="s0">, </span><span class="s2">3.5</span><span class="s1">]]</span>
    <span class="s1">y = [[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]]</span>
    <span class="s1">order = tuple([</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">])</span>

    <span class="s1">chain = ClassifierChain(RandomForestClassifier()</span><span class="s0">, </span><span class="s1">order=order)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s3">&quot;invalid order&quot;</span><span class="s1">):</span>
        <span class="s1">chain.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_classifier_chain_verbose(capsys):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_multilabel_classification(</span>
        <span class="s1">n_samples=</span><span class="s2">100</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">n_classes=</span><span class="s2">3</span><span class="s0">, </span><span class="s1">n_labels=</span><span class="s2">3</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span>
    <span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s1">pattern = (</span>
        <span class="s3">r&quot;\[Chain\].*\(1 of 3\) Processing order 0, total=.*\n&quot;</span>
        <span class="s3">r&quot;\[Chain\].*\(2 of 3\) Processing order 1, total=.*\n&quot;</span>
        <span class="s3">r&quot;\[Chain\].*\(3 of 3\) Processing order 2, total=.*\n$&quot;</span>
    <span class="s1">)</span>

    <span class="s1">classifier = ClassifierChain(</span>
        <span class="s1">DecisionTreeClassifier()</span><span class="s0">,</span>
        <span class="s1">order=[</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s2">0</span><span class="s0">,</span>
        <span class="s1">verbose=</span><span class="s0">True,</span>
    <span class="s1">)</span>
    <span class="s1">classifier.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s0">assert </span><span class="s1">re.match(pattern</span><span class="s0">, </span><span class="s1">capsys.readouterr()[</span><span class="s2">0</span><span class="s1">])</span>


<span class="s0">def </span><span class="s1">test_regressor_chain_verbose(capsys):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_samples=</span><span class="s2">125</span><span class="s0">, </span><span class="s1">n_targets=</span><span class="s2">3</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s1">pattern = (</span>
        <span class="s3">r&quot;\[Chain\].*\(1 of 3\) Processing order 1, total=.*\n&quot;</span>
        <span class="s3">r&quot;\[Chain\].*\(2 of 3\) Processing order 0, total=.*\n&quot;</span>
        <span class="s3">r&quot;\[Chain\].*\(3 of 3\) Processing order 2, total=.*\n$&quot;</span>
    <span class="s1">)</span>
    <span class="s1">regressor = RegressorChain(</span>
        <span class="s1">LinearRegression()</span><span class="s0">,</span>
        <span class="s1">order=[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s2">0</span><span class="s0">,</span>
        <span class="s1">verbose=</span><span class="s0">True,</span>
    <span class="s1">)</span>
    <span class="s1">regressor.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s0">assert </span><span class="s1">re.match(pattern</span><span class="s0">, </span><span class="s1">capsys.readouterr()[</span><span class="s2">0</span><span class="s1">])</span>


<span class="s0">def </span><span class="s1">test_multioutputregressor_ducktypes_fitted_estimator():</span>
    <span class="s5">&quot;&quot;&quot;Test that MultiOutputRegressor checks the fitted estimator for 
    predict. Non-regression test for #16549.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = load_linnerud(return_X_y=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">stacker = StackingRegressor(</span>
        <span class="s1">estimators=[(</span><span class="s3">&quot;sgd&quot;</span><span class="s0">, </span><span class="s1">SGDRegressor(random_state=</span><span class="s2">1</span><span class="s1">))]</span><span class="s0">,</span>
        <span class="s1">final_estimator=Ridge()</span><span class="s0">,</span>
        <span class="s1">cv=</span><span class="s2">2</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">reg = MultiOutputRegressor(estimator=stacker).fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s4"># Does not raise</span>
    <span class="s1">reg.predict(X)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;Cls, method&quot;</span><span class="s0">, </span><span class="s1">[(ClassifierChain</span><span class="s0">, </span><span class="s3">&quot;fit&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(MultiOutputClassifier</span><span class="s0">, </span><span class="s3">&quot;partial_fit&quot;</span><span class="s1">)]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_fit_params_no_routing(Cls</span><span class="s0">, </span><span class="s1">method):</span>
    <span class="s5">&quot;&quot;&quot;Check that we raise an error when passing metadata not requested by the 
    underlying classifier. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s2">50</span><span class="s1">)</span>
    <span class="s1">clf = Cls(PassiveAggressiveClassifier())</span>

    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s3">&quot;is only supported if&quot;</span><span class="s1">):</span>
        <span class="s1">getattr(clf</span><span class="s0">, </span><span class="s1">method)(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">test=</span><span class="s2">1</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_multioutput_regressor_has_partial_fit():</span>
    <span class="s4"># Test that an unfitted MultiOutputRegressor handles available_if for</span>
    <span class="s4"># partial_fit correctly</span>
    <span class="s1">est = MultiOutputRegressor(LinearRegression())</span>
    <span class="s1">msg = </span><span class="s3">&quot;This 'MultiOutputRegressor' has no attribute 'partial_fit'&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(AttributeError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">getattr(est</span><span class="s0">, </span><span class="s3">&quot;partial_fit&quot;</span><span class="s1">)</span>
</pre>
</body>
</html>