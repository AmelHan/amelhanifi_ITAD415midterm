<html>
<head>
<title>l1_cvxopt.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
l1_cvxopt.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Holds files for l1 regularization of LikelihoodModel, using cvxopt. 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">statsmodels.base.l1_solvers_common </span><span class="s2">as </span><span class="s1">l1_solvers_common</span>


<span class="s2">def </span><span class="s1">fit_l1_cvxopt_cp(</span>
        <span class="s1">f</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">kwargs</span><span class="s2">, </span><span class="s1">disp=</span><span class="s2">False, </span><span class="s1">maxiter=</span><span class="s3">100</span><span class="s2">,</span>
        <span class="s1">callback=</span><span class="s2">None, </span><span class="s1">retall=</span><span class="s2">False, </span><span class="s1">full_output=</span><span class="s2">False, </span><span class="s1">hess=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Solve the l1 regularized problem using cvxopt.solvers.cp 
 
    Specifically:  We convert the convex but non-smooth problem 
 
    .. math:: \\min_\\beta f(\\beta) + \\sum_k\\alpha_k |\\beta_k| 
 
    via the transformation to the smooth, convex, constrained problem in twice 
    as many variables (adding the &quot;added variables&quot; :math:`u_k`) 
 
    .. math:: \\min_{\\beta,u} f(\\beta) + \\sum_k\\alpha_k u_k, 
 
    subject to 
 
    .. math:: -u_k \\leq \\beta_k \\leq u_k. 
 
    Parameters 
    ---------- 
    All the usual parameters from LikelhoodModel.fit 
    alpha : non-negative scalar or numpy array (same size as parameters) 
        The weight multiplying the l1 penalty term 
    trim_mode : 'auto, 'size', or 'off' 
        If not 'off', trim (set to zero) parameters that would have been zero 
            if the solver reached the theoretical minimum. 
        If 'auto', trim params using the Theory above. 
        If 'size', trim params if they have very small absolute value 
    size_trim_tol : float or 'auto' (default = 'auto') 
        For use when trim_mode === 'size' 
    auto_trim_tol : float 
        For sue when trim_mode == 'auto'.  Use 
    qc_tol : float 
        Print warning and do not allow auto trim when (ii) in &quot;Theory&quot; (above) 
        is violated by this much. 
    qc_verbose : bool 
        If true, print out a full QC report upon failure 
    abstol : float 
        absolute accuracy (default: 1e-7). 
    reltol : float 
        relative accuracy (default: 1e-6). 
    feastol : float 
        tolerance for feasibility conditions (default: 1e-7). 
    refinement : int 
        number of iterative refinement steps when solving KKT equations 
        (default: 1). 
    &quot;&quot;&quot;</span>
    <span class="s2">from </span><span class="s1">cvxopt </span><span class="s2">import </span><span class="s1">solvers</span><span class="s2">, </span><span class="s1">matrix</span>

    <span class="s1">start_params = np.array(start_params).ravel(</span><span class="s4">'F'</span><span class="s1">)</span>

    <span class="s5">## Extract arguments</span>
    <span class="s5"># k_params is total number of covariates, possibly including a leading constant.</span>
    <span class="s1">k_params = len(start_params)</span>
    <span class="s5"># The start point</span>
    <span class="s1">x0 = np.append(start_params</span><span class="s2">, </span><span class="s1">np.fabs(start_params))</span>
    <span class="s1">x0 = matrix(x0</span><span class="s2">, </span><span class="s1">(</span><span class="s3">2 </span><span class="s1">* k_params</span><span class="s2">, </span><span class="s3">1</span><span class="s1">))</span>
    <span class="s5"># The regularization parameter</span>
    <span class="s1">alpha = np.array(kwargs[</span><span class="s4">'alpha_rescaled'</span><span class="s1">]).ravel(</span><span class="s4">'F'</span><span class="s1">)</span>
    <span class="s5"># Make sure it's a vector</span>
    <span class="s1">alpha = alpha * np.ones(k_params)</span>
    <span class="s2">assert </span><span class="s1">alpha.min() &gt;= </span><span class="s3">0</span>

    <span class="s5">## Wrap up functions for cvxopt</span>
    <span class="s1">f_0 = </span><span class="s2">lambda </span><span class="s1">x: _objective_func(f</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">k_params</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">*args)</span>
    <span class="s1">Df = </span><span class="s2">lambda </span><span class="s1">x: _fprime(score</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">k_params</span><span class="s2">, </span><span class="s1">alpha)</span>
    <span class="s1">G = _get_G(k_params)  </span><span class="s5"># Inequality constraint matrix, Gx \leq h</span>
    <span class="s1">h = matrix(</span><span class="s3">0.0</span><span class="s2">, </span><span class="s1">(</span><span class="s3">2 </span><span class="s1">* k_params</span><span class="s2">, </span><span class="s3">1</span><span class="s1">))  </span><span class="s5"># RHS in inequality constraint</span>
    <span class="s1">H = </span><span class="s2">lambda </span><span class="s1">x</span><span class="s2">, </span><span class="s1">z: _hessian_wrapper(hess</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">z</span><span class="s2">, </span><span class="s1">k_params)</span>

    <span class="s5">## Define the optimization function</span>
    <span class="s2">def </span><span class="s1">F(x=</span><span class="s2">None, </span><span class="s1">z=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">if </span><span class="s1">x </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s3">0</span><span class="s2">, </span><span class="s1">x0</span>
        <span class="s2">elif </span><span class="s1">z </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">f_0(x)</span><span class="s2">, </span><span class="s1">Df(x)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">f_0(x)</span><span class="s2">, </span><span class="s1">Df(x)</span><span class="s2">, </span><span class="s1">H(x</span><span class="s2">, </span><span class="s1">z)</span>

    <span class="s5">## Convert optimization settings to cvxopt form</span>
    <span class="s1">solvers.options[</span><span class="s4">'show_progress'</span><span class="s1">] = disp</span>
    <span class="s1">solvers.options[</span><span class="s4">'maxiters'</span><span class="s1">] = maxiter</span>
    <span class="s2">if </span><span class="s4">'abstol' </span><span class="s2">in </span><span class="s1">kwargs:</span>
        <span class="s1">solvers.options[</span><span class="s4">'abstol'</span><span class="s1">] = kwargs[</span><span class="s4">'abstol'</span><span class="s1">]</span>
    <span class="s2">if </span><span class="s4">'reltol' </span><span class="s2">in </span><span class="s1">kwargs:</span>
        <span class="s1">solvers.options[</span><span class="s4">'reltol'</span><span class="s1">] = kwargs[</span><span class="s4">'reltol'</span><span class="s1">]</span>
    <span class="s2">if </span><span class="s4">'feastol' </span><span class="s2">in </span><span class="s1">kwargs:</span>
        <span class="s1">solvers.options[</span><span class="s4">'feastol'</span><span class="s1">] = kwargs[</span><span class="s4">'feastol'</span><span class="s1">]</span>
    <span class="s2">if </span><span class="s4">'refinement' </span><span class="s2">in </span><span class="s1">kwargs:</span>
        <span class="s1">solvers.options[</span><span class="s4">'refinement'</span><span class="s1">] = kwargs[</span><span class="s4">'refinement'</span><span class="s1">]</span>

    <span class="s5">### Call the optimizer</span>
    <span class="s1">results = solvers.cp(F</span><span class="s2">, </span><span class="s1">G</span><span class="s2">, </span><span class="s1">h)</span>
    <span class="s1">x = np.asarray(results[</span><span class="s4">'x'</span><span class="s1">]).ravel()</span>
    <span class="s1">params = x[:k_params]</span>

    <span class="s5">### Post-process</span>
    <span class="s5"># QC</span>
    <span class="s1">qc_tol = kwargs[</span><span class="s4">'qc_tol'</span><span class="s1">]</span>
    <span class="s1">qc_verbose = kwargs[</span><span class="s4">'qc_verbose'</span><span class="s1">]</span>
    <span class="s1">passed = l1_solvers_common.qc_results(</span>
        <span class="s1">params</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">qc_tol</span><span class="s2">, </span><span class="s1">qc_verbose)</span>
    <span class="s5"># Possibly trim</span>
    <span class="s1">trim_mode = kwargs[</span><span class="s4">'trim_mode'</span><span class="s1">]</span>
    <span class="s1">size_trim_tol = kwargs[</span><span class="s4">'size_trim_tol'</span><span class="s1">]</span>
    <span class="s1">auto_trim_tol = kwargs[</span><span class="s4">'auto_trim_tol'</span><span class="s1">]</span>
    <span class="s1">params</span><span class="s2">, </span><span class="s1">trimmed = l1_solvers_common.do_trim_params(</span>
        <span class="s1">params</span><span class="s2">, </span><span class="s1">k_params</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">passed</span><span class="s2">, </span><span class="s1">trim_mode</span><span class="s2">, </span><span class="s1">size_trim_tol</span><span class="s2">,</span>
        <span class="s1">auto_trim_tol)</span>

    <span class="s5">### Pack up return values for statsmodels</span>
    <span class="s5"># TODO These retvals are returned as mle_retvals...but the fit was not ML</span>
    <span class="s2">if </span><span class="s1">full_output:</span>
        <span class="s1">fopt = f_0(x)</span>
        <span class="s1">gopt = float(</span><span class="s4">'nan'</span><span class="s1">)  </span><span class="s5"># Objective is non-differentiable</span>
        <span class="s1">hopt = float(</span><span class="s4">'nan'</span><span class="s1">)</span>
        <span class="s1">iterations = float(</span><span class="s4">'nan'</span><span class="s1">)</span>
        <span class="s1">converged = (results[</span><span class="s4">'status'</span><span class="s1">] == </span><span class="s4">'optimal'</span><span class="s1">)</span>
        <span class="s1">warnflag = results[</span><span class="s4">'status'</span><span class="s1">]</span>
        <span class="s1">retvals = {</span>
            <span class="s4">'fopt'</span><span class="s1">: fopt</span><span class="s2">, </span><span class="s4">'converged'</span><span class="s1">: converged</span><span class="s2">, </span><span class="s4">'iterations'</span><span class="s1">: iterations</span><span class="s2">,</span>
            <span class="s4">'gopt'</span><span class="s1">: gopt</span><span class="s2">, </span><span class="s4">'hopt'</span><span class="s1">: hopt</span><span class="s2">, </span><span class="s4">'trimmed'</span><span class="s1">: trimmed</span><span class="s2">,</span>
            <span class="s4">'warnflag'</span><span class="s1">: warnflag}</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">x = np.array(results[</span><span class="s4">'x'</span><span class="s1">]).ravel()</span>
        <span class="s1">params = x[:k_params]</span>

    <span class="s5">### Return results</span>
    <span class="s2">if </span><span class="s1">full_output:</span>
        <span class="s2">return </span><span class="s1">params</span><span class="s2">, </span><span class="s1">retvals</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">params</span>


<span class="s2">def </span><span class="s1">_objective_func(f</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">k_params</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">*args):</span>
    <span class="s0">&quot;&quot;&quot; 
    The regularized objective function. 
    &quot;&quot;&quot;</span>
    <span class="s2">from </span><span class="s1">cvxopt </span><span class="s2">import </span><span class="s1">matrix</span>

    <span class="s1">x_arr = np.asarray(x)</span>
    <span class="s1">params = x_arr[:k_params].ravel()</span>
    <span class="s1">u = x_arr[k_params:]</span>
    <span class="s5"># Call the numpy version</span>
    <span class="s1">objective_func_arr = f(params</span><span class="s2">, </span><span class="s1">*args) + (alpha * u).sum()</span>
    <span class="s5"># Return</span>
    <span class="s2">return </span><span class="s1">matrix(objective_func_arr)</span>


<span class="s2">def </span><span class="s1">_fprime(score</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">k_params</span><span class="s2">, </span><span class="s1">alpha):</span>
    <span class="s0">&quot;&quot;&quot; 
    The regularized derivative. 
    &quot;&quot;&quot;</span>
    <span class="s2">from </span><span class="s1">cvxopt </span><span class="s2">import </span><span class="s1">matrix</span>

    <span class="s1">x_arr = np.asarray(x)</span>
    <span class="s1">params = x_arr[:k_params].ravel()</span>
    <span class="s5"># Call the numpy version</span>
    <span class="s5"># The derivative just appends a vector of constants</span>
    <span class="s1">fprime_arr = np.append(score(params)</span><span class="s2">, </span><span class="s1">alpha)</span>
    <span class="s5"># Return</span>
    <span class="s2">return </span><span class="s1">matrix(fprime_arr</span><span class="s2">, </span><span class="s1">(</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2 </span><span class="s1">* k_params))</span>


<span class="s2">def </span><span class="s1">_get_G(k_params):</span>
    <span class="s0">&quot;&quot;&quot; 
    The linear inequality constraint matrix. 
    &quot;&quot;&quot;</span>
    <span class="s2">from </span><span class="s1">cvxopt </span><span class="s2">import </span><span class="s1">matrix</span>

    <span class="s1">I = np.eye(k_params)  </span><span class="s5"># noqa:E741</span>
    <span class="s1">A = np.concatenate((-I</span><span class="s2">, </span><span class="s1">-I)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">B = np.concatenate((I</span><span class="s2">, </span><span class="s1">-I)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">C = np.concatenate((A</span><span class="s2">, </span><span class="s1">B)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s5"># Return</span>
    <span class="s2">return </span><span class="s1">matrix(C)</span>


<span class="s2">def </span><span class="s1">_hessian_wrapper(hess</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">z</span><span class="s2">, </span><span class="s1">k_params):</span>
    <span class="s0">&quot;&quot;&quot; 
    Wraps the hessian up in the form for cvxopt. 
 
    cvxopt wants the hessian of the objective function and the constraints. 
        Since our constraints are linear, this part is all zeros. 
    &quot;&quot;&quot;</span>
    <span class="s2">from </span><span class="s1">cvxopt </span><span class="s2">import </span><span class="s1">matrix</span>

    <span class="s1">x_arr = np.asarray(x)</span>
    <span class="s1">params = x_arr[:k_params].ravel()</span>
    <span class="s1">zh_x = np.asarray(z[</span><span class="s3">0</span><span class="s1">]) * hess(params)</span>
    <span class="s1">zero_mat = np.zeros(zh_x.shape)</span>
    <span class="s1">A = np.concatenate((zh_x</span><span class="s2">, </span><span class="s1">zero_mat)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">B = np.concatenate((zero_mat</span><span class="s2">, </span><span class="s1">zero_mat)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">zh_x_ext = np.concatenate((A</span><span class="s2">, </span><span class="s1">B)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">matrix(zh_x_ext</span><span class="s2">, </span><span class="s1">(</span><span class="s3">2 </span><span class="s1">* k_params</span><span class="s2">, </span><span class="s3">2 </span><span class="s1">* k_params))</span>
</pre>
</body>
</html>