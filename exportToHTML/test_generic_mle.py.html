<html>
<head>
<title>test_generic_mle.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_generic_mle.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot; 
 
Created on Fri Jun 28 14:19:26 2013 
 
Author: Josef Perktold 
&quot;&quot;&quot;</span>


<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">stats</span>
<span class="s3">from </span><span class="s1">statsmodels.base.model </span><span class="s3">import </span><span class="s1">GenericLikelihoodModel</span>

<span class="s3">from </span><span class="s1">numpy.testing </span><span class="s3">import </span><span class="s1">(assert_array_less</span><span class="s3">, </span><span class="s1">assert_almost_equal</span><span class="s3">,</span>
                           <span class="s1">assert_allclose)</span>

<span class="s3">class </span><span class="s1">MyPareto(GenericLikelihoodModel):</span>
    <span class="s2">'''Maximum Likelihood Estimation pareto distribution 
 
    first version: iid case, with constant parameters 
    '''</span>

    <span class="s3">def </span><span class="s1">initialize(self):   </span><span class="s0">#TODO needed or not</span>
        <span class="s1">super(MyPareto</span><span class="s3">, </span><span class="s1">self).initialize()</span>
        <span class="s1">extra_params_names = [</span><span class="s4">'shape'</span><span class="s3">, </span><span class="s4">'loc'</span><span class="s3">, </span><span class="s4">'scale'</span><span class="s1">]</span>
        <span class="s1">self._set_extra_params_names(extra_params_names)</span>

        <span class="s0">#start_params needs to be attribute</span>
        <span class="s1">self.start_params = np.array([</span><span class="s5">1.5</span><span class="s3">, </span><span class="s1">self.endog.min() - </span><span class="s5">1.5</span><span class="s3">, </span><span class="s5">1.</span><span class="s1">])</span>


    <span class="s0">#copied from stats.distribution</span>
    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">b):</span>
        <span class="s3">return </span><span class="s1">b * x**(-b-</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">return </span><span class="s1">-self.nloglikeobs(params).sum(</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s0"># TODO: design start_params needs to be an attribute,</span>
    <span class="s0"># so it can be overwritten</span>
<span class="s0">#    @property</span>
<span class="s0">#    def start_params(self):</span>
<span class="s0">#        return np.array([1.5, self.endog.min() - 1.5, 1.])</span>

    <span class="s3">def </span><span class="s1">nloglikeobs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">#print params.shape</span>
        <span class="s3">if </span><span class="s1">self.fixed_params </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s0">#print 'using fixed'</span>
            <span class="s1">params = self.expandparams(params)</span>
        <span class="s1">b = params[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">loc = params[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">scale = params[</span><span class="s5">2</span><span class="s1">]</span>
        <span class="s0">#loc = np.dot(self.exog, beta)</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">x = (endog - loc)/scale</span>
        <span class="s1">logpdf = np.log(b) - (b+</span><span class="s5">1.</span><span class="s1">)*np.log(x)  </span><span class="s0">#use np_log(1 + x) for Pareto II</span>
        <span class="s1">logpdf -= np.log(scale)</span>
        <span class="s0">#lb = loc + scale</span>
        <span class="s0">#logpdf[endog&lt;lb] = -inf</span>
        <span class="s0">#import pdb; pdb.set_trace()</span>
        <span class="s1">logpdf[x&lt;</span><span class="s5">1</span><span class="s1">] = -</span><span class="s5">10000 </span><span class="s0">#-np.inf</span>
        <span class="s3">return </span><span class="s1">-logpdf</span>


<span class="s3">class </span><span class="s1">CheckGenericMixin:</span>
    <span class="s0"># mostly smoke tests for now</span>

    <span class="s3">def </span><span class="s1">test_summary(self):</span>
        <span class="s1">summ = self.res1.summary()</span>
        <span class="s1">check_str = </span><span class="s4">'P&gt;|t|' </span><span class="s3">if </span><span class="s1">self.res1.use_t </span><span class="s3">else </span><span class="s4">'P&gt;|z|'</span>
        <span class="s3">assert </span><span class="s1">check_str </span><span class="s3">in </span><span class="s1">str(summ)</span>

    <span class="s3">def </span><span class="s1">test_use_t_summary(self):</span>
        <span class="s1">orig_val = self.res1.use_t</span>
        <span class="s1">self.res1.use_t = </span><span class="s3">True</span>
        <span class="s1">summ = self.res1.summary()</span>
        <span class="s3">assert </span><span class="s4">'P&gt;|t|' </span><span class="s3">in </span><span class="s1">str(summ)</span>
        <span class="s1">self.res1.use_t = orig_val</span>

    <span class="s3">def </span><span class="s1">test_ttest(self):</span>
        <span class="s1">self.res1.t_test(np.eye(len(self.res1.params)))</span>

    <span class="s3">def </span><span class="s1">test_params(self):</span>
        <span class="s1">params = self.res1.params</span>

        <span class="s1">params_true = np.array([</span><span class="s5">2</span><span class="s3">,</span><span class="s5">0</span><span class="s3">,</span><span class="s5">2</span><span class="s1">])</span>
        <span class="s3">if </span><span class="s1">self.res1.model.fixed_paramsmask </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">params_true = params_true[self.res1.model.fixed_paramsmask]</span>
        <span class="s1">assert_allclose(params</span><span class="s3">, </span><span class="s1">params_true</span><span class="s3">, </span><span class="s1">atol=</span><span class="s5">1.5</span><span class="s1">)</span>
        <span class="s1">assert_allclose(params</span><span class="s3">, </span><span class="s1">np.zeros(len(params))</span><span class="s3">, </span><span class="s1">atol=</span><span class="s5">4</span><span class="s1">)</span>

        <span class="s1">assert_allclose(self.res1.bse</span><span class="s3">, </span><span class="s1">np.zeros(len(params))</span><span class="s3">, </span><span class="s1">atol=</span><span class="s5">0.5</span><span class="s1">)</span>
        <span class="s3">if not </span><span class="s1">self.skip_bsejac:</span>
            <span class="s1">assert_allclose(self.res1.bse</span><span class="s3">, </span><span class="s1">self.res1.bsejac</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">0.05</span><span class="s3">,</span>
                            <span class="s1">atol=</span><span class="s5">0.15</span><span class="s1">)</span>
            <span class="s0"># bsejhj is very different from the other two</span>
            <span class="s0"># use huge atol as sanity check for availability</span>
            <span class="s1">assert_allclose(self.res1.bsejhj</span><span class="s3">, </span><span class="s1">self.res1.bsejac</span><span class="s3">,</span>
                            <span class="s1">rtol=</span><span class="s5">0.05</span><span class="s3">, </span><span class="s1">atol=</span><span class="s5">1.5</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">test_df(self):</span>
        <span class="s1">res = self.res1</span>
        <span class="s1">k_extra = getattr(self</span><span class="s3">, </span><span class="s4">&quot;k_extra&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">res.model.exog </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">nobs</span><span class="s3">, </span><span class="s1">k_vars = res.model.exog.shape</span>
            <span class="s1">k_constant = </span><span class="s5">1</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">nobs</span><span class="s3">, </span><span class="s1">k_vars = res.model.endog.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">0</span>
            <span class="s1">k_constant = </span><span class="s5">0</span>
        <span class="s3">assert </span><span class="s1">res.df_resid == nobs - k_vars - k_extra</span>
        <span class="s3">assert </span><span class="s1">res.df_model == k_vars - k_constant</span>
        <span class="s3">assert </span><span class="s1">len(res.params) == k_vars + k_extra</span>


<span class="s3">class </span><span class="s1">TestMyPareto1(CheckGenericMixin):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">params = [</span><span class="s5">2</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span>
        <span class="s1">nobs = </span><span class="s5">100</span>
        <span class="s1">np.random.seed(</span><span class="s5">1234</span><span class="s1">)</span>
        <span class="s1">rvs = stats.pareto.rvs(*params</span><span class="s3">, </span><span class="s1">**dict(size=nobs))</span>

        <span class="s1">mod_par = MyPareto(rvs)</span>
        <span class="s1">mod_par.fixed_params = </span><span class="s3">None</span>
        <span class="s1">mod_par.fixed_paramsmask = </span><span class="s3">None</span>
        <span class="s1">mod_par.df_model = </span><span class="s5">0</span>
        <span class="s1">mod_par.k_extra = k_extra = </span><span class="s5">3</span>
        <span class="s1">mod_par.df_resid = mod_par.endog.shape[</span><span class="s5">0</span><span class="s1">] - mod_par.df_model -  k_extra</span>
        <span class="s1">mod_par.data.xnames = [</span><span class="s4">'shape'</span><span class="s3">, </span><span class="s4">'loc'</span><span class="s3">, </span><span class="s4">'scale'</span><span class="s1">]</span>

        <span class="s1">cls.mod = mod_par</span>
        <span class="s1">cls.res1 = mod_par.fit(disp=</span><span class="s3">None</span><span class="s1">)</span>
        <span class="s1">cls.k_extra = k_extra</span>

        <span class="s0"># Note: possible problem with parameters close to min data boundary</span>
        <span class="s0"># see issue #968</span>
        <span class="s1">cls.skip_bsejac = </span><span class="s3">True</span>

    <span class="s3">def </span><span class="s1">test_minsupport(self):</span>
        <span class="s0"># rough sanity checks for convergence</span>
        <span class="s1">params = self.res1.params</span>
        <span class="s1">x_min = self.res1.endog.min()</span>
        <span class="s1">p_min = params[</span><span class="s5">1</span><span class="s1">] + params[</span><span class="s5">2</span><span class="s1">]</span>
        <span class="s1">assert_array_less(p_min</span><span class="s3">, </span><span class="s1">x_min)</span>
        <span class="s1">assert_almost_equal(p_min</span><span class="s3">, </span><span class="s1">x_min</span><span class="s3">, </span><span class="s1">decimal=</span><span class="s5">2</span><span class="s1">)</span>

<span class="s3">class </span><span class="s1">TestMyParetoRestriction(CheckGenericMixin):</span>


    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">params = [</span><span class="s5">2</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span>
        <span class="s1">nobs = </span><span class="s5">50</span>
        <span class="s1">np.random.seed(</span><span class="s5">1234</span><span class="s1">)</span>
        <span class="s1">rvs = stats.pareto.rvs(*params</span><span class="s3">, </span><span class="s1">**dict(size=nobs))</span>

        <span class="s1">mod_par = MyPareto(rvs)</span>
        <span class="s1">fixdf = np.nan * np.ones(</span><span class="s5">3</span><span class="s1">)</span>
        <span class="s1">fixdf[</span><span class="s5">1</span><span class="s1">] = -</span><span class="s5">0.1</span>
        <span class="s1">mod_par.fixed_params = fixdf</span>
        <span class="s1">mod_par.fixed_paramsmask = np.isnan(fixdf)</span>
        <span class="s1">mod_par.start_params = mod_par.start_params[mod_par.fixed_paramsmask]</span>
        <span class="s1">mod_par.df_model = </span><span class="s5">0</span>
        <span class="s1">mod_par.k_extra = k_extra = </span><span class="s5">2</span>
        <span class="s1">mod_par.df_resid = mod_par.endog.shape[</span><span class="s5">0</span><span class="s1">] - mod_par.df_model - k_extra</span>
        <span class="s1">mod_par.data.xnames = [</span><span class="s4">'shape'</span><span class="s3">, </span><span class="s4">'scale'</span><span class="s1">]</span>

        <span class="s1">cls.mod = mod_par</span>
        <span class="s1">cls.res1 = mod_par.fit(disp=</span><span class="s3">None</span><span class="s1">)</span>
        <span class="s1">cls.k_extra = k_extra</span>

        <span class="s0"># Note: loc is fixed, no problems with parameters close to min data</span>
        <span class="s1">cls.skip_bsejac = </span><span class="s3">False</span>


<span class="s3">class </span><span class="s1">TwoPeakLLHNoExog(GenericLikelihoodModel):</span>
    <span class="s2">&quot;&quot;&quot;Fit height of signal peak over background.&quot;&quot;&quot;</span>
    <span class="s1">start_params = [</span><span class="s5">10</span><span class="s3">, </span><span class="s5">1000</span><span class="s1">]</span>
    <span class="s1">cloneattr = [</span><span class="s4">'start_params'</span><span class="s3">, </span><span class="s4">'signal'</span><span class="s3">, </span><span class="s4">'background'</span><span class="s1">]</span>
    <span class="s1">exog_names = [</span><span class="s4">'n_signal'</span><span class="s3">, </span><span class="s4">'n_background'</span><span class="s1">]</span>
    <span class="s1">endog_names = [</span><span class="s4">'alpha'</span><span class="s1">]</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">signal=</span><span class="s3">None, </span><span class="s1">background=</span><span class="s3">None,</span>
                 <span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s0"># assume we know the shape + location of the two components,</span>
        <span class="s0"># so we re-use their PDFs here</span>
        <span class="s1">self.signal = signal</span>
        <span class="s1">self.background = background</span>
        <span class="s1">super(TwoPeakLLHNoExog</span><span class="s3">, </span><span class="s1">self).__init__(</span>
            <span class="s1">endog=endog</span><span class="s3">,</span>
            <span class="s1">exog=exog</span><span class="s3">,</span>
            <span class="s1">*args</span><span class="s3">,</span>
            <span class="s1">extra_params_names=self.exog_names</span><span class="s3">,</span>
            <span class="s1">**kwargs</span>
            <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):        </span><span class="s0"># pylint: disable=E0202</span>
        <span class="s3">return </span><span class="s1">-self.nloglike(params)</span>

    <span class="s3">def </span><span class="s1">nloglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s1">endog = self.endog</span>
        <span class="s3">return </span><span class="s1">self.nlnlike(params</span><span class="s3">, </span><span class="s1">endog)</span>

    <span class="s3">def </span><span class="s1">nlnlike(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">endog):</span>
        <span class="s1">n_sig = params[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">n_bkg = params[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">if </span><span class="s1">(n_sig &lt; </span><span class="s5">0</span><span class="s1">) </span><span class="s3">or </span><span class="s1">n_bkg &lt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">np.inf</span>
        <span class="s1">n_tot = n_bkg + n_sig</span>
        <span class="s1">alpha = endog</span>
        <span class="s1">sig = self.signal.pdf(alpha)</span>
        <span class="s1">bkg = self.background.pdf(alpha)</span>
        <span class="s1">sumlogl = np.sum(np.log((n_sig * sig) + (n_bkg * bkg)))</span>
        <span class="s1">sumlogl -= n_tot</span>
        <span class="s3">return </span><span class="s1">-sumlogl</span>


<span class="s3">class </span><span class="s1">TestTwoPeakLLHNoExog:</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">np.random.seed(</span><span class="s5">42</span><span class="s1">)</span>
        <span class="s1">pdf_a = stats.halfcauchy(loc=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">scale=</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">pdf_b = stats.uniform(loc=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">scale=</span><span class="s5">100</span><span class="s1">)</span>

        <span class="s1">n_a = </span><span class="s5">50</span>
        <span class="s1">n_b = </span><span class="s5">200</span>
        <span class="s1">params = [n_a</span><span class="s3">, </span><span class="s1">n_b]</span>

        <span class="s1">X = np.concatenate([pdf_a.rvs(size=n_a)</span><span class="s3">,</span>
                            <span class="s1">pdf_b.rvs(size=n_b)</span><span class="s3">,</span>
                            <span class="s1">])[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">cls.X = X</span>
        <span class="s1">cls.params = params</span>
        <span class="s1">cls.pdf_a = pdf_a</span>
        <span class="s1">cls.pdf_b = pdf_b</span>

    <span class="s3">def </span><span class="s1">test_fit(self):</span>
        <span class="s1">np.random.seed(</span><span class="s5">42</span><span class="s1">)</span>
        <span class="s1">llh_noexog = TwoPeakLLHNoExog(self.X</span><span class="s3">,</span>
                                      <span class="s1">signal=self.pdf_a</span><span class="s3">,</span>
                                      <span class="s1">background=self.pdf_b)</span>

        <span class="s1">res = llh_noexog.fit()</span>
        <span class="s1">assert_allclose(res.params</span><span class="s3">, </span><span class="s1">self.params</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-1</span><span class="s1">)</span>
        <span class="s0"># TODO: nan if exog is None,</span>
        <span class="s3">assert </span><span class="s1">res.df_resid == </span><span class="s5">248</span>
        <span class="s3">assert </span><span class="s1">res.df_model == </span><span class="s5">0</span>
        <span class="s1">res_bs = res.bootstrap(nrep=</span><span class="s5">50</span><span class="s1">)</span>
        <span class="s1">assert_allclose(res_bs[</span><span class="s5">2</span><span class="s1">].mean(</span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">self.params</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-1</span><span class="s1">)</span>
        <span class="s0"># SMOKE test,</span>
        <span class="s1">res.summary()</span>
</pre>
</body>
</html>