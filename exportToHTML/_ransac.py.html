<html>
<head>
<title>_ransac.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_ransac.py</font>
</center></td></tr></table>
<pre><span class="s0"># Author: Johannes Sch√∂nberger</span>
<span class="s0">#</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">numbers </span><span class="s2">import </span><span class="s1">Integral</span><span class="s2">, </span><span class="s1">Real</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>

<span class="s2">from </span><span class="s1">..base </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">BaseEstimator</span><span class="s2">,</span>
    <span class="s1">MetaEstimatorMixin</span><span class="s2">,</span>
    <span class="s1">MultiOutputMixin</span><span class="s2">,</span>
    <span class="s1">RegressorMixin</span><span class="s2">,</span>
    <span class="s1">_fit_context</span><span class="s2">,</span>
    <span class="s1">clone</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">..exceptions </span><span class="s2">import </span><span class="s1">ConvergenceWarning</span>
<span class="s2">from </span><span class="s1">..utils </span><span class="s2">import </span><span class="s1">check_consistent_length</span><span class="s2">, </span><span class="s1">check_random_state</span>
<span class="s2">from </span><span class="s1">..utils._param_validation </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">HasMethods</span><span class="s2">,</span>
    <span class="s1">Interval</span><span class="s2">,</span>
    <span class="s1">Options</span><span class="s2">,</span>
    <span class="s1">RealNotInt</span><span class="s2">,</span>
    <span class="s1">StrOptions</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">..utils.random </span><span class="s2">import </span><span class="s1">sample_without_replacement</span>
<span class="s2">from </span><span class="s1">..utils.validation </span><span class="s2">import </span><span class="s1">_check_sample_weight</span><span class="s2">, </span><span class="s1">check_is_fitted</span><span class="s2">, </span><span class="s1">has_fit_parameter</span>
<span class="s2">from </span><span class="s1">._base </span><span class="s2">import </span><span class="s1">LinearRegression</span>

<span class="s1">_EPSILON = np.spacing(</span><span class="s3">1</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">_dynamic_max_trials(n_inliers</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">min_samples</span><span class="s2">, </span><span class="s1">probability):</span>
    <span class="s4">&quot;&quot;&quot;Determine number trials such that at least one outlier-free subset is 
    sampled for the given inlier/outlier ratio. 
 
    Parameters 
    ---------- 
    n_inliers : int 
        Number of inliers in the data. 
 
    n_samples : int 
        Total number of samples in the data. 
 
    min_samples : int 
        Minimum number of samples chosen randomly from original data. 
 
    probability : float 
        Probability (confidence) that one outlier-free sample is generated. 
 
    Returns 
    ------- 
    trials : int 
        Number of trials. 
 
    &quot;&quot;&quot;</span>
    <span class="s1">inlier_ratio = n_inliers / float(n_samples)</span>
    <span class="s1">nom = max(_EPSILON</span><span class="s2">, </span><span class="s3">1 </span><span class="s1">- probability)</span>
    <span class="s1">denom = max(_EPSILON</span><span class="s2">, </span><span class="s3">1 </span><span class="s1">- inlier_ratio**min_samples)</span>
    <span class="s2">if </span><span class="s1">nom == </span><span class="s3">1</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s3">0</span>
    <span class="s2">if </span><span class="s1">denom == </span><span class="s3">1</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">float(</span><span class="s5">&quot;inf&quot;</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">abs(float(np.ceil(np.log(nom) / np.log(denom))))</span>


<span class="s2">class </span><span class="s1">RANSACRegressor(</span>
    <span class="s1">MetaEstimatorMixin</span><span class="s2">, </span><span class="s1">RegressorMixin</span><span class="s2">, </span><span class="s1">MultiOutputMixin</span><span class="s2">, </span><span class="s1">BaseEstimator</span>
<span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot;RANSAC (RANdom SAmple Consensus) algorithm. 
 
    RANSAC is an iterative algorithm for the robust estimation of parameters 
    from a subset of inliers from the complete data set. 
 
    Read more in the :ref:`User Guide &lt;ransac_regression&gt;`. 
 
    Parameters 
    ---------- 
    estimator : object, default=None 
        Base estimator object which implements the following methods: 
 
         * `fit(X, y)`: Fit model to given training data and target values. 
         * `score(X, y)`: Returns the mean accuracy on the given test data, 
           which is used for the stop criterion defined by `stop_score`. 
           Additionally, the score is used to decide which of two equally 
           large consensus sets is chosen as the better one. 
         * `predict(X)`: Returns predicted values using the linear model, 
           which is used to compute residual error using loss function. 
 
        If `estimator` is None, then 
        :class:`~sklearn.linear_model.LinearRegression` is used for 
        target values of dtype float. 
 
        Note that the current implementation only supports regression 
        estimators. 
 
    min_samples : int (&gt;= 1) or float ([0, 1]), default=None 
        Minimum number of samples chosen randomly from original data. Treated 
        as an absolute number of samples for `min_samples &gt;= 1`, treated as a 
        relative number `ceil(min_samples * X.shape[0])` for 
        `min_samples &lt; 1`. This is typically chosen as the minimal number of 
        samples necessary to estimate the given `estimator`. By default a 
        :class:`~sklearn.linear_model.LinearRegression` estimator is assumed and 
        `min_samples` is chosen as ``X.shape[1] + 1``. This parameter is highly 
        dependent upon the model, so if a `estimator` other than 
        :class:`~sklearn.linear_model.LinearRegression` is used, the user must 
        provide a value. 
 
    residual_threshold : float, default=None 
        Maximum residual for a data sample to be classified as an inlier. 
        By default the threshold is chosen as the MAD (median absolute 
        deviation) of the target values `y`. Points whose residuals are 
        strictly equal to the threshold are considered as inliers. 
 
    is_data_valid : callable, default=None 
        This function is called with the randomly selected data before the 
        model is fitted to it: `is_data_valid(X, y)`. If its return value is 
        False the current randomly chosen sub-sample is skipped. 
 
    is_model_valid : callable, default=None 
        This function is called with the estimated model and the randomly 
        selected data: `is_model_valid(model, X, y)`. If its return value is 
        False the current randomly chosen sub-sample is skipped. 
        Rejecting samples with this function is computationally costlier than 
        with `is_data_valid`. `is_model_valid` should therefore only be used if 
        the estimated model is needed for making the rejection decision. 
 
    max_trials : int, default=100 
        Maximum number of iterations for random sample selection. 
 
    max_skips : int, default=np.inf 
        Maximum number of iterations that can be skipped due to finding zero 
        inliers or invalid data defined by ``is_data_valid`` or invalid models 
        defined by ``is_model_valid``. 
 
        .. versionadded:: 0.19 
 
    stop_n_inliers : int, default=np.inf 
        Stop iteration if at least this number of inliers are found. 
 
    stop_score : float, default=np.inf 
        Stop iteration if score is greater equal than this threshold. 
 
    stop_probability : float in range [0, 1], default=0.99 
        RANSAC iteration stops if at least one outlier-free set of the training 
        data is sampled in RANSAC. This requires to generate at least N 
        samples (iterations):: 
 
            N &gt;= log(1 - probability) / log(1 - e**m) 
 
        where the probability (confidence) is typically set to high value such 
        as 0.99 (the default) and e is the current fraction of inliers w.r.t. 
        the total number of samples. 
 
    loss : str, callable, default='absolute_error' 
        String inputs, 'absolute_error' and 'squared_error' are supported which 
        find the absolute error and squared error per sample respectively. 
 
        If ``loss`` is a callable, then it should be a function that takes 
        two arrays as inputs, the true and predicted value and returns a 1-D 
        array with the i-th value of the array corresponding to the loss 
        on ``X[i]``. 
 
        If the loss on a sample is greater than the ``residual_threshold``, 
        then this sample is classified as an outlier. 
 
        .. versionadded:: 0.18 
 
    random_state : int, RandomState instance, default=None 
        The generator used to initialize the centers. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    Attributes 
    ---------- 
    estimator_ : object 
        Best fitted model (copy of the `estimator` object). 
 
    n_trials_ : int 
        Number of random selection trials until one of the stop criteria is 
        met. It is always ``&lt;= max_trials``. 
 
    inlier_mask_ : bool array of shape [n_samples] 
        Boolean mask of inliers classified as ``True``. 
 
    n_skips_no_inliers_ : int 
        Number of iterations skipped due to finding zero inliers. 
 
        .. versionadded:: 0.19 
 
    n_skips_invalid_data_ : int 
        Number of iterations skipped due to invalid data defined by 
        ``is_data_valid``. 
 
        .. versionadded:: 0.19 
 
    n_skips_invalid_model_ : int 
        Number of iterations skipped due to an invalid model defined by 
        ``is_model_valid``. 
 
        .. versionadded:: 0.19 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    HuberRegressor : Linear regression model that is robust to outliers. 
    TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model. 
    SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD. 
 
    References 
    ---------- 
    .. [1] https://en.wikipedia.org/wiki/RANSAC 
    .. [2] https://www.sri.com/wp-content/uploads/2021/12/ransac-publication.pdf 
    .. [3] http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.linear_model import RANSACRegressor 
    &gt;&gt;&gt; from sklearn.datasets import make_regression 
    &gt;&gt;&gt; X, y = make_regression( 
    ...     n_samples=200, n_features=2, noise=4.0, random_state=0) 
    &gt;&gt;&gt; reg = RANSACRegressor(random_state=0).fit(X, y) 
    &gt;&gt;&gt; reg.score(X, y) 
    0.9885... 
    &gt;&gt;&gt; reg.predict(X[:1,]) 
    array([-31.9417...]) 
    &quot;&quot;&quot;  </span><span class="s0"># noqa: E501</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s5">&quot;estimator&quot;</span><span class="s1">: [HasMethods([</span><span class="s5">&quot;fit&quot;</span><span class="s2">, </span><span class="s5">&quot;score&quot;</span><span class="s2">, </span><span class="s5">&quot;predict&quot;</span><span class="s1">])</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;min_samples&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Integral</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">Interval(RealNotInt</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">closed=</span><span class="s5">&quot;both&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s2">None,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;residual_threshold&quot;</span><span class="s1">: [Interval(Real</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;is_data_valid&quot;</span><span class="s1">: [callable</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;is_model_valid&quot;</span><span class="s1">: [callable</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;max_trials&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Integral</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">Options(Real</span><span class="s2">, </span><span class="s1">{np.inf})</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;max_skips&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Integral</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">Options(Real</span><span class="s2">, </span><span class="s1">{np.inf})</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;stop_n_inliers&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Integral</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">Options(Real</span><span class="s2">, </span><span class="s1">{np.inf})</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;stop_score&quot;</span><span class="s1">: [Interval(Real</span><span class="s2">, None, None, </span><span class="s1">closed=</span><span class="s5">&quot;both&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s5">&quot;stop_probability&quot;</span><span class="s1">: [Interval(Real</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">closed=</span><span class="s5">&quot;both&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s5">&quot;loss&quot;</span><span class="s1">: [StrOptions({</span><span class="s5">&quot;absolute_error&quot;</span><span class="s2">, </span><span class="s5">&quot;squared_error&quot;</span><span class="s1">})</span><span class="s2">, </span><span class="s1">callable]</span><span class="s2">,</span>
        <span class="s5">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s5">&quot;random_state&quot;</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">estimator=</span><span class="s2">None,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">min_samples=</span><span class="s2">None,</span>
        <span class="s1">residual_threshold=</span><span class="s2">None,</span>
        <span class="s1">is_data_valid=</span><span class="s2">None,</span>
        <span class="s1">is_model_valid=</span><span class="s2">None,</span>
        <span class="s1">max_trials=</span><span class="s3">100</span><span class="s2">,</span>
        <span class="s1">max_skips=np.inf</span><span class="s2">,</span>
        <span class="s1">stop_n_inliers=np.inf</span><span class="s2">,</span>
        <span class="s1">stop_score=np.inf</span><span class="s2">,</span>
        <span class="s1">stop_probability=</span><span class="s3">0.99</span><span class="s2">,</span>
        <span class="s1">loss=</span><span class="s5">&quot;absolute_error&quot;</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s2">None,</span>
    <span class="s1">):</span>
        <span class="s1">self.estimator = estimator</span>
        <span class="s1">self.min_samples = min_samples</span>
        <span class="s1">self.residual_threshold = residual_threshold</span>
        <span class="s1">self.is_data_valid = is_data_valid</span>
        <span class="s1">self.is_model_valid = is_model_valid</span>
        <span class="s1">self.max_trials = max_trials</span>
        <span class="s1">self.max_skips = max_skips</span>
        <span class="s1">self.stop_n_inliers = stop_n_inliers</span>
        <span class="s1">self.stop_score = stop_score</span>
        <span class="s1">self.stop_probability = stop_probability</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.loss = loss</span>

    <span class="s1">@_fit_context(</span>
        <span class="s0"># RansacRegressor.estimator is not validated yet</span>
        <span class="s1">prefer_skip_nested_validation=</span><span class="s2">False</span>
    <span class="s1">)</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s4">&quot;&quot;&quot;Fit estimator using RANSAC algorithm. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training data. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_targets) 
            Target values. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Individual weights for each sample 
            raises error if sample_weight is passed and estimator 
            fit method does not support it. 
 
            .. versionadded:: 0.18 
 
        Returns 
        ------- 
        self : object 
            Fitted `RANSACRegressor` estimator. 
 
        Raises 
        ------ 
        ValueError 
            If no valid consensus set could be found. This occurs if 
            `is_data_valid` and `is_model_valid` return False for all 
            `max_trials` randomly chosen sub-samples. 
        &quot;&quot;&quot;</span>
        <span class="s0"># Need to validate separately here. We can't pass multi_output=True</span>
        <span class="s0"># because that would allow y to be csr. Delay expensive finiteness</span>
        <span class="s0"># check to the estimator's own input validation.</span>
        <span class="s1">check_X_params = dict(accept_sparse=</span><span class="s5">&quot;csr&quot;</span><span class="s2">, </span><span class="s1">force_all_finite=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">check_y_params = dict(ensure_2d=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">validate_separately=(check_X_params</span><span class="s2">, </span><span class="s1">check_y_params)</span>
        <span class="s1">)</span>
        <span class="s1">check_consistent_length(X</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s2">if </span><span class="s1">self.estimator </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">estimator = clone(self.estimator)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">estimator = LinearRegression()</span>

        <span class="s2">if </span><span class="s1">self.min_samples </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">if not </span><span class="s1">isinstance(estimator</span><span class="s2">, </span><span class="s1">LinearRegression):</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s5">&quot;`min_samples` needs to be explicitly set when estimator &quot;</span>
                    <span class="s5">&quot;is not a LinearRegression.&quot;</span>
                <span class="s1">)</span>
            <span class="s1">min_samples = X.shape[</span><span class="s3">1</span><span class="s1">] + </span><span class="s3">1</span>
        <span class="s2">elif </span><span class="s3">0 </span><span class="s1">&lt; self.min_samples &lt; </span><span class="s3">1</span><span class="s1">:</span>
            <span class="s1">min_samples = np.ceil(self.min_samples * X.shape[</span><span class="s3">0</span><span class="s1">])</span>
        <span class="s2">elif </span><span class="s1">self.min_samples &gt;= </span><span class="s3">1</span><span class="s1">:</span>
            <span class="s1">min_samples = self.min_samples</span>
        <span class="s2">if </span><span class="s1">min_samples &gt; X.shape[</span><span class="s3">0</span><span class="s1">]:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s5">&quot;`min_samples` may not be larger than number &quot;</span>
                <span class="s5">&quot;of samples: n_samples = %d.&quot; </span><span class="s1">% (X.shape[</span><span class="s3">0</span><span class="s1">])</span>
            <span class="s1">)</span>

        <span class="s2">if </span><span class="s1">self.residual_threshold </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s0"># MAD (median absolute deviation)</span>
            <span class="s1">residual_threshold = np.median(np.abs(y - np.median(y)))</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">residual_threshold = self.residual_threshold</span>

        <span class="s2">if </span><span class="s1">self.loss == </span><span class="s5">&quot;absolute_error&quot;</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">y.ndim == </span><span class="s3">1</span><span class="s1">:</span>
                <span class="s1">loss_function = </span><span class="s2">lambda </span><span class="s1">y_true</span><span class="s2">, </span><span class="s1">y_pred: np.abs(y_true - y_pred)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">loss_function = </span><span class="s2">lambda </span><span class="s1">y_true</span><span class="s2">, </span><span class="s1">y_pred: np.sum(</span>
                    <span class="s1">np.abs(y_true - y_pred)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span>
                <span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">self.loss == </span><span class="s5">&quot;squared_error&quot;</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">y.ndim == </span><span class="s3">1</span><span class="s1">:</span>
                <span class="s1">loss_function = </span><span class="s2">lambda </span><span class="s1">y_true</span><span class="s2">, </span><span class="s1">y_pred: (y_true - y_pred) ** </span><span class="s3">2</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">loss_function = </span><span class="s2">lambda </span><span class="s1">y_true</span><span class="s2">, </span><span class="s1">y_pred: np.sum(</span>
                    <span class="s1">(y_true - y_pred) ** </span><span class="s3">2</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span>
                <span class="s1">)</span>

        <span class="s2">elif </span><span class="s1">callable(self.loss):</span>
            <span class="s1">loss_function = self.loss</span>

        <span class="s1">random_state = check_random_state(self.random_state)</span>

        <span class="s2">try</span><span class="s1">:  </span><span class="s0"># Not all estimator accept a random_state</span>
            <span class="s1">estimator.set_params(random_state=random_state)</span>
        <span class="s2">except </span><span class="s1">ValueError:</span>
            <span class="s2">pass</span>

        <span class="s1">estimator_fit_has_sample_weight = has_fit_parameter(estimator</span><span class="s2">, </span><span class="s5">&quot;sample_weight&quot;</span><span class="s1">)</span>
        <span class="s1">estimator_name = type(estimator).__name__</span>
        <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is not None and not </span><span class="s1">estimator_fit_has_sample_weight:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s5">&quot;%s does not support sample_weight. Samples&quot;</span>
                <span class="s5">&quot; weights are only used for the calibration&quot;</span>
                <span class="s5">&quot; itself.&quot; </span><span class="s1">% estimator_name</span>
            <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s2">, </span><span class="s1">X)</span>

        <span class="s1">n_inliers_best = </span><span class="s3">1</span>
        <span class="s1">score_best = -np.inf</span>
        <span class="s1">inlier_mask_best = </span><span class="s2">None</span>
        <span class="s1">X_inlier_best = </span><span class="s2">None</span>
        <span class="s1">y_inlier_best = </span><span class="s2">None</span>
        <span class="s1">inlier_best_idxs_subset = </span><span class="s2">None</span>
        <span class="s1">self.n_skips_no_inliers_ = </span><span class="s3">0</span>
        <span class="s1">self.n_skips_invalid_data_ = </span><span class="s3">0</span>
        <span class="s1">self.n_skips_invalid_model_ = </span><span class="s3">0</span>

        <span class="s0"># number of data samples</span>
        <span class="s1">n_samples = X.shape[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">sample_idxs = np.arange(n_samples)</span>

        <span class="s1">self.n_trials_ = </span><span class="s3">0</span>
        <span class="s1">max_trials = self.max_trials</span>
        <span class="s2">while </span><span class="s1">self.n_trials_ &lt; max_trials:</span>
            <span class="s1">self.n_trials_ += </span><span class="s3">1</span>

            <span class="s2">if </span><span class="s1">(</span>
                <span class="s1">self.n_skips_no_inliers_</span>
                <span class="s1">+ self.n_skips_invalid_data_</span>
                <span class="s1">+ self.n_skips_invalid_model_</span>
            <span class="s1">) &gt; self.max_skips:</span>
                <span class="s2">break</span>

            <span class="s0"># choose random sample set</span>
            <span class="s1">subset_idxs = sample_without_replacement(</span>
                <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">min_samples</span><span class="s2">, </span><span class="s1">random_state=random_state</span>
            <span class="s1">)</span>
            <span class="s1">X_subset = X[subset_idxs]</span>
            <span class="s1">y_subset = y[subset_idxs]</span>

            <span class="s0"># check if random sample set is valid</span>
            <span class="s2">if </span><span class="s1">self.is_data_valid </span><span class="s2">is not None and not </span><span class="s1">self.is_data_valid(</span>
                <span class="s1">X_subset</span><span class="s2">, </span><span class="s1">y_subset</span>
            <span class="s1">):</span>
                <span class="s1">self.n_skips_invalid_data_ += </span><span class="s3">1</span>
                <span class="s2">continue</span>

            <span class="s0"># fit model for current random sample set</span>
            <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s1">estimator.fit(X_subset</span><span class="s2">, </span><span class="s1">y_subset)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">estimator.fit(</span>
                    <span class="s1">X_subset</span><span class="s2">, </span><span class="s1">y_subset</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight[subset_idxs]</span>
                <span class="s1">)</span>

            <span class="s0"># check if estimated model is valid</span>
            <span class="s2">if </span><span class="s1">self.is_model_valid </span><span class="s2">is not None and not </span><span class="s1">self.is_model_valid(</span>
                <span class="s1">estimator</span><span class="s2">, </span><span class="s1">X_subset</span><span class="s2">, </span><span class="s1">y_subset</span>
            <span class="s1">):</span>
                <span class="s1">self.n_skips_invalid_model_ += </span><span class="s3">1</span>
                <span class="s2">continue</span>

            <span class="s0"># residuals of all data for current random sample model</span>
            <span class="s1">y_pred = estimator.predict(X)</span>
            <span class="s1">residuals_subset = loss_function(y</span><span class="s2">, </span><span class="s1">y_pred)</span>

            <span class="s0"># classify data into inliers and outliers</span>
            <span class="s1">inlier_mask_subset = residuals_subset &lt;= residual_threshold</span>
            <span class="s1">n_inliers_subset = np.sum(inlier_mask_subset)</span>

            <span class="s0"># less inliers -&gt; skip current random sample</span>
            <span class="s2">if </span><span class="s1">n_inliers_subset &lt; n_inliers_best:</span>
                <span class="s1">self.n_skips_no_inliers_ += </span><span class="s3">1</span>
                <span class="s2">continue</span>

            <span class="s0"># extract inlier data set</span>
            <span class="s1">inlier_idxs_subset = sample_idxs[inlier_mask_subset]</span>
            <span class="s1">X_inlier_subset = X[inlier_idxs_subset]</span>
            <span class="s1">y_inlier_subset = y[inlier_idxs_subset]</span>

            <span class="s0"># score of inlier data set</span>
            <span class="s1">score_subset = estimator.score(X_inlier_subset</span><span class="s2">, </span><span class="s1">y_inlier_subset)</span>

            <span class="s0"># same number of inliers but worse score -&gt; skip current random</span>
            <span class="s0"># sample</span>
            <span class="s2">if </span><span class="s1">n_inliers_subset == n_inliers_best </span><span class="s2">and </span><span class="s1">score_subset &lt; score_best:</span>
                <span class="s2">continue</span>

            <span class="s0"># save current random sample as best sample</span>
            <span class="s1">n_inliers_best = n_inliers_subset</span>
            <span class="s1">score_best = score_subset</span>
            <span class="s1">inlier_mask_best = inlier_mask_subset</span>
            <span class="s1">X_inlier_best = X_inlier_subset</span>
            <span class="s1">y_inlier_best = y_inlier_subset</span>
            <span class="s1">inlier_best_idxs_subset = inlier_idxs_subset</span>

            <span class="s1">max_trials = min(</span>
                <span class="s1">max_trials</span><span class="s2">,</span>
                <span class="s1">_dynamic_max_trials(</span>
                    <span class="s1">n_inliers_best</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">min_samples</span><span class="s2">, </span><span class="s1">self.stop_probability</span>
                <span class="s1">)</span><span class="s2">,</span>
            <span class="s1">)</span>

            <span class="s0"># break if sufficient number of inliers or score is reached</span>
            <span class="s2">if </span><span class="s1">n_inliers_best &gt;= self.stop_n_inliers </span><span class="s2">or </span><span class="s1">score_best &gt;= self.stop_score:</span>
                <span class="s2">break</span>

        <span class="s0"># if none of the iterations met the required criteria</span>
        <span class="s2">if </span><span class="s1">inlier_mask_best </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">(</span>
                <span class="s1">self.n_skips_no_inliers_</span>
                <span class="s1">+ self.n_skips_invalid_data_</span>
                <span class="s1">+ self.n_skips_invalid_model_</span>
            <span class="s1">) &gt; self.max_skips:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s5">&quot;RANSAC skipped more iterations than `max_skips` without&quot;</span>
                    <span class="s5">&quot; finding a valid consensus set. Iterations were skipped&quot;</span>
                    <span class="s5">&quot; because each randomly chosen sub-sample failed the&quot;</span>
                    <span class="s5">&quot; passing criteria. See estimator attributes for&quot;</span>
                    <span class="s5">&quot; diagnostics (n_skips*).&quot;</span>
                <span class="s1">)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s5">&quot;RANSAC could not find a valid consensus set. All&quot;</span>
                    <span class="s5">&quot; `max_trials` iterations were skipped because each&quot;</span>
                    <span class="s5">&quot; randomly chosen sub-sample failed the passing criteria.&quot;</span>
                    <span class="s5">&quot; See estimator attributes for diagnostics (n_skips*).&quot;</span>
                <span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">(</span>
                <span class="s1">self.n_skips_no_inliers_</span>
                <span class="s1">+ self.n_skips_invalid_data_</span>
                <span class="s1">+ self.n_skips_invalid_model_</span>
            <span class="s1">) &gt; self.max_skips:</span>
                <span class="s1">warnings.warn(</span>
                    <span class="s1">(</span>
                        <span class="s5">&quot;RANSAC found a valid consensus set but exited&quot;</span>
                        <span class="s5">&quot; early due to skipping more iterations than&quot;</span>
                        <span class="s5">&quot; `max_skips`. See estimator attributes for&quot;</span>
                        <span class="s5">&quot; diagnostics (n_skips*).&quot;</span>
                    <span class="s1">)</span><span class="s2">,</span>
                    <span class="s1">ConvergenceWarning</span><span class="s2">,</span>
                <span class="s1">)</span>

        <span class="s0"># estimate final model using all inliers</span>
        <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">estimator.fit(X_inlier_best</span><span class="s2">, </span><span class="s1">y_inlier_best)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">estimator.fit(</span>
                <span class="s1">X_inlier_best</span><span class="s2">,</span>
                <span class="s1">y_inlier_best</span><span class="s2">,</span>
                <span class="s1">sample_weight=sample_weight[inlier_best_idxs_subset]</span><span class="s2">,</span>
            <span class="s1">)</span>

        <span class="s1">self.estimator_ = estimator</span>
        <span class="s1">self.inlier_mask_ = inlier_mask_best</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s4">&quot;&quot;&quot;Predict using the estimated model. 
 
        This is a wrapper for `estimator_.predict(X)`. 
 
        Parameters 
        ---------- 
        X : {array-like or sparse matrix} of shape (n_samples, n_features) 
            Input data. 
 
        Returns 
        ------- 
        y : array, shape = [n_samples] or [n_samples, n_targets] 
            Returns predicted values. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">force_all_finite=</span><span class="s2">False,</span>
            <span class="s1">accept_sparse=</span><span class="s2">True,</span>
            <span class="s1">reset=</span><span class="s2">False,</span>
        <span class="s1">)</span>
        <span class="s2">return </span><span class="s1">self.estimator_.predict(X)</span>

    <span class="s2">def </span><span class="s1">score(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s4">&quot;&quot;&quot;Return the score of the prediction. 
 
        This is a wrapper for `estimator_.score(X, y)`. 
 
        Parameters 
        ---------- 
        X : (array-like or sparse matrix} of shape (n_samples, n_features) 
            Training data. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_targets) 
            Target values. 
 
        Returns 
        ------- 
        z : float 
            Score of the prediction. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">force_all_finite=</span><span class="s2">False,</span>
            <span class="s1">accept_sparse=</span><span class="s2">True,</span>
            <span class="s1">reset=</span><span class="s2">False,</span>
        <span class="s1">)</span>
        <span class="s2">return </span><span class="s1">self.estimator_.score(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span>
            <span class="s5">&quot;_xfail_checks&quot;</span><span class="s1">: {</span>
                <span class="s5">&quot;check_sample_weights_invariance&quot;</span><span class="s1">: (</span>
                    <span class="s5">&quot;zero sample_weight is not equivalent to removing samples&quot;</span>
                <span class="s1">)</span><span class="s2">,</span>
            <span class="s1">}</span>
        <span class="s1">}</span>
</pre>
</body>
</html>