<html>
<head>
<title>test_gam.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_gam.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot;Tests for gam.AdditiveModel and GAM with Polynomials compared to OLS and GLM 
 
 
Created on Sat Nov 05 14:16:07 2011 
 
Author: Josef Perktold 
License: BSD 
 
 
Notes 
----- 
 
TODO: TestGAMGamma: has test failure (GLM looks good), 
        adding log-link did not help 
        resolved: gamma does not fail anymore after tightening the 
                  convergence criterium (rtol=1e-6) 
TODO: TestGAMNegativeBinomial: rvs generation does not work, 
        nbinom needs 2 parameters 
TODO: TestGAMGaussianLogLink: test failure, 
        but maybe precision issue, not completely off 
 
        but something is wrong, either the testcase or with the link 
        &gt;&gt;&gt; tt3.__class__ 
        &lt;class '__main__.TestGAMGaussianLogLink'&gt; 
        &gt;&gt;&gt; tt3.res2.mu_pred.mean() 
        3.5616368292650766 
        &gt;&gt;&gt; tt3.res1.mu_pred.mean() 
        3.6144278964707679 
        &gt;&gt;&gt; tt3.mu_true.mean() 
        34.821904835958122 
        &gt;&gt;&gt; 
        &gt;&gt;&gt; tt3.y_true.mean() 
        2.685225067611543 
        &gt;&gt;&gt; tt3.res1.y_pred.mean() 
        0.52991541684645616 
        &gt;&gt;&gt; tt3.res2.y_pred.mean() 
        0.44626406889363229 
 
 
 
one possible change 
~~~~~~~~~~~~~~~~~~~ 
add average, integral based tests, instead of or additional to sup 
    * for example mean squared error for mu and eta (predict, fittedvalues) 
      or mean absolute error, what's the scale for this? required precision? 
    * this will also work for real non-parametric tests 
 
example: Gamma looks good in average bias and average RMSE (RMISE) 
 
&gt;&gt;&gt; tt3 = _estGAMGamma() 
&gt;&gt;&gt; np.mean((tt3.res2.mu_pred - tt3.mu_true))/tt3.mu_true.mean() 
-0.0051829977497423706 
&gt;&gt;&gt; np.mean((tt3.res2.y_pred - tt3.y_true))/tt3.y_true.mean() 
0.00015255264651864049 
&gt;&gt;&gt; np.mean((tt3.res1.y_pred - tt3.y_true))/tt3.y_true.mean() 
0.00015255538823786711 
&gt;&gt;&gt; np.mean((tt3.res1.mu_pred - tt3.mu_true))/tt3.mu_true.mean() 
-0.0051937668989744494 
&gt;&gt;&gt; np.sqrt(np.mean((tt3.res1.mu_pred - tt3.mu_true)**2))/tt3.mu_true.mean() 
0.022946118520401692 
&gt;&gt;&gt; np.sqrt(np.mean((tt3.res2.mu_pred - tt3.mu_true)**2))/tt3.mu_true.mean() 
0.022953913332599746 
&gt;&gt;&gt; maxabs = lambda x: np.max(np.abs(x)) 
&gt;&gt;&gt; maxabs((tt3.res1.mu_pred - tt3.mu_true))/tt3.mu_true.mean() 
0.079540546242707733 
&gt;&gt;&gt; maxabs((tt3.res2.mu_pred - tt3.mu_true))/tt3.mu_true.mean() 
0.079578857986784574 
&gt;&gt;&gt; maxabs((tt3.res2.y_pred - tt3.y_true))/tt3.y_true.mean() 
0.016282852522951426 
&gt;&gt;&gt; maxabs((tt3.res1.y_pred - tt3.y_true))/tt3.y_true.mean() 
0.016288391235613865 
 
 
 
&quot;&quot;&quot;</span>
<span class="s3">from </span><span class="s1">statsmodels.compat.python </span><span class="s3">import </span><span class="s1">lrange</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">numpy.testing </span><span class="s3">import </span><span class="s1">assert_almost_equal</span><span class="s3">, </span><span class="s1">assert_equal</span>

<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">stats</span>
<span class="s3">import </span><span class="s1">pytest</span>

<span class="s3">from </span><span class="s1">statsmodels.sandbox.gam </span><span class="s3">import </span><span class="s1">AdditiveModel</span>
<span class="s3">from </span><span class="s1">statsmodels.sandbox.gam </span><span class="s3">import </span><span class="s1">Model </span><span class="s3">as </span><span class="s1">GAM </span><span class="s0">#?</span>
<span class="s3">from </span><span class="s1">statsmodels.genmod.families </span><span class="s3">import </span><span class="s1">family</span><span class="s3">, </span><span class="s1">links</span>
<span class="s3">from </span><span class="s1">statsmodels.genmod.generalized_linear_model </span><span class="s3">import </span><span class="s1">GLM</span>
<span class="s3">from </span><span class="s1">statsmodels.regression.linear_model </span><span class="s3">import </span><span class="s1">OLS</span>


<span class="s3">class </span><span class="s1">Dummy:</span>
    <span class="s3">pass</span>

<span class="s3">class </span><span class="s1">CheckAM:</span>

    <span class="s3">def </span><span class="s1">test_predict(self):</span>
        <span class="s1">assert_almost_equal(self.res1.y_pred</span><span class="s3">,</span>
                            <span class="s1">self.res2.y_pred</span><span class="s3">, </span><span class="s1">decimal=</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">assert_almost_equal(self.res1.y_predshort</span><span class="s3">,</span>
                            <span class="s1">self.res2.y_pred[:</span><span class="s4">10</span><span class="s1">]</span><span class="s3">, </span><span class="s1">decimal=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s1">@pytest.mark.xfail(reason=</span><span class="s5">&quot;Unknown, results do not match expected&quot;</span><span class="s3">,</span>
                       <span class="s1">raises=AssertionError</span><span class="s3">, </span><span class="s1">strict=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">test_fitted(self):</span>
        <span class="s0"># check definition of fitted in GLM: eta or mu</span>
        <span class="s1">assert_almost_equal(self.res1.y_pred</span><span class="s3">,</span>
                            <span class="s1">self.res2.fittedvalues</span><span class="s3">, </span><span class="s1">decimal=</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">assert_almost_equal(self.res1.y_predshort</span><span class="s3">,</span>
                            <span class="s1">self.res2.fittedvalues[:</span><span class="s4">10</span><span class="s1">]</span><span class="s3">, </span><span class="s1">decimal=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">test_params(self):</span>
        <span class="s0">#note: only testing slope coefficients</span>
        <span class="s0">#constant is far off in example 4 versus 2</span>
        <span class="s1">assert_almost_equal(self.res1.params[</span><span class="s4">1</span><span class="s1">:]</span><span class="s3">,</span>
                            <span class="s1">self.res2.params[</span><span class="s4">1</span><span class="s1">:]</span><span class="s3">, </span><span class="s1">decimal=</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s0">#constant</span>
        <span class="s1">assert_almost_equal(self.res1.params[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
                            <span class="s1">self.res2.params[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">decimal=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s1">@pytest.mark.xfail(reason=</span><span class="s5">&quot;res_ps attribute does not exist&quot;</span><span class="s3">,</span>
                       <span class="s1">strict=</span><span class="s3">True, </span><span class="s1">raises=AttributeError)</span>
    <span class="s3">def </span><span class="s1">test_df(self):</span>
        <span class="s0"># not used yet, copied from PolySmoother tests</span>
        <span class="s1">assert_equal(self.res_ps.df_model()</span><span class="s3">, </span><span class="s1">self.res2.df_model)</span>
        <span class="s1">assert_equal(self.res_ps.df_fit()</span><span class="s3">, </span><span class="s1">self.res2.df_model) </span><span class="s0">#alias</span>
        <span class="s1">assert_equal(self.res_ps.df_resid()</span><span class="s3">, </span><span class="s1">self.res2.df_resid)</span>


<span class="s3">class </span><span class="s1">CheckGAM(CheckAM):</span>

    <span class="s3">def </span><span class="s1">test_mu(self):</span>
        <span class="s0"># problem with scale for precision</span>
        <span class="s1">assert_almost_equal(self.res1.mu_pred</span><span class="s3">,</span>
                            <span class="s1">self.res2.mu_pred</span><span class="s3">, </span><span class="s1">decimal=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">test_prediction(self):</span>
        <span class="s0"># problem with scale for precision</span>
        <span class="s1">assert_almost_equal(self.res1.y_predshort</span><span class="s3">,</span>
                            <span class="s1">self.res2.y_pred[:</span><span class="s4">10</span><span class="s1">]</span><span class="s3">, </span><span class="s1">decimal=</span><span class="s4">2</span><span class="s1">)</span>


<span class="s3">class </span><span class="s1">BaseAM:</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">#DGP: simple polynomial</span>
        <span class="s1">order = </span><span class="s4">3</span>
        <span class="s1">nobs = </span><span class="s4">200</span>
        <span class="s1">lb</span><span class="s3">, </span><span class="s1">ub = -</span><span class="s4">3.5</span><span class="s3">, </span><span class="s4">3</span>
        <span class="s1">x1 = np.linspace(lb</span><span class="s3">, </span><span class="s1">ub</span><span class="s3">, </span><span class="s1">nobs)</span>
        <span class="s1">x2 = np.sin(</span><span class="s4">2</span><span class="s1">*x1)</span>
        <span class="s1">x = np.column_stack((x1/x1.max()*</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1.</span><span class="s1">*x2))</span>
        <span class="s1">exog = (x[:</span><span class="s3">,</span><span class="s1">:</span><span class="s3">,None</span><span class="s1">]**np.arange(order+</span><span class="s4">1</span><span class="s1">)[</span><span class="s3">None, None, </span><span class="s1">:]).reshape(nobs</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">idx = lrange((order+</span><span class="s4">1</span><span class="s1">)*</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s3">del </span><span class="s1">idx[order+</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">exog_reduced = exog[:</span><span class="s3">,</span><span class="s1">idx]  </span><span class="s0">#remove duplicate constant</span>
        <span class="s1">y_true = exog.sum(</span><span class="s4">1</span><span class="s1">) </span><span class="s0">#/ 4.</span>
        <span class="s0">#z = y_true #alias check</span>
        <span class="s0">#d = x</span>

        <span class="s1">cls.nobs = nobs</span>
        <span class="s1">cls.y_true</span><span class="s3">, </span><span class="s1">cls.x</span><span class="s3">, </span><span class="s1">cls.exog = y_true</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">exog_reduced</span>


<span class="s3">class </span><span class="s1">TestAdditiveModel(BaseAM</span><span class="s3">, </span><span class="s1">CheckAM):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">super(TestAdditiveModel</span><span class="s3">, </span><span class="s1">cls).setup_class() </span><span class="s0">#initialize DGP</span>

        <span class="s1">nobs = cls.nobs</span>
        <span class="s1">y_true</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">exog = cls.y_true</span><span class="s3">, </span><span class="s1">cls.x</span><span class="s3">, </span><span class="s1">cls.exog</span>

        <span class="s1">np.random.seed(</span><span class="s4">8765993</span><span class="s1">)</span>
        <span class="s1">sigma_noise = </span><span class="s4">0.1</span>
        <span class="s1">y = y_true + sigma_noise * np.random.randn(nobs)</span>

        <span class="s1">m = AdditiveModel(x)</span>
        <span class="s1">m.fit(y)</span>
        <span class="s1">res_gam = m.results </span><span class="s0">#TODO: currently attached to class</span>

        <span class="s1">res_ols = OLS(y</span><span class="s3">, </span><span class="s1">exog).fit()</span>

        <span class="s0">#Note: there still are some naming inconsistencies</span>
        <span class="s1">cls.res1 = res1 = Dummy() </span><span class="s0">#for gam model</span>
        <span class="s0">#res2 = Dummy() #for benchmark</span>
        <span class="s1">cls.res2 = res2 = res_ols  </span><span class="s0">#reuse existing ols results, will add additional</span>

        <span class="s1">res1.y_pred = res_gam.predict(x)</span>
        <span class="s1">res2.y_pred = res_ols.model.predict(res_ols.params</span><span class="s3">, </span><span class="s1">exog)</span>
        <span class="s1">res1.y_predshort = res_gam.predict(x[:</span><span class="s4">10</span><span class="s1">])</span>

        <span class="s1">slopes = [i </span><span class="s3">for </span><span class="s1">ss </span><span class="s3">in </span><span class="s1">m.smoothers </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">ss.params[</span><span class="s4">1</span><span class="s1">:]]</span>

        <span class="s1">const = res_gam.alpha + sum([ss.params[</span><span class="s4">1</span><span class="s1">] </span><span class="s3">for </span><span class="s1">ss </span><span class="s3">in </span><span class="s1">m.smoothers])</span>
        <span class="s0">#print const, slopes</span>
        <span class="s1">res1.params = np.array([const] + slopes)</span>

    <span class="s3">def </span><span class="s1">test_fitted(self):</span>
        <span class="s0"># We have to override the base class because this case does not fail,</span>
        <span class="s0">#  while all others in this module do (as of 2019-05-22)</span>
        <span class="s1">super(TestAdditiveModel</span><span class="s3">, </span><span class="s1">self).test_fitted()</span>


<span class="s3">class </span><span class="s1">BaseGAM(BaseAM</span><span class="s3">, </span><span class="s1">CheckGAM):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">init(cls):</span>
        <span class="s1">nobs = cls.nobs</span>
        <span class="s1">y_true</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">exog = cls.y_true</span><span class="s3">, </span><span class="s1">cls.x</span><span class="s3">, </span><span class="s1">cls.exog</span>
        <span class="s3">if not </span><span class="s1">hasattr(cls</span><span class="s3">, </span><span class="s5">'scale'</span><span class="s1">):</span>
            <span class="s1">scale = </span><span class="s4">1</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">scale = cls.scale</span>

        <span class="s1">f = cls.family</span>

        <span class="s1">cls.mu_true = mu_true = f.link.inverse(y_true)</span>

        <span class="s1">np.random.seed(</span><span class="s4">8765993</span><span class="s1">)</span>
        <span class="s0"># Discrete distributions do not take `scale`.</span>
        <span class="s3">try</span><span class="s1">:</span>
            <span class="s1">y_obs = cls.rvs(mu_true</span><span class="s3">, </span><span class="s1">scale=scale</span><span class="s3">, </span><span class="s1">size=nobs)</span>
        <span class="s3">except </span><span class="s1">TypeError:</span>
            <span class="s1">y_obs = cls.rvs(mu_true</span><span class="s3">, </span><span class="s1">size=nobs)</span>

        <span class="s1">m = GAM(y_obs</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">family=f)  </span><span class="s0">#TODO: y_obs is twice __init__ and fit</span>
        <span class="s1">m.fit(y_obs</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s4">100</span><span class="s1">)</span>
        <span class="s1">res_gam = m.results</span>
        <span class="s1">cls.res_gam = res_gam   </span><span class="s0">#attached for debugging</span>
        <span class="s1">cls.mod_gam = m   </span><span class="s0">#attached for debugging</span>

        <span class="s1">res_glm = GLM(y_obs</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">family=f).fit()</span>

        <span class="s0">#Note: there still are some naming inconsistencies</span>
        <span class="s1">cls.res1 = res1 = Dummy() </span><span class="s0">#for gam model</span>
        <span class="s0">#res2 = Dummy() #for benchmark</span>
        <span class="s1">cls.res2 = res2 = res_glm  </span><span class="s0">#reuse existing glm results, will add additional</span>

        <span class="s0">#eta in GLM terminology</span>
        <span class="s1">res2.y_pred = res_glm.model.predict(res_glm.params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">which=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s1">res1.y_pred = res_gam.predict(x)</span>
        <span class="s1">res1.y_predshort = res_gam.predict(x[:</span><span class="s4">10</span><span class="s1">]) </span><span class="s0">#, linear=True)</span>

        <span class="s0">#mu</span>
        <span class="s1">res2.mu_pred = res_glm.model.predict(res_glm.params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">which=</span><span class="s5">&quot;mean&quot;</span><span class="s1">)</span>
        <span class="s1">res1.mu_pred = res_gam.mu</span>

        <span class="s0">#parameters</span>
        <span class="s1">slopes = [i </span><span class="s3">for </span><span class="s1">ss </span><span class="s3">in </span><span class="s1">m.smoothers </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">ss.params[</span><span class="s4">1</span><span class="s1">:]]</span>
        <span class="s1">const = res_gam.alpha + sum([ss.params[</span><span class="s4">1</span><span class="s1">] </span><span class="s3">for </span><span class="s1">ss </span><span class="s3">in </span><span class="s1">m.smoothers])</span>
        <span class="s1">res1.params = np.array([const] + slopes)</span>


<span class="s3">class </span><span class="s1">TestGAMPoisson(BaseGAM):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">super(TestGAMPoisson</span><span class="s3">, </span><span class="s1">cls).setup_class() </span><span class="s0">#initialize DGP</span>

        <span class="s1">cls.family = family.Poisson()</span>
        <span class="s1">cls.rvs = stats.poisson.rvs</span>

        <span class="s1">cls.init()</span>

<span class="s3">class </span><span class="s1">TestGAMBinomial(BaseGAM):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">super(TestGAMBinomial</span><span class="s3">, </span><span class="s1">cls).setup_class() </span><span class="s0">#initialize DGP</span>

        <span class="s1">cls.family = family.Binomial()</span>
        <span class="s1">cls.rvs = stats.bernoulli.rvs</span>

        <span class="s1">cls.init()</span>


<span class="s1">@pytest.mark.xfail(reason=</span><span class="s5">&quot;Unknown, results do not match expected.&quot;</span><span class="s3">,</span>
                   <span class="s1">strict=</span><span class="s3">True, </span><span class="s1">raises=AssertionError)</span>
<span class="s3">class </span><span class="s1">TestGAMGaussianLogLink(BaseGAM):</span>
    <span class="s0">#test failure, but maybe precision issue, not far off</span>
    <span class="s0">#&gt;&gt;&gt; np.mean(np.abs(tt.res2.mu_pred - tt.mu_true))</span>
    <span class="s0">#0.80409736263199649</span>
    <span class="s0">#&gt;&gt;&gt; np.mean(np.abs(tt.res2.mu_pred - tt.mu_true))/tt.mu_true.mean()</span>
    <span class="s0">#0.023258245077813208</span>
    <span class="s0">#&gt;&gt;&gt; np.mean((tt.res2.mu_pred - tt.mu_true)**2)/tt.mu_true.mean()</span>
    <span class="s0">#0.022989403735692578</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">super(TestGAMGaussianLogLink</span><span class="s3">, </span><span class="s1">cls).setup_class()  </span><span class="s0"># initialize DGP</span>

        <span class="s1">cls.family = family.Gaussian(links.Log())</span>
        <span class="s1">cls.rvs = stats.norm.rvs</span>
        <span class="s1">cls.scale = </span><span class="s4">5</span>

        <span class="s1">cls.init()</span>


<span class="s3">class </span><span class="s1">TestGAMGamma(BaseGAM):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">super(TestGAMGamma</span><span class="s3">, </span><span class="s1">cls).setup_class() </span><span class="s0">#initialize DGP</span>

        <span class="s1">cls.family = family.Gamma(links.Log())</span>
        <span class="s1">cls.rvs = stats.gamma.rvs</span>

        <span class="s1">cls.init()</span>


<span class="s1">@pytest.mark.xfail(reason=</span><span class="s5">&quot;Passing wrong number of args/kwargs &quot;</span>
                          <span class="s5">&quot;to _parse_args_rvs&quot;</span><span class="s3">,</span>
                   <span class="s1">strict=</span><span class="s3">True, </span><span class="s1">raises=TypeError)</span>
<span class="s3">class </span><span class="s1">TestGAMNegativeBinomial(BaseGAM):</span>
    <span class="s0"># TODO: rvs generation does not work, nbinom needs 2 parameters</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">super(TestGAMNegativeBinomial</span><span class="s3">, </span><span class="s1">cls).setup_class()  </span><span class="s0"># initialize DGP</span>

        <span class="s1">cls.family = family.NegativeBinomial()</span>
        <span class="s1">cls.rvs = stats.nbinom.rvs</span>

        <span class="s1">cls.init()</span>

    <span class="s1">@pytest.mark.xfail(reason=</span><span class="s5">&quot;Passing wrong number of args/kwargs &quot;</span>
                              <span class="s5">&quot;to _parse_args_rvs&quot;</span><span class="s3">,</span>
                       <span class="s1">strict=</span><span class="s3">True, </span><span class="s1">raises=TypeError)</span>
    <span class="s3">def </span><span class="s1">test_fitted(self):</span>
        <span class="s0"># We have to override the base class method in order to correctly</span>
        <span class="s0">#  specify the type of failure we are expecting.</span>
        <span class="s1">super(TestGAMNegativeBinomial</span><span class="s3">, </span><span class="s1">self).test_fitted()</span>

    <span class="s1">@pytest.mark.xfail(reason=</span><span class="s5">&quot;Passing wrong number of args/kwargs &quot;</span>
                              <span class="s5">&quot;to _parse_args_rvs&quot;</span><span class="s3">,</span>
                       <span class="s1">strict=</span><span class="s3">True, </span><span class="s1">raises=TypeError)</span>
    <span class="s3">def </span><span class="s1">test_df(self):</span>
        <span class="s0"># We have to override the base class method in order to correctly</span>
        <span class="s0">#  specify the type of failure we are expecting.</span>
        <span class="s1">super(TestGAMNegativeBinomial</span><span class="s3">, </span><span class="s1">self).test_df()</span>
</pre>
</body>
</html>