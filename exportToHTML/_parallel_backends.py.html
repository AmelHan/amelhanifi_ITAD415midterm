<html>
<head>
<title>_parallel_backends.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_parallel_backends.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Backends for embarrassingly parallel code. 
&quot;&quot;&quot;</span>

<span class="s2">import </span><span class="s1">gc</span>
<span class="s2">import </span><span class="s1">os</span>
<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">import </span><span class="s1">threading</span>
<span class="s2">import </span><span class="s1">contextlib</span>
<span class="s2">from </span><span class="s1">abc </span><span class="s2">import </span><span class="s1">ABCMeta</span><span class="s2">, </span><span class="s1">abstractmethod</span>


<span class="s2">from </span><span class="s1">._multiprocessing_helpers </span><span class="s2">import </span><span class="s1">mp</span>

<span class="s2">if </span><span class="s1">mp </span><span class="s2">is not None</span><span class="s1">:</span>
    <span class="s2">from </span><span class="s1">.pool </span><span class="s2">import </span><span class="s1">MemmappingPool</span>
    <span class="s2">from </span><span class="s1">multiprocessing.pool </span><span class="s2">import </span><span class="s1">ThreadPool</span>
    <span class="s2">from </span><span class="s1">.executor </span><span class="s2">import </span><span class="s1">get_memmapping_executor</span>

    <span class="s3"># Import loky only if multiprocessing is present</span>
    <span class="s2">from </span><span class="s1">.externals.loky </span><span class="s2">import </span><span class="s1">process_executor</span><span class="s2">, </span><span class="s1">cpu_count</span>
    <span class="s2">from </span><span class="s1">.externals.loky.process_executor </span><span class="s2">import </span><span class="s1">ShutdownExecutorError</span>
    <span class="s2">from </span><span class="s1">.externals.loky.process_executor </span><span class="s2">import </span><span class="s1">_ExceptionWithTraceback</span>


<span class="s2">class </span><span class="s1">ParallelBackendBase(metaclass=ABCMeta):</span>
    <span class="s0">&quot;&quot;&quot;Helper abc which defines all methods a ParallelBackend must implement&quot;&quot;&quot;</span>

    <span class="s1">supports_inner_max_num_threads = </span><span class="s2">False</span>
    <span class="s1">supports_retrieve_callback = </span><span class="s2">False</span>
    <span class="s1">default_n_jobs = </span><span class="s4">1</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">supports_return_generator(self):</span>
        <span class="s2">return </span><span class="s1">self.supports_retrieve_callback</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">supports_timeout(self):</span>
        <span class="s2">return </span><span class="s1">self.supports_retrieve_callback</span>

    <span class="s1">nesting_level = </span><span class="s2">None</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">nesting_level=</span><span class="s2">None, </span><span class="s1">inner_max_num_threads=</span><span class="s2">None,</span>
                 <span class="s1">**kwargs):</span>
        <span class="s1">super().__init__(**kwargs)</span>
        <span class="s1">self.nesting_level = nesting_level</span>
        <span class="s1">self.inner_max_num_threads = inner_max_num_threads</span>

    <span class="s1">MAX_NUM_THREADS_VARS = [</span>
        <span class="s5">'OMP_NUM_THREADS'</span><span class="s2">, </span><span class="s5">'OPENBLAS_NUM_THREADS'</span><span class="s2">, </span><span class="s5">'MKL_NUM_THREADS'</span><span class="s2">,</span>
        <span class="s5">'BLIS_NUM_THREADS'</span><span class="s2">, </span><span class="s5">'VECLIB_MAXIMUM_THREADS'</span><span class="s2">, </span><span class="s5">'NUMBA_NUM_THREADS'</span><span class="s2">,</span>
        <span class="s5">'NUMEXPR_NUM_THREADS'</span><span class="s2">,</span>
    <span class="s1">]</span>

    <span class="s1">TBB_ENABLE_IPC_VAR = </span><span class="s5">&quot;ENABLE_IPC&quot;</span>

    <span class="s1">@abstractmethod</span>
    <span class="s2">def </span><span class="s1">effective_n_jobs(self</span><span class="s2">, </span><span class="s1">n_jobs):</span>
        <span class="s0">&quot;&quot;&quot;Determine the number of jobs that can actually run in parallel 
 
        n_jobs is the number of workers requested by the callers. Passing 
        n_jobs=-1 means requesting all available workers for instance matching 
        the number of CPU cores on the worker host(s). 
 
        This method should return a guesstimate of the number of workers that 
        can actually perform work concurrently. The primary use case is to make 
        it possible for the caller to know in how many chunks to slice the 
        work. 
 
        In general working on larger data chunks is more efficient (less 
        scheduling overhead and better use of CPU cache prefetching heuristics) 
        as long as all the workers have enough work to do. 
        &quot;&quot;&quot;</span>

    <span class="s1">@abstractmethod</span>
    <span class="s2">def </span><span class="s1">apply_async(self</span><span class="s2">, </span><span class="s1">func</span><span class="s2">, </span><span class="s1">callback=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Schedule a func to be run&quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">retrieve_result_callback(self</span><span class="s2">, </span><span class="s1">out):</span>
        <span class="s0">&quot;&quot;&quot;Called within the callback function passed in apply_async. 
 
        The argument of this function is the argument given to a callback in 
        the considered backend. It is supposed to return the outcome of a task 
        if it succeeded or raise the exception if it failed. 
        &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">configure(self</span><span class="s2">, </span><span class="s1">n_jobs=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">parallel=</span><span class="s2">None, </span><span class="s1">prefer=</span><span class="s2">None, </span><span class="s1">require=</span><span class="s2">None,</span>
                  <span class="s1">**backend_args):</span>
        <span class="s0">&quot;&quot;&quot;Reconfigure the backend and return the number of workers. 
 
        This makes it possible to reuse an existing backend instance for 
        successive independent calls to Parallel with different parameters. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.parallel = parallel</span>
        <span class="s2">return </span><span class="s1">self.effective_n_jobs(n_jobs)</span>

    <span class="s2">def </span><span class="s1">start_call(self):</span>
        <span class="s0">&quot;&quot;&quot;Call-back method called at the beginning of a Parallel call&quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">stop_call(self):</span>
        <span class="s0">&quot;&quot;&quot;Call-back method called at the end of a Parallel call&quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">terminate(self):</span>
        <span class="s0">&quot;&quot;&quot;Shutdown the workers and free the shared memory.&quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">compute_batch_size(self):</span>
        <span class="s0">&quot;&quot;&quot;Determine the optimal batch size&quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s4">1</span>

    <span class="s2">def </span><span class="s1">batch_completed(self</span><span class="s2">, </span><span class="s1">batch_size</span><span class="s2">, </span><span class="s1">duration):</span>
        <span class="s0">&quot;&quot;&quot;Callback indicate how long it took to run a batch&quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">get_exceptions(self):</span>
        <span class="s0">&quot;&quot;&quot;List of exception types to be captured.&quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">[]</span>

    <span class="s2">def </span><span class="s1">abort_everything(self</span><span class="s2">, </span><span class="s1">ensure_ready=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Abort any running tasks 
 
        This is called when an exception has been raised when executing a task 
        and all the remaining tasks will be ignored and can therefore be 
        aborted to spare computation resources. 
 
        If ensure_ready is True, the backend should be left in an operating 
        state as future tasks might be re-submitted via that same backend 
        instance. 
 
        If ensure_ready is False, the implementer of this method can decide 
        to leave the backend in a closed / terminated state as no new task 
        are expected to be submitted to this backend. 
 
        Setting ensure_ready to False is an optimization that can be leveraged 
        when aborting tasks via killing processes from a local process pool 
        managed by the backend it-self: if we expect no new tasks, there is no 
        point in re-creating new workers. 
        &quot;&quot;&quot;</span>
        <span class="s3"># Does nothing by default: to be overridden in subclasses when</span>
        <span class="s3"># canceling tasks is possible.</span>
        <span class="s2">pass</span>

    <span class="s2">def </span><span class="s1">get_nested_backend(self):</span>
        <span class="s0">&quot;&quot;&quot;Backend instance to be used by nested Parallel calls. 
 
        By default a thread-based backend is used for the first level of 
        nesting. Beyond, switch to sequential backend to avoid spawning too 
        many threads on the host. 
        &quot;&quot;&quot;</span>
        <span class="s1">nesting_level = getattr(self</span><span class="s2">, </span><span class="s5">'nesting_level'</span><span class="s2">, </span><span class="s4">0</span><span class="s1">) + </span><span class="s4">1</span>
        <span class="s2">if </span><span class="s1">nesting_level &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">SequentialBackend(nesting_level=nesting_level)</span><span class="s2">, None</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">ThreadingBackend(nesting_level=nesting_level)</span><span class="s2">, None</span>

    <span class="s1">@contextlib.contextmanager</span>
    <span class="s2">def </span><span class="s1">retrieval_context(self):</span>
        <span class="s0">&quot;&quot;&quot;Context manager to manage an execution context. 
 
        Calls to Parallel.retrieve will be made inside this context. 
 
        By default, this does nothing. It may be useful for subclasses to 
        handle nested parallelism. In particular, it may be required to avoid 
        deadlocks if a backend manages a fixed number of workers, when those 
        workers may be asked to do nested Parallel calls. Without 
        'retrieval_context' this could lead to deadlock, as all the workers 
        managed by the backend may be &quot;busy&quot; waiting for the nested parallel 
        calls to finish, but the backend has no free workers to execute those 
        tasks. 
        &quot;&quot;&quot;</span>
        <span class="s2">yield</span>

    <span class="s2">def </span><span class="s1">_prepare_worker_env(self</span><span class="s2">, </span><span class="s1">n_jobs):</span>
        <span class="s0">&quot;&quot;&quot;Return environment variables limiting threadpools in external libs. 
 
        This function return a dict containing environment variables to pass 
        when creating a pool of process. These environment variables limit the 
        number of threads to `n_threads` for OpenMP, MKL, Accelerated and 
        OpenBLAS libraries in the child processes. 
        &quot;&quot;&quot;</span>
        <span class="s1">explicit_n_threads = self.inner_max_num_threads</span>
        <span class="s1">default_n_threads = str(max(cpu_count() // n_jobs</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>

        <span class="s3"># Set the inner environment variables to self.inner_max_num_threads if</span>
        <span class="s3"># it is given. Else, default to cpu_count // n_jobs unless the variable</span>
        <span class="s3"># is already present in the parent process environment.</span>
        <span class="s1">env = {}</span>
        <span class="s2">for </span><span class="s1">var </span><span class="s2">in </span><span class="s1">self.MAX_NUM_THREADS_VARS:</span>
            <span class="s2">if </span><span class="s1">explicit_n_threads </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s1">var_value = os.environ.get(var</span><span class="s2">, None</span><span class="s1">)</span>
                <span class="s2">if </span><span class="s1">var_value </span><span class="s2">is None</span><span class="s1">:</span>
                    <span class="s1">var_value = default_n_threads</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">var_value = str(explicit_n_threads)</span>

            <span class="s1">env[var] = var_value</span>

        <span class="s2">if </span><span class="s1">self.TBB_ENABLE_IPC_VAR </span><span class="s2">not in </span><span class="s1">os.environ:</span>
            <span class="s3"># To avoid over-subscription when using TBB, let the TBB schedulers</span>
            <span class="s3"># use Inter Process Communication to coordinate:</span>
            <span class="s1">env[self.TBB_ENABLE_IPC_VAR] = </span><span class="s5">&quot;1&quot;</span>
        <span class="s2">return </span><span class="s1">env</span>

    <span class="s1">@staticmethod</span>
    <span class="s2">def </span><span class="s1">in_main_thread():</span>
        <span class="s2">return </span><span class="s1">isinstance(threading.current_thread()</span><span class="s2">, </span><span class="s1">threading._MainThread)</span>


<span class="s2">class </span><span class="s1">SequentialBackend(ParallelBackendBase):</span>
    <span class="s0">&quot;&quot;&quot;A ParallelBackend which will execute all batches sequentially. 
 
    Does not use/create any threading objects, and hence has minimal 
    overhead. Used when n_jobs == 1. 
    &quot;&quot;&quot;</span>

    <span class="s1">uses_threads = </span><span class="s2">True</span>
    <span class="s1">supports_timeout = </span><span class="s2">False</span>
    <span class="s1">supports_retrieve_callback = </span><span class="s2">False</span>
    <span class="s1">supports_sharedmem = </span><span class="s2">True</span>

    <span class="s2">def </span><span class="s1">effective_n_jobs(self</span><span class="s2">, </span><span class="s1">n_jobs):</span>
        <span class="s0">&quot;&quot;&quot;Determine the number of jobs which are going to run in parallel&quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">n_jobs == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'n_jobs == 0 in Parallel has no meaning'</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s4">1</span>

    <span class="s2">def </span><span class="s1">apply_async(self</span><span class="s2">, </span><span class="s1">func</span><span class="s2">, </span><span class="s1">callback=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Schedule a func to be run&quot;&quot;&quot;</span>
        <span class="s2">raise </span><span class="s1">RuntimeError(</span><span class="s5">&quot;Should never be called for SequentialBackend.&quot;</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">retrieve_result_callback(self</span><span class="s2">, </span><span class="s1">out):</span>
        <span class="s2">raise </span><span class="s1">RuntimeError(</span><span class="s5">&quot;Should never be called for SequentialBackend.&quot;</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">get_nested_backend(self):</span>
        <span class="s3"># import is not top level to avoid cyclic import errors.</span>
        <span class="s2">from </span><span class="s1">.parallel </span><span class="s2">import </span><span class="s1">get_active_backend</span>

        <span class="s3"># SequentialBackend should neither change the nesting level, the</span>
        <span class="s3"># default backend or the number of jobs. Just return the current one.</span>
        <span class="s2">return </span><span class="s1">get_active_backend()</span>


<span class="s2">class </span><span class="s1">PoolManagerMixin(object):</span>
    <span class="s0">&quot;&quot;&quot;A helper class for managing pool of workers.&quot;&quot;&quot;</span>

    <span class="s1">_pool = </span><span class="s2">None</span>

    <span class="s2">def </span><span class="s1">effective_n_jobs(self</span><span class="s2">, </span><span class="s1">n_jobs):</span>
        <span class="s0">&quot;&quot;&quot;Determine the number of jobs which are going to run in parallel&quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">n_jobs == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'n_jobs == 0 in Parallel has no meaning'</span><span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">mp </span><span class="s2">is None or </span><span class="s1">n_jobs </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s3"># multiprocessing is not available or disabled, fallback</span>
            <span class="s3"># to sequential mode</span>
            <span class="s2">return </span><span class="s4">1</span>
        <span class="s2">elif </span><span class="s1">n_jobs &lt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">n_jobs = max(cpu_count() + </span><span class="s4">1 </span><span class="s1">+ n_jobs</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">n_jobs</span>

    <span class="s2">def </span><span class="s1">terminate(self):</span>
        <span class="s0">&quot;&quot;&quot;Shutdown the process or thread pool&quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">self._pool </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">self._pool.close()</span>
            <span class="s1">self._pool.terminate()  </span><span class="s3"># terminate does a join()</span>
            <span class="s1">self._pool = </span><span class="s2">None</span>

    <span class="s2">def </span><span class="s1">_get_pool(self):</span>
        <span class="s0">&quot;&quot;&quot;Used by apply_async to make it possible to implement lazy init&quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self._pool</span>

    <span class="s1">@staticmethod</span>
    <span class="s2">def </span><span class="s1">_wrap_func_call(func):</span>
        <span class="s0">&quot;&quot;&quot;Protect function call and return error with traceback.&quot;&quot;&quot;</span>
        <span class="s2">try</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">func()</span>
        <span class="s2">except </span><span class="s1">BaseException </span><span class="s2">as </span><span class="s1">e:</span>
            <span class="s2">return </span><span class="s1">_ExceptionWithTraceback(e)</span>

    <span class="s2">def </span><span class="s1">apply_async(self</span><span class="s2">, </span><span class="s1">func</span><span class="s2">, </span><span class="s1">callback=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Schedule a func to be run&quot;&quot;&quot;</span>
        <span class="s3"># Here, we need a wrapper to avoid crashes on KeyboardInterruptErrors.</span>
        <span class="s3"># We also call the callback on error, to make sure the pool does not</span>
        <span class="s3"># wait on crashed jobs.</span>
        <span class="s2">return </span><span class="s1">self._get_pool().apply_async(</span>
            <span class="s1">self._wrap_func_call</span><span class="s2">, </span><span class="s1">(func</span><span class="s2">,</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">callback=callback</span><span class="s2">, </span><span class="s1">error_callback=callback</span>
        <span class="s1">)</span>

    <span class="s2">def </span><span class="s1">retrieve_result_callback(self</span><span class="s2">, </span><span class="s1">out):</span>
        <span class="s0">&quot;&quot;&quot;Mimic concurrent.futures results, raising an error if needed.&quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">isinstance(out</span><span class="s2">, </span><span class="s1">_ExceptionWithTraceback):</span>
            <span class="s1">rebuild</span><span class="s2">, </span><span class="s1">args = out.__reduce__()</span>
            <span class="s1">out = rebuild(*args)</span>
        <span class="s2">if </span><span class="s1">isinstance(out</span><span class="s2">, </span><span class="s1">BaseException):</span>
            <span class="s2">raise </span><span class="s1">out</span>
        <span class="s2">return </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">abort_everything(self</span><span class="s2">, </span><span class="s1">ensure_ready=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Shutdown the pool and restart a new one with the same parameters&quot;&quot;&quot;</span>
        <span class="s1">self.terminate()</span>
        <span class="s2">if </span><span class="s1">ensure_ready:</span>
            <span class="s1">self.configure(n_jobs=self.parallel.n_jobs</span><span class="s2">, </span><span class="s1">parallel=self.parallel</span><span class="s2">,</span>
                           <span class="s1">**self.parallel._backend_args)</span>


<span class="s2">class </span><span class="s1">AutoBatchingMixin(object):</span>
    <span class="s0">&quot;&quot;&quot;A helper class for automagically batching jobs.&quot;&quot;&quot;</span>

    <span class="s3"># In seconds, should be big enough to hide multiprocessing dispatching</span>
    <span class="s3"># overhead.</span>
    <span class="s3"># This settings was found by running benchmarks/bench_auto_batching.py</span>
    <span class="s3"># with various parameters on various platforms.</span>
    <span class="s1">MIN_IDEAL_BATCH_DURATION = </span><span class="s4">.2</span>

    <span class="s3"># Should not be too high to avoid stragglers: long jobs running alone</span>
    <span class="s3"># on a single worker while other workers have no work to process any more.</span>
    <span class="s1">MAX_IDEAL_BATCH_DURATION = </span><span class="s4">2</span>

    <span class="s3"># Batching counters default values</span>
    <span class="s1">_DEFAULT_EFFECTIVE_BATCH_SIZE = </span><span class="s4">1</span>
    <span class="s1">_DEFAULT_SMOOTHED_BATCH_DURATION = </span><span class="s4">0.0</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">**kwargs):</span>
        <span class="s1">super().__init__(**kwargs)</span>
        <span class="s1">self._effective_batch_size = self._DEFAULT_EFFECTIVE_BATCH_SIZE</span>
        <span class="s1">self._smoothed_batch_duration = self._DEFAULT_SMOOTHED_BATCH_DURATION</span>

    <span class="s2">def </span><span class="s1">compute_batch_size(self):</span>
        <span class="s0">&quot;&quot;&quot;Determine the optimal batch size&quot;&quot;&quot;</span>
        <span class="s1">old_batch_size = self._effective_batch_size</span>
        <span class="s1">batch_duration = self._smoothed_batch_duration</span>
        <span class="s2">if </span><span class="s1">(batch_duration &gt; </span><span class="s4">0 </span><span class="s2">and</span>
                <span class="s1">batch_duration &lt; self.MIN_IDEAL_BATCH_DURATION):</span>
            <span class="s3"># The current batch size is too small: the duration of the</span>
            <span class="s3"># processing of a batch of task is not large enough to hide</span>
            <span class="s3"># the scheduling overhead.</span>
            <span class="s1">ideal_batch_size = int(old_batch_size *</span>
                                   <span class="s1">self.MIN_IDEAL_BATCH_DURATION /</span>
                                   <span class="s1">batch_duration)</span>
            <span class="s3"># Multiply by two to limit oscilations between min and max.</span>
            <span class="s1">ideal_batch_size *= </span><span class="s4">2</span>

            <span class="s3"># dont increase the batch size too fast to limit huge batch sizes</span>
            <span class="s3"># potentially leading to starving worker</span>
            <span class="s1">batch_size = min(</span><span class="s4">2 </span><span class="s1">* old_batch_size</span><span class="s2">, </span><span class="s1">ideal_batch_size)</span>

            <span class="s1">batch_size = max(batch_size</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>

            <span class="s1">self._effective_batch_size = batch_size</span>
            <span class="s2">if </span><span class="s1">self.parallel.verbose &gt;= </span><span class="s4">10</span><span class="s1">:</span>
                <span class="s1">self.parallel._print(</span>
                    <span class="s5">f&quot;Batch computation too fast (</span><span class="s2">{</span><span class="s1">batch_duration</span><span class="s2">}</span><span class="s5">s.) &quot;</span>
                    <span class="s5">f&quot;Setting batch_size=</span><span class="s2">{</span><span class="s1">batch_size</span><span class="s2">}</span><span class="s5">.&quot;</span>
                <span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">(batch_duration &gt; self.MAX_IDEAL_BATCH_DURATION </span><span class="s2">and</span>
              <span class="s1">old_batch_size &gt;= </span><span class="s4">2</span><span class="s1">):</span>
            <span class="s3"># The current batch size is too big. If we schedule overly long</span>
            <span class="s3"># running batches some CPUs might wait with nothing left to do</span>
            <span class="s3"># while a couple of CPUs a left processing a few long running</span>
            <span class="s3"># batches. Better reduce the batch size a bit to limit the</span>
            <span class="s3"># likelihood of scheduling such stragglers.</span>

            <span class="s3"># decrease the batch size quickly to limit potential starving</span>
            <span class="s1">ideal_batch_size = int(</span>
                <span class="s1">old_batch_size * self.MIN_IDEAL_BATCH_DURATION / batch_duration</span>
            <span class="s1">)</span>
            <span class="s3"># Multiply by two to limit oscilations between min and max.</span>
            <span class="s1">batch_size = max(</span><span class="s4">2 </span><span class="s1">* ideal_batch_size</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">self._effective_batch_size = batch_size</span>
            <span class="s2">if </span><span class="s1">self.parallel.verbose &gt;= </span><span class="s4">10</span><span class="s1">:</span>
                <span class="s1">self.parallel._print(</span>
                    <span class="s5">f&quot;Batch computation too slow (</span><span class="s2">{</span><span class="s1">batch_duration</span><span class="s2">}</span><span class="s5">s.) &quot;</span>
                    <span class="s5">f&quot;Setting batch_size=</span><span class="s2">{</span><span class="s1">batch_size</span><span class="s2">}</span><span class="s5">.&quot;</span>
                <span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s3"># No batch size adjustment</span>
            <span class="s1">batch_size = old_batch_size</span>

        <span class="s2">if </span><span class="s1">batch_size != old_batch_size:</span>
            <span class="s3"># Reset estimation of the smoothed mean batch duration: this</span>
            <span class="s3"># estimate is updated in the multiprocessing apply_async</span>
            <span class="s3"># CallBack as long as the batch_size is constant. Therefore</span>
            <span class="s3"># we need to reset the estimate whenever we re-tune the batch</span>
            <span class="s3"># size.</span>
            <span class="s1">self._smoothed_batch_duration = \</span>
                <span class="s1">self._DEFAULT_SMOOTHED_BATCH_DURATION</span>

        <span class="s2">return </span><span class="s1">batch_size</span>

    <span class="s2">def </span><span class="s1">batch_completed(self</span><span class="s2">, </span><span class="s1">batch_size</span><span class="s2">, </span><span class="s1">duration):</span>
        <span class="s0">&quot;&quot;&quot;Callback indicate how long it took to run a batch&quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">batch_size == self._effective_batch_size:</span>
            <span class="s3"># Update the smoothed streaming estimate of the duration of a batch</span>
            <span class="s3"># from dispatch to completion</span>
            <span class="s1">old_duration = self._smoothed_batch_duration</span>
            <span class="s2">if </span><span class="s1">old_duration == self._DEFAULT_SMOOTHED_BATCH_DURATION:</span>
                <span class="s3"># First record of duration for this batch size after the last</span>
                <span class="s3"># reset.</span>
                <span class="s1">new_duration = duration</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s3"># Update the exponentially weighted average of the duration of</span>
                <span class="s3"># batch for the current effective size.</span>
                <span class="s1">new_duration = </span><span class="s4">0.8 </span><span class="s1">* old_duration + </span><span class="s4">0.2 </span><span class="s1">* duration</span>
            <span class="s1">self._smoothed_batch_duration = new_duration</span>

    <span class="s2">def </span><span class="s1">reset_batch_stats(self):</span>
        <span class="s0">&quot;&quot;&quot;Reset batch statistics to default values. 
 
        This avoids interferences with future jobs. 
        &quot;&quot;&quot;</span>
        <span class="s1">self._effective_batch_size = self._DEFAULT_EFFECTIVE_BATCH_SIZE</span>
        <span class="s1">self._smoothed_batch_duration = self._DEFAULT_SMOOTHED_BATCH_DURATION</span>


<span class="s2">class </span><span class="s1">ThreadingBackend(PoolManagerMixin</span><span class="s2">, </span><span class="s1">ParallelBackendBase):</span>
    <span class="s0">&quot;&quot;&quot;A ParallelBackend which will use a thread pool to execute batches in. 
 
    This is a low-overhead backend but it suffers from the Python Global 
    Interpreter Lock if the called function relies a lot on Python objects. 
    Mostly useful when the execution bottleneck is a compiled extension that 
    explicitly releases the GIL (for instance a Cython loop wrapped in a &quot;with 
    nogil&quot; block or an expensive call to a library such as NumPy). 
 
    The actual thread pool is lazily initialized: the actual thread pool 
    construction is delayed to the first call to apply_async. 
 
    ThreadingBackend is used as the default backend for nested calls. 
    &quot;&quot;&quot;</span>

    <span class="s1">supports_retrieve_callback = </span><span class="s2">True</span>
    <span class="s1">uses_threads = </span><span class="s2">True</span>
    <span class="s1">supports_sharedmem = </span><span class="s2">True</span>

    <span class="s2">def </span><span class="s1">configure(self</span><span class="s2">, </span><span class="s1">n_jobs=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">parallel=</span><span class="s2">None, </span><span class="s1">**backend_args):</span>
        <span class="s0">&quot;&quot;&quot;Build a process or thread pool and return the number of workers&quot;&quot;&quot;</span>
        <span class="s1">n_jobs = self.effective_n_jobs(n_jobs)</span>
        <span class="s2">if </span><span class="s1">n_jobs == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s3"># Avoid unnecessary overhead and use sequential backend instead.</span>
            <span class="s2">raise </span><span class="s1">FallbackToBackend(</span>
                <span class="s1">SequentialBackend(nesting_level=self.nesting_level))</span>
        <span class="s1">self.parallel = parallel</span>
        <span class="s1">self._n_jobs = n_jobs</span>
        <span class="s2">return </span><span class="s1">n_jobs</span>

    <span class="s2">def </span><span class="s1">_get_pool(self):</span>
        <span class="s0">&quot;&quot;&quot;Lazily initialize the thread pool 
 
        The actual pool of worker threads is only initialized at the first 
        call to apply_async. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">self._pool </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">self._pool = ThreadPool(self._n_jobs)</span>
        <span class="s2">return </span><span class="s1">self._pool</span>


<span class="s2">class </span><span class="s1">MultiprocessingBackend(PoolManagerMixin</span><span class="s2">, </span><span class="s1">AutoBatchingMixin</span><span class="s2">,</span>
                             <span class="s1">ParallelBackendBase):</span>
    <span class="s0">&quot;&quot;&quot;A ParallelBackend which will use a multiprocessing.Pool. 
 
    Will introduce some communication and memory overhead when exchanging 
    input and output data with the with the worker Python processes. 
    However, does not suffer from the Python Global Interpreter Lock. 
    &quot;&quot;&quot;</span>

    <span class="s1">supports_retrieve_callback = </span><span class="s2">True</span>
    <span class="s1">supports_return_generator = </span><span class="s2">False</span>

    <span class="s2">def </span><span class="s1">effective_n_jobs(self</span><span class="s2">, </span><span class="s1">n_jobs):</span>
        <span class="s0">&quot;&quot;&quot;Determine the number of jobs which are going to run in parallel. 
 
        This also checks if we are attempting to create a nested parallel 
        loop. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">mp </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s4">1</span>

        <span class="s2">if </span><span class="s1">mp.current_process().daemon:</span>
            <span class="s3"># Daemonic processes cannot have children</span>
            <span class="s2">if </span><span class="s1">n_jobs != </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s2">if </span><span class="s1">inside_dask_worker():</span>
                    <span class="s1">msg = (</span>
                        <span class="s5">&quot;Inside a Dask worker with daemon=True, &quot;</span>
                        <span class="s5">&quot;setting n_jobs=1.</span><span class="s2">\n</span><span class="s5">Possible work-arounds:</span><span class="s2">\n</span><span class="s5">&quot;</span>
                        <span class="s5">&quot;- dask.config.set(&quot;</span>
                        <span class="s5">&quot;{'distributed.worker.daemon': False})&quot;</span>
                        <span class="s5">&quot;- set the environment variable &quot;</span>
                        <span class="s5">&quot;DASK_DISTRIBUTED__WORKER__DAEMON=False</span><span class="s2">\n</span><span class="s5">&quot;</span>
                        <span class="s5">&quot;before creating your Dask cluster.&quot;</span>
                    <span class="s1">)</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">msg = (</span>
                        <span class="s5">'Multiprocessing-backed parallel loops '</span>
                        <span class="s5">'cannot be nested, setting n_jobs=1'</span>
                    <span class="s1">)</span>
                <span class="s1">warnings.warn(msg</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">3</span><span class="s1">)</span>
            <span class="s2">return </span><span class="s4">1</span>

        <span class="s2">if </span><span class="s1">process_executor._CURRENT_DEPTH &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s3"># Mixing loky and multiprocessing in nested loop is not supported</span>
            <span class="s2">if </span><span class="s1">n_jobs != </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">warnings.warn(</span>
                    <span class="s5">'Multiprocessing-backed parallel loops cannot be nested,'</span>
                    <span class="s5">' below loky, setting n_jobs=1'</span><span class="s2">,</span>
                    <span class="s1">stacklevel=</span><span class="s4">3</span><span class="s1">)</span>
            <span class="s2">return </span><span class="s4">1</span>

        <span class="s2">elif not </span><span class="s1">(self.in_main_thread() </span><span class="s2">or </span><span class="s1">self.nesting_level == </span><span class="s4">0</span><span class="s1">):</span>
            <span class="s3"># Prevent posix fork inside in non-main posix threads</span>
            <span class="s2">if </span><span class="s1">n_jobs != </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">warnings.warn(</span>
                    <span class="s5">'Multiprocessing-backed parallel loops cannot be nested'</span>
                    <span class="s5">' below threads, setting n_jobs=1'</span><span class="s2">,</span>
                    <span class="s1">stacklevel=</span><span class="s4">3</span><span class="s1">)</span>
            <span class="s2">return </span><span class="s4">1</span>

        <span class="s2">return </span><span class="s1">super(MultiprocessingBackend</span><span class="s2">, </span><span class="s1">self).effective_n_jobs(n_jobs)</span>

    <span class="s2">def </span><span class="s1">configure(self</span><span class="s2">, </span><span class="s1">n_jobs=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">parallel=</span><span class="s2">None, </span><span class="s1">prefer=</span><span class="s2">None, </span><span class="s1">require=</span><span class="s2">None,</span>
                  <span class="s1">**memmappingpool_args):</span>
        <span class="s0">&quot;&quot;&quot;Build a process or thread pool and return the number of workers&quot;&quot;&quot;</span>
        <span class="s1">n_jobs = self.effective_n_jobs(n_jobs)</span>
        <span class="s2">if </span><span class="s1">n_jobs == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">FallbackToBackend(</span>
                <span class="s1">SequentialBackend(nesting_level=self.nesting_level))</span>

        <span class="s3"># Make sure to free as much memory as possible before forking</span>
        <span class="s1">gc.collect()</span>
        <span class="s1">self._pool = MemmappingPool(n_jobs</span><span class="s2">, </span><span class="s1">**memmappingpool_args)</span>
        <span class="s1">self.parallel = parallel</span>
        <span class="s2">return </span><span class="s1">n_jobs</span>

    <span class="s2">def </span><span class="s1">terminate(self):</span>
        <span class="s0">&quot;&quot;&quot;Shutdown the process or thread pool&quot;&quot;&quot;</span>
        <span class="s1">super(MultiprocessingBackend</span><span class="s2">, </span><span class="s1">self).terminate()</span>
        <span class="s1">self.reset_batch_stats()</span>


<span class="s2">class </span><span class="s1">LokyBackend(AutoBatchingMixin</span><span class="s2">, </span><span class="s1">ParallelBackendBase):</span>
    <span class="s0">&quot;&quot;&quot;Managing pool of workers with loky instead of multiprocessing.&quot;&quot;&quot;</span>

    <span class="s1">supports_retrieve_callback = </span><span class="s2">True</span>
    <span class="s1">supports_inner_max_num_threads = </span><span class="s2">True</span>

    <span class="s2">def </span><span class="s1">configure(self</span><span class="s2">, </span><span class="s1">n_jobs=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">parallel=</span><span class="s2">None, </span><span class="s1">prefer=</span><span class="s2">None, </span><span class="s1">require=</span><span class="s2">None,</span>
                  <span class="s1">idle_worker_timeout=</span><span class="s4">300</span><span class="s2">, </span><span class="s1">**memmappingexecutor_args):</span>
        <span class="s0">&quot;&quot;&quot;Build a process executor and return the number of workers&quot;&quot;&quot;</span>
        <span class="s1">n_jobs = self.effective_n_jobs(n_jobs)</span>
        <span class="s2">if </span><span class="s1">n_jobs == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">FallbackToBackend(</span>
                <span class="s1">SequentialBackend(nesting_level=self.nesting_level))</span>

        <span class="s1">self._workers = get_memmapping_executor(</span>
            <span class="s1">n_jobs</span><span class="s2">, </span><span class="s1">timeout=idle_worker_timeout</span><span class="s2">,</span>
            <span class="s1">env=self._prepare_worker_env(n_jobs=n_jobs)</span><span class="s2">,</span>
            <span class="s1">context_id=parallel._id</span><span class="s2">, </span><span class="s1">**memmappingexecutor_args)</span>
        <span class="s1">self.parallel = parallel</span>
        <span class="s2">return </span><span class="s1">n_jobs</span>

    <span class="s2">def </span><span class="s1">effective_n_jobs(self</span><span class="s2">, </span><span class="s1">n_jobs):</span>
        <span class="s0">&quot;&quot;&quot;Determine the number of jobs which are going to run in parallel&quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">n_jobs == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'n_jobs == 0 in Parallel has no meaning'</span><span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">mp </span><span class="s2">is None or </span><span class="s1">n_jobs </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s3"># multiprocessing is not available or disabled, fallback</span>
            <span class="s3"># to sequential mode</span>
            <span class="s2">return </span><span class="s4">1</span>
        <span class="s2">elif </span><span class="s1">mp.current_process().daemon:</span>
            <span class="s3"># Daemonic processes cannot have children</span>
            <span class="s2">if </span><span class="s1">n_jobs != </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s2">if </span><span class="s1">inside_dask_worker():</span>
                    <span class="s1">msg = (</span>
                        <span class="s5">&quot;Inside a Dask worker with daemon=True, &quot;</span>
                        <span class="s5">&quot;setting n_jobs=1.</span><span class="s2">\n</span><span class="s5">Possible work-arounds:</span><span class="s2">\n</span><span class="s5">&quot;</span>
                        <span class="s5">&quot;- dask.config.set(&quot;</span>
                        <span class="s5">&quot;{'distributed.worker.daemon': False})</span><span class="s2">\n</span><span class="s5">&quot;</span>
                        <span class="s5">&quot;- set the environment variable &quot;</span>
                        <span class="s5">&quot;DASK_DISTRIBUTED__WORKER__DAEMON=False</span><span class="s2">\n</span><span class="s5">&quot;</span>
                        <span class="s5">&quot;before creating your Dask cluster.&quot;</span>
                    <span class="s1">)</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">msg = (</span>
                        <span class="s5">'Loky-backed parallel loops cannot be called in a'</span>
                        <span class="s5">' multiprocessing, setting n_jobs=1'</span>
                    <span class="s1">)</span>
                <span class="s1">warnings.warn(msg</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">3</span><span class="s1">)</span>

            <span class="s2">return </span><span class="s4">1</span>
        <span class="s2">elif not </span><span class="s1">(self.in_main_thread() </span><span class="s2">or </span><span class="s1">self.nesting_level == </span><span class="s4">0</span><span class="s1">):</span>
            <span class="s3"># Prevent posix fork inside in non-main posix threads</span>
            <span class="s2">if </span><span class="s1">n_jobs != </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">warnings.warn(</span>
                    <span class="s5">'Loky-backed parallel loops cannot be nested below '</span>
                    <span class="s5">'threads, setting n_jobs=1'</span><span class="s2">,</span>
                    <span class="s1">stacklevel=</span><span class="s4">3</span><span class="s1">)</span>
            <span class="s2">return </span><span class="s4">1</span>
        <span class="s2">elif </span><span class="s1">n_jobs &lt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">n_jobs = max(cpu_count() + </span><span class="s4">1 </span><span class="s1">+ n_jobs</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">n_jobs</span>

    <span class="s2">def </span><span class="s1">apply_async(self</span><span class="s2">, </span><span class="s1">func</span><span class="s2">, </span><span class="s1">callback=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Schedule a func to be run&quot;&quot;&quot;</span>
        <span class="s1">future = self._workers.submit(func)</span>
        <span class="s2">if </span><span class="s1">callback </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">future.add_done_callback(callback)</span>
        <span class="s2">return </span><span class="s1">future</span>

    <span class="s2">def </span><span class="s1">retrieve_result_callback(self</span><span class="s2">, </span><span class="s1">out):</span>
        <span class="s2">try</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">out.result()</span>
        <span class="s2">except </span><span class="s1">ShutdownExecutorError:</span>
            <span class="s2">raise </span><span class="s1">RuntimeError(</span>
                <span class="s5">&quot;The executor underlying Parallel has been shutdown. &quot;</span>
                <span class="s5">&quot;This is likely due to the garbage collection of a previous &quot;</span>
                <span class="s5">&quot;generator from a call to Parallel with return_as='generator'.&quot;</span>
                <span class="s5">&quot; Make sure the generator is not garbage collected when &quot;</span>
                <span class="s5">&quot;submitting a new job or that it is first properly exhausted.&quot;</span>
            <span class="s1">)</span>

    <span class="s2">def </span><span class="s1">terminate(self):</span>
        <span class="s2">if </span><span class="s1">self._workers </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s3"># Don't terminate the workers as we want to reuse them in later</span>
            <span class="s3"># calls, but cleanup the temporary resources that the Parallel call</span>
            <span class="s3"># created. This 'hack' requires a private, low-level operation.</span>
            <span class="s1">self._workers._temp_folder_manager._clean_temporary_resources(</span>
                <span class="s1">context_id=self.parallel._id</span><span class="s2">, </span><span class="s1">force=</span><span class="s2">False</span>
            <span class="s1">)</span>
            <span class="s1">self._workers = </span><span class="s2">None</span>

        <span class="s1">self.reset_batch_stats()</span>

    <span class="s2">def </span><span class="s1">abort_everything(self</span><span class="s2">, </span><span class="s1">ensure_ready=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Shutdown the workers and restart a new one with the same parameters 
        &quot;&quot;&quot;</span>
        <span class="s1">self._workers.terminate(kill_workers=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">self._workers = </span><span class="s2">None</span>

        <span class="s2">if </span><span class="s1">ensure_ready:</span>
            <span class="s1">self.configure(n_jobs=self.parallel.n_jobs</span><span class="s2">, </span><span class="s1">parallel=self.parallel)</span>


<span class="s2">class </span><span class="s1">FallbackToBackend(Exception):</span>
    <span class="s0">&quot;&quot;&quot;Raised when configuration should fallback to another backend&quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">backend):</span>
        <span class="s1">self.backend = backend</span>


<span class="s2">def </span><span class="s1">inside_dask_worker():</span>
    <span class="s0">&quot;&quot;&quot;Check whether the current function is executed inside a Dask worker. 
    &quot;&quot;&quot;</span>
    <span class="s3"># This function can not be in joblib._dask because there would be a</span>
    <span class="s3"># circular import:</span>
    <span class="s3"># _dask imports _parallel_backend that imports _dask ...</span>
    <span class="s2">try</span><span class="s1">:</span>
        <span class="s2">from </span><span class="s1">distributed </span><span class="s2">import </span><span class="s1">get_worker</span>
    <span class="s2">except </span><span class="s1">ImportError:</span>
        <span class="s2">return False</span>

    <span class="s2">try</span><span class="s1">:</span>
        <span class="s1">get_worker()</span>
        <span class="s2">return True</span>
    <span class="s2">except </span><span class="s1">ValueError:</span>
        <span class="s2">return False</span>
</pre>
</body>
</html>