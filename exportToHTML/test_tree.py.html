<html>
<head>
<title>test_tree.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_tree.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Testing for the tree module (sklearn.tree). 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">copy</span>
<span class="s2">import </span><span class="s1">copyreg</span>
<span class="s2">import </span><span class="s1">io</span>
<span class="s2">import </span><span class="s1">pickle</span>
<span class="s2">import </span><span class="s1">struct</span>
<span class="s2">from </span><span class="s1">itertools </span><span class="s2">import </span><span class="s1">chain</span><span class="s2">, </span><span class="s1">product</span>

<span class="s2">import </span><span class="s1">joblib</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">joblib.numpy_pickle </span><span class="s2">import </span><span class="s1">NumpyPickler</span>
<span class="s2">from </span><span class="s1">numpy.testing </span><span class="s2">import </span><span class="s1">assert_allclose</span>
<span class="s2">from </span><span class="s1">scipy.sparse </span><span class="s2">import </span><span class="s1">coo_matrix</span><span class="s2">, </span><span class="s1">csc_matrix</span><span class="s2">, </span><span class="s1">csr_matrix</span>

<span class="s2">from </span><span class="s1">sklearn </span><span class="s2">import </span><span class="s1">datasets</span><span class="s2">, </span><span class="s1">tree</span>
<span class="s2">from </span><span class="s1">sklearn.dummy </span><span class="s2">import </span><span class="s1">DummyRegressor</span>
<span class="s2">from </span><span class="s1">sklearn.exceptions </span><span class="s2">import </span><span class="s1">NotFittedError</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">accuracy_score</span><span class="s2">, </span><span class="s1">mean_poisson_deviance</span><span class="s2">, </span><span class="s1">mean_squared_error</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">train_test_split</span>
<span class="s2">from </span><span class="s1">sklearn.random_projection </span><span class="s2">import </span><span class="s1">_sparse_random_matrix</span>
<span class="s2">from </span><span class="s1">sklearn.tree </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">DecisionTreeClassifier</span><span class="s2">,</span>
    <span class="s1">DecisionTreeRegressor</span><span class="s2">,</span>
    <span class="s1">ExtraTreeClassifier</span><span class="s2">,</span>
    <span class="s1">ExtraTreeRegressor</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.tree._classes </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">CRITERIA_CLF</span><span class="s2">,</span>
    <span class="s1">CRITERIA_REG</span><span class="s2">,</span>
    <span class="s1">DENSE_SPLITTERS</span><span class="s2">,</span>
    <span class="s1">SPARSE_SPLITTERS</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.tree._tree </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">NODE_DTYPE</span><span class="s2">,</span>
    <span class="s1">TREE_LEAF</span><span class="s2">,</span>
    <span class="s1">TREE_UNDEFINED</span><span class="s2">,</span>
    <span class="s1">_check_n_classes</span><span class="s2">,</span>
    <span class="s1">_check_node_ndarray</span><span class="s2">,</span>
    <span class="s1">_check_value_ndarray</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.tree._tree </span><span class="s2">import </span><span class="s1">Tree </span><span class="s2">as </span><span class="s1">CythonTree</span>
<span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">_IS_32BIT</span><span class="s2">, </span><span class="s1">compute_sample_weight</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">assert_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_equal</span><span class="s2">,</span>
    <span class="s1">create_memmap_backed_data</span><span class="s2">,</span>
    <span class="s1">ignore_warnings</span><span class="s2">,</span>
    <span class="s1">skip_if_32bit</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.utils.estimator_checks </span><span class="s2">import </span><span class="s1">check_sample_weights_invariance</span>
<span class="s2">from </span><span class="s1">sklearn.utils.validation </span><span class="s2">import </span><span class="s1">check_random_state</span>

<span class="s1">CLF_CRITERIONS = (</span><span class="s3">&quot;gini&quot;</span><span class="s2">, </span><span class="s3">&quot;log_loss&quot;</span><span class="s1">)</span>
<span class="s1">REG_CRITERIONS = (</span><span class="s3">&quot;squared_error&quot;</span><span class="s2">, </span><span class="s3">&quot;absolute_error&quot;</span><span class="s2">, </span><span class="s3">&quot;friedman_mse&quot;</span><span class="s2">, </span><span class="s3">&quot;poisson&quot;</span><span class="s1">)</span>

<span class="s1">CLF_TREES = {</span>
    <span class="s3">&quot;DecisionTreeClassifier&quot;</span><span class="s1">: DecisionTreeClassifier</span><span class="s2">,</span>
    <span class="s3">&quot;ExtraTreeClassifier&quot;</span><span class="s1">: ExtraTreeClassifier</span><span class="s2">,</span>
<span class="s1">}</span>

<span class="s1">REG_TREES = {</span>
    <span class="s3">&quot;DecisionTreeRegressor&quot;</span><span class="s1">: DecisionTreeRegressor</span><span class="s2">,</span>
    <span class="s3">&quot;ExtraTreeRegressor&quot;</span><span class="s1">: ExtraTreeRegressor</span><span class="s2">,</span>
<span class="s1">}</span>

<span class="s1">ALL_TREES: dict = dict()</span>
<span class="s1">ALL_TREES.update(CLF_TREES)</span>
<span class="s1">ALL_TREES.update(REG_TREES)</span>

<span class="s1">SPARSE_TREES = [</span>
    <span class="s3">&quot;DecisionTreeClassifier&quot;</span><span class="s2">,</span>
    <span class="s3">&quot;DecisionTreeRegressor&quot;</span><span class="s2">,</span>
    <span class="s3">&quot;ExtraTreeClassifier&quot;</span><span class="s2">,</span>
    <span class="s3">&quot;ExtraTreeRegressor&quot;</span><span class="s2">,</span>
<span class="s1">]</span>


<span class="s1">X_small = np.array(</span>
    <span class="s1">[</span>
        <span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">14</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">4</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">4</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">5</span><span class="s2">, </span><span class="s4">0.2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">4.5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">2.1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">4.5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0.2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s1">-</span><span class="s4">3</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">3.2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s1">-</span><span class="s4">4</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2.11</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">6</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">11</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">3.2</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">3</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2.11</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">6</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">11</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">3.2</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2.11</span><span class="s2">, </span><span class="s4">8</span><span class="s2">, </span><span class="s1">-</span><span class="s4">6</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">11</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">3.2</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2.11</span><span class="s2">, </span><span class="s4">8</span><span class="s2">, </span><span class="s1">-</span><span class="s4">6</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">11</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">3.2</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">8</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s1">-</span><span class="s4">4</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">5</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">5</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">4</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">5</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2.11</span><span class="s2">, </span><span class="s4">8</span><span class="s2">, </span><span class="s1">-</span><span class="s4">6</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">3.2</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">3</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2.11</span><span class="s2">, </span><span class="s4">8</span><span class="s2">, </span><span class="s1">-</span><span class="s4">6</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">3.2</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">1.5</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2.11</span><span class="s2">, </span><span class="s4">8</span><span class="s2">, </span><span class="s1">-</span><span class="s4">6</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">3.2</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">5</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">3</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">5</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">4</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">]</span>
<span class="s1">)</span>

<span class="s1">y_small = [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>
<span class="s1">y_small_reg = [</span>
    <span class="s4">1.0</span><span class="s2">,</span>
    <span class="s4">2.1</span><span class="s2">,</span>
    <span class="s4">1.2</span><span class="s2">,</span>
    <span class="s4">0.05</span><span class="s2">,</span>
    <span class="s4">10</span><span class="s2">,</span>
    <span class="s4">2.4</span><span class="s2">,</span>
    <span class="s4">3.1</span><span class="s2">,</span>
    <span class="s4">1.01</span><span class="s2">,</span>
    <span class="s4">0.01</span><span class="s2">,</span>
    <span class="s4">2.98</span><span class="s2">,</span>
    <span class="s4">3.1</span><span class="s2">,</span>
    <span class="s4">1.1</span><span class="s2">,</span>
    <span class="s4">0.0</span><span class="s2">,</span>
    <span class="s4">1.2</span><span class="s2">,</span>
    <span class="s4">2</span><span class="s2">,</span>
    <span class="s4">11</span><span class="s2">,</span>
    <span class="s4">0</span><span class="s2">,</span>
    <span class="s4">0</span><span class="s2">,</span>
    <span class="s4">4.5</span><span class="s2">,</span>
    <span class="s4">0.201</span><span class="s2">,</span>
    <span class="s4">1.06</span><span class="s2">,</span>
    <span class="s4">0.9</span><span class="s2">,</span>
    <span class="s4">0</span><span class="s2">,</span>
<span class="s1">]</span>

<span class="s5"># toy sample</span>
<span class="s1">X = [[-</span><span class="s4">2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]]</span>
<span class="s1">y = [-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>
<span class="s1">T = [[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]]</span>
<span class="s1">true_result = [-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>

<span class="s5"># also load the iris dataset</span>
<span class="s5"># and randomly permute it</span>
<span class="s1">iris = datasets.load_iris()</span>
<span class="s1">rng = np.random.RandomState(</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">perm = rng.permutation(iris.target.size)</span>
<span class="s1">iris.data = iris.data[perm]</span>
<span class="s1">iris.target = iris.target[perm]</span>

<span class="s5"># also load the diabetes dataset</span>
<span class="s5"># and randomly permute it</span>
<span class="s1">diabetes = datasets.load_diabetes()</span>
<span class="s1">perm = rng.permutation(diabetes.target.size)</span>
<span class="s1">diabetes.data = diabetes.data[perm]</span>
<span class="s1">diabetes.target = diabetes.target[perm]</span>

<span class="s1">digits = datasets.load_digits()</span>
<span class="s1">perm = rng.permutation(digits.target.size)</span>
<span class="s1">digits.data = digits.data[perm]</span>
<span class="s1">digits.target = digits.target[perm]</span>

<span class="s1">random_state = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
<span class="s1">X_multilabel</span><span class="s2">, </span><span class="s1">y_multilabel = datasets.make_multilabel_classification(</span>
    <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">n_samples=</span><span class="s4">30</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">10</span>
<span class="s1">)</span>

<span class="s5"># NB: despite their names X_sparse_* are numpy arrays (and not sparse matrices)</span>
<span class="s1">X_sparse_pos = random_state.uniform(size=(</span><span class="s4">20</span><span class="s2">, </span><span class="s4">5</span><span class="s1">))</span>
<span class="s1">X_sparse_pos[X_sparse_pos &lt;= </span><span class="s4">0.8</span><span class="s1">] = </span><span class="s4">0.0</span>
<span class="s1">y_random = random_state.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s1">size=(</span><span class="s4">20</span><span class="s2">,</span><span class="s1">))</span>
<span class="s1">X_sparse_mix = _sparse_random_matrix(</span><span class="s4">20</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s1">density=</span><span class="s4">0.25</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).toarray()</span>


<span class="s1">DATASETS = {</span>
    <span class="s3">&quot;iris&quot;</span><span class="s1">: {</span><span class="s3">&quot;X&quot;</span><span class="s1">: iris.data</span><span class="s2">, </span><span class="s3">&quot;y&quot;</span><span class="s1">: iris.target}</span><span class="s2">,</span>
    <span class="s3">&quot;diabetes&quot;</span><span class="s1">: {</span><span class="s3">&quot;X&quot;</span><span class="s1">: diabetes.data</span><span class="s2">, </span><span class="s3">&quot;y&quot;</span><span class="s1">: diabetes.target}</span><span class="s2">,</span>
    <span class="s3">&quot;digits&quot;</span><span class="s1">: {</span><span class="s3">&quot;X&quot;</span><span class="s1">: digits.data</span><span class="s2">, </span><span class="s3">&quot;y&quot;</span><span class="s1">: digits.target}</span><span class="s2">,</span>
    <span class="s3">&quot;toy&quot;</span><span class="s1">: {</span><span class="s3">&quot;X&quot;</span><span class="s1">: X</span><span class="s2">, </span><span class="s3">&quot;y&quot;</span><span class="s1">: y}</span><span class="s2">,</span>
    <span class="s3">&quot;clf_small&quot;</span><span class="s1">: {</span><span class="s3">&quot;X&quot;</span><span class="s1">: X_small</span><span class="s2">, </span><span class="s3">&quot;y&quot;</span><span class="s1">: y_small}</span><span class="s2">,</span>
    <span class="s3">&quot;reg_small&quot;</span><span class="s1">: {</span><span class="s3">&quot;X&quot;</span><span class="s1">: X_small</span><span class="s2">, </span><span class="s3">&quot;y&quot;</span><span class="s1">: y_small_reg}</span><span class="s2">,</span>
    <span class="s3">&quot;multilabel&quot;</span><span class="s1">: {</span><span class="s3">&quot;X&quot;</span><span class="s1">: X_multilabel</span><span class="s2">, </span><span class="s3">&quot;y&quot;</span><span class="s1">: y_multilabel}</span><span class="s2">,</span>
    <span class="s3">&quot;sparse-pos&quot;</span><span class="s1">: {</span><span class="s3">&quot;X&quot;</span><span class="s1">: X_sparse_pos</span><span class="s2">, </span><span class="s3">&quot;y&quot;</span><span class="s1">: y_random}</span><span class="s2">,</span>
    <span class="s3">&quot;sparse-neg&quot;</span><span class="s1">: {</span><span class="s3">&quot;X&quot;</span><span class="s1">: -X_sparse_pos</span><span class="s2">, </span><span class="s3">&quot;y&quot;</span><span class="s1">: y_random}</span><span class="s2">,</span>
    <span class="s3">&quot;sparse-mix&quot;</span><span class="s1">: {</span><span class="s3">&quot;X&quot;</span><span class="s1">: X_sparse_mix</span><span class="s2">, </span><span class="s3">&quot;y&quot;</span><span class="s1">: y_random}</span><span class="s2">,</span>
    <span class="s3">&quot;zeros&quot;</span><span class="s1">: {</span><span class="s3">&quot;X&quot;</span><span class="s1">: np.zeros((</span><span class="s4">20</span><span class="s2">, </span><span class="s4">3</span><span class="s1">))</span><span class="s2">, </span><span class="s3">&quot;y&quot;</span><span class="s1">: y_random}</span><span class="s2">,</span>
<span class="s1">}</span>

<span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">DATASETS:</span>
    <span class="s1">DATASETS[name][</span><span class="s3">&quot;X_sparse&quot;</span><span class="s1">] = csc_matrix(DATASETS[name][</span><span class="s3">&quot;X&quot;</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">assert_tree_equal(d</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">message):</span>
    <span class="s2">assert </span><span class="s1">(</span>
        <span class="s1">s.node_count == d.node_count</span>
    <span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;{0}: inequal number of node ({1} != {2})&quot;</span><span class="s1">.format(</span>
        <span class="s1">message</span><span class="s2">, </span><span class="s1">s.node_count</span><span class="s2">, </span><span class="s1">d.node_count</span>
    <span class="s1">)</span>

    <span class="s1">assert_array_equal(</span>
        <span class="s1">d.children_right</span><span class="s2">, </span><span class="s1">s.children_right</span><span class="s2">, </span><span class="s1">message + </span><span class="s3">&quot;: inequal children_right&quot;</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">d.children_left</span><span class="s2">, </span><span class="s1">s.children_left</span><span class="s2">, </span><span class="s1">message + </span><span class="s3">&quot;: inequal children_left&quot;</span>
    <span class="s1">)</span>

    <span class="s1">external = d.children_right == TREE_LEAF</span>
    <span class="s1">internal = np.logical_not(external)</span>

    <span class="s1">assert_array_equal(</span>
        <span class="s1">d.feature[internal]</span><span class="s2">, </span><span class="s1">s.feature[internal]</span><span class="s2">, </span><span class="s1">message + </span><span class="s3">&quot;: inequal features&quot;</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">d.threshold[internal]</span><span class="s2">, </span><span class="s1">s.threshold[internal]</span><span class="s2">, </span><span class="s1">message + </span><span class="s3">&quot;: inequal threshold&quot;</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">d.n_node_samples.sum()</span><span class="s2">,</span>
        <span class="s1">s.n_node_samples.sum()</span><span class="s2">,</span>
        <span class="s1">message + </span><span class="s3">&quot;: inequal sum(n_node_samples)&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">d.n_node_samples</span><span class="s2">, </span><span class="s1">s.n_node_samples</span><span class="s2">, </span><span class="s1">message + </span><span class="s3">&quot;: inequal n_node_samples&quot;</span>
    <span class="s1">)</span>

    <span class="s1">assert_almost_equal(d.impurity</span><span class="s2">, </span><span class="s1">s.impurity</span><span class="s2">, </span><span class="s1">err_msg=message + </span><span class="s3">&quot;: inequal impurity&quot;</span><span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">d.value[external]</span><span class="s2">, </span><span class="s1">s.value[external]</span><span class="s2">, </span><span class="s1">err_msg=message + </span><span class="s3">&quot;: inequal value&quot;</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_classification_toy():</span>
    <span class="s5"># Check classification on a toy dataset.</span>
    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">Tree </span><span class="s2">in </span><span class="s1">CLF_TREES.items():</span>
        <span class="s1">clf = Tree(random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">assert_array_equal(clf.predict(T)</span><span class="s2">, </span><span class="s1">true_result</span><span class="s2">, </span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name))</span>

        <span class="s1">clf = Tree(max_features=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">assert_array_equal(clf.predict(T)</span><span class="s2">, </span><span class="s1">true_result</span><span class="s2">, </span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name))</span>


<span class="s2">def </span><span class="s1">test_weighted_classification_toy():</span>
    <span class="s5"># Check classification on a weighted toy dataset.</span>
    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">Tree </span><span class="s2">in </span><span class="s1">CLF_TREES.items():</span>
        <span class="s1">clf = Tree(random_state=</span><span class="s4">0</span><span class="s1">)</span>

        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=np.ones(len(X)))</span>
        <span class="s1">assert_array_equal(clf.predict(T)</span><span class="s2">, </span><span class="s1">true_result</span><span class="s2">, </span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name))</span>

        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=np.full(len(X)</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">))</span>
        <span class="s1">assert_array_equal(clf.predict(T)</span><span class="s2">, </span><span class="s1">true_result</span><span class="s2">, </span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;Tree&quot;</span><span class="s2">, </span><span class="s1">REG_TREES.values())</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;criterion&quot;</span><span class="s2">, </span><span class="s1">REG_CRITERIONS)</span>
<span class="s2">def </span><span class="s1">test_regression_toy(Tree</span><span class="s2">, </span><span class="s1">criterion):</span>
    <span class="s5"># Check regression on a toy dataset.</span>
    <span class="s2">if </span><span class="s1">criterion == </span><span class="s3">&quot;poisson&quot;</span><span class="s1">:</span>
        <span class="s5"># make target positive while not touching the original y and</span>
        <span class="s5"># true_result</span>
        <span class="s1">a = np.abs(np.min(y)) + </span><span class="s4">1</span>
        <span class="s1">y_train = np.array(y) + a</span>
        <span class="s1">y_test = np.array(true_result) + a</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">y_train = y</span>
        <span class="s1">y_test = true_result</span>

    <span class="s1">reg = Tree(criterion=criterion</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s1">assert_allclose(reg.predict(T)</span><span class="s2">, </span><span class="s1">y_test)</span>

    <span class="s1">clf = Tree(criterion=criterion</span><span class="s2">, </span><span class="s1">max_features=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s1">assert_allclose(reg.predict(T)</span><span class="s2">, </span><span class="s1">y_test)</span>


<span class="s2">def </span><span class="s1">test_xor():</span>
    <span class="s5"># Check on a XOR problem</span>
    <span class="s1">y = np.zeros((</span><span class="s4">10</span><span class="s2">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">y[:</span><span class="s4">5</span><span class="s2">, </span><span class="s1">:</span><span class="s4">5</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">y[</span><span class="s4">5</span><span class="s1">:</span><span class="s2">, </span><span class="s4">5</span><span class="s1">:] = </span><span class="s4">1</span>

    <span class="s1">gridx</span><span class="s2">, </span><span class="s1">gridy = np.indices(y.shape)</span>

    <span class="s1">X = np.vstack([gridx.ravel()</span><span class="s2">, </span><span class="s1">gridy.ravel()]).T</span>
    <span class="s1">y = y.ravel()</span>

    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">Tree </span><span class="s2">in </span><span class="s1">CLF_TREES.items():</span>
        <span class="s1">clf = Tree(random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">clf.score(X</span><span class="s2">, </span><span class="s1">y) == </span><span class="s4">1.0</span><span class="s2">, </span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span>

        <span class="s1">clf = Tree(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_features=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">clf.score(X</span><span class="s2">, </span><span class="s1">y) == </span><span class="s4">1.0</span><span class="s2">, </span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span>


<span class="s2">def </span><span class="s1">test_iris():</span>
    <span class="s5"># Check consistency on dataset iris.</span>
    <span class="s2">for </span><span class="s1">(name</span><span class="s2">, </span><span class="s1">Tree)</span><span class="s2">, </span><span class="s1">criterion </span><span class="s2">in </span><span class="s1">product(CLF_TREES.items()</span><span class="s2">, </span><span class="s1">CLF_CRITERIONS):</span>
        <span class="s1">clf = Tree(criterion=criterion</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">clf.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s1">score = accuracy_score(clf.predict(iris.data)</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s2">assert </span><span class="s1">score &gt; </span><span class="s4">0.9</span><span class="s2">, </span><span class="s3">&quot;Failed with {0}, criterion = {1} and score = {2}&quot;</span><span class="s1">.format(</span>
            <span class="s1">name</span><span class="s2">, </span><span class="s1">criterion</span><span class="s2">, </span><span class="s1">score</span>
        <span class="s1">)</span>

        <span class="s1">clf = Tree(criterion=criterion</span><span class="s2">, </span><span class="s1">max_features=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">clf.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s1">score = accuracy_score(clf.predict(iris.data)</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s2">assert </span><span class="s1">score &gt; </span><span class="s4">0.5</span><span class="s2">, </span><span class="s3">&quot;Failed with {0}, criterion = {1} and score = {2}&quot;</span><span class="s1">.format(</span>
            <span class="s1">name</span><span class="s2">, </span><span class="s1">criterion</span><span class="s2">, </span><span class="s1">score</span>
        <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name, Tree&quot;</span><span class="s2">, </span><span class="s1">REG_TREES.items())</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;criterion&quot;</span><span class="s2">, </span><span class="s1">REG_CRITERIONS)</span>
<span class="s2">def </span><span class="s1">test_diabetes_overfit(name</span><span class="s2">, </span><span class="s1">Tree</span><span class="s2">, </span><span class="s1">criterion):</span>
    <span class="s5"># check consistency of overfitted trees on the diabetes dataset</span>
    <span class="s5"># since the trees will overfit, we expect an MSE of 0</span>
    <span class="s1">reg = Tree(criterion=criterion</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">reg.fit(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>
    <span class="s1">score = mean_squared_error(diabetes.target</span><span class="s2">, </span><span class="s1">reg.predict(diabetes.data))</span>
    <span class="s2">assert </span><span class="s1">score == pytest.approx(</span>
        <span class="s4">0</span>
    <span class="s1">)</span><span class="s2">, </span><span class="s3">f&quot;Failed with </span><span class="s2">{</span><span class="s1">name</span><span class="s2">}</span><span class="s3">, criterion = </span><span class="s2">{</span><span class="s1">criterion</span><span class="s2">} </span><span class="s3">and score = </span><span class="s2">{</span><span class="s1">score</span><span class="s2">}</span><span class="s3">&quot;</span>


<span class="s1">@skip_if_32bit</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name, Tree&quot;</span><span class="s2">, </span><span class="s1">REG_TREES.items())</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;criterion, max_depth, metric, max_loss&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s3">&quot;squared_error&quot;</span><span class="s2">, </span><span class="s4">15</span><span class="s2">, </span><span class="s1">mean_squared_error</span><span class="s2">, </span><span class="s4">60</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s3">&quot;absolute_error&quot;</span><span class="s2">, </span><span class="s4">20</span><span class="s2">, </span><span class="s1">mean_squared_error</span><span class="s2">, </span><span class="s4">60</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s3">&quot;friedman_mse&quot;</span><span class="s2">, </span><span class="s4">15</span><span class="s2">, </span><span class="s1">mean_squared_error</span><span class="s2">, </span><span class="s4">60</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s3">&quot;poisson&quot;</span><span class="s2">, </span><span class="s4">15</span><span class="s2">, </span><span class="s1">mean_poisson_deviance</span><span class="s2">, </span><span class="s4">30</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_diabetes_underfit(name</span><span class="s2">, </span><span class="s1">Tree</span><span class="s2">, </span><span class="s1">criterion</span><span class="s2">, </span><span class="s1">max_depth</span><span class="s2">, </span><span class="s1">metric</span><span class="s2">, </span><span class="s1">max_loss):</span>
    <span class="s5"># check consistency of trees when the depth and the number of features are</span>
    <span class="s5"># limited</span>

    <span class="s1">reg = Tree(criterion=criterion</span><span class="s2">, </span><span class="s1">max_depth=max_depth</span><span class="s2">, </span><span class="s1">max_features=</span><span class="s4">6</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">reg.fit(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>
    <span class="s1">loss = metric(diabetes.target</span><span class="s2">, </span><span class="s1">reg.predict(diabetes.data))</span>
    <span class="s2">assert </span><span class="s4">0 </span><span class="s1">&lt; loss &lt; max_loss</span>


<span class="s2">def </span><span class="s1">test_probability():</span>
    <span class="s5"># Predict probabilities using DecisionTreeClassifier.</span>

    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">Tree </span><span class="s2">in </span><span class="s1">CLF_TREES.items():</span>
        <span class="s1">clf = Tree(max_depth=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">max_features=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
        <span class="s1">clf.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>

        <span class="s1">prob_predict = clf.predict_proba(iris.data)</span>
        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">np.sum(prob_predict</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">np.ones(iris.data.shape[</span><span class="s4">0</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">err_msg=</span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">assert_array_equal(</span>
            <span class="s1">np.argmax(prob_predict</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">clf.predict(iris.data)</span><span class="s2">,</span>
            <span class="s1">err_msg=</span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">assert_almost_equal(</span>
            <span class="s1">clf.predict_proba(iris.data)</span><span class="s2">,</span>
            <span class="s1">np.exp(clf.predict_log_proba(iris.data))</span><span class="s2">,</span>
            <span class="s4">8</span><span class="s2">,</span>
            <span class="s1">err_msg=</span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span><span class="s2">,</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_arrayrepr():</span>
    <span class="s5"># Check the array representation.</span>
    <span class="s5"># Check resize</span>
    <span class="s1">X = np.arange(</span><span class="s4">10000</span><span class="s1">)[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">y = np.arange(</span><span class="s4">10000</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">Tree </span><span class="s2">in </span><span class="s1">REG_TREES.items():</span>
        <span class="s1">reg = Tree(max_depth=</span><span class="s2">None, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_pure_set():</span>
    <span class="s5"># Check when y is pure.</span>
    <span class="s1">X = [[-</span><span class="s4">2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>

    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeClassifier </span><span class="s2">in </span><span class="s1">CLF_TREES.items():</span>
        <span class="s1">clf = TreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">assert_array_equal(clf.predict(X)</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">err_msg=</span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name))</span>

    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeRegressor </span><span class="s2">in </span><span class="s1">REG_TREES.items():</span>
        <span class="s1">reg = TreeRegressor(random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">assert_almost_equal(reg.predict(X)</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">err_msg=</span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name))</span>


<span class="s2">def </span><span class="s1">test_numerical_stability():</span>
    <span class="s5"># Check numerical stability.</span>
    <span class="s1">X = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">152.08097839</span><span class="s2">, </span><span class="s4">140.40744019</span><span class="s2">, </span><span class="s4">129.75102234</span><span class="s2">, </span><span class="s4">159.90493774</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">142.50700378</span><span class="s2">, </span><span class="s4">135.81935120</span><span class="s2">, </span><span class="s4">117.82884979</span><span class="s2">, </span><span class="s4">162.75781250</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">127.28772736</span><span class="s2">, </span><span class="s4">140.40744019</span><span class="s2">, </span><span class="s4">129.75102234</span><span class="s2">, </span><span class="s4">159.90493774</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">132.37025452</span><span class="s2">, </span><span class="s4">143.71923828</span><span class="s2">, </span><span class="s4">138.35694885</span><span class="s2">, </span><span class="s4">157.84558105</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">103.10237122</span><span class="s2">, </span><span class="s4">143.71928406</span><span class="s2">, </span><span class="s4">138.35696411</span><span class="s2">, </span><span class="s4">157.84559631</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">127.71276855</span><span class="s2">, </span><span class="s4">143.71923828</span><span class="s2">, </span><span class="s4">138.35694885</span><span class="s2">, </span><span class="s4">157.84558105</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">120.91514587</span><span class="s2">, </span><span class="s4">140.40744019</span><span class="s2">, </span><span class="s4">129.75102234</span><span class="s2">, </span><span class="s4">159.90493774</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">y = np.array([</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.70209277</span><span class="s2">, </span><span class="s4">0.53896582</span><span class="s2">, </span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">0.90914464</span><span class="s2">, </span><span class="s4">0.48026916</span><span class="s2">, </span><span class="s4">0.49622521</span><span class="s1">])</span>

    <span class="s2">with </span><span class="s1">np.errstate(all=</span><span class="s3">&quot;raise&quot;</span><span class="s1">):</span>
        <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">Tree </span><span class="s2">in </span><span class="s1">REG_TREES.items():</span>
            <span class="s1">reg = Tree(random_state=</span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
            <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">-y)</span>
            <span class="s1">reg.fit(-X</span><span class="s2">, </span><span class="s1">y)</span>
            <span class="s1">reg.fit(-X</span><span class="s2">, </span><span class="s1">-y)</span>


<span class="s2">def </span><span class="s1">test_importances():</span>
    <span class="s5"># Check variable importances.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">5000</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">10</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">n_redundant=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">n_repeated=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">Tree </span><span class="s2">in </span><span class="s1">CLF_TREES.items():</span>
        <span class="s1">clf = Tree(random_state=</span><span class="s4">0</span><span class="s1">)</span>

        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">importances = clf.feature_importances_</span>
        <span class="s1">n_important = np.sum(importances &gt; </span><span class="s4">0.1</span><span class="s1">)</span>

        <span class="s2">assert </span><span class="s1">importances.shape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">10</span><span class="s2">, </span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span>
        <span class="s2">assert </span><span class="s1">n_important == </span><span class="s4">3</span><span class="s2">, </span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span>

    <span class="s5"># Check on iris that importances are the same for all builders</span>
    <span class="s1">clf = DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">clf2 = DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_leaf_nodes=len(iris.data))</span>
    <span class="s1">clf2.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>

    <span class="s1">assert_array_equal(clf.feature_importances_</span><span class="s2">, </span><span class="s1">clf2.feature_importances_)</span>


<span class="s2">def </span><span class="s1">test_importances_raises():</span>
    <span class="s5"># Check if variable importance before fit raises ValueError.</span>
    <span class="s1">clf = DecisionTreeClassifier()</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">getattr(clf</span><span class="s2">, </span><span class="s3">&quot;feature_importances_&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_importances_gini_equal_squared_error():</span>
    <span class="s5"># Check that gini is equivalent to squared_error for binary output variable</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">2000</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">10</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">n_redundant=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">n_repeated=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s5"># The gini index and the mean square error (variance) might differ due</span>
    <span class="s5"># to numerical instability. Since those instabilities mainly occurs at</span>
    <span class="s5"># high tree depth, we restrict this maximal depth.</span>
    <span class="s1">clf = DecisionTreeClassifier(criterion=</span><span class="s3">&quot;gini&quot;</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span>
    <span class="s1">)</span>
    <span class="s1">reg = DecisionTreeRegressor(</span>
        <span class="s1">criterion=</span><span class="s3">&quot;squared_error&quot;</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">assert_almost_equal(clf.feature_importances_</span><span class="s2">, </span><span class="s1">reg.feature_importances_)</span>
    <span class="s1">assert_array_equal(clf.tree_.feature</span><span class="s2">, </span><span class="s1">reg.tree_.feature)</span>
    <span class="s1">assert_array_equal(clf.tree_.children_left</span><span class="s2">, </span><span class="s1">reg.tree_.children_left)</span>
    <span class="s1">assert_array_equal(clf.tree_.children_right</span><span class="s2">, </span><span class="s1">reg.tree_.children_right)</span>
    <span class="s1">assert_array_equal(clf.tree_.n_node_samples</span><span class="s2">, </span><span class="s1">reg.tree_.n_node_samples)</span>


<span class="s2">def </span><span class="s1">test_max_features():</span>
    <span class="s5"># Check max_features.</span>
    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeEstimator </span><span class="s2">in </span><span class="s1">ALL_TREES.items():</span>
        <span class="s1">est = TreeEstimator(max_features=</span><span class="s3">&quot;sqrt&quot;</span><span class="s1">)</span>
        <span class="s1">est.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s2">assert </span><span class="s1">est.max_features_ == int(np.sqrt(iris.data.shape[</span><span class="s4">1</span><span class="s1">]))</span>

        <span class="s1">est = TreeEstimator(max_features=</span><span class="s3">&quot;log2&quot;</span><span class="s1">)</span>
        <span class="s1">est.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s2">assert </span><span class="s1">est.max_features_ == int(np.log2(iris.data.shape[</span><span class="s4">1</span><span class="s1">]))</span>

        <span class="s1">est = TreeEstimator(max_features=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">est.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s2">assert </span><span class="s1">est.max_features_ == </span><span class="s4">1</span>

        <span class="s1">est = TreeEstimator(max_features=</span><span class="s4">3</span><span class="s1">)</span>
        <span class="s1">est.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s2">assert </span><span class="s1">est.max_features_ == </span><span class="s4">3</span>

        <span class="s1">est = TreeEstimator(max_features=</span><span class="s4">0.01</span><span class="s1">)</span>
        <span class="s1">est.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s2">assert </span><span class="s1">est.max_features_ == </span><span class="s4">1</span>

        <span class="s1">est = TreeEstimator(max_features=</span><span class="s4">0.5</span><span class="s1">)</span>
        <span class="s1">est.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s2">assert </span><span class="s1">est.max_features_ == int(</span><span class="s4">0.5 </span><span class="s1">* iris.data.shape[</span><span class="s4">1</span><span class="s1">])</span>

        <span class="s1">est = TreeEstimator(max_features=</span><span class="s4">1.0</span><span class="s1">)</span>
        <span class="s1">est.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s2">assert </span><span class="s1">est.max_features_ == iris.data.shape[</span><span class="s4">1</span><span class="s1">]</span>

        <span class="s1">est = TreeEstimator(max_features=</span><span class="s2">None</span><span class="s1">)</span>
        <span class="s1">est.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s2">assert </span><span class="s1">est.max_features_ == iris.data.shape[</span><span class="s4">1</span><span class="s1">]</span>


<span class="s2">def </span><span class="s1">test_error():</span>
    <span class="s5"># Test that it gives proper exception on deficient input.</span>
    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeEstimator </span><span class="s2">in </span><span class="s1">CLF_TREES.items():</span>
        <span class="s5"># predict before fit</span>
        <span class="s1">est = TreeEstimator()</span>
        <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError):</span>
            <span class="s1">est.predict_proba(X)</span>

        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">X2 = [[-</span><span class="s4">2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]]  </span><span class="s5"># wrong feature shape for sample</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
            <span class="s1">est.predict_proba(X2)</span>

        <span class="s5"># Wrong dimensions</span>
        <span class="s1">est = TreeEstimator()</span>
        <span class="s1">y2 = y[:-</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
            <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y2)</span>

        <span class="s5"># Test with arrays that are non-contiguous.</span>
        <span class="s1">Xf = np.asfortranarray(X)</span>
        <span class="s1">est = TreeEstimator()</span>
        <span class="s1">est.fit(Xf</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">assert_almost_equal(est.predict(T)</span><span class="s2">, </span><span class="s1">true_result)</span>

        <span class="s5"># predict before fitting</span>
        <span class="s1">est = TreeEstimator()</span>
        <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError):</span>
            <span class="s1">est.predict(T)</span>

        <span class="s5"># predict on vector with different dims</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">t = np.asarray(T)</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
            <span class="s1">est.predict(t[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">:])</span>

        <span class="s5"># wrong sample shape</span>
        <span class="s1">Xt = np.array(X).T</span>

        <span class="s1">est = TreeEstimator()</span>
        <span class="s1">est.fit(np.dot(X</span><span class="s2">, </span><span class="s1">Xt)</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
            <span class="s1">est.predict(X)</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
            <span class="s1">est.apply(X)</span>

        <span class="s1">clf = TreeEstimator()</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
            <span class="s1">clf.predict(Xt)</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
            <span class="s1">clf.apply(Xt)</span>

        <span class="s5"># apply before fitting</span>
        <span class="s1">est = TreeEstimator()</span>
        <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError):</span>
            <span class="s1">est.apply(T)</span>

    <span class="s5"># non positive target for Poisson splitting Criterion</span>
    <span class="s1">est = DecisionTreeRegressor(criterion=</span><span class="s3">&quot;poisson&quot;</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;y is not positive.*Poisson&quot;</span><span class="s1">):</span>
        <span class="s1">est.fit([[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;Some.*y are negative.*Poisson&quot;</span><span class="s1">):</span>
        <span class="s1">est.fit([[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_min_samples_split():</span>
    <span class="s0">&quot;&quot;&quot;Test min_samples_split parameter&quot;&quot;&quot;</span>
    <span class="s1">X = np.asfortranarray(iris.data</span><span class="s2">, </span><span class="s1">dtype=tree._tree.DTYPE)</span>
    <span class="s1">y = iris.target</span>

    <span class="s5"># test both DepthFirstTreeBuilder and BestFirstTreeBuilder</span>
    <span class="s5"># by setting max_leaf_nodes</span>
    <span class="s2">for </span><span class="s1">max_leaf_nodes</span><span class="s2">, </span><span class="s1">name </span><span class="s2">in </span><span class="s1">product((</span><span class="s2">None, </span><span class="s4">1000</span><span class="s1">)</span><span class="s2">, </span><span class="s1">ALL_TREES.keys()):</span>
        <span class="s1">TreeEstimator = ALL_TREES[name]</span>

        <span class="s5"># test for integer parameter</span>
        <span class="s1">est = TreeEstimator(</span>
            <span class="s1">min_samples_split=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">max_leaf_nodes=max_leaf_nodes</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
        <span class="s1">)</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s5"># count samples on nodes, -1 means it is a leaf</span>
        <span class="s1">node_samples = est.tree_.n_node_samples[est.tree_.children_left != -</span><span class="s4">1</span><span class="s1">]</span>

        <span class="s2">assert </span><span class="s1">np.min(node_samples) &gt; </span><span class="s4">9</span><span class="s2">, </span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span>

        <span class="s5"># test for float parameter</span>
        <span class="s1">est = TreeEstimator(</span>
            <span class="s1">min_samples_split=</span><span class="s4">0.2</span><span class="s2">, </span><span class="s1">max_leaf_nodes=max_leaf_nodes</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
        <span class="s1">)</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s5"># count samples on nodes, -1 means it is a leaf</span>
        <span class="s1">node_samples = est.tree_.n_node_samples[est.tree_.children_left != -</span><span class="s4">1</span><span class="s1">]</span>

        <span class="s2">assert </span><span class="s1">np.min(node_samples) &gt; </span><span class="s4">9</span><span class="s2">, </span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span>


<span class="s2">def </span><span class="s1">test_min_samples_leaf():</span>
    <span class="s5"># Test if leaves contain more than leaf_count training examples</span>
    <span class="s1">X = np.asfortranarray(iris.data</span><span class="s2">, </span><span class="s1">dtype=tree._tree.DTYPE)</span>
    <span class="s1">y = iris.target</span>

    <span class="s5"># test both DepthFirstTreeBuilder and BestFirstTreeBuilder</span>
    <span class="s5"># by setting max_leaf_nodes</span>
    <span class="s2">for </span><span class="s1">max_leaf_nodes</span><span class="s2">, </span><span class="s1">name </span><span class="s2">in </span><span class="s1">product((</span><span class="s2">None, </span><span class="s4">1000</span><span class="s1">)</span><span class="s2">, </span><span class="s1">ALL_TREES.keys()):</span>
        <span class="s1">TreeEstimator = ALL_TREES[name]</span>

        <span class="s5"># test integer parameter</span>
        <span class="s1">est = TreeEstimator(</span>
            <span class="s1">min_samples_leaf=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">max_leaf_nodes=max_leaf_nodes</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
        <span class="s1">)</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">out = est.tree_.apply(X)</span>
        <span class="s1">node_counts = np.bincount(out)</span>
        <span class="s5"># drop inner nodes</span>
        <span class="s1">leaf_count = node_counts[node_counts != </span><span class="s4">0</span><span class="s1">]</span>
        <span class="s2">assert </span><span class="s1">np.min(leaf_count) &gt; </span><span class="s4">4</span><span class="s2">, </span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span>

        <span class="s5"># test float parameter</span>
        <span class="s1">est = TreeEstimator(</span>
            <span class="s1">min_samples_leaf=</span><span class="s4">0.1</span><span class="s2">, </span><span class="s1">max_leaf_nodes=max_leaf_nodes</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
        <span class="s1">)</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">out = est.tree_.apply(X)</span>
        <span class="s1">node_counts = np.bincount(out)</span>
        <span class="s5"># drop inner nodes</span>
        <span class="s1">leaf_count = node_counts[node_counts != </span><span class="s4">0</span><span class="s1">]</span>
        <span class="s2">assert </span><span class="s1">np.min(leaf_count) &gt; </span><span class="s4">4</span><span class="s2">, </span><span class="s3">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span>


<span class="s2">def </span><span class="s1">check_min_weight_fraction_leaf(name</span><span class="s2">, </span><span class="s1">datasets</span><span class="s2">, </span><span class="s1">sparse=</span><span class="s2">False</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Test if leaves contain at least min_weight_fraction_leaf of the 
    training set&quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">sparse:</span>
        <span class="s1">X = DATASETS[datasets][</span><span class="s3">&quot;X_sparse&quot;</span><span class="s1">].astype(np.float32)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">X = DATASETS[datasets][</span><span class="s3">&quot;X&quot;</span><span class="s1">].astype(np.float32)</span>
    <span class="s1">y = DATASETS[datasets][</span><span class="s3">&quot;y&quot;</span><span class="s1">]</span>

    <span class="s1">weights = rng.rand(X.shape[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">total_weight = np.sum(weights)</span>

    <span class="s1">TreeEstimator = ALL_TREES[name]</span>

    <span class="s5"># test both DepthFirstTreeBuilder and BestFirstTreeBuilder</span>
    <span class="s5"># by setting max_leaf_nodes</span>
    <span class="s2">for </span><span class="s1">max_leaf_nodes</span><span class="s2">, </span><span class="s1">frac </span><span class="s2">in </span><span class="s1">product((</span><span class="s2">None, </span><span class="s4">1000</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.linspace(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">6</span><span class="s1">)):</span>
        <span class="s1">est = TreeEstimator(</span>
            <span class="s1">min_weight_fraction_leaf=frac</span><span class="s2">, </span><span class="s1">max_leaf_nodes=max_leaf_nodes</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
        <span class="s1">)</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=weights)</span>

        <span class="s2">if </span><span class="s1">sparse:</span>
            <span class="s1">out = est.tree_.apply(X.tocsr())</span>

        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">out = est.tree_.apply(X)</span>

        <span class="s1">node_weights = np.bincount(out</span><span class="s2">, </span><span class="s1">weights=weights)</span>
        <span class="s5"># drop inner nodes</span>
        <span class="s1">leaf_weights = node_weights[node_weights != </span><span class="s4">0</span><span class="s1">]</span>
        <span class="s2">assert </span><span class="s1">(</span>
            <span class="s1">np.min(leaf_weights) &gt;= total_weight * est.min_weight_fraction_leaf</span>
        <span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;Failed with {0} min_weight_fraction_leaf={1}&quot;</span><span class="s1">.format(</span>
            <span class="s1">name</span><span class="s2">, </span><span class="s1">est.min_weight_fraction_leaf</span>
        <span class="s1">)</span>

    <span class="s5"># test case with no weights passed in</span>
    <span class="s1">total_weight = X.shape[</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s2">for </span><span class="s1">max_leaf_nodes</span><span class="s2">, </span><span class="s1">frac </span><span class="s2">in </span><span class="s1">product((</span><span class="s2">None, </span><span class="s4">1000</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.linspace(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">6</span><span class="s1">)):</span>
        <span class="s1">est = TreeEstimator(</span>
            <span class="s1">min_weight_fraction_leaf=frac</span><span class="s2">, </span><span class="s1">max_leaf_nodes=max_leaf_nodes</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
        <span class="s1">)</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s2">if </span><span class="s1">sparse:</span>
            <span class="s1">out = est.tree_.apply(X.tocsr())</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">out = est.tree_.apply(X)</span>

        <span class="s1">node_weights = np.bincount(out)</span>
        <span class="s5"># drop inner nodes</span>
        <span class="s1">leaf_weights = node_weights[node_weights != </span><span class="s4">0</span><span class="s1">]</span>
        <span class="s2">assert </span><span class="s1">(</span>
            <span class="s1">np.min(leaf_weights) &gt;= total_weight * est.min_weight_fraction_leaf</span>
        <span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;Failed with {0} min_weight_fraction_leaf={1}&quot;</span><span class="s1">.format(</span>
            <span class="s1">name</span><span class="s2">, </span><span class="s1">est.min_weight_fraction_leaf</span>
        <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name&quot;</span><span class="s2">, </span><span class="s1">ALL_TREES)</span>
<span class="s2">def </span><span class="s1">test_min_weight_fraction_leaf_on_dense_input(name):</span>
    <span class="s1">check_min_weight_fraction_leaf(name</span><span class="s2">, </span><span class="s3">&quot;iris&quot;</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name&quot;</span><span class="s2">, </span><span class="s1">SPARSE_TREES)</span>
<span class="s2">def </span><span class="s1">test_min_weight_fraction_leaf_on_sparse_input(name):</span>
    <span class="s1">check_min_weight_fraction_leaf(name</span><span class="s2">, </span><span class="s3">&quot;multilabel&quot;</span><span class="s2">, True</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">check_min_weight_fraction_leaf_with_min_samples_leaf(name</span><span class="s2">, </span><span class="s1">datasets</span><span class="s2">, </span><span class="s1">sparse=</span><span class="s2">False</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Test the interaction between min_weight_fraction_leaf and 
    min_samples_leaf when sample_weights is not provided in fit.&quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">sparse:</span>
        <span class="s1">X = DATASETS[datasets][</span><span class="s3">&quot;X_sparse&quot;</span><span class="s1">].astype(np.float32)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">X = DATASETS[datasets][</span><span class="s3">&quot;X&quot;</span><span class="s1">].astype(np.float32)</span>
    <span class="s1">y = DATASETS[datasets][</span><span class="s3">&quot;y&quot;</span><span class="s1">]</span>

    <span class="s1">total_weight = X.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">TreeEstimator = ALL_TREES[name]</span>
    <span class="s2">for </span><span class="s1">max_leaf_nodes</span><span class="s2">, </span><span class="s1">frac </span><span class="s2">in </span><span class="s1">product((</span><span class="s2">None, </span><span class="s4">1000</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.linspace(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">3</span><span class="s1">)):</span>
        <span class="s5"># test integer min_samples_leaf</span>
        <span class="s1">est = TreeEstimator(</span>
            <span class="s1">min_weight_fraction_leaf=frac</span><span class="s2">,</span>
            <span class="s1">max_leaf_nodes=max_leaf_nodes</span><span class="s2">,</span>
            <span class="s1">min_samples_leaf=</span><span class="s4">5</span><span class="s2">,</span>
            <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s2">if </span><span class="s1">sparse:</span>
            <span class="s1">out = est.tree_.apply(X.tocsr())</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">out = est.tree_.apply(X)</span>

        <span class="s1">node_weights = np.bincount(out)</span>
        <span class="s5"># drop inner nodes</span>
        <span class="s1">leaf_weights = node_weights[node_weights != </span><span class="s4">0</span><span class="s1">]</span>
        <span class="s2">assert </span><span class="s1">np.min(leaf_weights) &gt;= max(</span>
            <span class="s1">(total_weight * est.min_weight_fraction_leaf)</span><span class="s2">, </span><span class="s4">5</span>
        <span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;Failed with {0} min_weight_fraction_leaf={1}, min_samples_leaf={2}&quot;</span><span class="s1">.format(</span>
            <span class="s1">name</span><span class="s2">, </span><span class="s1">est.min_weight_fraction_leaf</span><span class="s2">, </span><span class="s1">est.min_samples_leaf</span>
        <span class="s1">)</span>
    <span class="s2">for </span><span class="s1">max_leaf_nodes</span><span class="s2">, </span><span class="s1">frac </span><span class="s2">in </span><span class="s1">product((</span><span class="s2">None, </span><span class="s4">1000</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.linspace(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">3</span><span class="s1">)):</span>
        <span class="s5"># test float min_samples_leaf</span>
        <span class="s1">est = TreeEstimator(</span>
            <span class="s1">min_weight_fraction_leaf=frac</span><span class="s2">,</span>
            <span class="s1">max_leaf_nodes=max_leaf_nodes</span><span class="s2">,</span>
            <span class="s1">min_samples_leaf=</span><span class="s4">0.1</span><span class="s2">,</span>
            <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s2">if </span><span class="s1">sparse:</span>
            <span class="s1">out = est.tree_.apply(X.tocsr())</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">out = est.tree_.apply(X)</span>

        <span class="s1">node_weights = np.bincount(out)</span>
        <span class="s5"># drop inner nodes</span>
        <span class="s1">leaf_weights = node_weights[node_weights != </span><span class="s4">0</span><span class="s1">]</span>
        <span class="s2">assert </span><span class="s1">np.min(leaf_weights) &gt;= max(</span>
            <span class="s1">(total_weight * est.min_weight_fraction_leaf)</span><span class="s2">,</span>
            <span class="s1">(total_weight * est.min_samples_leaf)</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;Failed with {0} min_weight_fraction_leaf={1}, min_samples_leaf={2}&quot;</span><span class="s1">.format(</span>
            <span class="s1">name</span><span class="s2">, </span><span class="s1">est.min_weight_fraction_leaf</span><span class="s2">, </span><span class="s1">est.min_samples_leaf</span>
        <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name&quot;</span><span class="s2">, </span><span class="s1">ALL_TREES)</span>
<span class="s2">def </span><span class="s1">test_min_weight_fraction_leaf_with_min_samples_leaf_on_dense_input(name):</span>
    <span class="s1">check_min_weight_fraction_leaf_with_min_samples_leaf(name</span><span class="s2">, </span><span class="s3">&quot;iris&quot;</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name&quot;</span><span class="s2">, </span><span class="s1">SPARSE_TREES)</span>
<span class="s2">def </span><span class="s1">test_min_weight_fraction_leaf_with_min_samples_leaf_on_sparse_input(name):</span>
    <span class="s1">check_min_weight_fraction_leaf_with_min_samples_leaf(name</span><span class="s2">, </span><span class="s3">&quot;multilabel&quot;</span><span class="s2">, True</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_min_impurity_decrease(global_random_seed):</span>
    <span class="s5"># test if min_impurity_decrease ensure that a split is made only if</span>
    <span class="s5"># if the impurity decrease is at least that value</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_classification(n_samples=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>

    <span class="s5"># test both DepthFirstTreeBuilder and BestFirstTreeBuilder</span>
    <span class="s5"># by setting max_leaf_nodes</span>
    <span class="s2">for </span><span class="s1">max_leaf_nodes</span><span class="s2">, </span><span class="s1">name </span><span class="s2">in </span><span class="s1">product((</span><span class="s2">None, </span><span class="s4">1000</span><span class="s1">)</span><span class="s2">, </span><span class="s1">ALL_TREES.keys()):</span>
        <span class="s1">TreeEstimator = ALL_TREES[name]</span>

        <span class="s5"># Check default value of min_impurity_decrease, 1e-7</span>
        <span class="s1">est1 = TreeEstimator(max_leaf_nodes=max_leaf_nodes</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s5"># Check with explicit value of 0.05</span>
        <span class="s1">est2 = TreeEstimator(</span>
            <span class="s1">max_leaf_nodes=max_leaf_nodes</span><span class="s2">, </span><span class="s1">min_impurity_decrease=</span><span class="s4">0.05</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
        <span class="s1">)</span>
        <span class="s5"># Check with a much lower value of 0.0001</span>
        <span class="s1">est3 = TreeEstimator(</span>
            <span class="s1">max_leaf_nodes=max_leaf_nodes</span><span class="s2">, </span><span class="s1">min_impurity_decrease=</span><span class="s4">0.0001</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
        <span class="s1">)</span>
        <span class="s5"># Check with a much lower value of 0.1</span>
        <span class="s1">est4 = TreeEstimator(</span>
            <span class="s1">max_leaf_nodes=max_leaf_nodes</span><span class="s2">, </span><span class="s1">min_impurity_decrease=</span><span class="s4">0.1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
        <span class="s1">)</span>

        <span class="s2">for </span><span class="s1">est</span><span class="s2">, </span><span class="s1">expected_decrease </span><span class="s2">in </span><span class="s1">(</span>
            <span class="s1">(est1</span><span class="s2">, </span><span class="s4">1e-7</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">(est2</span><span class="s2">, </span><span class="s4">0.05</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">(est3</span><span class="s2">, </span><span class="s4">0.0001</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">(est4</span><span class="s2">, </span><span class="s4">0.1</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">):</span>
            <span class="s2">assert </span><span class="s1">(</span>
                <span class="s1">est.min_impurity_decrease &lt;= expected_decrease</span>
            <span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;Failed, min_impurity_decrease = {0} &gt; {1}&quot;</span><span class="s1">.format(</span>
                <span class="s1">est.min_impurity_decrease</span><span class="s2">, </span><span class="s1">expected_decrease</span>
            <span class="s1">)</span>
            <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
            <span class="s2">for </span><span class="s1">node </span><span class="s2">in </span><span class="s1">range(est.tree_.node_count):</span>
                <span class="s5"># If current node is a not leaf node, check if the split was</span>
                <span class="s5"># justified w.r.t the min_impurity_decrease</span>
                <span class="s2">if </span><span class="s1">est.tree_.children_left[node] != TREE_LEAF:</span>
                    <span class="s1">imp_parent = est.tree_.impurity[node]</span>
                    <span class="s1">wtd_n_node = est.tree_.weighted_n_node_samples[node]</span>

                    <span class="s1">left = est.tree_.children_left[node]</span>
                    <span class="s1">wtd_n_left = est.tree_.weighted_n_node_samples[left]</span>
                    <span class="s1">imp_left = est.tree_.impurity[left]</span>
                    <span class="s1">wtd_imp_left = wtd_n_left * imp_left</span>

                    <span class="s1">right = est.tree_.children_right[node]</span>
                    <span class="s1">wtd_n_right = est.tree_.weighted_n_node_samples[right]</span>
                    <span class="s1">imp_right = est.tree_.impurity[right]</span>
                    <span class="s1">wtd_imp_right = wtd_n_right * imp_right</span>

                    <span class="s1">wtd_avg_left_right_imp = wtd_imp_right + wtd_imp_left</span>
                    <span class="s1">wtd_avg_left_right_imp /= wtd_n_node</span>

                    <span class="s1">fractional_node_weight = (</span>
                        <span class="s1">est.tree_.weighted_n_node_samples[node] / X.shape[</span><span class="s4">0</span><span class="s1">]</span>
                    <span class="s1">)</span>

                    <span class="s1">actual_decrease = fractional_node_weight * (</span>
                        <span class="s1">imp_parent - wtd_avg_left_right_imp</span>
                    <span class="s1">)</span>

                    <span class="s2">assert </span><span class="s1">(</span>
                        <span class="s1">actual_decrease &gt;= expected_decrease</span>
                    <span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;Failed with {0} expected min_impurity_decrease={1}&quot;</span><span class="s1">.format(</span>
                        <span class="s1">actual_decrease</span><span class="s2">, </span><span class="s1">expected_decrease</span>
                    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_pickle():</span>
    <span class="s0">&quot;&quot;&quot;Test pickling preserves Tree properties and performance.&quot;&quot;&quot;</span>
    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeEstimator </span><span class="s2">in </span><span class="s1">ALL_TREES.items():</span>
        <span class="s2">if </span><span class="s3">&quot;Classifier&quot; </span><span class="s2">in </span><span class="s1">name:</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y = iris.data</span><span class="s2">, </span><span class="s1">iris.target</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y = diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target</span>

        <span class="s1">est = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">score = est.score(X</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s5"># test that all class properties are maintained</span>
        <span class="s1">attributes = [</span>
            <span class="s3">&quot;max_depth&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;node_count&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;capacity&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;n_classes&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;children_left&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;children_right&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;n_leaves&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;feature&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;threshold&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;impurity&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;n_node_samples&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;weighted_n_node_samples&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;value&quot;</span><span class="s2">,</span>
        <span class="s1">]</span>
        <span class="s1">fitted_attribute = {</span>
            <span class="s1">attribute: getattr(est.tree_</span><span class="s2">, </span><span class="s1">attribute) </span><span class="s2">for </span><span class="s1">attribute </span><span class="s2">in </span><span class="s1">attributes</span>
        <span class="s1">}</span>

        <span class="s1">serialized_object = pickle.dumps(est)</span>
        <span class="s1">est2 = pickle.loads(serialized_object)</span>
        <span class="s2">assert </span><span class="s1">type(est2) == est.__class__</span>

        <span class="s1">score2 = est2.score(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">(</span>
            <span class="s1">score == score2</span>
        <span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;Failed to generate same score  after pickling with {0}&quot;</span><span class="s1">.format(name)</span>
        <span class="s2">for </span><span class="s1">attribute </span><span class="s2">in </span><span class="s1">fitted_attribute:</span>
            <span class="s1">assert_array_equal(</span>
                <span class="s1">getattr(est2.tree_</span><span class="s2">, </span><span class="s1">attribute)</span><span class="s2">,</span>
                <span class="s1">fitted_attribute[attribute]</span><span class="s2">,</span>
                <span class="s1">err_msg=(</span>
                    <span class="s3">f&quot;Failed to generate same attribute </span><span class="s2">{</span><span class="s1">attribute</span><span class="s2">} </span><span class="s3">after pickling with&quot;</span>
                    <span class="s3">f&quot; </span><span class="s2">{</span><span class="s1">name</span><span class="s2">}</span><span class="s3">&quot;</span>
                <span class="s1">)</span><span class="s2">,</span>
            <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_multioutput():</span>
    <span class="s5"># Check estimators on multi-output problems.</span>
    <span class="s1">X = [</span>
        <span class="s1">[-</span><span class="s4">2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[-</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">]</span>

    <span class="s1">y = [</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">]</span>

    <span class="s1">T = [[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]]</span>
    <span class="s1">y_true = [[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]]</span>

    <span class="s5"># toy classification problem</span>
    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeClassifier </span><span class="s2">in </span><span class="s1">CLF_TREES.items():</span>
        <span class="s1">clf = TreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">y_hat = clf.fit(X</span><span class="s2">, </span><span class="s1">y).predict(T)</span>
        <span class="s1">assert_array_equal(y_hat</span><span class="s2">, </span><span class="s1">y_true)</span>
        <span class="s2">assert </span><span class="s1">y_hat.shape == (</span><span class="s4">4</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>

        <span class="s1">proba = clf.predict_proba(T)</span>
        <span class="s2">assert </span><span class="s1">len(proba) == </span><span class="s4">2</span>
        <span class="s2">assert </span><span class="s1">proba[</span><span class="s4">0</span><span class="s1">].shape == (</span><span class="s4">4</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">proba[</span><span class="s4">1</span><span class="s1">].shape == (</span><span class="s4">4</span><span class="s2">, </span><span class="s4">4</span><span class="s1">)</span>

        <span class="s1">log_proba = clf.predict_log_proba(T)</span>
        <span class="s2">assert </span><span class="s1">len(log_proba) == </span><span class="s4">2</span>
        <span class="s2">assert </span><span class="s1">log_proba[</span><span class="s4">0</span><span class="s1">].shape == (</span><span class="s4">4</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">log_proba[</span><span class="s4">1</span><span class="s1">].shape == (</span><span class="s4">4</span><span class="s2">, </span><span class="s4">4</span><span class="s1">)</span>

    <span class="s5"># toy regression problem</span>
    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeRegressor </span><span class="s2">in </span><span class="s1">REG_TREES.items():</span>
        <span class="s1">reg = TreeRegressor(random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">y_hat = reg.fit(X</span><span class="s2">, </span><span class="s1">y).predict(T)</span>
        <span class="s1">assert_almost_equal(y_hat</span><span class="s2">, </span><span class="s1">y_true)</span>
        <span class="s2">assert </span><span class="s1">y_hat.shape == (</span><span class="s4">4</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_classes_shape():</span>
    <span class="s5"># Test that n_classes_ and classes_ have proper shape.</span>
    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeClassifier </span><span class="s2">in </span><span class="s1">CLF_TREES.items():</span>
        <span class="s5"># Classification, single output</span>
        <span class="s1">clf = TreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s2">assert </span><span class="s1">clf.n_classes_ == </span><span class="s4">2</span>
        <span class="s1">assert_array_equal(clf.classes_</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>

        <span class="s5"># Classification, multi-output</span>
        <span class="s1">_y = np.vstack((y</span><span class="s2">, </span><span class="s1">np.array(y) * </span><span class="s4">2</span><span class="s1">)).T</span>
        <span class="s1">clf = TreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">_y)</span>
        <span class="s2">assert </span><span class="s1">len(clf.n_classes_) == </span><span class="s4">2</span>
        <span class="s2">assert </span><span class="s1">len(clf.classes_) == </span><span class="s4">2</span>
        <span class="s1">assert_array_equal(clf.n_classes_</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>
        <span class="s1">assert_array_equal(clf.classes_</span><span class="s2">, </span><span class="s1">[[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]])</span>


<span class="s2">def </span><span class="s1">test_unbalanced_iris():</span>
    <span class="s5"># Check class rebalancing.</span>
    <span class="s1">unbalanced_X = iris.data[:</span><span class="s4">125</span><span class="s1">]</span>
    <span class="s1">unbalanced_y = iris.target[:</span><span class="s4">125</span><span class="s1">]</span>
    <span class="s1">sample_weight = compute_sample_weight(</span><span class="s3">&quot;balanced&quot;</span><span class="s2">, </span><span class="s1">unbalanced_y)</span>

    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeClassifier </span><span class="s2">in </span><span class="s1">CLF_TREES.items():</span>
        <span class="s1">clf = TreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">clf.fit(unbalanced_X</span><span class="s2">, </span><span class="s1">unbalanced_y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
        <span class="s1">assert_almost_equal(clf.predict(unbalanced_X)</span><span class="s2">, </span><span class="s1">unbalanced_y)</span>


<span class="s2">def </span><span class="s1">test_memory_layout():</span>
    <span class="s5"># Check that it works no matter the memory layout</span>
    <span class="s2">for </span><span class="s1">(name</span><span class="s2">, </span><span class="s1">TreeEstimator)</span><span class="s2">, </span><span class="s1">dtype </span><span class="s2">in </span><span class="s1">product(</span>
        <span class="s1">ALL_TREES.items()</span><span class="s2">, </span><span class="s1">[np.float64</span><span class="s2">, </span><span class="s1">np.float32]</span>
    <span class="s1">):</span>
        <span class="s1">est = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s1">)</span>

        <span class="s5"># Nothing</span>
        <span class="s1">X = np.asarray(iris.data</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>
        <span class="s1">y = iris.target</span>
        <span class="s1">assert_array_equal(est.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s5"># C-order</span>
        <span class="s1">X = np.asarray(iris.data</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;C&quot;</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>
        <span class="s1">y = iris.target</span>
        <span class="s1">assert_array_equal(est.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s5"># F-order</span>
        <span class="s1">X = np.asarray(iris.data</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>
        <span class="s1">y = iris.target</span>
        <span class="s1">assert_array_equal(est.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s5"># Contiguous</span>
        <span class="s1">X = np.ascontiguousarray(iris.data</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>
        <span class="s1">y = iris.target</span>
        <span class="s1">assert_array_equal(est.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s5"># csr matrix</span>
        <span class="s1">X = csr_matrix(iris.data</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>
        <span class="s1">y = iris.target</span>
        <span class="s1">assert_array_equal(est.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s5"># csc_matrix</span>
        <span class="s1">X = csc_matrix(iris.data</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>
        <span class="s1">y = iris.target</span>
        <span class="s1">assert_array_equal(est.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s5"># Strided</span>
        <span class="s1">X = np.asarray(iris.data[::</span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>
        <span class="s1">y = iris.target[::</span><span class="s4">3</span><span class="s1">]</span>
        <span class="s1">assert_array_equal(est.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_sample_weight():</span>
    <span class="s5"># Check sample weighting.</span>
    <span class="s5"># Test that zero-weighted samples are not taken into account</span>
    <span class="s1">X = np.arange(</span><span class="s4">100</span><span class="s1">)[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">y = np.ones(</span><span class="s4">100</span><span class="s1">)</span>
    <span class="s1">y[:</span><span class="s4">50</span><span class="s1">] = </span><span class="s4">0.0</span>

    <span class="s1">sample_weight = np.ones(</span><span class="s4">100</span><span class="s1">)</span>
    <span class="s1">sample_weight[y == </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">0.0</span>

    <span class="s1">clf = DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_array_equal(clf.predict(X)</span><span class="s2">, </span><span class="s1">np.ones(</span><span class="s4">100</span><span class="s1">))</span>

    <span class="s5"># Test that low weighted samples are not taken into account at low depth</span>
    <span class="s1">X = np.arange(</span><span class="s4">200</span><span class="s1">)[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">y = np.zeros(</span><span class="s4">200</span><span class="s1">)</span>
    <span class="s1">y[</span><span class="s4">50</span><span class="s1">:</span><span class="s4">100</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">y[</span><span class="s4">100</span><span class="s1">:</span><span class="s4">200</span><span class="s1">] = </span><span class="s4">2</span>
    <span class="s1">X[</span><span class="s4">100</span><span class="s1">:</span><span class="s4">200</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">200</span>

    <span class="s1">sample_weight = np.ones(</span><span class="s4">200</span><span class="s1">)</span>

    <span class="s1">sample_weight[y == </span><span class="s4">2</span><span class="s1">] = </span><span class="s4">0.51  </span><span class="s5"># Samples of class '2' are still weightier</span>
    <span class="s1">clf = DecisionTreeClassifier(max_depth=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s2">assert </span><span class="s1">clf.tree_.threshold[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">149.5</span>

    <span class="s1">sample_weight[y == </span><span class="s4">2</span><span class="s1">] = </span><span class="s4">0.5  </span><span class="s5"># Samples of class '2' are no longer weightier</span>
    <span class="s1">clf = DecisionTreeClassifier(max_depth=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s2">assert </span><span class="s1">clf.tree_.threshold[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">49.5  </span><span class="s5"># Threshold should have moved</span>

    <span class="s5"># Test that sample weighting is the same as having duplicates</span>
    <span class="s1">X = iris.data</span>
    <span class="s1">y = iris.target</span>

    <span class="s1">duplicates = rng.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s4">100</span><span class="s1">)</span>

    <span class="s1">clf = DecisionTreeClassifier(random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X[duplicates]</span><span class="s2">, </span><span class="s1">y[duplicates])</span>

    <span class="s1">sample_weight = np.bincount(duplicates</span><span class="s2">, </span><span class="s1">minlength=X.shape[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">clf2 = DecisionTreeClassifier(random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf2.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">internal = clf.tree_.children_left != tree._tree.TREE_LEAF</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">clf.tree_.threshold[internal]</span><span class="s2">, </span><span class="s1">clf2.tree_.threshold[internal]</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_sample_weight_invalid():</span>
    <span class="s5"># Check sample weighting raises errors.</span>
    <span class="s1">X = np.arange(</span><span class="s4">100</span><span class="s1">)[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">y = np.ones(</span><span class="s4">100</span><span class="s1">)</span>
    <span class="s1">y[:</span><span class="s4">50</span><span class="s1">] = </span><span class="s4">0.0</span>

    <span class="s1">clf = DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">sample_weight = np.random.rand(</span><span class="s4">100</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">sample_weight = np.array(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">expected_err = </span><span class="s3">r&quot;Singleton.* cannot be considered a valid collection&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(TypeError</span><span class="s2">, </span><span class="s1">match=expected_err):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>


<span class="s2">def </span><span class="s1">check_class_weights(name):</span>
    <span class="s0">&quot;&quot;&quot;Check class_weights resemble sample_weights behavior.&quot;&quot;&quot;</span>
    <span class="s1">TreeClassifier = CLF_TREES[name]</span>

    <span class="s5"># Iris is balanced, so no effect expected for using 'balanced' weights</span>
    <span class="s1">clf1 = TreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf1.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">clf2 = TreeClassifier(class_weight=</span><span class="s3">&quot;balanced&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf2.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">assert_almost_equal(clf1.feature_importances_</span><span class="s2">, </span><span class="s1">clf2.feature_importances_)</span>

    <span class="s5"># Make a multi-output problem with three copies of Iris</span>
    <span class="s1">iris_multi = np.vstack((iris.target</span><span class="s2">, </span><span class="s1">iris.target</span><span class="s2">, </span><span class="s1">iris.target)).T</span>
    <span class="s5"># Create user-defined weights that should balance over the outputs</span>
    <span class="s1">clf3 = TreeClassifier(</span>
        <span class="s1">class_weight=[</span>
            <span class="s1">{</span><span class="s4">0</span><span class="s1">: </span><span class="s4">2.0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">: </span><span class="s4">2.0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">: </span><span class="s4">1.0</span><span class="s1">}</span><span class="s2">,</span>
            <span class="s1">{</span><span class="s4">0</span><span class="s1">: </span><span class="s4">2.0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">: </span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">: </span><span class="s4">2.0</span><span class="s1">}</span><span class="s2">,</span>
            <span class="s1">{</span><span class="s4">0</span><span class="s1">: </span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">: </span><span class="s4">2.0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">: </span><span class="s4">2.0</span><span class="s1">}</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">clf3.fit(iris.data</span><span class="s2">, </span><span class="s1">iris_multi)</span>
    <span class="s1">assert_almost_equal(clf2.feature_importances_</span><span class="s2">, </span><span class="s1">clf3.feature_importances_)</span>
    <span class="s5"># Check against multi-output &quot;auto&quot; which should also have no effect</span>
    <span class="s1">clf4 = TreeClassifier(class_weight=</span><span class="s3">&quot;balanced&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf4.fit(iris.data</span><span class="s2">, </span><span class="s1">iris_multi)</span>
    <span class="s1">assert_almost_equal(clf3.feature_importances_</span><span class="s2">, </span><span class="s1">clf4.feature_importances_)</span>

    <span class="s5"># Inflate importance of class 1, check against user-defined weights</span>
    <span class="s1">sample_weight = np.ones(iris.target.shape)</span>
    <span class="s1">sample_weight[iris.target == </span><span class="s4">1</span><span class="s1">] *= </span><span class="s4">100</span>
    <span class="s1">class_weight = {</span><span class="s4">0</span><span class="s1">: </span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">: </span><span class="s4">100.0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">: </span><span class="s4">1.0</span><span class="s1">}</span>
    <span class="s1">clf1 = TreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf1.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target</span><span class="s2">, </span><span class="s1">sample_weight)</span>
    <span class="s1">clf2 = TreeClassifier(class_weight=class_weight</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf2.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">assert_almost_equal(clf1.feature_importances_</span><span class="s2">, </span><span class="s1">clf2.feature_importances_)</span>

    <span class="s5"># Check that sample_weight and class_weight are multiplicative</span>
    <span class="s1">clf1 = TreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf1.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target</span><span class="s2">, </span><span class="s1">sample_weight**</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">clf2 = TreeClassifier(class_weight=class_weight</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf2.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target</span><span class="s2">, </span><span class="s1">sample_weight)</span>
    <span class="s1">assert_almost_equal(clf1.feature_importances_</span><span class="s2">, </span><span class="s1">clf2.feature_importances_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name&quot;</span><span class="s2">, </span><span class="s1">CLF_TREES)</span>
<span class="s2">def </span><span class="s1">test_class_weights(name):</span>
    <span class="s1">check_class_weights(name)</span>


<span class="s2">def </span><span class="s1">check_class_weight_errors(name):</span>
    <span class="s5"># Test if class_weight raises errors and warnings when expected.</span>
    <span class="s1">TreeClassifier = CLF_TREES[name]</span>
    <span class="s1">_y = np.vstack((y</span><span class="s2">, </span><span class="s1">np.array(y) * </span><span class="s4">2</span><span class="s1">)).T</span>

    <span class="s5"># Incorrect length list for multi-output</span>
    <span class="s1">clf = TreeClassifier(class_weight=[{-</span><span class="s4">1</span><span class="s1">: </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">1</span><span class="s1">: </span><span class="s4">1.0</span><span class="s1">}]</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">err_msg = </span><span class="s3">&quot;number of elements in class_weight should match number of outputs.&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">_y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name&quot;</span><span class="s2">, </span><span class="s1">CLF_TREES)</span>
<span class="s2">def </span><span class="s1">test_class_weight_errors(name):</span>
    <span class="s1">check_class_weight_errors(name)</span>


<span class="s2">def </span><span class="s1">test_max_leaf_nodes():</span>
    <span class="s5"># Test greedy trees with max_depth + 1 leafs.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_hastie_10_2(n_samples=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">k = </span><span class="s4">4</span>
    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeEstimator </span><span class="s2">in </span><span class="s1">ALL_TREES.items():</span>
        <span class="s1">est = TreeEstimator(max_depth=</span><span class="s2">None, </span><span class="s1">max_leaf_nodes=k + </span><span class="s4">1</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">est.get_n_leaves() == k + </span><span class="s4">1</span>


<span class="s2">def </span><span class="s1">test_max_leaf_nodes_max_depth():</span>
    <span class="s5"># Test precedence of max_leaf_nodes over max_depth.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_hastie_10_2(n_samples=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">k = </span><span class="s4">4</span>
    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeEstimator </span><span class="s2">in </span><span class="s1">ALL_TREES.items():</span>
        <span class="s1">est = TreeEstimator(max_depth=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">max_leaf_nodes=k).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">est.get_depth() == </span><span class="s4">1</span>


<span class="s2">def </span><span class="s1">test_arrays_persist():</span>
    <span class="s5"># Ensure property arrays' memory stays alive when tree disappears</span>
    <span class="s5"># non-regression for #2726</span>
    <span class="s2">for </span><span class="s1">attr </span><span class="s2">in </span><span class="s1">[</span>
        <span class="s3">&quot;n_classes&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;value&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;children_left&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;children_right&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;threshold&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;impurity&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;feature&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;n_node_samples&quot;</span><span class="s2">,</span>
    <span class="s1">]:</span>
        <span class="s1">value = getattr(DecisionTreeClassifier().fit([[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]).tree_</span><span class="s2">, </span><span class="s1">attr)</span>
        <span class="s5"># if pointing to freed memory, contents may be arbitrary</span>
        <span class="s2">assert </span><span class="s1">-</span><span class="s4">3 </span><span class="s1">&lt;= value.flat[</span><span class="s4">0</span><span class="s1">] &lt; </span><span class="s4">3</span><span class="s2">, </span><span class="s3">&quot;Array points to arbitrary memory&quot;</span>


<span class="s2">def </span><span class="s1">test_only_constant_features():</span>
    <span class="s1">random_state = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = np.zeros((</span><span class="s4">10</span><span class="s2">, </span><span class="s4">20</span><span class="s1">))</span>
    <span class="s1">y = random_state.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s1">(</span><span class="s4">10</span><span class="s2">,</span><span class="s1">))</span>
    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeEstimator </span><span class="s2">in </span><span class="s1">ALL_TREES.items():</span>
        <span class="s1">est = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">est.tree_.max_depth == </span><span class="s4">0</span>


<span class="s2">def </span><span class="s1">test_behaviour_constant_feature_after_splits():</span>
    <span class="s1">X = np.transpose(</span>
        <span class="s1">np.vstack(([[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">7</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">np.zeros((</span><span class="s4">4</span><span class="s2">, </span><span class="s4">11</span><span class="s1">))))</span>
    <span class="s1">)</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span>
    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeEstimator </span><span class="s2">in </span><span class="s1">ALL_TREES.items():</span>
        <span class="s5"># do not check extra random trees</span>
        <span class="s2">if </span><span class="s3">&quot;ExtraTree&quot; </span><span class="s2">not in </span><span class="s1">name:</span>
            <span class="s1">est = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_features=</span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
            <span class="s2">assert </span><span class="s1">est.tree_.max_depth == </span><span class="s4">2</span>
            <span class="s2">assert </span><span class="s1">est.tree_.node_count == </span><span class="s4">5</span>


<span class="s2">def </span><span class="s1">test_with_only_one_non_constant_features():</span>
    <span class="s1">X = np.hstack([np.array([[</span><span class="s4">1.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.0</span><span class="s1">]])</span><span class="s2">, </span><span class="s1">np.zeros((</span><span class="s4">4</span><span class="s2">, </span><span class="s4">1000</span><span class="s1">))])</span>

    <span class="s1">y = np.array([</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">])</span>
    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeEstimator </span><span class="s2">in </span><span class="s1">CLF_TREES.items():</span>
        <span class="s1">est = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_features=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">est.tree_.max_depth == </span><span class="s4">1</span>
        <span class="s1">assert_array_equal(est.predict_proba(X)</span><span class="s2">, </span><span class="s1">np.full((</span><span class="s4">4</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">))</span>

    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">TreeEstimator </span><span class="s2">in </span><span class="s1">REG_TREES.items():</span>
        <span class="s1">est = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_features=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">est.tree_.max_depth == </span><span class="s4">1</span>
        <span class="s1">assert_array_equal(est.predict(X)</span><span class="s2">, </span><span class="s1">np.full((</span><span class="s4">4</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">))</span>


<span class="s2">def </span><span class="s1">test_big_input():</span>
    <span class="s5"># Test if the warning for too large inputs is appropriate.</span>
    <span class="s1">X = np.repeat(</span><span class="s4">10</span><span class="s1">**</span><span class="s4">40.0</span><span class="s2">, </span><span class="s4">4</span><span class="s1">).astype(np.float64).reshape(-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf = DecisionTreeClassifier()</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;float32&quot;</span><span class="s1">):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_realloc():</span>
    <span class="s2">from </span><span class="s1">sklearn.tree._utils </span><span class="s2">import </span><span class="s1">_realloc_test</span>

    <span class="s2">with </span><span class="s1">pytest.raises(MemoryError):</span>
        <span class="s1">_realloc_test()</span>


<span class="s2">def </span><span class="s1">test_huge_allocations():</span>
    <span class="s1">n_bits = </span><span class="s4">8 </span><span class="s1">* struct.calcsize(</span><span class="s3">&quot;P&quot;</span><span class="s1">)</span>

    <span class="s1">X = np.random.randn(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">y = np.random.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)</span>

    <span class="s5"># Sanity check: we cannot request more memory than the size of the address</span>
    <span class="s5"># space. Currently raises OverflowError.</span>
    <span class="s1">huge = </span><span class="s4">2 </span><span class="s1">** (n_bits + </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf = DecisionTreeClassifier(splitter=</span><span class="s3">&quot;best&quot;</span><span class="s2">, </span><span class="s1">max_leaf_nodes=huge)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(Exception):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s5"># Non-regression test: MemoryError used to be dropped by Cython</span>
    <span class="s5"># because of missing &quot;except *&quot;.</span>
    <span class="s1">huge = </span><span class="s4">2 </span><span class="s1">** (n_bits - </span><span class="s4">1</span><span class="s1">) - </span><span class="s4">1</span>
    <span class="s1">clf = DecisionTreeClassifier(splitter=</span><span class="s3">&quot;best&quot;</span><span class="s2">, </span><span class="s1">max_leaf_nodes=huge)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(MemoryError):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">check_sparse_input(tree</span><span class="s2">, </span><span class="s1">dataset</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s1">TreeEstimator = ALL_TREES[tree]</span>
    <span class="s1">X = DATASETS[dataset][</span><span class="s3">&quot;X&quot;</span><span class="s1">]</span>
    <span class="s1">X_sparse = DATASETS[dataset][</span><span class="s3">&quot;X_sparse&quot;</span><span class="s1">]</span>
    <span class="s1">y = DATASETS[dataset][</span><span class="s3">&quot;y&quot;</span><span class="s1">]</span>

    <span class="s5"># Gain testing time</span>
    <span class="s2">if </span><span class="s1">dataset </span><span class="s2">in </span><span class="s1">[</span><span class="s3">&quot;digits&quot;</span><span class="s2">, </span><span class="s3">&quot;diabetes&quot;</span><span class="s1">]:</span>
        <span class="s1">n_samples = X.shape[</span><span class="s4">0</span><span class="s1">] // </span><span class="s4">5</span>
        <span class="s1">X = X[:n_samples]</span>
        <span class="s1">X_sparse = X_sparse[:n_samples]</span>
        <span class="s1">y = y[:n_samples]</span>

    <span class="s2">for </span><span class="s1">sparse_format </span><span class="s2">in </span><span class="s1">(csr_matrix</span><span class="s2">, </span><span class="s1">csc_matrix</span><span class="s2">, </span><span class="s1">coo_matrix):</span>
        <span class="s1">X_sparse = sparse_format(X_sparse)</span>

        <span class="s5"># Check the default (depth first search)</span>
        <span class="s1">d = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_depth=max_depth).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">s = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_depth=max_depth).fit(X_sparse</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s1">assert_tree_equal(</span>
            <span class="s1">d.tree_</span><span class="s2">,</span>
            <span class="s1">s.tree_</span><span class="s2">,</span>
            <span class="s3">&quot;{0} with dense and sparse format gave different trees&quot;</span><span class="s1">.format(tree)</span><span class="s2">,</span>
        <span class="s1">)</span>

        <span class="s1">y_pred = d.predict(X)</span>
        <span class="s2">if </span><span class="s1">tree </span><span class="s2">in </span><span class="s1">CLF_TREES:</span>
            <span class="s1">y_proba = d.predict_proba(X)</span>
            <span class="s1">y_log_proba = d.predict_log_proba(X)</span>

        <span class="s2">for </span><span class="s1">sparse_matrix </span><span class="s2">in </span><span class="s1">(csr_matrix</span><span class="s2">, </span><span class="s1">csc_matrix</span><span class="s2">, </span><span class="s1">coo_matrix):</span>
            <span class="s1">X_sparse_test = sparse_matrix(X_sparse</span><span class="s2">, </span><span class="s1">dtype=np.float32)</span>

            <span class="s1">assert_array_almost_equal(s.predict(X_sparse_test)</span><span class="s2">, </span><span class="s1">y_pred)</span>

            <span class="s2">if </span><span class="s1">tree </span><span class="s2">in </span><span class="s1">CLF_TREES:</span>
                <span class="s1">assert_array_almost_equal(s.predict_proba(X_sparse_test)</span><span class="s2">, </span><span class="s1">y_proba)</span>
                <span class="s1">assert_array_almost_equal(</span>
                    <span class="s1">s.predict_log_proba(X_sparse_test)</span><span class="s2">, </span><span class="s1">y_log_proba</span>
                <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;tree_type&quot;</span><span class="s2">, </span><span class="s1">SPARSE_TREES)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;dataset&quot;</span><span class="s2">,</span>
    <span class="s1">(</span>
        <span class="s3">&quot;clf_small&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;toy&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;digits&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;multilabel&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;sparse-pos&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;sparse-neg&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;sparse-mix&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;zeros&quot;</span><span class="s2">,</span>
    <span class="s1">)</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_sparse_input(tree_type</span><span class="s2">, </span><span class="s1">dataset):</span>
    <span class="s1">max_depth = </span><span class="s4">3 </span><span class="s2">if </span><span class="s1">dataset == </span><span class="s3">&quot;digits&quot; </span><span class="s2">else None</span>
    <span class="s1">check_sparse_input(tree_type</span><span class="s2">, </span><span class="s1">dataset</span><span class="s2">, </span><span class="s1">max_depth)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;tree_type&quot;</span><span class="s2">, </span><span class="s1">sorted(set(SPARSE_TREES).intersection(REG_TREES)))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;dataset&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;diabetes&quot;</span><span class="s2">, </span><span class="s3">&quot;reg_small&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_sparse_input_reg_trees(tree_type</span><span class="s2">, </span><span class="s1">dataset):</span>
    <span class="s5"># Due to numerical instability of MSE and too strict test, we limit the</span>
    <span class="s5"># maximal depth</span>
    <span class="s1">check_sparse_input(tree_type</span><span class="s2">, </span><span class="s1">dataset</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">check_sparse_parameters(tree</span><span class="s2">, </span><span class="s1">dataset):</span>
    <span class="s1">TreeEstimator = ALL_TREES[tree]</span>
    <span class="s1">X = DATASETS[dataset][</span><span class="s3">&quot;X&quot;</span><span class="s1">]</span>
    <span class="s1">X_sparse = DATASETS[dataset][</span><span class="s3">&quot;X_sparse&quot;</span><span class="s1">]</span>
    <span class="s1">y = DATASETS[dataset][</span><span class="s3">&quot;y&quot;</span><span class="s1">]</span>

    <span class="s5"># Check max_features</span>
    <span class="s1">d = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_features=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">2</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">s = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_features=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">2</span><span class="s1">).fit(X_sparse</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_tree_equal(</span>
        <span class="s1">d.tree_</span><span class="s2">,</span>
        <span class="s1">s.tree_</span><span class="s2">,</span>
        <span class="s3">&quot;{0} with dense and sparse format gave different trees&quot;</span><span class="s1">.format(tree)</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(s.predict(X)</span><span class="s2">, </span><span class="s1">d.predict(X))</span>

    <span class="s5"># Check min_samples_split</span>
    <span class="s1">d = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_features=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">min_samples_split=</span><span class="s4">10</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">s = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_features=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">min_samples_split=</span><span class="s4">10</span><span class="s1">).fit(</span>
        <span class="s1">X_sparse</span><span class="s2">, </span><span class="s1">y</span>
    <span class="s1">)</span>
    <span class="s1">assert_tree_equal(</span>
        <span class="s1">d.tree_</span><span class="s2">,</span>
        <span class="s1">s.tree_</span><span class="s2">,</span>
        <span class="s3">&quot;{0} with dense and sparse format gave different trees&quot;</span><span class="s1">.format(tree)</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(s.predict(X)</span><span class="s2">, </span><span class="s1">d.predict(X))</span>

    <span class="s5"># Check min_samples_leaf</span>
    <span class="s1">d = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">min_samples_leaf=X_sparse.shape[</span><span class="s4">0</span><span class="s1">] // </span><span class="s4">2</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">s = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">min_samples_leaf=X_sparse.shape[</span><span class="s4">0</span><span class="s1">] // </span><span class="s4">2</span><span class="s1">).fit(</span>
        <span class="s1">X_sparse</span><span class="s2">, </span><span class="s1">y</span>
    <span class="s1">)</span>
    <span class="s1">assert_tree_equal(</span>
        <span class="s1">d.tree_</span><span class="s2">,</span>
        <span class="s1">s.tree_</span><span class="s2">,</span>
        <span class="s3">&quot;{0} with dense and sparse format gave different trees&quot;</span><span class="s1">.format(tree)</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(s.predict(X)</span><span class="s2">, </span><span class="s1">d.predict(X))</span>

    <span class="s5"># Check best-first search</span>
    <span class="s1">d = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_leaf_nodes=</span><span class="s4">3</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">s = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_leaf_nodes=</span><span class="s4">3</span><span class="s1">).fit(X_sparse</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_tree_equal(</span>
        <span class="s1">d.tree_</span><span class="s2">,</span>
        <span class="s1">s.tree_</span><span class="s2">,</span>
        <span class="s3">&quot;{0} with dense and sparse format gave different trees&quot;</span><span class="s1">.format(tree)</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(s.predict(X)</span><span class="s2">, </span><span class="s1">d.predict(X))</span>


<span class="s2">def </span><span class="s1">check_sparse_criterion(tree</span><span class="s2">, </span><span class="s1">dataset):</span>
    <span class="s1">TreeEstimator = ALL_TREES[tree]</span>
    <span class="s1">X = DATASETS[dataset][</span><span class="s3">&quot;X&quot;</span><span class="s1">]</span>
    <span class="s1">X_sparse = DATASETS[dataset][</span><span class="s3">&quot;X_sparse&quot;</span><span class="s1">]</span>
    <span class="s1">y = DATASETS[dataset][</span><span class="s3">&quot;y&quot;</span><span class="s1">]</span>

    <span class="s5"># Check various criterion</span>
    <span class="s1">CRITERIONS = REG_CRITERIONS </span><span class="s2">if </span><span class="s1">tree </span><span class="s2">in </span><span class="s1">REG_TREES </span><span class="s2">else </span><span class="s1">CLF_CRITERIONS</span>
    <span class="s2">for </span><span class="s1">criterion </span><span class="s2">in </span><span class="s1">CRITERIONS:</span>
        <span class="s1">d = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">criterion=criterion).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">s = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">criterion=criterion).fit(</span>
            <span class="s1">X_sparse</span><span class="s2">, </span><span class="s1">y</span>
        <span class="s1">)</span>

        <span class="s1">assert_tree_equal(</span>
            <span class="s1">d.tree_</span><span class="s2">,</span>
            <span class="s1">s.tree_</span><span class="s2">,</span>
            <span class="s3">&quot;{0} with dense and sparse format gave different trees&quot;</span><span class="s1">.format(tree)</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">assert_array_almost_equal(s.predict(X)</span><span class="s2">, </span><span class="s1">d.predict(X))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;tree_type&quot;</span><span class="s2">, </span><span class="s1">SPARSE_TREES)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;dataset&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;sparse-pos&quot;</span><span class="s2">, </span><span class="s3">&quot;sparse-neg&quot;</span><span class="s2">, </span><span class="s3">&quot;sparse-mix&quot;</span><span class="s2">, </span><span class="s3">&quot;zeros&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;check&quot;</span><span class="s2">, </span><span class="s1">[check_sparse_parameters</span><span class="s2">, </span><span class="s1">check_sparse_criterion])</span>
<span class="s2">def </span><span class="s1">test_sparse(tree_type</span><span class="s2">, </span><span class="s1">dataset</span><span class="s2">, </span><span class="s1">check):</span>
    <span class="s1">check(tree_type</span><span class="s2">, </span><span class="s1">dataset)</span>


<span class="s2">def </span><span class="s1">check_explicit_sparse_zeros(tree</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">10</span><span class="s1">):</span>
    <span class="s1">TreeEstimator = ALL_TREES[tree]</span>

    <span class="s5"># n_samples set n_feature to ease construction of a simultaneous</span>
    <span class="s5"># construction of a csr and csc matrix</span>
    <span class="s1">n_samples = n_features</span>
    <span class="s1">samples = np.arange(n_samples)</span>

    <span class="s5"># Generate X, y</span>
    <span class="s1">random_state = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">indices = []</span>
    <span class="s1">data = []</span>
    <span class="s1">offset = </span><span class="s4">0</span>
    <span class="s1">indptr = [offset]</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_features):</span>
        <span class="s1">n_nonzero_i = random_state.binomial(n_samples</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">)</span>
        <span class="s1">indices_i = random_state.permutation(samples)[:n_nonzero_i]</span>
        <span class="s1">indices.append(indices_i)</span>
        <span class="s1">data_i = random_state.binomial(</span><span class="s4">3</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s1">size=(n_nonzero_i</span><span class="s2">,</span><span class="s1">)) - </span><span class="s4">1</span>
        <span class="s1">data.append(data_i)</span>
        <span class="s1">offset += n_nonzero_i</span>
        <span class="s1">indptr.append(offset)</span>

    <span class="s1">indices = np.concatenate(indices)</span>
    <span class="s1">data = np.array(np.concatenate(data)</span><span class="s2">, </span><span class="s1">dtype=np.float32)</span>
    <span class="s1">X_sparse = csc_matrix((data</span><span class="s2">, </span><span class="s1">indices</span><span class="s2">, </span><span class="s1">indptr)</span><span class="s2">, </span><span class="s1">shape=(n_samples</span><span class="s2">, </span><span class="s1">n_features))</span>
    <span class="s1">X = X_sparse.toarray()</span>
    <span class="s1">X_sparse_test = csr_matrix((data</span><span class="s2">, </span><span class="s1">indices</span><span class="s2">, </span><span class="s1">indptr)</span><span class="s2">, </span><span class="s1">shape=(n_samples</span><span class="s2">, </span><span class="s1">n_features))</span>
    <span class="s1">X_test = X_sparse_test.toarray()</span>
    <span class="s1">y = random_state.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s1">size=(n_samples</span><span class="s2">,</span><span class="s1">))</span>

    <span class="s5"># Ensure that X_sparse_test owns its data, indices and indptr array</span>
    <span class="s1">X_sparse_test = X_sparse_test.copy()</span>

    <span class="s5"># Ensure that we have explicit zeros</span>
    <span class="s2">assert </span><span class="s1">(X_sparse.data == </span><span class="s4">0.0</span><span class="s1">).sum() &gt; </span><span class="s4">0</span>
    <span class="s2">assert </span><span class="s1">(X_sparse_test.data == </span><span class="s4">0.0</span><span class="s1">).sum() &gt; </span><span class="s4">0</span>

    <span class="s5"># Perform the comparison</span>
    <span class="s1">d = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_depth=max_depth).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">s = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_depth=max_depth).fit(X_sparse</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">assert_tree_equal(</span>
        <span class="s1">d.tree_</span><span class="s2">,</span>
        <span class="s1">s.tree_</span><span class="s2">,</span>
        <span class="s3">&quot;{0} with dense and sparse format gave different trees&quot;</span><span class="s1">.format(tree)</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">Xs = (X_test</span><span class="s2">, </span><span class="s1">X_sparse_test)</span>
    <span class="s2">for </span><span class="s1">X1</span><span class="s2">, </span><span class="s1">X2 </span><span class="s2">in </span><span class="s1">product(Xs</span><span class="s2">, </span><span class="s1">Xs):</span>
        <span class="s1">assert_array_almost_equal(s.tree_.apply(X1)</span><span class="s2">, </span><span class="s1">d.tree_.apply(X2))</span>
        <span class="s1">assert_array_almost_equal(s.apply(X1)</span><span class="s2">, </span><span class="s1">d.apply(X2))</span>
        <span class="s1">assert_array_almost_equal(s.apply(X1)</span><span class="s2">, </span><span class="s1">s.tree_.apply(X1))</span>

        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">s.tree_.decision_path(X1).toarray()</span><span class="s2">, </span><span class="s1">d.tree_.decision_path(X2).toarray()</span>
        <span class="s1">)</span>
        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">s.decision_path(X1).toarray()</span><span class="s2">, </span><span class="s1">d.decision_path(X2).toarray()</span>
        <span class="s1">)</span>
        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">s.decision_path(X1).toarray()</span><span class="s2">, </span><span class="s1">s.tree_.decision_path(X1).toarray()</span>
        <span class="s1">)</span>

        <span class="s1">assert_array_almost_equal(s.predict(X1)</span><span class="s2">, </span><span class="s1">d.predict(X2))</span>

        <span class="s2">if </span><span class="s1">tree </span><span class="s2">in </span><span class="s1">CLF_TREES:</span>
            <span class="s1">assert_array_almost_equal(s.predict_proba(X1)</span><span class="s2">, </span><span class="s1">d.predict_proba(X2))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;tree_type&quot;</span><span class="s2">, </span><span class="s1">SPARSE_TREES)</span>
<span class="s2">def </span><span class="s1">test_explicit_sparse_zeros(tree_type):</span>
    <span class="s1">check_explicit_sparse_zeros(tree_type)</span>


<span class="s1">@ignore_warnings</span>
<span class="s2">def </span><span class="s1">check_raise_error_on_1d_input(name):</span>
    <span class="s1">TreeEstimator = ALL_TREES[name]</span>

    <span class="s1">X = iris.data[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">].ravel()</span>
    <span class="s1">X_2d = iris.data[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">].reshape((-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">y = iris.target</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">TreeEstimator(random_state=</span><span class="s4">0</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">est = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">est.fit(X_2d</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">est.predict([X])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name&quot;</span><span class="s2">, </span><span class="s1">ALL_TREES)</span>
<span class="s2">def </span><span class="s1">test_1d_input(name):</span>
    <span class="s2">with </span><span class="s1">ignore_warnings():</span>
        <span class="s1">check_raise_error_on_1d_input(name)</span>


<span class="s2">def </span><span class="s1">_check_min_weight_leaf_split_level(TreeEstimator</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight):</span>
    <span class="s1">est = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s2">assert </span><span class="s1">est.tree_.max_depth == </span><span class="s4">1</span>

    <span class="s1">est = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">min_weight_fraction_leaf=</span><span class="s4">0.4</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s2">assert </span><span class="s1">est.tree_.max_depth == </span><span class="s4">0</span>


<span class="s2">def </span><span class="s1">check_min_weight_leaf_split_level(name):</span>
    <span class="s1">TreeEstimator = ALL_TREES[name]</span>

    <span class="s1">X = np.array([[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">sample_weight = [</span><span class="s4">0.2</span><span class="s2">, </span><span class="s4">0.2</span><span class="s2">, </span><span class="s4">0.2</span><span class="s2">, </span><span class="s4">0.2</span><span class="s2">, </span><span class="s4">0.2</span><span class="s1">]</span>
    <span class="s1">_check_min_weight_leaf_split_level(TreeEstimator</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight)</span>

    <span class="s1">_check_min_weight_leaf_split_level(TreeEstimator</span><span class="s2">, </span><span class="s1">csc_matrix(X)</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name&quot;</span><span class="s2">, </span><span class="s1">ALL_TREES)</span>
<span class="s2">def </span><span class="s1">test_min_weight_leaf_split_level(name):</span>
    <span class="s1">check_min_weight_leaf_split_level(name)</span>


<span class="s2">def </span><span class="s1">check_public_apply(name):</span>
    <span class="s1">X_small32 = X_small.astype(tree._tree.DTYPE</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s1">est = ALL_TREES[name]()</span>
    <span class="s1">est.fit(X_small</span><span class="s2">, </span><span class="s1">y_small)</span>
    <span class="s1">assert_array_equal(est.apply(X_small)</span><span class="s2">, </span><span class="s1">est.tree_.apply(X_small32))</span>


<span class="s2">def </span><span class="s1">check_public_apply_sparse(name):</span>
    <span class="s1">X_small32 = csr_matrix(X_small.astype(tree._tree.DTYPE</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">))</span>

    <span class="s1">est = ALL_TREES[name]()</span>
    <span class="s1">est.fit(X_small</span><span class="s2">, </span><span class="s1">y_small)</span>
    <span class="s1">assert_array_equal(est.apply(X_small)</span><span class="s2">, </span><span class="s1">est.tree_.apply(X_small32))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name&quot;</span><span class="s2">, </span><span class="s1">ALL_TREES)</span>
<span class="s2">def </span><span class="s1">test_public_apply_all_trees(name):</span>
    <span class="s1">check_public_apply(name)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name&quot;</span><span class="s2">, </span><span class="s1">SPARSE_TREES)</span>
<span class="s2">def </span><span class="s1">test_public_apply_sparse_trees(name):</span>
    <span class="s1">check_public_apply_sparse(name)</span>


<span class="s2">def </span><span class="s1">test_decision_path_hardcoded():</span>
    <span class="s1">X = iris.data</span>
    <span class="s1">y = iris.target</span>
    <span class="s1">est = DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">1</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">node_indicator = est.decision_path(X[:</span><span class="s4">2</span><span class="s1">]).toarray()</span>
    <span class="s1">assert_array_equal(node_indicator</span><span class="s2">, </span><span class="s1">[[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]])</span>


<span class="s2">def </span><span class="s1">check_decision_path(name):</span>
    <span class="s1">X = iris.data</span>
    <span class="s1">y = iris.target</span>
    <span class="s1">n_samples = X.shape[</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s1">TreeEstimator = ALL_TREES[name]</span>
    <span class="s1">est = TreeEstimator(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">node_indicator_csr = est.decision_path(X)</span>
    <span class="s1">node_indicator = node_indicator_csr.toarray()</span>
    <span class="s2">assert </span><span class="s1">node_indicator.shape == (n_samples</span><span class="s2">, </span><span class="s1">est.tree_.node_count)</span>

    <span class="s5"># Assert that leaves index are correct</span>
    <span class="s1">leaves = est.apply(X)</span>
    <span class="s1">leave_indicator = [node_indicator[i</span><span class="s2">, </span><span class="s1">j] </span><span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">j </span><span class="s2">in </span><span class="s1">enumerate(leaves)]</span>
    <span class="s1">assert_array_almost_equal(leave_indicator</span><span class="s2">, </span><span class="s1">np.ones(shape=n_samples))</span>

    <span class="s5"># Ensure only one leave node per sample</span>
    <span class="s1">all_leaves = est.tree_.children_left == TREE_LEAF</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">np.dot(node_indicator</span><span class="s2">, </span><span class="s1">all_leaves)</span><span class="s2">, </span><span class="s1">np.ones(shape=n_samples)</span>
    <span class="s1">)</span>

    <span class="s5"># Ensure max depth is consistent with sum of indicator</span>
    <span class="s1">max_depth = node_indicator.sum(axis=</span><span class="s4">1</span><span class="s1">).max()</span>
    <span class="s2">assert </span><span class="s1">est.tree_.max_depth &lt;= max_depth</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name&quot;</span><span class="s2">, </span><span class="s1">ALL_TREES)</span>
<span class="s2">def </span><span class="s1">test_decision_path(name):</span>
    <span class="s1">check_decision_path(name)</span>


<span class="s2">def </span><span class="s1">check_no_sparse_y_support(name):</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = X_multilabel</span><span class="s2">, </span><span class="s1">csr_matrix(y_multilabel)</span>
    <span class="s1">TreeEstimator = ALL_TREES[name]</span>
    <span class="s2">with </span><span class="s1">pytest.raises(TypeError):</span>
        <span class="s1">TreeEstimator(random_state=</span><span class="s4">0</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name&quot;</span><span class="s2">, </span><span class="s1">ALL_TREES)</span>
<span class="s2">def </span><span class="s1">test_no_sparse_y_support(name):</span>
    <span class="s5"># Currently we don't support sparse y</span>
    <span class="s1">check_no_sparse_y_support(name)</span>


<span class="s2">def </span><span class="s1">test_mae():</span>
    <span class="s0">&quot;&quot;&quot;Check MAE criterion produces correct results on small toy dataset: 
 
    ------------------ 
    | X | y | weight | 
    ------------------ 
    | 3 | 3 |  0.1   | 
    | 5 | 3 |  0.3   | 
    | 8 | 4 |  1.0   | 
    | 3 | 6 |  0.6   | 
    | 5 | 7 |  0.3   | 
    ------------------ 
    |sum wt:|  2.3   | 
    ------------------ 
 
    Because we are dealing with sample weights, we cannot find the median by 
    simply choosing/averaging the centre value(s), instead we consider the 
    median where 50% of the cumulative weight is found (in a y sorted data set) 
    . Therefore with regards to this test data, the cumulative weight is &gt;= 50% 
    when y = 4.  Therefore: 
    Median = 4 
 
    For all the samples, we can get the total error by summing: 
    Absolute(Median - y) * weight 
 
    I.e., total error = (Absolute(4 - 3) * 0.1) 
                      + (Absolute(4 - 3) * 0.3) 
                      + (Absolute(4 - 4) * 1.0) 
                      + (Absolute(4 - 6) * 0.6) 
                      + (Absolute(4 - 7) * 0.3) 
                      = 2.5 
 
    Impurity = Total error / total weight 
             = 2.5 / 2.3 
             = 1.08695652173913 
             ------------------ 
 
    From this root node, the next best split is between X values of 3 and 5. 
    Thus, we have left and right child nodes: 
 
    LEFT                    RIGHT 
    ------------------      ------------------ 
    | X | y | weight |      | X | y | weight | 
    ------------------      ------------------ 
    | 3 | 3 |  0.1   |      | 5 | 3 |  0.3   | 
    | 3 | 6 |  0.6   |      | 8 | 4 |  1.0   | 
    ------------------      | 5 | 7 |  0.3   | 
    |sum wt:|  0.7   |      ------------------ 
    ------------------      |sum wt:|  1.6   | 
                            ------------------ 
 
    Impurity is found in the same way: 
    Left node Median = 6 
    Total error = (Absolute(6 - 3) * 0.1) 
                + (Absolute(6 - 6) * 0.6) 
                = 0.3 
 
    Left Impurity = Total error / total weight 
            = 0.3 / 0.7 
            = 0.428571428571429 
            ------------------- 
 
    Likewise for Right node: 
    Right node Median = 4 
    Total error = (Absolute(4 - 3) * 0.3) 
                + (Absolute(4 - 4) * 1.0) 
                + (Absolute(4 - 7) * 0.3) 
                = 1.2 
 
    Right Impurity = Total error / total weight 
            = 1.2 / 1.6 
            = 0.75 
            ------ 
    &quot;&quot;&quot;</span>
    <span class="s1">dt_mae = DecisionTreeRegressor(</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">criterion=</span><span class="s3">&quot;absolute_error&quot;</span><span class="s2">, </span><span class="s1">max_leaf_nodes=</span><span class="s4">2</span>
    <span class="s1">)</span>

    <span class="s5"># Test MAE where sample weights are non-uniform (as illustrated above):</span>
    <span class="s1">dt_mae.fit(</span>
        <span class="s1">X=[[</span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">8</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s1">]]</span><span class="s2">,</span>
        <span class="s1">y=[</span><span class="s4">6</span><span class="s2">, </span><span class="s4">7</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">sample_weight=[</span><span class="s4">0.6</span><span class="s2">, </span><span class="s4">0.3</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.3</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(dt_mae.tree_.impurity</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2.5 </span><span class="s1">/ </span><span class="s4">2.3</span><span class="s2">, </span><span class="s4">0.3 </span><span class="s1">/ </span><span class="s4">0.7</span><span class="s2">, </span><span class="s4">1.2 </span><span class="s1">/ </span><span class="s4">1.6</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(dt_mae.tree_.value.flat</span><span class="s2">, </span><span class="s1">[</span><span class="s4">4.0</span><span class="s2">, </span><span class="s4">6.0</span><span class="s2">, </span><span class="s4">4.0</span><span class="s1">])</span>

    <span class="s5"># Test MAE where all sample weights are uniform:</span>
    <span class="s1">dt_mae.fit(X=[[</span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">8</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">y=[</span><span class="s4">6</span><span class="s2">, </span><span class="s4">7</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">sample_weight=np.ones(</span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">assert_array_equal(dt_mae.tree_.impurity</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1.4</span><span class="s2">, </span><span class="s4">1.5</span><span class="s2">, </span><span class="s4">4.0 </span><span class="s1">/ </span><span class="s4">3.0</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(dt_mae.tree_.value.flat</span><span class="s2">, </span><span class="s1">[</span><span class="s4">4</span><span class="s2">, </span><span class="s4">4.5</span><span class="s2">, </span><span class="s4">4.0</span><span class="s1">])</span>

    <span class="s5"># Test MAE where a `sample_weight` is not explicitly provided.</span>
    <span class="s5"># This is equivalent to providing uniform sample weights, though</span>
    <span class="s5"># the internal logic is different:</span>
    <span class="s1">dt_mae.fit(X=[[</span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">8</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">y=[</span><span class="s4">6</span><span class="s2">, </span><span class="s4">7</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(dt_mae.tree_.impurity</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1.4</span><span class="s2">, </span><span class="s4">1.5</span><span class="s2">, </span><span class="s4">4.0 </span><span class="s1">/ </span><span class="s4">3.0</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(dt_mae.tree_.value.flat</span><span class="s2">, </span><span class="s1">[</span><span class="s4">4</span><span class="s2">, </span><span class="s4">4.5</span><span class="s2">, </span><span class="s4">4.0</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_criterion_copy():</span>
    <span class="s5"># Let's check whether copy of our criterion has the same type</span>
    <span class="s5"># and properties as original</span>
    <span class="s1">n_outputs = </span><span class="s4">3</span>
    <span class="s1">n_classes = np.arange(</span><span class="s4">3</span><span class="s2">, </span><span class="s1">dtype=np.intp)</span>
    <span class="s1">n_samples = </span><span class="s4">100</span>

    <span class="s2">def </span><span class="s1">_pickle_copy(obj):</span>
        <span class="s2">return </span><span class="s1">pickle.loads(pickle.dumps(obj))</span>

    <span class="s2">for </span><span class="s1">copy_func </span><span class="s2">in </span><span class="s1">[copy.copy</span><span class="s2">, </span><span class="s1">copy.deepcopy</span><span class="s2">, </span><span class="s1">_pickle_copy]:</span>
        <span class="s2">for </span><span class="s1">_</span><span class="s2">, </span><span class="s1">typename </span><span class="s2">in </span><span class="s1">CRITERIA_CLF.items():</span>
            <span class="s1">criteria = typename(n_outputs</span><span class="s2">, </span><span class="s1">n_classes)</span>
            <span class="s1">result = copy_func(criteria).__reduce__()</span>
            <span class="s1">typename_</span><span class="s2">, </span><span class="s1">(n_outputs_</span><span class="s2">, </span><span class="s1">n_classes_)</span><span class="s2">, </span><span class="s1">_ = result</span>
            <span class="s2">assert </span><span class="s1">typename == typename_</span>
            <span class="s2">assert </span><span class="s1">n_outputs == n_outputs_</span>
            <span class="s1">assert_array_equal(n_classes</span><span class="s2">, </span><span class="s1">n_classes_)</span>

        <span class="s2">for </span><span class="s1">_</span><span class="s2">, </span><span class="s1">typename </span><span class="s2">in </span><span class="s1">CRITERIA_REG.items():</span>
            <span class="s1">criteria = typename(n_outputs</span><span class="s2">, </span><span class="s1">n_samples)</span>
            <span class="s1">result = copy_func(criteria).__reduce__()</span>
            <span class="s1">typename_</span><span class="s2">, </span><span class="s1">(n_outputs_</span><span class="s2">, </span><span class="s1">n_samples_)</span><span class="s2">, </span><span class="s1">_ = result</span>
            <span class="s2">assert </span><span class="s1">typename == typename_</span>
            <span class="s2">assert </span><span class="s1">n_outputs == n_outputs_</span>
            <span class="s2">assert </span><span class="s1">n_samples == n_samples_</span>


<span class="s2">def </span><span class="s1">test_empty_leaf_infinite_threshold():</span>
    <span class="s5"># try to make empty leaf by using near infinite value.</span>
    <span class="s1">data = np.random.RandomState(</span><span class="s4">0</span><span class="s1">).randn(</span><span class="s4">100</span><span class="s2">, </span><span class="s4">11</span><span class="s1">) * </span><span class="s4">2e38</span>
    <span class="s1">data = np.nan_to_num(data.astype(</span><span class="s3">&quot;float32&quot;</span><span class="s1">))</span>
    <span class="s1">X_full = data[:</span><span class="s2">, </span><span class="s1">:-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">X_sparse = csc_matrix(X_full)</span>
    <span class="s1">y = data[:</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s2">for </span><span class="s1">X </span><span class="s2">in </span><span class="s1">[X_full</span><span class="s2">, </span><span class="s1">X_sparse]:</span>
        <span class="s1">tree = DecisionTreeRegressor(random_state=</span><span class="s4">0</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">terminal_regions = tree.apply(X)</span>
        <span class="s1">left_leaf = set(np.where(tree.tree_.children_left == TREE_LEAF)[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">empty_leaf = left_leaf.difference(terminal_regions)</span>
        <span class="s1">infinite_threshold = np.where(~np.isfinite(tree.tree_.threshold))[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s2">assert </span><span class="s1">len(infinite_threshold) == </span><span class="s4">0</span>
        <span class="s2">assert </span><span class="s1">len(empty_leaf) == </span><span class="s4">0</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;criterion&quot;</span><span class="s2">, </span><span class="s1">CLF_CRITERIONS)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;dataset&quot;</span><span class="s2">, </span><span class="s1">sorted(set(DATASETS.keys()) - {</span><span class="s3">&quot;reg_small&quot;</span><span class="s2">, </span><span class="s3">&quot;diabetes&quot;</span><span class="s1">})</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;tree_cls&quot;</span><span class="s2">, </span><span class="s1">[DecisionTreeClassifier</span><span class="s2">, </span><span class="s1">ExtraTreeClassifier])</span>
<span class="s2">def </span><span class="s1">test_prune_tree_classifier_are_subtrees(criterion</span><span class="s2">, </span><span class="s1">dataset</span><span class="s2">, </span><span class="s1">tree_cls):</span>
    <span class="s1">dataset = DATASETS[dataset]</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = dataset[</span><span class="s3">&quot;X&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dataset[</span><span class="s3">&quot;y&quot;</span><span class="s1">]</span>
    <span class="s1">est = tree_cls(max_leaf_nodes=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">info = est.cost_complexity_pruning_path(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">pruning_path = info.ccp_alphas</span>
    <span class="s1">impurities = info.impurities</span>
    <span class="s2">assert </span><span class="s1">np.all(np.diff(pruning_path) &gt;= </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">np.all(np.diff(impurities) &gt;= </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">assert_pruning_creates_subtree(tree_cls</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">pruning_path)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;criterion&quot;</span><span class="s2">, </span><span class="s1">REG_CRITERIONS)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;dataset&quot;</span><span class="s2">, </span><span class="s1">DATASETS.keys())</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;tree_cls&quot;</span><span class="s2">, </span><span class="s1">[DecisionTreeRegressor</span><span class="s2">, </span><span class="s1">ExtraTreeRegressor])</span>
<span class="s2">def </span><span class="s1">test_prune_tree_regression_are_subtrees(criterion</span><span class="s2">, </span><span class="s1">dataset</span><span class="s2">, </span><span class="s1">tree_cls):</span>
    <span class="s1">dataset = DATASETS[dataset]</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = dataset[</span><span class="s3">&quot;X&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dataset[</span><span class="s3">&quot;y&quot;</span><span class="s1">]</span>

    <span class="s1">est = tree_cls(max_leaf_nodes=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">info = est.cost_complexity_pruning_path(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">pruning_path = info.ccp_alphas</span>
    <span class="s1">impurities = info.impurities</span>
    <span class="s2">assert </span><span class="s1">np.all(np.diff(pruning_path) &gt;= </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">np.all(np.diff(impurities) &gt;= </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">assert_pruning_creates_subtree(tree_cls</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">pruning_path)</span>


<span class="s2">def </span><span class="s1">test_prune_single_node_tree():</span>
    <span class="s5"># single node tree</span>
    <span class="s1">clf1 = DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf1.fit([[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span>

    <span class="s5"># pruned single node tree</span>
    <span class="s1">clf2 = DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">ccp_alpha=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">clf2.fit([[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span>

    <span class="s1">assert_is_subtree(clf1.tree_</span><span class="s2">, </span><span class="s1">clf2.tree_)</span>


<span class="s2">def </span><span class="s1">assert_pruning_creates_subtree(estimator_cls</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">pruning_path):</span>
    <span class="s5"># generate trees with increasing alphas</span>
    <span class="s1">estimators = []</span>
    <span class="s2">for </span><span class="s1">ccp_alpha </span><span class="s2">in </span><span class="s1">pruning_path:</span>
        <span class="s1">est = estimator_cls(max_leaf_nodes=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">ccp_alpha=ccp_alpha</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span>
        <span class="s1">)</span>
        <span class="s1">estimators.append(est)</span>

    <span class="s5"># A pruned tree must be a subtree of the previous tree (which had a</span>
    <span class="s5"># smaller ccp_alpha)</span>
    <span class="s2">for </span><span class="s1">prev_est</span><span class="s2">, </span><span class="s1">next_est </span><span class="s2">in </span><span class="s1">zip(estimators</span><span class="s2">, </span><span class="s1">estimators[</span><span class="s4">1</span><span class="s1">:]):</span>
        <span class="s1">assert_is_subtree(prev_est.tree_</span><span class="s2">, </span><span class="s1">next_est.tree_)</span>


<span class="s2">def </span><span class="s1">assert_is_subtree(tree</span><span class="s2">, </span><span class="s1">subtree):</span>
    <span class="s2">assert </span><span class="s1">tree.node_count &gt;= subtree.node_count</span>
    <span class="s2">assert </span><span class="s1">tree.max_depth &gt;= subtree.max_depth</span>

    <span class="s1">tree_c_left = tree.children_left</span>
    <span class="s1">tree_c_right = tree.children_right</span>
    <span class="s1">subtree_c_left = subtree.children_left</span>
    <span class="s1">subtree_c_right = subtree.children_right</span>

    <span class="s1">stack = [(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)]</span>
    <span class="s2">while </span><span class="s1">stack:</span>
        <span class="s1">tree_node_idx</span><span class="s2">, </span><span class="s1">subtree_node_idx = stack.pop()</span>
        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">tree.value[tree_node_idx]</span><span class="s2">, </span><span class="s1">subtree.value[subtree_node_idx]</span>
        <span class="s1">)</span>
        <span class="s1">assert_almost_equal(</span>
            <span class="s1">tree.impurity[tree_node_idx]</span><span class="s2">, </span><span class="s1">subtree.impurity[subtree_node_idx]</span>
        <span class="s1">)</span>
        <span class="s1">assert_almost_equal(</span>
            <span class="s1">tree.n_node_samples[tree_node_idx]</span><span class="s2">, </span><span class="s1">subtree.n_node_samples[subtree_node_idx]</span>
        <span class="s1">)</span>
        <span class="s1">assert_almost_equal(</span>
            <span class="s1">tree.weighted_n_node_samples[tree_node_idx]</span><span class="s2">,</span>
            <span class="s1">subtree.weighted_n_node_samples[subtree_node_idx]</span><span class="s2">,</span>
        <span class="s1">)</span>

        <span class="s2">if </span><span class="s1">subtree_c_left[subtree_node_idx] == subtree_c_right[subtree_node_idx]:</span>
            <span class="s5"># is a leaf</span>
            <span class="s1">assert_almost_equal(TREE_UNDEFINED</span><span class="s2">, </span><span class="s1">subtree.threshold[subtree_node_idx])</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s5"># not a leaf</span>
            <span class="s1">assert_almost_equal(</span>
                <span class="s1">tree.threshold[tree_node_idx]</span><span class="s2">, </span><span class="s1">subtree.threshold[subtree_node_idx]</span>
            <span class="s1">)</span>
            <span class="s1">stack.append((tree_c_left[tree_node_idx]</span><span class="s2">, </span><span class="s1">subtree_c_left[subtree_node_idx]))</span>
            <span class="s1">stack.append(</span>
                <span class="s1">(tree_c_right[tree_node_idx]</span><span class="s2">, </span><span class="s1">subtree_c_right[subtree_node_idx])</span>
            <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;name&quot;</span><span class="s2">, </span><span class="s1">ALL_TREES)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;splitter&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;best&quot;</span><span class="s2">, </span><span class="s3">&quot;random&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;X_format&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;dense&quot;</span><span class="s2">, </span><span class="s3">&quot;csr&quot;</span><span class="s2">, </span><span class="s3">&quot;csc&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_apply_path_readonly_all_trees(name</span><span class="s2">, </span><span class="s1">splitter</span><span class="s2">, </span><span class="s1">X_format):</span>
    <span class="s1">dataset = DATASETS[</span><span class="s3">&quot;clf_small&quot;</span><span class="s1">]</span>
    <span class="s1">X_small = dataset[</span><span class="s3">&quot;X&quot;</span><span class="s1">].astype(tree._tree.DTYPE</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">X_format == </span><span class="s3">&quot;dense&quot;</span><span class="s1">:</span>
        <span class="s1">X_readonly = create_memmap_backed_data(X_small)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">X_readonly = dataset[</span><span class="s3">&quot;X_sparse&quot;</span><span class="s1">]  </span><span class="s5"># CSR</span>
        <span class="s2">if </span><span class="s1">X_format == </span><span class="s3">&quot;csc&quot;</span><span class="s1">:</span>
            <span class="s5"># Cheap CSR to CSC conversion</span>
            <span class="s1">X_readonly = X_readonly.tocsc()</span>

        <span class="s1">X_readonly.data = np.array(X_readonly.data</span><span class="s2">, </span><span class="s1">dtype=tree._tree.DTYPE)</span>
        <span class="s1">(</span>
            <span class="s1">X_readonly.data</span><span class="s2">,</span>
            <span class="s1">X_readonly.indices</span><span class="s2">,</span>
            <span class="s1">X_readonly.indptr</span><span class="s2">,</span>
        <span class="s1">) = create_memmap_backed_data(</span>
            <span class="s1">(X_readonly.data</span><span class="s2">, </span><span class="s1">X_readonly.indices</span><span class="s2">, </span><span class="s1">X_readonly.indptr)</span>
        <span class="s1">)</span>

    <span class="s1">y_readonly = create_memmap_backed_data(np.array(y_small</span><span class="s2">, </span><span class="s1">dtype=tree._tree.DTYPE))</span>
    <span class="s1">est = ALL_TREES[name](splitter=splitter)</span>
    <span class="s1">est.fit(X_readonly</span><span class="s2">, </span><span class="s1">y_readonly)</span>
    <span class="s1">assert_array_equal(est.predict(X_readonly)</span><span class="s2">, </span><span class="s1">est.predict(X_small))</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">est.decision_path(X_readonly).todense()</span><span class="s2">, </span><span class="s1">est.decision_path(X_small).todense()</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;criterion&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;squared_error&quot;</span><span class="s2">, </span><span class="s3">&quot;friedman_mse&quot;</span><span class="s2">, </span><span class="s3">&quot;poisson&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;Tree&quot;</span><span class="s2">, </span><span class="s1">REG_TREES.values())</span>
<span class="s2">def </span><span class="s1">test_balance_property(criterion</span><span class="s2">, </span><span class="s1">Tree):</span>
    <span class="s5"># Test that sum(y_pred)=sum(y_true) on training set.</span>
    <span class="s5"># This works if the mean is predicted (should even be true for each leaf).</span>
    <span class="s5"># MAE predicts the median and is therefore excluded from this test.</span>

    <span class="s5"># Choose a training set with non-negative targets (for poisson)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target</span>
    <span class="s1">reg = Tree(criterion=criterion)</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">np.sum(reg.predict(X)) == pytest.approx(np.sum(y))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;seed&quot;</span><span class="s2">, </span><span class="s1">range(</span><span class="s4">3</span><span class="s1">))</span>
<span class="s2">def </span><span class="s1">test_poisson_zero_nodes(seed):</span>
    <span class="s5"># Test that sum(y)=0 and therefore y_pred=0 is forbidden on nodes.</span>
    <span class="s1">X = [[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]</span>
    <span class="s5"># Note that X[:, 0] == 0 is a 100% indicator for y == 0. The tree can</span>
    <span class="s5"># easily learn that:</span>
    <span class="s1">reg = DecisionTreeRegressor(criterion=</span><span class="s3">&quot;squared_error&quot;</span><span class="s2">, </span><span class="s1">random_state=seed)</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">np.amin(reg.predict(X)) == </span><span class="s4">0</span>
    <span class="s5"># whereas Poisson must predict strictly positive numbers</span>
    <span class="s1">reg = DecisionTreeRegressor(criterion=</span><span class="s3">&quot;poisson&quot;</span><span class="s2">, </span><span class="s1">random_state=seed)</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">np.all(reg.predict(X) &gt; </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s5"># Test additional dataset where something could go wrong.</span>
    <span class="s1">n_features = </span><span class="s4">10</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_regression(</span>
        <span class="s1">effective_rank=n_features * </span><span class="s4">2 </span><span class="s1">// </span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">tail_strength=</span><span class="s4">0.6</span><span class="s2">,</span>
        <span class="s1">n_samples=</span><span class="s4">1_000</span><span class="s2">,</span>
        <span class="s1">n_features=n_features</span><span class="s2">,</span>
        <span class="s1">n_informative=n_features * </span><span class="s4">2 </span><span class="s1">// </span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">random_state=seed</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s5"># some excess zeros</span>
    <span class="s1">y[(-</span><span class="s4">1 </span><span class="s1">&lt; y) &amp; (y &lt; </span><span class="s4">0</span><span class="s1">)] = </span><span class="s4">0</span>
    <span class="s5"># make sure the target is positive</span>
    <span class="s1">y = np.abs(y)</span>
    <span class="s1">reg = DecisionTreeRegressor(criterion=</span><span class="s3">&quot;poisson&quot;</span><span class="s2">, </span><span class="s1">random_state=seed)</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">np.all(reg.predict(X) &gt; </span><span class="s4">0</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_poisson_vs_mse():</span>
    <span class="s5"># For a Poisson distributed target, Poisson loss should give better results</span>
    <span class="s5"># than squared error measured in Poisson deviance as metric.</span>
    <span class="s5"># We have a similar test, test_poisson(), in</span>
    <span class="s5"># sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">n_train</span><span class="s2">, </span><span class="s1">n_test</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">500</span><span class="s2">, </span><span class="s4">500</span><span class="s2">, </span><span class="s4">10</span>
    <span class="s1">X = datasets.make_low_rank_matrix(</span>
        <span class="s1">n_samples=n_train + n_test</span><span class="s2">, </span><span class="s1">n_features=n_features</span><span class="s2">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s5"># We create a log-linear Poisson model and downscale coef as it will get</span>
    <span class="s5"># exponentiated.</span>
    <span class="s1">coef = rng.uniform(low=-</span><span class="s4">2</span><span class="s2">, </span><span class="s1">high=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">size=n_features) / np.max(X</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">y = rng.poisson(lam=np.exp(X @ coef))</span>
    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">test_size=n_test</span><span class="s2">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s5"># We prevent some overfitting by setting min_samples_split=10.</span>
    <span class="s1">tree_poi = DecisionTreeRegressor(</span>
        <span class="s1">criterion=</span><span class="s3">&quot;poisson&quot;</span><span class="s2">, </span><span class="s1">min_samples_split=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">tree_mse = DecisionTreeRegressor(</span>
        <span class="s1">criterion=</span><span class="s3">&quot;squared_error&quot;</span><span class="s2">, </span><span class="s1">min_samples_split=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>

    <span class="s1">tree_poi.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s1">tree_mse.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s1">dummy = DummyRegressor(strategy=</span><span class="s3">&quot;mean&quot;</span><span class="s1">).fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>

    <span class="s2">for </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">val </span><span class="s2">in </span><span class="s1">[(X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s3">&quot;train&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(X_test</span><span class="s2">, </span><span class="s1">y_test</span><span class="s2">, </span><span class="s3">&quot;test&quot;</span><span class="s1">)]:</span>
        <span class="s1">metric_poi = mean_poisson_deviance(y</span><span class="s2">, </span><span class="s1">tree_poi.predict(X))</span>
        <span class="s5"># squared_error might produce non-positive predictions =&gt; clip</span>
        <span class="s1">metric_mse = mean_poisson_deviance(y</span><span class="s2">, </span><span class="s1">np.clip(tree_mse.predict(X)</span><span class="s2">, </span><span class="s4">1e-15</span><span class="s2">, None</span><span class="s1">))</span>
        <span class="s1">metric_dummy = mean_poisson_deviance(y</span><span class="s2">, </span><span class="s1">dummy.predict(X))</span>
        <span class="s5"># As squared_error might correctly predict 0 in train set, its train</span>
        <span class="s5"># score can be better than Poisson. This is no longer the case for the</span>
        <span class="s5"># test set.</span>
        <span class="s2">if </span><span class="s1">val == </span><span class="s3">&quot;test&quot;</span><span class="s1">:</span>
            <span class="s2">assert </span><span class="s1">metric_poi &lt; </span><span class="s4">0.5 </span><span class="s1">* metric_mse</span>
        <span class="s2">assert </span><span class="s1">metric_poi &lt; </span><span class="s4">0.75 </span><span class="s1">* metric_dummy</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;criterion&quot;</span><span class="s2">, </span><span class="s1">REG_CRITERIONS)</span>
<span class="s2">def </span><span class="s1">test_decision_tree_regressor_sample_weight_consistency(criterion):</span>
    <span class="s0">&quot;&quot;&quot;Test that the impact of sample_weight is consistent.&quot;&quot;&quot;</span>
    <span class="s1">tree_params = dict(criterion=criterion)</span>
    <span class="s1">tree = DecisionTreeRegressor(**tree_params</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s2">for </span><span class="s1">kind </span><span class="s2">in </span><span class="s1">[</span><span class="s3">&quot;zeros&quot;</span><span class="s2">, </span><span class="s3">&quot;ones&quot;</span><span class="s1">]:</span>
        <span class="s1">check_sample_weights_invariance(</span>
            <span class="s3">&quot;DecisionTreeRegressor_&quot; </span><span class="s1">+ criterion</span><span class="s2">, </span><span class="s1">tree</span><span class="s2">, </span><span class="s1">kind=</span><span class="s3">&quot;zeros&quot;</span>
        <span class="s1">)</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">10</span><span class="s2">, </span><span class="s4">5</span>

    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">y = np.mean(X</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">) + rng.rand(n_samples)</span>
    <span class="s5"># make it positive in order to work also for poisson criterion</span>
    <span class="s1">y += np.min(y) + </span><span class="s4">0.1</span>

    <span class="s5"># check that multiplying sample_weight by 2 is equivalent</span>
    <span class="s5"># to repeating corresponding samples twice</span>
    <span class="s1">X2 = np.concatenate([X</span><span class="s2">, </span><span class="s1">X[: n_samples // </span><span class="s4">2</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">y2 = np.concatenate([y</span><span class="s2">, </span><span class="s1">y[: n_samples // </span><span class="s4">2</span><span class="s1">]])</span>
    <span class="s1">sample_weight_1 = np.ones(len(y))</span>
    <span class="s1">sample_weight_1[: n_samples // </span><span class="s4">2</span><span class="s1">] = </span><span class="s4">2</span>

    <span class="s1">tree1 = DecisionTreeRegressor(**tree_params).fit(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight_1</span>
    <span class="s1">)</span>

    <span class="s1">tree2 = DecisionTreeRegressor(**tree_params).fit(X2</span><span class="s2">, </span><span class="s1">y2</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">)</span>

    <span class="s2">assert </span><span class="s1">tree1.tree_.node_count == tree2.tree_.node_count</span>
    <span class="s5"># Thresholds, tree.tree_.threshold, and values, tree.tree_.value, are not</span>
    <span class="s5"># exactly the same, but on the training set, those differences do not</span>
    <span class="s5"># matter and thus predictions are the same.</span>
    <span class="s1">assert_allclose(tree1.predict(X)</span><span class="s2">, </span><span class="s1">tree2.predict(X))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;Tree&quot;</span><span class="s2">, </span><span class="s1">[DecisionTreeClassifier</span><span class="s2">, </span><span class="s1">ExtraTreeClassifier])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;n_classes&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">4</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_criterion_entropy_same_as_log_loss(Tree</span><span class="s2">, </span><span class="s1">n_classes):</span>
    <span class="s0">&quot;&quot;&quot;Test that criterion=entropy gives same as log_loss.&quot;&quot;&quot;</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">50</span><span class="s2">, </span><span class="s4">5</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_classification(</span>
        <span class="s1">n_classes=n_classes</span><span class="s2">,</span>
        <span class="s1">n_samples=n_samples</span><span class="s2">,</span>
        <span class="s1">n_features=n_features</span><span class="s2">,</span>
        <span class="s1">n_informative=n_features</span><span class="s2">,</span>
        <span class="s1">n_redundant=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s4">42</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">tree_log_loss = Tree(criterion=</span><span class="s3">&quot;log_loss&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">43</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">tree_entropy = Tree(criterion=</span><span class="s3">&quot;entropy&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">43</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">assert_tree_equal(</span>
        <span class="s1">tree_log_loss.tree_</span><span class="s2">,</span>
        <span class="s1">tree_entropy.tree_</span><span class="s2">,</span>
        <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">Tree</span><span class="s2">!r} </span><span class="s3">with criterion 'entropy' and 'log_loss' gave different trees.&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(tree_log_loss.predict(X)</span><span class="s2">, </span><span class="s1">tree_entropy.predict(X))</span>


<span class="s2">def </span><span class="s1">test_different_endianness_pickle():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_classification(random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">clf = DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">score = clf.score(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">def </span><span class="s1">reduce_ndarray(arr):</span>
        <span class="s2">return </span><span class="s1">arr.byteswap().newbyteorder().__reduce__()</span>

    <span class="s2">def </span><span class="s1">get_pickle_non_native_endianness():</span>
        <span class="s1">f = io.BytesIO()</span>
        <span class="s1">p = pickle.Pickler(f)</span>
        <span class="s1">p.dispatch_table = copyreg.dispatch_table.copy()</span>
        <span class="s1">p.dispatch_table[np.ndarray] = reduce_ndarray</span>

        <span class="s1">p.dump(clf)</span>
        <span class="s1">f.seek(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">f</span>

    <span class="s1">new_clf = pickle.load(get_pickle_non_native_endianness())</span>
    <span class="s1">new_score = new_clf.score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">np.isclose(score</span><span class="s2">, </span><span class="s1">new_score)</span>


<span class="s2">def </span><span class="s1">test_different_endianness_joblib_pickle():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_classification(random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">clf = DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">score = clf.score(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">class </span><span class="s1">NonNativeEndiannessNumpyPickler(NumpyPickler):</span>
        <span class="s2">def </span><span class="s1">save(self</span><span class="s2">, </span><span class="s1">obj):</span>
            <span class="s2">if </span><span class="s1">isinstance(obj</span><span class="s2">, </span><span class="s1">np.ndarray):</span>
                <span class="s1">obj = obj.byteswap().newbyteorder()</span>
            <span class="s1">super().save(obj)</span>

    <span class="s2">def </span><span class="s1">get_joblib_pickle_non_native_endianness():</span>
        <span class="s1">f = io.BytesIO()</span>
        <span class="s1">p = NonNativeEndiannessNumpyPickler(f)</span>

        <span class="s1">p.dump(clf)</span>
        <span class="s1">f.seek(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">f</span>

    <span class="s1">new_clf = joblib.load(get_joblib_pickle_non_native_endianness())</span>
    <span class="s1">new_score = new_clf.score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">np.isclose(score</span><span class="s2">, </span><span class="s1">new_score)</span>


<span class="s2">def </span><span class="s1">get_different_bitness_node_ndarray(node_ndarray):</span>
    <span class="s1">new_dtype_for_indexing_fields = np.int64 </span><span class="s2">if </span><span class="s1">_IS_32BIT </span><span class="s2">else </span><span class="s1">np.int32</span>

    <span class="s5"># field names in Node struct with SIZE_t types (see sklearn/tree/_tree.pxd)</span>
    <span class="s1">indexing_field_names = [</span><span class="s3">&quot;left_child&quot;</span><span class="s2">, </span><span class="s3">&quot;right_child&quot;</span><span class="s2">, </span><span class="s3">&quot;feature&quot;</span><span class="s2">, </span><span class="s3">&quot;n_node_samples&quot;</span><span class="s1">]</span>

    <span class="s1">new_dtype_dict = {</span>
        <span class="s1">name: dtype </span><span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">(dtype</span><span class="s2">, </span><span class="s1">_) </span><span class="s2">in </span><span class="s1">node_ndarray.dtype.fields.items()</span>
    <span class="s1">}</span>
    <span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">indexing_field_names:</span>
        <span class="s1">new_dtype_dict[name] = new_dtype_for_indexing_fields</span>

    <span class="s1">new_dtype = np.dtype(</span>
        <span class="s1">{</span><span class="s3">&quot;names&quot;</span><span class="s1">: list(new_dtype_dict.keys())</span><span class="s2">, </span><span class="s3">&quot;formats&quot;</span><span class="s1">: list(new_dtype_dict.values())}</span>
    <span class="s1">)</span>
    <span class="s2">return </span><span class="s1">node_ndarray.astype(new_dtype</span><span class="s2">, </span><span class="s1">casting=</span><span class="s3">&quot;same_kind&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">get_different_alignment_node_ndarray(node_ndarray):</span>
    <span class="s1">new_dtype_dict = {</span>
        <span class="s1">name: dtype </span><span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">(dtype</span><span class="s2">, </span><span class="s1">_) </span><span class="s2">in </span><span class="s1">node_ndarray.dtype.fields.items()</span>
    <span class="s1">}</span>
    <span class="s1">offsets = [offset </span><span class="s2">for </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">offset </span><span class="s2">in </span><span class="s1">node_ndarray.dtype.fields.values()]</span>
    <span class="s1">shifted_offsets = [</span><span class="s4">8 </span><span class="s1">+ offset </span><span class="s2">for </span><span class="s1">offset </span><span class="s2">in </span><span class="s1">offsets]</span>

    <span class="s1">new_dtype = np.dtype(</span>
        <span class="s1">{</span>
            <span class="s3">&quot;names&quot;</span><span class="s1">: list(new_dtype_dict.keys())</span><span class="s2">,</span>
            <span class="s3">&quot;formats&quot;</span><span class="s1">: list(new_dtype_dict.values())</span><span class="s2">,</span>
            <span class="s3">&quot;offsets&quot;</span><span class="s1">: shifted_offsets</span><span class="s2">,</span>
        <span class="s1">}</span>
    <span class="s1">)</span>
    <span class="s2">return </span><span class="s1">node_ndarray.astype(new_dtype</span><span class="s2">, </span><span class="s1">casting=</span><span class="s3">&quot;same_kind&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">reduce_tree_with_different_bitness(tree):</span>
    <span class="s1">new_dtype = np.int64 </span><span class="s2">if </span><span class="s1">_IS_32BIT </span><span class="s2">else </span><span class="s1">np.int32</span>
    <span class="s1">tree_cls</span><span class="s2">, </span><span class="s1">(n_features</span><span class="s2">, </span><span class="s1">n_classes</span><span class="s2">, </span><span class="s1">n_outputs)</span><span class="s2">, </span><span class="s1">state = tree.__reduce__()</span>
    <span class="s1">new_n_classes = n_classes.astype(new_dtype</span><span class="s2">, </span><span class="s1">casting=</span><span class="s3">&quot;same_kind&quot;</span><span class="s1">)</span>

    <span class="s1">new_state = state.copy()</span>
    <span class="s1">new_state[</span><span class="s3">&quot;nodes&quot;</span><span class="s1">] = get_different_bitness_node_ndarray(new_state[</span><span class="s3">&quot;nodes&quot;</span><span class="s1">])</span>

    <span class="s2">return </span><span class="s1">(tree_cls</span><span class="s2">, </span><span class="s1">(n_features</span><span class="s2">, </span><span class="s1">new_n_classes</span><span class="s2">, </span><span class="s1">n_outputs)</span><span class="s2">, </span><span class="s1">new_state)</span>


<span class="s2">def </span><span class="s1">test_different_bitness_pickle():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_classification(random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">clf = DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">score = clf.score(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">def </span><span class="s1">pickle_dump_with_different_bitness():</span>
        <span class="s1">f = io.BytesIO()</span>
        <span class="s1">p = pickle.Pickler(f)</span>
        <span class="s1">p.dispatch_table = copyreg.dispatch_table.copy()</span>
        <span class="s1">p.dispatch_table[CythonTree] = reduce_tree_with_different_bitness</span>

        <span class="s1">p.dump(clf)</span>
        <span class="s1">f.seek(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">f</span>

    <span class="s1">new_clf = pickle.load(pickle_dump_with_different_bitness())</span>
    <span class="s1">new_score = new_clf.score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">score == pytest.approx(new_score)</span>


<span class="s2">def </span><span class="s1">test_different_bitness_joblib_pickle():</span>
    <span class="s5"># Make sure that a platform specific pickle generated on a 64 bit</span>
    <span class="s5"># platform can be converted at pickle load time into an estimator</span>
    <span class="s5"># with Cython code that works with the host's native integer precision</span>
    <span class="s5"># to index nodes in the tree data structure when the host is a 32 bit</span>
    <span class="s5"># platform (and vice versa).</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = datasets.make_classification(random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">clf = DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">score = clf.score(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">def </span><span class="s1">joblib_dump_with_different_bitness():</span>
        <span class="s1">f = io.BytesIO()</span>
        <span class="s1">p = NumpyPickler(f)</span>
        <span class="s1">p.dispatch_table = copyreg.dispatch_table.copy()</span>
        <span class="s1">p.dispatch_table[CythonTree] = reduce_tree_with_different_bitness</span>

        <span class="s1">p.dump(clf)</span>
        <span class="s1">f.seek(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">f</span>

    <span class="s1">new_clf = joblib.load(joblib_dump_with_different_bitness())</span>
    <span class="s1">new_score = new_clf.score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">score == pytest.approx(new_score)</span>


<span class="s2">def </span><span class="s1">test_check_n_classes():</span>
    <span class="s1">expected_dtype = np.dtype(np.int32) </span><span class="s2">if </span><span class="s1">_IS_32BIT </span><span class="s2">else </span><span class="s1">np.dtype(np.int64)</span>
    <span class="s1">allowed_dtypes = [np.dtype(np.int32)</span><span class="s2">, </span><span class="s1">np.dtype(np.int64)]</span>
    <span class="s1">allowed_dtypes += [dt.newbyteorder() </span><span class="s2">for </span><span class="s1">dt </span><span class="s2">in </span><span class="s1">allowed_dtypes]</span>

    <span class="s1">n_classes = np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=expected_dtype)</span>
    <span class="s2">for </span><span class="s1">dt </span><span class="s2">in </span><span class="s1">allowed_dtypes:</span>
        <span class="s1">_check_n_classes(n_classes.astype(dt)</span><span class="s2">, </span><span class="s1">expected_dtype)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;Wrong dimensions.+n_classes&quot;</span><span class="s1">):</span>
        <span class="s1">wrong_dim_n_classes = np.array([[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">dtype=expected_dtype)</span>
        <span class="s1">_check_n_classes(wrong_dim_n_classes</span><span class="s2">, </span><span class="s1">expected_dtype)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;n_classes.+incompatible dtype&quot;</span><span class="s1">):</span>
        <span class="s1">wrong_dtype_n_classes = n_classes.astype(np.float64)</span>
        <span class="s1">_check_n_classes(wrong_dtype_n_classes</span><span class="s2">, </span><span class="s1">expected_dtype)</span>


<span class="s2">def </span><span class="s1">test_check_value_ndarray():</span>
    <span class="s1">expected_dtype = np.dtype(np.float64)</span>
    <span class="s1">expected_shape = (</span><span class="s4">5</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">value_ndarray = np.zeros(expected_shape</span><span class="s2">, </span><span class="s1">dtype=expected_dtype)</span>

    <span class="s1">allowed_dtypes = [expected_dtype</span><span class="s2">, </span><span class="s1">expected_dtype.newbyteorder()]</span>

    <span class="s2">for </span><span class="s1">dt </span><span class="s2">in </span><span class="s1">allowed_dtypes:</span>
        <span class="s1">_check_value_ndarray(</span>
            <span class="s1">value_ndarray</span><span class="s2">, </span><span class="s1">expected_dtype=dt</span><span class="s2">, </span><span class="s1">expected_shape=expected_shape</span>
        <span class="s1">)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;Wrong shape.+value array&quot;</span><span class="s1">):</span>
        <span class="s1">_check_value_ndarray(</span>
            <span class="s1">value_ndarray</span><span class="s2">, </span><span class="s1">expected_dtype=expected_dtype</span><span class="s2">, </span><span class="s1">expected_shape=(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">)</span>

    <span class="s2">for </span><span class="s1">problematic_arr </span><span class="s2">in </span><span class="s1">[value_ndarray[:</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">:</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">np.asfortranarray(value_ndarray)]:</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;value array.+C-contiguous&quot;</span><span class="s1">):</span>
            <span class="s1">_check_value_ndarray(</span>
                <span class="s1">problematic_arr</span><span class="s2">,</span>
                <span class="s1">expected_dtype=expected_dtype</span><span class="s2">,</span>
                <span class="s1">expected_shape=problematic_arr.shape</span><span class="s2">,</span>
            <span class="s1">)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;value array.+incompatible dtype&quot;</span><span class="s1">):</span>
        <span class="s1">_check_value_ndarray(</span>
            <span class="s1">value_ndarray.astype(np.float32)</span><span class="s2">,</span>
            <span class="s1">expected_dtype=expected_dtype</span><span class="s2">,</span>
            <span class="s1">expected_shape=expected_shape</span><span class="s2">,</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_check_node_ndarray():</span>
    <span class="s1">expected_dtype = NODE_DTYPE</span>

    <span class="s1">node_ndarray = np.zeros((</span><span class="s4">5</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype=expected_dtype)</span>

    <span class="s1">valid_node_ndarrays = [</span>
        <span class="s1">node_ndarray</span><span class="s2">,</span>
        <span class="s1">get_different_bitness_node_ndarray(node_ndarray)</span><span class="s2">,</span>
        <span class="s1">get_different_alignment_node_ndarray(node_ndarray)</span><span class="s2">,</span>
    <span class="s1">]</span>
    <span class="s1">valid_node_ndarrays += [</span>
        <span class="s1">arr.astype(arr.dtype.newbyteorder()) </span><span class="s2">for </span><span class="s1">arr </span><span class="s2">in </span><span class="s1">valid_node_ndarrays</span>
    <span class="s1">]</span>

    <span class="s2">for </span><span class="s1">arr </span><span class="s2">in </span><span class="s1">valid_node_ndarrays:</span>
        <span class="s1">_check_node_ndarray(node_ndarray</span><span class="s2">, </span><span class="s1">expected_dtype=expected_dtype)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;Wrong dimensions.+node array&quot;</span><span class="s1">):</span>
        <span class="s1">problematic_node_ndarray = np.zeros((</span><span class="s4">5</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype=expected_dtype)</span>
        <span class="s1">_check_node_ndarray(problematic_node_ndarray</span><span class="s2">, </span><span class="s1">expected_dtype=expected_dtype)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;node array.+C-contiguous&quot;</span><span class="s1">):</span>
        <span class="s1">problematic_node_ndarray = node_ndarray[::</span><span class="s4">2</span><span class="s1">]</span>
        <span class="s1">_check_node_ndarray(problematic_node_ndarray</span><span class="s2">, </span><span class="s1">expected_dtype=expected_dtype)</span>

    <span class="s1">dtype_dict = {name: dtype </span><span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">(dtype</span><span class="s2">, </span><span class="s1">_) </span><span class="s2">in </span><span class="s1">node_ndarray.dtype.fields.items()}</span>

    <span class="s5"># array with wrong 'threshold' field dtype (int64 rather than float64)</span>
    <span class="s1">new_dtype_dict = dtype_dict.copy()</span>
    <span class="s1">new_dtype_dict[</span><span class="s3">&quot;threshold&quot;</span><span class="s1">] = np.int64</span>

    <span class="s1">new_dtype = np.dtype(</span>
        <span class="s1">{</span><span class="s3">&quot;names&quot;</span><span class="s1">: list(new_dtype_dict.keys())</span><span class="s2">, </span><span class="s3">&quot;formats&quot;</span><span class="s1">: list(new_dtype_dict.values())}</span>
    <span class="s1">)</span>
    <span class="s1">problematic_node_ndarray = node_ndarray.astype(new_dtype)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;node array.+incompatible dtype&quot;</span><span class="s1">):</span>
        <span class="s1">_check_node_ndarray(problematic_node_ndarray</span><span class="s2">, </span><span class="s1">expected_dtype=expected_dtype)</span>

    <span class="s5"># array with wrong 'left_child' field dtype (float64 rather than int64 or int32)</span>
    <span class="s1">new_dtype_dict = dtype_dict.copy()</span>
    <span class="s1">new_dtype_dict[</span><span class="s3">&quot;left_child&quot;</span><span class="s1">] = np.float64</span>
    <span class="s1">new_dtype = np.dtype(</span>
        <span class="s1">{</span><span class="s3">&quot;names&quot;</span><span class="s1">: list(new_dtype_dict.keys())</span><span class="s2">, </span><span class="s3">&quot;formats&quot;</span><span class="s1">: list(new_dtype_dict.values())}</span>
    <span class="s1">)</span>

    <span class="s1">problematic_node_ndarray = node_ndarray.astype(new_dtype)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;node array.+incompatible dtype&quot;</span><span class="s1">):</span>
        <span class="s1">_check_node_ndarray(problematic_node_ndarray</span><span class="s2">, </span><span class="s1">expected_dtype=expected_dtype)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;Splitter&quot;</span><span class="s2">, </span><span class="s1">chain(DENSE_SPLITTERS.values()</span><span class="s2">, </span><span class="s1">SPARSE_SPLITTERS.values())</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_splitter_serializable(Splitter):</span>
    <span class="s0">&quot;&quot;&quot;Check that splitters are serializable.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">max_features = </span><span class="s4">10</span>
    <span class="s1">n_outputs</span><span class="s2">, </span><span class="s1">n_classes = </span><span class="s4">2</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.intp)</span>

    <span class="s1">criterion = CRITERIA_CLF[</span><span class="s3">&quot;gini&quot;</span><span class="s1">](n_outputs</span><span class="s2">, </span><span class="s1">n_classes)</span>
    <span class="s1">splitter = Splitter(criterion</span><span class="s2">, </span><span class="s1">max_features</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s1">rng)</span>
    <span class="s1">splitter_serialize = pickle.dumps(splitter)</span>

    <span class="s1">splitter_back = pickle.loads(splitter_serialize)</span>
    <span class="s2">assert </span><span class="s1">splitter_back.max_features == max_features</span>
    <span class="s2">assert </span><span class="s1">isinstance(splitter_back</span><span class="s2">, </span><span class="s1">Splitter)</span>


<span class="s2">def </span><span class="s1">test_tree_deserialization_from_read_only_buffer(tmpdir):</span>
    <span class="s0">&quot;&quot;&quot;Check that Trees can be deserialized with read only buffers. 
 
    Non-regression test for gh-25584. 
    &quot;&quot;&quot;</span>
    <span class="s1">pickle_path = str(tmpdir.join(</span><span class="s3">&quot;clf.joblib&quot;</span><span class="s1">))</span>
    <span class="s1">clf = DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_small</span><span class="s2">, </span><span class="s1">y_small)</span>

    <span class="s1">joblib.dump(clf</span><span class="s2">, </span><span class="s1">pickle_path)</span>
    <span class="s1">loaded_clf = joblib.load(pickle_path</span><span class="s2">, </span><span class="s1">mmap_mode=</span><span class="s3">&quot;r&quot;</span><span class="s1">)</span>

    <span class="s1">assert_tree_equal(</span>
        <span class="s1">loaded_clf.tree_</span><span class="s2">,</span>
        <span class="s1">clf.tree_</span><span class="s2">,</span>
        <span class="s3">&quot;The trees of the original and loaded classifiers are not equal.&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;Tree&quot;</span><span class="s2">, </span><span class="s1">ALL_TREES.values())</span>
<span class="s2">def </span><span class="s1">test_min_sample_split_1_error(Tree):</span>
    <span class="s0">&quot;&quot;&quot;Check that an error is raised when min_sample_split=1. 
 
    non-regression test for issue gh-25481. 
    &quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>

    <span class="s5"># min_samples_split=1.0 is valid</span>
    <span class="s1">Tree(min_samples_split=</span><span class="s4">1.0</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s5"># min_samples_split=1 is invalid</span>
    <span class="s1">tree = Tree(min_samples_split=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">msg = (</span>
        <span class="s3">r&quot;'min_samples_split' .* must be an int in the range \[2, inf\) &quot;</span>
        <span class="s3">r&quot;or a float in the range \(0.0, 1.0\]&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">tree.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;criterion&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;squared_error&quot;</span><span class="s2">, </span><span class="s3">&quot;friedman_mse&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_missing_values_on_equal_nodes_no_missing(criterion):</span>
    <span class="s0">&quot;&quot;&quot;Check missing values goes to correct node during predictions&quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">8</span><span class="s2">, </span><span class="s4">9</span><span class="s2">, </span><span class="s4">11</span><span class="s2">, </span><span class="s4">12</span><span class="s2">, </span><span class="s4">15</span><span class="s1">]]).T</span>
    <span class="s1">y = np.array([</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.2</span><span class="s2">, </span><span class="s4">0.3</span><span class="s2">, </span><span class="s4">0.2</span><span class="s2">, </span><span class="s4">1.4</span><span class="s2">, </span><span class="s4">1.4</span><span class="s2">, </span><span class="s4">1.5</span><span class="s2">, </span><span class="s4">1.6</span><span class="s2">, </span><span class="s4">2.6</span><span class="s1">])</span>

    <span class="s1">dtc = DecisionTreeRegressor(random_state=</span><span class="s4">42</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">criterion=criterion)</span>
    <span class="s1">dtc.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s5"># Goes to right node because it has the most data points</span>
    <span class="s1">y_pred = dtc.predict([[np.nan]])</span>
    <span class="s1">assert_allclose(y_pred</span><span class="s2">, </span><span class="s1">[np.mean(y[-</span><span class="s4">5</span><span class="s1">:])])</span>

    <span class="s5"># equal number of elements in both nodes</span>
    <span class="s1">X_equal = X[:-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">y_equal = y[:-</span><span class="s4">1</span><span class="s1">]</span>

    <span class="s1">dtc = DecisionTreeRegressor(random_state=</span><span class="s4">42</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">criterion=criterion)</span>
    <span class="s1">dtc.fit(X_equal</span><span class="s2">, </span><span class="s1">y_equal)</span>

    <span class="s5"># Goes to right node because the implementation sets:</span>
    <span class="s5"># missing_go_to_left = n_left &gt; n_right, which is False</span>
    <span class="s1">y_pred = dtc.predict([[np.nan]])</span>
    <span class="s1">assert_allclose(y_pred</span><span class="s2">, </span><span class="s1">[np.mean(y_equal[-</span><span class="s4">4</span><span class="s1">:])])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;criterion&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;entropy&quot;</span><span class="s2">, </span><span class="s3">&quot;gini&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_missing_values_best_splitter_three_classes(criterion):</span>
    <span class="s0">&quot;&quot;&quot;Test when missing values are uniquely present in a class among 3 classes.&quot;&quot;&quot;</span>
    <span class="s1">missing_values_class = </span><span class="s4">0</span>
    <span class="s1">X = np.array([[np.nan] * </span><span class="s4">4 </span><span class="s1">+ [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">8</span><span class="s2">, </span><span class="s4">9</span><span class="s2">, </span><span class="s4">11</span><span class="s2">, </span><span class="s4">12</span><span class="s1">]]).T</span>
    <span class="s1">y = np.array([missing_values_class] * </span><span class="s4">4 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">4 </span><span class="s1">+ [</span><span class="s4">2</span><span class="s1">] * </span><span class="s4">4</span><span class="s1">)</span>
    <span class="s1">dtc = DecisionTreeClassifier(random_state=</span><span class="s4">42</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">criterion=criterion)</span>
    <span class="s1">dtc.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">X_test = np.array([[np.nan</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">12</span><span class="s1">]]).T</span>
    <span class="s1">y_nan_pred = dtc.predict(X_test)</span>
    <span class="s5"># Missing values necessarily are associated to the observed class.</span>
    <span class="s1">assert_array_equal(y_nan_pred</span><span class="s2">, </span><span class="s1">[missing_values_class</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;criterion&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;entropy&quot;</span><span class="s2">, </span><span class="s3">&quot;gini&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_missing_values_best_splitter_to_left(criterion):</span>
    <span class="s0">&quot;&quot;&quot;Missing values spanning only one class at fit-time must make missing 
    values at predict-time be classified has belonging to this class.&quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[np.nan] * </span><span class="s4">4 </span><span class="s1">+ [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">5</span><span class="s1">]]).T</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">4 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">6</span><span class="s1">)</span>

    <span class="s1">dtc = DecisionTreeClassifier(random_state=</span><span class="s4">42</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">criterion=criterion)</span>
    <span class="s1">dtc.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">X_test = np.array([[np.nan</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s1">np.nan]]).T</span>
    <span class="s1">y_pred = dtc.predict(X_test)</span>

    <span class="s1">assert_array_equal(y_pred</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;criterion&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;entropy&quot;</span><span class="s2">, </span><span class="s3">&quot;gini&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_missing_values_best_splitter_to_right(criterion):</span>
    <span class="s0">&quot;&quot;&quot;Missing values and non-missing values sharing one class at fit-time 
    must make missing values at predict-time be classified has belonging 
    to this class.&quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[np.nan] * </span><span class="s4">4 </span><span class="s1">+ [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">5</span><span class="s1">]]).T</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">4 </span><span class="s1">+ [</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">4 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">2</span><span class="s1">)</span>

    <span class="s1">dtc = DecisionTreeClassifier(random_state=</span><span class="s4">42</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">criterion=criterion)</span>
    <span class="s1">dtc.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">X_test = np.array([[np.nan</span><span class="s2">, </span><span class="s4">1.2</span><span class="s2">, </span><span class="s4">4.8</span><span class="s1">]]).T</span>
    <span class="s1">y_pred = dtc.predict(X_test)</span>

    <span class="s1">assert_array_equal(y_pred</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;criterion&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;entropy&quot;</span><span class="s2">, </span><span class="s3">&quot;gini&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_missing_values_missing_both_classes_has_nan(criterion):</span>
    <span class="s0">&quot;&quot;&quot;Check behavior of missing value when there is one missing value in each class.&quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s1">np.nan</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">20</span><span class="s2">, </span><span class="s4">30</span><span class="s2">, </span><span class="s4">60</span><span class="s2">, </span><span class="s1">np.nan]]).T</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">5 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">5</span><span class="s1">)</span>

    <span class="s1">dtc = DecisionTreeClassifier(random_state=</span><span class="s4">42</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">criterion=criterion)</span>
    <span class="s1">dtc.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">X_test = np.array([[np.nan</span><span class="s2">, </span><span class="s4">2.3</span><span class="s2">, </span><span class="s4">34.2</span><span class="s1">]]).T</span>
    <span class="s1">y_pred = dtc.predict(X_test)</span>

    <span class="s5"># Missing value goes to the class at the right (here 1) because the implementation</span>
    <span class="s5"># searches right first.</span>
    <span class="s1">assert_array_equal(y_pred</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;is_sparse&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;tree&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">DecisionTreeClassifier(splitter=</span><span class="s3">&quot;random&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">DecisionTreeRegressor(criterion=</span><span class="s3">&quot;absolute_error&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_missing_value_errors(is_sparse</span><span class="s2">, </span><span class="s1">tree):</span>
    <span class="s0">&quot;&quot;&quot;Check unsupported configurations for missing values.&quot;&quot;&quot;</span>

    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s1">np.nan</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">20</span><span class="s2">, </span><span class="s4">30</span><span class="s2">, </span><span class="s4">60</span><span class="s2">, </span><span class="s1">np.nan]]).T</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">5 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">5</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">is_sparse:</span>
        <span class="s1">X = csr_matrix(X)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;Input X contains NaN&quot;</span><span class="s1">):</span>
        <span class="s1">tree.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_missing_values_poisson():</span>
    <span class="s0">&quot;&quot;&quot;Smoke test for poisson regression and missing values.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = diabetes.data.copy()</span><span class="s2">, </span><span class="s1">diabetes.target</span>

    <span class="s5"># Set some values missing</span>
    <span class="s1">X[::</span><span class="s4">5</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = np.nan</span>
    <span class="s1">X[::</span><span class="s4">6</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">] = np.nan</span>

    <span class="s1">reg = DecisionTreeRegressor(criterion=</span><span class="s3">&quot;poisson&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">y_pred = reg.predict(X)</span>
    <span class="s2">assert </span><span class="s1">(y_pred &gt;= </span><span class="s4">0.0</span><span class="s1">).all()</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;make_data, Tree&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(datasets.make_regression</span><span class="s2">, </span><span class="s1">DecisionTreeRegressor)</span><span class="s2">,</span>
        <span class="s1">(datasets.make_classification</span><span class="s2">, </span><span class="s1">DecisionTreeClassifier)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;sample_weight_train&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">None, </span><span class="s3">&quot;ones&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_missing_values_is_resilience(make_data</span><span class="s2">, </span><span class="s1">Tree</span><span class="s2">, </span><span class="s1">sample_weight_train):</span>
    <span class="s0">&quot;&quot;&quot;Check that trees can deal with missing values and have decent performance.&quot;&quot;&quot;</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">1000</span><span class="s2">, </span><span class="s4">50</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_data(n_samples=n_samples</span><span class="s2">, </span><span class="s1">n_features=n_features</span><span class="s2">, </span><span class="s1">random_state=rng)</span>

    <span class="s5"># Create dataset with missing values</span>
    <span class="s1">X_missing = X.copy()</span>
    <span class="s1">X_missing[rng.choice([</span><span class="s2">False, True</span><span class="s1">]</span><span class="s2">, </span><span class="s1">size=X.shape</span><span class="s2">, </span><span class="s1">p=[</span><span class="s4">0.9</span><span class="s2">, </span><span class="s4">0.1</span><span class="s1">])] = np.nan</span>
    <span class="s1">X_missing_train</span><span class="s2">, </span><span class="s1">X_missing_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">X_missing</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>

    <span class="s2">if </span><span class="s1">sample_weight_train == </span><span class="s3">&quot;ones&quot;</span><span class="s1">:</span>
        <span class="s1">sample_weight_train = np.ones(X_missing_train.shape[</span><span class="s4">0</span><span class="s1">])</span>

    <span class="s5"># Train tree with missing values</span>
    <span class="s1">tree_with_missing = Tree(random_state=rng)</span>
    <span class="s1">tree_with_missing.fit(X_missing_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight_train)</span>
    <span class="s1">score_with_missing = tree_with_missing.score(X_missing_test</span><span class="s2">, </span><span class="s1">y_test)</span>

    <span class="s5"># Train tree without missing values</span>
    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test = train_test_split(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">tree = Tree(random_state=rng)</span>
    <span class="s1">tree.fit(X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight_train)</span>
    <span class="s1">score_without_missing = tree.score(X_test</span><span class="s2">, </span><span class="s1">y_test)</span>

    <span class="s5"># Score is still 90 percent of the tree's score that had no missing values</span>
    <span class="s2">assert </span><span class="s1">score_with_missing &gt;= </span><span class="s4">0.9 </span><span class="s1">* score_without_missing</span>


<span class="s2">def </span><span class="s1">test_missing_value_is_predictive():</span>
    <span class="s0">&quot;&quot;&quot;Check the tree learns when only the missing value is predictive.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s4">1000</span>

    <span class="s1">X = rng.standard_normal(size=(n_samples</span><span class="s2">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">y = rng.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">high=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">size=n_samples)</span>

    <span class="s5"># Create a predictive feature using `y` and with some noise</span>
    <span class="s1">X_random_mask = rng.choice([</span><span class="s2">False, True</span><span class="s1">]</span><span class="s2">, </span><span class="s1">size=n_samples</span><span class="s2">, </span><span class="s1">p=[</span><span class="s4">0.95</span><span class="s2">, </span><span class="s4">0.05</span><span class="s1">])</span>
    <span class="s1">y_mask = y.copy().astype(bool)</span>
    <span class="s1">y_mask[X_random_mask] = ~y_mask[X_random_mask]</span>

    <span class="s1">X_predictive = rng.standard_normal(size=n_samples)</span>
    <span class="s1">X_predictive[y_mask] = np.nan</span>

    <span class="s1">X[:</span><span class="s2">, </span><span class="s4">5</span><span class="s1">] = X_predictive</span>

    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test = train_test_split(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">random_state=rng)</span>
    <span class="s1">tree = DecisionTreeClassifier(random_state=rng).fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>

    <span class="s2">assert </span><span class="s1">tree.score(X_train</span><span class="s2">, </span><span class="s1">y_train) &gt;= </span><span class="s4">0.85</span>
    <span class="s2">assert </span><span class="s1">tree.score(X_test</span><span class="s2">, </span><span class="s1">y_test) &gt;= </span><span class="s4">0.85</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;make_data, Tree&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(datasets.make_regression</span><span class="s2">, </span><span class="s1">DecisionTreeRegressor)</span><span class="s2">,</span>
        <span class="s1">(datasets.make_classification</span><span class="s2">, </span><span class="s1">DecisionTreeClassifier)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_sample_weight_non_uniform(make_data</span><span class="s2">, </span><span class="s1">Tree):</span>
    <span class="s0">&quot;&quot;&quot;Check sample weight is correctly handled with missing values.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">1000</span><span class="s2">, </span><span class="s4">10</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_data(n_samples=n_samples</span><span class="s2">, </span><span class="s1">n_features=n_features</span><span class="s2">, </span><span class="s1">random_state=rng)</span>

    <span class="s5"># Create dataset with missing values</span>
    <span class="s1">X[rng.choice([</span><span class="s2">False, True</span><span class="s1">]</span><span class="s2">, </span><span class="s1">size=X.shape</span><span class="s2">, </span><span class="s1">p=[</span><span class="s4">0.9</span><span class="s2">, </span><span class="s4">0.1</span><span class="s1">])] = np.nan</span>

    <span class="s5"># Zero sample weight is the same as removing the sample</span>
    <span class="s1">sample_weight = np.ones(X.shape[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">sample_weight[::</span><span class="s4">2</span><span class="s1">] = </span><span class="s4">0.0</span>

    <span class="s1">tree_with_sw = Tree(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">tree_with_sw.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">tree_samples_removed = Tree(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">tree_samples_removed.fit(X[</span><span class="s4">1</span><span class="s1">::</span><span class="s4">2</span><span class="s2">, </span><span class="s1">:]</span><span class="s2">, </span><span class="s1">y[</span><span class="s4">1</span><span class="s1">::</span><span class="s4">2</span><span class="s1">])</span>

    <span class="s1">assert_allclose(tree_samples_removed.predict(X)</span><span class="s2">, </span><span class="s1">tree_with_sw.predict(X))</span>


<span class="s2">def </span><span class="s1">test_deterministic_pickle():</span>
    <span class="s5"># Non-regression test for:</span>
    <span class="s5"># https://github.com/scikit-learn/scikit-learn/issues/27268</span>
    <span class="s5"># Uninitialised memory would lead to the two pickle strings being different.</span>
    <span class="s1">tree1 = DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">).fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">tree2 = DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">).fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>

    <span class="s1">pickle1 = pickle.dumps(tree1)</span>
    <span class="s1">pickle2 = pickle.dumps(tree2)</span>

    <span class="s2">assert </span><span class="s1">pickle1 == pickle2</span>
</pre>
</body>
</html>