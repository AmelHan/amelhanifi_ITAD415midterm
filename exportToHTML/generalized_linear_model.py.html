<html>
<head>
<title>generalized_linear_model.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
generalized_linear_model.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Generalized linear models currently supports estimation using the one-parameter 
exponential families 
 
References 
---------- 
Gill, Jeff. 2000. Generalized Linear Models: A Unified Approach. 
    SAGE QASS Series. 
 
Green, PJ. 1984.  &quot;Iteratively reweighted least squares for maximum 
    likelihood estimation, and some robust and resistant alternatives.&quot; 
    Journal of the Royal Statistical Society, Series B, 46, 149-192. 
 
Hardin, J.W. and Hilbe, J.M. 2007.  &quot;Generalized Linear Models and 
    Extensions.&quot;  2nd ed.  Stata Press, College Station, TX. 
 
McCullagh, P. and Nelder, J.A.  1989.  &quot;Generalized Linear Models.&quot; 2nd ed. 
    Chapman &amp; Hall, Boca Rotan. 
&quot;&quot;&quot;</span>
<span class="s2">from </span><span class="s1">statsmodels.compat.pandas </span><span class="s2">import </span><span class="s1">Appender</span>

<span class="s2">import </span><span class="s1">warnings</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">numpy.linalg.linalg </span><span class="s2">import </span><span class="s1">LinAlgError</span>

<span class="s2">import </span><span class="s1">statsmodels.base.model </span><span class="s2">as </span><span class="s1">base</span>
<span class="s2">import </span><span class="s1">statsmodels.base.wrapper </span><span class="s2">as </span><span class="s1">wrap</span>

<span class="s2">from </span><span class="s1">statsmodels.base </span><span class="s2">import </span><span class="s1">_prediction_inference </span><span class="s2">as </span><span class="s1">pred</span>
<span class="s2">from </span><span class="s1">statsmodels.base._prediction_inference </span><span class="s2">import </span><span class="s1">PredictionResultsMean</span>
<span class="s2">import </span><span class="s1">statsmodels.base._parameter_inference </span><span class="s2">as </span><span class="s1">pinfer</span>

<span class="s2">from </span><span class="s1">statsmodels.graphics._regressionplots_doc </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">_plot_added_variable_doc</span><span class="s2">,</span>
    <span class="s1">_plot_ceres_residuals_doc</span><span class="s2">,</span>
    <span class="s1">_plot_partial_residuals_doc</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">import </span><span class="s1">statsmodels.regression._tools </span><span class="s2">as </span><span class="s1">reg_tools</span>
<span class="s2">import </span><span class="s1">statsmodels.regression.linear_model </span><span class="s2">as </span><span class="s1">lm</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.decorators </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">cache_readonly</span><span class="s2">,</span>
    <span class="s1">cached_data</span><span class="s2">,</span>
    <span class="s1">cached_value</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.docstring </span><span class="s2">import </span><span class="s1">Docstring</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">DomainWarning</span><span class="s2">,</span>
    <span class="s1">HessianInversionWarning</span><span class="s2">,</span>
    <span class="s1">PerfectSeparationWarning</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.validation </span><span class="s2">import </span><span class="s1">float_like</span>

<span class="s3"># need import in module instead of lazily to copy `__doc__`</span>
<span class="s2">from </span><span class="s1">. </span><span class="s2">import </span><span class="s1">families</span>

<span class="s1">__all__ = [</span><span class="s4">'GLM'</span><span class="s2">, </span><span class="s4">'PredictionResultsMean'</span><span class="s1">]</span>


<span class="s2">def </span><span class="s1">_check_convergence(criterion</span><span class="s2">, </span><span class="s1">iteration</span><span class="s2">, </span><span class="s1">atol</span><span class="s2">, </span><span class="s1">rtol):</span>
    <span class="s2">return </span><span class="s1">np.allclose(criterion[iteration]</span><span class="s2">, </span><span class="s1">criterion[iteration + </span><span class="s5">1</span><span class="s1">]</span><span class="s2">,</span>
                       <span class="s1">atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>


<span class="s3"># Remove after 0.13 when bic changes to bic llf</span>
<span class="s2">class </span><span class="s1">_ModuleVariable:</span>
    <span class="s1">_value = </span><span class="s2">None</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">use_bic_llf(self):</span>
        <span class="s2">return </span><span class="s1">self._value</span>

    <span class="s2">def </span><span class="s1">set_use_bic_llf(self</span><span class="s2">, </span><span class="s1">val):</span>
        <span class="s2">if </span><span class="s1">val </span><span class="s2">not in </span><span class="s1">(</span><span class="s2">True, False, None</span><span class="s1">):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Must be True, False or None&quot;</span><span class="s1">)</span>
        <span class="s1">self._value = bool(val) </span><span class="s2">if </span><span class="s1">val </span><span class="s2">is not None else </span><span class="s1">val</span>


<span class="s1">_use_bic_helper = _ModuleVariable()</span>
<span class="s1">SET_USE_BIC_LLF = _use_bic_helper.set_use_bic_llf</span>


<span class="s2">class </span><span class="s1">GLM(base.LikelihoodModel):</span>
    <span class="s1">__doc__ = </span><span class="s4">&quot;&quot;&quot; 
    Generalized Linear Models 
 
    GLM inherits from statsmodels.base.model.LikelihoodModel 
 
    Parameters 
    ---------- 
    endog : array_like 
        1d array of endogenous response variable.  This array can be 1d or 2d. 
        Binomial family models accept a 2d array with two columns. If 
        supplied, each observation is expected to be [success, failure]. 
    exog : array_like 
        A nobs x k array where `nobs` is the number of observations and `k` 
        is the number of regressors. An intercept is not included by default 
        and should be added by the user (models specified using a formula 
        include an intercept by default). See `statsmodels.tools.add_constant`. 
    family : family class instance 
        The default is Gaussian.  To specify the binomial distribution 
        family = sm.family.Binomial() 
        Each family can take a link instance as an argument.  See 
        statsmodels.family.family for more information. 
    offset : array_like or None 
        An offset to be included in the model.  If provided, must be 
        an array whose length is the number of rows in exog. 
    exposure : array_like or None 
        Log(exposure) will be added to the linear prediction in the model. 
        Exposure is only valid if the log link is used. If provided, it must be 
        an array with the same length as endog. 
    freq_weights : array_like 
        1d array of frequency weights. The default is None. If None is selected 
        or a blank value, then the algorithm will replace with an array of 1's 
        with length equal to the endog. 
        WARNING: Using weights is not verified yet for all possible options 
        and results, see Notes. 
    var_weights : array_like 
        1d array of variance (analytic) weights. The default is None. If None 
        is selected or a blank value, then the algorithm will replace with an 
        array of 1's with length equal to the endog. 
        WARNING: Using weights is not verified yet for all possible options 
        and results, see Notes. 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    df_model : float 
        Model degrees of freedom is equal to p - 1, where p is the number 
        of regressors.  Note that the intercept is not reported as a 
        degree of freedom. 
    df_resid : float 
        Residual degrees of freedom is equal to the number of observation n 
        minus the number of regressors p. 
    endog : ndarray 
        See Notes.  Note that `endog` is a reference to the data so that if 
        data is already an array and it is changed, then `endog` changes 
        as well. 
    exposure : array_like 
        Include ln(exposure) in model with coefficient constrained to 1. Can 
        only be used if the link is the logarithm function. 
    exog : ndarray 
        See Notes.  Note that `exog` is a reference to the data so that if 
        data is already an array and it is changed, then `exog` changes 
        as well. 
    freq_weights : ndarray 
        See Notes. Note that `freq_weights` is a reference to the data so that 
        if data is already an array and it is changed, then `freq_weights` 
        changes as well. 
    var_weights : ndarray 
        See Notes. Note that `var_weights` is a reference to the data so that 
        if data is already an array and it is changed, then `var_weights` 
        changes as well. 
    iteration : int 
        The number of iterations that fit has run.  Initialized at 0. 
    family : family class instance 
        The distribution family of the model. Can be any family in 
        statsmodels.families.  Default is Gaussian. 
    mu : ndarray 
        The mean response of the transformed variable.  `mu` is the value of 
        the inverse of the link function at lin_pred, where lin_pred is the 
        linear predicted value of the WLS fit of the transformed variable. 
        `mu` is only available after fit is called.  See 
        statsmodels.families.family.fitted of the distribution family for more 
        information. 
    n_trials : ndarray 
        See Notes. Note that `n_trials` is a reference to the data so that if 
        data is already an array and it is changed, then `n_trials` changes 
        as well. `n_trials` is the number of binomial trials and only available 
        with that distribution. See statsmodels.families.Binomial for more 
        information. 
    normalized_cov_params : ndarray 
        The p x p normalized covariance of the design / exogenous data. 
        This is approximately equal to (X.T X)^(-1) 
    offset : array_like 
        Include offset in model with coefficient constrained to 1. 
    scale : float 
        The estimate of the scale / dispersion of the model fit.  Only 
        available after fit is called.  See GLM.fit and GLM.estimate_scale 
        for more information. 
    scaletype : str 
        The scaling used for fitting the model.  This is only available after 
        fit is called.  The default is None.  See GLM.fit for more information. 
    weights : ndarray 
        The value of the weights after the last iteration of fit.  Only 
        available after fit is called.  See statsmodels.families.family for 
        the specific distribution weighting functions. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import statsmodels.api as sm 
    &gt;&gt;&gt; data = sm.datasets.scotland.load() 
    &gt;&gt;&gt; data.exog = sm.add_constant(data.exog) 
 
    Instantiate a gamma family model with the default link function. 
 
    &gt;&gt;&gt; gamma_model = sm.GLM(data.endog, data.exog, 
    ...                      family=sm.families.Gamma()) 
 
    &gt;&gt;&gt; gamma_results = gamma_model.fit() 
    &gt;&gt;&gt; gamma_results.params 
    array([-0.01776527,  0.00004962,  0.00203442, -0.00007181,  0.00011185, 
           -0.00000015, -0.00051868, -0.00000243]) 
    &gt;&gt;&gt; gamma_results.scale 
    0.0035842831734919055 
    &gt;&gt;&gt; gamma_results.deviance 
    0.087388516416999198 
    &gt;&gt;&gt; gamma_results.pearson_chi2 
    0.086022796163805704 
    &gt;&gt;&gt; gamma_results.llf 
    -83.017202161073527 
 
    See Also 
    -------- 
    statsmodels.genmod.families.family.Family 
    :ref:`families` 
    :ref:`links` 
 
    Notes 
    ----- 
    Note: PerfectSeparationError exception has been converted to a 
    PerfectSeparationWarning and perfect separation or perfect prediction will 
    not raise an exception by default. (changed in version 0.14) 
 
    Only the following combinations make sense for family and link: 
 
     ============= ===== === ===== ====== ======= === ==== ====== ====== ==== 
     Family        ident log logit probit cloglog pow opow nbinom loglog logc 
     ============= ===== === ===== ====== ======= === ==== ====== ====== ==== 
     Gaussian      x     x   x     x      x       x   x     x      x 
     inv Gaussian  x     x                        x 
     binomial      x     x   x     x      x       x   x           x      x 
     Poisson       x     x                        x 
     neg binomial  x     x                        x        x 
     gamma         x     x                        x 
     Tweedie       x     x                        x 
     ============= ===== === ===== ====== ======= === ==== ====== ====== ==== 
 
    Not all of these link functions are currently available. 
 
    Endog and exog are references so that if the data they refer to are already 
    arrays and these arrays are changed, endog and exog will change. 
 
    statsmodels supports two separate definitions of weights: frequency weights 
    and variance weights. 
 
    Frequency weights produce the same results as repeating observations by the 
    frequencies (if those are integers). Frequency weights will keep the number 
    of observations consistent, but the degrees of freedom will change to 
    reflect the new weights. 
 
    Variance weights (referred to in other packages as analytic weights) are 
    used when ``endog`` represents an an average or mean. This relies on the 
    assumption that that the inverse variance scales proportionally to the 
    weight--an observation that is deemed more credible should have less 
    variance and therefore have more weight. For the ``Poisson`` family--which 
    assumes that occurrences scale proportionally with time--a natural practice 
    would be to use the amount of time as the variance weight and set ``endog`` 
    to be a rate (occurrences per period of time). Similarly, using a 
    compound Poisson family, namely ``Tweedie``, makes a similar assumption 
    about the rate (or frequency) of occurrences having variance proportional to 
    time. 
 
    Both frequency and variance weights are verified for all basic results with 
    nonrobust or heteroscedasticity robust ``cov_type``. Other robust 
    covariance types have not yet been verified, and at least the small sample 
    correction is currently not based on the correct total frequency count. 
 
    Currently, all residuals are not weighted by frequency, although they may 
    incorporate ``n_trials`` for ``Binomial`` and ``var_weights`` 
 
    +---------------+----------------------------------+ 
    | Residual Type | Applicable weights               | 
    +===============+==================================+ 
    | Anscombe      | ``var_weights``                  | 
    +---------------+----------------------------------+ 
    | Deviance      | ``var_weights``                  | 
    +---------------+----------------------------------+ 
    | Pearson       | ``var_weights`` and ``n_trials`` | 
    +---------------+----------------------------------+ 
    | Reponse       | ``n_trials``                     | 
    +---------------+----------------------------------+ 
    | Working       | ``n_trials``                     | 
    +---------------+----------------------------------+ 
 
    WARNING: Loglikelihood and deviance are not valid in models where 
    scale is equal to 1 (i.e., ``Binomial``, ``NegativeBinomial``, and 
    ``Poisson``). If variance weights are specified, then results such as 
    ``loglike`` and ``deviance`` are based on a quasi-likelihood 
    interpretation. The loglikelihood is not correctly specified in this case, 
    and statistics based on it, such AIC or likelihood ratio tests, are not 
    appropriate. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s4">'extra_params'</span><span class="s1">: base._missing_param_doc}</span>
    <span class="s3"># Maximum number of endogenous variables when using a formula</span>
    <span class="s1">_formula_max_endog = </span><span class="s5">2</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">family=</span><span class="s2">None, </span><span class="s1">offset=</span><span class="s2">None,</span>
                 <span class="s1">exposure=</span><span class="s2">None, </span><span class="s1">freq_weights=</span><span class="s2">None, </span><span class="s1">var_weights=</span><span class="s2">None,</span>
                 <span class="s1">missing=</span><span class="s4">'none'</span><span class="s2">, </span><span class="s1">**kwargs):</span>

        <span class="s2">if </span><span class="s1">type(self) </span><span class="s2">is </span><span class="s1">GLM:</span>
            <span class="s1">self._check_kwargs(kwargs</span><span class="s2">, </span><span class="s1">[</span><span class="s4">'n_trials'</span><span class="s1">])</span>

        <span class="s2">if </span><span class="s1">(family </span><span class="s2">is not None</span><span class="s1">) </span><span class="s2">and not </span><span class="s1">isinstance(family.link</span><span class="s2">,</span>
                                                   <span class="s1">tuple(family.safe_links)):</span>

            <span class="s1">warnings.warn((</span><span class="s4">f&quot;The </span><span class="s2">{</span><span class="s1">type(family.link).__name__</span><span class="s2">} </span><span class="s4">link function &quot;</span>
                           <span class="s4">&quot;does not respect the domain of the &quot;</span>
                           <span class="s4">f&quot;</span><span class="s2">{</span><span class="s1">type(family).__name__</span><span class="s2">} </span><span class="s4">family.&quot;</span><span class="s1">)</span><span class="s2">,</span>
                          <span class="s1">DomainWarning)</span>

        <span class="s2">if </span><span class="s1">exposure </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">exposure = np.log(exposure)</span>
        <span class="s2">if </span><span class="s1">offset </span><span class="s2">is not None</span><span class="s1">:  </span><span class="s3"># this should probably be done upstream</span>
            <span class="s1">offset = np.asarray(offset)</span>

        <span class="s2">if </span><span class="s1">freq_weights </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">freq_weights = np.asarray(freq_weights)</span>
        <span class="s2">if </span><span class="s1">var_weights </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">var_weights = np.asarray(var_weights)</span>

        <span class="s1">self.freq_weights = freq_weights</span>
        <span class="s1">self.var_weights = var_weights</span>

        <span class="s1">super(GLM</span><span class="s2">, </span><span class="s1">self).__init__(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">missing=missing</span><span class="s2">,</span>
                                  <span class="s1">offset=offset</span><span class="s2">, </span><span class="s1">exposure=exposure</span><span class="s2">,</span>
                                  <span class="s1">freq_weights=freq_weights</span><span class="s2">,</span>
                                  <span class="s1">var_weights=var_weights</span><span class="s2">, </span><span class="s1">**kwargs)</span>
        <span class="s1">self._check_inputs(family</span><span class="s2">, </span><span class="s1">self.offset</span><span class="s2">, </span><span class="s1">self.exposure</span><span class="s2">, </span><span class="s1">self.endog</span><span class="s2">,</span>
                           <span class="s1">self.freq_weights</span><span class="s2">, </span><span class="s1">self.var_weights)</span>
        <span class="s2">if </span><span class="s1">offset </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">delattr(self</span><span class="s2">, </span><span class="s4">'offset'</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">exposure </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">delattr(self</span><span class="s2">, </span><span class="s4">'exposure'</span><span class="s1">)</span>

        <span class="s1">self.nobs = self.endog.shape[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s3"># things to remove_data</span>
        <span class="s1">self._data_attr.extend([</span><span class="s4">'weights'</span><span class="s2">, </span><span class="s4">'mu'</span><span class="s2">, </span><span class="s4">'freq_weights'</span><span class="s2">,</span>
                                <span class="s4">'var_weights'</span><span class="s2">, </span><span class="s4">'iweights'</span><span class="s2">, </span><span class="s4">'_offset_exposure'</span><span class="s2">,</span>
                                <span class="s4">'n_trials'</span><span class="s1">])</span>
        <span class="s3"># register kwds for __init__, offset and exposure are added by super</span>
        <span class="s1">self._init_keys.append(</span><span class="s4">'family'</span><span class="s1">)</span>

        <span class="s1">self._setup_binomial()</span>
        <span class="s3"># internal usage for recreating a model</span>
        <span class="s2">if </span><span class="s4">'n_trials' </span><span class="s2">in </span><span class="s1">kwargs:</span>
            <span class="s1">self.n_trials = kwargs[</span><span class="s4">'n_trials'</span><span class="s1">]</span>

        <span class="s3"># Construct a combined offset/exposure term.  Note that</span>
        <span class="s3"># exposure has already been logged if present.</span>
        <span class="s1">offset_exposure = </span><span class="s5">0.</span>
        <span class="s2">if </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s4">'offset'</span><span class="s1">):</span>
            <span class="s1">offset_exposure = self.offset</span>
        <span class="s2">if </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s4">'exposure'</span><span class="s1">):</span>
            <span class="s1">offset_exposure = offset_exposure + self.exposure</span>
        <span class="s1">self._offset_exposure = offset_exposure</span>

        <span class="s1">self.scaletype = </span><span class="s2">None</span>

    <span class="s2">def </span><span class="s1">initialize(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Initialize a generalized linear model. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.df_model = np.linalg.matrix_rank(self.exog) - </span><span class="s5">1</span>

        <span class="s2">if </span><span class="s1">(self.freq_weights </span><span class="s2">is not None</span><span class="s1">) </span><span class="s2">and </span><span class="s1">\</span>
           <span class="s1">(self.freq_weights.shape[</span><span class="s5">0</span><span class="s1">] == self.endog.shape[</span><span class="s5">0</span><span class="s1">]):</span>
            <span class="s1">self.wnobs = self.freq_weights.sum()</span>
            <span class="s1">self.df_resid = self.wnobs - self.df_model - </span><span class="s5">1</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self.wnobs = self.exog.shape[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">self.df_resid = self.exog.shape[</span><span class="s5">0</span><span class="s1">] - self.df_model - </span><span class="s5">1</span>

    <span class="s2">def </span><span class="s1">_check_inputs(self</span><span class="s2">, </span><span class="s1">family</span><span class="s2">, </span><span class="s1">offset</span><span class="s2">, </span><span class="s1">exposure</span><span class="s2">, </span><span class="s1">endog</span><span class="s2">, </span><span class="s1">freq_weights</span><span class="s2">,</span>
                      <span class="s1">var_weights):</span>

        <span class="s3"># Default family is Gaussian</span>
        <span class="s2">if </span><span class="s1">family </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">family = families.Gaussian()</span>
        <span class="s1">self.family = family</span>

        <span class="s2">if </span><span class="s1">exposure </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">if not </span><span class="s1">isinstance(self.family.link</span><span class="s2">, </span><span class="s1">families.links.Log):</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;exposure can only be used with the log &quot;</span>
                                 <span class="s4">&quot;link function&quot;</span><span class="s1">)</span>
            <span class="s2">elif </span><span class="s1">exposure.shape[</span><span class="s5">0</span><span class="s1">] != endog.shape[</span><span class="s5">0</span><span class="s1">]:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;exposure is not the same length as endog&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">offset </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">offset.shape[</span><span class="s5">0</span><span class="s1">] != endog.shape[</span><span class="s5">0</span><span class="s1">]:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;offset is not the same length as endog&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">freq_weights </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">freq_weights.shape[</span><span class="s5">0</span><span class="s1">] != endog.shape[</span><span class="s5">0</span><span class="s1">]:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;freq weights not the same length as endog&quot;</span><span class="s1">)</span>
            <span class="s2">if </span><span class="s1">len(freq_weights.shape) &gt; </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;freq weights has too many dimensions&quot;</span><span class="s1">)</span>

        <span class="s3"># internal flag to store whether freq_weights were not None</span>
        <span class="s1">self._has_freq_weights = (self.freq_weights </span><span class="s2">is not None</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">self.freq_weights </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">self.freq_weights = np.ones((endog.shape[</span><span class="s5">0</span><span class="s1">]))</span>
            <span class="s3"># TODO: check do we want to keep None as sentinel for freq_weights</span>

        <span class="s2">if </span><span class="s1">np.shape(self.freq_weights) == () </span><span class="s2">and </span><span class="s1">self.freq_weights &gt; </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">self.freq_weights = (self.freq_weights *</span>
                                 <span class="s1">np.ones((endog.shape[</span><span class="s5">0</span><span class="s1">])))</span>

        <span class="s2">if </span><span class="s1">var_weights </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">var_weights.shape[</span><span class="s5">0</span><span class="s1">] != endog.shape[</span><span class="s5">0</span><span class="s1">]:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;var weights not the same length as endog&quot;</span><span class="s1">)</span>
            <span class="s2">if </span><span class="s1">len(var_weights.shape) &gt; </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;var weights has too many dimensions&quot;</span><span class="s1">)</span>

        <span class="s3"># internal flag to store whether var_weights were not None</span>
        <span class="s1">self._has_var_weights = (var_weights </span><span class="s2">is not None</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">var_weights </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">self.var_weights = np.ones((endog.shape[</span><span class="s5">0</span><span class="s1">]))</span>
            <span class="s3"># TODO: check do we want to keep None as sentinel for var_weights</span>
        <span class="s1">self.iweights = np.asarray(self.freq_weights * self.var_weights)</span>

    <span class="s2">def </span><span class="s1">_get_init_kwds(self):</span>
        <span class="s3"># this is a temporary fixup because exposure has been transformed</span>
        <span class="s3"># see #1609, copied from discrete_model.CountModel</span>
        <span class="s1">kwds = super(GLM</span><span class="s2">, </span><span class="s1">self)._get_init_kwds()</span>
        <span class="s2">if </span><span class="s4">'exposure' </span><span class="s2">in </span><span class="s1">kwds </span><span class="s2">and </span><span class="s1">kwds[</span><span class="s4">'exposure'</span><span class="s1">] </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">kwds[</span><span class="s4">'exposure'</span><span class="s1">] = np.exp(kwds[</span><span class="s4">'exposure'</span><span class="s1">])</span>
        <span class="s2">return </span><span class="s1">kwds</span>

    <span class="s2">def </span><span class="s1">loglike_mu(self</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">scale=</span><span class="s5">1.</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Evaluate the log-likelihood for a generalized linear model. 
        &quot;&quot;&quot;</span>
        <span class="s1">scale = float_like(scale</span><span class="s2">, </span><span class="s4">&quot;scale&quot;</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">self.family.loglike(self.endog</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">self.var_weights</span><span class="s2">,</span>
                                   <span class="s1">self.freq_weights</span><span class="s2">, </span><span class="s1">scale)</span>

    <span class="s2">def </span><span class="s1">loglike(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">scale=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Evaluate the log-likelihood for a generalized linear model. 
        &quot;&quot;&quot;</span>
        <span class="s1">scale = float_like(scale</span><span class="s2">, </span><span class="s4">&quot;scale&quot;</span><span class="s2">, </span><span class="s1">optional=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">lin_pred = np.dot(self.exog</span><span class="s2">, </span><span class="s1">params) + self._offset_exposure</span>
        <span class="s1">expval = self.family.link.inverse(lin_pred)</span>
        <span class="s2">if </span><span class="s1">scale </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">scale = self.estimate_scale(expval)</span>
        <span class="s1">llf = self.family.loglike(self.endog</span><span class="s2">, </span><span class="s1">expval</span><span class="s2">, </span><span class="s1">self.var_weights</span><span class="s2">,</span>
                                  <span class="s1">self.freq_weights</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s2">return </span><span class="s1">llf</span>

    <span class="s2">def </span><span class="s1">score_obs(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">scale=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;score first derivative of the loglikelihood for each observation. 
 
        Parameters 
        ---------- 
        params : ndarray 
            Parameter at which score is evaluated. 
        scale : None or float 
            If scale is None, then the default scale will be calculated. 
            Default scale is defined by `self.scaletype` and set in fit. 
            If scale is not None, then it is used as a fixed scale. 
 
        Returns 
        ------- 
        score_obs : ndarray, 2d 
            The first derivative of the loglikelihood function evaluated at 
            params for each observation. 
        &quot;&quot;&quot;</span>
        <span class="s1">scale = float_like(scale</span><span class="s2">, </span><span class="s4">&quot;scale&quot;</span><span class="s2">, </span><span class="s1">optional=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">score_factor = self.score_factor(params</span><span class="s2">, </span><span class="s1">scale=scale)</span>
        <span class="s2">return </span><span class="s1">score_factor[:</span><span class="s2">, None</span><span class="s1">] * self.exog</span>

    <span class="s2">def </span><span class="s1">score(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">scale=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;score, first derivative of the loglikelihood function 
 
        Parameters 
        ---------- 
        params : ndarray 
            Parameter at which score is evaluated. 
        scale : None or float 
            If scale is None, then the default scale will be calculated. 
            Default scale is defined by `self.scaletype` and set in fit. 
            If scale is not None, then it is used as a fixed scale. 
 
        Returns 
        ------- 
        score : ndarray_1d 
            The first derivative of the loglikelihood function calculated as 
            the sum of `score_obs` 
        &quot;&quot;&quot;</span>
        <span class="s1">scale = float_like(scale</span><span class="s2">, </span><span class="s4">&quot;scale&quot;</span><span class="s2">, </span><span class="s1">optional=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">score_factor = self.score_factor(params</span><span class="s2">, </span><span class="s1">scale=scale)</span>
        <span class="s2">return </span><span class="s1">np.dot(score_factor</span><span class="s2">, </span><span class="s1">self.exog)</span>

    <span class="s2">def </span><span class="s1">score_factor(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">scale=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;weights for score for each observation 
 
        This can be considered as score residuals. 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which score is evaluated 
        scale : None or float 
            If scale is None, then the default scale will be calculated. 
            Default scale is defined by `self.scaletype` and set in fit. 
            If scale is not None, then it is used as a fixed scale. 
 
        Returns 
        ------- 
        score_factor : ndarray_1d 
            A 1d weight vector used in the calculation of the score_obs. 
            The score_obs are obtained by `score_factor[:, None] * exog` 
        &quot;&quot;&quot;</span>
        <span class="s1">scale = float_like(scale</span><span class="s2">, </span><span class="s4">&quot;scale&quot;</span><span class="s2">, </span><span class="s1">optional=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">mu = self.predict(params)</span>
        <span class="s2">if </span><span class="s1">scale </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">scale = self.estimate_scale(mu)</span>

        <span class="s1">score_factor = (self.endog - mu) / self.family.link.deriv(mu)</span>
        <span class="s1">score_factor /= self.family.variance(mu)</span>
        <span class="s1">score_factor *= self.iweights * self.n_trials</span>

        <span class="s2">if not </span><span class="s1">scale == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">score_factor /= scale</span>

        <span class="s2">return </span><span class="s1">score_factor</span>

    <span class="s2">def </span><span class="s1">hessian_factor(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">scale=</span><span class="s2">None, </span><span class="s1">observed=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Weights for calculating Hessian 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which Hessian is evaluated 
        scale : None or float 
            If scale is None, then the default scale will be calculated. 
            Default scale is defined by `self.scaletype` and set in fit. 
            If scale is not None, then it is used as a fixed scale. 
        observed : bool 
            If True, then the observed Hessian is returned. If false then the 
            expected information matrix is returned. 
 
        Returns 
        ------- 
        hessian_factor : ndarray, 1d 
            A 1d weight vector used in the calculation of the Hessian. 
            The hessian is obtained by `(exog.T * hessian_factor).dot(exog)` 
        &quot;&quot;&quot;</span>

        <span class="s3"># calculating eim_factor</span>
        <span class="s1">mu = self.predict(params)</span>
        <span class="s2">if </span><span class="s1">scale </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">scale = self.estimate_scale(mu)</span>

        <span class="s1">eim_factor = </span><span class="s5">1 </span><span class="s1">/ (self.family.link.deriv(mu)**</span><span class="s5">2 </span><span class="s1">*</span>
                          <span class="s1">self.family.variance(mu))</span>
        <span class="s1">eim_factor *= self.iweights * self.n_trials</span>

        <span class="s2">if not </span><span class="s1">observed:</span>
            <span class="s2">if not </span><span class="s1">scale == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">eim_factor /= scale</span>
            <span class="s2">return </span><span class="s1">eim_factor</span>

        <span class="s3"># calculating oim_factor, eim_factor is with scale=1</span>

        <span class="s1">score_factor = self.score_factor(params</span><span class="s2">, </span><span class="s1">scale=</span><span class="s5">1.</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">eim_factor.ndim &gt; </span><span class="s5">1 </span><span class="s2">or </span><span class="s1">score_factor.ndim &gt; </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">RuntimeError(</span><span class="s4">'something wrong'</span><span class="s1">)</span>

        <span class="s1">tmp = self.family.variance(mu) * self.family.link.deriv2(mu)</span>
        <span class="s1">tmp += self.family.variance.deriv(mu) * self.family.link.deriv(mu)</span>

        <span class="s1">tmp = score_factor * tmp</span>
        <span class="s3"># correct for duplicatee iweights in oim_factor and score_factor</span>
        <span class="s1">tmp /= self.iweights * self.n_trials</span>
        <span class="s1">oim_factor = eim_factor * (</span><span class="s5">1 </span><span class="s1">+ tmp)</span>

        <span class="s2">if </span><span class="s1">tmp.ndim &gt; </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">RuntimeError(</span><span class="s4">'something wrong'</span><span class="s1">)</span>

        <span class="s2">if not </span><span class="s1">scale == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">oim_factor /= scale</span>

        <span class="s2">return </span><span class="s1">oim_factor</span>

    <span class="s2">def </span><span class="s1">hessian(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">scale=</span><span class="s2">None, </span><span class="s1">observed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Hessian, second derivative of loglikelihood function 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which Hessian is evaluated 
        scale : None or float 
            If scale is None, then the default scale will be calculated. 
            Default scale is defined by `self.scaletype` and set in fit. 
            If scale is not None, then it is used as a fixed scale. 
        observed : bool 
            If True, then the observed Hessian is returned (default). 
            If False, then the expected information matrix is returned. 
 
        Returns 
        ------- 
        hessian : ndarray 
            Hessian, i.e. observed information, or expected information matrix. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">observed </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">getattr(self</span><span class="s2">, </span><span class="s4">'_optim_hessian'</span><span class="s2">, None</span><span class="s1">) == </span><span class="s4">'eim'</span><span class="s1">:</span>
                <span class="s1">observed = </span><span class="s2">False</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">observed = </span><span class="s2">True</span>
        <span class="s1">scale = float_like(scale</span><span class="s2">, </span><span class="s4">&quot;scale&quot;</span><span class="s2">, </span><span class="s1">optional=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">tmp = getattr(self</span><span class="s2">, </span><span class="s4">'_tmp_like_exog'</span><span class="s2">, </span><span class="s1">np.empty_like(self.exog</span><span class="s2">, </span><span class="s1">dtype=float))</span>

        <span class="s1">factor = self.hessian_factor(params</span><span class="s2">, </span><span class="s1">scale=scale</span><span class="s2">, </span><span class="s1">observed=observed)</span>
        <span class="s1">np.multiply(self.exog.T</span><span class="s2">, </span><span class="s1">factor</span><span class="s2">, </span><span class="s1">out=tmp.T)</span>
        <span class="s2">return </span><span class="s1">-tmp.T.dot(self.exog)</span>

    <span class="s2">def </span><span class="s1">information(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">scale=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fisher information matrix. 
        &quot;&quot;&quot;</span>
        <span class="s1">scale = float_like(scale</span><span class="s2">, </span><span class="s4">&quot;scale&quot;</span><span class="s2">, </span><span class="s1">optional=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">self.hessian(params</span><span class="s2">, </span><span class="s1">scale=scale</span><span class="s2">, </span><span class="s1">observed=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">_deriv_mean_dparams(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Derivative of the expected endog with respect to the parameters. 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which score is evaluated 
 
        Returns 
        ------- 
        The value of the derivative of the expected endog with respect 
        to the parameter vector. 
        &quot;&quot;&quot;</span>
        <span class="s1">lin_pred = self.predict(params</span><span class="s2">, </span><span class="s1">which=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s1">idl = self.family.link.inverse_deriv(lin_pred)</span>
        <span class="s1">dmat = self.exog * idl[:</span><span class="s2">, None</span><span class="s1">]</span>
        <span class="s2">return </span><span class="s1">dmat</span>

    <span class="s2">def </span><span class="s1">_deriv_score_obs_dendog(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">scale=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;derivative of score_obs w.r.t. endog 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which score is evaluated 
        scale : None or float 
            If scale is None, then the default scale will be calculated. 
            Default scale is defined by `self.scaletype` and set in fit. 
            If scale is not None, then it is used as a fixed scale. 
 
        Returns 
        ------- 
        derivative : ndarray_2d 
            The derivative of the score_obs with respect to endog. This 
            can is given by `score_factor0[:, None] * exog` where 
            `score_factor0` is the score_factor without the residual. 
        &quot;&quot;&quot;</span>
        <span class="s1">scale = float_like(scale</span><span class="s2">, </span><span class="s4">&quot;scale&quot;</span><span class="s2">, </span><span class="s1">optional=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">mu = self.predict(params)</span>
        <span class="s2">if </span><span class="s1">scale </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">scale = self.estimate_scale(mu)</span>

        <span class="s1">score_factor = </span><span class="s5">1 </span><span class="s1">/ self.family.link.deriv(mu)</span>
        <span class="s1">score_factor /= self.family.variance(mu)</span>
        <span class="s1">score_factor *= self.iweights * self.n_trials</span>

        <span class="s2">if not </span><span class="s1">scale == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">score_factor /= scale</span>

        <span class="s2">return </span><span class="s1">score_factor[:</span><span class="s2">, None</span><span class="s1">] * self.exog</span>

    <span class="s2">def </span><span class="s1">score_test(self</span><span class="s2">, </span><span class="s1">params_constrained</span><span class="s2">, </span><span class="s1">k_constraints=</span><span class="s2">None,</span>
                   <span class="s1">exog_extra=</span><span class="s2">None, </span><span class="s1">observed=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;score test for restrictions or for omitted variables 
 
        The covariance matrix for the score is based on the Hessian, i.e. 
        observed information matrix or optionally on the expected information 
        matrix.. 
 
        Parameters 
        ---------- 
        params_constrained : array_like 
            estimated parameter of the restricted model. This can be the 
            parameter estimate for the current when testing for omitted 
            variables. 
        k_constraints : int or None 
            Number of constraints that were used in the estimation of params 
            restricted relative to the number of exog in the model. 
            This must be provided if no exog_extra are given. If exog_extra is 
            not None, then k_constraints is assumed to be zero if it is None. 
        exog_extra : None or array_like 
            Explanatory variables that are jointly tested for inclusion in the 
            model, i.e. omitted variables. 
        observed : bool 
            If True, then the observed Hessian is used in calculating the 
            covariance matrix of the score. If false then the expected 
            information matrix is used. 
 
        Returns 
        ------- 
        chi2_stat : float 
            chisquare statistic for the score test 
        p-value : float 
            P-value of the score test based on the chisquare distribution. 
        df : int 
            Degrees of freedom used in the p-value calculation. This is equal 
            to the number of constraints. 
 
        Notes 
        ----- 
        not yet verified for case with scale not equal to 1. 
        &quot;&quot;&quot;</span>

        <span class="s2">if </span><span class="s1">exog_extra </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">k_constraints </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">'if exog_extra is None, then k_constraints'</span>
                                 <span class="s4">'needs to be given'</span><span class="s1">)</span>

            <span class="s1">score = self.score(params_constrained)</span>
            <span class="s1">hessian = self.hessian(params_constrained</span><span class="s2">, </span><span class="s1">observed=observed)</span>

        <span class="s2">else</span><span class="s1">:</span>
            <span class="s3"># exog_extra = np.asarray(exog_extra)</span>
            <span class="s2">if </span><span class="s1">k_constraints </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s1">k_constraints = </span><span class="s5">0</span>

            <span class="s1">ex = np.column_stack((self.exog</span><span class="s2">, </span><span class="s1">exog_extra))</span>
            <span class="s1">k_constraints += ex.shape[</span><span class="s5">1</span><span class="s1">] - self.exog.shape[</span><span class="s5">1</span><span class="s1">]</span>

            <span class="s1">score_factor = self.score_factor(params_constrained)</span>
            <span class="s1">score = (score_factor[:</span><span class="s2">, None</span><span class="s1">] * ex).sum(</span><span class="s5">0</span><span class="s1">)</span>
            <span class="s1">hessian_factor = self.hessian_factor(params_constrained</span><span class="s2">,</span>
                                                 <span class="s1">observed=observed)</span>
            <span class="s1">hessian = -np.dot(ex.T * hessian_factor</span><span class="s2">, </span><span class="s1">ex)</span>

        <span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">stats</span>

        <span class="s3"># TODO check sign, why minus?</span>
        <span class="s1">chi2stat = -score.dot(np.linalg.solve(hessian</span><span class="s2">, </span><span class="s1">score[:</span><span class="s2">, None</span><span class="s1">]))</span>
        <span class="s1">pval = stats.chi2.sf(chi2stat</span><span class="s2">, </span><span class="s1">k_constraints)</span>
        <span class="s3"># return a stats results instance instead?  Contrast?</span>
        <span class="s2">return </span><span class="s1">chi2stat</span><span class="s2">, </span><span class="s1">pval</span><span class="s2">, </span><span class="s1">k_constraints</span>

    <span class="s2">def </span><span class="s1">_update_history(self</span><span class="s2">, </span><span class="s1">tmp_result</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">history):</span>
        <span class="s0">&quot;&quot;&quot; 
        Helper method to update history during iterative fit. 
        &quot;&quot;&quot;</span>
        <span class="s1">history[</span><span class="s4">'params'</span><span class="s1">].append(tmp_result.params)</span>
        <span class="s1">history[</span><span class="s4">'deviance'</span><span class="s1">].append(self.family.deviance(self.endog</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">,</span>
                                                        <span class="s1">self.var_weights</span><span class="s2">,</span>
                                                        <span class="s1">self.freq_weights</span><span class="s2">,</span>
                                                        <span class="s1">self.scale))</span>
        <span class="s2">return </span><span class="s1">history</span>

    <span class="s2">def </span><span class="s1">estimate_scale(self</span><span class="s2">, </span><span class="s1">mu):</span>
        <span class="s0">&quot;&quot;&quot; 
        Estimate the dispersion/scale. 
 
        Type of scale can be chose in the fit method. 
 
        Parameters 
        ---------- 
        mu : ndarray 
            mu is the mean response estimate 
 
        Returns 
        ------- 
        Estimate of scale 
 
        Notes 
        ----- 
        The default scale for Binomial, Poisson and Negative Binomial 
        families is 1.  The default for the other families is Pearson's 
        Chi-Square estimate. 
 
        See Also 
        -------- 
        statsmodels.genmod.generalized_linear_model.GLM.fit 
        &quot;&quot;&quot;</span>
        <span class="s2">if not </span><span class="s1">self.scaletype:</span>
            <span class="s2">if </span><span class="s1">isinstance(self.family</span><span class="s2">, </span><span class="s1">(families.Binomial</span><span class="s2">, </span><span class="s1">families.Poisson</span><span class="s2">,</span>
                                        <span class="s1">families.NegativeBinomial)):</span>
                <span class="s2">return </span><span class="s5">1.</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">self._estimate_x2_scale(mu)</span>

        <span class="s2">if </span><span class="s1">isinstance(self.scaletype</span><span class="s2">, </span><span class="s1">float):</span>
            <span class="s2">return </span><span class="s1">np.array(self.scaletype)</span>

        <span class="s2">if </span><span class="s1">isinstance(self.scaletype</span><span class="s2">, </span><span class="s1">str):</span>
            <span class="s2">if </span><span class="s1">self.scaletype.lower() == </span><span class="s4">'x2'</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">self._estimate_x2_scale(mu)</span>
            <span class="s2">elif </span><span class="s1">self.scaletype.lower() == </span><span class="s4">'dev'</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">(self.family.deviance(self.endog</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">self.var_weights</span><span class="s2">,</span>
                                             <span class="s1">self.freq_weights</span><span class="s2">, </span><span class="s5">1.</span><span class="s1">) /</span>
                        <span class="s1">(self.df_resid))</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Scale %s with type %s not understood&quot; </span><span class="s1">%</span>
                                 <span class="s1">(self.scaletype</span><span class="s2">, </span><span class="s1">type(self.scaletype)))</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Scale %s with type %s not understood&quot; </span><span class="s1">%</span>
                             <span class="s1">(self.scaletype</span><span class="s2">, </span><span class="s1">type(self.scaletype)))</span>

    <span class="s2">def </span><span class="s1">_estimate_x2_scale(self</span><span class="s2">, </span><span class="s1">mu):</span>
        <span class="s1">resid = np.power(self.endog - mu</span><span class="s2">, </span><span class="s5">2</span><span class="s1">) * self.iweights</span>
        <span class="s2">return </span><span class="s1">np.sum(resid / self.family.variance(mu)) / self.df_resid</span>

    <span class="s2">def </span><span class="s1">estimate_tweedie_power(self</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">method=</span><span class="s4">'brentq'</span><span class="s2">, </span><span class="s1">low=</span><span class="s5">1.01</span><span class="s2">, </span><span class="s1">high=</span><span class="s5">5.</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Tweedie specific function to estimate scale and the variance parameter. 
        The variance parameter is also referred to as p, xi, or shape. 
 
        Parameters 
        ---------- 
        mu : array_like 
            Fitted mean response variable 
        method : str, defaults to 'brentq' 
            Scipy optimizer used to solve the Pearson equation. Only brentq 
            currently supported. 
        low : float, optional 
            Low end of the bracketing interval [a,b] to be used in the search 
            for the power. Defaults to 1.01. 
        high : float, optional 
            High end of the bracketing interval [a,b] to be used in the search 
            for the power. Defaults to 5. 
 
        Returns 
        ------- 
        power : float 
            The estimated shape or power. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">method == </span><span class="s4">'brentq'</span><span class="s1">:</span>
            <span class="s2">from </span><span class="s1">scipy.optimize </span><span class="s2">import </span><span class="s1">brentq</span>

            <span class="s2">def </span><span class="s1">psi_p(power</span><span class="s2">, </span><span class="s1">mu):</span>
                <span class="s1">scale = ((self.iweights * (self.endog - mu) ** </span><span class="s5">2 </span><span class="s1">/</span>
                          <span class="s1">(mu ** power)).sum() / self.df_resid)</span>
                <span class="s2">return </span><span class="s1">(np.sum(self.iweights * ((self.endog - mu) ** </span><span class="s5">2 </span><span class="s1">/</span>
                               <span class="s1">(scale * (mu ** power)) - </span><span class="s5">1</span><span class="s1">) *</span>
                               <span class="s1">np.log(mu)) / self.freq_weights.sum())</span>
            <span class="s1">power = brentq(psi_p</span><span class="s2">, </span><span class="s1">low</span><span class="s2">, </span><span class="s1">high</span><span class="s2">, </span><span class="s1">args=(mu))</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">NotImplementedError(</span><span class="s4">'Only brentq can currently be used'</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">power</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">exog=</span><span class="s2">None, </span><span class="s1">exposure=</span><span class="s2">None, </span><span class="s1">offset=</span><span class="s2">None,</span>
                <span class="s1">which=</span><span class="s4">&quot;mean&quot;</span><span class="s2">, </span><span class="s1">linear=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return predicted values for a design matrix 
 
        Parameters 
        ---------- 
        params : array_like 
            Parameters / coefficients of a GLM. 
        exog : array_like, optional 
            Design / exogenous data. Is exog is None, model exog is used. 
        exposure : array_like, optional 
            Exposure time values, only can be used with the log link 
            function.  See notes for details. 
        offset : array_like, optional 
            Offset values.  See notes for details. 
        which : 'mean', 'linear', 'var'(optional) 
            Statitistic to predict. Default is 'mean'. 
 
            - 'mean' returns the conditional expectation of endog E(y | x), 
              i.e. inverse of the model's link function of linear predictor. 
            - 'linear' returns the linear predictor of the mean function. 
            - 'var_unscaled' variance of endog implied by the likelihood model. 
              This does not include scale or var_weights. 
 
        linear : bool 
            The ``linear` keyword is deprecated and will be removed, 
            use ``which`` keyword instead. 
            If True, returns the linear predicted values.  If False or None, 
            then the statistic specified by ``which`` will be returned. 
 
 
        Returns 
        ------- 
        An array of fitted values 
 
        Notes 
        ----- 
        Any `exposure` and `offset` provided here take precedence over 
        the `exposure` and `offset` used in the model fit.  If `exog` 
        is passed as an argument here, then any `exposure` and 
        `offset` values in the fit will be ignored. 
 
        Exposure values must be strictly positive. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">linear </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">msg = </span><span class="s4">'linear keyword is deprecated, use which=&quot;linear&quot;'</span>
            <span class="s1">warnings.warn(msg</span><span class="s2">, </span><span class="s1">FutureWarning)</span>
            <span class="s2">if </span><span class="s1">linear </span><span class="s2">is True</span><span class="s1">:</span>
                <span class="s1">which = </span><span class="s4">&quot;linear&quot;</span>

        <span class="s3"># Use fit offset if appropriate</span>
        <span class="s2">if </span><span class="s1">offset </span><span class="s2">is None and </span><span class="s1">exog </span><span class="s2">is None and </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s4">'offset'</span><span class="s1">):</span>
            <span class="s1">offset = self.offset</span>
        <span class="s2">elif </span><span class="s1">offset </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">offset = </span><span class="s5">0.</span>

        <span class="s2">if </span><span class="s1">exposure </span><span class="s2">is not None and not </span><span class="s1">isinstance(self.family.link</span><span class="s2">,</span>
                                                   <span class="s1">families.links.Log):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;exposure can only be used with the log link &quot;</span>
                             <span class="s4">&quot;function&quot;</span><span class="s1">)</span>

        <span class="s3"># Use fit exposure if appropriate</span>
        <span class="s2">if </span><span class="s1">exposure </span><span class="s2">is None and </span><span class="s1">exog </span><span class="s2">is None and </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s4">'exposure'</span><span class="s1">):</span>
            <span class="s3"># Already logged</span>
            <span class="s1">exposure = self.exposure</span>
        <span class="s2">elif </span><span class="s1">exposure </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">exposure = </span><span class="s5">0.</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">exposure = np.log(np.asarray(exposure))</span>

        <span class="s2">if </span><span class="s1">exog </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>

        <span class="s1">linpred = np.dot(exog</span><span class="s2">, </span><span class="s1">params) + offset + exposure</span>

        <span class="s2">if </span><span class="s1">which == </span><span class="s4">&quot;mean&quot;</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">self.family.fitted(linpred)</span>
        <span class="s2">elif </span><span class="s1">which == </span><span class="s4">&quot;linear&quot;</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">linpred</span>
        <span class="s2">elif </span><span class="s1">which == </span><span class="s4">&quot;var_unscaled&quot;</span><span class="s1">:</span>
            <span class="s1">mean = self.family.fitted(linpred)</span>
            <span class="s1">var_ = self.family.variance(mean)</span>
            <span class="s2">return </span><span class="s1">var_</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">f'The which value &quot;</span><span class="s2">{</span><span class="s1">which</span><span class="s2">}</span><span class="s4">&quot; is not recognized'</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">get_distribution(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">scale=</span><span class="s2">None, </span><span class="s1">exog=</span><span class="s2">None, </span><span class="s1">exposure=</span><span class="s2">None,</span>
                         <span class="s1">offset=</span><span class="s2">None, </span><span class="s1">var_weights=</span><span class="s5">1.</span><span class="s2">, </span><span class="s1">n_trials=</span><span class="s5">1.</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return a instance of the predictive distribution. 
 
        Parameters 
        ---------- 
        params : array_like 
            The model parameters. 
        scale : scalar 
            The scale parameter. 
        exog : array_like 
            The predictor variable matrix. 
        offset : array_like or None 
            Offset variable for predicted mean. 
        exposure : array_like or None 
            Log(exposure) will be added to the linear prediction. 
        var_weights : array_like 
            1d array of variance (analytic) weights. The default is None. 
        n_trials : int 
            Number of trials for the binomial distribution. The default is 1 
            which corresponds to a Bernoulli random variable. 
 
        Returns 
        ------- 
        gen 
            Instance of a scipy frozen distribution based on estimated 
            parameters. 
            Use the ``rvs`` method to generate random values. 
 
        Notes 
        ----- 
        Due to the behavior of ``scipy.stats.distributions objects``, the 
        returned random number generator must be called with ``gen.rvs(n)`` 
        where ``n`` is the number of observations in the data set used 
        to fit the model.  If any other value is used for ``n``, misleading 
        results will be produced. 
        &quot;&quot;&quot;</span>
        <span class="s1">scale = float_like(scale</span><span class="s2">, </span><span class="s4">&quot;scale&quot;</span><span class="s2">, </span><span class="s1">optional=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s3"># use scale=1, independent of QMLE scale for discrete</span>
        <span class="s2">if </span><span class="s1">isinstance(self.family</span><span class="s2">, </span><span class="s1">(families.Binomial</span><span class="s2">, </span><span class="s1">families.Poisson</span><span class="s2">,</span>
                                    <span class="s1">families.NegativeBinomial)):</span>
            <span class="s1">scale = </span><span class="s5">1.</span>

        <span class="s1">mu = self.predict(params</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">exposure</span><span class="s2">, </span><span class="s1">offset</span><span class="s2">, </span><span class="s1">which=</span><span class="s4">&quot;mean&quot;</span><span class="s1">)</span>

        <span class="s1">kwds = {}</span>
        <span class="s2">if </span><span class="s1">(np.any(n_trials != </span><span class="s5">1</span><span class="s1">) </span><span class="s2">and</span>
                <span class="s1">isinstance(self.family</span><span class="s2">, </span><span class="s1">families.Binomial)):</span>

            <span class="s1">kwds[</span><span class="s4">&quot;n_trials&quot;</span><span class="s1">] = n_trials</span>

        <span class="s1">distr = self.family.get_distribution(mu</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">,</span>
                                             <span class="s1">var_weights=var_weights</span><span class="s2">, </span><span class="s1">**kwds)</span>
        <span class="s2">return </span><span class="s1">distr</span>

    <span class="s2">def </span><span class="s1">_setup_binomial(self):</span>
        <span class="s3"># this checks what kind of data is given for Binomial.</span>
        <span class="s3"># family will need a reference to endog if this is to be removed from</span>
        <span class="s3"># preprocessing</span>
        <span class="s1">self.n_trials = np.ones((self.endog.shape[</span><span class="s5">0</span><span class="s1">]))  </span><span class="s3"># For binomial</span>
        <span class="s2">if </span><span class="s1">isinstance(self.family</span><span class="s2">, </span><span class="s1">families.Binomial):</span>
            <span class="s1">tmp = self.family.initialize(self.endog</span><span class="s2">, </span><span class="s1">self.freq_weights)</span>
            <span class="s1">self.endog = tmp[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">self.n_trials = tmp[</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s1">self._init_keys.append(</span><span class="s4">'n_trials'</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">start_params=</span><span class="s2">None, </span><span class="s1">maxiter=</span><span class="s5">100</span><span class="s2">, </span><span class="s1">method=</span><span class="s4">'IRLS'</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s2">,</span>
            <span class="s1">scale=</span><span class="s2">None, </span><span class="s1">cov_type=</span><span class="s4">'nonrobust'</span><span class="s2">, </span><span class="s1">cov_kwds=</span><span class="s2">None, </span><span class="s1">use_t=</span><span class="s2">None,</span>
            <span class="s1">full_output=</span><span class="s2">True, </span><span class="s1">disp=</span><span class="s2">False, </span><span class="s1">max_start_irls=</span><span class="s5">3</span><span class="s2">, </span><span class="s1">**kwargs):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fits a generalized linear model for a given family. 
 
        Parameters 
        ---------- 
        start_params : array_like, optional 
            Initial guess of the solution for the loglikelihood maximization. 
            The default is family-specific and is given by the 
            ``family.starting_mu(endog)``. If start_params is given then the 
            initial mean will be calculated as ``np.dot(exog, start_params)``. 
        maxiter : int, optional 
            Default is 100. 
        method : str 
            Default is 'IRLS' for iteratively reweighted least squares. 
            Otherwise gradient optimization is used. 
        tol : float 
            Convergence tolerance.  Default is 1e-8. 
        scale : str or float, optional 
            `scale` can be 'X2', 'dev', or a float 
            The default value is None, which uses `X2` for Gamma, Gaussian, 
            and Inverse Gaussian. 
            `X2` is Pearson's chi-square divided by `df_resid`. 
            The default is 1 for the Binomial and Poisson families. 
            `dev` is the deviance divided by df_resid 
        cov_type : str 
            The type of parameter estimate covariance matrix to compute. 
        cov_kwds : dict-like 
            Extra arguments for calculating the covariance of the parameter 
            estimates. 
        use_t : bool 
            If True, the Student t-distribution is used for inference. 
        full_output : bool, optional 
            Set to True to have all available output in the Results object's 
            mle_retvals attribute. The output is dependent on the solver. 
            See LikelihoodModelResults notes section for more information. 
            Not used if methhod is IRLS. 
        disp : bool, optional 
            Set to True to print convergence messages.  Not used if method is 
            IRLS. 
        max_start_irls : int 
            The number of IRLS iterations used to obtain starting 
            values for gradient optimization.  Only relevant if 
            `method` is set to something other than 'IRLS'. 
        atol : float, optional 
            (available with IRLS fits) The absolute tolerance criterion that 
            must be satisfied. Defaults to ``tol``. Convergence is attained 
            when: :math:`rtol * prior + atol &gt; abs(current - prior)` 
        rtol : float, optional 
            (available with IRLS fits) The relative tolerance criterion that 
            must be satisfied. Defaults to 0 which means ``rtol`` is not used. 
            Convergence is attained when: 
            :math:`rtol * prior + atol &gt; abs(current - prior)` 
        tol_criterion : str, optional 
            (available with IRLS fits) Defaults to ``'deviance'``. Can 
            optionally be ``'params'``. 
        wls_method : str, optional 
            (available with IRLS fits) options are 'lstsq', 'pinv' and 'qr' 
            specifies which linear algebra function to use for the irls 
            optimization. Default is `lstsq` which uses the same underlying 
            svd based approach as 'pinv', but is faster during iterations. 
            'lstsq' and 'pinv' regularize the estimate in singular and 
            near-singular cases by truncating small singular values based 
            on `rcond` of the respective numpy.linalg function. 'qr' is 
            only valid for cases that are not singular nor near-singular. 
        optim_hessian : {'eim', 'oim'}, optional 
            (available with scipy optimizer fits) When 'oim'--the default--the 
            observed Hessian is used in fitting. 'eim' is the expected Hessian. 
            This may provide more stable fits, but adds assumption that the 
            Hessian is correctly specified. 
 
        Notes 
        ----- 
        If method is 'IRLS', then an additional keyword 'attach_wls' is 
        available. This is currently for internal use only and might change 
        in future versions. If attach_wls' is true, then the final WLS 
        instance of the IRLS iteration is attached to the results instance 
        as `results_wls` attribute. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">isinstance(scale</span><span class="s2">, </span><span class="s1">str):</span>
            <span class="s1">scale = scale.lower()</span>
            <span class="s2">if </span><span class="s1">scale </span><span class="s2">not in </span><span class="s1">(</span><span class="s4">&quot;x2&quot;</span><span class="s2">, </span><span class="s4">&quot;dev&quot;</span><span class="s1">):</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;scale must be either X2 or dev when a string.&quot;</span>
                <span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">scale </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s3"># GH-6627</span>
            <span class="s2">try</span><span class="s1">:</span>
                <span class="s1">scale = float(scale)</span>
            <span class="s2">except </span><span class="s1">Exception </span><span class="s2">as </span><span class="s1">exc:</span>
                <span class="s2">raise </span><span class="s1">type(exc)(</span>
                    <span class="s4">&quot;scale must be a float if given and no a string.&quot;</span>
                <span class="s1">)</span>
        <span class="s1">self.scaletype = scale</span>

        <span class="s2">if </span><span class="s1">method.lower() == </span><span class="s4">&quot;irls&quot;</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">cov_type.lower() == </span><span class="s4">'eim'</span><span class="s1">:</span>
                <span class="s1">cov_type = </span><span class="s4">'nonrobust'</span>
            <span class="s2">return </span><span class="s1">self._fit_irls(start_params=start_params</span><span class="s2">, </span><span class="s1">maxiter=maxiter</span><span class="s2">,</span>
                                  <span class="s1">tol=tol</span><span class="s2">, </span><span class="s1">scale=scale</span><span class="s2">, </span><span class="s1">cov_type=cov_type</span><span class="s2">,</span>
                                  <span class="s1">cov_kwds=cov_kwds</span><span class="s2">, </span><span class="s1">use_t=use_t</span><span class="s2">, </span><span class="s1">**kwargs)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self._optim_hessian = kwargs.get(</span><span class="s4">'optim_hessian'</span><span class="s1">)</span>
            <span class="s2">if </span><span class="s1">self._optim_hessian </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s2">del </span><span class="s1">kwargs[</span><span class="s4">'optim_hessian'</span><span class="s1">]</span>
            <span class="s1">self._tmp_like_exog = np.empty_like(self.exog</span><span class="s2">, </span><span class="s1">dtype=float)</span>
            <span class="s1">fit_ = self._fit_gradient(start_params=start_params</span><span class="s2">,</span>
                                      <span class="s1">method=method</span><span class="s2">,</span>
                                      <span class="s1">maxiter=maxiter</span><span class="s2">,</span>
                                      <span class="s1">tol=tol</span><span class="s2">, </span><span class="s1">scale=scale</span><span class="s2">,</span>
                                      <span class="s1">full_output=full_output</span><span class="s2">,</span>
                                      <span class="s1">disp=disp</span><span class="s2">, </span><span class="s1">cov_type=cov_type</span><span class="s2">,</span>
                                      <span class="s1">cov_kwds=cov_kwds</span><span class="s2">, </span><span class="s1">use_t=use_t</span><span class="s2">,</span>
                                      <span class="s1">max_start_irls=max_start_irls</span><span class="s2">,</span>
                                      <span class="s1">**kwargs)</span>
            <span class="s2">del </span><span class="s1">self._optim_hessian</span>
            <span class="s2">del </span><span class="s1">self._tmp_like_exog</span>
            <span class="s2">return </span><span class="s1">fit_</span>

    <span class="s2">def </span><span class="s1">_fit_gradient(self</span><span class="s2">, </span><span class="s1">start_params=</span><span class="s2">None, </span><span class="s1">method=</span><span class="s4">&quot;newton&quot;</span><span class="s2">,</span>
                      <span class="s1">maxiter=</span><span class="s5">100</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s2">, </span><span class="s1">full_output=</span><span class="s2">True,</span>
                      <span class="s1">disp=</span><span class="s2">True, </span><span class="s1">scale=</span><span class="s2">None, </span><span class="s1">cov_type=</span><span class="s4">'nonrobust'</span><span class="s2">,</span>
                      <span class="s1">cov_kwds=</span><span class="s2">None, </span><span class="s1">use_t=</span><span class="s2">None, </span><span class="s1">max_start_irls=</span><span class="s5">3</span><span class="s2">,</span>
                      <span class="s1">**kwargs):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fits a generalized linear model for a given family iteratively 
        using the scipy gradient optimizers. 
        &quot;&quot;&quot;</span>

        <span class="s3"># fix scale during optimization, see #4616</span>
        <span class="s1">scaletype = self.scaletype</span>
        <span class="s1">self.scaletype = </span><span class="s5">1.</span>

        <span class="s2">if </span><span class="s1">(max_start_irls &gt; </span><span class="s5">0</span><span class="s1">) </span><span class="s2">and </span><span class="s1">(start_params </span><span class="s2">is None</span><span class="s1">):</span>
            <span class="s1">irls_rslt = self._fit_irls(start_params=start_params</span><span class="s2">,</span>
                                       <span class="s1">maxiter=max_start_irls</span><span class="s2">,</span>
                                       <span class="s1">tol=tol</span><span class="s2">, </span><span class="s1">scale=</span><span class="s5">1.</span><span class="s2">, </span><span class="s1">cov_type=</span><span class="s4">'nonrobust'</span><span class="s2">,</span>
                                       <span class="s1">cov_kwds=</span><span class="s2">None, </span><span class="s1">use_t=</span><span class="s2">None,</span>
                                       <span class="s1">**kwargs)</span>
            <span class="s1">start_params = irls_rslt.params</span>
            <span class="s2">del </span><span class="s1">irls_rslt</span>
        <span class="s1">rslt = super(GLM</span><span class="s2">, </span><span class="s1">self).fit(start_params=start_params</span><span class="s2">,</span>
                                    <span class="s1">maxiter=maxiter</span><span class="s2">, </span><span class="s1">full_output=full_output</span><span class="s2">,</span>
                                    <span class="s1">method=method</span><span class="s2">, </span><span class="s1">disp=disp</span><span class="s2">, </span><span class="s1">**kwargs)</span>

        <span class="s3"># reset scaletype to original</span>
        <span class="s1">self.scaletype = scaletype</span>

        <span class="s1">mu = self.predict(rslt.params)</span>
        <span class="s1">scale = self.estimate_scale(mu)</span>

        <span class="s2">if </span><span class="s1">rslt.normalized_cov_params </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">cov_p = </span><span class="s2">None</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">cov_p = rslt.normalized_cov_params / scale</span>

        <span class="s2">if </span><span class="s1">cov_type.lower() == </span><span class="s4">'eim'</span><span class="s1">:</span>
            <span class="s1">oim = </span><span class="s2">False</span>
            <span class="s1">cov_type = </span><span class="s4">'nonrobust'</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">oim = </span><span class="s2">True</span>

        <span class="s2">try</span><span class="s1">:</span>
            <span class="s1">cov_p = np.linalg.inv(-self.hessian(rslt.params</span><span class="s2">, </span><span class="s1">observed=oim)) / scale</span>
        <span class="s2">except </span><span class="s1">LinAlgError:</span>
            <span class="s1">warnings.warn(</span><span class="s4">'Inverting hessian failed, no bse or cov_params '</span>
                          <span class="s4">'available'</span><span class="s2">, </span><span class="s1">HessianInversionWarning)</span>
            <span class="s1">cov_p = </span><span class="s2">None</span>

        <span class="s1">results_class = getattr(self</span><span class="s2">, </span><span class="s4">'_results_class'</span><span class="s2">, </span><span class="s1">GLMResults)</span>
        <span class="s1">results_class_wrapper = getattr(self</span><span class="s2">, </span><span class="s4">'_results_class_wrapper'</span><span class="s2">, </span><span class="s1">GLMResultsWrapper)</span>
        <span class="s1">glm_results = results_class(self</span><span class="s2">, </span><span class="s1">rslt.params</span><span class="s2">,</span>
                                    <span class="s1">cov_p</span><span class="s2">,</span>
                                    <span class="s1">scale</span><span class="s2">,</span>
                                    <span class="s1">cov_type=cov_type</span><span class="s2">, </span><span class="s1">cov_kwds=cov_kwds</span><span class="s2">,</span>
                                    <span class="s1">use_t=use_t)</span>

        <span class="s3"># TODO: iteration count is not always available</span>
        <span class="s1">history = {</span><span class="s4">'iteration'</span><span class="s1">: </span><span class="s5">0</span><span class="s1">}</span>
        <span class="s2">if </span><span class="s1">full_output:</span>
            <span class="s1">glm_results.mle_retvals = rslt.mle_retvals</span>
            <span class="s2">if </span><span class="s4">'iterations' </span><span class="s2">in </span><span class="s1">rslt.mle_retvals:</span>
                <span class="s1">history[</span><span class="s4">'iteration'</span><span class="s1">] = rslt.mle_retvals[</span><span class="s4">'iterations'</span><span class="s1">]</span>
        <span class="s1">glm_results.method = method</span>
        <span class="s1">glm_results.fit_history = history</span>

        <span class="s2">return </span><span class="s1">results_class_wrapper(glm_results)</span>

    <span class="s2">def </span><span class="s1">_fit_irls(self</span><span class="s2">, </span><span class="s1">start_params=</span><span class="s2">None, </span><span class="s1">maxiter=</span><span class="s5">100</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s2">,</span>
                  <span class="s1">scale=</span><span class="s2">None, </span><span class="s1">cov_type=</span><span class="s4">'nonrobust'</span><span class="s2">, </span><span class="s1">cov_kwds=</span><span class="s2">None,</span>
                  <span class="s1">use_t=</span><span class="s2">None, </span><span class="s1">**kwargs):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fits a generalized linear model for a given family using 
        iteratively reweighted least squares (IRLS). 
        &quot;&quot;&quot;</span>
        <span class="s1">attach_wls = kwargs.pop(</span><span class="s4">'attach_wls'</span><span class="s2">, False</span><span class="s1">)</span>
        <span class="s1">atol = kwargs.get(</span><span class="s4">'atol'</span><span class="s1">)</span>
        <span class="s1">rtol = kwargs.get(</span><span class="s4">'rtol'</span><span class="s2">, </span><span class="s5">0.</span><span class="s1">)</span>
        <span class="s1">tol_criterion = kwargs.get(</span><span class="s4">'tol_criterion'</span><span class="s2">, </span><span class="s4">'deviance'</span><span class="s1">)</span>
        <span class="s1">wls_method = kwargs.get(</span><span class="s4">'wls_method'</span><span class="s2">, </span><span class="s4">'lstsq'</span><span class="s1">)</span>
        <span class="s1">atol = tol </span><span class="s2">if </span><span class="s1">atol </span><span class="s2">is None else </span><span class="s1">atol</span>

        <span class="s1">endog = self.endog</span>
        <span class="s1">wlsexog = self.exog</span>
        <span class="s2">if </span><span class="s1">start_params </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">start_params = np.zeros(self.exog.shape[</span><span class="s5">1</span><span class="s1">])</span>
            <span class="s1">mu = self.family.starting_mu(self.endog)</span>
            <span class="s1">lin_pred = self.family.predict(mu)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">lin_pred = np.dot(wlsexog</span><span class="s2">, </span><span class="s1">start_params) + self._offset_exposure</span>
            <span class="s1">mu = self.family.fitted(lin_pred)</span>
        <span class="s1">self.scale = self.estimate_scale(mu)</span>
        <span class="s1">dev = self.family.deviance(self.endog</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">self.var_weights</span><span class="s2">,</span>
                                   <span class="s1">self.freq_weights</span><span class="s2">, </span><span class="s1">self.scale)</span>
        <span class="s2">if </span><span class="s1">np.isnan(dev):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;The first guess on the deviance function &quot;</span>
                             <span class="s4">&quot;returned a nan.  This could be a boundary &quot;</span>
                             <span class="s4">&quot; problem and should be reported.&quot;</span><span class="s1">)</span>

        <span class="s3"># first guess on the deviance is assumed to be scaled by 1.</span>
        <span class="s3"># params are none to start, so they line up with the deviance</span>
        <span class="s1">history = dict(params=[np.inf</span><span class="s2">, </span><span class="s1">start_params]</span><span class="s2">, </span><span class="s1">deviance=[np.inf</span><span class="s2">, </span><span class="s1">dev])</span>
        <span class="s1">converged = </span><span class="s2">False</span>
        <span class="s1">criterion = history[tol_criterion]</span>
        <span class="s3"># This special case is used to get the likelihood for a specific</span>
        <span class="s3"># params vector.</span>
        <span class="s2">if </span><span class="s1">maxiter == </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">mu = self.family.fitted(lin_pred)</span>
            <span class="s1">self.scale = self.estimate_scale(mu)</span>
            <span class="s1">wls_results = lm.RegressionResults(self</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, None</span><span class="s1">)</span>
            <span class="s1">iteration = </span><span class="s5">0</span>
        <span class="s2">for </span><span class="s1">iteration </span><span class="s2">in </span><span class="s1">range(maxiter):</span>
            <span class="s1">self.weights = (self.iweights * self.n_trials *</span>
                            <span class="s1">self.family.weights(mu))</span>
            <span class="s1">wlsendog = (lin_pred + self.family.link.deriv(mu) * (self.endog-mu)</span>
                        <span class="s1">- self._offset_exposure)</span>
            <span class="s1">wls_mod = reg_tools._MinimalWLS(wlsendog</span><span class="s2">, </span><span class="s1">wlsexog</span><span class="s2">,</span>
                                            <span class="s1">self.weights</span><span class="s2">, </span><span class="s1">check_endog=</span><span class="s2">True,</span>
                                            <span class="s1">check_weights=</span><span class="s2">True</span><span class="s1">)</span>
            <span class="s1">wls_results = wls_mod.fit(method=wls_method)</span>
            <span class="s1">lin_pred = np.dot(self.exog</span><span class="s2">, </span><span class="s1">wls_results.params)</span>
            <span class="s1">lin_pred += self._offset_exposure</span>
            <span class="s1">mu = self.family.fitted(lin_pred)</span>
            <span class="s1">history = self._update_history(wls_results</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">history)</span>
            <span class="s1">self.scale = self.estimate_scale(mu)</span>
            <span class="s2">if </span><span class="s1">endog.squeeze().ndim == </span><span class="s5">1 </span><span class="s2">and </span><span class="s1">np.allclose(mu - endog</span><span class="s2">, </span><span class="s5">0</span><span class="s1">):</span>
                <span class="s1">msg = (</span><span class="s4">&quot;Perfect separation or prediction detected, &quot;</span>
                       <span class="s4">&quot;parameter may not be identified&quot;</span><span class="s1">)</span>
                <span class="s1">warnings.warn(msg</span><span class="s2">, </span><span class="s1">category=PerfectSeparationWarning)</span>
            <span class="s1">converged = _check_convergence(criterion</span><span class="s2">, </span><span class="s1">iteration + </span><span class="s5">1</span><span class="s2">, </span><span class="s1">atol</span><span class="s2">,</span>
                                           <span class="s1">rtol)</span>
            <span class="s2">if </span><span class="s1">converged:</span>
                <span class="s2">break</span>
        <span class="s1">self.mu = mu</span>

        <span class="s2">if </span><span class="s1">maxiter &gt; </span><span class="s5">0</span><span class="s1">:  </span><span class="s3"># Only if iterative used</span>
            <span class="s1">wls_method2 = </span><span class="s4">'pinv' </span><span class="s2">if </span><span class="s1">wls_method == </span><span class="s4">'lstsq' </span><span class="s2">else </span><span class="s1">wls_method</span>
            <span class="s1">wls_model = lm.WLS(wlsendog</span><span class="s2">, </span><span class="s1">wlsexog</span><span class="s2">, </span><span class="s1">self.weights)</span>
            <span class="s1">wls_results = wls_model.fit(method=wls_method2)</span>

        <span class="s1">glm_results = GLMResults(self</span><span class="s2">, </span><span class="s1">wls_results.params</span><span class="s2">,</span>
                                 <span class="s1">wls_results.normalized_cov_params</span><span class="s2">,</span>
                                 <span class="s1">self.scale</span><span class="s2">,</span>
                                 <span class="s1">cov_type=cov_type</span><span class="s2">, </span><span class="s1">cov_kwds=cov_kwds</span><span class="s2">,</span>
                                 <span class="s1">use_t=use_t)</span>

        <span class="s1">glm_results.method = </span><span class="s4">&quot;IRLS&quot;</span>
        <span class="s1">glm_results.mle_settings = {}</span>
        <span class="s1">glm_results.mle_settings[</span><span class="s4">'wls_method'</span><span class="s1">] = wls_method</span>
        <span class="s1">glm_results.mle_settings[</span><span class="s4">'optimizer'</span><span class="s1">] = glm_results.method</span>
        <span class="s2">if </span><span class="s1">(maxiter &gt; </span><span class="s5">0</span><span class="s1">) </span><span class="s2">and </span><span class="s1">(attach_wls </span><span class="s2">is True</span><span class="s1">):</span>
            <span class="s1">glm_results.results_wls = wls_results</span>
        <span class="s1">history[</span><span class="s4">'iteration'</span><span class="s1">] = iteration + </span><span class="s5">1</span>
        <span class="s1">glm_results.fit_history = history</span>
        <span class="s1">glm_results.converged = converged</span>
        <span class="s2">return </span><span class="s1">GLMResultsWrapper(glm_results)</span>

    <span class="s2">def </span><span class="s1">fit_regularized(self</span><span class="s2">, </span><span class="s1">method=</span><span class="s4">&quot;elastic_net&quot;</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s5">0.</span><span class="s2">,</span>
                        <span class="s1">start_params=</span><span class="s2">None, </span><span class="s1">refit=</span><span class="s2">False,</span>
                        <span class="s1">opt_method=</span><span class="s4">&quot;bfgs&quot;</span><span class="s2">, </span><span class="s1">**kwargs):</span>
        <span class="s0">r&quot;&quot;&quot; 
        Return a regularized fit to a linear regression model. 
 
        Parameters 
        ---------- 
        method : {'elastic_net'} 
            Only the `elastic_net` approach is currently implemented. 
        alpha : scalar or array_like 
            The penalty weight.  If a scalar, the same penalty weight 
            applies to all variables in the model.  If a vector, it 
            must have the same length as `params`, and contains a 
            penalty weight for each coefficient. 
        start_params : array_like 
            Starting values for `params`. 
        refit : bool 
            If True, the model is refit using only the variables that 
            have non-zero coefficients in the regularized fit.  The 
            refitted model is not regularized. 
        opt_method : string 
            The method used for numerical optimization. 
        **kwargs 
            Additional keyword arguments used when fitting the model. 
 
        Returns 
        ------- 
        GLMResults 
            An array or a GLMResults object, same type returned by `fit`. 
 
        Notes 
        ----- 
        The penalty is the ``elastic net`` penalty, which is a 
        combination of L1 and L2 penalties. 
 
        The function that is minimized is: 
 
        .. math:: 
 
            -loglike/n + alpha*((1-L1\_wt)*|params|_2^2/2 + L1\_wt*|params|_1) 
 
        where :math:`|*|_1` and :math:`|*|_2` are the L1 and L2 norms. 
 
        Post-estimation results are based on the same data used to 
        select variables, hence may be subject to overfitting biases. 
 
        The elastic_net method uses the following keyword arguments: 
 
        maxiter : int 
            Maximum number of iterations 
        L1_wt  : float 
            Must be in [0, 1].  The L1 penalty has weight L1_wt and the 
            L2 penalty has weight 1 - L1_wt. 
        cnvrg_tol : float 
            Convergence threshold for maximum parameter change after 
            one sweep through all coefficients. 
        zero_tol : float 
            Coefficients below this threshold are treated as zero. 
        &quot;&quot;&quot;</span>

        <span class="s2">if </span><span class="s1">kwargs.get(</span><span class="s4">&quot;L1_wt&quot;</span><span class="s2">, </span><span class="s5">1</span><span class="s1">) == </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">self._fit_ridge(alpha</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">opt_method)</span>

        <span class="s2">from </span><span class="s1">statsmodels.base.elastic_net </span><span class="s2">import </span><span class="s1">fit_elasticnet</span>

        <span class="s2">if </span><span class="s1">method != </span><span class="s4">&quot;elastic_net&quot;</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;method for fit_regularized must be elastic_net&quot;</span><span class="s1">)</span>

        <span class="s1">defaults = {</span><span class="s4">&quot;maxiter&quot;</span><span class="s1">: </span><span class="s5">50</span><span class="s2">, </span><span class="s4">&quot;L1_wt&quot;</span><span class="s1">: </span><span class="s5">1</span><span class="s2">, </span><span class="s4">&quot;cnvrg_tol&quot;</span><span class="s1">: </span><span class="s5">1e-10</span><span class="s2">,</span>
                    <span class="s4">&quot;zero_tol&quot;</span><span class="s1">: </span><span class="s5">1e-10</span><span class="s1">}</span>
        <span class="s1">defaults.update(kwargs)</span>

        <span class="s1">llkw = kwargs.get(</span><span class="s4">&quot;loglike_kwds&quot;</span><span class="s2">, </span><span class="s1">{})</span>
        <span class="s1">sckw = kwargs.get(</span><span class="s4">&quot;score_kwds&quot;</span><span class="s2">, </span><span class="s1">{})</span>
        <span class="s1">hekw = kwargs.get(</span><span class="s4">&quot;hess_kwds&quot;</span><span class="s2">, </span><span class="s1">{})</span>
        <span class="s1">llkw[</span><span class="s4">&quot;scale&quot;</span><span class="s1">] = </span><span class="s5">1</span>
        <span class="s1">sckw[</span><span class="s4">&quot;scale&quot;</span><span class="s1">] = </span><span class="s5">1</span>
        <span class="s1">hekw[</span><span class="s4">&quot;scale&quot;</span><span class="s1">] = </span><span class="s5">1</span>
        <span class="s1">defaults[</span><span class="s4">&quot;loglike_kwds&quot;</span><span class="s1">] = llkw</span>
        <span class="s1">defaults[</span><span class="s4">&quot;score_kwds&quot;</span><span class="s1">] = sckw</span>
        <span class="s1">defaults[</span><span class="s4">&quot;hess_kwds&quot;</span><span class="s1">] = hekw</span>

        <span class="s1">result = fit_elasticnet(self</span><span class="s2">, </span><span class="s1">method=method</span><span class="s2">,</span>
                                <span class="s1">alpha=alpha</span><span class="s2">,</span>
                                <span class="s1">start_params=start_params</span><span class="s2">,</span>
                                <span class="s1">refit=refit</span><span class="s2">,</span>
                                <span class="s1">**defaults)</span>

        <span class="s1">self.mu = self.predict(result.params)</span>
        <span class="s1">self.scale = self.estimate_scale(self.mu)</span>

        <span class="s2">if not </span><span class="s1">result.converged:</span>
            <span class="s1">warnings.warn(</span><span class="s4">&quot;Elastic net fitting did not converge&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">result</span>

    <span class="s2">def </span><span class="s1">_fit_ridge(self</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">method):</span>

        <span class="s2">if </span><span class="s1">start_params </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">start_params = np.zeros(self.exog.shape[</span><span class="s5">1</span><span class="s1">])</span>

        <span class="s2">def </span><span class="s1">fun(x):</span>
            <span class="s2">return </span><span class="s1">-(self.loglike(x) / self.nobs - np.sum(alpha * x**</span><span class="s5">2</span><span class="s1">) / </span><span class="s5">2</span><span class="s1">)</span>

        <span class="s2">def </span><span class="s1">grad(x):</span>
            <span class="s2">return </span><span class="s1">-(self.score(x) / self.nobs - alpha * x)</span>

        <span class="s2">from </span><span class="s1">scipy.optimize </span><span class="s2">import </span><span class="s1">minimize</span>

        <span class="s2">from </span><span class="s1">statsmodels.base.elastic_net </span><span class="s2">import </span><span class="s1">(</span>
            <span class="s1">RegularizedResults</span><span class="s2">,</span>
            <span class="s1">RegularizedResultsWrapper</span><span class="s2">,</span>
        <span class="s1">)</span>

        <span class="s1">mr = minimize(fun</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">jac=grad</span><span class="s2">, </span><span class="s1">method=method)</span>
        <span class="s1">params = mr.x</span>

        <span class="s2">if not </span><span class="s1">mr.success:</span>
            <span class="s1">ngrad = np.sqrt(np.sum(mr.jac**</span><span class="s5">2</span><span class="s1">))</span>
            <span class="s1">msg = </span><span class="s4">&quot;GLM ridge optimization may have failed, |grad|=%f&quot; </span><span class="s1">% ngrad</span>
            <span class="s1">warnings.warn(msg)</span>

        <span class="s1">results = RegularizedResults(self</span><span class="s2">, </span><span class="s1">params)</span>
        <span class="s1">results = RegularizedResultsWrapper(results)</span>

        <span class="s2">return </span><span class="s1">results</span>

    <span class="s2">def </span><span class="s1">fit_constrained(self</span><span class="s2">, </span><span class="s1">constraints</span><span class="s2">, </span><span class="s1">start_params=</span><span class="s2">None, </span><span class="s1">**fit_kwds):</span>
        <span class="s0">&quot;&quot;&quot;fit the model subject to linear equality constraints 
 
        The constraints are of the form   `R params = q` 
        where R is the constraint_matrix and q is the vector of 
        constraint_values. 
 
        The estimation creates a new model with transformed design matrix, 
        exog, and converts the results back to the original parameterization. 
 
 
        Parameters 
        ---------- 
        constraints : formula expression or tuple 
            If it is a tuple, then the constraint needs to be given by two 
            arrays (constraint_matrix, constraint_value), i.e. (R, q). 
            Otherwise, the constraints can be given as strings or list of 
            strings. 
            see t_test for details 
        start_params : None or array_like 
            starting values for the optimization. `start_params` needs to be 
            given in the original parameter space and are internally 
            transformed. 
        **fit_kwds : keyword arguments 
            fit_kwds are used in the optimization of the transformed model. 
 
        Returns 
        ------- 
        results : Results instance 
        &quot;&quot;&quot;</span>

        <span class="s2">from </span><span class="s1">patsy </span><span class="s2">import </span><span class="s1">DesignInfo</span>

        <span class="s2">from </span><span class="s1">statsmodels.base._constraints </span><span class="s2">import </span><span class="s1">(</span>
            <span class="s1">LinearConstraints</span><span class="s2">,</span>
            <span class="s1">fit_constrained</span><span class="s2">,</span>
        <span class="s1">)</span>

        <span class="s3"># same pattern as in base.LikelihoodModel.t_test</span>
        <span class="s1">lc = DesignInfo(self.exog_names).linear_constraint(constraints)</span>
        <span class="s1">R</span><span class="s2">, </span><span class="s1">q = lc.coefs</span><span class="s2">, </span><span class="s1">lc.constants</span>

        <span class="s3"># TODO: add start_params option, need access to tranformation</span>
        <span class="s3">#       fit_constrained needs to do the transformation</span>
        <span class="s1">params</span><span class="s2">, </span><span class="s1">cov</span><span class="s2">, </span><span class="s1">res_constr = fit_constrained(self</span><span class="s2">, </span><span class="s1">R</span><span class="s2">, </span><span class="s1">q</span><span class="s2">,</span>
                                                  <span class="s1">start_params=start_params</span><span class="s2">,</span>
                                                  <span class="s1">fit_kwds=fit_kwds)</span>
        <span class="s3"># create dummy results Instance, TODO: wire up properly</span>
        <span class="s1">res = self.fit(start_params=params</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s5">0</span><span class="s1">)  </span><span class="s3"># we get a wrapper back</span>
        <span class="s1">res._results.params = params</span>
        <span class="s1">res._results.cov_params_default = cov</span>
        <span class="s1">cov_type = fit_kwds.get(</span><span class="s4">'cov_type'</span><span class="s2">, </span><span class="s4">'nonrobust'</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">cov_type != </span><span class="s4">'nonrobust'</span><span class="s1">:</span>
            <span class="s1">res._results.normalized_cov_params = cov / res_constr.scale</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">res._results.normalized_cov_params = </span><span class="s2">None</span>
        <span class="s1">res._results.scale = res_constr.scale</span>
        <span class="s1">k_constr = len(q)</span>
        <span class="s1">res._results.df_resid += k_constr</span>
        <span class="s1">res._results.df_model -= k_constr</span>
        <span class="s1">res._results.constraints = LinearConstraints.from_patsy(lc)</span>
        <span class="s1">res._results.k_constr = k_constr</span>
        <span class="s1">res._results.results_constrained = res_constr</span>
        <span class="s2">return </span><span class="s1">res</span>


<span class="s1">get_prediction_doc = Docstring(pred.get_prediction_glm.__doc__)</span>
<span class="s1">get_prediction_doc.remove_parameters(</span><span class="s4">&quot;pred_kwds&quot;</span><span class="s1">)</span>


<span class="s2">class </span><span class="s1">GLMResults(base.LikelihoodModelResults):</span>
    <span class="s0">&quot;&quot;&quot; 
    Class to contain GLM results. 
 
    GLMResults inherits from statsmodels.LikelihoodModelResults 
 
    Attributes 
    ---------- 
    df_model : float 
        See GLM.df_model 
    df_resid : float 
        See GLM.df_resid 
    fit_history : dict 
        Contains information about the iterations. Its keys are `iterations`, 
        `deviance` and `params`. 
    model : class instance 
        Pointer to GLM model instance that called fit. 
    nobs : float 
        The number of observations n. 
    normalized_cov_params : ndarray 
        See GLM docstring 
    params : ndarray 
        The coefficients of the fitted model.  Note that interpretation 
        of the coefficients often depends on the distribution family and the 
        data. 
    pvalues : ndarray 
        The two-tailed p-values for the parameters. 
    scale : float 
        The estimate of the scale / dispersion for the model fit. 
        See GLM.fit and GLM.estimate_scale for more information. 
    stand_errors : ndarray 
        The standard errors of the fitted GLM.   #TODO still named bse 
 
    See Also 
    -------- 
    statsmodels.base.model.LikelihoodModelResults 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">model</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">normalized_cov_params</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">,</span>
                 <span class="s1">cov_type=</span><span class="s4">'nonrobust'</span><span class="s2">, </span><span class="s1">cov_kwds=</span><span class="s2">None, </span><span class="s1">use_t=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super(GLMResults</span><span class="s2">, </span><span class="s1">self).__init__(</span>
                <span class="s1">model</span><span class="s2">,</span>
                <span class="s1">params</span><span class="s2">,</span>
                <span class="s1">normalized_cov_params=normalized_cov_params</span><span class="s2">,</span>
                <span class="s1">scale=scale)</span>
        <span class="s1">self.family = model.family</span>
        <span class="s1">self._endog = model.endog</span>
        <span class="s1">self.nobs = model.endog.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">self._freq_weights = model.freq_weights</span>
        <span class="s1">self._var_weights = model.var_weights</span>
        <span class="s1">self._iweights = model.iweights</span>
        <span class="s2">if </span><span class="s1">isinstance(self.family</span><span class="s2">, </span><span class="s1">families.Binomial):</span>
            <span class="s1">self._n_trials = self.model.n_trials</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self._n_trials = </span><span class="s5">1</span>
        <span class="s1">self.df_resid = model.df_resid</span>
        <span class="s1">self.df_model = model.df_model</span>
        <span class="s1">self._cache = {}</span>
        <span class="s3"># are these intermediate results needed or can we just</span>
        <span class="s3"># call the model's attributes?</span>

        <span class="s3"># for remove data and pickle without large arrays</span>
        <span class="s1">self._data_attr.extend([</span><span class="s4">'results_constrained'</span><span class="s2">, </span><span class="s4">'_freq_weights'</span><span class="s2">,</span>
                                <span class="s4">'_var_weights'</span><span class="s2">, </span><span class="s4">'_iweights'</span><span class="s1">])</span>
        <span class="s1">self._data_in_cache.extend([</span><span class="s4">'null'</span><span class="s2">, </span><span class="s4">'mu'</span><span class="s1">])</span>
        <span class="s1">self._data_attr_model = getattr(self</span><span class="s2">, </span><span class="s4">'_data_attr_model'</span><span class="s2">, </span><span class="s1">[])</span>
        <span class="s1">self._data_attr_model.append(</span><span class="s4">'mu'</span><span class="s1">)</span>

        <span class="s3"># robust covariance</span>
        <span class="s2">from </span><span class="s1">statsmodels.base.covtype </span><span class="s2">import </span><span class="s1">get_robustcov_results</span>
        <span class="s2">if </span><span class="s1">use_t </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">self.use_t = </span><span class="s2">False    </span><span class="s3"># TODO: class default</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self.use_t = use_t</span>

        <span class="s3"># temporary warning</span>
        <span class="s1">ct = (cov_type == </span><span class="s4">'nonrobust'</span><span class="s1">) </span><span class="s2">or </span><span class="s1">(cov_type.upper().startswith(</span><span class="s4">'HC'</span><span class="s1">))</span>
        <span class="s2">if </span><span class="s1">self.model._has_freq_weights </span><span class="s2">and not </span><span class="s1">ct:</span>

            <span class="s2">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s2">import </span><span class="s1">SpecificationWarning</span>
            <span class="s1">warnings.warn(</span><span class="s4">'cov_type not fully supported with freq_weights'</span><span class="s2">,</span>
                          <span class="s1">SpecificationWarning)</span>

        <span class="s2">if </span><span class="s1">self.model._has_var_weights </span><span class="s2">and not </span><span class="s1">ct:</span>

            <span class="s2">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s2">import </span><span class="s1">SpecificationWarning</span>
            <span class="s1">warnings.warn(</span><span class="s4">'cov_type not fully supported with var_weights'</span><span class="s2">,</span>
                          <span class="s1">SpecificationWarning)</span>

        <span class="s2">if </span><span class="s1">cov_type == </span><span class="s4">'nonrobust'</span><span class="s1">:</span>
            <span class="s1">self.cov_type = </span><span class="s4">'nonrobust'</span>
            <span class="s1">self.cov_kwds = {</span><span class="s4">'description'</span><span class="s1">: </span><span class="s4">'Standard Errors assume that the' </span><span class="s1">+</span>
                             <span class="s4">' covariance matrix of the errors is correctly ' </span><span class="s1">+</span>
                             <span class="s4">'specified.'</span><span class="s1">}</span>

        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">cov_kwds </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s1">cov_kwds = {}</span>
            <span class="s1">get_robustcov_results(self</span><span class="s2">, </span><span class="s1">cov_type=cov_type</span><span class="s2">, </span><span class="s1">use_self=</span><span class="s2">True,</span>
                                  <span class="s1">use_t=use_t</span><span class="s2">, </span><span class="s1">**cov_kwds)</span>

    <span class="s1">@cached_data</span>
    <span class="s2">def </span><span class="s1">resid_response(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Response residuals.  The response residuals are defined as 
        `endog` - `fittedvalues` 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self._n_trials * (self._endog-self.mu)</span>

    <span class="s1">@cached_data</span>
    <span class="s2">def </span><span class="s1">resid_pearson(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Pearson residuals.  The Pearson residuals are defined as 
        (`endog` - `mu`)/sqrt(VAR(`mu`)) where VAR is the distribution 
        specific variance function.  See statsmodels.families.family and 
        statsmodels.families.varfuncs for more information. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">(np.sqrt(self._n_trials) * (self._endog-self.mu) *</span>
                <span class="s1">np.sqrt(self._var_weights) /</span>
                <span class="s1">np.sqrt(self.family.variance(self.mu)))</span>

    <span class="s1">@cached_data</span>
    <span class="s2">def </span><span class="s1">resid_working(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Working residuals.  The working residuals are defined as 
        `resid_response`/link'(`mu`).  See statsmodels.family.links for the 
        derivatives of the link functions.  They are defined analytically. 
        &quot;&quot;&quot;</span>
        <span class="s3"># Isn't self.resid_response is already adjusted by _n_trials?</span>
        <span class="s1">val = (self.resid_response * self.family.link.deriv(self.mu))</span>
        <span class="s1">val *= self._n_trials</span>
        <span class="s2">return </span><span class="s1">val</span>

    <span class="s1">@cached_data</span>
    <span class="s2">def </span><span class="s1">resid_anscombe(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Anscombe residuals.  See statsmodels.families.family for distribution- 
        specific Anscombe residuals. Currently, the unscaled residuals are 
        provided. In a future version, the scaled residuals will be provided. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.resid_anscombe_scaled</span>

    <span class="s1">@cached_data</span>
    <span class="s2">def </span><span class="s1">resid_anscombe_scaled(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Scaled Anscombe residuals.  See statsmodels.families.family for 
        distribution-specific Anscombe residuals. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.family.resid_anscombe(self._endog</span><span class="s2">, </span><span class="s1">self.fittedvalues</span><span class="s2">,</span>
                                          <span class="s1">var_weights=self._var_weights</span><span class="s2">,</span>
                                          <span class="s1">scale=self.scale)</span>

    <span class="s1">@cached_data</span>
    <span class="s2">def </span><span class="s1">resid_anscombe_unscaled(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Unscaled Anscombe residuals.  See statsmodels.families.family for 
        distribution-specific Anscombe residuals. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.family.resid_anscombe(self._endog</span><span class="s2">, </span><span class="s1">self.fittedvalues</span><span class="s2">,</span>
                                          <span class="s1">var_weights=self._var_weights</span><span class="s2">,</span>
                                          <span class="s1">scale=</span><span class="s5">1.</span><span class="s1">)</span>

    <span class="s1">@cached_data</span>
    <span class="s2">def </span><span class="s1">resid_deviance(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Deviance residuals.  See statsmodels.families.family for distribution- 
        specific deviance residuals. 
        &quot;&quot;&quot;</span>
        <span class="s1">dev = self.family.resid_dev(self._endog</span><span class="s2">, </span><span class="s1">self.fittedvalues</span><span class="s2">,</span>
                                    <span class="s1">var_weights=self._var_weights</span><span class="s2">,</span>
                                    <span class="s1">scale=</span><span class="s5">1.</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">dev</span>

    <span class="s1">@cached_value</span>
    <span class="s2">def </span><span class="s1">pearson_chi2(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Pearson's Chi-Squared statistic is defined as the sum of the squares 
        of the Pearson residuals. 
        &quot;&quot;&quot;</span>
        <span class="s1">chisq = (self._endog - self.mu)**</span><span class="s5">2 </span><span class="s1">/ self.family.variance(self.mu)</span>
        <span class="s1">chisq *= self._iweights * self._n_trials</span>
        <span class="s1">chisqsum = np.sum(chisq)</span>
        <span class="s2">return </span><span class="s1">chisqsum</span>

    <span class="s1">@cached_data</span>
    <span class="s2">def </span><span class="s1">fittedvalues(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        The estimated mean response. 
 
        This is the value of the inverse of the link function at 
        lin_pred, where lin_pred is the linear predicted value 
        obtained by multiplying the design matrix by the coefficient 
        vector. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.mu</span>

    <span class="s1">@cached_data</span>
    <span class="s2">def </span><span class="s1">mu(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        See GLM docstring. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.model.predict(self.params)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s2">def </span><span class="s1">null(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fitted values of the null model 
        &quot;&quot;&quot;</span>
        <span class="s1">endog = self._endog</span>
        <span class="s1">model = self.model</span>
        <span class="s1">exog = np.ones((len(endog)</span><span class="s2">, </span><span class="s5">1</span><span class="s1">))</span>

        <span class="s1">kwargs = model._get_init_kwds().copy()</span>
        <span class="s1">kwargs.pop(</span><span class="s4">'family'</span><span class="s1">)</span>

        <span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">getattr(model</span><span class="s2">, </span><span class="s4">'_null_drop_keys'</span><span class="s2">, </span><span class="s1">[]):</span>
            <span class="s2">del </span><span class="s1">kwargs[key]</span>
        <span class="s1">start_params = np.atleast_1d(self.family.link(endog.mean()))</span>
        <span class="s1">oe = self.model._offset_exposure</span>
        <span class="s2">if not </span><span class="s1">(np.size(oe) == </span><span class="s5">1 </span><span class="s2">and </span><span class="s1">oe == </span><span class="s5">0</span><span class="s1">):</span>
            <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
                <span class="s1">warnings.simplefilter(</span><span class="s4">&quot;ignore&quot;</span><span class="s2">, </span><span class="s1">DomainWarning)</span>
                <span class="s1">mod = GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">family=self.family</span><span class="s2">, </span><span class="s1">**kwargs)</span>
                <span class="s1">fitted = mod.fit(start_params=start_params).fittedvalues</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s3"># correct if fitted is identical across observations</span>
            <span class="s1">wls_model = lm.WLS(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">,</span>
                               <span class="s1">weights=self._iweights * self._n_trials)</span>
            <span class="s1">fitted = wls_model.fit().fittedvalues</span>

        <span class="s2">return </span><span class="s1">fitted</span>

    <span class="s1">@cache_readonly</span>
    <span class="s2">def </span><span class="s1">deviance(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        See statsmodels.families.family for the distribution-specific deviance 
        functions. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.family.deviance(self._endog</span><span class="s2">, </span><span class="s1">self.mu</span><span class="s2">, </span><span class="s1">self._var_weights</span><span class="s2">,</span>
                                    <span class="s1">self._freq_weights)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s2">def </span><span class="s1">null_deviance(self):</span>
        <span class="s0">&quot;&quot;&quot;The value of the deviance function for the model fit with a constant 
        as the only regressor.&quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.family.deviance(self._endog</span><span class="s2">, </span><span class="s1">self.null</span><span class="s2">, </span><span class="s1">self._var_weights</span><span class="s2">,</span>
                                    <span class="s1">self._freq_weights)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s2">def </span><span class="s1">llnull(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Log-likelihood of the model fit with a constant as the only regressor 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.family.loglike(self._endog</span><span class="s2">, </span><span class="s1">self.null</span><span class="s2">,</span>
                                   <span class="s1">var_weights=self._var_weights</span><span class="s2">,</span>
                                   <span class="s1">freq_weights=self._freq_weights</span><span class="s2">,</span>
                                   <span class="s1">scale=self.scale)</span>

    <span class="s2">def </span><span class="s1">llf_scaled(self</span><span class="s2">, </span><span class="s1">scale=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return the log-likelihood at the given scale, using the 
        estimated scale if the provided scale is None.  In the Gaussian 
        case with linear link, the concentrated log-likelihood is 
        returned. 
        &quot;&quot;&quot;</span>

        <span class="s1">_modelfamily = self.family</span>
        <span class="s2">if </span><span class="s1">scale </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">(isinstance(self.family</span><span class="s2">, </span><span class="s1">families.Gaussian) </span><span class="s2">and</span>
                    <span class="s1">isinstance(self.family.link</span><span class="s2">, </span><span class="s1">families.links.Power) </span><span class="s2">and</span>
                    <span class="s1">(self.family.link.power == </span><span class="s5">1.</span><span class="s1">)):</span>
                <span class="s3"># Scale for the concentrated Gaussian log likelihood</span>
                <span class="s3"># (profile log likelihood with the scale parameter</span>
                <span class="s3"># profiled out).</span>
                <span class="s1">scale = (np.power(self._endog - self.mu</span><span class="s2">, </span><span class="s5">2</span><span class="s1">) * self._iweights).sum()</span>
                <span class="s1">scale /= self.model.wnobs</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">scale = self.scale</span>
        <span class="s1">val = _modelfamily.loglike(self._endog</span><span class="s2">, </span><span class="s1">self.mu</span><span class="s2">,</span>
                                   <span class="s1">var_weights=self._var_weights</span><span class="s2">,</span>
                                   <span class="s1">freq_weights=self._freq_weights</span><span class="s2">,</span>
                                   <span class="s1">scale=scale)</span>
        <span class="s2">return </span><span class="s1">val</span>

    <span class="s1">@cached_value</span>
    <span class="s2">def </span><span class="s1">llf(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Value of the loglikelihood function evalued at params. 
        See statsmodels.families.family for distribution-specific 
        loglikelihoods.  The result uses the concentrated 
        log-likelihood if the family is Gaussian and the link is linear, 
        otherwise it uses the non-concentrated log-likelihood evaluated 
        at the estimated scale. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.llf_scaled()</span>

    <span class="s2">def </span><span class="s1">pseudo_rsquared(self</span><span class="s2">, </span><span class="s1">kind=</span><span class="s4">&quot;cs&quot;</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Pseudo R-squared 
 
        Cox-Snell likelihood ratio pseudo R-squared is valid for both discrete 
        and continuous data. McFadden's pseudo R-squared is only valid for 
        discrete data. 
 
        Cox &amp; Snell's pseudo-R-squared:  1 - exp((llnull - llf)*(2/nobs)) 
 
        McFadden's pseudo-R-squared: 1 - (llf / llnull) 
 
        Parameters 
        ---------- 
        kind : P&quot;cs&quot;, &quot;mcf&quot;} 
            Type of pseudo R-square to return 
 
        Returns 
        ------- 
        float 
            Pseudo R-squared 
        &quot;&quot;&quot;</span>
        <span class="s1">kind = kind.lower()</span>
        <span class="s2">if </span><span class="s1">kind.startswith(</span><span class="s4">&quot;mcf&quot;</span><span class="s1">):</span>
            <span class="s1">prsq = </span><span class="s5">1 </span><span class="s1">- self.llf / self.llnull</span>
        <span class="s2">elif </span><span class="s1">kind.startswith(</span><span class="s4">&quot;cox&quot;</span><span class="s1">) </span><span class="s2">or </span><span class="s1">kind </span><span class="s2">in </span><span class="s1">[</span><span class="s4">&quot;cs&quot;</span><span class="s2">, </span><span class="s4">&quot;lr&quot;</span><span class="s1">]:</span>
            <span class="s1">prsq = </span><span class="s5">1 </span><span class="s1">- np.exp((self.llnull - self.llf) * (</span><span class="s5">2 </span><span class="s1">/ self.nobs))</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;only McFadden and Cox-Snell are available&quot;</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">prsq</span>

    <span class="s1">@cached_value</span>
    <span class="s2">def </span><span class="s1">aic(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Akaike Information Criterion 
        -2 * `llf` + 2 * (`df_model` + 1) 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.info_criteria(</span><span class="s4">&quot;aic&quot;</span><span class="s1">)</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">bic(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Bayes Information Criterion 
 
        `deviance` - `df_resid` * log(`nobs`) 
 
        .. warning:: 
 
            The current definition is based on the deviance rather than the 
            log-likelihood. This is not consistent with the AIC definition, 
            and after 0.13 both will make use of the log-likelihood definition. 
 
        Notes 
        ----- 
        The log-likelihood version is defined 
        -2 * `llf` + (`df_model` + 1)*log(n) 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">_use_bic_helper.use_bic_llf </span><span class="s2">not in </span><span class="s1">(</span><span class="s2">True, False</span><span class="s1">):</span>
            <span class="s1">warnings.warn(</span>
                <span class="s4">&quot;The bic value is computed using the deviance formula. After &quot;</span>
                <span class="s4">&quot;0.13 this will change to the log-likelihood based formula. &quot;</span>
                <span class="s4">&quot;This change has no impact on the relative rank of models &quot;</span>
                <span class="s4">&quot;compared using BIC. You can directly access the &quot;</span>
                <span class="s4">&quot;log-likelihood version using the `bic_llf` attribute. You &quot;</span>
                <span class="s4">&quot;can suppress this message by calling &quot;</span>
                <span class="s4">&quot;statsmodels.genmod.generalized_linear_model.SET_USE_BIC_LLF &quot;</span>
                <span class="s4">&quot;with True to get the LLF-based version now or False to retain&quot;</span>
                <span class="s4">&quot;the deviance version.&quot;</span><span class="s2">,</span>
                <span class="s1">FutureWarning</span>
            <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">bool(_use_bic_helper.use_bic_llf):</span>
            <span class="s2">return </span><span class="s1">self.bic_llf</span>

        <span class="s2">return </span><span class="s1">self.bic_deviance</span>

    <span class="s1">@cached_value</span>
    <span class="s2">def </span><span class="s1">bic_deviance(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Bayes Information Criterion 
 
        Based on the deviance, 
        `deviance` - `df_resid` * log(`nobs`) 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">(self.deviance -</span>
                <span class="s1">(self.model.wnobs - self.df_model - </span><span class="s5">1</span><span class="s1">) *</span>
                <span class="s1">np.log(self.model.wnobs))</span>

    <span class="s1">@cached_value</span>
    <span class="s2">def </span><span class="s1">bic_llf(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Bayes Information Criterion 
 
        Based on the log-likelihood, 
        -2 * `llf` + log(n) * (`df_model` + 1) 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.info_criteria(</span><span class="s4">&quot;bic&quot;</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">info_criteria(self</span><span class="s2">, </span><span class="s1">crit</span><span class="s2">, </span><span class="s1">scale=</span><span class="s2">None, </span><span class="s1">dk_params=</span><span class="s5">0</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Return an information criterion for the model. 
 
        Parameters 
        ---------- 
        crit : string 
            One of 'aic', 'bic', or 'qaic'. 
        scale : float 
            The scale parameter estimated using the parent model, 
            used only for qaic. 
        dk_params : int or float 
            Correction to the number of parameters used in the information 
            criterion. By default, only mean parameters are included, the 
            scale parameter is not included in the parameter count. 
            Use ``dk_params=1`` to include scale in the parameter count. 
 
        Returns 
        ------- 
        Value of information criterion. 
 
        Notes 
        ----- 
        The quasi-Akaike Information criterion (qaic) is -2 * 
        `llf`/`scale` + 2 * (`df_model` + 1).  It may not give 
        meaningful results except for Poisson and related models. 
 
        The QAIC (ic_type='qaic') must be evaluated with a provided 
        scale parameter.  Two QAIC values are only comparable if they 
        are calculated using the same scale parameter.  The scale 
        parameter should be estimated using the largest model among 
        all models being compared. 
 
        References 
        ---------- 
        Burnham KP, Anderson KR (2002). Model Selection and Multimodel 
        Inference; Springer New York. 
        &quot;&quot;&quot;</span>
        <span class="s1">crit = crit.lower()</span>
        <span class="s1">k_params = self.df_model + </span><span class="s5">1 </span><span class="s1">+ dk_params</span>

        <span class="s2">if </span><span class="s1">crit == </span><span class="s4">&quot;aic&quot;</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">-</span><span class="s5">2 </span><span class="s1">* self.llf + </span><span class="s5">2 </span><span class="s1">* k_params</span>
        <span class="s2">elif </span><span class="s1">crit == </span><span class="s4">&quot;bic&quot;</span><span class="s1">:</span>
            <span class="s1">nobs = self.df_model + self.df_resid + </span><span class="s5">1</span>
            <span class="s1">bic = -</span><span class="s5">2</span><span class="s1">*self.llf + k_params*np.log(nobs)</span>
            <span class="s2">return </span><span class="s1">bic</span>
        <span class="s2">elif </span><span class="s1">crit == </span><span class="s4">&quot;qaic&quot;</span><span class="s1">:</span>
            <span class="s1">f = self.model.family</span>
            <span class="s1">fl = (families.Poisson</span><span class="s2">, </span><span class="s1">families.NegativeBinomial</span><span class="s2">,</span>
                  <span class="s1">families.Binomial)</span>
            <span class="s2">if not </span><span class="s1">isinstance(f</span><span class="s2">, </span><span class="s1">fl):</span>
                <span class="s1">msg = </span><span class="s4">&quot;QAIC is only valid for Binomial, Poisson and &quot;</span>
                <span class="s1">msg += </span><span class="s4">&quot;Negative Binomial families.&quot;</span>
                <span class="s1">warnings.warn(msg)</span>
            <span class="s1">llf = self.llf_scaled(scale=</span><span class="s5">1</span><span class="s1">)</span>
            <span class="s2">return </span><span class="s1">-</span><span class="s5">2 </span><span class="s1">* llf/scale + </span><span class="s5">2 </span><span class="s1">* k_params</span>

    <span class="s3"># now explicit docs, old and new behavior, copied from generic classes</span>
    <span class="s3"># @Appender(str(get_prediction_doc))</span>
    <span class="s2">def </span><span class="s1">get_prediction(self</span><span class="s2">, </span><span class="s1">exog=</span><span class="s2">None, </span><span class="s1">exposure=</span><span class="s2">None, </span><span class="s1">offset=</span><span class="s2">None,</span>
                       <span class="s1">transform=</span><span class="s2">True, </span><span class="s1">which=</span><span class="s2">None, </span><span class="s1">linear=</span><span class="s2">None,</span>
                       <span class="s1">average=</span><span class="s2">False, </span><span class="s1">agg_weights=</span><span class="s2">None,</span>
                       <span class="s1">row_labels=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
    Compute prediction results for GLM compatible models. 
 
    Options and return class depend on whether &quot;which&quot; is None or not. 
 
    Parameters 
    ---------- 
    exog : array_like, optional 
        The values for which you want to predict. 
    exposure : array_like, optional 
        Exposure time values, only can be used with the log link 
        function. 
    offset : array_like, optional 
        Offset values. 
    transform : bool, optional 
        If the model was fit via a formula, do you want to pass 
        exog through the formula. Default is True. E.g., if you fit 
        a model y ~ log(x1) + log(x2), and transform is True, then 
        you can pass a data structure that contains x1 and x2 in 
        their original form. Otherwise, you'd need to log the data 
        first. 
    which : 'mean', 'linear', 'var'(optional) 
        Statitistic to predict. Default is 'mean'. 
        If which is None, then the deprecated keyword &quot;linear&quot; applies. 
        If which is not None, then a generic Prediction results class will 
        be returned. Some options are only available if which is not None. 
        See notes. 
 
        - 'mean' returns the conditional expectation of endog E(y | x), 
          i.e. inverse of the model's link function of linear predictor. 
        - 'linear' returns the linear predictor of the mean function. 
        - 'var_unscaled' variance of endog implied by the likelihood model. 
          This does not include scale or var_weights. 
 
    linear : bool 
        The ``linear` keyword is deprecated and will be removed, 
        use ``which`` keyword instead. 
        If which is None, then the linear keyword is used, otherwise it will 
        be ignored. 
        If True and which is None, the linear predicted values are returned. 
        If False or None, then the statistic specified by ``which`` will be 
        returned. 
    average : bool 
        Keyword is only used if ``which`` is not None. 
        If average is True, then the mean prediction is computed, that is, 
        predictions are computed for individual exog and then the average 
        over observation is used. 
        If average is False, then the results are the predictions for all 
        observations, i.e. same length as ``exog``. 
    agg_weights : ndarray, optional 
        Keyword is only used if ``which`` is not None. 
        Aggregation weights, only used if average is True. 
    row_labels : list of str or None 
        If row_lables are provided, then they will replace the generated 
        labels. 
 
    Returns 
    ------- 
    prediction_results : instance of a PredictionResults class. 
        The prediction results instance contains prediction and prediction 
        variance and can on demand calculate confidence intervals and summary 
        tables for the prediction of the mean and of new observations. 
        The Results class of the return depends on the value of ``which``. 
 
    See Also 
    -------- 
    GLM.predict 
    GLMResults.predict 
 
    Notes 
    ----- 
    Changes in statsmodels 0.14: The ``which`` keyword has been added. 
    If ``which`` is None, then the behavior is the same as in previous 
    versions, and returns the mean and linear prediction results. 
    If the ``which`` keyword is not None, then a generic prediction results 
    class is returned and is not backwards compatible with the old prediction 
    results class, e.g. column names of summary_frame differs. 
    There are more choices for the returned predicted statistic using 
    ``which``. More choices will be added in the next release. 
    Two additional keyword, average and agg_weights options are now also 
    available if ``which`` is not None. 
    In a future version ``which`` will become not None and the backwards 
    compatible prediction results class will be removed. 
 
    &quot;&quot;&quot;</span>

        <span class="s2">import </span><span class="s1">statsmodels.regression._prediction </span><span class="s2">as </span><span class="s1">linpred</span>

        <span class="s1">pred_kwds = {</span><span class="s4">'exposure'</span><span class="s1">: exposure</span><span class="s2">, </span><span class="s4">'offset'</span><span class="s1">: offset</span><span class="s2">, </span><span class="s4">'which'</span><span class="s1">: </span><span class="s4">'linear'</span><span class="s1">}</span>

        <span class="s2">if </span><span class="s1">which </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s3"># two calls to a get_prediction duplicates exog generation if patsy</span>
            <span class="s1">res_linpred = linpred.get_prediction(self</span><span class="s2">, </span><span class="s1">exog=exog</span><span class="s2">,</span>
                                                 <span class="s1">transform=transform</span><span class="s2">,</span>
                                                 <span class="s1">row_labels=row_labels</span><span class="s2">,</span>
                                                 <span class="s1">pred_kwds=pred_kwds)</span>

            <span class="s1">pred_kwds[</span><span class="s4">'which'</span><span class="s1">] = </span><span class="s4">'mean'</span>
            <span class="s1">res = pred.get_prediction_glm(self</span><span class="s2">, </span><span class="s1">exog=exog</span><span class="s2">, </span><span class="s1">transform=transform</span><span class="s2">,</span>
                                          <span class="s1">row_labels=row_labels</span><span class="s2">,</span>
                                          <span class="s1">linpred=res_linpred</span><span class="s2">,</span>
                                          <span class="s1">link=self.model.family.link</span><span class="s2">,</span>
                                          <span class="s1">pred_kwds=pred_kwds)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s3"># new generic version, if 'which' is specified</span>

            <span class="s1">pred_kwds = {</span><span class="s4">'exposure'</span><span class="s1">: exposure</span><span class="s2">, </span><span class="s4">'offset'</span><span class="s1">: offset}</span>
            <span class="s3"># not yet, only applies to count families</span>
            <span class="s3"># y_values is explicit so we can add it to the docstring</span>
            <span class="s3"># if y_values is not None:</span>
            <span class="s3">#    pred_kwds[&quot;y_values&quot;] = y_values</span>

            <span class="s1">res = pred.get_prediction(</span>
                <span class="s1">self</span><span class="s2">,</span>
                <span class="s1">exog=exog</span><span class="s2">,</span>
                <span class="s1">which=which</span><span class="s2">,</span>
                <span class="s1">transform=transform</span><span class="s2">,</span>
                <span class="s1">row_labels=row_labels</span><span class="s2">,</span>
                <span class="s1">average=average</span><span class="s2">,</span>
                <span class="s1">agg_weights=agg_weights</span><span class="s2">,</span>
                <span class="s1">pred_kwds=pred_kwds</span>
                <span class="s1">)</span>

        <span class="s2">return </span><span class="s1">res</span>

    <span class="s1">@Appender(pinfer.score_test.__doc__)</span>
    <span class="s2">def </span><span class="s1">score_test(self</span><span class="s2">, </span><span class="s1">exog_extra=</span><span class="s2">None, </span><span class="s1">params_constrained=</span><span class="s2">None,</span>
                   <span class="s1">hypothesis=</span><span class="s4">'joint'</span><span class="s2">, </span><span class="s1">cov_type=</span><span class="s2">None, </span><span class="s1">cov_kwds=</span><span class="s2">None,</span>
                   <span class="s1">k_constraints=</span><span class="s2">None, </span><span class="s1">observed=</span><span class="s2">True</span><span class="s1">):</span>

        <span class="s2">if </span><span class="s1">self.model._has_freq_weights </span><span class="s2">is True</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span><span class="s4">&quot;score test has not been verified with freq_weights&quot;</span><span class="s2">,</span>
                          <span class="s1">UserWarning)</span>
        <span class="s2">if </span><span class="s1">self.model._has_var_weights </span><span class="s2">is True</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span><span class="s4">&quot;score test has not been verified with var_weights&quot;</span><span class="s2">,</span>
                          <span class="s1">UserWarning)</span>

        <span class="s3"># We need to temporarily change model.df_resid for scale computation</span>
        <span class="s3"># TODO: find a nicer way. gh #7840</span>
        <span class="s1">mod_df_resid = self.model.df_resid</span>
        <span class="s1">self.model.df_resid = self.df_resid</span>
        <span class="s2">if </span><span class="s1">k_constraints </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">self.model.df_resid += k_constraints</span>
        <span class="s1">res = pinfer.score_test(self</span><span class="s2">, </span><span class="s1">exog_extra=exog_extra</span><span class="s2">,</span>
                                <span class="s1">params_constrained=params_constrained</span><span class="s2">,</span>
                                <span class="s1">hypothesis=hypothesis</span><span class="s2">,</span>
                                <span class="s1">cov_type=cov_type</span><span class="s2">, </span><span class="s1">cov_kwds=cov_kwds</span><span class="s2">,</span>
                                <span class="s1">k_constraints=k_constraints</span><span class="s2">,</span>
                                <span class="s1">scale=</span><span class="s2">None,</span>
                                <span class="s1">observed=observed)</span>

        <span class="s1">self.model.df_resid = mod_df_resid</span>
        <span class="s2">return </span><span class="s1">res</span>

    <span class="s2">def </span><span class="s1">get_hat_matrix_diag(self</span><span class="s2">, </span><span class="s1">observed=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Compute the diagonal of the hat matrix 
 
        Parameters 
        ---------- 
        observed : bool 
            If true, then observed hessian is used in the hat matrix 
            computation. If false, then the expected hessian is used. 
            In the case of a canonical link function both are the same. 
 
        Returns 
        ------- 
        hat_matrix_diag : ndarray 
            The diagonal of the hat matrix computed from the observed 
            or expected hessian. 
        &quot;&quot;&quot;</span>
        <span class="s1">weights = self.model.hessian_factor(self.params</span><span class="s2">, </span><span class="s1">observed=observed)</span>
        <span class="s1">wexog = np.sqrt(weights)[:</span><span class="s2">, None</span><span class="s1">] * self.model.exog</span>

        <span class="s1">hd = (wexog * np.linalg.pinv(wexog).T).sum(</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">hd</span>

    <span class="s2">def </span><span class="s1">get_influence(self</span><span class="s2">, </span><span class="s1">observed=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Get an instance of GLMInfluence with influence and outlier measures 
 
        Parameters 
        ---------- 
        observed : bool 
            If true, then observed hessian is used in the hat matrix 
            computation. If false, then the expected hessian is used. 
            In the case of a canonical link function both are the same. 
 
        Returns 
        ------- 
        infl : GLMInfluence instance 
            The instance has methods to calculate the main influence and 
            outlier measures as attributes. 
 
        See Also 
        -------- 
        statsmodels.stats.outliers_influence.GLMInfluence 
        &quot;&quot;&quot;</span>
        <span class="s2">from </span><span class="s1">statsmodels.stats.outliers_influence </span><span class="s2">import </span><span class="s1">GLMInfluence</span>

        <span class="s1">weights = self.model.hessian_factor(self.params</span><span class="s2">, </span><span class="s1">observed=observed)</span>
        <span class="s1">weights_sqrt = np.sqrt(weights)</span>
        <span class="s1">wexog = weights_sqrt[:</span><span class="s2">, None</span><span class="s1">] * self.model.exog</span>
        <span class="s1">wendog = weights_sqrt * self.model.endog</span>

        <span class="s3"># using get_hat_matrix_diag has duplicated computation</span>
        <span class="s1">hat_matrix_diag = self.get_hat_matrix_diag(observed=observed)</span>
        <span class="s1">infl = GLMInfluence(self</span><span class="s2">, </span><span class="s1">endog=wendog</span><span class="s2">, </span><span class="s1">exog=wexog</span><span class="s2">,</span>
                         <span class="s1">resid=self.resid_pearson / np.sqrt(self.scale)</span><span class="s2">,</span>
                         <span class="s1">hat_matrix_diag=hat_matrix_diag)</span>
        <span class="s2">return </span><span class="s1">infl</span>

    <span class="s2">def </span><span class="s1">get_distribution(self</span><span class="s2">, </span><span class="s1">exog=</span><span class="s2">None, </span><span class="s1">exposure=</span><span class="s2">None,</span>
                         <span class="s1">offset=</span><span class="s2">None, </span><span class="s1">var_weights=</span><span class="s5">1.</span><span class="s2">, </span><span class="s1">n_trials=</span><span class="s5">1.</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return a instance of the predictive distribution. 
 
        Parameters 
        ---------- 
        scale : scalar 
            The scale parameter. 
        exog : array_like 
            The predictor variable matrix. 
        offset : array_like or None 
            Offset variable for predicted mean. 
        exposure : array_like or None 
            Log(exposure) will be added to the linear prediction. 
        var_weights : array_like 
            1d array of variance (analytic) weights. The default is None. 
        n_trials : int 
            Number of trials for the binomial distribution. The default is 1 
            which corresponds to a Bernoulli random variable. 
 
        Returns 
        ------- 
        gen 
            Instance of a scipy frozen distribution based on estimated 
            parameters. 
            Use the ``rvs`` method to generate random values. 
 
        Notes 
        ----- 
        Due to the behavior of ``scipy.stats.distributions objects``, the 
        returned random number generator must be called with ``gen.rvs(n)`` 
        where ``n`` is the number of observations in the data set used 
        to fit the model.  If any other value is used for ``n``, misleading 
        results will be produced. 
        &quot;&quot;&quot;</span>
        <span class="s3"># Note this is mostly a copy of GLM.get_prediction</span>
        <span class="s3"># calling here results.predict avoids the exog check and trasnform</span>

        <span class="s2">if </span><span class="s1">isinstance(self.model.family</span><span class="s2">, </span><span class="s1">(families.Binomial</span><span class="s2">, </span><span class="s1">families.Poisson</span><span class="s2">,</span>
                                    <span class="s1">families.NegativeBinomial)):</span>
            <span class="s3"># use scale=1, independent of QMLE scale for discrete</span>
            <span class="s1">scale = </span><span class="s5">1.</span>
            <span class="s2">if </span><span class="s1">self.scale != </span><span class="s5">1.</span><span class="s1">:</span>
                <span class="s1">msg = </span><span class="s4">&quot;using scale=1, no exess dispersion in distribution&quot;</span>
                <span class="s1">warnings.warn(msg</span><span class="s2">, </span><span class="s1">UserWarning)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">scale = self.scale</span>

        <span class="s1">mu = self.predict(exog</span><span class="s2">, </span><span class="s1">exposure</span><span class="s2">, </span><span class="s1">offset</span><span class="s2">, </span><span class="s1">which=</span><span class="s4">&quot;mean&quot;</span><span class="s1">)</span>

        <span class="s1">kwds = {}</span>
        <span class="s2">if </span><span class="s1">(np.any(n_trials != </span><span class="s5">1</span><span class="s1">) </span><span class="s2">and</span>
                <span class="s1">isinstance(self.model.family</span><span class="s2">, </span><span class="s1">families.Binomial)):</span>

            <span class="s1">kwds[</span><span class="s4">&quot;n_trials&quot;</span><span class="s1">] = n_trials</span>

        <span class="s1">distr = self.model.family.get_distribution(</span>
            <span class="s1">mu</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">var_weights=var_weights</span><span class="s2">, </span><span class="s1">**kwds)</span>
        <span class="s2">return </span><span class="s1">distr</span>


    <span class="s1">@Appender(base.LikelihoodModelResults.remove_data.__doc__)</span>
    <span class="s2">def </span><span class="s1">remove_data(self):</span>
        <span class="s3"># GLM has alias/reference in result instance</span>
        <span class="s1">self._data_attr.extend([i </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">self.model._data_attr</span>
                                <span class="s2">if </span><span class="s4">'_data.' </span><span class="s2">not in </span><span class="s1">i])</span>
        <span class="s1">super(self.__class__</span><span class="s2">, </span><span class="s1">self).remove_data()</span>

        <span class="s3"># TODO: what are these in results?</span>
        <span class="s1">self._endog = </span><span class="s2">None</span>
        <span class="s1">self._freq_weights = </span><span class="s2">None</span>
        <span class="s1">self._var_weights = </span><span class="s2">None</span>
        <span class="s1">self._iweights = </span><span class="s2">None</span>
        <span class="s1">self._n_trials = </span><span class="s2">None</span>

    <span class="s1">@Appender(_plot_added_variable_doc % {</span><span class="s4">'extra_params_doc'</span><span class="s1">: </span><span class="s4">''</span><span class="s1">})</span>
    <span class="s2">def </span><span class="s1">plot_added_variable(self</span><span class="s2">, </span><span class="s1">focus_exog</span><span class="s2">, </span><span class="s1">resid_type=</span><span class="s2">None,</span>
                            <span class="s1">use_glm_weights=</span><span class="s2">True, </span><span class="s1">fit_kwargs=</span><span class="s2">None,</span>
                            <span class="s1">ax=</span><span class="s2">None</span><span class="s1">):</span>

        <span class="s2">from </span><span class="s1">statsmodels.graphics.regressionplots </span><span class="s2">import </span><span class="s1">plot_added_variable</span>

        <span class="s1">fig = plot_added_variable(self</span><span class="s2">, </span><span class="s1">focus_exog</span><span class="s2">,</span>
                                  <span class="s1">resid_type=resid_type</span><span class="s2">,</span>
                                  <span class="s1">use_glm_weights=use_glm_weights</span><span class="s2">,</span>
                                  <span class="s1">fit_kwargs=fit_kwargs</span><span class="s2">, </span><span class="s1">ax=ax)</span>

        <span class="s2">return </span><span class="s1">fig</span>

    <span class="s1">@Appender(_plot_partial_residuals_doc % {</span><span class="s4">'extra_params_doc'</span><span class="s1">: </span><span class="s4">''</span><span class="s1">})</span>
    <span class="s2">def </span><span class="s1">plot_partial_residuals(self</span><span class="s2">, </span><span class="s1">focus_exog</span><span class="s2">, </span><span class="s1">ax=</span><span class="s2">None</span><span class="s1">):</span>

        <span class="s2">from </span><span class="s1">statsmodels.graphics.regressionplots </span><span class="s2">import </span><span class="s1">plot_partial_residuals</span>

        <span class="s2">return </span><span class="s1">plot_partial_residuals(self</span><span class="s2">, </span><span class="s1">focus_exog</span><span class="s2">, </span><span class="s1">ax=ax)</span>

    <span class="s1">@Appender(_plot_ceres_residuals_doc % {</span><span class="s4">'extra_params_doc'</span><span class="s1">: </span><span class="s4">''</span><span class="s1">})</span>
    <span class="s2">def </span><span class="s1">plot_ceres_residuals(self</span><span class="s2">, </span><span class="s1">focus_exog</span><span class="s2">, </span><span class="s1">frac=</span><span class="s5">0.66</span><span class="s2">, </span><span class="s1">cond_means=</span><span class="s2">None,</span>
                             <span class="s1">ax=</span><span class="s2">None</span><span class="s1">):</span>

        <span class="s2">from </span><span class="s1">statsmodels.graphics.regressionplots </span><span class="s2">import </span><span class="s1">plot_ceres_residuals</span>

        <span class="s2">return </span><span class="s1">plot_ceres_residuals(self</span><span class="s2">, </span><span class="s1">focus_exog</span><span class="s2">, </span><span class="s1">frac</span><span class="s2">,</span>
                                    <span class="s1">cond_means=cond_means</span><span class="s2">, </span><span class="s1">ax=ax)</span>

    <span class="s2">def </span><span class="s1">summary(self</span><span class="s2">, </span><span class="s1">yname=</span><span class="s2">None, </span><span class="s1">xname=</span><span class="s2">None, </span><span class="s1">title=</span><span class="s2">None, </span><span class="s1">alpha=</span><span class="s5">.05</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Summarize the Regression Results 
 
        Parameters 
        ---------- 
        yname : str, optional 
            Default is `y` 
        xname : list[str], optional 
            Names for the exogenous variables, default is `var_#` for ## in 
            the number of regressors. Must match the number of parameters in 
            the model 
        title : str, optional 
            Title for the top table. If not None, then this replaces the 
            default title 
        alpha : float 
            significance level for the confidence intervals 
 
        Returns 
        ------- 
        smry : Summary instance 
            this holds the summary tables and text, which can be printed or 
            converted to various output formats. 
 
        See Also 
        -------- 
        statsmodels.iolib.summary.Summary : class to hold summary results 
        &quot;&quot;&quot;</span>

        <span class="s1">top_left = [(</span><span class="s4">'Dep. Variable:'</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s4">'Model:'</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s4">'Model Family:'</span><span class="s2">, </span><span class="s1">[self.family.__class__.__name__])</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s4">'Link Function:'</span><span class="s2">, </span><span class="s1">[self.family.link.__class__.__name__])</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s4">'Method:'</span><span class="s2">, </span><span class="s1">[self.method])</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s4">'Date:'</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s4">'Time:'</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s4">'No. Iterations:'</span><span class="s2">,</span>
                     <span class="s1">[</span><span class="s4">&quot;%d&quot; </span><span class="s1">% self.fit_history[</span><span class="s4">'iteration'</span><span class="s1">]])</span><span class="s2">,</span>
                    <span class="s1">]</span>

        <span class="s2">try</span><span class="s1">:</span>
            <span class="s1">prsquared = self.pseudo_rsquared(kind=</span><span class="s4">&quot;cs&quot;</span><span class="s1">)</span>
        <span class="s2">except </span><span class="s1">ValueError:</span>
            <span class="s1">prsquared = np.nan</span>

        <span class="s1">top_right = [(</span><span class="s4">'No. Observations:'</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">,</span>
                     <span class="s1">(</span><span class="s4">'Df Residuals:'</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">,</span>
                     <span class="s1">(</span><span class="s4">'Df Model:'</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">,</span>
                     <span class="s1">(</span><span class="s4">'Scale:'</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;%#8.5g&quot; </span><span class="s1">% self.scale])</span><span class="s2">,</span>
                     <span class="s1">(</span><span class="s4">'Log-Likelihood:'</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">,</span>
                     <span class="s1">(</span><span class="s4">'Deviance:'</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;%#8.5g&quot; </span><span class="s1">% self.deviance])</span><span class="s2">,</span>
                     <span class="s1">(</span><span class="s4">'Pearson chi2:'</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;%#6.3g&quot; </span><span class="s1">% self.pearson_chi2])</span><span class="s2">,</span>
                     <span class="s1">(</span><span class="s4">'Pseudo R-squ. (CS):'</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;%#6.4g&quot; </span><span class="s1">% prsquared])</span>
                     <span class="s1">]</span>

        <span class="s2">if </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s4">'cov_type'</span><span class="s1">):</span>
            <span class="s1">top_left.append((</span><span class="s4">'Covariance Type:'</span><span class="s2">, </span><span class="s1">[self.cov_type]))</span>

        <span class="s2">if </span><span class="s1">title </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">title = </span><span class="s4">&quot;Generalized Linear Model Regression Results&quot;</span>

        <span class="s3"># create summary tables</span>
        <span class="s2">from </span><span class="s1">statsmodels.iolib.summary </span><span class="s2">import </span><span class="s1">Summary</span>
        <span class="s1">smry = Summary()</span>
        <span class="s1">smry.add_table_2cols(self</span><span class="s2">, </span><span class="s1">gleft=top_left</span><span class="s2">, </span><span class="s1">gright=top_right</span><span class="s2">,</span>
                             <span class="s1">yname=yname</span><span class="s2">, </span><span class="s1">xname=xname</span><span class="s2">, </span><span class="s1">title=title)</span>
        <span class="s1">smry.add_table_params(self</span><span class="s2">, </span><span class="s1">yname=yname</span><span class="s2">, </span><span class="s1">xname=xname</span><span class="s2">, </span><span class="s1">alpha=alpha</span><span class="s2">,</span>
                              <span class="s1">use_t=self.use_t)</span>

        <span class="s2">if </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s4">'constraints'</span><span class="s1">):</span>
            <span class="s1">smry.add_extra_txt([</span><span class="s4">'Model has been estimated subject to linear '</span>
                                <span class="s4">'equality constraints.'</span><span class="s1">])</span>
        <span class="s2">return </span><span class="s1">smry</span>

    <span class="s2">def </span><span class="s1">summary2(self</span><span class="s2">, </span><span class="s1">yname=</span><span class="s2">None, </span><span class="s1">xname=</span><span class="s2">None, </span><span class="s1">title=</span><span class="s2">None, </span><span class="s1">alpha=</span><span class="s5">.05</span><span class="s2">,</span>
                 <span class="s1">float_format=</span><span class="s4">&quot;%.4f&quot;</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Experimental summary for regression Results 
 
        Parameters 
        ---------- 
        yname : str 
            Name of the dependent variable (optional) 
        xname : list[str], optional 
            Names for the exogenous variables, default is `var_#` for ## in 
            the number of regressors. Must match the number of parameters in 
            the model 
        title : str, optional 
            Title for the top table. If not None, then this replaces the 
            default title 
        alpha : float 
            significance level for the confidence intervals 
        float_format : str 
            print format for floats in parameters summary 
 
        Returns 
        ------- 
        smry : Summary instance 
            this holds the summary tables and text, which can be printed or 
            converted to various output formats. 
 
        See Also 
        -------- 
        statsmodels.iolib.summary2.Summary : class to hold summary results 
        &quot;&quot;&quot;</span>
        <span class="s1">self.method = </span><span class="s4">'IRLS'</span>
        <span class="s2">from </span><span class="s1">statsmodels.iolib </span><span class="s2">import </span><span class="s1">summary2</span>
        <span class="s1">smry = summary2.Summary()</span>
        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s4">&quot;ignore&quot;</span><span class="s2">, </span><span class="s1">FutureWarning)</span>
            <span class="s1">smry.add_base(results=self</span><span class="s2">, </span><span class="s1">alpha=alpha</span><span class="s2">, </span><span class="s1">float_format=float_format</span><span class="s2">,</span>
                          <span class="s1">xname=xname</span><span class="s2">, </span><span class="s1">yname=yname</span><span class="s2">, </span><span class="s1">title=title)</span>
        <span class="s2">if </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s4">'constraints'</span><span class="s1">):</span>
            <span class="s1">smry.add_text(</span><span class="s4">'Model has been estimated subject to linear '</span>
                          <span class="s4">'equality constraints.'</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">smry</span>


<span class="s2">class </span><span class="s1">GLMResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s1">_attrs = {</span>
        <span class="s4">'resid_anscombe'</span><span class="s1">: </span><span class="s4">'rows'</span><span class="s2">,</span>
        <span class="s4">'resid_deviance'</span><span class="s1">: </span><span class="s4">'rows'</span><span class="s2">,</span>
        <span class="s4">'resid_pearson'</span><span class="s1">: </span><span class="s4">'rows'</span><span class="s2">,</span>
        <span class="s4">'resid_response'</span><span class="s1">: </span><span class="s4">'rows'</span><span class="s2">,</span>
        <span class="s4">'resid_working'</span><span class="s1">: </span><span class="s4">'rows'</span>
    <span class="s1">}</span>
    <span class="s1">_wrap_attrs = wrap.union_dicts(lm.RegressionResultsWrapper._wrap_attrs</span><span class="s2">,</span>
                                   <span class="s1">_attrs)</span>


<span class="s1">wrap.populate_wrapper(GLMResultsWrapper</span><span class="s2">, </span><span class="s1">GLMResults)</span>

<span class="s2">if </span><span class="s1">__name__ == </span><span class="s4">&quot;__main__&quot;</span><span class="s1">:</span>
    <span class="s2">from </span><span class="s1">statsmodels.datasets </span><span class="s2">import </span><span class="s1">longley</span>
    <span class="s1">data = longley.load()</span>
    <span class="s3"># data.exog = add_constant(data.exog)</span>
    <span class="s1">GLMmod = GLM(data.endog</span><span class="s2">, </span><span class="s1">data.exog).fit()</span>
    <span class="s1">GLMT = GLMmod.summary(returns=</span><span class="s4">'tables'</span><span class="s1">)</span>
    <span class="s3"># GLMT[0].extend_right(GLMT[1])</span>
    <span class="s3"># print(GLMT[0])</span>
    <span class="s3"># print(GLMT[2])</span>
    <span class="s1">GLMTp = GLMmod.summary(title=</span><span class="s4">'Test GLM'</span><span class="s1">)</span>
    <span class="s4">&quot;&quot;&quot; 
From Stata 
. webuse beetle 
. glm r i.beetle ldose, family(binomial n) link(cloglog) 
 
Iteration 0:   log likelihood = -79.012269 
Iteration 1:   log likelihood =  -76.94951 
Iteration 2:   log likelihood = -76.945645 
Iteration 3:   log likelihood = -76.945645 
 
Generalized linear models                          No. of obs      =        24 
Optimization     : ML                              Residual df     =        20 
                                                   Scale parameter =         1 
Deviance         =  73.76505595                    (1/df) Deviance =  3.688253 
Pearson          =   71.8901173                    (1/df) Pearson  =  3.594506 
 
Variance function: V(u) = u*(1-u/n)                [Binomial] 
Link function    : g(u) = ln(-ln(1-u/n))           [Complementary log-log] 
 
                                                   AIC             =   6.74547 
Log likelihood   = -76.94564525                    BIC             =  10.20398 
 
------------------------------------------------------------------------------ 
             |                 OIM 
           r |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval] 
-------------+---------------------------------------------------------------- 
      beetle | 
          2  |  -.0910396   .1076132    -0.85   0.398    -.3019576    .1198783 
          3  |  -1.836058   .1307125   -14.05   0.000     -2.09225   -1.579867 
             | 
       ldose |   19.41558   .9954265    19.50   0.000     17.46458    21.36658 
       _cons |  -34.84602    1.79333   -19.43   0.000    -38.36089   -31.33116 
------------------------------------------------------------------------------ 
&quot;&quot;&quot;</span>
</pre>
</body>
</html>