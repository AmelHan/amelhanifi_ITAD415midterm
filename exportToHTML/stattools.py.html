<html>
<head>
<title>stattools.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
stattools.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Statistical tests to be used in conjunction with the models 
 
Notes 
----- 
These functions have not been formally tested. 
&quot;&quot;&quot;</span>

<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">stats</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s2">import </span><span class="s1">ValueWarning</span>


<span class="s2">def </span><span class="s1">durbin_watson(resids</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">):</span>
    <span class="s0">r&quot;&quot;&quot; 
    Calculates the Durbin-Watson statistic. 
 
    Parameters 
    ---------- 
    resids : array_like 
        Data for which to compute the Durbin-Watson statistic. Usually 
        regression model residuals. 
    axis : int, optional 
        Axis to use if data has more than 1 dimension. Default is 0. 
 
    Returns 
    ------- 
    dw : float, array_like 
        The Durbin-Watson statistic. 
 
    Notes 
    ----- 
    The null hypothesis of the test is that there is no serial correlation 
    in the residuals. 
    The Durbin-Watson test statistic is defined as: 
 
    .. math:: 
 
       \sum_{t=2}^T((e_t - e_{t-1})^2)/\sum_{t=1}^Te_t^2 
 
    The test statistic is approximately equal to 2*(1-r) where ``r`` is the 
    sample autocorrelation of the residuals. Thus, for r == 0, indicating no 
    serial correlation, the test statistic equals 2. This statistic will 
    always be between 0 and 4. The closer to 0 the statistic, the more 
    evidence for positive serial correlation. The closer to 4, the more 
    evidence for negative serial correlation. 
    &quot;&quot;&quot;</span>
    <span class="s1">resids = np.asarray(resids)</span>
    <span class="s1">diff_resids = np.diff(resids</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">axis=axis)</span>
    <span class="s1">dw = np.sum(diff_resids**</span><span class="s3">2</span><span class="s2">, </span><span class="s1">axis=axis) / np.sum(resids**</span><span class="s3">2</span><span class="s2">, </span><span class="s1">axis=axis)</span>
    <span class="s2">return </span><span class="s1">dw</span>


<span class="s2">def </span><span class="s1">omni_normtest(resids</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Omnibus test for normality 
 
    Parameters 
    ---------- 
    resid : array_like 
    axis : int, optional 
        Default is 0 
 
    Returns 
    ------- 
    Chi^2 score, two-tail probability 
    &quot;&quot;&quot;</span>
    <span class="s4"># TODO: change to exception in summary branch and catch in summary()</span>
    <span class="s4">#   behavior changed between scipy 0.9 and 0.10</span>
    <span class="s1">resids = np.asarray(resids)</span>
    <span class="s1">n = resids.shape[axis]</span>
    <span class="s2">if </span><span class="s1">n &lt; </span><span class="s3">8</span><span class="s1">:</span>
        <span class="s2">from </span><span class="s1">warnings </span><span class="s2">import </span><span class="s1">warn</span>
        <span class="s1">warn(</span><span class="s5">&quot;omni_normtest is not valid with less than 8 observations; %i &quot;</span>
             <span class="s5">&quot;samples were given.&quot; </span><span class="s1">% int(n)</span><span class="s2">, </span><span class="s1">ValueWarning)</span>
        <span class="s2">return </span><span class="s1">np.nan</span><span class="s2">, </span><span class="s1">np.nan</span>

    <span class="s2">return </span><span class="s1">stats.normaltest(resids</span><span class="s2">, </span><span class="s1">axis=axis)</span>


<span class="s2">def </span><span class="s1">jarque_bera(resids</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">):</span>
    <span class="s0">r&quot;&quot;&quot; 
    The Jarque-Bera test of normality. 
 
    Parameters 
    ---------- 
    resids : array_like 
        Data to test for normality. Usually regression model residuals that 
        are mean 0. 
    axis : int, optional 
        Axis to use if data has more than 1 dimension. Default is 0. 
 
    Returns 
    ------- 
    JB : {float, ndarray} 
        The Jarque-Bera test statistic. 
    JBpv : {float, ndarray} 
        The pvalue of the test statistic. 
    skew : {float, ndarray} 
        Estimated skewness of the data. 
    kurtosis : {float, ndarray} 
        Estimated kurtosis of the data. 
 
    Notes 
    ----- 
    Each output returned has 1 dimension fewer than data 
 
    The Jarque-Bera test statistic tests the null that the data is normally 
    distributed against an alternative that the data follow some other 
    distribution. The test statistic is based on two moments of the data, 
    the skewness, and the kurtosis, and has an asymptotic :math:`\chi^2_2` 
    distribution. 
 
    The test statistic is defined 
 
    .. math:: JB = n(S^2/6+(K-3)^2/24) 
 
    where n is the number of data points, S is the sample skewness, and K is 
    the sample kurtosis of the data. 
    &quot;&quot;&quot;</span>
    <span class="s1">resids = np.atleast_1d(np.asarray(resids</span><span class="s2">, </span><span class="s1">dtype=float))</span>
    <span class="s2">if </span><span class="s1">resids.size &lt; </span><span class="s3">2</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;resids must contain at least 2 elements&quot;</span><span class="s1">)</span>
    <span class="s4"># Calculate residual skewness and kurtosis</span>
    <span class="s1">skew = stats.skew(resids</span><span class="s2">, </span><span class="s1">axis=axis)</span>
    <span class="s1">kurtosis = </span><span class="s3">3 </span><span class="s1">+ stats.kurtosis(resids</span><span class="s2">, </span><span class="s1">axis=axis)</span>

    <span class="s4"># Calculate the Jarque-Bera test for normality</span>
    <span class="s1">n = resids.shape[axis]</span>
    <span class="s1">jb = (n / </span><span class="s3">6.</span><span class="s1">) * (skew ** </span><span class="s3">2 </span><span class="s1">+ (</span><span class="s3">1 </span><span class="s1">/ </span><span class="s3">4.</span><span class="s1">) * (kurtosis - </span><span class="s3">3</span><span class="s1">) ** </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">jb_pv = stats.chi2.sf(jb</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span>

    <span class="s2">return </span><span class="s1">jb</span><span class="s2">, </span><span class="s1">jb_pv</span><span class="s2">, </span><span class="s1">skew</span><span class="s2">, </span><span class="s1">kurtosis</span>


<span class="s2">def </span><span class="s1">robust_skewness(y</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Calculates the four skewness measures in Kim &amp; White 
 
    Parameters 
    ---------- 
    y : array_like 
        Data to compute use in the estimator. 
    axis : int or None, optional 
        Axis along which the skewness measures are computed.  If `None`, the 
        entire array is used. 
 
    Returns 
    ------- 
    sk1 : ndarray 
          The standard skewness estimator. 
    sk2 : ndarray 
          Skewness estimator based on quartiles. 
    sk3 : ndarray 
          Skewness estimator based on mean-median difference, standardized by 
          absolute deviation. 
    sk4 : ndarray 
          Skewness estimator based on mean-median difference, standardized by 
          standard deviation. 
 
    Notes 
    ----- 
    The robust skewness measures are defined 
 
    .. math:: 
 
        SK_{2}=\\frac{\\left(q_{.75}-q_{.5}\\right) 
        -\\left(q_{.5}-q_{.25}\\right)}{q_{.75}-q_{.25}} 
 
    .. math:: 
 
        SK_{3}=\\frac{\\mu-\\hat{q}_{0.5}} 
        {\\hat{E}\\left[\\left|y-\\hat{\\mu}\\right|\\right]} 
 
    .. math:: 
 
        SK_{4}=\\frac{\\mu-\\hat{q}_{0.5}}{\\hat{\\sigma}} 
 
    .. [*] Tae-Hwan Kim and Halbert White, &quot;On more robust estimation of 
       skewness and kurtosis,&quot; Finance Research Letters, vol. 1, pp. 56-73, 
       March 2004. 
    &quot;&quot;&quot;</span>

    <span class="s2">if </span><span class="s1">axis </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">y = y.ravel()</span>
        <span class="s1">axis = </span><span class="s3">0</span>

    <span class="s1">y = np.sort(y</span><span class="s2">, </span><span class="s1">axis)</span>

    <span class="s1">q1</span><span class="s2">, </span><span class="s1">q2</span><span class="s2">, </span><span class="s1">q3 = np.percentile(y</span><span class="s2">, </span><span class="s1">[</span><span class="s3">25.0</span><span class="s2">, </span><span class="s3">50.0</span><span class="s2">, </span><span class="s3">75.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">axis=axis)</span>

    <span class="s1">mu = y.mean(axis)</span>
    <span class="s1">shape = (y.size</span><span class="s2">,</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">axis </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">shape = list(mu.shape)</span>
        <span class="s1">shape.insert(axis</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">shape = tuple(shape)</span>

    <span class="s1">mu_b = np.reshape(mu</span><span class="s2">, </span><span class="s1">shape)</span>
    <span class="s1">q2_b = np.reshape(q2</span><span class="s2">, </span><span class="s1">shape)</span>

    <span class="s1">sigma = np.sqrt(np.mean(((y - mu_b)**</span><span class="s3">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">axis))</span>

    <span class="s1">sk1 = stats.skew(y</span><span class="s2">, </span><span class="s1">axis=axis)</span>
    <span class="s1">sk2 = (q1 + q3 - </span><span class="s3">2.0 </span><span class="s1">* q2) / (q3 - q1)</span>
    <span class="s1">sk3 = (mu - q2) / np.mean(abs(y - q2_b)</span><span class="s2">, </span><span class="s1">axis=axis)</span>
    <span class="s1">sk4 = (mu - q2) / sigma</span>

    <span class="s2">return </span><span class="s1">sk1</span><span class="s2">, </span><span class="s1">sk2</span><span class="s2">, </span><span class="s1">sk3</span><span class="s2">, </span><span class="s1">sk4</span>


<span class="s2">def </span><span class="s1">_kr3(y</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s3">5.0</span><span class="s2">, </span><span class="s1">beta=</span><span class="s3">50.0</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    KR3 estimator from Kim &amp; White 
 
    Parameters 
    ---------- 
    y : array_like, 1-d 
        Data to compute use in the estimator. 
    alpha : float, optional 
        Lower cut-off for measuring expectation in tail. 
    beta :  float, optional 
        Lower cut-off for measuring expectation in center. 
 
    Returns 
    ------- 
    kr3 : float 
        Robust kurtosis estimator based on standardized lower- and upper-tail 
        expected values 
 
    Notes 
    ----- 
    .. [*] Tae-Hwan Kim and Halbert White, &quot;On more robust estimation of 
       skewness and kurtosis,&quot; Finance Research Letters, vol. 1, pp. 56-73, 
       March 2004. 
    &quot;&quot;&quot;</span>
    <span class="s1">perc = (alpha</span><span class="s2">, </span><span class="s3">100.0 </span><span class="s1">- alpha</span><span class="s2">, </span><span class="s1">beta</span><span class="s2">, </span><span class="s3">100.0 </span><span class="s1">- beta)</span>
    <span class="s1">lower_alpha</span><span class="s2">, </span><span class="s1">upper_alpha</span><span class="s2">, </span><span class="s1">lower_beta</span><span class="s2">, </span><span class="s1">upper_beta = np.percentile(y</span><span class="s2">, </span><span class="s1">perc)</span>
    <span class="s1">l_alpha = np.mean(y[y &lt; lower_alpha])</span>
    <span class="s1">u_alpha = np.mean(y[y &gt; upper_alpha])</span>

    <span class="s1">l_beta = np.mean(y[y &lt; lower_beta])</span>
    <span class="s1">u_beta = np.mean(y[y &gt; upper_beta])</span>

    <span class="s2">return </span><span class="s1">(u_alpha - l_alpha) / (u_beta - l_beta)</span>


<span class="s2">def </span><span class="s1">expected_robust_kurtosis(ab=(</span><span class="s3">5.0</span><span class="s2">, </span><span class="s3">50.0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dg=(</span><span class="s3">2.5</span><span class="s2">, </span><span class="s3">25.0</span><span class="s1">)):</span>
    <span class="s0">&quot;&quot;&quot; 
    Calculates the expected value of the robust kurtosis measures in Kim and 
    White assuming the data are normally distributed. 
 
    Parameters 
    ---------- 
    ab : iterable, optional 
        Contains 100*(alpha, beta) in the kr3 measure where alpha is the tail 
        quantile cut-off for measuring the extreme tail and beta is the central 
        quantile cutoff for the standardization of the measure 
    db : iterable, optional 
        Contains 100*(delta, gamma) in the kr4 measure where delta is the tail 
        quantile for measuring extreme values and gamma is the central quantile 
        used in the the standardization of the measure 
 
    Returns 
    ------- 
    ekr : ndarray, 4-element 
        Contains the expected values of the 4 robust kurtosis measures 
 
    Notes 
    ----- 
    See `robust_kurtosis` for definitions of the robust kurtosis measures 
    &quot;&quot;&quot;</span>

    <span class="s1">alpha</span><span class="s2">, </span><span class="s1">beta = ab</span>
    <span class="s1">delta</span><span class="s2">, </span><span class="s1">gamma = dg</span>
    <span class="s1">expected_value = np.zeros(</span><span class="s3">4</span><span class="s1">)</span>
    <span class="s1">ppf = stats.norm.ppf</span>
    <span class="s1">pdf = stats.norm.pdf</span>
    <span class="s1">q1</span><span class="s2">, </span><span class="s1">q2</span><span class="s2">, </span><span class="s1">q3</span><span class="s2">, </span><span class="s1">q5</span><span class="s2">, </span><span class="s1">q6</span><span class="s2">, </span><span class="s1">q7 = ppf(np.array((</span><span class="s3">1.0</span><span class="s2">, </span><span class="s3">2.0</span><span class="s2">, </span><span class="s3">3.0</span><span class="s2">, </span><span class="s3">5.0</span><span class="s2">, </span><span class="s3">6.0</span><span class="s2">, </span><span class="s3">7.0</span><span class="s1">)) / </span><span class="s3">8</span><span class="s1">)</span>
    <span class="s1">expected_value[</span><span class="s3">0</span><span class="s1">] = </span><span class="s3">3</span>

    <span class="s1">expected_value[</span><span class="s3">1</span><span class="s1">] = ((q7 - q5) + (q3 - q1)) / (q6 - q2)</span>

    <span class="s1">q_alpha</span><span class="s2">, </span><span class="s1">q_beta = ppf(np.array((alpha / </span><span class="s3">100.0</span><span class="s2">, </span><span class="s1">beta / </span><span class="s3">100.0</span><span class="s1">)))</span>
    <span class="s1">expected_value[</span><span class="s3">2</span><span class="s1">] = (</span><span class="s3">2 </span><span class="s1">* pdf(q_alpha) / alpha) / (</span><span class="s3">2 </span><span class="s1">* pdf(q_beta) / beta)</span>

    <span class="s1">q_delta</span><span class="s2">, </span><span class="s1">q_gamma = ppf(np.array((delta / </span><span class="s3">100.0</span><span class="s2">, </span><span class="s1">gamma / </span><span class="s3">100.0</span><span class="s1">)))</span>
    <span class="s1">expected_value[</span><span class="s3">3</span><span class="s1">] = (-</span><span class="s3">2.0 </span><span class="s1">* q_delta) / (-</span><span class="s3">2.0 </span><span class="s1">* q_gamma)</span>

    <span class="s2">return </span><span class="s1">expected_value</span>


<span class="s2">def </span><span class="s1">robust_kurtosis(y</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">ab=(</span><span class="s3">5.0</span><span class="s2">, </span><span class="s3">50.0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dg=(</span><span class="s3">2.5</span><span class="s2">, </span><span class="s3">25.0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">excess=</span><span class="s2">True</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Calculates the four kurtosis measures in Kim &amp; White 
 
    Parameters 
    ---------- 
    y : array_like 
        Data to compute use in the estimator. 
    axis : int or None, optional 
        Axis along which the kurtosis are computed.  If `None`, the 
        entire array is used. 
    a iterable, optional 
        Contains 100*(alpha, beta) in the kr3 measure where alpha is the tail 
        quantile cut-off for measuring the extreme tail and beta is the central 
        quantile cutoff for the standardization of the measure 
    db : iterable, optional 
        Contains 100*(delta, gamma) in the kr4 measure where delta is the tail 
        quantile for measuring extreme values and gamma is the central quantile 
        used in the the standardization of the measure 
    excess : bool, optional 
        If true (default), computed values are excess of those for a standard 
        normal distribution. 
 
    Returns 
    ------- 
    kr1 : ndarray 
          The standard kurtosis estimator. 
    kr2 : ndarray 
          Kurtosis estimator based on octiles. 
    kr3 : ndarray 
          Kurtosis estimators based on exceedance expectations. 
    kr4 : ndarray 
          Kurtosis measure based on the spread between high and low quantiles. 
 
    Notes 
    ----- 
    The robust kurtosis measures are defined 
 
    .. math:: 
 
        KR_{2}=\\frac{\\left(\\hat{q}_{.875}-\\hat{q}_{.625}\\right) 
        +\\left(\\hat{q}_{.375}-\\hat{q}_{.125}\\right)} 
        {\\hat{q}_{.75}-\\hat{q}_{.25}} 
 
    .. math:: 
 
        KR_{3}=\\frac{\\hat{E}\\left(y|y&gt;\\hat{q}_{1-\\alpha}\\right) 
        -\\hat{E}\\left(y|y&lt;\\hat{q}_{\\alpha}\\right)} 
        {\\hat{E}\\left(y|y&gt;\\hat{q}_{1-\\beta}\\right) 
        -\\hat{E}\\left(y|y&lt;\\hat{q}_{\\beta}\\right)} 
 
    .. math:: 
 
        KR_{4}=\\frac{\\hat{q}_{1-\\delta}-\\hat{q}_{\\delta}} 
        {\\hat{q}_{1-\\gamma}-\\hat{q}_{\\gamma}} 
 
    where :math:`\\hat{q}_{p}` is the estimated quantile at :math:`p`. 
 
    .. [*] Tae-Hwan Kim and Halbert White, &quot;On more robust estimation of 
       skewness and kurtosis,&quot; Finance Research Letters, vol. 1, pp. 56-73, 
       March 2004. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">(axis </span><span class="s2">is None or</span>
            <span class="s1">(y.squeeze().ndim == </span><span class="s3">1 </span><span class="s2">and </span><span class="s1">y.ndim != </span><span class="s3">1</span><span class="s1">)):</span>
        <span class="s1">y = y.ravel()</span>
        <span class="s1">axis = </span><span class="s3">0</span>

    <span class="s1">alpha</span><span class="s2">, </span><span class="s1">beta = ab</span>
    <span class="s1">delta</span><span class="s2">, </span><span class="s1">gamma = dg</span>

    <span class="s1">perc = (</span><span class="s3">12.5</span><span class="s2">, </span><span class="s3">25.0</span><span class="s2">, </span><span class="s3">37.5</span><span class="s2">, </span><span class="s3">62.5</span><span class="s2">, </span><span class="s3">75.0</span><span class="s2">, </span><span class="s3">87.5</span><span class="s2">,</span>
            <span class="s1">delta</span><span class="s2">, </span><span class="s3">100.0 </span><span class="s1">- delta</span><span class="s2">, </span><span class="s1">gamma</span><span class="s2">, </span><span class="s3">100.0 </span><span class="s1">- gamma)</span>
    <span class="s1">e1</span><span class="s2">, </span><span class="s1">e2</span><span class="s2">, </span><span class="s1">e3</span><span class="s2">, </span><span class="s1">e5</span><span class="s2">, </span><span class="s1">e6</span><span class="s2">, </span><span class="s1">e7</span><span class="s2">, </span><span class="s1">fd</span><span class="s2">, </span><span class="s1">f1md</span><span class="s2">, </span><span class="s1">fg</span><span class="s2">, </span><span class="s1">f1mg = np.percentile(y</span><span class="s2">, </span><span class="s1">perc</span><span class="s2">,</span>
                                                               <span class="s1">axis=axis)</span>

    <span class="s1">expected_value = (expected_robust_kurtosis(ab</span><span class="s2">, </span><span class="s1">dg)</span>
                      <span class="s2">if </span><span class="s1">excess </span><span class="s2">else </span><span class="s1">np.zeros(</span><span class="s3">4</span><span class="s1">))</span>

    <span class="s1">kr1 = stats.kurtosis(y</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, False</span><span class="s1">) - expected_value[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">kr2 = ((e7 - e5) + (e3 - e1)) / (e6 - e2) - expected_value[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s2">if </span><span class="s1">y.ndim == </span><span class="s3">1</span><span class="s1">:</span>
        <span class="s1">kr3 = _kr3(y</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">beta)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">kr3 = np.apply_along_axis(_kr3</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">beta)</span>
    <span class="s1">kr3 -= expected_value[</span><span class="s3">2</span><span class="s1">]</span>
    <span class="s1">kr4 = (f1md - fd) / (f1mg - fg) - expected_value[</span><span class="s3">3</span><span class="s1">]</span>
    <span class="s2">return </span><span class="s1">kr1</span><span class="s2">, </span><span class="s1">kr2</span><span class="s2">, </span><span class="s1">kr3</span><span class="s2">, </span><span class="s1">kr4</span>


<span class="s2">def </span><span class="s1">_medcouple_1d(y):</span>
    <span class="s0">&quot;&quot;&quot; 
    Calculates the medcouple robust measure of skew. 
 
    Parameters 
    ---------- 
    y : array_like, 1-d 
        Data to compute use in the estimator. 
 
    Returns 
    ------- 
    mc : float 
        The medcouple statistic 
 
    Notes 
    ----- 
    The current algorithm requires a O(N**2) memory allocations, and so may 
    not work for very large arrays (N&gt;10000). 
 
    .. [*] M. Hubert and E. Vandervieren, &quot;An adjusted boxplot for skewed 
       distributions&quot; Computational Statistics &amp; Data Analysis, vol. 52, pp. 
       5186-5201, August 2008. 
    &quot;&quot;&quot;</span>

    <span class="s4"># Parameter changes the algorithm to the slower for large n</span>

    <span class="s1">y = np.squeeze(np.asarray(y))</span>
    <span class="s2">if </span><span class="s1">y.ndim != </span><span class="s3">1</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;y must be squeezable to a 1-d array&quot;</span><span class="s1">)</span>

    <span class="s1">y = np.sort(y)</span>

    <span class="s1">n = y.shape[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s2">if </span><span class="s1">n % </span><span class="s3">2 </span><span class="s1">== </span><span class="s3">0</span><span class="s1">:</span>
        <span class="s1">mf = (y[n // </span><span class="s3">2 </span><span class="s1">- </span><span class="s3">1</span><span class="s1">] + y[n // </span><span class="s3">2</span><span class="s1">]) / </span><span class="s3">2</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">mf = y[(n - </span><span class="s3">1</span><span class="s1">) // </span><span class="s3">2</span><span class="s1">]</span>

    <span class="s1">z = y - mf</span>
    <span class="s1">lower = z[z &lt;= </span><span class="s3">0.0</span><span class="s1">]</span>
    <span class="s1">upper = z[z &gt;= </span><span class="s3">0.0</span><span class="s1">]</span>
    <span class="s1">upper = upper[:</span><span class="s2">, None</span><span class="s1">]</span>
    <span class="s1">standardization = upper - lower</span>
    <span class="s1">is_zero = np.logical_and(lower == </span><span class="s3">0.0</span><span class="s2">, </span><span class="s1">upper == </span><span class="s3">0.0</span><span class="s1">)</span>
    <span class="s1">standardization[is_zero] = np.inf</span>
    <span class="s1">spread = upper + lower</span>
    <span class="s1">h = spread / standardization</span>
    <span class="s4"># GH5395</span>
    <span class="s1">num_ties = np.sum(lower == </span><span class="s3">0.0</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">num_ties:</span>
        <span class="s4"># Replacements has -1 above the anti-diagonal, 0 on the anti-diagonal,</span>
        <span class="s4"># and 1 below the anti-diagonal</span>
        <span class="s1">replacements = np.ones((num_ties</span><span class="s2">, </span><span class="s1">num_ties)) - np.eye(num_ties)</span>
        <span class="s1">replacements -= </span><span class="s3">2 </span><span class="s1">* np.triu(replacements)</span>
        <span class="s4"># Convert diagonal to anti-diagonal</span>
        <span class="s1">replacements = np.fliplr(replacements)</span>
        <span class="s4"># Always replace upper right block</span>
        <span class="s1">h[:num_ties</span><span class="s2">, </span><span class="s1">-num_ties:] = replacements</span>

    <span class="s2">return </span><span class="s1">np.median(h)</span>


<span class="s2">def </span><span class="s1">medcouple(y</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Calculate the medcouple robust measure of skew. 
 
    Parameters 
    ---------- 
    y : array_like 
        Data to compute use in the estimator. 
    axis : {int, None} 
        Axis along which the medcouple statistic is computed.  If `None`, the 
        entire array is used. 
 
    Returns 
    ------- 
    mc : ndarray 
        The medcouple statistic with the same shape as `y`, with the specified 
        axis removed. 
 
    Notes 
    ----- 
    The current algorithm requires a O(N**2) memory allocations, and so may 
    not work for very large arrays (N&gt;10000). 
 
    .. [*] M. Hubert and E. Vandervieren, &quot;An adjusted boxplot for skewed 
       distributions&quot; Computational Statistics &amp; Data Analysis, vol. 52, pp. 
       5186-5201, August 2008. 
    &quot;&quot;&quot;</span>
    <span class="s1">y = np.asarray(y</span><span class="s2">, </span><span class="s1">dtype=np.double)  </span><span class="s4"># GH 4243</span>
    <span class="s2">if </span><span class="s1">axis </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">_medcouple_1d(y.ravel())</span>

    <span class="s2">return </span><span class="s1">np.apply_along_axis(_medcouple_1d</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s1">y)</span>
</pre>
</body>
</html>