<html>
<head>
<title>test_calibration.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_calibration.py</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: Alexandre Gramfort &lt;alexandre.gramfort@telecom-paristech.fr&gt;</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">numpy.testing </span><span class="s2">import </span><span class="s1">assert_allclose</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">sparse</span>

<span class="s2">from </span><span class="s1">sklearn.base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">clone</span>
<span class="s2">from </span><span class="s1">sklearn.calibration </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">CalibratedClassifierCV</span><span class="s2">,</span>
    <span class="s1">CalibrationDisplay</span><span class="s2">,</span>
    <span class="s1">_CalibratedClassifier</span><span class="s2">,</span>
    <span class="s1">_sigmoid_calibration</span><span class="s2">,</span>
    <span class="s1">_SigmoidCalibration</span><span class="s2">,</span>
    <span class="s1">calibration_curve</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">load_iris</span><span class="s2">, </span><span class="s1">make_blobs</span><span class="s2">, </span><span class="s1">make_classification</span>
<span class="s2">from </span><span class="s1">sklearn.dummy </span><span class="s2">import </span><span class="s1">DummyClassifier</span>
<span class="s2">from </span><span class="s1">sklearn.ensemble </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">RandomForestClassifier</span><span class="s2">,</span>
    <span class="s1">VotingClassifier</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.exceptions </span><span class="s2">import </span><span class="s1">NotFittedError</span>
<span class="s2">from </span><span class="s1">sklearn.feature_extraction </span><span class="s2">import </span><span class="s1">DictVectorizer</span>
<span class="s2">from </span><span class="s1">sklearn.impute </span><span class="s2">import </span><span class="s1">SimpleImputer</span>
<span class="s2">from </span><span class="s1">sklearn.isotonic </span><span class="s2">import </span><span class="s1">IsotonicRegression</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">LogisticRegression</span><span class="s2">, </span><span class="s1">SGDClassifier</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">brier_score_loss</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">KFold</span><span class="s2">,</span>
    <span class="s1">LeaveOneOut</span><span class="s2">,</span>
    <span class="s1">check_cv</span><span class="s2">,</span>
    <span class="s1">cross_val_predict</span><span class="s2">,</span>
    <span class="s1">cross_val_score</span><span class="s2">,</span>
    <span class="s1">train_test_split</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.naive_bayes </span><span class="s2">import </span><span class="s1">MultinomialNB</span>
<span class="s2">from </span><span class="s1">sklearn.pipeline </span><span class="s2">import </span><span class="s1">Pipeline</span><span class="s2">, </span><span class="s1">make_pipeline</span>
<span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">LabelEncoder</span><span class="s2">, </span><span class="s1">StandardScaler</span>
<span class="s2">from </span><span class="s1">sklearn.svm </span><span class="s2">import </span><span class="s1">LinearSVC</span>
<span class="s2">from </span><span class="s1">sklearn.tree </span><span class="s2">import </span><span class="s1">DecisionTreeClassifier</span>
<span class="s2">from </span><span class="s1">sklearn.utils._mocking </span><span class="s2">import </span><span class="s1">CheckingClassifier</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">_convert_container</span><span class="s2">,</span>
    <span class="s1">assert_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_equal</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.utils.extmath </span><span class="s2">import </span><span class="s1">softmax</span>

<span class="s1">N_SAMPLES = </span><span class="s3">200</span>


<span class="s1">@pytest.fixture(scope=</span><span class="s4">&quot;module&quot;</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">data():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=N_SAMPLES</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">6</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;sigmoid&quot;</span><span class="s2">, </span><span class="s4">&quot;isotonic&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;ensemble&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_calibration(data</span><span class="s2">, </span><span class="s1">method</span><span class="s2">, </span><span class="s1">ensemble):</span>
    <span class="s0"># Test calibration objects with isotonic and sigmoid</span>
    <span class="s1">n_samples = N_SAMPLES // </span><span class="s3">2</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = data</span>
    <span class="s1">sample_weight = np.random.RandomState(seed=</span><span class="s3">42</span><span class="s1">).uniform(size=y.size)</span>

    <span class="s1">X -= X.min()  </span><span class="s0"># MultinomialNB only allows positive X</span>

    <span class="s0"># split train and test</span>
    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">sw_train = X[:n_samples]</span><span class="s2">, </span><span class="s1">y[:n_samples]</span><span class="s2">, </span><span class="s1">sample_weight[:n_samples]</span>
    <span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test = X[n_samples:]</span><span class="s2">, </span><span class="s1">y[n_samples:]</span>

    <span class="s0"># Naive-Bayes</span>
    <span class="s1">clf = MultinomialNB(force_alpha=</span><span class="s2">True</span><span class="s1">).fit(X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">sample_weight=sw_train)</span>
    <span class="s1">prob_pos_clf = clf.predict_proba(X_test)[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">cal_clf = CalibratedClassifierCV(clf</span><span class="s2">, </span><span class="s1">cv=y.size + </span><span class="s3">1</span><span class="s2">, </span><span class="s1">ensemble=ensemble)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">cal_clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s0"># Naive Bayes with calibration</span>
    <span class="s2">for </span><span class="s1">this_X_train</span><span class="s2">, </span><span class="s1">this_X_test </span><span class="s2">in </span><span class="s1">[</span>
        <span class="s1">(X_train</span><span class="s2">, </span><span class="s1">X_test)</span><span class="s2">,</span>
        <span class="s1">(sparse.csr_matrix(X_train)</span><span class="s2">, </span><span class="s1">sparse.csr_matrix(X_test))</span><span class="s2">,</span>
    <span class="s1">]:</span>
        <span class="s1">cal_clf = CalibratedClassifierCV(clf</span><span class="s2">, </span><span class="s1">method=method</span><span class="s2">, </span><span class="s1">cv=</span><span class="s3">5</span><span class="s2">, </span><span class="s1">ensemble=ensemble)</span>
        <span class="s0"># Note that this fit overwrites the fit on the entire training</span>
        <span class="s0"># set</span>
        <span class="s1">cal_clf.fit(this_X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">sample_weight=sw_train)</span>
        <span class="s1">prob_pos_cal_clf = cal_clf.predict_proba(this_X_test)[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>

        <span class="s0"># Check that brier score has improved after calibration</span>
        <span class="s2">assert </span><span class="s1">brier_score_loss(y_test</span><span class="s2">, </span><span class="s1">prob_pos_clf) &gt; brier_score_loss(</span>
            <span class="s1">y_test</span><span class="s2">, </span><span class="s1">prob_pos_cal_clf</span>
        <span class="s1">)</span>

        <span class="s0"># Check invariance against relabeling [0, 1] -&gt; [1, 2]</span>
        <span class="s1">cal_clf.fit(this_X_train</span><span class="s2">, </span><span class="s1">y_train + </span><span class="s3">1</span><span class="s2">, </span><span class="s1">sample_weight=sw_train)</span>
        <span class="s1">prob_pos_cal_clf_relabeled = cal_clf.predict_proba(this_X_test)[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">assert_array_almost_equal(prob_pos_cal_clf</span><span class="s2">, </span><span class="s1">prob_pos_cal_clf_relabeled)</span>

        <span class="s0"># Check invariance against relabeling [0, 1] -&gt; [-1, 1]</span>
        <span class="s1">cal_clf.fit(this_X_train</span><span class="s2">, </span><span class="s3">2 </span><span class="s1">* y_train - </span><span class="s3">1</span><span class="s2">, </span><span class="s1">sample_weight=sw_train)</span>
        <span class="s1">prob_pos_cal_clf_relabeled = cal_clf.predict_proba(this_X_test)[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">assert_array_almost_equal(prob_pos_cal_clf</span><span class="s2">, </span><span class="s1">prob_pos_cal_clf_relabeled)</span>

        <span class="s0"># Check invariance against relabeling [0, 1] -&gt; [1, 0]</span>
        <span class="s1">cal_clf.fit(this_X_train</span><span class="s2">, </span><span class="s1">(y_train + </span><span class="s3">1</span><span class="s1">) % </span><span class="s3">2</span><span class="s2">, </span><span class="s1">sample_weight=sw_train)</span>
        <span class="s1">prob_pos_cal_clf_relabeled = cal_clf.predict_proba(this_X_test)[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>
        <span class="s2">if </span><span class="s1">method == </span><span class="s4">&quot;sigmoid&quot;</span><span class="s1">:</span>
            <span class="s1">assert_array_almost_equal(prob_pos_cal_clf</span><span class="s2">, </span><span class="s3">1 </span><span class="s1">- prob_pos_cal_clf_relabeled)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s0"># Isotonic calibration is not invariant against relabeling</span>
            <span class="s0"># but should improve in both cases</span>
            <span class="s2">assert </span><span class="s1">brier_score_loss(y_test</span><span class="s2">, </span><span class="s1">prob_pos_clf) &gt; brier_score_loss(</span>
                <span class="s1">(y_test + </span><span class="s3">1</span><span class="s1">) % </span><span class="s3">2</span><span class="s2">, </span><span class="s1">prob_pos_cal_clf_relabeled</span>
            <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_calibration_default_estimator(data):</span>
    <span class="s0"># Check estimator default is LinearSVC</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = data</span>
    <span class="s1">calib_clf = CalibratedClassifierCV(cv=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">calib_clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">base_est = calib_clf.calibrated_classifiers_[</span><span class="s3">0</span><span class="s1">].estimator</span>
    <span class="s2">assert </span><span class="s1">isinstance(base_est</span><span class="s2">, </span><span class="s1">LinearSVC)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;ensemble&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_calibration_cv_splitter(data</span><span class="s2">, </span><span class="s1">ensemble):</span>
    <span class="s0"># Check when `cv` is a CV splitter</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = data</span>

    <span class="s1">splits = </span><span class="s3">5</span>
    <span class="s1">kfold = KFold(n_splits=splits)</span>
    <span class="s1">calib_clf = CalibratedClassifierCV(cv=kfold</span><span class="s2">, </span><span class="s1">ensemble=ensemble)</span>
    <span class="s2">assert </span><span class="s1">isinstance(calib_clf.cv</span><span class="s2">, </span><span class="s1">KFold)</span>
    <span class="s2">assert </span><span class="s1">calib_clf.cv.n_splits == splits</span>

    <span class="s1">calib_clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">expected_n_clf = splits </span><span class="s2">if </span><span class="s1">ensemble </span><span class="s2">else </span><span class="s3">1</span>
    <span class="s2">assert </span><span class="s1">len(calib_clf.calibrated_classifiers_) == expected_n_clf</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;sigmoid&quot;</span><span class="s2">, </span><span class="s4">&quot;isotonic&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;ensemble&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_sample_weight(data</span><span class="s2">, </span><span class="s1">method</span><span class="s2">, </span><span class="s1">ensemble):</span>
    <span class="s1">n_samples = N_SAMPLES // </span><span class="s3">2</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = data</span>

    <span class="s1">sample_weight = np.random.RandomState(seed=</span><span class="s3">42</span><span class="s1">).uniform(size=len(y))</span>
    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">sw_train = X[:n_samples]</span><span class="s2">, </span><span class="s1">y[:n_samples]</span><span class="s2">, </span><span class="s1">sample_weight[:n_samples]</span>
    <span class="s1">X_test = X[n_samples:]</span>

    <span class="s1">estimator = LinearSVC(dual=</span><span class="s4">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">calibrated_clf = CalibratedClassifierCV(estimator</span><span class="s2">, </span><span class="s1">method=method</span><span class="s2">, </span><span class="s1">ensemble=ensemble)</span>
    <span class="s1">calibrated_clf.fit(X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">sample_weight=sw_train)</span>
    <span class="s1">probs_with_sw = calibrated_clf.predict_proba(X_test)</span>

    <span class="s0"># As the weights are used for the calibration, they should still yield</span>
    <span class="s0"># different predictions</span>
    <span class="s1">calibrated_clf.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s1">probs_without_sw = calibrated_clf.predict_proba(X_test)</span>

    <span class="s1">diff = np.linalg.norm(probs_with_sw - probs_without_sw)</span>
    <span class="s2">assert </span><span class="s1">diff &gt; </span><span class="s3">0.1</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;sigmoid&quot;</span><span class="s2">, </span><span class="s4">&quot;isotonic&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;ensemble&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_parallel_execution(data</span><span class="s2">, </span><span class="s1">method</span><span class="s2">, </span><span class="s1">ensemble):</span>
    <span class="s5">&quot;&quot;&quot;Test parallel calibration&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = data</span>
    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test = train_test_split(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span>

    <span class="s1">estimator = make_pipeline(StandardScaler()</span><span class="s2">, </span><span class="s1">LinearSVC(dual=</span><span class="s4">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">))</span>

    <span class="s1">cal_clf_parallel = CalibratedClassifierCV(</span>
        <span class="s1">estimator</span><span class="s2">, </span><span class="s1">method=method</span><span class="s2">, </span><span class="s1">n_jobs=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">ensemble=ensemble</span>
    <span class="s1">)</span>
    <span class="s1">cal_clf_parallel.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s1">probs_parallel = cal_clf_parallel.predict_proba(X_test)</span>

    <span class="s1">cal_clf_sequential = CalibratedClassifierCV(</span>
        <span class="s1">estimator</span><span class="s2">, </span><span class="s1">method=method</span><span class="s2">, </span><span class="s1">n_jobs=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">ensemble=ensemble</span>
    <span class="s1">)</span>
    <span class="s1">cal_clf_sequential.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s1">probs_sequential = cal_clf_sequential.predict_proba(X_test)</span>

    <span class="s1">assert_allclose(probs_parallel</span><span class="s2">, </span><span class="s1">probs_sequential)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;sigmoid&quot;</span><span class="s2">, </span><span class="s4">&quot;isotonic&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;ensemble&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s0"># increase the number of RNG seeds to assess the statistical stability of this</span>
<span class="s0"># test:</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;seed&quot;</span><span class="s2">, </span><span class="s1">range(</span><span class="s3">2</span><span class="s1">))</span>
<span class="s2">def </span><span class="s1">test_calibration_multiclass(method</span><span class="s2">, </span><span class="s1">ensemble</span><span class="s2">, </span><span class="s1">seed):</span>
    <span class="s2">def </span><span class="s1">multiclass_brier(y_true</span><span class="s2">, </span><span class="s1">proba_pred</span><span class="s2">, </span><span class="s1">n_classes):</span>
        <span class="s1">Y_onehot = np.eye(n_classes)[y_true]</span>
        <span class="s2">return </span><span class="s1">np.sum((Y_onehot - proba_pred) ** </span><span class="s3">2</span><span class="s1">) / Y_onehot.shape[</span><span class="s3">0</span><span class="s1">]</span>

    <span class="s0"># Test calibration for multiclass with classifier that implements</span>
    <span class="s0"># only decision function.</span>
    <span class="s1">clf = LinearSVC(dual=</span><span class="s4">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">7</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s3">500</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">100</span><span class="s2">, </span><span class="s1">random_state=seed</span><span class="s2">, </span><span class="s1">centers=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">cluster_std=</span><span class="s3">15.0</span>
    <span class="s1">)</span>

    <span class="s0"># Use an unbalanced dataset by collapsing 8 clusters into one class</span>
    <span class="s0"># to make the naive calibration based on a softmax more unlikely</span>
    <span class="s0"># to work.</span>
    <span class="s1">y[y &gt; </span><span class="s3">2</span><span class="s1">] = </span><span class="s3">2</span>
    <span class="s1">n_classes = np.unique(y).shape[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train = X[::</span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y[::</span><span class="s3">2</span><span class="s1">]</span>
    <span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test = X[</span><span class="s3">1</span><span class="s1">::</span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y[</span><span class="s3">1</span><span class="s1">::</span><span class="s3">2</span><span class="s1">]</span>

    <span class="s1">clf.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>

    <span class="s1">cal_clf = CalibratedClassifierCV(clf</span><span class="s2">, </span><span class="s1">method=method</span><span class="s2">, </span><span class="s1">cv=</span><span class="s3">5</span><span class="s2">, </span><span class="s1">ensemble=ensemble)</span>
    <span class="s1">cal_clf.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s1">probas = cal_clf.predict_proba(X_test)</span>
    <span class="s0"># Check probabilities sum to 1</span>
    <span class="s1">assert_allclose(np.sum(probas</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.ones(len(X_test)))</span>

    <span class="s0"># Check that the dataset is not too trivial, otherwise it's hard</span>
    <span class="s0"># to get interesting calibration data during the internal</span>
    <span class="s0"># cross-validation loop.</span>
    <span class="s2">assert </span><span class="s3">0.65 </span><span class="s1">&lt; clf.score(X_test</span><span class="s2">, </span><span class="s1">y_test) &lt; </span><span class="s3">0.95</span>

    <span class="s0"># Check that the accuracy of the calibrated model is never degraded</span>
    <span class="s0"># too much compared to the original classifier.</span>
    <span class="s2">assert </span><span class="s1">cal_clf.score(X_test</span><span class="s2">, </span><span class="s1">y_test) &gt; </span><span class="s3">0.95 </span><span class="s1">* clf.score(X_test</span><span class="s2">, </span><span class="s1">y_test)</span>

    <span class="s0"># Check that Brier loss of calibrated classifier is smaller than</span>
    <span class="s0"># loss obtained by naively turning OvR decision function to</span>
    <span class="s0"># probabilities via a softmax</span>
    <span class="s1">uncalibrated_brier = multiclass_brier(</span>
        <span class="s1">y_test</span><span class="s2">, </span><span class="s1">softmax(clf.decision_function(X_test))</span><span class="s2">, </span><span class="s1">n_classes=n_classes</span>
    <span class="s1">)</span>
    <span class="s1">calibrated_brier = multiclass_brier(y_test</span><span class="s2">, </span><span class="s1">probas</span><span class="s2">, </span><span class="s1">n_classes=n_classes)</span>

    <span class="s2">assert </span><span class="s1">calibrated_brier &lt; </span><span class="s3">1.1 </span><span class="s1">* uncalibrated_brier</span>

    <span class="s0"># Test that calibration of a multiclass classifier decreases log-loss</span>
    <span class="s0"># for RandomForestClassifier</span>
    <span class="s1">clf = RandomForestClassifier(n_estimators=</span><span class="s3">30</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s1">clf_probs = clf.predict_proba(X_test)</span>
    <span class="s1">uncalibrated_brier = multiclass_brier(y_test</span><span class="s2">, </span><span class="s1">clf_probs</span><span class="s2">, </span><span class="s1">n_classes=n_classes)</span>

    <span class="s1">cal_clf = CalibratedClassifierCV(clf</span><span class="s2">, </span><span class="s1">method=method</span><span class="s2">, </span><span class="s1">cv=</span><span class="s3">5</span><span class="s2">, </span><span class="s1">ensemble=ensemble)</span>
    <span class="s1">cal_clf.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s1">cal_clf_probs = cal_clf.predict_proba(X_test)</span>
    <span class="s1">calibrated_brier = multiclass_brier(y_test</span><span class="s2">, </span><span class="s1">cal_clf_probs</span><span class="s2">, </span><span class="s1">n_classes=n_classes)</span>
    <span class="s2">assert </span><span class="s1">calibrated_brier &lt; </span><span class="s3">1.1 </span><span class="s1">* uncalibrated_brier</span>


<span class="s2">def </span><span class="s1">test_calibration_zero_probability():</span>
    <span class="s0"># Test an edge case where _CalibratedClassifier avoids numerical errors</span>
    <span class="s0"># in the multiclass normalization step if all the calibrators output</span>
    <span class="s0"># are zero all at once for a given sample and instead fallback to uniform</span>
    <span class="s0"># probabilities.</span>
    <span class="s2">class </span><span class="s1">ZeroCalibrator:</span>
        <span class="s0"># This function is called from _CalibratedClassifier.predict_proba.</span>
        <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
            <span class="s2">return </span><span class="s1">np.zeros(X.shape[</span><span class="s3">0</span><span class="s1">])</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s3">50</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">7</span><span class="s2">, </span><span class="s1">centers=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">cluster_std=</span><span class="s3">15.0</span>
    <span class="s1">)</span>
    <span class="s1">clf = DummyClassifier().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">calibrator = ZeroCalibrator()</span>
    <span class="s1">cal_clf = _CalibratedClassifier(</span>
        <span class="s1">estimator=clf</span><span class="s2">, </span><span class="s1">calibrators=[calibrator]</span><span class="s2">, </span><span class="s1">classes=clf.classes_</span>
    <span class="s1">)</span>

    <span class="s1">probas = cal_clf.predict_proba(X)</span>

    <span class="s0"># Check that all probabilities are uniformly 1. / clf.n_classes_</span>
    <span class="s1">assert_allclose(probas</span><span class="s2">, </span><span class="s3">1.0 </span><span class="s1">/ clf.n_classes_)</span>


<span class="s2">def </span><span class="s1">test_calibration_prefit():</span>
    <span class="s5">&quot;&quot;&quot;Test calibration for prefitted classifiers&quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s3">50</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s3">3 </span><span class="s1">* n_samples</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">6</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">sample_weight = np.random.RandomState(seed=</span><span class="s3">42</span><span class="s1">).uniform(size=y.size)</span>

    <span class="s1">X -= X.min()  </span><span class="s0"># MultinomialNB only allows positive X</span>

    <span class="s0"># split train and test</span>
    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">sw_train = X[:n_samples]</span><span class="s2">, </span><span class="s1">y[:n_samples]</span><span class="s2">, </span><span class="s1">sample_weight[:n_samples]</span>
    <span class="s1">X_calib</span><span class="s2">, </span><span class="s1">y_calib</span><span class="s2">, </span><span class="s1">sw_calib = (</span>
        <span class="s1">X[n_samples : </span><span class="s3">2 </span><span class="s1">* n_samples]</span><span class="s2">,</span>
        <span class="s1">y[n_samples : </span><span class="s3">2 </span><span class="s1">* n_samples]</span><span class="s2">,</span>
        <span class="s1">sample_weight[n_samples : </span><span class="s3">2 </span><span class="s1">* n_samples]</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test = X[</span><span class="s3">2 </span><span class="s1">* n_samples :]</span><span class="s2">, </span><span class="s1">y[</span><span class="s3">2 </span><span class="s1">* n_samples :]</span>

    <span class="s0"># Naive-Bayes</span>
    <span class="s1">clf = MultinomialNB(force_alpha=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s0"># Check error if clf not prefit</span>
    <span class="s1">unfit_clf = CalibratedClassifierCV(clf</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">&quot;prefit&quot;</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError):</span>
        <span class="s1">unfit_clf.fit(X_calib</span><span class="s2">, </span><span class="s1">y_calib)</span>

    <span class="s1">clf.fit(X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">sw_train)</span>
    <span class="s1">prob_pos_clf = clf.predict_proba(X_test)[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>

    <span class="s0"># Naive Bayes with calibration</span>
    <span class="s2">for </span><span class="s1">this_X_calib</span><span class="s2">, </span><span class="s1">this_X_test </span><span class="s2">in </span><span class="s1">[</span>
        <span class="s1">(X_calib</span><span class="s2">, </span><span class="s1">X_test)</span><span class="s2">,</span>
        <span class="s1">(sparse.csr_matrix(X_calib)</span><span class="s2">, </span><span class="s1">sparse.csr_matrix(X_test))</span><span class="s2">,</span>
    <span class="s1">]:</span>
        <span class="s2">for </span><span class="s1">method </span><span class="s2">in </span><span class="s1">[</span><span class="s4">&quot;isotonic&quot;</span><span class="s2">, </span><span class="s4">&quot;sigmoid&quot;</span><span class="s1">]:</span>
            <span class="s1">cal_clf = CalibratedClassifierCV(clf</span><span class="s2">, </span><span class="s1">method=method</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">&quot;prefit&quot;</span><span class="s1">)</span>

            <span class="s2">for </span><span class="s1">sw </span><span class="s2">in </span><span class="s1">[sw_calib</span><span class="s2">, None</span><span class="s1">]:</span>
                <span class="s1">cal_clf.fit(this_X_calib</span><span class="s2">, </span><span class="s1">y_calib</span><span class="s2">, </span><span class="s1">sample_weight=sw)</span>
                <span class="s1">y_prob = cal_clf.predict_proba(this_X_test)</span>
                <span class="s1">y_pred = cal_clf.predict(this_X_test)</span>
                <span class="s1">prob_pos_cal_clf = y_prob[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>
                <span class="s1">assert_array_equal(y_pred</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s1">])[np.argmax(y_prob</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)])</span>

                <span class="s2">assert </span><span class="s1">brier_score_loss(y_test</span><span class="s2">, </span><span class="s1">prob_pos_clf) &gt; brier_score_loss(</span>
                    <span class="s1">y_test</span><span class="s2">, </span><span class="s1">prob_pos_cal_clf</span>
                <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;sigmoid&quot;</span><span class="s2">, </span><span class="s4">&quot;isotonic&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_calibration_ensemble_false(data</span><span class="s2">, </span><span class="s1">method):</span>
    <span class="s0"># Test that `ensemble=False` is the same as using predictions from</span>
    <span class="s0"># `cross_val_predict` to train calibrator.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = data</span>
    <span class="s1">clf = LinearSVC(dual=</span><span class="s4">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">7</span><span class="s1">)</span>

    <span class="s1">cal_clf = CalibratedClassifierCV(clf</span><span class="s2">, </span><span class="s1">method=method</span><span class="s2">, </span><span class="s1">cv=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">ensemble=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">cal_clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">cal_probas = cal_clf.predict_proba(X)</span>

    <span class="s0"># Get probas manually</span>
    <span class="s1">unbiased_preds = cross_val_predict(clf</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">cv=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">method=</span><span class="s4">&quot;decision_function&quot;</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">method == </span><span class="s4">&quot;isotonic&quot;</span><span class="s1">:</span>
        <span class="s1">calibrator = IsotonicRegression(out_of_bounds=</span><span class="s4">&quot;clip&quot;</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">calibrator = _SigmoidCalibration()</span>
    <span class="s1">calibrator.fit(unbiased_preds</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s0"># Use `clf` fit on all data</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf_df = clf.decision_function(X)</span>
    <span class="s1">manual_probas = calibrator.predict(clf_df)</span>
    <span class="s1">assert_allclose(cal_probas[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">manual_probas)</span>


<span class="s2">def </span><span class="s1">test_sigmoid_calibration():</span>
    <span class="s5">&quot;&quot;&quot;Test calibration values with Platt sigmoid model&quot;&quot;&quot;</span>
    <span class="s1">exF = np.array([</span><span class="s3">5</span><span class="s2">, </span><span class="s1">-</span><span class="s3">4</span><span class="s2">, </span><span class="s3">1.0</span><span class="s1">])</span>
    <span class="s1">exY = np.array([</span><span class="s3">1</span><span class="s2">, </span><span class="s1">-</span><span class="s3">1</span><span class="s2">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s0"># computed from my python port of the C++ code in LibSVM</span>
    <span class="s1">AB_lin_libsvm = np.array([-</span><span class="s3">0.20261354391187855</span><span class="s2">, </span><span class="s3">0.65236314980010512</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(AB_lin_libsvm</span><span class="s2">, </span><span class="s1">_sigmoid_calibration(exF</span><span class="s2">, </span><span class="s1">exY)</span><span class="s2">, </span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">lin_prob = </span><span class="s3">1.0 </span><span class="s1">/ (</span><span class="s3">1.0 </span><span class="s1">+ np.exp(AB_lin_libsvm[</span><span class="s3">0</span><span class="s1">] * exF + AB_lin_libsvm[</span><span class="s3">1</span><span class="s1">]))</span>
    <span class="s1">sk_prob = _SigmoidCalibration().fit(exF</span><span class="s2">, </span><span class="s1">exY).predict(exF)</span>
    <span class="s1">assert_array_almost_equal(lin_prob</span><span class="s2">, </span><span class="s1">sk_prob</span><span class="s2">, </span><span class="s3">6</span><span class="s1">)</span>

    <span class="s0"># check that _SigmoidCalibration().fit only accepts 1d array or 2d column</span>
    <span class="s0"># arrays</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">_SigmoidCalibration().fit(np.vstack((exF</span><span class="s2">, </span><span class="s1">exF))</span><span class="s2">, </span><span class="s1">exY)</span>


<span class="s2">def </span><span class="s1">test_calibration_curve():</span>
    <span class="s5">&quot;&quot;&quot;Check calibration_curve function&quot;&quot;&quot;</span>
    <span class="s1">y_true = np.array([</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">y_pred = np.array([</span><span class="s3">0.0</span><span class="s2">, </span><span class="s3">0.1</span><span class="s2">, </span><span class="s3">0.2</span><span class="s2">, </span><span class="s3">0.8</span><span class="s2">, </span><span class="s3">0.9</span><span class="s2">, </span><span class="s3">1.0</span><span class="s1">])</span>
    <span class="s1">prob_true</span><span class="s2">, </span><span class="s1">prob_pred = calibration_curve(y_true</span><span class="s2">, </span><span class="s1">y_pred</span><span class="s2">, </span><span class="s1">n_bins=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">len(prob_true) == len(prob_pred)</span>
    <span class="s2">assert </span><span class="s1">len(prob_true) == </span><span class="s3">2</span>
    <span class="s1">assert_almost_equal(prob_true</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">assert_almost_equal(prob_pred</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s2">, </span><span class="s3">0.9</span><span class="s1">])</span>

    <span class="s0"># Probabilities outside [0, 1] should not be accepted at all.</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">calibration_curve([</span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s3">0.1</span><span class="s1">])</span>

    <span class="s0"># test that quantiles work as expected</span>
    <span class="s1">y_true2 = np.array([</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">y_pred2 = np.array([</span><span class="s3">0.0</span><span class="s2">, </span><span class="s3">0.1</span><span class="s2">, </span><span class="s3">0.2</span><span class="s2">, </span><span class="s3">0.5</span><span class="s2">, </span><span class="s3">0.9</span><span class="s2">, </span><span class="s3">1.0</span><span class="s1">])</span>
    <span class="s1">prob_true_quantile</span><span class="s2">, </span><span class="s1">prob_pred_quantile = calibration_curve(</span>
        <span class="s1">y_true2</span><span class="s2">, </span><span class="s1">y_pred2</span><span class="s2">, </span><span class="s1">n_bins=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">strategy=</span><span class="s4">&quot;quantile&quot;</span>
    <span class="s1">)</span>

    <span class="s2">assert </span><span class="s1">len(prob_true_quantile) == len(prob_pred_quantile)</span>
    <span class="s2">assert </span><span class="s1">len(prob_true_quantile) == </span><span class="s3">2</span>
    <span class="s1">assert_almost_equal(prob_true_quantile</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">2 </span><span class="s1">/ </span><span class="s3">3</span><span class="s1">])</span>
    <span class="s1">assert_almost_equal(prob_pred_quantile</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s2">, </span><span class="s3">0.8</span><span class="s1">])</span>

    <span class="s0"># Check that error is raised when invalid strategy is selected</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">calibration_curve(y_true2</span><span class="s2">, </span><span class="s1">y_pred2</span><span class="s2">, </span><span class="s1">strategy=</span><span class="s4">&quot;percentile&quot;</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;ensemble&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_calibration_nan_imputer(ensemble):</span>
    <span class="s5">&quot;&quot;&quot;Test that calibration can accept nan&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">n_informative=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">n_redundant=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">42</span>
    <span class="s1">)</span>
    <span class="s1">X[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s1">] = np.nan</span>
    <span class="s1">clf = Pipeline(</span>
        <span class="s1">[(</span><span class="s4">&quot;imputer&quot;</span><span class="s2">, </span><span class="s1">SimpleImputer())</span><span class="s2">, </span><span class="s1">(</span><span class="s4">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">RandomForestClassifier(n_estimators=</span><span class="s3">1</span><span class="s1">))]</span>
    <span class="s1">)</span>
    <span class="s1">clf_c = CalibratedClassifierCV(clf</span><span class="s2">, </span><span class="s1">cv=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">method=</span><span class="s4">&quot;isotonic&quot;</span><span class="s2">, </span><span class="s1">ensemble=ensemble)</span>
    <span class="s1">clf_c.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf_c.predict(X)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;ensemble&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_calibration_prob_sum(ensemble):</span>
    <span class="s0"># Test that sum of probabilities is 1. A non-regression test for</span>
    <span class="s0"># issue #7796</span>
    <span class="s1">num_classes = </span><span class="s3">2</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">5</span><span class="s2">, </span><span class="s1">n_classes=num_classes)</span>
    <span class="s1">clf = LinearSVC(dual=</span><span class="s4">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s3">1.0</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">7</span><span class="s1">)</span>
    <span class="s1">clf_prob = CalibratedClassifierCV(</span>
        <span class="s1">clf</span><span class="s2">, </span><span class="s1">method=</span><span class="s4">&quot;sigmoid&quot;</span><span class="s2">, </span><span class="s1">cv=LeaveOneOut()</span><span class="s2">, </span><span class="s1">ensemble=ensemble</span>
    <span class="s1">)</span>
    <span class="s1">clf_prob.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">probs = clf_prob.predict_proba(X)</span>
    <span class="s1">assert_array_almost_equal(probs.sum(axis=</span><span class="s3">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.ones(probs.shape[</span><span class="s3">0</span><span class="s1">]))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;ensemble&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_calibration_less_classes(ensemble):</span>
    <span class="s0"># Test to check calibration works fine when train set in a test-train</span>
    <span class="s0"># split does not contain all classes</span>
    <span class="s0"># Since this test uses LOO, at each iteration train set will not contain a</span>
    <span class="s0"># class label</span>
    <span class="s1">X = np.random.randn(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">5</span><span class="s1">)</span>
    <span class="s1">y = np.arange(</span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">clf = LinearSVC(dual=</span><span class="s4">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s3">1.0</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">7</span><span class="s1">)</span>
    <span class="s1">cal_clf = CalibratedClassifierCV(</span>
        <span class="s1">clf</span><span class="s2">, </span><span class="s1">method=</span><span class="s4">&quot;sigmoid&quot;</span><span class="s2">, </span><span class="s1">cv=LeaveOneOut()</span><span class="s2">, </span><span class="s1">ensemble=ensemble</span>
    <span class="s1">)</span>
    <span class="s1">cal_clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">calibrated_classifier </span><span class="s2">in </span><span class="s1">enumerate(cal_clf.calibrated_classifiers_):</span>
        <span class="s1">proba = calibrated_classifier.predict_proba(X)</span>
        <span class="s2">if </span><span class="s1">ensemble:</span>
            <span class="s0"># Check that the unobserved class has proba=0</span>
            <span class="s1">assert_array_equal(proba[:</span><span class="s2">, </span><span class="s1">i]</span><span class="s2">, </span><span class="s1">np.zeros(len(y)))</span>
            <span class="s0"># Check for all other classes proba&gt;0</span>
            <span class="s2">assert </span><span class="s1">np.all(proba[:</span><span class="s2">, </span><span class="s1">:i] &gt; </span><span class="s3">0</span><span class="s1">)</span>
            <span class="s2">assert </span><span class="s1">np.all(proba[:</span><span class="s2">, </span><span class="s1">i + </span><span class="s3">1 </span><span class="s1">:] &gt; </span><span class="s3">0</span><span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s0"># Check `proba` are all 1/n_classes</span>
            <span class="s2">assert </span><span class="s1">np.allclose(proba</span><span class="s2">, </span><span class="s3">1 </span><span class="s1">/ proba.shape[</span><span class="s3">0</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;X&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">np.random.RandomState(</span><span class="s3">42</span><span class="s1">).randn(</span><span class="s3">15</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">np.random.RandomState(</span><span class="s3">42</span><span class="s1">).randn(</span><span class="s3">15</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">6</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_calibration_accepts_ndarray(X):</span>
    <span class="s5">&quot;&quot;&quot;Test that calibration accepts n-dimensional arrays as input&quot;&quot;&quot;</span>
    <span class="s1">y = [</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]</span>

    <span class="s2">class </span><span class="s1">MockTensorClassifier(BaseEstimator):</span>
        <span class="s5">&quot;&quot;&quot;A toy estimator that accepts tensor inputs&quot;&quot;&quot;</span>

        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
            <span class="s1">self.classes_ = np.unique(y)</span>
            <span class="s2">return </span><span class="s1">self</span>

        <span class="s2">def </span><span class="s1">decision_function(self</span><span class="s2">, </span><span class="s1">X):</span>
            <span class="s0"># toy decision function that just needs to have the right shape:</span>
            <span class="s2">return </span><span class="s1">X.reshape(X.shape[</span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">).sum(axis=</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">calibrated_clf = CalibratedClassifierCV(MockTensorClassifier())</span>
    <span class="s0"># we should be able to fit this classifier with no error</span>
    <span class="s1">calibrated_clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s1">@pytest.fixture</span>
<span class="s2">def </span><span class="s1">dict_data():</span>
    <span class="s1">dict_data = [</span>
        <span class="s1">{</span><span class="s4">&quot;state&quot;</span><span class="s1">: </span><span class="s4">&quot;NY&quot;</span><span class="s2">, </span><span class="s4">&quot;age&quot;</span><span class="s1">: </span><span class="s4">&quot;adult&quot;</span><span class="s1">}</span><span class="s2">,</span>
        <span class="s1">{</span><span class="s4">&quot;state&quot;</span><span class="s1">: </span><span class="s4">&quot;TX&quot;</span><span class="s2">, </span><span class="s4">&quot;age&quot;</span><span class="s1">: </span><span class="s4">&quot;adult&quot;</span><span class="s1">}</span><span class="s2">,</span>
        <span class="s1">{</span><span class="s4">&quot;state&quot;</span><span class="s1">: </span><span class="s4">&quot;VT&quot;</span><span class="s2">, </span><span class="s4">&quot;age&quot;</span><span class="s1">: </span><span class="s4">&quot;child&quot;</span><span class="s1">}</span><span class="s2">,</span>
    <span class="s1">]</span>
    <span class="s1">text_labels = [</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>
    <span class="s2">return </span><span class="s1">dict_data</span><span class="s2">, </span><span class="s1">text_labels</span>


<span class="s1">@pytest.fixture</span>
<span class="s2">def </span><span class="s1">dict_data_pipeline(dict_data):</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = dict_data</span>
    <span class="s1">pipeline_prefit = Pipeline(</span>
        <span class="s1">[(</span><span class="s4">&quot;vectorizer&quot;</span><span class="s2">, </span><span class="s1">DictVectorizer())</span><span class="s2">, </span><span class="s1">(</span><span class="s4">&quot;clf&quot;</span><span class="s2">, </span><span class="s1">RandomForestClassifier())]</span>
    <span class="s1">)</span>
    <span class="s2">return </span><span class="s1">pipeline_prefit.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_calibration_dict_pipeline(dict_data</span><span class="s2">, </span><span class="s1">dict_data_pipeline):</span>
    <span class="s5">&quot;&quot;&quot;Test that calibration works in prefit pipeline with transformer 
 
    `X` is not array-like, sparse matrix or dataframe at the start. 
    See https://github.com/scikit-learn/scikit-learn/issues/8710 
 
    Also test it can predict without running into validation errors. 
    See https://github.com/scikit-learn/scikit-learn/issues/19637 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = dict_data</span>
    <span class="s1">clf = dict_data_pipeline</span>
    <span class="s1">calib_clf = CalibratedClassifierCV(clf</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">&quot;prefit&quot;</span><span class="s1">)</span>
    <span class="s1">calib_clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s0"># Check attributes are obtained from fitted estimator</span>
    <span class="s1">assert_array_equal(calib_clf.classes_</span><span class="s2">, </span><span class="s1">clf.classes_)</span>

    <span class="s0"># Neither the pipeline nor the calibration meta-estimator</span>
    <span class="s0"># expose the n_features_in_ check on this kind of data.</span>
    <span class="s2">assert not </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s4">&quot;n_features_in_&quot;</span><span class="s1">)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(calib_clf</span><span class="s2">, </span><span class="s4">&quot;n_features_in_&quot;</span><span class="s1">)</span>

    <span class="s0"># Ensure that no error is thrown with predict and predict_proba</span>
    <span class="s1">calib_clf.predict(X)</span>
    <span class="s1">calib_clf.predict_proba(X)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;clf, cv&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">pytest.param(LinearSVC(dual=</span><span class="s4">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s3">1</span><span class="s1">)</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">pytest.param(LinearSVC(dual=</span><span class="s4">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s3">1</span><span class="s1">)</span><span class="s2">, </span><span class="s4">&quot;prefit&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_calibration_attributes(clf</span><span class="s2">, </span><span class="s1">cv):</span>
    <span class="s0"># Check that `n_features_in_` and `classes_` attributes created properly</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">5</span><span class="s2">, </span><span class="s1">n_classes=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">7</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">cv == </span><span class="s4">&quot;prefit&quot;</span><span class="s1">:</span>
        <span class="s1">clf = clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">calib_clf = CalibratedClassifierCV(clf</span><span class="s2">, </span><span class="s1">cv=cv)</span>
    <span class="s1">calib_clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">if </span><span class="s1">cv == </span><span class="s4">&quot;prefit&quot;</span><span class="s1">:</span>
        <span class="s1">assert_array_equal(calib_clf.classes_</span><span class="s2">, </span><span class="s1">clf.classes_)</span>
        <span class="s2">assert </span><span class="s1">calib_clf.n_features_in_ == clf.n_features_in_</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">classes = LabelEncoder().fit(y).classes_</span>
        <span class="s1">assert_array_equal(calib_clf.classes_</span><span class="s2">, </span><span class="s1">classes)</span>
        <span class="s2">assert </span><span class="s1">calib_clf.n_features_in_ == X.shape[</span><span class="s3">1</span><span class="s1">]</span>


<span class="s2">def </span><span class="s1">test_calibration_inconsistent_prefit_n_features_in():</span>
    <span class="s0"># Check that `n_features_in_` from prefit base estimator</span>
    <span class="s0"># is consistent with training set</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">5</span><span class="s2">, </span><span class="s1">n_classes=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">7</span><span class="s1">)</span>
    <span class="s1">clf = LinearSVC(dual=</span><span class="s4">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s3">1</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">calib_clf = CalibratedClassifierCV(clf</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">&quot;prefit&quot;</span><span class="s1">)</span>

    <span class="s1">msg = </span><span class="s4">&quot;X has 3 features, but LinearSVC is expecting 5 features as input.&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">calib_clf.fit(X[:</span><span class="s2">, </span><span class="s1">:</span><span class="s3">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_calibration_votingclassifier():</span>
    <span class="s0"># Check that `CalibratedClassifier` works with `VotingClassifier`.</span>
    <span class="s0"># The method `predict_proba` from `VotingClassifier` is dynamically</span>
    <span class="s0"># defined via a property that only works when voting=&quot;soft&quot;.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">5</span><span class="s2">, </span><span class="s1">n_classes=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">7</span><span class="s1">)</span>
    <span class="s1">vote = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s4">&quot;lr&quot; </span><span class="s1">+ str(i)</span><span class="s2">, </span><span class="s1">LogisticRegression()) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(</span><span class="s3">3</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s1">voting=</span><span class="s4">&quot;soft&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">vote.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">calib_clf = CalibratedClassifierCV(estimator=vote</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">&quot;prefit&quot;</span><span class="s1">)</span>
    <span class="s0"># smoke test: should not raise an error</span>
    <span class="s1">calib_clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s1">@pytest.fixture(scope=</span><span class="s4">&quot;module&quot;</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">iris_data():</span>
    <span class="s2">return </span><span class="s1">load_iris(return_X_y=</span><span class="s2">True</span><span class="s1">)</span>


<span class="s1">@pytest.fixture(scope=</span><span class="s4">&quot;module&quot;</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">iris_data_binary(iris_data):</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = iris_data</span>
    <span class="s2">return </span><span class="s1">X[y &lt; </span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y[y &lt; </span><span class="s3">2</span><span class="s1">]</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;n_bins&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">5</span><span class="s2">, </span><span class="s3">10</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;strategy&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;uniform&quot;</span><span class="s2">, </span><span class="s4">&quot;quantile&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_calibration_display_compute(pyplot</span><span class="s2">, </span><span class="s1">iris_data_binary</span><span class="s2">, </span><span class="s1">n_bins</span><span class="s2">, </span><span class="s1">strategy):</span>
    <span class="s0"># Ensure `CalibrationDisplay.from_predictions` and `calibration_curve`</span>
    <span class="s0"># compute the same results. Also checks attributes of the</span>
    <span class="s0"># CalibrationDisplay object.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = iris_data_binary</span>

    <span class="s1">lr = LogisticRegression().fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">viz = CalibrationDisplay.from_estimator(</span>
        <span class="s1">lr</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">n_bins=n_bins</span><span class="s2">, </span><span class="s1">strategy=strategy</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s3">0.8</span>
    <span class="s1">)</span>

    <span class="s1">y_prob = lr.predict_proba(X)[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">prob_true</span><span class="s2">, </span><span class="s1">prob_pred = calibration_curve(</span>
        <span class="s1">y</span><span class="s2">, </span><span class="s1">y_prob</span><span class="s2">, </span><span class="s1">n_bins=n_bins</span><span class="s2">, </span><span class="s1">strategy=strategy</span>
    <span class="s1">)</span>

    <span class="s1">assert_allclose(viz.prob_true</span><span class="s2">, </span><span class="s1">prob_true)</span>
    <span class="s1">assert_allclose(viz.prob_pred</span><span class="s2">, </span><span class="s1">prob_pred)</span>
    <span class="s1">assert_allclose(viz.y_prob</span><span class="s2">, </span><span class="s1">y_prob)</span>

    <span class="s2">assert </span><span class="s1">viz.estimator_name == </span><span class="s4">&quot;LogisticRegression&quot;</span>

    <span class="s0"># cannot fail thanks to pyplot fixture</span>
    <span class="s2">import </span><span class="s1">matplotlib </span><span class="s2">as </span><span class="s1">mpl  </span><span class="s0"># noqa</span>

    <span class="s2">assert </span><span class="s1">isinstance(viz.line_</span><span class="s2">, </span><span class="s1">mpl.lines.Line2D)</span>
    <span class="s2">assert </span><span class="s1">viz.line_.get_alpha() == </span><span class="s3">0.8</span>
    <span class="s2">assert </span><span class="s1">isinstance(viz.ax_</span><span class="s2">, </span><span class="s1">mpl.axes.Axes)</span>
    <span class="s2">assert </span><span class="s1">isinstance(viz.figure_</span><span class="s2">, </span><span class="s1">mpl.figure.Figure)</span>

    <span class="s2">assert </span><span class="s1">viz.ax_.get_xlabel() == </span><span class="s4">&quot;Mean predicted probability (Positive class: 1)&quot;</span>
    <span class="s2">assert </span><span class="s1">viz.ax_.get_ylabel() == </span><span class="s4">&quot;Fraction of positives (Positive class: 1)&quot;</span>

    <span class="s1">expected_legend_labels = [</span><span class="s4">&quot;LogisticRegression&quot;</span><span class="s2">, </span><span class="s4">&quot;Perfectly calibrated&quot;</span><span class="s1">]</span>
    <span class="s1">legend_labels = viz.ax_.get_legend().get_texts()</span>
    <span class="s2">assert </span><span class="s1">len(legend_labels) == len(expected_legend_labels)</span>
    <span class="s2">for </span><span class="s1">labels </span><span class="s2">in </span><span class="s1">legend_labels:</span>
        <span class="s2">assert </span><span class="s1">labels.get_text() </span><span class="s2">in </span><span class="s1">expected_legend_labels</span>


<span class="s2">def </span><span class="s1">test_plot_calibration_curve_pipeline(pyplot</span><span class="s2">, </span><span class="s1">iris_data_binary):</span>
    <span class="s0"># Ensure pipelines are supported by CalibrationDisplay.from_estimator</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = iris_data_binary</span>
    <span class="s1">clf = make_pipeline(StandardScaler()</span><span class="s2">, </span><span class="s1">LogisticRegression())</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">viz = CalibrationDisplay.from_estimator(clf</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">expected_legend_labels = [viz.estimator_name</span><span class="s2">, </span><span class="s4">&quot;Perfectly calibrated&quot;</span><span class="s1">]</span>
    <span class="s1">legend_labels = viz.ax_.get_legend().get_texts()</span>
    <span class="s2">assert </span><span class="s1">len(legend_labels) == len(expected_legend_labels)</span>
    <span class="s2">for </span><span class="s1">labels </span><span class="s2">in </span><span class="s1">legend_labels:</span>
        <span class="s2">assert </span><span class="s1">labels.get_text() </span><span class="s2">in </span><span class="s1">expected_legend_labels</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;name, expected_label&quot;</span><span class="s2">, </span><span class="s1">[(</span><span class="s2">None, </span><span class="s4">&quot;_line1&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s4">&quot;my_est&quot;</span><span class="s2">, </span><span class="s4">&quot;my_est&quot;</span><span class="s1">)]</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_calibration_display_default_labels(pyplot</span><span class="s2">, </span><span class="s1">name</span><span class="s2">, </span><span class="s1">expected_label):</span>
    <span class="s1">prob_true = np.array([</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">prob_pred = np.array([</span><span class="s3">0.2</span><span class="s2">, </span><span class="s3">0.8</span><span class="s2">, </span><span class="s3">0.8</span><span class="s2">, </span><span class="s3">0.4</span><span class="s1">])</span>
    <span class="s1">y_prob = np.array([])</span>

    <span class="s1">viz = CalibrationDisplay(prob_true</span><span class="s2">, </span><span class="s1">prob_pred</span><span class="s2">, </span><span class="s1">y_prob</span><span class="s2">, </span><span class="s1">estimator_name=name)</span>
    <span class="s1">viz.plot()</span>

    <span class="s1">expected_legend_labels = [] </span><span class="s2">if </span><span class="s1">name </span><span class="s2">is None else </span><span class="s1">[name]</span>
    <span class="s1">expected_legend_labels.append(</span><span class="s4">&quot;Perfectly calibrated&quot;</span><span class="s1">)</span>
    <span class="s1">legend_labels = viz.ax_.get_legend().get_texts()</span>
    <span class="s2">assert </span><span class="s1">len(legend_labels) == len(expected_legend_labels)</span>
    <span class="s2">for </span><span class="s1">labels </span><span class="s2">in </span><span class="s1">legend_labels:</span>
        <span class="s2">assert </span><span class="s1">labels.get_text() </span><span class="s2">in </span><span class="s1">expected_legend_labels</span>


<span class="s2">def </span><span class="s1">test_calibration_display_label_class_plot(pyplot):</span>
    <span class="s0"># Checks that when instantiating `CalibrationDisplay` class then calling</span>
    <span class="s0"># `plot`, `self.estimator_name` is the one given in `plot`</span>
    <span class="s1">prob_true = np.array([</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">prob_pred = np.array([</span><span class="s3">0.2</span><span class="s2">, </span><span class="s3">0.8</span><span class="s2">, </span><span class="s3">0.8</span><span class="s2">, </span><span class="s3">0.4</span><span class="s1">])</span>
    <span class="s1">y_prob = np.array([])</span>

    <span class="s1">name = </span><span class="s4">&quot;name one&quot;</span>
    <span class="s1">viz = CalibrationDisplay(prob_true</span><span class="s2">, </span><span class="s1">prob_pred</span><span class="s2">, </span><span class="s1">y_prob</span><span class="s2">, </span><span class="s1">estimator_name=name)</span>
    <span class="s2">assert </span><span class="s1">viz.estimator_name == name</span>
    <span class="s1">name = </span><span class="s4">&quot;name two&quot;</span>
    <span class="s1">viz.plot(name=name)</span>

    <span class="s1">expected_legend_labels = [name</span><span class="s2">, </span><span class="s4">&quot;Perfectly calibrated&quot;</span><span class="s1">]</span>
    <span class="s1">legend_labels = viz.ax_.get_legend().get_texts()</span>
    <span class="s2">assert </span><span class="s1">len(legend_labels) == len(expected_legend_labels)</span>
    <span class="s2">for </span><span class="s1">labels </span><span class="s2">in </span><span class="s1">legend_labels:</span>
        <span class="s2">assert </span><span class="s1">labels.get_text() </span><span class="s2">in </span><span class="s1">expected_legend_labels</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;constructor_name&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;from_estimator&quot;</span><span class="s2">, </span><span class="s4">&quot;from_predictions&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_calibration_display_name_multiple_calls(</span>
    <span class="s1">constructor_name</span><span class="s2">, </span><span class="s1">pyplot</span><span class="s2">, </span><span class="s1">iris_data_binary</span>
<span class="s1">):</span>
    <span class="s0"># Check that the `name` used when calling</span>
    <span class="s0"># `CalibrationDisplay.from_predictions` or</span>
    <span class="s0"># `CalibrationDisplay.from_estimator` is used when multiple</span>
    <span class="s0"># `CalibrationDisplay.viz.plot()` calls are made.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = iris_data_binary</span>
    <span class="s1">clf_name = </span><span class="s4">&quot;my hand-crafted name&quot;</span>
    <span class="s1">clf = LogisticRegression().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">y_prob = clf.predict_proba(X)[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">constructor = getattr(CalibrationDisplay</span><span class="s2">, </span><span class="s1">constructor_name)</span>
    <span class="s1">params = (clf</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y) </span><span class="s2">if </span><span class="s1">constructor_name == </span><span class="s4">&quot;from_estimator&quot; </span><span class="s2">else </span><span class="s1">(y</span><span class="s2">, </span><span class="s1">y_prob)</span>

    <span class="s1">viz = constructor(*params</span><span class="s2">, </span><span class="s1">name=clf_name)</span>
    <span class="s2">assert </span><span class="s1">viz.estimator_name == clf_name</span>
    <span class="s1">pyplot.close(</span><span class="s4">&quot;all&quot;</span><span class="s1">)</span>
    <span class="s1">viz.plot()</span>

    <span class="s1">expected_legend_labels = [clf_name</span><span class="s2">, </span><span class="s4">&quot;Perfectly calibrated&quot;</span><span class="s1">]</span>
    <span class="s1">legend_labels = viz.ax_.get_legend().get_texts()</span>
    <span class="s2">assert </span><span class="s1">len(legend_labels) == len(expected_legend_labels)</span>
    <span class="s2">for </span><span class="s1">labels </span><span class="s2">in </span><span class="s1">legend_labels:</span>
        <span class="s2">assert </span><span class="s1">labels.get_text() </span><span class="s2">in </span><span class="s1">expected_legend_labels</span>

    <span class="s1">pyplot.close(</span><span class="s4">&quot;all&quot;</span><span class="s1">)</span>
    <span class="s1">clf_name = </span><span class="s4">&quot;another_name&quot;</span>
    <span class="s1">viz.plot(name=clf_name)</span>
    <span class="s2">assert </span><span class="s1">len(legend_labels) == len(expected_legend_labels)</span>
    <span class="s2">for </span><span class="s1">labels </span><span class="s2">in </span><span class="s1">legend_labels:</span>
        <span class="s2">assert </span><span class="s1">labels.get_text() </span><span class="s2">in </span><span class="s1">expected_legend_labels</span>


<span class="s2">def </span><span class="s1">test_calibration_display_ref_line(pyplot</span><span class="s2">, </span><span class="s1">iris_data_binary):</span>
    <span class="s0"># Check that `ref_line` only appears once</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = iris_data_binary</span>
    <span class="s1">lr = LogisticRegression().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">dt = DecisionTreeClassifier().fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">viz = CalibrationDisplay.from_estimator(lr</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">viz2 = CalibrationDisplay.from_estimator(dt</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">ax=viz.ax_)</span>

    <span class="s1">labels = viz2.ax_.get_legend_handles_labels()[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s2">assert </span><span class="s1">labels.count(</span><span class="s4">&quot;Perfectly calibrated&quot;</span><span class="s1">) == </span><span class="s3">1</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;dtype_y_str&quot;</span><span class="s2">, </span><span class="s1">[str</span><span class="s2">, </span><span class="s1">object])</span>
<span class="s2">def </span><span class="s1">test_calibration_curve_pos_label_error_str(dtype_y_str):</span>
    <span class="s5">&quot;&quot;&quot;Check error message when a `pos_label` is not specified with `str` targets.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">y1 = np.array([</span><span class="s4">&quot;spam&quot;</span><span class="s1">] * </span><span class="s3">3 </span><span class="s1">+ [</span><span class="s4">&quot;eggs&quot;</span><span class="s1">] * </span><span class="s3">2</span><span class="s2">, </span><span class="s1">dtype=dtype_y_str)</span>
    <span class="s1">y2 = rng.randint(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s1">size=y1.size)</span>

    <span class="s1">err_msg = (</span>
        <span class="s4">&quot;y_true takes value in {'eggs', 'spam'} and pos_label is not &quot;</span>
        <span class="s4">&quot;specified: either make y_true take value in {0, 1} or {-1, 1} or &quot;</span>
        <span class="s4">&quot;pass pos_label explicitly&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">calibration_curve(y1</span><span class="s2">, </span><span class="s1">y2)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;dtype_y_str&quot;</span><span class="s2">, </span><span class="s1">[str</span><span class="s2">, </span><span class="s1">object])</span>
<span class="s2">def </span><span class="s1">test_calibration_curve_pos_label(dtype_y_str):</span>
    <span class="s5">&quot;&quot;&quot;Check the behaviour when passing explicitly `pos_label`.&quot;&quot;&quot;</span>
    <span class="s1">y_true = np.array([</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">classes = np.array([</span><span class="s4">&quot;spam&quot;</span><span class="s2">, </span><span class="s4">&quot;egg&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=dtype_y_str)</span>
    <span class="s1">y_true_str = classes[y_true]</span>
    <span class="s1">y_pred = np.array([</span><span class="s3">0.1</span><span class="s2">, </span><span class="s3">0.2</span><span class="s2">, </span><span class="s3">0.3</span><span class="s2">, </span><span class="s3">0.4</span><span class="s2">, </span><span class="s3">0.65</span><span class="s2">, </span><span class="s3">0.7</span><span class="s2">, </span><span class="s3">0.8</span><span class="s2">, </span><span class="s3">0.9</span><span class="s2">, </span><span class="s3">1.0</span><span class="s1">])</span>

    <span class="s0"># default case</span>
    <span class="s1">prob_true</span><span class="s2">, </span><span class="s1">_ = calibration_curve(y_true</span><span class="s2">, </span><span class="s1">y_pred</span><span class="s2">, </span><span class="s1">n_bins=</span><span class="s3">4</span><span class="s1">)</span>
    <span class="s1">assert_allclose(prob_true</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0.5</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">])</span>
    <span class="s0"># if `y_true` contains `str`, then `pos_label` is required</span>
    <span class="s1">prob_true</span><span class="s2">, </span><span class="s1">_ = calibration_curve(y_true_str</span><span class="s2">, </span><span class="s1">y_pred</span><span class="s2">, </span><span class="s1">n_bins=</span><span class="s3">4</span><span class="s2">, </span><span class="s1">pos_label=</span><span class="s4">&quot;egg&quot;</span><span class="s1">)</span>
    <span class="s1">assert_allclose(prob_true</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0.5</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">])</span>

    <span class="s1">prob_true</span><span class="s2">, </span><span class="s1">_ = calibration_curve(y_true</span><span class="s2">, </span><span class="s3">1 </span><span class="s1">- y_pred</span><span class="s2">, </span><span class="s1">n_bins=</span><span class="s3">4</span><span class="s2">, </span><span class="s1">pos_label=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">assert_allclose(prob_true</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0.5</span><span class="s2">, </span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">prob_true</span><span class="s2">, </span><span class="s1">_ = calibration_curve(y_true_str</span><span class="s2">, </span><span class="s3">1 </span><span class="s1">- y_pred</span><span class="s2">, </span><span class="s1">n_bins=</span><span class="s3">4</span><span class="s2">, </span><span class="s1">pos_label=</span><span class="s4">&quot;spam&quot;</span><span class="s1">)</span>
    <span class="s1">assert_allclose(prob_true</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0.5</span><span class="s2">, </span><span class="s3">1</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;pos_label, expected_pos_label&quot;</span><span class="s2">, </span><span class="s1">[(</span><span class="s2">None, </span><span class="s3">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)])</span>
<span class="s2">def </span><span class="s1">test_calibration_display_pos_label(</span>
    <span class="s1">pyplot</span><span class="s2">, </span><span class="s1">iris_data_binary</span><span class="s2">, </span><span class="s1">pos_label</span><span class="s2">, </span><span class="s1">expected_pos_label</span>
<span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot;Check the behaviour of `pos_label` in the `CalibrationDisplay`.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = iris_data_binary</span>

    <span class="s1">lr = LogisticRegression().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">viz = CalibrationDisplay.from_estimator(lr</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">pos_label=pos_label)</span>

    <span class="s1">y_prob = lr.predict_proba(X)[:</span><span class="s2">, </span><span class="s1">expected_pos_label]</span>
    <span class="s1">prob_true</span><span class="s2">, </span><span class="s1">prob_pred = calibration_curve(y</span><span class="s2">, </span><span class="s1">y_prob</span><span class="s2">, </span><span class="s1">pos_label=pos_label)</span>

    <span class="s1">assert_allclose(viz.prob_true</span><span class="s2">, </span><span class="s1">prob_true)</span>
    <span class="s1">assert_allclose(viz.prob_pred</span><span class="s2">, </span><span class="s1">prob_pred)</span>
    <span class="s1">assert_allclose(viz.y_prob</span><span class="s2">, </span><span class="s1">y_prob)</span>

    <span class="s2">assert </span><span class="s1">(</span>
        <span class="s1">viz.ax_.get_xlabel()</span>
        <span class="s1">== </span><span class="s4">f&quot;Mean predicted probability (Positive class: </span><span class="s2">{</span><span class="s1">expected_pos_label</span><span class="s2">}</span><span class="s4">)&quot;</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">(</span>
        <span class="s1">viz.ax_.get_ylabel()</span>
        <span class="s1">== </span><span class="s4">f&quot;Fraction of positives (Positive class: </span><span class="s2">{</span><span class="s1">expected_pos_label</span><span class="s2">}</span><span class="s4">)&quot;</span>
    <span class="s1">)</span>

    <span class="s1">expected_legend_labels = [lr.__class__.__name__</span><span class="s2">, </span><span class="s4">&quot;Perfectly calibrated&quot;</span><span class="s1">]</span>
    <span class="s1">legend_labels = viz.ax_.get_legend().get_texts()</span>
    <span class="s2">assert </span><span class="s1">len(legend_labels) == len(expected_legend_labels)</span>
    <span class="s2">for </span><span class="s1">labels </span><span class="s2">in </span><span class="s1">legend_labels:</span>
        <span class="s2">assert </span><span class="s1">labels.get_text() </span><span class="s2">in </span><span class="s1">expected_legend_labels</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;sigmoid&quot;</span><span class="s2">, </span><span class="s4">&quot;isotonic&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;ensemble&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_calibrated_classifier_cv_double_sample_weights_equivalence(method</span><span class="s2">, </span><span class="s1">ensemble):</span>
    <span class="s5">&quot;&quot;&quot;Check that passing repeating twice the dataset `X` is equivalent to 
    passing a `sample_weight` with a factor 2.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = load_iris(return_X_y=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s0"># Scale the data to avoid any convergence issue</span>
    <span class="s1">X = StandardScaler().fit_transform(X)</span>
    <span class="s0"># Only use 2 classes</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = X[:</span><span class="s3">100</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y[:</span><span class="s3">100</span><span class="s1">]</span>
    <span class="s1">sample_weight = np.ones_like(y) * </span><span class="s3">2</span>

    <span class="s0"># Interlace the data such that a 2-fold cross-validation will be equivalent</span>
    <span class="s0"># to using the original dataset with a sample weights of 2</span>
    <span class="s1">X_twice = np.zeros((X.shape[</span><span class="s3">0</span><span class="s1">] * </span><span class="s3">2</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s3">1</span><span class="s1">])</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>
    <span class="s1">X_twice[::</span><span class="s3">2</span><span class="s2">, </span><span class="s1">:] = X</span>
    <span class="s1">X_twice[</span><span class="s3">1</span><span class="s1">::</span><span class="s3">2</span><span class="s2">, </span><span class="s1">:] = X</span>
    <span class="s1">y_twice = np.zeros(y.shape[</span><span class="s3">0</span><span class="s1">] * </span><span class="s3">2</span><span class="s2">, </span><span class="s1">dtype=y.dtype)</span>
    <span class="s1">y_twice[::</span><span class="s3">2</span><span class="s1">] = y</span>
    <span class="s1">y_twice[</span><span class="s3">1</span><span class="s1">::</span><span class="s3">2</span><span class="s1">] = y</span>

    <span class="s1">estimator = LogisticRegression()</span>
    <span class="s1">calibrated_clf_without_weights = CalibratedClassifierCV(</span>
        <span class="s1">estimator</span><span class="s2">,</span>
        <span class="s1">method=method</span><span class="s2">,</span>
        <span class="s1">ensemble=ensemble</span><span class="s2">,</span>
        <span class="s1">cv=</span><span class="s3">2</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">calibrated_clf_with_weights = clone(calibrated_clf_without_weights)</span>

    <span class="s1">calibrated_clf_with_weights.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">calibrated_clf_without_weights.fit(X_twice</span><span class="s2">, </span><span class="s1">y_twice)</span>

    <span class="s0"># Check that the underlying fitted estimators have the same coefficients</span>
    <span class="s2">for </span><span class="s1">est_with_weights</span><span class="s2">, </span><span class="s1">est_without_weights </span><span class="s2">in </span><span class="s1">zip(</span>
        <span class="s1">calibrated_clf_with_weights.calibrated_classifiers_</span><span class="s2">,</span>
        <span class="s1">calibrated_clf_without_weights.calibrated_classifiers_</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">assert_allclose(</span>
            <span class="s1">est_with_weights.estimator.coef_</span><span class="s2">,</span>
            <span class="s1">est_without_weights.estimator.coef_</span><span class="s2">,</span>
        <span class="s1">)</span>

    <span class="s0"># Check that the predictions are the same</span>
    <span class="s1">y_pred_with_weights = calibrated_clf_with_weights.predict_proba(X)</span>
    <span class="s1">y_pred_without_weights = calibrated_clf_without_weights.predict_proba(X)</span>

    <span class="s1">assert_allclose(y_pred_with_weights</span><span class="s2">, </span><span class="s1">y_pred_without_weights)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;fit_params_type&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;list&quot;</span><span class="s2">, </span><span class="s4">&quot;array&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_calibration_with_fit_params(fit_params_type</span><span class="s2">, </span><span class="s1">data):</span>
    <span class="s5">&quot;&quot;&quot;Tests that fit_params are passed to the underlying base estimator. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/12384 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = data</span>
    <span class="s1">fit_params = {</span>
        <span class="s4">&quot;a&quot;</span><span class="s1">: _convert_container(y</span><span class="s2">, </span><span class="s1">fit_params_type)</span><span class="s2">,</span>
        <span class="s4">&quot;b&quot;</span><span class="s1">: _convert_container(y</span><span class="s2">, </span><span class="s1">fit_params_type)</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s1">clf = CheckingClassifier(expected_fit_params=[</span><span class="s4">&quot;a&quot;</span><span class="s2">, </span><span class="s4">&quot;b&quot;</span><span class="s1">])</span>
    <span class="s1">pc_clf = CalibratedClassifierCV(clf)</span>

    <span class="s1">pc_clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">**fit_params)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;sample_weight&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">[</span><span class="s3">1.0</span><span class="s1">] * N_SAMPLES</span><span class="s2">,</span>
        <span class="s1">np.ones(N_SAMPLES)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_calibration_with_sample_weight_base_estimator(sample_weight</span><span class="s2">, </span><span class="s1">data):</span>
    <span class="s5">&quot;&quot;&quot;Tests that sample_weight is passed to the underlying base 
    estimator. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = data</span>
    <span class="s1">clf = CheckingClassifier(expected_sample_weight=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">pc_clf = CalibratedClassifierCV(clf)</span>

    <span class="s1">pc_clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>


<span class="s2">def </span><span class="s1">test_calibration_without_sample_weight_base_estimator(data):</span>
    <span class="s5">&quot;&quot;&quot;Check that even if the estimator doesn't support 
    sample_weight, fitting with sample_weight still works. 
 
    There should be a warning, since the sample_weight is not passed 
    on to the estimator. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = data</span>
    <span class="s1">sample_weight = np.ones_like(y)</span>

    <span class="s2">class </span><span class="s1">ClfWithoutSampleWeight(CheckingClassifier):</span>
        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">**fit_params):</span>
            <span class="s2">assert </span><span class="s4">&quot;sample_weight&quot; </span><span class="s2">not in </span><span class="s1">fit_params</span>
            <span class="s2">return </span><span class="s1">super().fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">**fit_params)</span>

    <span class="s1">clf = ClfWithoutSampleWeight()</span>
    <span class="s1">pc_clf = CalibratedClassifierCV(clf)</span>

    <span class="s2">with </span><span class="s1">pytest.warns(UserWarning):</span>
        <span class="s1">pc_clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;sigmoid&quot;</span><span class="s2">, </span><span class="s4">&quot;isotonic&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;ensemble&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_calibrated_classifier_cv_zeros_sample_weights_equivalence(method</span><span class="s2">, </span><span class="s1">ensemble):</span>
    <span class="s5">&quot;&quot;&quot;Check that passing removing some sample from the dataset `X` is 
    equivalent to passing a `sample_weight` with a factor 0.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = load_iris(return_X_y=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s0"># Scale the data to avoid any convergence issue</span>
    <span class="s1">X = StandardScaler().fit_transform(X)</span>
    <span class="s0"># Only use 2 classes and select samples such that 2-fold cross-validation</span>
    <span class="s0"># split will lead to an equivalence with a `sample_weight` of 0</span>
    <span class="s1">X = np.vstack((X[:</span><span class="s3">40</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X[</span><span class="s3">50</span><span class="s1">:</span><span class="s3">90</span><span class="s1">]))</span>
    <span class="s1">y = np.hstack((y[:</span><span class="s3">40</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y[</span><span class="s3">50</span><span class="s1">:</span><span class="s3">90</span><span class="s1">]))</span>
    <span class="s1">sample_weight = np.zeros_like(y)</span>
    <span class="s1">sample_weight[::</span><span class="s3">2</span><span class="s1">] = </span><span class="s3">1</span>

    <span class="s1">estimator = LogisticRegression()</span>
    <span class="s1">calibrated_clf_without_weights = CalibratedClassifierCV(</span>
        <span class="s1">estimator</span><span class="s2">,</span>
        <span class="s1">method=method</span><span class="s2">,</span>
        <span class="s1">ensemble=ensemble</span><span class="s2">,</span>
        <span class="s1">cv=</span><span class="s3">2</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">calibrated_clf_with_weights = clone(calibrated_clf_without_weights)</span>

    <span class="s1">calibrated_clf_with_weights.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">calibrated_clf_without_weights.fit(X[::</span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y[::</span><span class="s3">2</span><span class="s1">])</span>

    <span class="s0"># Check that the underlying fitted estimators have the same coefficients</span>
    <span class="s2">for </span><span class="s1">est_with_weights</span><span class="s2">, </span><span class="s1">est_without_weights </span><span class="s2">in </span><span class="s1">zip(</span>
        <span class="s1">calibrated_clf_with_weights.calibrated_classifiers_</span><span class="s2">,</span>
        <span class="s1">calibrated_clf_without_weights.calibrated_classifiers_</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">assert_allclose(</span>
            <span class="s1">est_with_weights.estimator.coef_</span><span class="s2">,</span>
            <span class="s1">est_without_weights.estimator.coef_</span><span class="s2">,</span>
        <span class="s1">)</span>

    <span class="s0"># Check that the predictions are the same</span>
    <span class="s1">y_pred_with_weights = calibrated_clf_with_weights.predict_proba(X)</span>
    <span class="s1">y_pred_without_weights = calibrated_clf_without_weights.predict_proba(X)</span>

    <span class="s1">assert_allclose(y_pred_with_weights</span><span class="s2">, </span><span class="s1">y_pred_without_weights)</span>


<span class="s0"># TODO(1.4): Remove</span>
<span class="s2">def </span><span class="s1">test_calibrated_classifier_error_base_estimator(data):</span>
    <span class="s5">&quot;&quot;&quot;Check that we raise an error is a user set both `base_estimator` and 
    `estimator`.&quot;&quot;&quot;</span>
    <span class="s1">calibrated_classifier = CalibratedClassifierCV(</span>
        <span class="s1">base_estimator=LogisticRegression()</span><span class="s2">, </span><span class="s1">estimator=LogisticRegression()</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s4">&quot;Both `base_estimator` and `estimator`&quot;</span><span class="s1">):</span>
        <span class="s1">calibrated_classifier.fit(*data)</span>


<span class="s0"># TODO(1.4): Remove</span>
<span class="s2">def </span><span class="s1">test_calibrated_classifier_deprecation_base_estimator(data):</span>
    <span class="s5">&quot;&quot;&quot;Check that we raise a warning regarding the deprecation of 
    `base_estimator`.&quot;&quot;&quot;</span>
    <span class="s1">calibrated_classifier = CalibratedClassifierCV(base_estimator=LogisticRegression())</span>
    <span class="s1">warn_msg = </span><span class="s4">&quot;`base_estimator` was renamed to `estimator`&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s2">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">calibrated_classifier.fit(*data)</span>


<span class="s2">def </span><span class="s1">test_calibration_with_non_sample_aligned_fit_param(data):</span>
    <span class="s5">&quot;&quot;&quot;Check that CalibratedClassifierCV does not enforce sample alignment 
    for fit parameters.&quot;&quot;&quot;</span>

    <span class="s2">class </span><span class="s1">TestClassifier(LogisticRegression):</span>
        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None, </span><span class="s1">fit_param=</span><span class="s2">None</span><span class="s1">):</span>
            <span class="s2">assert </span><span class="s1">fit_param </span><span class="s2">is not None</span>
            <span class="s2">return </span><span class="s1">super().fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">CalibratedClassifierCV(estimator=TestClassifier()).fit(</span>
        <span class="s1">*data</span><span class="s2">, </span><span class="s1">fit_param=np.ones(len(data[</span><span class="s3">1</span><span class="s1">]) + </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_calibrated_classifier_cv_works_with_large_confidence_scores(</span>
    <span class="s1">global_random_seed</span><span class="s2">,</span>
<span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot;Test that :class:`CalibratedClassifierCV` works with large confidence 
    scores when using the `sigmoid` method, particularly with the 
    :class:`SGDClassifier`. 
 
    Non-regression test for issue #26766. 
    &quot;&quot;&quot;</span>
    <span class="s1">prob = </span><span class="s3">0.67</span>
    <span class="s1">n = </span><span class="s3">1000</span>
    <span class="s1">random_noise = np.random.default_rng(global_random_seed).normal(size=n)</span>

    <span class="s1">y = np.array([</span><span class="s3">1</span><span class="s1">] * int(n * prob) + [</span><span class="s3">0</span><span class="s1">] * (n - int(n * prob)))</span>
    <span class="s1">X = </span><span class="s3">1e5 </span><span class="s1">* y.reshape((-</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)) + random_noise</span>

    <span class="s0"># Check that the decision function of SGDClassifier produces predicted</span>
    <span class="s0"># values that are quite large, for the data under consideration.</span>
    <span class="s1">cv = check_cv(cv=</span><span class="s2">None, </span><span class="s1">y=y</span><span class="s2">, </span><span class="s1">classifier=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">indices = cv.split(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">for </span><span class="s1">train</span><span class="s2">, </span><span class="s1">test </span><span class="s2">in </span><span class="s1">indices:</span>
        <span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train = X[train]</span><span class="s2">, </span><span class="s1">y[train]</span>
        <span class="s1">X_test = X[test]</span>
        <span class="s1">sgd_clf = SGDClassifier(loss=</span><span class="s4">&quot;squared_hinge&quot;</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>
        <span class="s1">sgd_clf.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
        <span class="s1">predictions = sgd_clf.decision_function(X_test)</span>
        <span class="s2">assert </span><span class="s1">(predictions &gt; </span><span class="s3">1e4</span><span class="s1">).any()</span>

    <span class="s0"># Compare the CalibratedClassifierCV using the sigmoid method with the</span>
    <span class="s0"># CalibratedClassifierCV using the isotonic method. The isotonic method</span>
    <span class="s0"># is used for comparison because it is numerically stable.</span>
    <span class="s1">clf_sigmoid = CalibratedClassifierCV(</span>
        <span class="s1">SGDClassifier(loss=</span><span class="s4">&quot;squared_hinge&quot;</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span><span class="s2">,</span>
        <span class="s1">method=</span><span class="s4">&quot;sigmoid&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">score_sigmoid = cross_val_score(clf_sigmoid</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s4">&quot;roc_auc&quot;</span><span class="s1">)</span>

    <span class="s0"># The isotonic method is used for comparison because it is numerically</span>
    <span class="s0"># stable.</span>
    <span class="s1">clf_isotonic = CalibratedClassifierCV(</span>
        <span class="s1">SGDClassifier(loss=</span><span class="s4">&quot;squared_hinge&quot;</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span><span class="s2">,</span>
        <span class="s1">method=</span><span class="s4">&quot;isotonic&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">score_isotonic = cross_val_score(clf_isotonic</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s4">&quot;roc_auc&quot;</span><span class="s1">)</span>

    <span class="s0"># The AUC score should be the same because it is invariant under</span>
    <span class="s0"># strictly monotonic conditions</span>
    <span class="s1">assert_allclose(score_sigmoid</span><span class="s2">, </span><span class="s1">score_isotonic)</span>


<span class="s2">def </span><span class="s1">test_sigmoid_calibration_max_abs_prediction_threshold(global_random_seed):</span>
    <span class="s1">random_state = np.random.RandomState(seed=global_random_seed)</span>
    <span class="s1">n = </span><span class="s3">100</span>
    <span class="s1">y = random_state.randint(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s1">size=n)</span>

    <span class="s0"># Check that for small enough predictions ranging from -2 to 2, the</span>
    <span class="s0"># threshold value has no impact on the outcome</span>
    <span class="s1">predictions_small = random_state.uniform(low=-</span><span class="s3">2</span><span class="s2">, </span><span class="s1">high=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">size=</span><span class="s3">100</span><span class="s1">)</span>

    <span class="s0"># Using a threshold lower than the maximum absolute value of the</span>
    <span class="s0"># predictions enables internal re-scaling by max(abs(predictions_small)).</span>
    <span class="s1">threshold_1 = </span><span class="s3">0.1</span>
    <span class="s1">a1</span><span class="s2">, </span><span class="s1">b1 = _sigmoid_calibration(</span>
        <span class="s1">predictions=predictions_small</span><span class="s2">,</span>
        <span class="s1">y=y</span><span class="s2">,</span>
        <span class="s1">max_abs_prediction_threshold=threshold_1</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s0"># Using a larger threshold disables rescaling.</span>
    <span class="s1">threshold_2 = </span><span class="s3">10</span>
    <span class="s1">a2</span><span class="s2">, </span><span class="s1">b2 = _sigmoid_calibration(</span>
        <span class="s1">predictions=predictions_small</span><span class="s2">,</span>
        <span class="s1">y=y</span><span class="s2">,</span>
        <span class="s1">max_abs_prediction_threshold=threshold_2</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s0"># Using default threshold of 30 also disables the scaling.</span>
    <span class="s1">a3</span><span class="s2">, </span><span class="s1">b3 = _sigmoid_calibration(</span>
        <span class="s1">predictions=predictions_small</span><span class="s2">,</span>
        <span class="s1">y=y</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s0"># Depends on the tolerance of the underlying quasy-newton solver which is</span>
    <span class="s0"># not too strict by default.</span>
    <span class="s1">atol = </span><span class="s3">1e-6</span>
    <span class="s1">assert_allclose(a1</span><span class="s2">, </span><span class="s1">a2</span><span class="s2">, </span><span class="s1">atol=atol)</span>
    <span class="s1">assert_allclose(a2</span><span class="s2">, </span><span class="s1">a3</span><span class="s2">, </span><span class="s1">atol=atol)</span>
    <span class="s1">assert_allclose(b1</span><span class="s2">, </span><span class="s1">b2</span><span class="s2">, </span><span class="s1">atol=atol)</span>
    <span class="s1">assert_allclose(b2</span><span class="s2">, </span><span class="s1">b3</span><span class="s2">, </span><span class="s1">atol=atol)</span>
</pre>
</body>
</html>