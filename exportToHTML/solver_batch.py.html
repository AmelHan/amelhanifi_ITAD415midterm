<html>
<head>
<title>solver_batch.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #6897bb;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
solver_batch.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">scipy </span><span class="s0">as </span><span class="s1">sp</span>
<span class="s0">import </span><span class="s1">warnings</span>
<span class="s0">from </span><span class="s1">scipy.linalg </span><span class="s0">import </span><span class="s1">LinAlgWarning</span>
<span class="s0">from </span><span class="s1">sklearn.exceptions </span><span class="s0">import </span><span class="s1">DataConversionWarning</span>
<span class="s0">from </span><span class="s1">sklearn.base </span><span class="s0">import </span><span class="s1">BaseEstimator</span><span class="s0">, </span><span class="s1">RegressorMixin</span>

<span class="s0">from </span><span class="s1">sklearn.utils.validation </span><span class="s0">import </span><span class="s1">check_is_fitted</span>
<span class="s0">from </span><span class="s1">sklearn.utils.extmath </span><span class="s0">import </span><span class="s1">safe_sparse_dot</span>
<span class="s0">from </span><span class="s1">sklearn.utils </span><span class="s0">import </span><span class="s1">check_X_y</span><span class="s0">, </span><span class="s1">check_array</span>

<span class="s1">warnings.simplefilter(</span><span class="s2">&quot;ignore&quot;</span><span class="s0">, </span><span class="s1">LinAlgWarning)</span>

<span class="s0">class </span><span class="s1">BatchCholeskySolver(BaseEstimator</span><span class="s0">, </span><span class="s1">RegressorMixin):</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s3">1e-7</span><span class="s1">):</span>
        <span class="s1">self.alpha = alpha</span>

    <span class="s0">def </span><span class="s1">_init_XY(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
        <span class="s4">&quot;&quot;&quot;Initialize covariance matrices, including a separate bias term. 
        &quot;&quot;&quot;</span>
        <span class="s1">d_in = X.shape[</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">self._XtX = np.eye(d_in + </span><span class="s3">1</span><span class="s1">) * self.alpha</span>
        <span class="s1">self._XtX[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">] = </span><span class="s3">0</span>
        <span class="s0">if </span><span class="s1">len(y.shape) == </span><span class="s3">1</span><span class="s1">:</span>
            <span class="s1">self._XtY = np.zeros((d_in + </span><span class="s3">1</span><span class="s0">,</span><span class="s1">))</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">self._XtY = np.zeros((d_in + </span><span class="s3">1</span><span class="s0">, </span><span class="s1">y.shape[</span><span class="s3">1</span><span class="s1">]))</span>

    <span class="s1">@property</span>
    <span class="s0">def </span><span class="s1">XtY_(self):</span>
        <span class="s0">return </span><span class="s1">self._XtY</span>

    <span class="s1">@property</span>
    <span class="s0">def </span><span class="s1">XtX_(self):</span>
        <span class="s0">return </span><span class="s1">self._XtX</span>

    <span class="s1">@XtY_.setter</span>
    <span class="s0">def </span><span class="s1">XtY_(self</span><span class="s0">, </span><span class="s1">value):</span>
        <span class="s1">self._XtY = value</span>

    <span class="s1">@XtX_.setter</span>
    <span class="s0">def </span><span class="s1">XtX_(self</span><span class="s0">, </span><span class="s1">value):</span>
        <span class="s1">self._XtX = value</span>

    <span class="s0">def </span><span class="s1">_solve(self):</span>
        <span class="s4">&quot;&quot;&quot;Second stage of solution (X'X)B = X'Y using Cholesky decomposition. 
 
        Sets `is_fitted_` to True. 
        &quot;&quot;&quot;</span>
        <span class="s1">B = sp.linalg.solve(self._XtX</span><span class="s0">, </span><span class="s1">self._XtY</span><span class="s0">, </span><span class="s1">assume_a=</span><span class="s2">'sym'</span><span class="s0">, </span><span class="s1">overwrite_a=</span><span class="s0">False, </span><span class="s1">overwrite_b=</span><span class="s0">False</span><span class="s1">)</span>
        <span class="s1">self.coef_ = B[</span><span class="s3">1</span><span class="s1">:]</span>
        <span class="s1">self.intercept_ = B[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">self.is_fitted_ = </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">_reset(self):</span>
        <span class="s4">&quot;&quot;&quot;Erase solution and data matrices. 
        &quot;&quot;&quot;</span>
        <span class="s1">[delattr(self</span><span class="s0">, </span><span class="s1">attr) </span><span class="s0">for </span><span class="s1">attr </span><span class="s0">in </span><span class="s1">(</span><span class="s2">'_XtX'</span><span class="s0">, </span><span class="s2">'_XtY'</span><span class="s0">, </span><span class="s2">'coef_'</span><span class="s0">, </span><span class="s2">'intercept_'</span><span class="s0">, </span><span class="s2">'is_fitted_'</span><span class="s1">) </span><span class="s0">if </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s1">attr)]</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
        <span class="s4">&quot;&quot;&quot;Solves an L2-regularized linear system like Ridge regression, overwrites any previous solutions. 
        &quot;&quot;&quot;</span>
        <span class="s1">self._reset()  </span><span class="s5"># remove old solution</span>
        <span class="s1">self.partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">compute_output_weights=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">self</span>

    <span class="s0">def </span><span class="s1">partial_fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">forget=</span><span class="s0">False, </span><span class="s1">compute_output_weights=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s4">&quot;&quot;&quot;Update model with a new batch of data. 
 
        Output weight computation can be temporary turned off for faster processing. This will mark model as 
        not fit. Enable `compute_output_weights` in the final call to `partial_fit`. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix}, shape=[n_samples, n_features] 
            Training input samples 
 
        y : array-like, shape=[n_samples, n_targets] 
            Training targets 
 
        forget : boolean, default False 
            Performs a negative update, effectively removing the information given by training 
            samples from the model. Output weights need to be re-computed after forgetting data. 
 
        compute_output_weights : boolean, optional, default True 
            Whether to compute new output weights (coef_, intercept_). Disable this in intermediate `partial_fit` 
            steps to run computations faster, then enable in the last call to compute the new solution. 
 
            .. Note:: 
                Solution can be updated without extra data by setting `X=None` and `y=None`. 
        &quot;&quot;&quot;</span>

        <span class="s0">if </span><span class="s1">self.alpha &lt; </span><span class="s3">0</span><span class="s1">:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;Regularization parameter alpha must be non-negative.&quot;</span><span class="s1">)</span>

        <span class="s5"># solution only</span>
        <span class="s0">if </span><span class="s1">X </span><span class="s0">is None and </span><span class="s1">y </span><span class="s0">is None and </span><span class="s1">compute_output_weights:</span>
            <span class="s1">self._solve()</span>
            <span class="s0">return </span><span class="s1">self</span>

        <span class="s5"># validate parameters</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y = check_X_y(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">accept_sparse=</span><span class="s0">True, </span><span class="s1">multi_output=</span><span class="s0">True, </span><span class="s1">y_numeric=</span><span class="s0">True, </span><span class="s1">ensure_2d=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">len(y.shape) &gt; </span><span class="s3">1 </span><span class="s0">and </span><span class="s1">y.shape[</span><span class="s3">1</span><span class="s1">] == </span><span class="s3">1</span><span class="s1">:</span>
            <span class="s1">msg = </span><span class="s2">&quot;A column-vector y was passed when a 1d array was expected.</span><span class="s0">\ 
                   </span><span class="s2">Please change the shape of y to (n_samples, ), for example using ravel().&quot;</span>
            <span class="s1">warnings.warn(msg</span><span class="s0">, </span><span class="s1">DataConversionWarning)</span>

        <span class="s5"># init temporary data storage</span>
        <span class="s0">if not </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s2">'_XtX'</span><span class="s1">):</span>
            <span class="s1">self._init_XY(X</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">X.shape[</span><span class="s3">1</span><span class="s1">] + </span><span class="s3">1 </span><span class="s1">!= self._XtX.shape[</span><span class="s3">0</span><span class="s1">]:</span>
                <span class="s1">n_new</span><span class="s0">, </span><span class="s1">n_old = X.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">self._XtX.shape[</span><span class="s3">0</span><span class="s1">] - </span><span class="s3">1</span>
                <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;Number of features %d does not match previous data %d.&quot; </span><span class="s1">% (n_new</span><span class="s0">, </span><span class="s1">n_old))</span>

        <span class="s5"># compute temporary data</span>
        <span class="s1">X_sum = safe_sparse_dot(X.T</span><span class="s0">, </span><span class="s1">np.ones((X.shape[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span><span class="s1">)))</span>
        <span class="s1">y_sum = safe_sparse_dot(y.T</span><span class="s0">, </span><span class="s1">np.ones((y.shape[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span><span class="s1">)))</span>

        <span class="s0">if not </span><span class="s1">forget:</span>
            <span class="s1">self._XtX[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">] += X.shape[</span><span class="s3">0</span><span class="s1">]</span>
            <span class="s1">self._XtX[</span><span class="s3">1</span><span class="s1">:</span><span class="s0">, </span><span class="s3">0</span><span class="s1">] += X_sum</span>
            <span class="s1">self._XtX[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">:] += X_sum</span>
            <span class="s1">self._XtX[</span><span class="s3">1</span><span class="s1">:</span><span class="s0">, </span><span class="s3">1</span><span class="s1">:] += X.T @ X</span>

            <span class="s1">self._XtY[</span><span class="s3">0</span><span class="s1">] += y_sum</span>
            <span class="s1">self._XtY[</span><span class="s3">1</span><span class="s1">:] += X.T @ y</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">print(</span><span class="s2">&quot;!!! forgetting&quot;</span><span class="s1">)</span>
            <span class="s1">self._XtX[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">] -= X.shape[</span><span class="s3">0</span><span class="s1">]</span>
            <span class="s1">self._XtX[</span><span class="s3">1</span><span class="s1">:</span><span class="s0">, </span><span class="s3">0</span><span class="s1">] -= X_sum</span>
            <span class="s1">self._XtX[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">:] -= X_sum</span>
            <span class="s1">self._XtX[</span><span class="s3">1</span><span class="s1">:</span><span class="s0">, </span><span class="s3">1</span><span class="s1">:] -= X.T @ X</span>

            <span class="s1">self._XtY[</span><span class="s3">0</span><span class="s1">] -= y_sum</span>
            <span class="s1">self._XtY[</span><span class="s3">1</span><span class="s1">:] -= X.T @ y</span>

        <span class="s5"># solve</span>
        <span class="s0">if not </span><span class="s1">compute_output_weights:</span>
            <span class="s5"># mark as not fitted</span>
            <span class="s1">[delattr(self</span><span class="s0">, </span><span class="s1">attr) </span><span class="s0">for </span><span class="s1">attr </span><span class="s0">in </span><span class="s1">(</span><span class="s2">'coef_'</span><span class="s0">, </span><span class="s2">'intercept_'</span><span class="s0">, </span><span class="s2">'is_fitted_'</span><span class="s1">) </span><span class="s0">if </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s1">attr)]</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">self._solve()</span>
        <span class="s0">return </span><span class="s1">self</span>

    <span class="s0">def </span><span class="s1">predict(self</span><span class="s0">, </span><span class="s1">X):</span>
        <span class="s1">check_is_fitted(self</span><span class="s0">, </span><span class="s2">'is_fitted_'</span><span class="s1">)</span>
        <span class="s1">X = check_array(X</span><span class="s0">, </span><span class="s1">accept_sparse=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">safe_sparse_dot(X</span><span class="s0">, </span><span class="s1">self.coef_</span><span class="s0">, </span><span class="s1">dense_output=</span><span class="s0">True</span><span class="s1">) + self.intercept_</span>
</pre>
</body>
</html>