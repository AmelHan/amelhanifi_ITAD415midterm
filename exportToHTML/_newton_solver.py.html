<html>
<head>
<title>_newton_solver.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_newton_solver.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Newton solver for Generalized Linear Models 
&quot;&quot;&quot;</span>

<span class="s2"># Author: Christian Lorentzen &lt;lorentzen.ch@gmail.com&gt;</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">abc </span><span class="s3">import </span><span class="s1">ABC</span><span class="s3">, </span><span class="s1">abstractmethod</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">scipy.linalg</span>
<span class="s3">import </span><span class="s1">scipy.optimize</span>

<span class="s3">from </span><span class="s1">..._loss.loss </span><span class="s3">import </span><span class="s1">HalfSquaredError</span>
<span class="s3">from </span><span class="s1">...exceptions </span><span class="s3">import </span><span class="s1">ConvergenceWarning</span>
<span class="s3">from </span><span class="s1">...utils.optimize </span><span class="s3">import </span><span class="s1">_check_optimize_result</span>
<span class="s3">from </span><span class="s1">.._linear_loss </span><span class="s3">import </span><span class="s1">LinearModelLoss</span>


<span class="s3">class </span><span class="s1">NewtonSolver(ABC):</span>
    <span class="s0">&quot;&quot;&quot;Newton solver for GLMs. 
 
    This class implements Newton/2nd-order optimization routines for GLMs. Each Newton 
    iteration aims at finding the Newton step which is done by the inner solver. With 
    Hessian H, gradient g and coefficients coef, one step solves: 
 
        H @ coef_newton = -g 
 
    For our GLM / LinearModelLoss, we have gradient g and Hessian H: 
 
        g = X.T @ loss.gradient + l2_reg_strength * coef 
        H = X.T @ diag(loss.hessian) @ X + l2_reg_strength * identity 
 
    Backtracking line search updates coef = coef_old + t * coef_newton for some t in 
    (0, 1]. 
 
    This is a base class, actual implementations (child classes) may deviate from the 
    above pattern and use structure specific tricks. 
 
    Usage pattern: 
        - initialize solver: sol = NewtonSolver(...) 
        - solve the problem: sol.solve(X, y, sample_weight) 
 
    References 
    ---------- 
    - Jorge Nocedal, Stephen J. Wright. (2006) &quot;Numerical Optimization&quot; 
      2nd edition 
      https://doi.org/10.1007/978-0-387-40065-5 
 
    - Stephen P. Boyd, Lieven Vandenberghe. (2004) &quot;Convex Optimization.&quot; 
      Cambridge University Press, 2004. 
      https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf 
 
    Parameters 
    ---------- 
    coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,) 
        Initial coefficients of a linear model. 
        If shape (n_classes * n_dof,), the classes of one feature are contiguous, 
        i.e. one reconstructs the 2d-array via 
        coef.reshape((n_classes, -1), order=&quot;F&quot;). 
 
    linear_loss : LinearModelLoss 
        The loss to be minimized. 
 
    l2_reg_strength : float, default=0.0 
        L2 regularization strength. 
 
    tol : float, default=1e-4 
        The optimization problem is solved when each of the following condition is 
        fulfilled: 
        1. maximum |gradient| &lt;= tol 
        2. Newton decrement d: 1/2 * d^2 &lt;= tol 
 
    max_iter : int, default=100 
        Maximum number of Newton steps allowed. 
 
    n_threads : int, default=1 
        Number of OpenMP threads to use for the computation of the Hessian and gradient 
        of the loss function. 
 
    Attributes 
    ---------- 
    coef_old : ndarray of shape coef.shape 
        Coefficient of previous iteration. 
 
    coef_newton : ndarray of shape coef.shape 
        Newton step. 
 
    gradient : ndarray of shape coef.shape 
        Gradient of the loss w.r.t. the coefficients. 
 
    gradient_old : ndarray of shape coef.shape 
        Gradient of previous iteration. 
 
    loss_value : float 
        Value of objective function = loss + penalty. 
 
    loss_value_old : float 
        Value of objective function of previous itertion. 
 
    raw_prediction : ndarray of shape (n_samples,) or (n_samples, n_classes) 
 
    converged : bool 
        Indicator for convergence of the solver. 
 
    iteration : int 
        Number of Newton steps, i.e. calls to inner_solve 
 
    use_fallback_lbfgs_solve : bool 
        If set to True, the solver will resort to call LBFGS to finish the optimisation 
        procedure in case of convergence issues. 
 
    gradient_times_newton : float 
        gradient @ coef_newton, set in inner_solve and used by line_search. If the 
        Newton step is a descent direction, this is negative. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">coef</span><span class="s3">,</span>
        <span class="s1">linear_loss=LinearModelLoss(base_loss=HalfSquaredError()</span><span class="s3">, </span><span class="s1">fit_intercept=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">l2_reg_strength=</span><span class="s4">0.0</span><span class="s3">,</span>
        <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s4">100</span><span class="s3">,</span>
        <span class="s1">n_threads=</span><span class="s4">1</span><span class="s3">,</span>
        <span class="s1">verbose=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">self.coef = coef</span>
        <span class="s1">self.linear_loss = linear_loss</span>
        <span class="s1">self.l2_reg_strength = l2_reg_strength</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.n_threads = n_threads</span>
        <span class="s1">self.verbose = verbose</span>

    <span class="s3">def </span><span class="s1">setup(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight):</span>
        <span class="s0">&quot;&quot;&quot;Precomputations 
 
        If None, initializes: 
            - self.coef 
        Sets: 
            - self.raw_prediction 
            - self.loss_value 
        &quot;&quot;&quot;</span>
        <span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">self.raw_prediction = self.linear_loss.weight_intercept_raw(self.coef</span><span class="s3">, </span><span class="s1">X)</span>
        <span class="s1">self.loss_value = self.linear_loss.loss(</span>
            <span class="s1">coef=self.coef</span><span class="s3">,</span>
            <span class="s1">X=X</span><span class="s3">,</span>
            <span class="s1">y=y</span><span class="s3">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
            <span class="s1">l2_reg_strength=self.l2_reg_strength</span><span class="s3">,</span>
            <span class="s1">n_threads=self.n_threads</span><span class="s3">,</span>
            <span class="s1">raw_prediction=self.raw_prediction</span><span class="s3">,</span>
        <span class="s1">)</span>

    <span class="s1">@abstractmethod</span>
    <span class="s3">def </span><span class="s1">update_gradient_hessian(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight):</span>
        <span class="s0">&quot;&quot;&quot;Update gradient and Hessian.&quot;&quot;&quot;</span>

    <span class="s1">@abstractmethod</span>
    <span class="s3">def </span><span class="s1">inner_solve(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight):</span>
        <span class="s0">&quot;&quot;&quot;Compute Newton step. 
 
        Sets: 
            - self.coef_newton 
            - self.gradient_times_newton 
        &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">fallback_lbfgs_solve(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight):</span>
        <span class="s0">&quot;&quot;&quot;Fallback solver in case of emergency. 
 
        If a solver detects convergence problems, it may fall back to this methods in 
        the hope to exit with success instead of raising an error. 
 
        Sets: 
            - self.coef 
            - self.converged 
        &quot;&quot;&quot;</span>
        <span class="s1">opt_res = scipy.optimize.minimize(</span>
            <span class="s1">self.linear_loss.loss_gradient</span><span class="s3">,</span>
            <span class="s1">self.coef</span><span class="s3">,</span>
            <span class="s1">method=</span><span class="s5">&quot;L-BFGS-B&quot;</span><span class="s3">,</span>
            <span class="s1">jac=</span><span class="s3">True,</span>
            <span class="s1">options={</span>
                <span class="s5">&quot;maxiter&quot;</span><span class="s1">: self.max_iter</span><span class="s3">,</span>
                <span class="s5">&quot;maxls&quot;</span><span class="s1">: </span><span class="s4">50</span><span class="s3">,  </span><span class="s2"># default is 20</span>
                <span class="s5">&quot;iprint&quot;</span><span class="s1">: self.verbose - </span><span class="s4">1</span><span class="s3">,</span>
                <span class="s5">&quot;gtol&quot;</span><span class="s1">: self.tol</span><span class="s3">,</span>
                <span class="s5">&quot;ftol&quot;</span><span class="s1">: </span><span class="s4">64 </span><span class="s1">* np.finfo(np.float64).eps</span><span class="s3">,</span>
            <span class="s1">}</span><span class="s3">,</span>
            <span class="s1">args=(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">self.l2_reg_strength</span><span class="s3">, </span><span class="s1">self.n_threads)</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">self.n_iter_ = _check_optimize_result(</span><span class="s5">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s1">opt_res)</span>
        <span class="s1">self.coef = opt_res.x</span>
        <span class="s1">self.converged = opt_res.status == </span><span class="s4">0</span>

    <span class="s3">def </span><span class="s1">line_search(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight):</span>
        <span class="s0">&quot;&quot;&quot;Backtracking line search. 
 
        Sets: 
            - self.coef_old 
            - self.coef 
            - self.loss_value_old 
            - self.loss_value 
            - self.gradient_old 
            - self.gradient 
            - self.raw_prediction 
        &quot;&quot;&quot;</span>
        <span class="s2"># line search parameters</span>
        <span class="s1">beta</span><span class="s3">, </span><span class="s1">sigma = </span><span class="s4">0.5</span><span class="s3">, </span><span class="s4">0.00048828125  </span><span class="s2"># 1/2, 1/2**11</span>
        <span class="s1">eps = </span><span class="s4">16 </span><span class="s1">* np.finfo(self.loss_value.dtype).eps</span>
        <span class="s1">t = </span><span class="s4">1  </span><span class="s2"># step size</span>

        <span class="s2"># gradient_times_newton = self.gradient @ self.coef_newton</span>
        <span class="s2"># was computed in inner_solve.</span>
        <span class="s1">armijo_term = sigma * self.gradient_times_newton</span>
        <span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">raw_prediction_newton = self.linear_loss.weight_intercept_raw(</span>
            <span class="s1">self.coef_newton</span><span class="s3">, </span><span class="s1">X</span>
        <span class="s1">)</span>

        <span class="s1">self.coef_old = self.coef</span>
        <span class="s1">self.loss_value_old = self.loss_value</span>
        <span class="s1">self.gradient_old = self.gradient</span>

        <span class="s2"># np.sum(np.abs(self.gradient_old))</span>
        <span class="s1">sum_abs_grad_old = -</span><span class="s4">1</span>

        <span class="s1">is_verbose = self.verbose &gt;= </span><span class="s4">2</span>
        <span class="s3">if </span><span class="s1">is_verbose:</span>
            <span class="s1">print(</span><span class="s5">&quot;  Backtracking Line Search&quot;</span><span class="s1">)</span>
            <span class="s1">print(</span><span class="s5">f&quot;    eps=10 * finfo.eps=</span><span class="s3">{</span><span class="s1">eps</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>

        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(</span><span class="s4">21</span><span class="s1">):  </span><span class="s2"># until and including t = beta**20 ~ 1e-6</span>
            <span class="s1">self.coef = self.coef_old + t * self.coef_newton</span>
            <span class="s1">raw = self.raw_prediction + t * raw_prediction_newton</span>
            <span class="s1">self.loss_value</span><span class="s3">, </span><span class="s1">self.gradient = self.linear_loss.loss_gradient(</span>
                <span class="s1">coef=self.coef</span><span class="s3">,</span>
                <span class="s1">X=X</span><span class="s3">,</span>
                <span class="s1">y=y</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
                <span class="s1">l2_reg_strength=self.l2_reg_strength</span><span class="s3">,</span>
                <span class="s1">n_threads=self.n_threads</span><span class="s3">,</span>
                <span class="s1">raw_prediction=raw</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s2"># Note: If coef_newton is too large, loss_gradient may produce inf values,</span>
            <span class="s2"># potentially accompanied by a RuntimeWarning.</span>
            <span class="s2"># This case will be captured by the Armijo condition.</span>

            <span class="s2"># 1. Check Armijo / sufficient decrease condition.</span>
            <span class="s2"># The smaller (more negative) the better.</span>
            <span class="s1">loss_improvement = self.loss_value - self.loss_value_old</span>
            <span class="s1">check = loss_improvement &lt;= t * armijo_term</span>
            <span class="s3">if </span><span class="s1">is_verbose:</span>
                <span class="s1">print(</span>
                    <span class="s5">f&quot;    line search iteration=</span><span class="s3">{</span><span class="s1">i+</span><span class="s4">1</span><span class="s3">}</span><span class="s5">, step size=</span><span class="s3">{</span><span class="s1">t</span><span class="s3">}\n</span><span class="s5">&quot;</span>
                    <span class="s5">f&quot;      check loss improvement &lt;= armijo term: </span><span class="s3">{</span><span class="s1">loss_improvement</span><span class="s3">} </span><span class="s5">&quot;</span>
                    <span class="s5">f&quot;&lt;= </span><span class="s3">{</span><span class="s1">t * armijo_term</span><span class="s3">} {</span><span class="s1">check</span><span class="s3">}</span><span class="s5">&quot;</span>
                <span class="s1">)</span>
            <span class="s3">if </span><span class="s1">check:</span>
                <span class="s3">break</span>
            <span class="s2"># 2. Deal with relative loss differences around machine precision.</span>
            <span class="s1">tiny_loss = np.abs(self.loss_value_old * eps)</span>
            <span class="s1">check = np.abs(loss_improvement) &lt;= tiny_loss</span>
            <span class="s3">if </span><span class="s1">is_verbose:</span>
                <span class="s1">print(</span>
                    <span class="s5">&quot;      check loss |improvement| &lt;= eps * |loss_old|:&quot;</span>
                    <span class="s5">f&quot; </span><span class="s3">{</span><span class="s1">np.abs(loss_improvement)</span><span class="s3">} </span><span class="s5">&lt;= </span><span class="s3">{</span><span class="s1">tiny_loss</span><span class="s3">} {</span><span class="s1">check</span><span class="s3">}</span><span class="s5">&quot;</span>
                <span class="s1">)</span>
            <span class="s3">if </span><span class="s1">check:</span>
                <span class="s3">if </span><span class="s1">sum_abs_grad_old &lt; </span><span class="s4">0</span><span class="s1">:</span>
                    <span class="s1">sum_abs_grad_old = scipy.linalg.norm(self.gradient_old</span><span class="s3">, </span><span class="s1">ord=</span><span class="s4">1</span><span class="s1">)</span>
                <span class="s2"># 2.1 Check sum of absolute gradients as alternative condition.</span>
                <span class="s1">sum_abs_grad = scipy.linalg.norm(self.gradient</span><span class="s3">, </span><span class="s1">ord=</span><span class="s4">1</span><span class="s1">)</span>
                <span class="s1">check = sum_abs_grad &lt; sum_abs_grad_old</span>
                <span class="s3">if </span><span class="s1">is_verbose:</span>
                    <span class="s1">print(</span>
                        <span class="s5">&quot;      check sum(|gradient|) &lt; sum(|gradient_old|): &quot;</span>
                        <span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">sum_abs_grad</span><span class="s3">} </span><span class="s5">&lt; </span><span class="s3">{</span><span class="s1">sum_abs_grad_old</span><span class="s3">} {</span><span class="s1">check</span><span class="s3">}</span><span class="s5">&quot;</span>
                    <span class="s1">)</span>
                <span class="s3">if </span><span class="s1">check:</span>
                    <span class="s3">break</span>

            <span class="s1">t *= beta</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s5">f&quot;Line search of Newton solver </span><span class="s3">{</span><span class="s1">self.__class__.__name__</span><span class="s3">} </span><span class="s5">at&quot;</span>
                    <span class="s5">f&quot; iteration #</span><span class="s3">{</span><span class="s1">self.iteration</span><span class="s3">} </span><span class="s5">did no converge after 21 line search&quot;</span>
                    <span class="s5">&quot; refinement iterations. It will now resort to lbfgs instead.&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">ConvergenceWarning</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">print(</span><span class="s5">&quot;  Line search did not converge and resorts to lbfgs instead.&quot;</span><span class="s1">)</span>
            <span class="s1">self.use_fallback_lbfgs_solve = </span><span class="s3">True</span>
            <span class="s3">return</span>

        <span class="s1">self.raw_prediction = raw</span>

    <span class="s3">def </span><span class="s1">check_convergence(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight):</span>
        <span class="s0">&quot;&quot;&quot;Check for convergence. 
 
        Sets self.converged. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self.verbose:</span>
            <span class="s1">print(</span><span class="s5">&quot;  Check Convergence&quot;</span><span class="s1">)</span>
        <span class="s2"># Note: Checking maximum relative change of coefficient &lt;= tol is a bad</span>
        <span class="s2"># convergence criterion because even a large step could have brought us close</span>
        <span class="s2"># to the true minimum.</span>
        <span class="s2"># coef_step = self.coef - self.coef_old</span>
        <span class="s2"># check = np.max(np.abs(coef_step) / np.maximum(1, np.abs(self.coef_old)))</span>

        <span class="s2"># 1. Criterion: maximum |gradient| &lt;= tol</span>
        <span class="s2">#    The gradient was already updated in line_search()</span>
        <span class="s1">check = np.max(np.abs(self.gradient))</span>
        <span class="s3">if </span><span class="s1">self.verbose:</span>
            <span class="s1">print(</span><span class="s5">f&quot;    1. max |gradient| </span><span class="s3">{</span><span class="s1">check</span><span class="s3">} </span><span class="s5">&lt;= </span><span class="s3">{</span><span class="s1">self.tol</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">check &gt; self.tol:</span>
            <span class="s3">return</span>

        <span class="s2"># 2. Criterion: For Newton decrement d, check 1/2 * d^2 &lt;= tol</span>
        <span class="s2">#       d = sqrt(grad @ hessian^-1 @ grad)</span>
        <span class="s2">#         = sqrt(coef_newton @ hessian @ coef_newton)</span>
        <span class="s2">#    See Boyd, Vanderberghe (2009) &quot;Convex Optimization&quot; Chapter 9.5.1.</span>
        <span class="s1">d2 = self.coef_newton @ self.hessian @ self.coef_newton</span>
        <span class="s3">if </span><span class="s1">self.verbose:</span>
            <span class="s1">print(</span><span class="s5">f&quot;    2. Newton decrement </span><span class="s3">{</span><span class="s4">0.5 </span><span class="s1">* d2</span><span class="s3">} </span><span class="s5">&lt;= </span><span class="s3">{</span><span class="s1">self.tol</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s4">0.5 </span><span class="s1">* d2 &gt; self.tol:</span>
            <span class="s3">return</span>

        <span class="s3">if </span><span class="s1">self.verbose:</span>
            <span class="s1">loss_value = self.linear_loss.loss(</span>
                <span class="s1">coef=self.coef</span><span class="s3">,</span>
                <span class="s1">X=X</span><span class="s3">,</span>
                <span class="s1">y=y</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
                <span class="s1">l2_reg_strength=self.l2_reg_strength</span><span class="s3">,</span>
                <span class="s1">n_threads=self.n_threads</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">print(</span><span class="s5">f&quot;  Solver did converge at loss = </span><span class="s3">{</span><span class="s1">loss_value</span><span class="s3">}</span><span class="s5">.&quot;</span><span class="s1">)</span>
        <span class="s1">self.converged = </span><span class="s3">True</span>

    <span class="s3">def </span><span class="s1">finalize(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight):</span>
        <span class="s0">&quot;&quot;&quot;Finalize the solvers results. 
 
        Some solvers may need this, others not. 
        &quot;&quot;&quot;</span>
        <span class="s3">pass</span>

    <span class="s3">def </span><span class="s1">solve(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight):</span>
        <span class="s0">&quot;&quot;&quot;Solve the optimization problem. 
 
        This is the main routine. 
 
        Order of calls: 
            self.setup() 
            while iteration: 
                self.update_gradient_hessian() 
                self.inner_solve() 
                self.line_search() 
                self.check_convergence() 
            self.finalize() 
 
        Returns 
        ------- 
        coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,) 
            Solution of the optimization problem. 
        &quot;&quot;&quot;</span>
        <span class="s2"># setup usually:</span>
        <span class="s2">#   - initializes self.coef if needed</span>
        <span class="s2">#   - initializes and calculates self.raw_predictions, self.loss_value</span>
        <span class="s1">self.setup(X=X</span><span class="s3">, </span><span class="s1">y=y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>

        <span class="s1">self.iteration = </span><span class="s4">1</span>
        <span class="s1">self.converged = </span><span class="s3">False</span>
        <span class="s1">self.use_fallback_lbfgs_solve = </span><span class="s3">False</span>

        <span class="s3">while </span><span class="s1">self.iteration &lt;= self.max_iter </span><span class="s3">and not </span><span class="s1">self.converged:</span>
            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">print(</span><span class="s5">f&quot;Newton iter=</span><span class="s3">{</span><span class="s1">self.iteration</span><span class="s3">}</span><span class="s5">&quot;</span><span class="s1">)</span>

            <span class="s1">self.use_fallback_lbfgs_solve = </span><span class="s3">False  </span><span class="s2"># Fallback solver.</span>

            <span class="s2"># 1. Update Hessian and gradient</span>
            <span class="s1">self.update_gradient_hessian(X=X</span><span class="s3">, </span><span class="s1">y=y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>

            <span class="s2"># TODO:</span>
            <span class="s2"># if iteration == 1:</span>
            <span class="s2"># We might stop early, e.g. we already are close to the optimum,</span>
            <span class="s2"># usually detected by zero gradients at this stage.</span>

            <span class="s2"># 2. Inner solver</span>
            <span class="s2">#    Calculate Newton step/direction</span>
            <span class="s2">#    This usually sets self.coef_newton and self.gradient_times_newton.</span>
            <span class="s1">self.inner_solve(X=X</span><span class="s3">, </span><span class="s1">y=y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>
            <span class="s3">if </span><span class="s1">self.use_fallback_lbfgs_solve:</span>
                <span class="s3">break</span>

            <span class="s2"># 3. Backtracking line search</span>
            <span class="s2">#    This usually sets self.coef_old, self.coef, self.loss_value_old</span>
            <span class="s2">#    self.loss_value, self.gradient_old, self.gradient,</span>
            <span class="s2">#    self.raw_prediction.</span>
            <span class="s1">self.line_search(X=X</span><span class="s3">, </span><span class="s1">y=y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>
            <span class="s3">if </span><span class="s1">self.use_fallback_lbfgs_solve:</span>
                <span class="s3">break</span>

            <span class="s2"># 4. Check convergence</span>
            <span class="s2">#    Sets self.converged.</span>
            <span class="s1">self.check_convergence(X=X</span><span class="s3">, </span><span class="s1">y=y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>

            <span class="s2"># 5. Next iteration</span>
            <span class="s1">self.iteration += </span><span class="s4">1</span>

        <span class="s3">if not </span><span class="s1">self.converged:</span>
            <span class="s3">if </span><span class="s1">self.use_fallback_lbfgs_solve:</span>
                <span class="s2"># Note: The fallback solver circumvents check_convergence and relies on</span>
                <span class="s2"># the convergence checks of lbfgs instead. Enough warnings have been</span>
                <span class="s2"># raised on the way.</span>
                <span class="s1">self.fallback_lbfgs_solve(X=X</span><span class="s3">, </span><span class="s1">y=y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">warnings.warn(</span>
                    <span class="s1">(</span>
                        <span class="s5">f&quot;Newton solver did not converge after </span><span class="s3">{</span><span class="s1">self.iteration - </span><span class="s4">1</span><span class="s3">} </span><span class="s5">&quot;</span>
                        <span class="s5">&quot;iterations.&quot;</span>
                    <span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">ConvergenceWarning</span><span class="s3">,</span>
                <span class="s1">)</span>

        <span class="s1">self.iteration -= </span><span class="s4">1</span>
        <span class="s1">self.finalize(X=X</span><span class="s3">, </span><span class="s1">y=y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>
        <span class="s3">return </span><span class="s1">self.coef</span>


<span class="s3">class </span><span class="s1">NewtonCholeskySolver(NewtonSolver):</span>
    <span class="s0">&quot;&quot;&quot;Cholesky based Newton solver. 
 
    Inner solver for finding the Newton step H w_newton = -g uses Cholesky based linear 
    solver. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">setup(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight):</span>
        <span class="s1">super().setup(X=X</span><span class="s3">, </span><span class="s1">y=y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>
        <span class="s1">n_dof = X.shape[</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s3">if </span><span class="s1">self.linear_loss.fit_intercept:</span>
            <span class="s1">n_dof += </span><span class="s4">1</span>
        <span class="s1">self.gradient = np.empty_like(self.coef)</span>
        <span class="s1">self.hessian = np.empty_like(self.coef</span><span class="s3">, </span><span class="s1">shape=(n_dof</span><span class="s3">, </span><span class="s1">n_dof))</span>

    <span class="s3">def </span><span class="s1">update_gradient_hessian(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight):</span>
        <span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">self.hessian_warning = self.linear_loss.gradient_hessian(</span>
            <span class="s1">coef=self.coef</span><span class="s3">,</span>
            <span class="s1">X=X</span><span class="s3">,</span>
            <span class="s1">y=y</span><span class="s3">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
            <span class="s1">l2_reg_strength=self.l2_reg_strength</span><span class="s3">,</span>
            <span class="s1">n_threads=self.n_threads</span><span class="s3">,</span>
            <span class="s1">gradient_out=self.gradient</span><span class="s3">,</span>
            <span class="s1">hessian_out=self.hessian</span><span class="s3">,</span>
            <span class="s1">raw_prediction=self.raw_prediction</span><span class="s3">,  </span><span class="s2"># this was updated in line_search</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">inner_solve(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight):</span>
        <span class="s3">if </span><span class="s1">self.hessian_warning:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s5">f&quot;The inner solver of </span><span class="s3">{</span><span class="s1">self.__class__.__name__</span><span class="s3">} </span><span class="s5">detected a &quot;</span>
                    <span class="s5">&quot;pointwise hessian with many negative values at iteration &quot;</span>
                    <span class="s5">f&quot;#</span><span class="s3">{</span><span class="s1">self.iteration</span><span class="s3">}</span><span class="s5">. It will now resort to lbfgs instead.&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">ConvergenceWarning</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">print(</span>
                    <span class="s5">&quot;  The inner solver detected a pointwise Hessian with many &quot;</span>
                    <span class="s5">&quot;negative values and resorts to lbfgs instead.&quot;</span>
                <span class="s1">)</span>
            <span class="s1">self.use_fallback_lbfgs_solve = </span><span class="s3">True</span>
            <span class="s3">return</span>

        <span class="s3">try</span><span class="s1">:</span>
            <span class="s3">with </span><span class="s1">warnings.catch_warnings():</span>
                <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;error&quot;</span><span class="s3">, </span><span class="s1">scipy.linalg.LinAlgWarning)</span>
                <span class="s1">self.coef_newton = scipy.linalg.solve(</span>
                    <span class="s1">self.hessian</span><span class="s3">, </span><span class="s1">-self.gradient</span><span class="s3">, </span><span class="s1">check_finite=</span><span class="s3">False, </span><span class="s1">assume_a=</span><span class="s5">&quot;sym&quot;</span>
                <span class="s1">)</span>
                <span class="s1">self.gradient_times_newton = self.gradient @ self.coef_newton</span>
                <span class="s3">if </span><span class="s1">self.gradient_times_newton &gt; </span><span class="s4">0</span><span class="s1">:</span>
                    <span class="s3">if </span><span class="s1">self.verbose:</span>
                        <span class="s1">print(</span>
                            <span class="s5">&quot;  The inner solver found a Newton step that is not a &quot;</span>
                            <span class="s5">&quot;descent direction and resorts to LBFGS steps instead.&quot;</span>
                        <span class="s1">)</span>
                    <span class="s1">self.use_fallback_lbfgs_solve = </span><span class="s3">True</span>
                    <span class="s3">return</span>
        <span class="s3">except </span><span class="s1">(np.linalg.LinAlgError</span><span class="s3">, </span><span class="s1">scipy.linalg.LinAlgWarning) </span><span class="s3">as </span><span class="s1">e:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s5">f&quot;The inner solver of </span><span class="s3">{</span><span class="s1">self.__class__.__name__</span><span class="s3">} </span><span class="s5">stumbled upon a &quot;</span>
                <span class="s5">&quot;singular or very ill-conditioned Hessian matrix at iteration &quot;</span>
                <span class="s5">f&quot;#</span><span class="s3">{</span><span class="s1">self.iteration</span><span class="s3">}</span><span class="s5">. It will now resort to lbfgs instead.</span><span class="s3">\n</span><span class="s5">&quot;</span>
                <span class="s5">&quot;Further options are to use another solver or to avoid such situation &quot;</span>
                <span class="s5">&quot;in the first place. Possible remedies are removing collinear features&quot;</span>
                <span class="s5">&quot; of X or increasing the penalization strengths.</span><span class="s3">\n</span><span class="s5">&quot;</span>
                <span class="s5">&quot;The original Linear Algebra message was:</span><span class="s3">\n</span><span class="s5">&quot;</span>
                <span class="s1">+ str(e)</span><span class="s3">,</span>
                <span class="s1">scipy.linalg.LinAlgWarning</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s2"># Possible causes:</span>
            <span class="s2"># 1. hess_pointwise is negative. But this is already taken care in</span>
            <span class="s2">#    LinearModelLoss.gradient_hessian.</span>
            <span class="s2"># 2. X is singular or ill-conditioned</span>
            <span class="s2">#    This might be the most probable cause.</span>
            <span class="s2">#</span>
            <span class="s2"># There are many possible ways to deal with this situation. Most of them</span>
            <span class="s2"># add, explicitly or implicitly, a matrix to the hessian to make it</span>
            <span class="s2"># positive definite, confer to Chapter 3.4 of Nocedal &amp; Wright 2nd ed.</span>
            <span class="s2"># Instead, we resort to lbfgs.</span>
            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">print(</span>
                    <span class="s5">&quot;  The inner solver stumbled upon an singular or ill-conditioned &quot;</span>
                    <span class="s5">&quot;Hessian matrix and resorts to LBFGS instead.&quot;</span>
                <span class="s1">)</span>
            <span class="s1">self.use_fallback_lbfgs_solve = </span><span class="s3">True</span>
            <span class="s3">return</span>
</pre>
</body>
</html>