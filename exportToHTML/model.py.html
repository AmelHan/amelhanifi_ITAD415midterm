<html>
<head>
<title>model.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
model.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">__future__ </span><span class="s0">import </span><span class="s1">annotations</span>

<span class="s0">from </span><span class="s1">statsmodels.compat.python </span><span class="s0">import </span><span class="s1">lzip</span>

<span class="s0">from </span><span class="s1">functools </span><span class="s0">import </span><span class="s1">reduce</span>
<span class="s0">import </span><span class="s1">warnings</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pandas </span><span class="s0">as </span><span class="s1">pd</span>
<span class="s0">from </span><span class="s1">scipy </span><span class="s0">import </span><span class="s1">stats</span>

<span class="s0">from </span><span class="s1">statsmodels.base.data </span><span class="s0">import </span><span class="s1">handle_data</span>
<span class="s0">from </span><span class="s1">statsmodels.base.optimizer </span><span class="s0">import </span><span class="s1">Optimizer</span>
<span class="s0">import </span><span class="s1">statsmodels.base.wrapper </span><span class="s0">as </span><span class="s1">wrap</span>
<span class="s0">from </span><span class="s1">statsmodels.formula </span><span class="s0">import </span><span class="s1">handle_formula_data</span>
<span class="s0">from </span><span class="s1">statsmodels.stats.contrast </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">ContrastResults</span><span class="s0">,</span>
    <span class="s1">WaldTestResults</span><span class="s0">,</span>
    <span class="s1">t_test_pairwise</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">statsmodels.tools.data </span><span class="s0">import </span><span class="s1">_is_using_pandas</span>
<span class="s0">from </span><span class="s1">statsmodels.tools.decorators </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">cache_readonly</span><span class="s0">,</span>
    <span class="s1">cached_data</span><span class="s0">,</span>
    <span class="s1">cached_value</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">statsmodels.tools.numdiff </span><span class="s0">import </span><span class="s1">approx_fprime</span>
<span class="s0">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">HessianInversionWarning</span><span class="s0">,</span>
    <span class="s1">ValueWarning</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">statsmodels.tools.tools </span><span class="s0">import </span><span class="s1">nan_dot</span><span class="s0">, </span><span class="s1">recipr</span>
<span class="s0">from </span><span class="s1">statsmodels.tools.validation </span><span class="s0">import </span><span class="s1">bool_like</span>

<span class="s1">ERROR_INIT_KWARGS = </span><span class="s0">False</span>

<span class="s1">_model_params_doc = </span><span class="s2">&quot;&quot;&quot;Parameters 
    ---------- 
    endog : array_like 
        A 1-d endogenous response variable. The dependent variable. 
    exog : array_like 
        A nobs x k array where `nobs` is the number of observations and `k` 
        is the number of regressors. An intercept is not included by default 
        and should be added by the user. See 
        :func:`statsmodels.tools.add_constant`.&quot;&quot;&quot;</span>

<span class="s1">_missing_param_doc = </span><span class="s2">&quot;&quot;&quot;</span><span class="s0">\ 
</span><span class="s2">missing : str 
        Available options are 'none', 'drop', and 'raise'. If 'none', no nan 
        checking is done. If 'drop', any observations with nans are dropped. 
        If 'raise', an error is raised. Default is 'none'.&quot;&quot;&quot;</span>
<span class="s1">_extra_param_doc = </span><span class="s2">&quot;&quot;&quot; 
    hasconst : None or bool 
        Indicates whether the RHS includes a user-supplied constant. If True, 
        a constant is not checked for and k_constant is set to 1 and all 
        result statistics are calculated as if a constant is present. If 
        False, a constant is not checked for and k_constant is set to 0. 
    **kwargs 
        Extra arguments that are used to set model properties when using the 
        formula interface.&quot;&quot;&quot;</span>


<span class="s0">class </span><span class="s1">Model:</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    A (predictive) statistical model. Intended to be subclassed not used. 
 
    %(params_doc)s 
    %(extra_params_doc)s 
 
    Attributes 
    ---------- 
    exog_names 
    endog_names 
 
    Notes 
    ----- 
    `endog` and `exog` are references to any data provided.  So if the data is 
    already stored in numpy arrays and it is changed then `endog` and `exog` 
    will change as well. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params_doc'</span><span class="s1">: _model_params_doc</span><span class="s0">,</span>
           <span class="s2">'extra_params_doc'</span><span class="s1">: _missing_param_doc + _extra_param_doc}</span>

    <span class="s3"># Maximum number of endogenous variables when using a formula</span>
    <span class="s3"># Default is 1, which is more common. Override in models when needed</span>
    <span class="s3"># Set to None to skip check</span>
    <span class="s1">_formula_max_endog = </span><span class="s4">1</span>
    <span class="s3"># kwargs that are generically allowed, maybe not supported in all models</span>
    <span class="s1">_kwargs_allowed = [</span>
        <span class="s2">&quot;missing&quot;</span><span class="s0">, </span><span class="s2">'missing_idx'</span><span class="s0">, </span><span class="s2">'formula'</span><span class="s0">, </span><span class="s2">'design_info'</span><span class="s0">, </span><span class="s2">&quot;hasconst&quot;</span><span class="s0">,</span>
        <span class="s1">]</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog=</span><span class="s0">None, </span><span class="s1">**kwargs):</span>
        <span class="s1">missing = kwargs.pop(</span><span class="s2">'missing'</span><span class="s0">, </span><span class="s2">'none'</span><span class="s1">)</span>
        <span class="s1">hasconst = kwargs.pop(</span><span class="s2">'hasconst'</span><span class="s0">, None</span><span class="s1">)</span>
        <span class="s1">self.data = self._handle_data(endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">missing</span><span class="s0">, </span><span class="s1">hasconst</span><span class="s0">,</span>
                                      <span class="s1">**kwargs)</span>
        <span class="s1">self.k_constant = self.data.k_constant</span>
        <span class="s1">self.exog = self.data.exog</span>
        <span class="s1">self.endog = self.data.endog</span>
        <span class="s1">self._data_attr = []</span>
        <span class="s1">self._data_attr.extend([</span><span class="s2">'exog'</span><span class="s0">, </span><span class="s2">'endog'</span><span class="s0">, </span><span class="s2">'data.exog'</span><span class="s0">, </span><span class="s2">'data.endog'</span><span class="s1">])</span>
        <span class="s0">if </span><span class="s2">'formula' </span><span class="s0">not in </span><span class="s1">kwargs:  </span><span class="s3"># will not be able to unpickle without these</span>
            <span class="s1">self._data_attr.extend([</span><span class="s2">'data.orig_endog'</span><span class="s0">, </span><span class="s2">'data.orig_exog'</span><span class="s1">])</span>
        <span class="s3"># store keys for extras if we need to recreate model instance</span>
        <span class="s3"># we do not need 'missing', maybe we need 'hasconst'</span>
        <span class="s1">self._init_keys = list(kwargs.keys())</span>
        <span class="s0">if </span><span class="s1">hasconst </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">self._init_keys.append(</span><span class="s2">'hasconst'</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">_get_init_kwds(self):</span>
        <span class="s5">&quot;&quot;&quot;return dictionary with extra keys used in model.__init__ 
        &quot;&quot;&quot;</span>
        <span class="s1">kwds = dict(((key</span><span class="s0">, </span><span class="s1">getattr(self</span><span class="s0">, </span><span class="s1">key</span><span class="s0">, None</span><span class="s1">))</span>
                     <span class="s0">for </span><span class="s1">key </span><span class="s0">in </span><span class="s1">self._init_keys))</span>

        <span class="s0">return </span><span class="s1">kwds</span>

    <span class="s0">def </span><span class="s1">_check_kwargs(self</span><span class="s0">, </span><span class="s1">kwargs</span><span class="s0">, </span><span class="s1">keys_extra=</span><span class="s0">None, </span><span class="s1">error=ERROR_INIT_KWARGS):</span>

        <span class="s1">kwargs_allowed = [</span>
            <span class="s2">&quot;missing&quot;</span><span class="s0">, </span><span class="s2">'missing_idx'</span><span class="s0">, </span><span class="s2">'formula'</span><span class="s0">, </span><span class="s2">'design_info'</span><span class="s0">, </span><span class="s2">&quot;hasconst&quot;</span><span class="s0">,</span>
            <span class="s1">]</span>
        <span class="s0">if </span><span class="s1">keys_extra:</span>
            <span class="s1">kwargs_allowed.extend(keys_extra)</span>

        <span class="s1">kwargs_invalid = [i </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">kwargs </span><span class="s0">if </span><span class="s1">i </span><span class="s0">not in </span><span class="s1">kwargs_allowed]</span>
        <span class="s0">if </span><span class="s1">kwargs_invalid:</span>
            <span class="s1">msg = </span><span class="s2">&quot;unknown kwargs &quot; </span><span class="s1">+ repr(kwargs_invalid)</span>
            <span class="s0">if </span><span class="s1">error </span><span class="s0">is False</span><span class="s1">:</span>
                <span class="s1">warnings.warn(msg</span><span class="s0">, </span><span class="s1">ValueWarning)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">raise </span><span class="s1">ValueError(msg)</span>

    <span class="s0">def </span><span class="s1">_handle_data(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">missing</span><span class="s0">, </span><span class="s1">hasconst</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s1">data = handle_data(endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">missing</span><span class="s0">, </span><span class="s1">hasconst</span><span class="s0">, </span><span class="s1">**kwargs)</span>
        <span class="s3"># kwargs arrays could have changed, easier to just attach here</span>
        <span class="s0">for </span><span class="s1">key </span><span class="s0">in </span><span class="s1">kwargs:</span>
            <span class="s0">if </span><span class="s1">key </span><span class="s0">in </span><span class="s1">[</span><span class="s2">'design_info'</span><span class="s0">, </span><span class="s2">'formula'</span><span class="s1">]:  </span><span class="s3"># leave attached to data</span>
                <span class="s0">continue</span>
            <span class="s3"># pop so we do not start keeping all these twice or references</span>
            <span class="s0">try</span><span class="s1">:</span>
                <span class="s1">setattr(self</span><span class="s0">, </span><span class="s1">key</span><span class="s0">, </span><span class="s1">data.__dict__.pop(key))</span>
            <span class="s0">except </span><span class="s1">KeyError:  </span><span class="s3"># panel already pops keys in data handling</span>
                <span class="s0">pass</span>
        <span class="s0">return </span><span class="s1">data</span>

    <span class="s1">@classmethod</span>
    <span class="s0">def </span><span class="s1">from_formula(cls</span><span class="s0">, </span><span class="s1">formula</span><span class="s0">, </span><span class="s1">data</span><span class="s0">, </span><span class="s1">subset=</span><span class="s0">None, </span><span class="s1">drop_cols=</span><span class="s0">None,</span>
                     <span class="s1">*args</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s5">&quot;&quot;&quot; 
        Create a Model from a formula and dataframe. 
 
        Parameters 
        ---------- 
        formula : str or generic Formula object 
            The formula specifying the model. 
        data : array_like 
            The data for the model. See Notes. 
        subset : array_like 
            An array-like object of booleans, integers, or index values that 
            indicate the subset of df to use in the model. Assumes df is a 
            `pandas.DataFrame`. 
        drop_cols : array_like 
            Columns to drop from the design matrix.  Cannot be used to 
            drop terms involving categoricals. 
        *args 
            Additional positional argument that are passed to the model. 
        **kwargs 
            These are passed to the model with one exception. The 
            ``eval_env`` keyword is passed to patsy. It can be either a 
            :class:`patsy:patsy.EvalEnvironment` object or an integer 
            indicating the depth of the namespace to use. For example, the 
            default ``eval_env=0`` uses the calling namespace. If you wish 
            to use a &quot;clean&quot; environment set ``eval_env=-1``. 
 
        Returns 
        ------- 
        model 
            The model instance. 
 
        Notes 
        ----- 
        data must define __getitem__ with the keys in the formula terms 
        args and kwargs are passed on to the model instantiation. E.g., 
        a numpy structured or rec array, a dictionary, or a pandas DataFrame. 
        &quot;&quot;&quot;</span>
        <span class="s3"># TODO: provide a docs template for args/kwargs from child models</span>
        <span class="s3"># TODO: subset could use syntax. issue #469.</span>
        <span class="s0">if </span><span class="s1">subset </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">data = data.loc[subset]</span>
        <span class="s1">eval_env = kwargs.pop(</span><span class="s2">'eval_env'</span><span class="s0">, None</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">eval_env </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">eval_env = </span><span class="s4">2</span>
        <span class="s0">elif </span><span class="s1">eval_env == -</span><span class="s4">1</span><span class="s1">:</span>
            <span class="s0">from </span><span class="s1">patsy </span><span class="s0">import </span><span class="s1">EvalEnvironment</span>
            <span class="s1">eval_env = EvalEnvironment({})</span>
        <span class="s0">elif </span><span class="s1">isinstance(eval_env</span><span class="s0">, </span><span class="s1">int):</span>
            <span class="s1">eval_env += </span><span class="s4">1  </span><span class="s3"># we're going down the stack again</span>
        <span class="s1">missing = kwargs.get(</span><span class="s2">'missing'</span><span class="s0">, </span><span class="s2">'drop'</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">missing == </span><span class="s2">'none'</span><span class="s1">:  </span><span class="s3"># with patsy it's drop or raise. let's raise.</span>
            <span class="s1">missing = </span><span class="s2">'raise'</span>

        <span class="s1">tmp = handle_formula_data(data</span><span class="s0">, None, </span><span class="s1">formula</span><span class="s0">, </span><span class="s1">depth=eval_env</span><span class="s0">,</span>
                                  <span class="s1">missing=missing)</span>
        <span class="s1">((endog</span><span class="s0">, </span><span class="s1">exog)</span><span class="s0">, </span><span class="s1">missing_idx</span><span class="s0">, </span><span class="s1">design_info) = tmp</span>
        <span class="s1">max_endog = cls._formula_max_endog</span>
        <span class="s0">if </span><span class="s1">(max_endog </span><span class="s0">is not None and</span>
                <span class="s1">endog.ndim &gt; </span><span class="s4">1 </span><span class="s0">and </span><span class="s1">endog.shape[</span><span class="s4">1</span><span class="s1">] &gt; max_endog):</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">'endog has evaluated to an array with multiple '</span>
                             <span class="s2">'columns that has shape {0}. This occurs when '</span>
                             <span class="s2">'the variable converted to endog is non-numeric'</span>
                             <span class="s2">' (e.g., bool or str).'</span><span class="s1">.format(endog.shape))</span>
        <span class="s0">if </span><span class="s1">drop_cols </span><span class="s0">is not None and </span><span class="s1">len(drop_cols) &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">cols = [x </span><span class="s0">for </span><span class="s1">x </span><span class="s0">in </span><span class="s1">exog.columns </span><span class="s0">if </span><span class="s1">x </span><span class="s0">not in </span><span class="s1">drop_cols]</span>
            <span class="s0">if </span><span class="s1">len(cols) &lt; len(exog.columns):</span>
                <span class="s1">exog = exog[cols]</span>
                <span class="s1">cols = list(design_info.term_names)</span>
                <span class="s0">for </span><span class="s1">col </span><span class="s0">in </span><span class="s1">drop_cols:</span>
                    <span class="s0">try</span><span class="s1">:</span>
                        <span class="s1">cols.remove(col)</span>
                    <span class="s0">except </span><span class="s1">ValueError:</span>
                        <span class="s0">pass  </span><span class="s3"># OK if not present</span>
                <span class="s1">design_info = design_info.subset(cols)</span>

        <span class="s1">kwargs.update({</span><span class="s2">'missing_idx'</span><span class="s1">: missing_idx</span><span class="s0">,</span>
                       <span class="s2">'missing'</span><span class="s1">: missing</span><span class="s0">,</span>
                       <span class="s2">'formula'</span><span class="s1">: formula</span><span class="s0">,  </span><span class="s3"># attach formula for unpckling</span>
                       <span class="s2">'design_info'</span><span class="s1">: design_info})</span>
        <span class="s1">mod = cls(endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kwargs)</span>
        <span class="s1">mod.formula = formula</span>
        <span class="s3"># since we got a dataframe, attach the original</span>
        <span class="s1">mod.data.frame = data</span>
        <span class="s0">return </span><span class="s1">mod</span>

    <span class="s1">@property</span>
    <span class="s0">def </span><span class="s1">endog_names(self):</span>
        <span class="s5">&quot;&quot;&quot; 
        Names of endogenous variables. 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">self.data.ynames</span>

    <span class="s1">@property</span>
    <span class="s0">def </span><span class="s1">exog_names(self) -&gt; list[str] | </span><span class="s0">None</span><span class="s1">:</span>
        <span class="s5">&quot;&quot;&quot; 
        Names of exogenous variables. 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">self.data.xnames</span>

    <span class="s0">def </span><span class="s1">fit(self):</span>
        <span class="s5">&quot;&quot;&quot; 
        Fit a model to data. 
        &quot;&quot;&quot;</span>
        <span class="s0">raise </span><span class="s1">NotImplementedError</span>

    <span class="s0">def </span><span class="s1">predict(self</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">exog=</span><span class="s0">None, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s5">&quot;&quot;&quot; 
        After a model has been fit predict returns the fitted values. 
 
        This is a placeholder intended to be overwritten by individual models. 
        &quot;&quot;&quot;</span>
        <span class="s0">raise </span><span class="s1">NotImplementedError</span>


<span class="s0">class </span><span class="s1">LikelihoodModel(Model):</span>
    <span class="s5">&quot;&quot;&quot; 
    Likelihood model is a subclass of Model. 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog=</span><span class="s0">None, </span><span class="s1">**kwargs):</span>
        <span class="s1">super().__init__(endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">**kwargs)</span>
        <span class="s1">self.initialize()</span>

    <span class="s0">def </span><span class="s1">initialize(self):</span>
        <span class="s5">&quot;&quot;&quot; 
        Initialize (possibly re-initialize) a Model instance. 
 
        For example, if the the design matrix of a linear model changes then 
        initialized can be used to recompute values using the modified design 
        matrix. 
        &quot;&quot;&quot;</span>
        <span class="s0">pass</span>

    <span class="s3"># TODO: if the intent is to re-initialize the model with new data then this</span>
    <span class="s3"># method needs to take inputs...</span>

    <span class="s0">def </span><span class="s1">loglike(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Log-likelihood of model. 
 
        Parameters 
        ---------- 
        params : ndarray 
            The model parameters used to compute the log-likelihood. 
 
        Notes 
        ----- 
        Must be overridden by subclasses. 
        &quot;&quot;&quot;</span>
        <span class="s0">raise </span><span class="s1">NotImplementedError</span>

    <span class="s0">def </span><span class="s1">score(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Score vector of model. 
 
        The gradient of logL with respect to each parameter. 
 
        Parameters 
        ---------- 
        params : ndarray 
            The parameters to use when evaluating the Hessian. 
 
        Returns 
        ------- 
        ndarray 
            The score vector evaluated at the parameters. 
        &quot;&quot;&quot;</span>
        <span class="s0">raise </span><span class="s1">NotImplementedError</span>

    <span class="s0">def </span><span class="s1">information(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Fisher information matrix of model. 
 
        Returns -1 * Hessian of the log-likelihood evaluated at params. 
 
        Parameters 
        ---------- 
        params : ndarray 
            The model parameters. 
        &quot;&quot;&quot;</span>
        <span class="s0">raise </span><span class="s1">NotImplementedError</span>

    <span class="s0">def </span><span class="s1">hessian(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        The Hessian matrix of the model. 
 
        Parameters 
        ---------- 
        params : ndarray 
            The parameters to use when evaluating the Hessian. 
 
        Returns 
        ------- 
        ndarray 
            The hessian evaluated at the parameters. 
        &quot;&quot;&quot;</span>
        <span class="s0">raise </span><span class="s1">NotImplementedError</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">start_params=</span><span class="s0">None, </span><span class="s1">method=</span><span class="s2">'newton'</span><span class="s0">, </span><span class="s1">maxiter=</span><span class="s4">100</span><span class="s0">,</span>
            <span class="s1">full_output=</span><span class="s0">True, </span><span class="s1">disp=</span><span class="s0">True, </span><span class="s1">fargs=()</span><span class="s0">, </span><span class="s1">callback=</span><span class="s0">None, </span><span class="s1">retall=</span><span class="s0">False,</span>
            <span class="s1">skip_hessian=</span><span class="s0">False, </span><span class="s1">**kwargs):</span>
        <span class="s5">&quot;&quot;&quot; 
        Fit method for likelihood based models 
 
        Parameters 
        ---------- 
        start_params : array_like, optional 
            Initial guess of the solution for the loglikelihood maximization. 
            The default is an array of zeros. 
        method : str, optional 
            The `method` determines which solver from `scipy.optimize` 
            is used, and it can be chosen from among the following strings: 
 
            - 'newton' for Newton-Raphson, 'nm' for Nelder-Mead 
            - 'bfgs' for Broyden-Fletcher-Goldfarb-Shanno (BFGS) 
            - 'lbfgs' for limited-memory BFGS with optional box constraints 
            - 'powell' for modified Powell's method 
            - 'cg' for conjugate gradient 
            - 'ncg' for Newton-conjugate gradient 
            - 'basinhopping' for global basin-hopping solver 
            - 'minimize' for generic wrapper of scipy minimize (BFGS by default) 
 
            The explicit arguments in `fit` are passed to the solver, 
            with the exception of the basin-hopping solver. Each 
            solver has several optional arguments that are not the same across 
            solvers. See the notes section below (or scipy.optimize) for the 
            available arguments and for the list of explicit arguments that the 
            basin-hopping solver supports. 
        maxiter : int, optional 
            The maximum number of iterations to perform. 
        full_output : bool, optional 
            Set to True to have all available output in the Results object's 
            mle_retvals attribute. The output is dependent on the solver. 
            See LikelihoodModelResults notes section for more information. 
        disp : bool, optional 
            Set to True to print convergence messages. 
        fargs : tuple, optional 
            Extra arguments passed to the likelihood function, i.e., 
            loglike(x,*args) 
        callback : callable callback(xk), optional 
            Called after each iteration, as callback(xk), where xk is the 
            current parameter vector. 
        retall : bool, optional 
            Set to True to return list of solutions at each iteration. 
            Available in Results object's mle_retvals attribute. 
        skip_hessian : bool, optional 
            If False (default), then the negative inverse hessian is calculated 
            after the optimization. If True, then the hessian will not be 
            calculated. However, it will be available in methods that use the 
            hessian in the optimization (currently only with `&quot;newton&quot;`). 
        kwargs : keywords 
            All kwargs are passed to the chosen solver with one exception. The 
            following keyword controls what happens after the fit:: 
 
                warn_convergence : bool, optional 
                    If True, checks the model for the converged flag. If the 
                    converged flag is False, a ConvergenceWarning is issued. 
 
        Notes 
        ----- 
        The 'basinhopping' solver ignores `maxiter`, `retall`, `full_output` 
        explicit arguments. 
 
        Optional arguments for solvers (see returned Results.mle_settings):: 
 
            'newton' 
                tol : float 
                    Relative error in params acceptable for convergence. 
            'nm' -- Nelder Mead 
                xtol : float 
                    Relative error in params acceptable for convergence 
                ftol : float 
                    Relative error in loglike(params) acceptable for 
                    convergence 
                maxfun : int 
                    Maximum number of function evaluations to make. 
            'bfgs' 
                gtol : float 
                    Stop when norm of gradient is less than gtol. 
                norm : float 
                    Order of norm (np.Inf is max, -np.Inf is min) 
                epsilon 
                    If fprime is approximated, use this value for the step 
                    size. Only relevant if LikelihoodModel.score is None. 
            'lbfgs' 
                m : int 
                    This many terms are used for the Hessian approximation. 
                factr : float 
                    A stop condition that is a variant of relative error. 
                pgtol : float 
                    A stop condition that uses the projected gradient. 
                epsilon 
                    If fprime is approximated, use this value for the step 
                    size. Only relevant if LikelihoodModel.score is None. 
                maxfun : int 
                    Maximum number of function evaluations to make. 
                bounds : sequence 
                    (min, max) pairs for each element in x, 
                    defining the bounds on that parameter. 
                    Use None for one of min or max when there is no bound 
                    in that direction. 
            'cg' 
                gtol : float 
                    Stop when norm of gradient is less than gtol. 
                norm : float 
                    Order of norm (np.Inf is max, -np.Inf is min) 
                epsilon : float 
                    If fprime is approximated, use this value for the step 
                    size. Can be scalar or vector.  Only relevant if 
                    Likelihoodmodel.score is None. 
            'ncg' 
                fhess_p : callable f'(x,*args) 
                    Function which computes the Hessian of f times an arbitrary 
                    vector, p.  Should only be supplied if 
                    LikelihoodModel.hessian is None. 
                avextol : float 
                    Stop when the average relative error in the minimizer 
                    falls below this amount. 
                epsilon : float or ndarray 
                    If fhess is approximated, use this value for the step size. 
                    Only relevant if Likelihoodmodel.hessian is None. 
            'powell' 
                xtol : float 
                    Line-search error tolerance 
                ftol : float 
                    Relative error in loglike(params) for acceptable for 
                    convergence. 
                maxfun : int 
                    Maximum number of function evaluations to make. 
                start_direc : ndarray 
                    Initial direction set. 
            'basinhopping' 
                niter : int 
                    The number of basin hopping iterations. 
                niter_success : int 
                    Stop the run if the global minimum candidate remains the 
                    same for this number of iterations. 
                T : float 
                    The &quot;temperature&quot; parameter for the accept or reject 
                    criterion. Higher &quot;temperatures&quot; mean that larger jumps 
                    in function value will be accepted. For best results 
                    `T` should be comparable to the separation (in function 
                    value) between local minima. 
                stepsize : float 
                    Initial step size for use in the random displacement. 
                interval : int 
                    The interval for how often to update the `stepsize`. 
                minimizer : dict 
                    Extra keyword arguments to be passed to the minimizer 
                    `scipy.optimize.minimize()`, for example 'method' - the 
                    minimization method (e.g. 'L-BFGS-B'), or 'tol' - the 
                    tolerance for termination. Other arguments are mapped from 
                    explicit argument of `fit`: 
                      - `args` &lt;- `fargs` 
                      - `jac` &lt;- `score` 
                      - `hess` &lt;- `hess` 
            'minimize' 
                min_method : str, optional 
                    Name of minimization method to use. 
                    Any method specific arguments can be passed directly. 
                    For a list of methods and their arguments, see 
                    documentation of `scipy.optimize.minimize`. 
                    If no method is specified, then BFGS is used. 
        &quot;&quot;&quot;</span>
        <span class="s1">Hinv = </span><span class="s0">None  </span><span class="s3"># JP error if full_output=0, Hinv not defined</span>

        <span class="s0">if </span><span class="s1">start_params </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s2">'start_params'</span><span class="s1">):</span>
                <span class="s1">start_params = self.start_params</span>
            <span class="s0">elif </span><span class="s1">self.exog </span><span class="s0">is not None</span><span class="s1">:</span>
                <span class="s3"># fails for shape (K,)?</span>
                <span class="s1">start_params = [</span><span class="s4">0</span><span class="s1">] * self.exog.shape[</span><span class="s4">1</span><span class="s1">]</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;If exog is None, then start_params should &quot;</span>
                                 <span class="s2">&quot;be specified&quot;</span><span class="s1">)</span>

        <span class="s3"># TODO: separate args from nonarg taking score and hessian, ie.,</span>
        <span class="s3"># user-supplied and numerically evaluated estimate frprime does not take</span>
        <span class="s3"># args in most (any?) of the optimize function</span>

        <span class="s1">nobs = self.endog.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s3"># f = lambda params, *args: -self.loglike(params, *args) / nobs</span>

        <span class="s0">def </span><span class="s1">f(params</span><span class="s0">, </span><span class="s1">*args):</span>
            <span class="s0">return </span><span class="s1">-self.loglike(params</span><span class="s0">, </span><span class="s1">*args) / nobs</span>

        <span class="s0">if </span><span class="s1">method == </span><span class="s2">'newton'</span><span class="s1">:</span>
            <span class="s3"># TODO: why are score and hess positive?</span>
            <span class="s0">def </span><span class="s1">score(params</span><span class="s0">, </span><span class="s1">*args):</span>
                <span class="s0">return </span><span class="s1">self.score(params</span><span class="s0">, </span><span class="s1">*args) / nobs</span>

            <span class="s0">def </span><span class="s1">hess(params</span><span class="s0">, </span><span class="s1">*args):</span>
                <span class="s0">return </span><span class="s1">self.hessian(params</span><span class="s0">, </span><span class="s1">*args) / nobs</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">def </span><span class="s1">score(params</span><span class="s0">, </span><span class="s1">*args):</span>
                <span class="s0">return </span><span class="s1">-self.score(params</span><span class="s0">, </span><span class="s1">*args) / nobs</span>

            <span class="s0">def </span><span class="s1">hess(params</span><span class="s0">, </span><span class="s1">*args):</span>
                <span class="s0">return </span><span class="s1">-self.hessian(params</span><span class="s0">, </span><span class="s1">*args) / nobs</span>

        <span class="s1">warn_convergence = kwargs.pop(</span><span class="s2">'warn_convergence'</span><span class="s0">, True</span><span class="s1">)</span>

        <span class="s3"># Remove covariance args before calling fir to allow strict checking</span>
        <span class="s0">if </span><span class="s2">'cov_type' </span><span class="s0">in </span><span class="s1">kwargs:</span>
            <span class="s1">cov_kwds = kwargs.get(</span><span class="s2">'cov_kwds'</span><span class="s0">, </span><span class="s1">{})</span>
            <span class="s1">kwds = {</span><span class="s2">'cov_type'</span><span class="s1">: kwargs[</span><span class="s2">'cov_type'</span><span class="s1">]</span><span class="s0">, </span><span class="s2">'cov_kwds'</span><span class="s1">: cov_kwds}</span>
            <span class="s0">if </span><span class="s1">cov_kwds:</span>
                <span class="s0">del </span><span class="s1">kwargs[</span><span class="s2">&quot;cov_kwds&quot;</span><span class="s1">]</span>
            <span class="s0">del </span><span class="s1">kwargs[</span><span class="s2">&quot;cov_type&quot;</span><span class="s1">]</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">kwds = {}</span>
        <span class="s0">if </span><span class="s2">'use_t' </span><span class="s0">in </span><span class="s1">kwargs:</span>
            <span class="s1">kwds[</span><span class="s2">'use_t'</span><span class="s1">] = kwargs[</span><span class="s2">'use_t'</span><span class="s1">]</span>
            <span class="s0">del </span><span class="s1">kwargs[</span><span class="s2">&quot;use_t&quot;</span><span class="s1">]</span>

        <span class="s1">optimizer = Optimizer()</span>
        <span class="s1">xopt</span><span class="s0">, </span><span class="s1">retvals</span><span class="s0">, </span><span class="s1">optim_settings = optimizer._fit(f</span><span class="s0">, </span><span class="s1">score</span><span class="s0">, </span><span class="s1">start_params</span><span class="s0">,</span>
                                                       <span class="s1">fargs</span><span class="s0">, </span><span class="s1">kwargs</span><span class="s0">,</span>
                                                       <span class="s1">hessian=hess</span><span class="s0">,</span>
                                                       <span class="s1">method=method</span><span class="s0">,</span>
                                                       <span class="s1">disp=disp</span><span class="s0">,</span>
                                                       <span class="s1">maxiter=maxiter</span><span class="s0">,</span>
                                                       <span class="s1">callback=callback</span><span class="s0">,</span>
                                                       <span class="s1">retall=retall</span><span class="s0">,</span>
                                                       <span class="s1">full_output=full_output)</span>
        <span class="s3"># Restore cov_type, cov_kwds and use_t</span>
        <span class="s1">optim_settings.update(kwds)</span>
        <span class="s3"># NOTE: this is for fit_regularized and should be generalized</span>
        <span class="s1">cov_params_func = kwargs.setdefault(</span><span class="s2">'cov_params_func'</span><span class="s0">, None</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">cov_params_func:</span>
            <span class="s1">Hinv = cov_params_func(self</span><span class="s0">, </span><span class="s1">xopt</span><span class="s0">, </span><span class="s1">retvals)</span>
        <span class="s0">elif </span><span class="s1">method == </span><span class="s2">'newton' </span><span class="s0">and </span><span class="s1">full_output:</span>
            <span class="s1">Hinv = np.linalg.inv(-retvals[</span><span class="s2">'Hessian'</span><span class="s1">]) / nobs</span>
        <span class="s0">elif not </span><span class="s1">skip_hessian:</span>
            <span class="s1">H = -</span><span class="s4">1 </span><span class="s1">* self.hessian(xopt)</span>
            <span class="s1">invertible = </span><span class="s0">False</span>
            <span class="s0">if </span><span class="s1">np.all(np.isfinite(H)):</span>
                <span class="s1">eigvals</span><span class="s0">, </span><span class="s1">eigvecs = np.linalg.eigh(H)</span>
                <span class="s0">if </span><span class="s1">np.min(eigvals) &gt; </span><span class="s4">0</span><span class="s1">:</span>
                    <span class="s1">invertible = </span><span class="s0">True</span>

            <span class="s0">if </span><span class="s1">invertible:</span>
                <span class="s1">Hinv = eigvecs.dot(np.diag(</span><span class="s4">1.0 </span><span class="s1">/ eigvals)).dot(eigvecs.T)</span>
                <span class="s1">Hinv = np.asfortranarray((Hinv + Hinv.T) / </span><span class="s4">2.0</span><span class="s1">)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">warnings.warn(</span><span class="s2">'Inverting hessian failed, no bse or cov_params '</span>
                              <span class="s2">'available'</span><span class="s0">, </span><span class="s1">HessianInversionWarning)</span>
                <span class="s1">Hinv = </span><span class="s0">None</span>

        <span class="s3"># TODO: add Hessian approximation and change the above if needed</span>
        <span class="s1">mlefit = LikelihoodModelResults(self</span><span class="s0">, </span><span class="s1">xopt</span><span class="s0">, </span><span class="s1">Hinv</span><span class="s0">, </span><span class="s1">scale=</span><span class="s4">1.</span><span class="s0">, </span><span class="s1">**kwds)</span>

        <span class="s3"># TODO: hardcode scale?</span>
        <span class="s1">mlefit.mle_retvals = retvals</span>
        <span class="s0">if </span><span class="s1">isinstance(retvals</span><span class="s0">, </span><span class="s1">dict):</span>
            <span class="s0">if </span><span class="s1">warn_convergence </span><span class="s0">and not </span><span class="s1">retvals[</span><span class="s2">'converged'</span><span class="s1">]:</span>
                <span class="s0">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s0">import </span><span class="s1">ConvergenceWarning</span>
                <span class="s1">warnings.warn(</span><span class="s2">&quot;Maximum Likelihood optimization failed to &quot;</span>
                              <span class="s2">&quot;converge. Check mle_retvals&quot;</span><span class="s0">,</span>
                              <span class="s1">ConvergenceWarning)</span>

        <span class="s1">mlefit.mle_settings = optim_settings</span>
        <span class="s0">return </span><span class="s1">mlefit</span>

    <span class="s0">def </span><span class="s1">_fit_zeros(self</span><span class="s0">, </span><span class="s1">keep_index=</span><span class="s0">None, </span><span class="s1">start_params=</span><span class="s0">None,</span>
                   <span class="s1">return_auxiliary=</span><span class="s0">False, </span><span class="s1">k_params=</span><span class="s0">None, </span><span class="s1">**fit_kwds):</span>
        <span class="s5">&quot;&quot;&quot;experimental, fit the model subject to zero constraints 
 
        Intended for internal use cases until we know what we need. 
        API will need to change to handle models with two exog. 
        This is not yet supported by all model subclasses. 
 
        This is essentially a simplified version of `fit_constrained`, and 
        does not need to use `offset`. 
 
        The estimation creates a new model with transformed design matrix, 
        exog, and converts the results back to the original parameterization. 
 
        Some subclasses could use a more efficient calculation than using a 
        new model. 
 
        Parameters 
        ---------- 
        keep_index : array_like (int or bool) or slice 
            variables that should be dropped. 
        start_params : None or array_like 
            starting values for the optimization. `start_params` needs to be 
            given in the original parameter space and are internally 
            transformed. 
        k_params : int or None 
            If None, then we try to infer from start_params or model. 
        **fit_kwds : keyword arguments 
            fit_kwds are used in the optimization of the transformed model. 
 
        Returns 
        ------- 
        results : Results instance 
        &quot;&quot;&quot;</span>
        <span class="s3"># we need to append index of extra params to keep_index as in</span>
        <span class="s3"># NegativeBinomial</span>
        <span class="s0">if </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s2">'k_extra'</span><span class="s1">) </span><span class="s0">and </span><span class="s1">self.k_extra &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s3"># we cannot change the original, TODO: should we add keep_index_params?</span>
            <span class="s1">keep_index = np.array(keep_index</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">True</span><span class="s1">)</span>
            <span class="s1">k = self.exog.shape[</span><span class="s4">1</span><span class="s1">]</span>
            <span class="s1">extra_index = np.arange(k</span><span class="s0">, </span><span class="s1">k + self.k_extra)</span>
            <span class="s1">keep_index_p = np.concatenate((keep_index</span><span class="s0">, </span><span class="s1">extra_index))</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">keep_index_p = keep_index</span>

        <span class="s3"># not all models support start_params, drop if None, hide them in fit_kwds</span>
        <span class="s0">if </span><span class="s1">start_params </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">fit_kwds[</span><span class="s2">'start_params'</span><span class="s1">] = start_params[keep_index_p]</span>
            <span class="s1">k_params = len(start_params)</span>
            <span class="s3"># ignore k_params in this case, or verify consisteny?</span>

        <span class="s3"># build auxiliary model and fit</span>
        <span class="s1">init_kwds = self._get_init_kwds()</span>
        <span class="s1">mod_constr = self.__class__(self.endog</span><span class="s0">, </span><span class="s1">self.exog[:</span><span class="s0">, </span><span class="s1">keep_index]</span><span class="s0">,</span>
                                    <span class="s1">**init_kwds)</span>
        <span class="s1">res_constr = mod_constr.fit(**fit_kwds)</span>
        <span class="s3"># switch name, only need keep_index for params below</span>
        <span class="s1">keep_index = keep_index_p</span>

        <span class="s0">if </span><span class="s1">k_params </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">k_params = self.exog.shape[</span><span class="s4">1</span><span class="s1">]</span>
            <span class="s1">k_params += getattr(self</span><span class="s0">, </span><span class="s2">'k_extra'</span><span class="s0">, </span><span class="s4">0</span><span class="s1">)</span>

        <span class="s1">params_full = np.zeros(k_params)</span>
        <span class="s1">params_full[keep_index] = res_constr.params</span>

        <span class="s3"># create dummy results Instance, TODO: wire up properly</span>
        <span class="s3"># TODO: this could be moved into separate private method if needed</span>
        <span class="s3"># discrete L1 fit_regularized doens't reestimate AFAICS</span>
        <span class="s3"># RLM does not have method, disp nor warn_convergence keywords</span>
        <span class="s3"># OLS, WLS swallows extra kwds with **kwargs, but does not have method='nm'</span>
        <span class="s0">try</span><span class="s1">:</span>
            <span class="s3"># Note: addding full_output=False causes exceptions</span>
            <span class="s1">res = self.fit(maxiter=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">disp=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">method=</span><span class="s2">'nm'</span><span class="s0">, </span><span class="s1">skip_hessian=</span><span class="s0">True,</span>
                           <span class="s1">warn_convergence=</span><span class="s0">False, </span><span class="s1">start_params=params_full)</span>
            <span class="s3"># we get a wrapper back</span>
        <span class="s0">except </span><span class="s1">(TypeError</span><span class="s0">, </span><span class="s1">ValueError):</span>
            <span class="s1">res = self.fit()</span>

        <span class="s3"># Warning: make sure we are not just changing the wrapper instead of</span>
        <span class="s3"># results #2400</span>
        <span class="s3"># TODO: do we need to change res._results.scale in some models?</span>
        <span class="s0">if </span><span class="s1">hasattr(res_constr.model</span><span class="s0">, </span><span class="s2">'scale'</span><span class="s1">):</span>
            <span class="s3"># Note: res.model is self</span>
            <span class="s3"># GLM problem, see #2399,</span>
            <span class="s3"># TODO: remove from model if not needed anymore</span>
            <span class="s1">res.model.scale = res._results.scale = res_constr.model.scale</span>

        <span class="s0">if </span><span class="s1">hasattr(res_constr</span><span class="s0">, </span><span class="s2">'mle_retvals'</span><span class="s1">):</span>
            <span class="s1">res._results.mle_retvals = res_constr.mle_retvals</span>
            <span class="s3"># not available for not scipy optimization, e.g. glm irls</span>
            <span class="s3"># TODO: what retvals should be required?</span>
            <span class="s3"># res.mle_retvals['fcall'] = res_constr.mle_retvals.get('fcall', np.nan)</span>
            <span class="s3"># res.mle_retvals['iterations'] = res_constr.mle_retvals.get(</span>
            <span class="s3">#                                                 'iterations', np.nan)</span>
            <span class="s3"># res.mle_retvals['converged'] = res_constr.mle_retvals['converged']</span>
        <span class="s3"># overwrite all mle_settings</span>
        <span class="s0">if </span><span class="s1">hasattr(res_constr</span><span class="s0">, </span><span class="s2">'mle_settings'</span><span class="s1">):</span>
            <span class="s1">res._results.mle_settings = res_constr.mle_settings</span>

        <span class="s1">res._results.params = params_full</span>
        <span class="s0">if </span><span class="s1">(</span><span class="s0">not </span><span class="s1">hasattr(res._results</span><span class="s0">, </span><span class="s2">'normalized_cov_params'</span><span class="s1">) </span><span class="s0">or</span>
                <span class="s1">res._results.normalized_cov_params </span><span class="s0">is None</span><span class="s1">):</span>
            <span class="s1">res._results.normalized_cov_params = np.zeros((k_params</span><span class="s0">, </span><span class="s1">k_params))</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">res._results.normalized_cov_params[...] = </span><span class="s4">0</span>

        <span class="s3"># fancy indexing requires integer array</span>
        <span class="s1">keep_index = np.array(keep_index)</span>
        <span class="s1">res._results.normalized_cov_params[keep_index[:</span><span class="s0">, None</span><span class="s1">]</span><span class="s0">, </span><span class="s1">keep_index] = \</span>
            <span class="s1">res_constr.normalized_cov_params</span>
        <span class="s1">k_constr = res_constr.df_resid - res._results.df_resid</span>
        <span class="s0">if </span><span class="s1">hasattr(res_constr</span><span class="s0">, </span><span class="s2">'cov_params_default'</span><span class="s1">):</span>
            <span class="s1">res._results.cov_params_default = np.zeros((k_params</span><span class="s0">, </span><span class="s1">k_params))</span>
            <span class="s1">res._results.cov_params_default[keep_index[:</span><span class="s0">, None</span><span class="s1">]</span><span class="s0">, </span><span class="s1">keep_index] = \</span>
                <span class="s1">res_constr.cov_params_default</span>
        <span class="s0">if </span><span class="s1">hasattr(res_constr</span><span class="s0">, </span><span class="s2">'cov_type'</span><span class="s1">):</span>
            <span class="s1">res._results.cov_type = res_constr.cov_type</span>
            <span class="s1">res._results.cov_kwds = res_constr.cov_kwds</span>

        <span class="s1">res._results.keep_index = keep_index</span>
        <span class="s1">res._results.df_resid = res_constr.df_resid</span>
        <span class="s1">res._results.df_model = res_constr.df_model</span>

        <span class="s1">res._results.k_constr = k_constr</span>
        <span class="s1">res._results.results_constrained = res_constr</span>

        <span class="s3"># special temporary workaround for RLM</span>
        <span class="s3"># need to be able to override robust covariances</span>
        <span class="s0">if </span><span class="s1">hasattr(res.model</span><span class="s0">, </span><span class="s2">'M'</span><span class="s1">):</span>
            <span class="s0">del </span><span class="s1">res._results._cache[</span><span class="s2">'resid'</span><span class="s1">]</span>
            <span class="s0">del </span><span class="s1">res._results._cache[</span><span class="s2">'fittedvalues'</span><span class="s1">]</span>
            <span class="s0">del </span><span class="s1">res._results._cache[</span><span class="s2">'sresid'</span><span class="s1">]</span>
            <span class="s1">cov = res._results._cache[</span><span class="s2">'bcov_scaled'</span><span class="s1">]</span>
            <span class="s3"># inplace adjustment</span>
            <span class="s1">cov[...] = </span><span class="s4">0</span>
            <span class="s1">cov[keep_index[:</span><span class="s0">, None</span><span class="s1">]</span><span class="s0">, </span><span class="s1">keep_index] = res_constr.bcov_scaled</span>
            <span class="s1">res._results.cov_params_default = cov</span>

        <span class="s0">return </span><span class="s1">res</span>

    <span class="s0">def </span><span class="s1">_fit_collinear(self</span><span class="s0">, </span><span class="s1">atol=</span><span class="s4">1e-14</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s4">1e-13</span><span class="s0">, </span><span class="s1">**kwds):</span>
        <span class="s5">&quot;&quot;&quot;experimental, fit of the model without collinear variables 
 
        This currently uses QR to drop variables based on the given 
        sequence. 
        Options will be added in future, when the supporting functions 
        to identify collinear variables become available. 
        &quot;&quot;&quot;</span>

        <span class="s3"># ------ copied from PR #2380 remove when merged</span>
        <span class="s1">x = self.exog</span>
        <span class="s1">tol = atol + rtol * x.var(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">r = np.linalg.qr(x</span><span class="s0">, </span><span class="s1">mode=</span><span class="s2">'r'</span><span class="s1">)</span>
        <span class="s1">mask = np.abs(r.diagonal()) &lt; np.sqrt(tol)</span>
        <span class="s3"># TODO add to results instance</span>
        <span class="s3"># idx_collinear = np.where(mask)[0]</span>
        <span class="s1">idx_keep = np.where(~mask)[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s0">return </span><span class="s1">self._fit_zeros(keep_index=idx_keep</span><span class="s0">, </span><span class="s1">**kwds)</span>


<span class="s3"># TODO: the below is unfinished</span>
<span class="s0">class </span><span class="s1">GenericLikelihoodModel(LikelihoodModel):</span>
    <span class="s5">&quot;&quot;&quot; 
    Allows the fitting of any likelihood function via maximum likelihood. 
 
    A subclass needs to specify at least the log-likelihood 
    If the log-likelihood is specified for each observation, then results that 
    require the Jacobian will be available. (The other case is not tested yet.) 
 
    Notes 
    ----- 
    Optimization methods that require only a likelihood function are 'nm' and 
    'powell' 
 
    Optimization methods that require a likelihood function and a 
    score/gradient are 'bfgs', 'cg', and 'ncg'. A function to compute the 
    Hessian is optional for 'ncg'. 
 
    Optimization method that require a likelihood function, a score/gradient, 
    and a Hessian is 'newton' 
 
    If they are not overwritten by a subclass, then numerical gradient, 
    Jacobian and Hessian of the log-likelihood are calculated by numerical 
    forward differentiation. This might results in some cases in precision 
    problems, and the Hessian might not be positive definite. Even if the 
    Hessian is not positive definite the covariance matrix of the parameter 
    estimates based on the outer product of the Jacobian might still be valid. 
 
 
    Examples 
    -------- 
    see also subclasses in directory miscmodels 
 
    import statsmodels.api as sm 
    data = sm.datasets.spector.load() 
    data.exog = sm.add_constant(data.exog) 
    # in this dir 
    from model import GenericLikelihoodModel 
    probit_mod = sm.Probit(data.endog, data.exog) 
    probit_res = probit_mod.fit() 
    loglike = probit_mod.loglike 
    score = probit_mod.score 
    mod = GenericLikelihoodModel(data.endog, data.exog, loglike, score) 
    res = mod.fit(method=&quot;nm&quot;, maxiter = 500) 
    import numpy as np 
    np.allclose(res.params, probit_res.params) 
    &quot;&quot;&quot;</span>
    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog=</span><span class="s0">None, </span><span class="s1">loglike=</span><span class="s0">None, </span><span class="s1">score=</span><span class="s0">None,</span>
                 <span class="s1">hessian=</span><span class="s0">None, </span><span class="s1">missing=</span><span class="s2">'none'</span><span class="s0">, </span><span class="s1">extra_params_names=</span><span class="s0">None,</span>
                 <span class="s1">**kwds):</span>
        <span class="s3"># let them be none in case user wants to use inheritance</span>
        <span class="s0">if </span><span class="s1">loglike </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">self.loglike = loglike</span>
        <span class="s0">if </span><span class="s1">score </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">self.score = score</span>
        <span class="s0">if </span><span class="s1">hessian </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">self.hessian = hessian</span>

        <span class="s1">hasconst = kwds.pop(</span><span class="s2">&quot;hasconst&quot;</span><span class="s0">, None</span><span class="s1">)</span>
        <span class="s1">self.__dict__.update(kwds)</span>

        <span class="s3"># TODO: data structures?</span>

        <span class="s3"># TODO temporary solution, force approx normal</span>
        <span class="s3"># self.df_model = 9999</span>
        <span class="s3"># somewhere: CacheWriteWarning: 'df_model' cannot be overwritten</span>
        <span class="s1">super(GenericLikelihoodModel</span><span class="s0">, </span><span class="s1">self).__init__(endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">,</span>
                                                     <span class="s1">missing=missing</span><span class="s0">,</span>
                                                     <span class="s1">hasconst=hasconst</span><span class="s0">,</span>
                                                     <span class="s1">**kwds</span>
                                                     <span class="s1">)</span>

        <span class="s3"># this will not work for ru2nmnl, maybe np.ndim of a dict?</span>
        <span class="s0">if </span><span class="s1">exog </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">self.nparams = (exog.shape[</span><span class="s4">1</span><span class="s1">] </span><span class="s0">if </span><span class="s1">np.ndim(exog) == </span><span class="s4">2 </span><span class="s0">else </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s0">if </span><span class="s1">extra_params_names </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">self._set_extra_params_names(extra_params_names)</span>

    <span class="s0">def </span><span class="s1">_set_extra_params_names(self</span><span class="s0">, </span><span class="s1">extra_params_names):</span>
        <span class="s3"># check param_names</span>
        <span class="s0">if </span><span class="s1">extra_params_names </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">self.exog </span><span class="s0">is not None</span><span class="s1">:</span>
                <span class="s1">self.exog_names.extend(extra_params_names)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">self.data.xnames = extra_params_names</span>

            <span class="s1">self.k_extra = len(extra_params_names)</span>
            <span class="s0">if </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s2">&quot;df_resid&quot;</span><span class="s1">):</span>
                <span class="s1">self.df_resid -= self.k_extra</span>

        <span class="s1">self.nparams = len(self.exog_names)</span>

    <span class="s3"># this is redundant and not used when subclassing</span>
    <span class="s0">def </span><span class="s1">initialize(self):</span>
        <span class="s5">&quot;&quot;&quot; 
        Initialize (possibly re-initialize) a Model instance. For 
        instance, the design matrix of a linear model may change 
        and some things must be recomputed. 
        &quot;&quot;&quot;</span>
        <span class="s0">if not </span><span class="s1">self.score:  </span><span class="s3"># right now score is not optional</span>
            <span class="s1">self.score = </span><span class="s0">lambda </span><span class="s1">x: approx_fprime(x</span><span class="s0">, </span><span class="s1">self.loglike)</span>
            <span class="s0">if not </span><span class="s1">self.hessian:</span>
                <span class="s0">pass</span>
        <span class="s0">else</span><span class="s1">:   </span><span class="s3"># can use approx_hess_p if we have a gradient</span>
            <span class="s0">if not </span><span class="s1">self.hessian:</span>
                <span class="s0">pass</span>
        <span class="s3"># Initialize is called by</span>
        <span class="s3"># statsmodels.model.LikelihoodModel.__init__</span>
        <span class="s3"># and should contain any preprocessing that needs to be done for a model</span>
        <span class="s0">if </span><span class="s1">self.exog </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s3"># assume constant</span>
            <span class="s1">er = np.linalg.matrix_rank(self.exog)</span>
            <span class="s1">self.df_model = float(er - </span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">self.df_resid = float(self.exog.shape[</span><span class="s4">0</span><span class="s1">] - er)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">self.df_model = np.nan</span>
            <span class="s1">self.df_resid = np.nan</span>
        <span class="s1">super(GenericLikelihoodModel</span><span class="s0">, </span><span class="s1">self).initialize()</span>

    <span class="s0">def </span><span class="s1">expandparams(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        expand to full parameter array when some parameters are fixed 
 
        Parameters 
        ---------- 
        params : ndarray 
            reduced parameter array 
 
        Returns 
        ------- 
        paramsfull : ndarray 
            expanded parameter array where fixed parameters are included 
 
        Notes 
        ----- 
        Calling this requires that self.fixed_params and self.fixed_paramsmask 
        are defined. 
 
        *developer notes:* 
 
        This can be used in the log-likelihood to ... 
 
        this could also be replaced by a more general parameter 
        transformation. 
        &quot;&quot;&quot;</span>
        <span class="s1">paramsfull = self.fixed_params.copy()</span>
        <span class="s1">paramsfull[self.fixed_paramsmask] = params</span>
        <span class="s0">return </span><span class="s1">paramsfull</span>

    <span class="s0">def </span><span class="s1">reduceparams(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot;Reduce parameters&quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">params[self.fixed_paramsmask]</span>

    <span class="s0">def </span><span class="s1">loglike(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot;Log-likelihood of model at params&quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">self.loglikeobs(params).sum(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">nloglike(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot;Negative log-likelihood of model at params&quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">-self.loglikeobs(params).sum(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">loglikeobs(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Log-likelihood of the model for all observations at params. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike : array_like 
            The log likelihood of the model evaluated at `params`. 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">-self.nloglikeobs(params)</span>

    <span class="s0">def </span><span class="s1">score(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Gradient of log-likelihood evaluated at params 
        &quot;&quot;&quot;</span>
        <span class="s1">kwds = {}</span>
        <span class="s1">kwds.setdefault(</span><span class="s2">'centered'</span><span class="s0">, True</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">approx_fprime(params</span><span class="s0">, </span><span class="s1">self.loglike</span><span class="s0">, </span><span class="s1">**kwds).ravel()</span>

    <span class="s0">def </span><span class="s1">score_obs(self</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">**kwds):</span>
        <span class="s5">&quot;&quot;&quot; 
        Jacobian/Gradient of log-likelihood evaluated at params for each 
        observation. 
        &quot;&quot;&quot;</span>
        <span class="s3"># kwds.setdefault('epsilon', 1e-4)</span>
        <span class="s1">kwds.setdefault(</span><span class="s2">'centered'</span><span class="s0">, True</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">approx_fprime(params</span><span class="s0">, </span><span class="s1">self.loglikeobs</span><span class="s0">, </span><span class="s1">**kwds)</span>

    <span class="s0">def </span><span class="s1">hessian(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Hessian of log-likelihood evaluated at params 
        &quot;&quot;&quot;</span>
        <span class="s0">from </span><span class="s1">statsmodels.tools.numdiff </span><span class="s0">import </span><span class="s1">approx_hess</span>

        <span class="s3"># need options for hess (epsilon)</span>
        <span class="s0">return </span><span class="s1">approx_hess(params</span><span class="s0">, </span><span class="s1">self.loglike)</span>

    <span class="s0">def </span><span class="s1">hessian_factor(self</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">scale=</span><span class="s0">None, </span><span class="s1">observed=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Weights for calculating Hessian 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which Hessian is evaluated 
        scale : None or float 
            If scale is None, then the default scale will be calculated. 
            Default scale is defined by `self.scaletype` and set in fit. 
            If scale is not None, then it is used as a fixed scale. 
        observed : bool 
            If True, then the observed Hessian is returned. If false then the 
            expected information matrix is returned. 
 
        Returns 
        ------- 
        hessian_factor : ndarray, 1d 
            A 1d weight vector used in the calculation of the Hessian. 
            The hessian is obtained by `(exog.T * hessian_factor).dot(exog)` 
        &quot;&quot;&quot;</span>

        <span class="s0">raise </span><span class="s1">NotImplementedError</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">start_params=</span><span class="s0">None, </span><span class="s1">method=</span><span class="s2">'nm'</span><span class="s0">, </span><span class="s1">maxiter=</span><span class="s4">500</span><span class="s0">, </span><span class="s1">full_output=</span><span class="s4">1</span><span class="s0">,</span>
            <span class="s1">disp=</span><span class="s4">1</span><span class="s0">, </span><span class="s1">callback=</span><span class="s0">None, </span><span class="s1">retall=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">**kwargs):</span>

        <span class="s0">if </span><span class="s1">start_params </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s2">'start_params'</span><span class="s1">):</span>
                <span class="s1">start_params = self.start_params</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">start_params = </span><span class="s4">0.1 </span><span class="s1">* np.ones(self.nparams)</span>

        <span class="s0">if </span><span class="s2">&quot;cov_type&quot; </span><span class="s0">not in </span><span class="s1">kwargs:</span>
            <span class="s3"># this will add default cov_type name and description</span>
            <span class="s1">kwargs[</span><span class="s2">&quot;cov_type&quot;</span><span class="s1">] = </span><span class="s2">'nonrobust'</span>

        <span class="s1">fit_method = super(GenericLikelihoodModel</span><span class="s0">, </span><span class="s1">self).fit</span>
        <span class="s1">mlefit = fit_method(start_params=start_params</span><span class="s0">,</span>
                            <span class="s1">method=method</span><span class="s0">, </span><span class="s1">maxiter=maxiter</span><span class="s0">,</span>
                            <span class="s1">full_output=full_output</span><span class="s0">,</span>
                            <span class="s1">disp=disp</span><span class="s0">, </span><span class="s1">callback=callback</span><span class="s0">, </span><span class="s1">**kwargs)</span>

        <span class="s1">results_class = getattr(self</span><span class="s0">, </span><span class="s2">'results_class'</span><span class="s0">,</span>
                                <span class="s1">GenericLikelihoodModelResults)</span>
        <span class="s1">genericmlefit = results_class(self</span><span class="s0">, </span><span class="s1">mlefit)</span>

        <span class="s3"># amend param names</span>
        <span class="s1">exog_names = [] </span><span class="s0">if </span><span class="s1">(self.exog_names </span><span class="s0">is None</span><span class="s1">) </span><span class="s0">else </span><span class="s1">self.exog_names</span>
        <span class="s1">k_miss = len(exog_names) - len(mlefit.params)</span>
        <span class="s0">if not </span><span class="s1">k_miss == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">k_miss &lt; </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s1">self._set_extra_params_names([</span><span class="s2">'par%d' </span><span class="s1">% i</span>
                                              <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(-k_miss)])</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s3"># I do not want to raise after we have already fit()</span>
                <span class="s1">warnings.warn(</span><span class="s2">'more exog_names than parameters'</span><span class="s0">, </span><span class="s1">ValueWarning)</span>

        <span class="s0">return </span><span class="s1">genericmlefit</span>


<span class="s0">class </span><span class="s1">Results:</span>
    <span class="s5">&quot;&quot;&quot; 
    Class to contain model results 
 
    Parameters 
    ---------- 
    model : class instance 
        the previously specified model instance 
    params : ndarray 
        parameter estimates from the fit model 
    &quot;&quot;&quot;</span>
    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">model</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">**kwd):</span>
        <span class="s1">self.__dict__.update(kwd)</span>
        <span class="s1">self.initialize(model</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">**kwd)</span>
        <span class="s1">self._data_attr = []</span>
        <span class="s3"># Variables to clear from cache</span>
        <span class="s1">self._data_in_cache = [</span><span class="s2">'fittedvalues'</span><span class="s0">, </span><span class="s2">'resid'</span><span class="s0">, </span><span class="s2">'wresid'</span><span class="s1">]</span>

    <span class="s0">def </span><span class="s1">initialize(self</span><span class="s0">, </span><span class="s1">model</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s5">&quot;&quot;&quot; 
        Initialize (possibly re-initialize) a Results instance. 
 
        Parameters 
        ---------- 
        model : Model 
            The model instance. 
        params : ndarray 
            The model parameters. 
        **kwargs 
            Any additional keyword arguments required to initialize the model. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.params = params</span>
        <span class="s1">self.model = model</span>
        <span class="s0">if </span><span class="s1">hasattr(model</span><span class="s0">, </span><span class="s2">'k_constant'</span><span class="s1">):</span>
            <span class="s1">self.k_constant = model.k_constant</span>

    <span class="s0">def </span><span class="s1">_transform_predict_exog(self</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">transform=</span><span class="s0">True</span><span class="s1">):</span>

        <span class="s1">is_pandas = _is_using_pandas(exog</span><span class="s0">, None</span><span class="s1">)</span>
        <span class="s1">exog_index = </span><span class="s0">None</span>
        <span class="s0">if </span><span class="s1">is_pandas:</span>
            <span class="s0">if </span><span class="s1">exog.ndim == </span><span class="s4">2 </span><span class="s0">or </span><span class="s1">self.params.size == </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">exog_index = exog.index</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">exog_index = [exog.index.name]</span>

        <span class="s0">if </span><span class="s1">transform </span><span class="s0">and </span><span class="s1">hasattr(self.model</span><span class="s0">, </span><span class="s2">'formula'</span><span class="s1">) </span><span class="s0">and </span><span class="s1">(exog </span><span class="s0">is not None</span><span class="s1">):</span>
            <span class="s3"># allow both location of design_info, see #7043</span>
            <span class="s1">design_info = (getattr(self.model</span><span class="s0">, </span><span class="s2">&quot;design_info&quot;</span><span class="s0">, None</span><span class="s1">) </span><span class="s0">or</span>
                           <span class="s1">self.model.data.design_info)</span>
            <span class="s0">from </span><span class="s1">patsy </span><span class="s0">import </span><span class="s1">dmatrix</span>
            <span class="s0">if </span><span class="s1">isinstance(exog</span><span class="s0">, </span><span class="s1">pd.Series):</span>
                <span class="s3"># we are guessing whether it should be column or row</span>
                <span class="s0">if </span><span class="s1">(hasattr(exog</span><span class="s0">, </span><span class="s2">'name'</span><span class="s1">) </span><span class="s0">and </span><span class="s1">isinstance(exog.name</span><span class="s0">, </span><span class="s1">str) </span><span class="s0">and</span>
                        <span class="s1">exog.name </span><span class="s0">in </span><span class="s1">design_info.describe()):</span>
                    <span class="s3"># assume we need one column</span>
                    <span class="s1">exog = pd.DataFrame(exog)</span>
                <span class="s0">else</span><span class="s1">:</span>
                    <span class="s3"># assume we need a row</span>
                    <span class="s1">exog = pd.DataFrame(exog).T</span>
                <span class="s1">exog_index = exog.index</span>
            <span class="s1">orig_exog_len = len(exog)</span>
            <span class="s1">is_dict = isinstance(exog</span><span class="s0">, </span><span class="s1">dict)</span>
            <span class="s0">try</span><span class="s1">:</span>
                <span class="s1">exog = dmatrix(design_info</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">return_type=</span><span class="s2">&quot;dataframe&quot;</span><span class="s1">)</span>
            <span class="s0">except </span><span class="s1">Exception </span><span class="s0">as </span><span class="s1">exc:</span>
                <span class="s1">msg = (</span><span class="s2">'predict requires that you use a DataFrame when '</span>
                       <span class="s2">'predicting from a model</span><span class="s0">\n</span><span class="s2">that was created using the '</span>
                       <span class="s2">'formula api.'</span>
                       <span class="s2">'</span><span class="s0">\n\n</span><span class="s2">The original error message returned by patsy is:</span><span class="s0">\n</span><span class="s2">'</span>
                       <span class="s2">'{0}'</span><span class="s1">.format(str(str(exc))))</span>
                <span class="s0">raise </span><span class="s1">exc.__class__(msg)</span>
            <span class="s0">if </span><span class="s1">orig_exog_len &gt; len(exog) </span><span class="s0">and not </span><span class="s1">is_dict:</span>
                <span class="s0">if </span><span class="s1">exog_index </span><span class="s0">is None</span><span class="s1">:</span>
                    <span class="s1">warnings.warn(</span><span class="s2">'nan values have been dropped'</span><span class="s0">, </span><span class="s1">ValueWarning)</span>
                <span class="s0">else</span><span class="s1">:</span>
                    <span class="s1">exog = exog.reindex(exog_index)</span>
            <span class="s1">exog_index = exog.index</span>

        <span class="s0">if </span><span class="s1">exog </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">exog = np.asarray(exog)</span>
            <span class="s0">if </span><span class="s1">exog.ndim == </span><span class="s4">1 </span><span class="s0">and </span><span class="s1">(self.model.exog.ndim == </span><span class="s4">1 </span><span class="s0">or</span>
                                   <span class="s1">self.model.exog.shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">):</span>
                <span class="s1">exog = exog[:</span><span class="s0">, None</span><span class="s1">]</span>
            <span class="s1">exog = np.atleast_2d(exog)  </span><span class="s3"># needed in count model shape[1]</span>

        <span class="s0">return </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">exog_index</span>

    <span class="s0">def </span><span class="s1">predict(self</span><span class="s0">, </span><span class="s1">exog=</span><span class="s0">None, </span><span class="s1">transform=</span><span class="s0">True, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s5">&quot;&quot;&quot; 
        Call self.model.predict with self.params as the first argument. 
 
        Parameters 
        ---------- 
        exog : array_like, optional 
            The values for which you want to predict. see Notes below. 
        transform : bool, optional 
            If the model was fit via a formula, do you want to pass 
            exog through the formula. Default is True. E.g., if you fit 
            a model y ~ log(x1) + log(x2), and transform is True, then 
            you can pass a data structure that contains x1 and x2 in 
            their original form. Otherwise, you'd need to log the data 
            first. 
        *args 
            Additional arguments to pass to the model, see the 
            predict method of the model for the details. 
        **kwargs 
            Additional keywords arguments to pass to the model, see the 
            predict method of the model for the details. 
 
        Returns 
        ------- 
        array_like 
            See self.model.predict. 
 
        Notes 
        ----- 
        The types of exog that are supported depends on whether a formula 
        was used in the specification of the model. 
 
        If a formula was used, then exog is processed in the same way as 
        the original data. This transformation needs to have key access to the 
        same variable names, and can be a pandas DataFrame or a dict like 
        object that contains numpy arrays. 
 
        If no formula was used, then the provided exog needs to have the 
        same number of columns as the original exog in the model. No 
        transformation of the data is performed except converting it to 
        a numpy array. 
 
        Row indices as in pandas data frames are supported, and added to the 
        returned prediction. 
        &quot;&quot;&quot;</span>
        <span class="s1">exog</span><span class="s0">, </span><span class="s1">exog_index = self._transform_predict_exog(exog</span><span class="s0">,</span>
                                                        <span class="s1">transform=transform)</span>

        <span class="s1">predict_results = self.model.predict(self.params</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">,</span>
                                             <span class="s1">**kwargs)</span>

        <span class="s0">if </span><span class="s1">exog_index </span><span class="s0">is not None and not </span><span class="s1">hasattr(predict_results</span><span class="s0">,</span>
                                                  <span class="s2">'predicted_values'</span><span class="s1">):</span>
            <span class="s0">if </span><span class="s1">predict_results.ndim == </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s0">return </span><span class="s1">pd.Series(predict_results</span><span class="s0">, </span><span class="s1">index=exog_index)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">return </span><span class="s1">pd.DataFrame(predict_results</span><span class="s0">, </span><span class="s1">index=exog_index)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">predict_results</span>

    <span class="s0">def </span><span class="s1">summary(self):</span>
        <span class="s5">&quot;&quot;&quot; 
        Summary 
 
        Not implemented 
        &quot;&quot;&quot;</span>
        <span class="s0">raise </span><span class="s1">NotImplementedError</span>


<span class="s3"># TODO: public method?</span>
<span class="s0">class </span><span class="s1">LikelihoodModelResults(Results):</span>
    <span class="s5">&quot;&quot;&quot; 
    Class to contain results from likelihood models 
 
    Parameters 
    ---------- 
    model : LikelihoodModel instance or subclass instance 
        LikelihoodModelResults holds a reference to the model that is fit. 
    params : 1d array_like 
        parameter estimates from estimated model 
    normalized_cov_params : 2d array 
       Normalized (before scaling) covariance of params. (dot(X.T,X))**-1 
    scale : float 
        For (some subset of models) scale will typically be the 
        mean square error from the estimated model (sigma^2) 
 
    Attributes 
    ---------- 
    mle_retvals : dict 
        Contains the values returned from the chosen optimization method if 
        full_output is True during the fit.  Available only if the model 
        is fit by maximum likelihood.  See notes below for the output from 
        the different methods. 
    mle_settings : dict 
        Contains the arguments passed to the chosen optimization method. 
        Available if the model is fit by maximum likelihood.  See 
        LikelihoodModel.fit for more information. 
    model : model instance 
        LikelihoodResults contains a reference to the model that is fit. 
    params : ndarray 
        The parameters estimated for the model. 
    scale : float 
        The scaling factor of the model given during instantiation. 
    tvalues : ndarray 
        The t-values of the standard errors. 
 
 
    Notes 
    ----- 
    The covariance of params is given by scale times normalized_cov_params. 
 
    Return values by solver if full_output is True during fit: 
 
        'newton' 
            fopt : float 
                The value of the (negative) loglikelihood at its 
                minimum. 
            iterations : int 
                Number of iterations performed. 
            score : ndarray 
                The score vector at the optimum. 
            Hessian : ndarray 
                The Hessian at the optimum. 
            warnflag : int 
                1 if maxiter is exceeded. 0 if successful convergence. 
            converged : bool 
                True: converged. False: did not converge. 
            allvecs : list 
                List of solutions at each iteration. 
        'nm' 
            fopt : float 
                The value of the (negative) loglikelihood at its 
                minimum. 
            iterations : int 
                Number of iterations performed. 
            warnflag : int 
                1: Maximum number of function evaluations made. 
                2: Maximum number of iterations reached. 
            converged : bool 
                True: converged. False: did not converge. 
            allvecs : list 
                List of solutions at each iteration. 
        'bfgs' 
            fopt : float 
                Value of the (negative) loglikelihood at its minimum. 
            gopt : float 
                Value of gradient at minimum, which should be near 0. 
            Hinv : ndarray 
                value of the inverse Hessian matrix at minimum.  Note 
                that this is just an approximation and will often be 
                different from the value of the analytic Hessian. 
            fcalls : int 
                Number of calls to loglike. 
            gcalls : int 
                Number of calls to gradient/score. 
            warnflag : int 
                1: Maximum number of iterations exceeded. 2: Gradient 
                and/or function calls are not changing. 
            converged : bool 
                True: converged.  False: did not converge. 
            allvecs : list 
                Results at each iteration. 
        'lbfgs' 
            fopt : float 
                Value of the (negative) loglikelihood at its minimum. 
            gopt : float 
                Value of gradient at minimum, which should be near 0. 
            fcalls : int 
                Number of calls to loglike. 
            warnflag : int 
                Warning flag: 
 
                - 0 if converged 
                - 1 if too many function evaluations or too many iterations 
                - 2 if stopped for another reason 
 
            converged : bool 
                True: converged.  False: did not converge. 
        'powell' 
            fopt : float 
                Value of the (negative) loglikelihood at its minimum. 
            direc : ndarray 
                Current direction set. 
            iterations : int 
                Number of iterations performed. 
            fcalls : int 
                Number of calls to loglike. 
            warnflag : int 
                1: Maximum number of function evaluations. 2: Maximum number 
                of iterations. 
            converged : bool 
                True : converged. False: did not converge. 
            allvecs : list 
                Results at each iteration. 
        'cg' 
            fopt : float 
                Value of the (negative) loglikelihood at its minimum. 
            fcalls : int 
                Number of calls to loglike. 
            gcalls : int 
                Number of calls to gradient/score. 
            warnflag : int 
                1: Maximum number of iterations exceeded. 2: Gradient and/ 
                or function calls not changing. 
            converged : bool 
                True: converged. False: did not converge. 
            allvecs : list 
                Results at each iteration. 
        'ncg' 
            fopt : float 
                Value of the (negative) loglikelihood at its minimum. 
            fcalls : int 
                Number of calls to loglike. 
            gcalls : int 
                Number of calls to gradient/score. 
            hcalls : int 
                Number of calls to hessian. 
            warnflag : int 
                1: Maximum number of iterations exceeded. 
            converged : bool 
                True: converged. False: did not converge. 
            allvecs : list 
                Results at each iteration. 
        &quot;&quot;&quot;</span>

    <span class="s3"># by default we use normal distribution</span>
    <span class="s3"># can be overwritten by instances or subclasses</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">model</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">normalized_cov_params=</span><span class="s0">None, </span><span class="s1">scale=</span><span class="s4">1.</span><span class="s0">,</span>
                 <span class="s1">**kwargs):</span>
        <span class="s1">super(LikelihoodModelResults</span><span class="s0">, </span><span class="s1">self).__init__(model</span><span class="s0">, </span><span class="s1">params)</span>
        <span class="s1">self.normalized_cov_params = normalized_cov_params</span>
        <span class="s1">self.scale = scale</span>
        <span class="s1">self._use_t = </span><span class="s0">False</span>
        <span class="s3"># robust covariance</span>
        <span class="s3"># We put cov_type in kwargs so subclasses can decide in fit whether to</span>
        <span class="s3"># use this generic implementation</span>
        <span class="s0">if </span><span class="s2">'use_t' </span><span class="s0">in </span><span class="s1">kwargs:</span>
            <span class="s1">use_t = kwargs[</span><span class="s2">'use_t'</span><span class="s1">]</span>
            <span class="s1">self.use_t = use_t </span><span class="s0">if </span><span class="s1">use_t </span><span class="s0">is not None else False</span>
        <span class="s0">if </span><span class="s2">'cov_type' </span><span class="s0">in </span><span class="s1">kwargs:</span>
            <span class="s1">cov_type = kwargs.get(</span><span class="s2">'cov_type'</span><span class="s0">, </span><span class="s2">'nonrobust'</span><span class="s1">)</span>
            <span class="s1">cov_kwds = kwargs.get(</span><span class="s2">'cov_kwds'</span><span class="s0">, </span><span class="s1">{})</span>

            <span class="s0">if </span><span class="s1">cov_type == </span><span class="s2">'nonrobust'</span><span class="s1">:</span>
                <span class="s1">self.cov_type = </span><span class="s2">'nonrobust'</span>
                <span class="s1">self.cov_kwds = {</span><span class="s2">'description'</span><span class="s1">: </span><span class="s2">'Standard Errors assume that the ' </span><span class="s1">+</span>
                                 <span class="s2">'covariance matrix of the errors is correctly ' </span><span class="s1">+</span>
                                 <span class="s2">'specified.'</span><span class="s1">}</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">from </span><span class="s1">statsmodels.base.covtype </span><span class="s0">import </span><span class="s1">get_robustcov_results</span>
                <span class="s0">if </span><span class="s1">cov_kwds </span><span class="s0">is None</span><span class="s1">:</span>
                    <span class="s1">cov_kwds = {}</span>
                <span class="s1">use_t = self.use_t</span>
                <span class="s3"># TODO: we should not need use_t in get_robustcov_results</span>
                <span class="s1">get_robustcov_results(self</span><span class="s0">, </span><span class="s1">cov_type=cov_type</span><span class="s0">, </span><span class="s1">use_self=</span><span class="s0">True,</span>
                                      <span class="s1">use_t=use_t</span><span class="s0">, </span><span class="s1">**cov_kwds)</span>

    <span class="s0">def </span><span class="s1">normalized_cov_params(self):</span>
        <span class="s5">&quot;&quot;&quot;See specific model class docstring&quot;&quot;&quot;</span>
        <span class="s0">raise </span><span class="s1">NotImplementedError</span>

    <span class="s0">def </span><span class="s1">_get_robustcov_results(self</span><span class="s0">, </span><span class="s1">cov_type=</span><span class="s2">'nonrobust'</span><span class="s0">, </span><span class="s1">use_self=</span><span class="s0">True,</span>
                               <span class="s1">use_t=</span><span class="s0">None, </span><span class="s1">**cov_kwds):</span>
        <span class="s0">if </span><span class="s1">use_self </span><span class="s0">is False</span><span class="s1">:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;use_self should have been removed long ago.  &quot;</span>
                             <span class="s2">&quot;See GH#4401&quot;</span><span class="s1">)</span>
        <span class="s0">from </span><span class="s1">statsmodels.base.covtype </span><span class="s0">import </span><span class="s1">get_robustcov_results</span>
        <span class="s0">if </span><span class="s1">cov_kwds </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">cov_kwds = {}</span>

        <span class="s0">if </span><span class="s1">cov_type == </span><span class="s2">'nonrobust'</span><span class="s1">:</span>
            <span class="s1">self.cov_type = </span><span class="s2">'nonrobust'</span>
            <span class="s1">self.cov_kwds = {</span><span class="s2">'description'</span><span class="s1">: </span><span class="s2">'Standard Errors assume that the ' </span><span class="s1">+</span>
                             <span class="s2">'covariance matrix of the errors is correctly ' </span><span class="s1">+</span>
                             <span class="s2">'specified.'</span><span class="s1">}</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s3"># TODO: we should not need use_t in get_robustcov_results</span>
            <span class="s1">get_robustcov_results(self</span><span class="s0">, </span><span class="s1">cov_type=cov_type</span><span class="s0">, </span><span class="s1">use_self=</span><span class="s0">True,</span>
                                  <span class="s1">use_t=use_t</span><span class="s0">, </span><span class="s1">**cov_kwds)</span>
    <span class="s1">@property</span>
    <span class="s0">def </span><span class="s1">use_t(self):</span>
        <span class="s5">&quot;&quot;&quot;Flag indicating to use the Student's distribution in inference.&quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">self._use_t</span>

    <span class="s1">@use_t.setter</span>
    <span class="s0">def </span><span class="s1">use_t(self</span><span class="s0">, </span><span class="s1">value):</span>
        <span class="s1">self._use_t = bool(value)</span>

    <span class="s1">@cached_value</span>
    <span class="s0">def </span><span class="s1">llf(self):</span>
        <span class="s5">&quot;&quot;&quot;Log-likelihood of model&quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">self.model.loglike(self.params)</span>

    <span class="s1">@cached_value</span>
    <span class="s0">def </span><span class="s1">bse(self):</span>
        <span class="s5">&quot;&quot;&quot;The standard errors of the parameter estimates.&quot;&quot;&quot;</span>
        <span class="s3"># Issue 3299</span>
        <span class="s0">if </span><span class="s1">((</span><span class="s0">not </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s2">'cov_params_default'</span><span class="s1">)) </span><span class="s0">and</span>
                <span class="s1">(self.normalized_cov_params </span><span class="s0">is None</span><span class="s1">)):</span>
            <span class="s1">bse_ = np.empty(len(self.params))</span>
            <span class="s1">bse_[:] = np.nan</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">with </span><span class="s1">warnings.catch_warnings():</span>
                <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;ignore&quot;</span><span class="s0">, </span><span class="s1">RuntimeWarning)</span>
                <span class="s1">bse_ = np.sqrt(np.diag(self.cov_params()))</span>
        <span class="s0">return </span><span class="s1">bse_</span>

    <span class="s1">@cached_value</span>
    <span class="s0">def </span><span class="s1">tvalues(self):</span>
        <span class="s5">&quot;&quot;&quot; 
        Return the t-statistic for a given parameter estimate. 
        &quot;&quot;&quot;</span>
        <span class="s0">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;ignore&quot;</span><span class="s0">, </span><span class="s1">RuntimeWarning)</span>
            <span class="s0">return </span><span class="s1">self.params / self.bse</span>

    <span class="s1">@cached_value</span>
    <span class="s0">def </span><span class="s1">pvalues(self):</span>
        <span class="s5">&quot;&quot;&quot;The two-tailed p values for the t-stats of the params.&quot;&quot;&quot;</span>
        <span class="s0">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;ignore&quot;</span><span class="s0">, </span><span class="s1">RuntimeWarning)</span>
            <span class="s0">if </span><span class="s1">self.use_t:</span>
                <span class="s1">df_resid = getattr(self</span><span class="s0">, </span><span class="s2">'df_resid_inference'</span><span class="s0">, </span><span class="s1">self.df_resid)</span>
                <span class="s0">return </span><span class="s1">stats.t.sf(np.abs(self.tvalues)</span><span class="s0">, </span><span class="s1">df_resid) * </span><span class="s4">2</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">return </span><span class="s1">stats.norm.sf(np.abs(self.tvalues)) * </span><span class="s4">2</span>

    <span class="s0">def </span><span class="s1">cov_params(self</span><span class="s0">, </span><span class="s1">r_matrix=</span><span class="s0">None, </span><span class="s1">column=</span><span class="s0">None, </span><span class="s1">scale=</span><span class="s0">None, </span><span class="s1">cov_p=</span><span class="s0">None,</span>
                   <span class="s1">other=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Compute the variance/covariance matrix. 
 
        The variance/covariance matrix can be of a linear contrast of the 
        estimated parameters or all params multiplied by scale which will 
        usually be an estimate of sigma^2.  Scale is assumed to be a scalar. 
 
        Parameters 
        ---------- 
        r_matrix : array_like 
            Can be 1d, or 2d.  Can be used alone or with other. 
        column : array_like, optional 
            Must be used on its own.  Can be 0d or 1d see below. 
        scale : float, optional 
            Can be specified or not.  Default is None, which means that 
            the scale argument is taken from the model. 
        cov_p : ndarray, optional 
            The covariance of the parameters. If not provided, this value is 
            read from `self.normalized_cov_params` or 
            `self.cov_params_default`. 
        other : array_like, optional 
            Can be used when r_matrix is specified. 
 
        Returns 
        ------- 
        ndarray 
            The covariance matrix of the parameter estimates or of linear 
            combination of parameter estimates. See Notes. 
 
        Notes 
        ----- 
        (The below are assumed to be in matrix notation.) 
 
        If no argument is specified returns the covariance matrix of a model 
        ``(scale)*(X.T X)^(-1)`` 
 
        If contrast is specified it pre and post-multiplies as follows 
        ``(scale) * r_matrix (X.T X)^(-1) r_matrix.T`` 
 
        If contrast and other are specified returns 
        ``(scale) * r_matrix (X.T X)^(-1) other.T`` 
 
        If column is specified returns 
        ``(scale) * (X.T X)^(-1)[column,column]`` if column is 0d 
 
        OR 
 
        ``(scale) * (X.T X)^(-1)[column][:,column]`` if column is 1d 
        &quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">(hasattr(self</span><span class="s0">, </span><span class="s2">'mle_settings'</span><span class="s1">) </span><span class="s0">and</span>
                <span class="s1">self.mle_settings[</span><span class="s2">'optimizer'</span><span class="s1">] </span><span class="s0">in </span><span class="s1">[</span><span class="s2">'l1'</span><span class="s0">, </span><span class="s2">'l1_cvxopt_cp'</span><span class="s1">]):</span>
            <span class="s1">dot_fun = nan_dot</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">dot_fun = np.dot</span>

        <span class="s0">if </span><span class="s1">(cov_p </span><span class="s0">is None and </span><span class="s1">self.normalized_cov_params </span><span class="s0">is None and</span>
                <span class="s0">not </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s2">'cov_params_default'</span><span class="s1">)):</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">'need covariance of parameters for computing '</span>
                             <span class="s2">'(unnormalized) covariances'</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">column </span><span class="s0">is not None and </span><span class="s1">(r_matrix </span><span class="s0">is not None or </span><span class="s1">other </span><span class="s0">is not None</span><span class="s1">):</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">'Column should be specified without other '</span>
                             <span class="s2">'arguments.'</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">other </span><span class="s0">is not None and </span><span class="s1">r_matrix </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">'other can only be specified with r_matrix'</span><span class="s1">)</span>

        <span class="s0">if </span><span class="s1">cov_p </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s2">'cov_params_default'</span><span class="s1">):</span>
                <span class="s1">cov_p = self.cov_params_default</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">if </span><span class="s1">scale </span><span class="s0">is None</span><span class="s1">:</span>
                    <span class="s1">scale = self.scale</span>
                <span class="s1">cov_p = self.normalized_cov_params * scale</span>

        <span class="s0">if </span><span class="s1">column </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">column = np.asarray(column)</span>
            <span class="s0">if </span><span class="s1">column.shape == ():</span>
                <span class="s0">return </span><span class="s1">cov_p[column</span><span class="s0">, </span><span class="s1">column]</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">return </span><span class="s1">cov_p[column[:</span><span class="s0">, None</span><span class="s1">]</span><span class="s0">, </span><span class="s1">column]</span>
        <span class="s0">elif </span><span class="s1">r_matrix </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">r_matrix = np.asarray(r_matrix)</span>
            <span class="s0">if </span><span class="s1">r_matrix.shape == ():</span>
                <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;r_matrix should be 1d or 2d&quot;</span><span class="s1">)</span>
            <span class="s0">if </span><span class="s1">other </span><span class="s0">is None</span><span class="s1">:</span>
                <span class="s1">other = r_matrix</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">other = np.asarray(other)</span>
            <span class="s1">tmp = dot_fun(r_matrix</span><span class="s0">, </span><span class="s1">dot_fun(cov_p</span><span class="s0">, </span><span class="s1">np.transpose(other)))</span>
            <span class="s0">return </span><span class="s1">tmp</span>
        <span class="s0">else</span><span class="s1">:  </span><span class="s3"># if r_matrix is None and column is None:</span>
            <span class="s0">return </span><span class="s1">cov_p</span>

    <span class="s3"># TODO: make sure this works as needed for GLMs</span>
    <span class="s0">def </span><span class="s1">t_test(self</span><span class="s0">, </span><span class="s1">r_matrix</span><span class="s0">, </span><span class="s1">cov_p=</span><span class="s0">None, </span><span class="s1">use_t=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Compute a t-test for a each linear hypothesis of the form Rb = q. 
 
        Parameters 
        ---------- 
        r_matrix : {array_like, str, tuple} 
            One of: 
 
            - array : If an array is given, a p x k 2d array or length k 1d 
              array specifying the linear restrictions. It is assumed 
              that the linear combination is equal to zero. 
            - str : The full hypotheses to test can be given as a string. 
              See the examples. 
            - tuple : A tuple of arrays in the form (R, q). If q is given, 
              can be either a scalar or a length p row vector. 
 
        cov_p : array_like, optional 
            An alternative estimate for the parameter covariance matrix. 
            If None is given, self.normalized_cov_params is used. 
        use_t : bool, optional 
            If use_t is None, then the default of the model is used. If use_t 
            is True, then the p-values are based on the t distribution. If 
            use_t is False, then the p-values are based on the normal 
            distribution. 
 
        Returns 
        ------- 
        ContrastResults 
            The results for the test are attributes of this results instance. 
            The available results have the same elements as the parameter table 
            in `summary()`. 
 
        See Also 
        -------- 
        tvalues : Individual t statistics for the estimated parameters. 
        f_test : Perform an F tests on model parameters. 
        patsy.DesignInfo.linear_constraint : Specify a linear constraint. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; import numpy as np 
        &gt;&gt;&gt; import statsmodels.api as sm 
        &gt;&gt;&gt; data = sm.datasets.longley.load() 
        &gt;&gt;&gt; data.exog = sm.add_constant(data.exog) 
        &gt;&gt;&gt; results = sm.OLS(data.endog, data.exog).fit() 
        &gt;&gt;&gt; r = np.zeros_like(results.params) 
        &gt;&gt;&gt; r[5:] = [1,-1] 
        &gt;&gt;&gt; print(r) 
        [ 0.  0.  0.  0.  0.  1. -1.] 
 
        r tests that the coefficients on the 5th and 6th independent 
        variable are the same. 
 
        &gt;&gt;&gt; T_test = results.t_test(r) 
        &gt;&gt;&gt; print(T_test) 
                                     Test for Constraints 
        ============================================================================== 
                         coef    std err          t      P&gt;|t|      [0.025      0.975] 
        ------------------------------------------------------------------------------ 
        c0         -1829.2026    455.391     -4.017      0.003   -2859.368    -799.037 
        ============================================================================== 
        &gt;&gt;&gt; T_test.effect 
        -1829.2025687192481 
        &gt;&gt;&gt; T_test.sd 
        455.39079425193762 
        &gt;&gt;&gt; T_test.tvalue 
        -4.0167754636411717 
        &gt;&gt;&gt; T_test.pvalue 
        0.0015163772380899498 
 
        Alternatively, you can specify the hypothesis tests using a string 
 
        &gt;&gt;&gt; from statsmodels.formula.api import ols 
        &gt;&gt;&gt; dta = sm.datasets.longley.load_pandas().data 
        &gt;&gt;&gt; formula = 'TOTEMP ~ GNPDEFL + GNP + UNEMP + ARMED + POP + YEAR' 
        &gt;&gt;&gt; results = ols(formula, dta).fit() 
        &gt;&gt;&gt; hypotheses = 'GNPDEFL = GNP, UNEMP = 2, YEAR/1829 = 1' 
        &gt;&gt;&gt; t_test = results.t_test(hypotheses) 
        &gt;&gt;&gt; print(t_test) 
                                     Test for Constraints 
        ============================================================================== 
                         coef    std err          t      P&gt;|t|      [0.025      0.975] 
        ------------------------------------------------------------------------------ 
        c0            15.0977     84.937      0.178      0.863    -177.042     207.238 
        c1            -2.0202      0.488     -8.231      0.000      -3.125      -0.915 
        c2             1.0001      0.249      0.000      1.000       0.437       1.563 
        ============================================================================== 
        &quot;&quot;&quot;</span>
        <span class="s0">from </span><span class="s1">patsy </span><span class="s0">import </span><span class="s1">DesignInfo</span>
        <span class="s1">use_t = bool_like(use_t</span><span class="s0">, </span><span class="s2">&quot;use_t&quot;</span><span class="s0">, </span><span class="s1">strict=</span><span class="s0">True, </span><span class="s1">optional=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s1">names = self.model.data.cov_names</span>
        <span class="s1">LC = DesignInfo(names).linear_constraint(r_matrix)</span>
        <span class="s1">r_matrix</span><span class="s0">, </span><span class="s1">q_matrix = LC.coefs</span><span class="s0">, </span><span class="s1">LC.constants</span>
        <span class="s1">num_ttests = r_matrix.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">num_params = r_matrix.shape[</span><span class="s4">1</span><span class="s1">]</span>

        <span class="s0">if </span><span class="s1">(cov_p </span><span class="s0">is None and </span><span class="s1">self.normalized_cov_params </span><span class="s0">is None and</span>
                <span class="s0">not </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s2">'cov_params_default'</span><span class="s1">)):</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">'Need covariance of parameters for computing '</span>
                             <span class="s2">'T statistics'</span><span class="s1">)</span>
        <span class="s1">params = self.params.ravel()</span>
        <span class="s0">if </span><span class="s1">num_params != params.shape[</span><span class="s4">0</span><span class="s1">]:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">'r_matrix and params are not aligned'</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">q_matrix </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">q_matrix = np.zeros(num_ttests)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">q_matrix = np.asarray(q_matrix)</span>
            <span class="s1">q_matrix = q_matrix.squeeze()</span>
        <span class="s0">if </span><span class="s1">q_matrix.size &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">q_matrix.shape[</span><span class="s4">0</span><span class="s1">] != num_ttests:</span>
                <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;r_matrix and q_matrix must have the same &quot;</span>
                                 <span class="s2">&quot;number of rows&quot;</span><span class="s1">)</span>

        <span class="s0">if </span><span class="s1">use_t </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s3"># switch to use_t false if undefined</span>
            <span class="s1">use_t = (hasattr(self</span><span class="s0">, </span><span class="s2">'use_t'</span><span class="s1">) </span><span class="s0">and </span><span class="s1">self.use_t)</span>

        <span class="s1">_effect = np.dot(r_matrix</span><span class="s0">, </span><span class="s1">params)</span>

        <span class="s3"># Perform the test</span>
        <span class="s0">if </span><span class="s1">num_ttests &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">_sd = np.sqrt(np.diag(self.cov_params(</span>
                <span class="s1">r_matrix=r_matrix</span><span class="s0">, </span><span class="s1">cov_p=cov_p)))</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">_sd = np.sqrt(self.cov_params(r_matrix=r_matrix</span><span class="s0">, </span><span class="s1">cov_p=cov_p))</span>
        <span class="s1">_t = (_effect - q_matrix) * recipr(_sd)</span>

        <span class="s1">df_resid = getattr(self</span><span class="s0">, </span><span class="s2">'df_resid_inference'</span><span class="s0">, </span><span class="s1">self.df_resid)</span>

        <span class="s0">if </span><span class="s1">use_t:</span>
            <span class="s0">return </span><span class="s1">ContrastResults(effect=_effect</span><span class="s0">, </span><span class="s1">t=_t</span><span class="s0">, </span><span class="s1">sd=_sd</span><span class="s0">,</span>
                                   <span class="s1">df_denom=df_resid)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">ContrastResults(effect=_effect</span><span class="s0">, </span><span class="s1">statistic=_t</span><span class="s0">, </span><span class="s1">sd=_sd</span><span class="s0">,</span>
                                   <span class="s1">df_denom=df_resid</span><span class="s0">,</span>
                                   <span class="s1">distribution=</span><span class="s2">'norm'</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">f_test(self</span><span class="s0">, </span><span class="s1">r_matrix</span><span class="s0">, </span><span class="s1">cov_p=</span><span class="s0">None, </span><span class="s1">invcov=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Compute the F-test for a joint linear hypothesis. 
 
        This is a special case of `wald_test` that always uses the F 
        distribution. 
 
        Parameters 
        ---------- 
        r_matrix : {array_like, str, tuple} 
            One of: 
 
            - array : An r x k array where r is the number of restrictions to 
              test and k is the number of regressors. It is assumed 
              that the linear combination is equal to zero. 
            - str : The full hypotheses to test can be given as a string. 
              See the examples. 
            - tuple : A tuple of arrays in the form (R, q), ``q`` can be 
              either a scalar or a length k row vector. 
 
        cov_p : array_like, optional 
            An alternative estimate for the parameter covariance matrix. 
            If None is given, self.normalized_cov_params is used. 
        invcov : array_like, optional 
            A q x q array to specify an inverse covariance matrix based on a 
            restrictions matrix. 
 
        Returns 
        ------- 
        ContrastResults 
            The results for the test are attributes of this results instance. 
 
        See Also 
        -------- 
        t_test : Perform a single hypothesis test. 
        wald_test : Perform a Wald-test using a quadratic form. 
        statsmodels.stats.contrast.ContrastResults : Test results. 
        patsy.DesignInfo.linear_constraint : Specify a linear constraint. 
 
        Notes 
        ----- 
        The matrix `r_matrix` is assumed to be non-singular. More precisely, 
 
        r_matrix (pX pX.T) r_matrix.T 
 
        is assumed invertible. Here, pX is the generalized inverse of the 
        design matrix of the model. There can be problems in non-OLS models 
        where the rank of the covariance of the noise is not full. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; import numpy as np 
        &gt;&gt;&gt; import statsmodels.api as sm 
        &gt;&gt;&gt; data = sm.datasets.longley.load() 
        &gt;&gt;&gt; data.exog = sm.add_constant(data.exog) 
        &gt;&gt;&gt; results = sm.OLS(data.endog, data.exog).fit() 
        &gt;&gt;&gt; A = np.identity(len(results.params)) 
        &gt;&gt;&gt; A = A[1:,:] 
 
        This tests that each coefficient is jointly statistically 
        significantly different from zero. 
 
        &gt;&gt;&gt; print(results.f_test(A)) 
        &lt;F test: F=array([[ 330.28533923]]), p=4.984030528700946e-10, df_denom=9, df_num=6&gt; 
 
        Compare this to 
 
        &gt;&gt;&gt; results.fvalue 
        330.2853392346658 
        &gt;&gt;&gt; results.f_pvalue 
        4.98403096572e-10 
 
        &gt;&gt;&gt; B = np.array(([0,0,1,-1,0,0,0],[0,0,0,0,0,1,-1])) 
 
        This tests that the coefficient on the 2nd and 3rd regressors are 
        equal and jointly that the coefficient on the 5th and 6th regressors 
        are equal. 
 
        &gt;&gt;&gt; print(results.f_test(B)) 
        &lt;F test: F=array([[ 9.74046187]]), p=0.005605288531708235, df_denom=9, df_num=2&gt; 
 
        Alternatively, you can specify the hypothesis tests using a string 
 
        &gt;&gt;&gt; from statsmodels.datasets import longley 
        &gt;&gt;&gt; from statsmodels.formula.api import ols 
        &gt;&gt;&gt; dta = longley.load_pandas().data 
        &gt;&gt;&gt; formula = 'TOTEMP ~ GNPDEFL + GNP + UNEMP + ARMED + POP + YEAR' 
        &gt;&gt;&gt; results = ols(formula, dta).fit() 
        &gt;&gt;&gt; hypotheses = '(GNPDEFL = GNP), (UNEMP = 2), (YEAR/1829 = 1)' 
        &gt;&gt;&gt; f_test = results.f_test(hypotheses) 
        &gt;&gt;&gt; print(f_test) 
        &lt;F test: F=array([[ 144.17976065]]), p=6.322026217355609e-08, df_denom=9, df_num=3&gt; 
        &quot;&quot;&quot;</span>
        <span class="s1">res = self.wald_test(r_matrix</span><span class="s0">, </span><span class="s1">cov_p=cov_p</span><span class="s0">, </span><span class="s1">invcov=invcov</span><span class="s0">, </span><span class="s1">use_f=</span><span class="s0">True, </span><span class="s1">scalar=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">res</span>

    <span class="s3"># TODO: untested for GLMs?</span>
    <span class="s0">def </span><span class="s1">wald_test(self</span><span class="s0">, </span><span class="s1">r_matrix</span><span class="s0">, </span><span class="s1">cov_p=</span><span class="s0">None, </span><span class="s1">invcov=</span><span class="s0">None,</span>
                  <span class="s1">use_f=</span><span class="s0">None, </span><span class="s1">df_constraints=</span><span class="s0">None, </span><span class="s1">scalar=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Compute a Wald-test for a joint linear hypothesis. 
 
        Parameters 
        ---------- 
        r_matrix : {array_like, str, tuple} 
            One of: 
 
            - array : An r x k array where r is the number of restrictions to 
              test and k is the number of regressors. It is assumed that the 
              linear combination is equal to zero. 
            - str : The full hypotheses to test can be given as a string. 
              See the examples. 
            - tuple : A tuple of arrays in the form (R, q), ``q`` can be 
              either a scalar or a length p row vector. 
 
        cov_p : array_like, optional 
            An alternative estimate for the parameter covariance matrix. 
            If None is given, self.normalized_cov_params is used. 
        invcov : array_like, optional 
            A q x q array to specify an inverse covariance matrix based on a 
            restrictions matrix. 
        use_f : bool 
            If True, then the F-distribution is used. If False, then the 
            asymptotic distribution, chisquare is used. If use_f is None, then 
            the F distribution is used if the model specifies that use_t is True. 
            The test statistic is proportionally adjusted for the distribution 
            by the number of constraints in the hypothesis. 
        df_constraints : int, optional 
            The number of constraints. If not provided the number of 
            constraints is determined from r_matrix. 
        scalar : bool, optional 
            Flag indicating whether the Wald test statistic should be returned 
            as a sclar float. The current behavior is to return an array. 
            This will switch to a scalar float after 0.14 is released. To 
            get the future behavior now, set scalar to True. To silence 
            the warning and retain the legacy behavior, set scalar to 
            False. 
 
        Returns 
        ------- 
        ContrastResults 
            The results for the test are attributes of this results instance. 
 
        See Also 
        -------- 
        f_test : Perform an F tests on model parameters. 
        t_test : Perform a single hypothesis test. 
        statsmodels.stats.contrast.ContrastResults : Test results. 
        patsy.DesignInfo.linear_constraint : Specify a linear constraint. 
 
        Notes 
        ----- 
        The matrix `r_matrix` is assumed to be non-singular. More precisely, 
 
        r_matrix (pX pX.T) r_matrix.T 
 
        is assumed invertible. Here, pX is the generalized inverse of the 
        design matrix of the model. There can be problems in non-OLS models 
        where the rank of the covariance of the noise is not full. 
        &quot;&quot;&quot;</span>
        <span class="s1">use_f = bool_like(use_f</span><span class="s0">, </span><span class="s2">&quot;use_f&quot;</span><span class="s0">, </span><span class="s1">strict=</span><span class="s0">True, </span><span class="s1">optional=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s1">scalar = bool_like(scalar</span><span class="s0">, </span><span class="s2">&quot;scalar&quot;</span><span class="s0">, </span><span class="s1">strict=</span><span class="s0">True, </span><span class="s1">optional=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">use_f </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s3"># switch to use_t false if undefined</span>
            <span class="s1">use_f = (hasattr(self</span><span class="s0">, </span><span class="s2">'use_t'</span><span class="s1">) </span><span class="s0">and </span><span class="s1">self.use_t)</span>

        <span class="s0">from </span><span class="s1">patsy </span><span class="s0">import </span><span class="s1">DesignInfo</span>
        <span class="s1">names = self.model.data.cov_names</span>
        <span class="s1">params = self.params.ravel()</span>
        <span class="s1">LC = DesignInfo(names).linear_constraint(r_matrix)</span>
        <span class="s1">r_matrix</span><span class="s0">, </span><span class="s1">q_matrix = LC.coefs</span><span class="s0">, </span><span class="s1">LC.constants</span>

        <span class="s0">if </span><span class="s1">(self.normalized_cov_params </span><span class="s0">is None and </span><span class="s1">cov_p </span><span class="s0">is None and</span>
                <span class="s1">invcov </span><span class="s0">is None and not </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s2">'cov_params_default'</span><span class="s1">)):</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">'need covariance of parameters for computing '</span>
                             <span class="s2">'F statistics'</span><span class="s1">)</span>

        <span class="s1">cparams = np.dot(r_matrix</span><span class="s0">, </span><span class="s1">params[:</span><span class="s0">, None</span><span class="s1">])</span>
        <span class="s1">J = float(r_matrix.shape[</span><span class="s4">0</span><span class="s1">])  </span><span class="s3"># number of restrictions</span>

        <span class="s0">if </span><span class="s1">q_matrix </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">q_matrix = np.zeros(J)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">q_matrix = np.asarray(q_matrix)</span>
        <span class="s0">if </span><span class="s1">q_matrix.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">q_matrix = q_matrix[:</span><span class="s0">, None</span><span class="s1">]</span>
            <span class="s0">if </span><span class="s1">q_matrix.shape[</span><span class="s4">0</span><span class="s1">] != J:</span>
                <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;r_matrix and q_matrix must have the same &quot;</span>
                                 <span class="s2">&quot;number of rows&quot;</span><span class="s1">)</span>
        <span class="s1">Rbq = cparams - q_matrix</span>
        <span class="s0">if </span><span class="s1">invcov </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">cov_p = self.cov_params(r_matrix=r_matrix</span><span class="s0">, </span><span class="s1">cov_p=cov_p)</span>
            <span class="s0">if </span><span class="s1">np.isnan(cov_p).max():</span>
                <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;r_matrix performs f_test for using &quot;</span>
                                 <span class="s2">&quot;dimensions that are asymptotically &quot;</span>
                                 <span class="s2">&quot;non-normal&quot;</span><span class="s1">)</span>
            <span class="s1">invcov = np.linalg.pinv(cov_p)</span>
            <span class="s1">J_ = np.linalg.matrix_rank(cov_p)</span>
            <span class="s0">if </span><span class="s1">J_ &lt; J:</span>
                <span class="s1">warnings.warn(</span><span class="s2">'covariance of constraints does not have full '</span>
                              <span class="s2">'rank. The number of constraints is %d, but '</span>
                              <span class="s2">'rank is %d' </span><span class="s1">% (J</span><span class="s0">, </span><span class="s1">J_)</span><span class="s0">, </span><span class="s1">ValueWarning)</span>
                <span class="s1">J = J_</span>

        <span class="s3"># TODO streamline computation, we do not need to compute J if given</span>
        <span class="s0">if </span><span class="s1">df_constraints </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s3"># let caller override J by df_constraint</span>
            <span class="s1">J = df_constraints</span>

        <span class="s0">if </span><span class="s1">(hasattr(self</span><span class="s0">, </span><span class="s2">'mle_settings'</span><span class="s1">) </span><span class="s0">and</span>
                <span class="s1">self.mle_settings[</span><span class="s2">'optimizer'</span><span class="s1">] </span><span class="s0">in </span><span class="s1">[</span><span class="s2">'l1'</span><span class="s0">, </span><span class="s2">'l1_cvxopt_cp'</span><span class="s1">]):</span>
            <span class="s1">F = nan_dot(nan_dot(Rbq.T</span><span class="s0">, </span><span class="s1">invcov)</span><span class="s0">, </span><span class="s1">Rbq)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">F = np.dot(np.dot(Rbq.T</span><span class="s0">, </span><span class="s1">invcov)</span><span class="s0">, </span><span class="s1">Rbq)</span>

        <span class="s1">df_resid = getattr(self</span><span class="s0">, </span><span class="s2">'df_resid_inference'</span><span class="s0">, </span><span class="s1">self.df_resid)</span>
        <span class="s0">if </span><span class="s1">scalar </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s2">&quot;The behavior of wald_test will change after 0.14 to returning &quot;</span>
                <span class="s2">&quot;scalar test statistic values. To get the future behavior now, &quot;</span>
                <span class="s2">&quot;set scalar to True. To silence this message while retaining &quot;</span>
                <span class="s2">&quot;the legacy behavior, set scalar to False.&quot;</span><span class="s0">,</span>
                <span class="s1">FutureWarning</span>
            <span class="s1">)</span>
            <span class="s1">scalar = </span><span class="s0">False</span>
        <span class="s0">if </span><span class="s1">scalar </span><span class="s0">and </span><span class="s1">F.size == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">F = float(np.squeeze(F))</span>
        <span class="s0">if </span><span class="s1">use_f:</span>
            <span class="s1">F /= J</span>
            <span class="s0">return </span><span class="s1">ContrastResults(F=F</span><span class="s0">, </span><span class="s1">df_denom=df_resid</span><span class="s0">,</span>
                                   <span class="s1">df_num=J) </span><span class="s3">#invcov.shape[0])</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">ContrastResults(chi2=F</span><span class="s0">, </span><span class="s1">df_denom=J</span><span class="s0">, </span><span class="s1">statistic=F</span><span class="s0">,</span>
                                   <span class="s1">distribution=</span><span class="s2">'chi2'</span><span class="s0">, </span><span class="s1">distargs=(J</span><span class="s0">,</span><span class="s1">))</span>

    <span class="s0">def </span><span class="s1">wald_test_terms(self</span><span class="s0">, </span><span class="s1">skip_single=</span><span class="s0">False, </span><span class="s1">extra_constraints=</span><span class="s0">None,</span>
                        <span class="s1">combine_terms=</span><span class="s0">None, </span><span class="s1">scalar=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Compute a sequence of Wald tests for terms over multiple columns. 
 
        This computes joined Wald tests for the hypothesis that all 
        coefficients corresponding to a `term` are zero. 
        `Terms` are defined by the underlying formula or by string matching. 
 
        Parameters 
        ---------- 
        skip_single : bool 
            If true, then terms that consist only of a single column and, 
            therefore, refers only to a single parameter is skipped. 
            If false, then all terms are included. 
        extra_constraints : ndarray 
            Additional constraints to test. Note that this input has not been 
            tested. 
        combine_terms : {list[str], None} 
            Each string in this list is matched to the name of the terms or 
            the name of the exogenous variables. All columns whose name 
            includes that string are combined in one joint test. 
        scalar : bool, optional 
            Flag indicating whether the Wald test statistic should be returned 
            as a sclar float. The current behavior is to return an array. 
            This will switch to a scalar float after 0.14 is released. To 
            get the future behavior now, set scalar to True. To silence 
            the warning and retain the legacy behavior, set scalar to 
            False. 
 
        Returns 
        ------- 
        WaldTestResults 
            The result instance contains `table` which is a pandas DataFrame 
            with the test results: test statistic, degrees of freedom and 
            pvalues. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; res_ols = ols(&quot;np.log(Days+1) ~ C(Duration, Sum)*C(Weight, Sum)&quot;, data).fit() 
        &gt;&gt;&gt; res_ols.wald_test_terms() 
        &lt;class 'statsmodels.stats.contrast.WaldTestResults'&gt; 
                                                  F                P&gt;F  df constraint  df denom 
        Intercept                        279.754525  2.37985521351e-22              1        51 
        C(Duration, Sum)                   5.367071    0.0245738436636              1        51 
        C(Weight, Sum)                    12.432445  3.99943118767e-05              2        51 
        C(Duration, Sum):C(Weight, Sum)    0.176002      0.83912310946              2        51 
 
        &gt;&gt;&gt; res_poi = Poisson.from_formula(&quot;Days ~ C(Weight) * C(Duration)&quot;, \ 
                                           data).fit(cov_type='HC0') 
        &gt;&gt;&gt; wt = res_poi.wald_test_terms(skip_single=False, \ 
                                         combine_terms=['Duration', 'Weight']) 
        &gt;&gt;&gt; print(wt) 
                                    chi2             P&gt;chi2  df constraint 
        Intercept              15.695625  7.43960374424e-05              1 
        C(Weight)              16.132616  0.000313940174705              2 
        C(Duration)             1.009147     0.315107378931              1 
        C(Weight):C(Duration)   0.216694     0.897315972824              2 
        Duration               11.187849     0.010752286833              3 
        Weight                 30.263368  4.32586407145e-06              4 
        &quot;&quot;&quot;</span>
        <span class="s3"># lazy import</span>
        <span class="s0">from </span><span class="s1">collections </span><span class="s0">import </span><span class="s1">defaultdict</span>

        <span class="s1">result = self</span>
        <span class="s0">if </span><span class="s1">extra_constraints </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">extra_constraints = []</span>
        <span class="s0">if </span><span class="s1">combine_terms </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">combine_terms = []</span>
        <span class="s1">design_info = getattr(result.model.data</span><span class="s0">, </span><span class="s2">'design_info'</span><span class="s0">, None</span><span class="s1">)</span>

        <span class="s0">if </span><span class="s1">design_info </span><span class="s0">is None and </span><span class="s1">extra_constraints </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">'no constraints, nothing to do'</span><span class="s1">)</span>

        <span class="s1">identity = np.eye(len(result.params))</span>
        <span class="s1">constraints = []</span>
        <span class="s1">combined = defaultdict(list)</span>
        <span class="s0">if </span><span class="s1">design_info </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s0">for </span><span class="s1">term </span><span class="s0">in </span><span class="s1">design_info.terms:</span>
                <span class="s1">cols = design_info.slice(term)</span>
                <span class="s1">name = term.name()</span>
                <span class="s1">constraint_matrix = identity[cols]</span>

                <span class="s3"># check if in combined</span>
                <span class="s0">for </span><span class="s1">cname </span><span class="s0">in </span><span class="s1">combine_terms:</span>
                    <span class="s0">if </span><span class="s1">cname </span><span class="s0">in </span><span class="s1">name:</span>
                        <span class="s1">combined[cname].append(constraint_matrix)</span>

                <span class="s1">k_constraint = constraint_matrix.shape[</span><span class="s4">0</span><span class="s1">]</span>
                <span class="s0">if </span><span class="s1">skip_single:</span>
                    <span class="s0">if </span><span class="s1">k_constraint == </span><span class="s4">1</span><span class="s1">:</span>
                        <span class="s0">continue</span>

                <span class="s1">constraints.append((name</span><span class="s0">, </span><span class="s1">constraint_matrix))</span>

            <span class="s1">combined_constraints = []</span>
            <span class="s0">for </span><span class="s1">cname </span><span class="s0">in </span><span class="s1">combine_terms:</span>
                <span class="s1">combined_constraints.append((cname</span><span class="s0">, </span><span class="s1">np.vstack(combined[cname])))</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s3"># check by exog/params names if there is no formula info</span>
            <span class="s0">for </span><span class="s1">col</span><span class="s0">, </span><span class="s1">name </span><span class="s0">in </span><span class="s1">enumerate(result.model.exog_names):</span>
                <span class="s1">constraint_matrix = np.atleast_2d(identity[col])</span>

                <span class="s3"># check if in combined</span>
                <span class="s0">for </span><span class="s1">cname </span><span class="s0">in </span><span class="s1">combine_terms:</span>
                    <span class="s0">if </span><span class="s1">cname </span><span class="s0">in </span><span class="s1">name:</span>
                        <span class="s1">combined[cname].append(constraint_matrix)</span>

                <span class="s0">if </span><span class="s1">skip_single:</span>
                    <span class="s0">continue</span>

                <span class="s1">constraints.append((name</span><span class="s0">, </span><span class="s1">constraint_matrix))</span>

            <span class="s1">combined_constraints = []</span>
            <span class="s0">for </span><span class="s1">cname </span><span class="s0">in </span><span class="s1">combine_terms:</span>
                <span class="s1">combined_constraints.append((cname</span><span class="s0">, </span><span class="s1">np.vstack(combined[cname])))</span>

        <span class="s1">use_t = result.use_t</span>
        <span class="s1">distribution = [</span><span class="s2">'chi2'</span><span class="s0">, </span><span class="s2">'F'</span><span class="s1">][use_t]</span>

        <span class="s1">res_wald = []</span>
        <span class="s1">index = []</span>
        <span class="s0">for </span><span class="s1">name</span><span class="s0">, </span><span class="s1">constraint </span><span class="s0">in </span><span class="s1">constraints + combined_constraints + extra_constraints:</span>
            <span class="s1">wt = result.wald_test(constraint</span><span class="s0">, </span><span class="s1">scalar=scalar)</span>
            <span class="s1">row = [wt.statistic</span><span class="s0">, </span><span class="s1">wt.pvalue</span><span class="s0">, </span><span class="s1">constraint.shape[</span><span class="s4">0</span><span class="s1">]]</span>
            <span class="s0">if </span><span class="s1">use_t:</span>
                <span class="s1">row.append(wt.df_denom)</span>
            <span class="s1">res_wald.append(row)</span>
            <span class="s1">index.append(name)</span>

        <span class="s3"># distribution nerutral names</span>
        <span class="s1">col_names = [</span><span class="s2">'statistic'</span><span class="s0">, </span><span class="s2">'pvalue'</span><span class="s0">, </span><span class="s2">'df_constraint'</span><span class="s1">]</span>
        <span class="s0">if </span><span class="s1">use_t:</span>
            <span class="s1">col_names.append(</span><span class="s2">'df_denom'</span><span class="s1">)</span>
        <span class="s3"># TODO: maybe move DataFrame creation to results class</span>
        <span class="s0">from </span><span class="s1">pandas </span><span class="s0">import </span><span class="s1">DataFrame</span>
        <span class="s1">table = DataFrame(res_wald</span><span class="s0">, </span><span class="s1">index=index</span><span class="s0">, </span><span class="s1">columns=col_names)</span>
        <span class="s1">res = WaldTestResults(</span><span class="s0">None, </span><span class="s1">distribution</span><span class="s0">, None, </span><span class="s1">table=table)</span>
        <span class="s3"># TODO: remove temp again, added for testing</span>
        <span class="s1">res.temp = constraints + combined_constraints + extra_constraints</span>
        <span class="s0">return </span><span class="s1">res</span>

    <span class="s0">def </span><span class="s1">t_test_pairwise(self</span><span class="s0">, </span><span class="s1">term_name</span><span class="s0">, </span><span class="s1">method=</span><span class="s2">'hs'</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s4">0.05</span><span class="s0">,</span>
                        <span class="s1">factor_labels=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Perform pairwise t_test with multiple testing corrected p-values. 
 
        This uses the formula design_info encoding contrast matrix and should 
        work for all encodings of a main effect. 
 
        Parameters 
        ---------- 
        term_name : str 
            The name of the term for which pairwise comparisons are computed. 
            Term names for categorical effects are created by patsy and 
            correspond to the main part of the exog names. 
        method : {str, list[str]} 
            The multiple testing p-value correction to apply. The default is 
            'hs'. See stats.multipletesting. 
        alpha : float 
            The significance level for multiple testing reject decision. 
        factor_labels : {list[str], None} 
            Labels for the factor levels used for pairwise labels. If not 
            provided, then the labels from the formula design_info are used. 
 
        Returns 
        ------- 
        MultiCompResult 
            The results are stored as attributes, the main attributes are the 
            following two. Other attributes are added for debugging purposes 
            or as background information. 
 
            - result_frame : pandas DataFrame with t_test results and multiple 
              testing corrected p-values. 
            - contrasts : matrix of constraints of the null hypothesis in the 
              t_test. 
 
        Notes 
        ----- 
        Status: experimental. Currently only checked for treatment coding with 
        and without specified reference level. 
 
        Currently there are no multiple testing corrected confidence intervals 
        available. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; res = ols(&quot;np.log(Days+1) ~ C(Weight) + C(Duration)&quot;, data).fit() 
        &gt;&gt;&gt; pw = res.t_test_pairwise(&quot;C(Weight)&quot;) 
        &gt;&gt;&gt; pw.result_frame 
                 coef   std err         t         P&gt;|t|  Conf. Int. Low 
        2-1  0.632315  0.230003  2.749157  8.028083e-03        0.171563 
        3-1  1.302555  0.230003  5.663201  5.331513e-07        0.841803 
        3-2  0.670240  0.230003  2.914044  5.119126e-03        0.209488 
             Conf. Int. Upp.  pvalue-hs reject-hs 
        2-1         1.093067   0.010212      True 
        3-1         1.763307   0.000002      True 
        3-2         1.130992   0.010212      True 
        &quot;&quot;&quot;</span>
        <span class="s1">res = t_test_pairwise(self</span><span class="s0">, </span><span class="s1">term_name</span><span class="s0">, </span><span class="s1">method=method</span><span class="s0">, </span><span class="s1">alpha=alpha</span><span class="s0">,</span>
                              <span class="s1">factor_labels=factor_labels)</span>
        <span class="s0">return </span><span class="s1">res</span>

    <span class="s0">def </span><span class="s1">_get_wald_nonlinear(self</span><span class="s0">, </span><span class="s1">func</span><span class="s0">, </span><span class="s1">deriv=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Experimental method for nonlinear prediction and tests 
 
        Parameters 
        ---------- 
        func : callable, f(params) 
            nonlinear function of the estimation parameters. The return of 
            the function can be vector valued, i.e. a 1-D array 
        deriv : function or None 
            first derivative or Jacobian of func. If deriv is None, then a 
            numerical derivative will be used. If func returns a 1-D array, 
            then the `deriv` should have rows corresponding to the elements 
            of the return of func. 
 
        Returns 
        ------- 
        nl : instance of `NonlinearDeltaCov` with attributes and methods to 
            calculate the results for the prediction or tests 
 
        &quot;&quot;&quot;</span>
        <span class="s0">from </span><span class="s1">statsmodels.stats._delta_method </span><span class="s0">import </span><span class="s1">NonlinearDeltaCov</span>
        <span class="s1">func_args = </span><span class="s0">None  </span><span class="s3"># TODO: not yet implemented, maybe skip - use partial</span>
        <span class="s1">nl = NonlinearDeltaCov(func</span><span class="s0">, </span><span class="s1">self.params</span><span class="s0">, </span><span class="s1">self.cov_params()</span><span class="s0">,</span>
                               <span class="s1">deriv=deriv</span><span class="s0">, </span><span class="s1">func_args=func_args)</span>

        <span class="s0">return </span><span class="s1">nl</span>

    <span class="s0">def </span><span class="s1">conf_int(self</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s4">.05</span><span class="s0">, </span><span class="s1">cols=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Construct confidence interval for the fitted parameters. 
 
        Parameters 
        ---------- 
        alpha : float, optional 
            The significance level for the confidence interval. The default 
            `alpha` = .05 returns a 95% confidence interval. 
        cols : array_like, optional 
            Specifies which confidence intervals to return. 
 
        .. deprecated: 0.13 
 
           cols is deprecated and will be removed after 0.14 is released. 
           cols only works when inputs are NumPy arrays and will fail 
           when using pandas Series or DataFrames as input. You can 
           subset the confidence intervals using slices. 
 
        Returns 
        ------- 
        array_like 
            Each row contains [lower, upper] limits of the confidence interval 
            for the corresponding parameter. The first column contains all 
            lower, the second column contains all upper limits. 
 
        Notes 
        ----- 
        The confidence interval is based on the standard normal distribution 
        if self.use_t is False. If self.use_t is True, then uses a Student's t 
        with self.df_resid_inference (or self.df_resid if df_resid_inference is 
        not defined) degrees of freedom. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; import statsmodels.api as sm 
        &gt;&gt;&gt; data = sm.datasets.longley.load() 
        &gt;&gt;&gt; data.exog = sm.add_constant(data.exog) 
        &gt;&gt;&gt; results = sm.OLS(data.endog, data.exog).fit() 
        &gt;&gt;&gt; results.conf_int() 
        array([[-5496529.48322745, -1467987.78596704], 
               [    -177.02903529,      207.15277984], 
               [      -0.1115811 ,        0.03994274], 
               [      -3.12506664,       -0.91539297], 
               [      -1.5179487 ,       -0.54850503], 
               [      -0.56251721,        0.460309  ], 
               [     798.7875153 ,     2859.51541392]]) 
 
        &gt;&gt;&gt; results.conf_int(cols=(2,3)) 
        array([[-0.1115811 ,  0.03994274], 
               [-3.12506664, -0.91539297]]) 
        &quot;&quot;&quot;</span>
        <span class="s1">bse = self.bse</span>

        <span class="s0">if </span><span class="s1">self.use_t:</span>
            <span class="s1">dist = stats.t</span>
            <span class="s1">df_resid = getattr(self</span><span class="s0">, </span><span class="s2">'df_resid_inference'</span><span class="s0">, </span><span class="s1">self.df_resid)</span>
            <span class="s1">q = dist.ppf(</span><span class="s4">1 </span><span class="s1">- alpha / </span><span class="s4">2</span><span class="s0">, </span><span class="s1">df_resid)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">dist = stats.norm</span>
            <span class="s1">q = dist.ppf(</span><span class="s4">1 </span><span class="s1">- alpha / </span><span class="s4">2</span><span class="s1">)</span>

        <span class="s1">params = self.params</span>
        <span class="s1">lower = params - q * bse</span>
        <span class="s1">upper = params + q * bse</span>
        <span class="s0">if </span><span class="s1">cols </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s2">&quot;cols is deprecated and will be removed after 0.14 is &quot;</span>
                <span class="s2">&quot;released. cols only works when inputs are NumPy arrays and &quot;</span>
                <span class="s2">&quot;will fail when using pandas Series or DataFrames as input. &quot;</span>
                <span class="s2">&quot;Subsets of confidence intervals can be selected using slices &quot;</span>
                <span class="s2">&quot;of the full confidence interval array.&quot;</span><span class="s0">,</span>
                <span class="s1">FutureWarning</span>
            <span class="s1">)</span>
            <span class="s1">cols = np.asarray(cols)</span>
            <span class="s1">lower = lower[cols]</span>
            <span class="s1">upper = upper[cols]</span>
        <span class="s0">return </span><span class="s1">np.asarray(lzip(lower</span><span class="s0">, </span><span class="s1">upper))</span>

    <span class="s0">def </span><span class="s1">save(self</span><span class="s0">, </span><span class="s1">fname</span><span class="s0">, </span><span class="s1">remove_data=</span><span class="s0">False</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Save a pickle of this instance. 
 
        Parameters 
        ---------- 
        fname : {str, handle} 
            A string filename or a file handle. 
        remove_data : bool 
            If False (default), then the instance is pickled without changes. 
            If True, then all arrays with length nobs are set to None before 
            pickling. See the remove_data method. 
            In some cases not all arrays will be set to None. 
 
        Notes 
        ----- 
        If remove_data is true and the model result does not implement a 
        remove_data method then this will raise an exception. 
        &quot;&quot;&quot;</span>

        <span class="s0">from </span><span class="s1">statsmodels.iolib.smpickle </span><span class="s0">import </span><span class="s1">save_pickle</span>

        <span class="s0">if </span><span class="s1">remove_data:</span>
            <span class="s1">self.remove_data()</span>

        <span class="s1">save_pickle(self</span><span class="s0">, </span><span class="s1">fname)</span>

    <span class="s1">@classmethod</span>
    <span class="s0">def </span><span class="s1">load(cls</span><span class="s0">, </span><span class="s1">fname):</span>
        <span class="s5">&quot;&quot;&quot; 
        Load a pickled results instance 
 
        .. warning:: 
 
           Loading pickled models is not secure against erroneous or 
           maliciously constructed data. Never unpickle data received from 
           an untrusted or unauthenticated source. 
 
        Parameters 
        ---------- 
        fname : {str, handle, pathlib.Path} 
            A string filename or a file handle. 
 
        Returns 
        ------- 
        Results 
            The unpickled results instance. 
        &quot;&quot;&quot;</span>

        <span class="s0">from </span><span class="s1">statsmodels.iolib.smpickle </span><span class="s0">import </span><span class="s1">load_pickle</span>
        <span class="s0">return </span><span class="s1">load_pickle(fname)</span>

    <span class="s0">def </span><span class="s1">remove_data(self):</span>
        <span class="s5">&quot;&quot;&quot; 
        Remove data arrays, all nobs arrays from result and model. 
 
        This reduces the size of the instance, so it can be pickled with less 
        memory. Currently tested for use with predict from an unpickled 
        results and model instance. 
 
        .. warning:: 
 
           Since data and some intermediate results have been removed 
           calculating new statistics that require them will raise exceptions. 
           The exception will occur the first time an attribute is accessed 
           that has been set to None. 
 
        Not fully tested for time series models, tsa, and might delete too much 
        for prediction or not all that would be possible. 
 
        The lists of arrays to delete are maintained as attributes of 
        the result and model instance, except for cached values. These 
        lists could be changed before calling remove_data. 
 
        The attributes to remove are named in: 
 
        model._data_attr : arrays attached to both the model instance 
            and the results instance with the same attribute name. 
 
        result._data_in_cache : arrays that may exist as values in 
            result._cache 
 
        result._data_attr_model : arrays attached to the model 
            instance but not to the results instance 
        &quot;&quot;&quot;</span>
        <span class="s1">cls = self.__class__</span>
        <span class="s3"># Note: we cannot just use `getattr(cls, x)` or `getattr(self, x)`</span>
        <span class="s3"># because of redirection involved with property-like accessors</span>
        <span class="s1">cls_attrs = {}</span>
        <span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">dir(cls):</span>
            <span class="s0">try</span><span class="s1">:</span>
                <span class="s1">attr = object.__getattribute__(cls</span><span class="s0">, </span><span class="s1">name)</span>
            <span class="s0">except </span><span class="s1">AttributeError:</span>
                <span class="s0">pass</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">cls_attrs[name] = attr</span>
        <span class="s1">data_attrs = [x </span><span class="s0">for </span><span class="s1">x </span><span class="s0">in </span><span class="s1">cls_attrs</span>
                      <span class="s0">if </span><span class="s1">isinstance(cls_attrs[x]</span><span class="s0">, </span><span class="s1">cached_data)]</span>
        <span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">data_attrs:</span>
            <span class="s1">self._cache[name] = </span><span class="s0">None</span>

        <span class="s0">def </span><span class="s1">wipe(obj</span><span class="s0">, </span><span class="s1">att):</span>
            <span class="s3"># get to last element in attribute path</span>
            <span class="s1">p = att.split(</span><span class="s2">'.'</span><span class="s1">)</span>
            <span class="s1">att_ = p.pop(-</span><span class="s4">1</span><span class="s1">)</span>
            <span class="s0">try</span><span class="s1">:</span>
                <span class="s1">obj_ = reduce(getattr</span><span class="s0">, </span><span class="s1">[obj] + p)</span>
                <span class="s0">if </span><span class="s1">hasattr(obj_</span><span class="s0">, </span><span class="s1">att_):</span>
                    <span class="s1">setattr(obj_</span><span class="s0">, </span><span class="s1">att_</span><span class="s0">, None</span><span class="s1">)</span>
            <span class="s0">except </span><span class="s1">AttributeError:</span>
                <span class="s0">pass</span>

        <span class="s1">model_only = [</span><span class="s2">'model.' </span><span class="s1">+ i </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">getattr(self</span><span class="s0">, </span><span class="s2">&quot;_data_attr_model&quot;</span><span class="s0">, </span><span class="s1">[])]</span>
        <span class="s1">model_attr = [</span><span class="s2">'model.' </span><span class="s1">+ i </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">self.model._data_attr]</span>
        <span class="s0">for </span><span class="s1">att </span><span class="s0">in </span><span class="s1">self._data_attr + model_attr + model_only:</span>
            <span class="s0">if </span><span class="s1">att </span><span class="s0">in </span><span class="s1">data_attrs:</span>
                <span class="s3"># these have been handled above, and trying to call wipe</span>
                <span class="s3"># would raise an Exception anyway, so skip these</span>
                <span class="s0">continue</span>
            <span class="s1">wipe(self</span><span class="s0">, </span><span class="s1">att)</span>

        <span class="s0">for </span><span class="s1">key </span><span class="s0">in </span><span class="s1">self._data_in_cache:</span>
            <span class="s0">try</span><span class="s1">:</span>
                <span class="s1">self._cache[key] = </span><span class="s0">None</span>
            <span class="s0">except </span><span class="s1">(AttributeError</span><span class="s0">, </span><span class="s1">KeyError):</span>
                <span class="s0">pass</span>


<span class="s0">class </span><span class="s1">LikelihoodResultsWrapper(wrap.ResultsWrapper):</span>
    <span class="s1">_attrs = {</span>
        <span class="s2">'params'</span><span class="s1">: </span><span class="s2">'columns'</span><span class="s0">,</span>
        <span class="s2">'bse'</span><span class="s1">: </span><span class="s2">'columns'</span><span class="s0">,</span>
        <span class="s2">'pvalues'</span><span class="s1">: </span><span class="s2">'columns'</span><span class="s0">,</span>
        <span class="s2">'tvalues'</span><span class="s1">: </span><span class="s2">'columns'</span><span class="s0">,</span>
        <span class="s2">'resid'</span><span class="s1">: </span><span class="s2">'rows'</span><span class="s0">,</span>
        <span class="s2">'fittedvalues'</span><span class="s1">: </span><span class="s2">'rows'</span><span class="s0">,</span>
        <span class="s2">'normalized_cov_params'</span><span class="s1">: </span><span class="s2">'cov'</span><span class="s0">,</span>
    <span class="s1">}</span>

    <span class="s1">_wrap_attrs = _attrs</span>
    <span class="s1">_wrap_methods = {</span>
        <span class="s2">'cov_params'</span><span class="s1">: </span><span class="s2">'cov'</span><span class="s0">,</span>
        <span class="s2">'conf_int'</span><span class="s1">: </span><span class="s2">'columns'</span>
    <span class="s1">}</span>

<span class="s1">wrap.populate_wrapper(LikelihoodResultsWrapper</span><span class="s0">,  </span><span class="s3"># noqa:E305</span>
                      <span class="s1">LikelihoodModelResults)</span>


<span class="s0">class </span><span class="s1">ResultMixin:</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">df_modelwc(self):</span>
        <span class="s5">&quot;&quot;&quot;Model WC&quot;&quot;&quot;</span>
        <span class="s3"># collect different ways of defining the number of parameters, used for</span>
        <span class="s3"># aic, bic</span>
        <span class="s1">k_extra = getattr(self.model</span><span class="s0">, </span><span class="s2">&quot;k_extra&quot;</span><span class="s0">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s2">'df_model'</span><span class="s1">):</span>
            <span class="s0">if </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s2">'k_constant'</span><span class="s1">):</span>
                <span class="s1">hasconst = self.k_constant</span>
            <span class="s0">elif </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s2">'hasconst'</span><span class="s1">):</span>
                <span class="s1">hasconst = self.hasconst</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s3"># default assumption</span>
                <span class="s1">hasconst = </span><span class="s4">1</span>
            <span class="s0">return </span><span class="s1">self.df_model + hasconst + k_extra</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">self.params.size</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">aic(self):</span>
        <span class="s5">&quot;&quot;&quot;Akaike information criterion&quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">-</span><span class="s4">2 </span><span class="s1">* self.llf + </span><span class="s4">2 </span><span class="s1">* (self.df_modelwc)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">bic(self):</span>
        <span class="s5">&quot;&quot;&quot;Bayesian information criterion&quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">-</span><span class="s4">2 </span><span class="s1">* self.llf + np.log(self.nobs) * (self.df_modelwc)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">score_obsv(self):</span>
        <span class="s5">&quot;&quot;&quot;cached Jacobian of log-likelihood 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">self.model.score_obs(self.params)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">hessv(self):</span>
        <span class="s5">&quot;&quot;&quot;cached Hessian of log-likelihood 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">self.model.hessian(self.params)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">covjac(self):</span>
        <span class="s5">&quot;&quot;&quot; 
        covariance of parameters based on outer product of jacobian of 
        log-likelihood 
        &quot;&quot;&quot;</span>
        <span class="s3">#  if not hasattr(self, '_results'):</span>
        <span class="s3">#      raise ValueError('need to call fit first')</span>
        <span class="s3">#      #self.fit()</span>
        <span class="s3">#  self.jacv = jacv = self.jac(self._results.params)</span>
        <span class="s1">jacv = self.score_obsv</span>
        <span class="s0">return </span><span class="s1">np.linalg.inv(np.dot(jacv.T</span><span class="s0">, </span><span class="s1">jacv))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">covjhj(self):</span>
        <span class="s5">&quot;&quot;&quot;covariance of parameters based on HJJH 
 
        dot product of Hessian, Jacobian, Jacobian, Hessian of likelihood 
 
        name should be covhjh 
        &quot;&quot;&quot;</span>
        <span class="s1">jacv = self.score_obsv</span>
        <span class="s1">hessv = self.hessv</span>
        <span class="s1">hessinv = np.linalg.inv(hessv)</span>
        <span class="s3">#  self.hessinv = hessin = self.cov_params()</span>
        <span class="s0">return </span><span class="s1">np.dot(hessinv</span><span class="s0">, </span><span class="s1">np.dot(np.dot(jacv.T</span><span class="s0">, </span><span class="s1">jacv)</span><span class="s0">, </span><span class="s1">hessinv))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">bsejhj(self):</span>
        <span class="s5">&quot;&quot;&quot;standard deviation of parameter estimates based on covHJH 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">np.sqrt(np.diag(self.covjhj))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">bsejac(self):</span>
        <span class="s5">&quot;&quot;&quot;standard deviation of parameter estimates based on covjac 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">np.sqrt(np.diag(self.covjac))</span>

    <span class="s0">def </span><span class="s1">bootstrap(self</span><span class="s0">, </span><span class="s1">nrep=</span><span class="s4">100</span><span class="s0">, </span><span class="s1">method=</span><span class="s2">'nm'</span><span class="s0">, </span><span class="s1">disp=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">store=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;simple bootstrap to get mean and variance of estimator 
 
        see notes 
 
        Parameters 
        ---------- 
        nrep : int 
            number of bootstrap replications 
        method : str 
            optimization method to use 
        disp : bool 
            If true, then optimization prints results 
        store : bool 
            If true, then parameter estimates for all bootstrap iterations 
            are attached in self.bootstrap_results 
 
        Returns 
        ------- 
        mean : ndarray 
            mean of parameter estimates over bootstrap replications 
        std : ndarray 
            standard deviation of parameter estimates over bootstrap 
            replications 
 
        Notes 
        ----- 
        This was mainly written to compare estimators of the standard errors of 
        the parameter estimates.  It uses independent random sampling from the 
        original endog and exog, and therefore is only correct if observations 
        are independently distributed. 
 
        This will be moved to apply only to models with independently 
        distributed observations. 
        &quot;&quot;&quot;</span>
        <span class="s1">results = []</span>
        <span class="s1">hascloneattr = </span><span class="s0">True if </span><span class="s1">hasattr(self.model</span><span class="s0">, </span><span class="s2">'cloneattr'</span><span class="s1">) </span><span class="s0">else False</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(nrep):</span>
            <span class="s1">rvsind = np.random.randint(self.nobs</span><span class="s0">, </span><span class="s1">size=self.nobs)</span>
            <span class="s3"># this needs to set startparam and get other defining attributes</span>
            <span class="s3"># need a clone method on model</span>
            <span class="s0">if </span><span class="s1">self.exog </span><span class="s0">is not None</span><span class="s1">:</span>
                <span class="s1">exog_resamp = self.exog[rvsind</span><span class="s0">, </span><span class="s1">:]</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">exog_resamp = </span><span class="s0">None</span>
            <span class="s3"># build auxiliary model and fit</span>
            <span class="s1">init_kwds = self.model._get_init_kwds()</span>
            <span class="s1">fitmod = self.model.__class__(self.endog[rvsind]</span><span class="s0">,</span>
                                          <span class="s1">exog=exog_resamp</span><span class="s0">, </span><span class="s1">**init_kwds)</span>
            <span class="s0">if </span><span class="s1">hascloneattr:</span>
                <span class="s0">for </span><span class="s1">attr </span><span class="s0">in </span><span class="s1">self.model.cloneattr:</span>
                    <span class="s1">setattr(fitmod</span><span class="s0">, </span><span class="s1">attr</span><span class="s0">, </span><span class="s1">getattr(self.model</span><span class="s0">, </span><span class="s1">attr))</span>

            <span class="s1">fitres = fitmod.fit(method=method</span><span class="s0">, </span><span class="s1">disp=disp)</span>
            <span class="s1">results.append(fitres.params)</span>
        <span class="s1">results = np.array(results)</span>
        <span class="s0">if </span><span class="s1">store:</span>
            <span class="s1">self.bootstrap_results = results</span>
        <span class="s0">return </span><span class="s1">results.mean(</span><span class="s4">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">results.std(</span><span class="s4">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">results</span>

    <span class="s0">def </span><span class="s1">get_nlfun(self</span><span class="s0">, </span><span class="s1">fun):</span>
        <span class="s5">&quot;&quot;&quot; 
        get_nlfun 
 
        This is not Implemented 
        &quot;&quot;&quot;</span>
        <span class="s3"># I think this is supposed to get the delta method that is currently</span>
        <span class="s3"># in miscmodels count (as part of Poisson example)</span>
        <span class="s0">raise </span><span class="s1">NotImplementedError</span>


<span class="s0">class </span><span class="s1">_LLRMixin():</span>
    <span class="s5">&quot;&quot;&quot;Mixin class for Null model and likelihood ratio 
    &quot;&quot;&quot;</span>
    <span class="s3"># methods copied from DiscreteResults, adjusted pseudo R2</span>

    <span class="s0">def </span><span class="s1">pseudo_rsquared(self</span><span class="s0">, </span><span class="s1">kind=</span><span class="s2">&quot;mcf&quot;</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        McFadden's pseudo-R-squared. `1 - (llf / llnull)` 
        &quot;&quot;&quot;</span>
        <span class="s1">kind = kind.lower()</span>
        <span class="s0">if </span><span class="s1">kind.startswith(</span><span class="s2">&quot;mcf&quot;</span><span class="s1">):</span>
            <span class="s1">prsq = </span><span class="s4">1 </span><span class="s1">- self.llf / self.llnull</span>
        <span class="s0">elif </span><span class="s1">kind.startswith(</span><span class="s2">&quot;cox&quot;</span><span class="s1">) </span><span class="s0">or </span><span class="s1">kind </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;cs&quot;</span><span class="s0">, </span><span class="s2">&quot;lr&quot;</span><span class="s1">]:</span>
            <span class="s1">prsq = </span><span class="s4">1 </span><span class="s1">- np.exp((self.llnull - self.llf) * (</span><span class="s4">2 </span><span class="s1">/ self.nobs))</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;only McFadden and Cox-Snell are available&quot;</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">prsq</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">llr(self):</span>
        <span class="s5">&quot;&quot;&quot; 
        Likelihood ratio chi-squared statistic; `-2*(llnull - llf)` 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">-</span><span class="s4">2</span><span class="s1">*(self.llnull - self.llf)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">llr_pvalue(self):</span>
        <span class="s5">&quot;&quot;&quot; 
        The chi-squared probability of getting a log-likelihood ratio 
        statistic greater than llr.  llr has a chi-squared distribution 
        with degrees of freedom `df_model`. 
        &quot;&quot;&quot;</span>
        <span class="s3"># see also RegressionModel compare_lr_test</span>
        <span class="s1">llr = self.llr</span>
        <span class="s1">df_full = self.df_resid</span>
        <span class="s1">df_restr = self.df_resid_null</span>
        <span class="s1">lrdf = (df_restr - df_full)</span>
        <span class="s1">self.df_lr_null = lrdf</span>
        <span class="s0">return </span><span class="s1">stats.distributions.chi2.sf(llr</span><span class="s0">, </span><span class="s1">lrdf)</span>

    <span class="s0">def </span><span class="s1">set_null_options(self</span><span class="s0">, </span><span class="s1">llnull=</span><span class="s0">None, </span><span class="s1">attach_results=</span><span class="s0">True, </span><span class="s1">**kwargs):</span>
        <span class="s5">&quot;&quot;&quot; 
        Set the fit options for the Null (constant-only) model. 
 
        This resets the cache for related attributes which is potentially 
        fragile. This only sets the option, the null model is estimated 
        when llnull is accessed, if llnull is not yet in cache. 
 
        Parameters 
        ---------- 
        llnull : {None, float} 
            If llnull is not None, then the value will be directly assigned to 
            the cached attribute &quot;llnull&quot;. 
        attach_results : bool 
            Sets an internal flag whether the results instance of the null 
            model should be attached. By default without calling this method, 
            thenull model results are not attached and only the loglikelihood 
            value llnull is stored. 
        **kwargs 
            Additional keyword arguments used as fit keyword arguments for the 
            null model. The override and model default values. 
 
        Notes 
        ----- 
        Modifies attributes of this instance, and so has no return. 
        &quot;&quot;&quot;</span>
        <span class="s3"># reset cache, note we need to add here anything that depends on</span>
        <span class="s3"># llnullor the null model. If something is missing, then the attribute</span>
        <span class="s3"># might be incorrect.</span>
        <span class="s1">self._cache.pop(</span><span class="s2">'llnull'</span><span class="s0">, None</span><span class="s1">)</span>
        <span class="s1">self._cache.pop(</span><span class="s2">'llr'</span><span class="s0">, None</span><span class="s1">)</span>
        <span class="s1">self._cache.pop(</span><span class="s2">'llr_pvalue'</span><span class="s0">, None</span><span class="s1">)</span>
        <span class="s1">self._cache.pop(</span><span class="s2">'prsquared'</span><span class="s0">, None</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">hasattr(self</span><span class="s0">, </span><span class="s2">'res_null'</span><span class="s1">):</span>
            <span class="s0">del </span><span class="s1">self.res_null</span>

        <span class="s0">if </span><span class="s1">llnull </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">self._cache[</span><span class="s2">'llnull'</span><span class="s1">] = llnull</span>
        <span class="s1">self._attach_nullmodel = attach_results</span>
        <span class="s1">self._optim_kwds_null = kwargs</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">llnull(self):</span>
        <span class="s5">&quot;&quot;&quot; 
        Value of the constant-only loglikelihood 
        &quot;&quot;&quot;</span>
        <span class="s1">model = self.model</span>
        <span class="s1">kwds = model._get_init_kwds().copy()</span>
        <span class="s0">for </span><span class="s1">key </span><span class="s0">in </span><span class="s1">getattr(model</span><span class="s0">, </span><span class="s2">'_null_drop_keys'</span><span class="s0">, </span><span class="s1">[]):</span>
            <span class="s0">del </span><span class="s1">kwds[key]</span>
        <span class="s3"># TODO: what parameters to pass to fit?</span>
        <span class="s1">mod_null = model.__class__(model.endog</span><span class="s0">, </span><span class="s1">np.ones(self.nobs)</span><span class="s0">, </span><span class="s1">**kwds)</span>
        <span class="s3"># TODO: consider catching and warning on convergence failure?</span>
        <span class="s3"># in the meantime, try hard to converge. see</span>
        <span class="s3"># TestPoissonConstrained1a.test_smoke</span>

        <span class="s1">optim_kwds = getattr(self</span><span class="s0">, </span><span class="s2">'_optim_kwds_null'</span><span class="s0">, </span><span class="s1">{}).copy()</span>

        <span class="s0">if </span><span class="s2">'start_params' </span><span class="s0">in </span><span class="s1">optim_kwds:</span>
            <span class="s3"># user provided</span>
            <span class="s1">sp_null = optim_kwds.pop(</span><span class="s2">'start_params'</span><span class="s1">)</span>
        <span class="s0">elif </span><span class="s1">hasattr(model</span><span class="s0">, </span><span class="s2">'_get_start_params_null'</span><span class="s1">):</span>
            <span class="s3"># get moment estimates if available</span>
            <span class="s1">sp_null = model._get_start_params_null()</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">sp_null = </span><span class="s0">None</span>

        <span class="s1">opt_kwds = dict(method=</span><span class="s2">'bfgs'</span><span class="s0">, </span><span class="s1">warn_convergence=</span><span class="s0">False, </span><span class="s1">maxiter=</span><span class="s4">10000</span><span class="s0">,</span>
                        <span class="s1">disp=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">opt_kwds.update(optim_kwds)</span>

        <span class="s0">if </span><span class="s1">optim_kwds:</span>
            <span class="s1">res_null = mod_null.fit(start_params=sp_null</span><span class="s0">, </span><span class="s1">**opt_kwds)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s3"># this should be a reasonably method case across versions</span>
            <span class="s1">res_null = mod_null.fit(start_params=sp_null</span><span class="s0">, </span><span class="s1">method=</span><span class="s2">'nm'</span><span class="s0">,</span>
                                    <span class="s1">warn_convergence=</span><span class="s0">False,</span>
                                    <span class="s1">maxiter=</span><span class="s4">10000</span><span class="s0">, </span><span class="s1">disp=</span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">res_null = mod_null.fit(start_params=res_null.params</span><span class="s0">, </span><span class="s1">method=</span><span class="s2">'bfgs'</span><span class="s0">,</span>
                                    <span class="s1">warn_convergence=</span><span class="s0">False,</span>
                                    <span class="s1">maxiter=</span><span class="s4">10000</span><span class="s0">, </span><span class="s1">disp=</span><span class="s4">0</span><span class="s1">)</span>

        <span class="s0">if </span><span class="s1">getattr(self</span><span class="s0">, </span><span class="s2">'_attach_nullmodel'</span><span class="s0">, False</span><span class="s1">) </span><span class="s0">is not False</span><span class="s1">:</span>
            <span class="s1">self.res_null = res_null</span>

        <span class="s1">self.k_null = len(res_null.params)</span>
        <span class="s1">self.df_resid_null = res_null.df_resid</span>
        <span class="s0">return </span><span class="s1">res_null.llf</span>


<span class="s0">class </span><span class="s1">GenericLikelihoodModelResults(LikelihoodModelResults</span><span class="s0">, </span><span class="s1">ResultMixin):</span>
    <span class="s5">&quot;&quot;&quot; 
    A results class for the discrete dependent variable models. 
 
    ..Warning : 
 
    The following description has not been updated to this version/class. 
    Where are AIC, BIC, ....? docstring looks like copy from discretemod 
 
    Parameters 
    ---------- 
    model : A DiscreteModel instance 
    mlefit : instance of LikelihoodResults 
        This contains the numerical optimization results as returned by 
        LikelihoodModel.fit(), in a superclass of GnericLikelihoodModels 
 
 
    Attributes 
    ---------- 
    aic : float 
        Akaike information criterion.  -2*(`llf` - p) where p is the number 
        of regressors including the intercept. 
    bic : float 
        Bayesian information criterion. -2*`llf` + ln(`nobs`)*p where p is the 
        number of regressors including the intercept. 
    bse : ndarray 
        The standard errors of the coefficients. 
    df_resid : float 
        See model definition. 
    df_model : float 
        See model definition. 
    fitted_values : ndarray 
        Linear predictor XB. 
    llf : float 
        Value of the loglikelihood 
    llnull : float 
        Value of the constant-only loglikelihood 
    llr : float 
        Likelihood ratio chi-squared statistic; -2*(`llnull` - `llf`) 
    llr_pvalue : float 
        The chi-squared probability of getting a log-likelihood ratio 
        statistic greater than llr.  llr has a chi-squared distribution 
        with degrees of freedom `df_model`. 
    prsquared : float 
        McFadden's pseudo-R-squared. 1 - (`llf`/`llnull`) 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">model</span><span class="s0">, </span><span class="s1">mlefit):</span>
        <span class="s1">self.model = model</span>
        <span class="s1">self.endog = model.endog</span>
        <span class="s1">self.exog = model.exog</span>
        <span class="s1">self.nobs = model.endog.shape[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s3"># TODO: possibly move to model.fit()</span>
        <span class="s3">#       and outsource together with patching names</span>
        <span class="s1">k_extra = getattr(self.model</span><span class="s0">, </span><span class="s2">&quot;k_extra&quot;</span><span class="s0">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">hasattr(model</span><span class="s0">, </span><span class="s2">'df_model'</span><span class="s1">) </span><span class="s0">and not </span><span class="s1">np.isnan(model.df_model):</span>
            <span class="s1">self.df_model = model.df_model</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">df_model = len(mlefit.params) - self.model.k_constant - k_extra</span>
            <span class="s1">self.df_model = df_model</span>
            <span class="s3"># retrofitting the model, used in t_test TODO: check design</span>
            <span class="s1">self.model.df_model = df_model</span>

        <span class="s0">if </span><span class="s1">hasattr(model</span><span class="s0">, </span><span class="s2">'df_resid'</span><span class="s1">) </span><span class="s0">and not </span><span class="s1">np.isnan(model.df_resid):</span>
            <span class="s1">self.df_resid = model.df_resid</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">self.df_resid = self.endog.shape[</span><span class="s4">0</span><span class="s1">] - self.df_model - k_extra</span>
            <span class="s3"># retrofitting the model, used in t_test TODO: check design</span>
            <span class="s1">self.model.df_resid = self.df_resid</span>

        <span class="s1">self._cache = {}</span>
        <span class="s1">self.__dict__.update(mlefit.__dict__)</span>

        <span class="s1">k_params = len(mlefit.params)</span>
        <span class="s3"># checks mainly for adding new models or subclassing</span>

        <span class="s0">if </span><span class="s1">self.df_model + self.model.k_constant + k_extra != k_params:</span>
            <span class="s1">warnings.warn(</span><span class="s2">&quot;df_model + k_constant + k_extra &quot;</span>
                          <span class="s2">&quot;differs from k_params&quot;</span><span class="s1">)</span>

        <span class="s0">if </span><span class="s1">self.df_resid != self.nobs - k_params:</span>
            <span class="s1">warnings.warn(</span><span class="s2">&quot;df_resid differs from nobs - k_params&quot;</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">get_prediction(</span>
            <span class="s1">self</span><span class="s0">,</span>
            <span class="s1">exog=</span><span class="s0">None,</span>
            <span class="s1">which=</span><span class="s2">&quot;mean&quot;</span><span class="s0">,</span>
            <span class="s1">transform=</span><span class="s0">True,</span>
            <span class="s1">row_labels=</span><span class="s0">None,</span>
            <span class="s1">average=</span><span class="s0">False,</span>
            <span class="s1">agg_weights=</span><span class="s0">None,</span>
            <span class="s1">**kwargs</span>
            <span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Compute prediction results when endpoint transformation is valid. 
 
        Parameters 
        ---------- 
        exog : array_like, optional 
            The values for which you want to predict. 
        transform : bool, optional 
            If the model was fit via a formula, do you want to pass 
            exog through the formula. Default is True. E.g., if you fit 
            a model y ~ log(x1) + log(x2), and transform is True, then 
            you can pass a data structure that contains x1 and x2 in 
            their original form. Otherwise, you'd need to log the data 
            first. 
        which : str 
            Which statistic is to be predicted. Default is &quot;mean&quot;. 
            The available statistics and options depend on the model. 
            see the model.predict docstring 
        row_labels : list of str or None 
            If row_lables are provided, then they will replace the generated 
            labels. 
        average : bool 
            If average is True, then the mean prediction is computed, that is, 
            predictions are computed for individual exog and then the average 
            over observation is used. 
            If average is False, then the results are the predictions for all 
            observations, i.e. same length as ``exog``. 
        agg_weights : ndarray, optional 
            Aggregation weights, only used if average is True. 
            The weights are not normalized. 
        **kwargs : 
            Some models can take additional keyword arguments, such as offset, 
            exposure or additional exog in multi-part models like zero inflated 
            models. 
            See the predict method of the model for the details. 
 
        Returns 
        ------- 
        prediction_results : PredictionResults 
            The prediction results instance contains prediction and prediction 
            variance and can on demand calculate confidence intervals and 
            summary dataframe for the prediction. 
 
        Notes 
        ----- 
        Status: new in 0.14, experimental 
        &quot;&quot;&quot;</span>
        <span class="s0">from </span><span class="s1">statsmodels.base._prediction_inference </span><span class="s0">import </span><span class="s1">get_prediction</span>

        <span class="s1">pred_kwds = kwargs</span>

        <span class="s1">res = get_prediction(</span>
            <span class="s1">self</span><span class="s0">,</span>
            <span class="s1">exog=exog</span><span class="s0">,</span>
            <span class="s1">which=which</span><span class="s0">,</span>
            <span class="s1">transform=transform</span><span class="s0">,</span>
            <span class="s1">row_labels=row_labels</span><span class="s0">,</span>
            <span class="s1">average=average</span><span class="s0">,</span>
            <span class="s1">agg_weights=agg_weights</span><span class="s0">,</span>
            <span class="s1">pred_kwds=pred_kwds</span>
            <span class="s1">)</span>
        <span class="s0">return </span><span class="s1">res</span>

    <span class="s0">def </span><span class="s1">summary(self</span><span class="s0">, </span><span class="s1">yname=</span><span class="s0">None, </span><span class="s1">xname=</span><span class="s0">None, </span><span class="s1">title=</span><span class="s0">None, </span><span class="s1">alpha=</span><span class="s4">.05</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Summarize the Regression Results 
 
        Parameters 
        ---------- 
        yname : str, optional 
            Default is `y` 
        xname : list[str], optional 
            Names for the exogenous variables, default is &quot;var_xx&quot;. 
            Must match the number of parameters in the model 
        title : str, optional 
            Title for the top table. If not None, then this replaces the 
            default title 
        alpha : float 
            significance level for the confidence intervals 
 
        Returns 
        ------- 
        smry : Summary instance 
            this holds the summary tables and text, which can be printed or 
            converted to various output formats. 
 
        See Also 
        -------- 
        statsmodels.iolib.summary.Summary : class to hold summary results 
        &quot;&quot;&quot;</span>

        <span class="s1">top_left = [(</span><span class="s2">'Dep. Variable:'</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
                    <span class="s1">(</span><span class="s2">'Model:'</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
                    <span class="s1">(</span><span class="s2">'Method:'</span><span class="s0">, </span><span class="s1">[</span><span class="s2">'Maximum Likelihood'</span><span class="s1">])</span><span class="s0">,</span>
                    <span class="s1">(</span><span class="s2">'Date:'</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
                    <span class="s1">(</span><span class="s2">'Time:'</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
                    <span class="s1">(</span><span class="s2">'No. Observations:'</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
                    <span class="s1">(</span><span class="s2">'Df Residuals:'</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
                    <span class="s1">(</span><span class="s2">'Df Model:'</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
                    <span class="s1">]</span>

        <span class="s1">top_right = [(</span><span class="s2">'Log-Likelihood:'</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
                     <span class="s1">(</span><span class="s2">'AIC:'</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;%#8.4g&quot; </span><span class="s1">% self.aic])</span><span class="s0">,</span>
                     <span class="s1">(</span><span class="s2">'BIC:'</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;%#8.4g&quot; </span><span class="s1">% self.bic])</span>
                     <span class="s1">]</span>

        <span class="s0">if </span><span class="s1">title </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">title = self.model.__class__.__name__ + </span><span class="s2">' ' </span><span class="s1">+ </span><span class="s2">&quot;Results&quot;</span>

        <span class="s3"># create summary table instance</span>
        <span class="s0">from </span><span class="s1">statsmodels.iolib.summary </span><span class="s0">import </span><span class="s1">Summary</span>
        <span class="s1">smry = Summary()</span>
        <span class="s1">smry.add_table_2cols(self</span><span class="s0">, </span><span class="s1">gleft=top_left</span><span class="s0">, </span><span class="s1">gright=top_right</span><span class="s0">,</span>
                             <span class="s1">yname=yname</span><span class="s0">, </span><span class="s1">xname=xname</span><span class="s0">, </span><span class="s1">title=title)</span>
        <span class="s1">smry.add_table_params(self</span><span class="s0">, </span><span class="s1">yname=yname</span><span class="s0">, </span><span class="s1">xname=xname</span><span class="s0">, </span><span class="s1">alpha=alpha</span><span class="s0">,</span>
                              <span class="s1">use_t=self.use_t)</span>

        <span class="s0">return </span><span class="s1">smry</span>
</pre>
</body>
</html>