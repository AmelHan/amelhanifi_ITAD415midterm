<html>
<head>
<title>test_gradient_boosting.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6897bb;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6a8759;}
.s5 { color: #808080;}
.s6 { color: #a5c261;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_gradient_boosting.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">re</span>
<span class="s0">import </span><span class="s1">warnings</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">assert_allclose</span><span class="s0">, </span><span class="s1">assert_array_equal</span>

<span class="s0">from </span><span class="s1">sklearn._loss.loss </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">AbsoluteError</span><span class="s0">,</span>
    <span class="s1">HalfBinomialLoss</span><span class="s0">,</span>
    <span class="s1">HalfSquaredError</span><span class="s0">,</span>
    <span class="s1">PinballLoss</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.base </span><span class="s0">import </span><span class="s1">BaseEstimator</span><span class="s0">, </span><span class="s1">TransformerMixin</span><span class="s0">, </span><span class="s1">clone</span><span class="s0">, </span><span class="s1">is_regressor</span>
<span class="s0">from </span><span class="s1">sklearn.compose </span><span class="s0">import </span><span class="s1">make_column_transformer</span>
<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">make_classification</span><span class="s0">, </span><span class="s1">make_low_rank_matrix</span><span class="s0">, </span><span class="s1">make_regression</span>
<span class="s0">from </span><span class="s1">sklearn.dummy </span><span class="s0">import </span><span class="s1">DummyRegressor</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">HistGradientBoostingClassifier</span><span class="s0">,</span>
    <span class="s1">HistGradientBoostingRegressor</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble._hist_gradient_boosting.binning </span><span class="s0">import </span><span class="s1">_BinMapper</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble._hist_gradient_boosting.common </span><span class="s0">import </span><span class="s1">G_H_DTYPE</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble._hist_gradient_boosting.grower </span><span class="s0">import </span><span class="s1">TreeGrower</span>
<span class="s0">from </span><span class="s1">sklearn.exceptions </span><span class="s0">import </span><span class="s1">NotFittedError</span>
<span class="s0">from </span><span class="s1">sklearn.metrics </span><span class="s0">import </span><span class="s1">mean_gamma_deviance</span><span class="s0">, </span><span class="s1">mean_poisson_deviance</span>
<span class="s0">from </span><span class="s1">sklearn.model_selection </span><span class="s0">import </span><span class="s1">cross_val_score</span><span class="s0">, </span><span class="s1">train_test_split</span>
<span class="s0">from </span><span class="s1">sklearn.pipeline </span><span class="s0">import </span><span class="s1">make_pipeline</span>
<span class="s0">from </span><span class="s1">sklearn.preprocessing </span><span class="s0">import </span><span class="s1">KBinsDiscretizer</span><span class="s0">, </span><span class="s1">MinMaxScaler</span><span class="s0">, </span><span class="s1">OneHotEncoder</span>
<span class="s0">from </span><span class="s1">sklearn.utils </span><span class="s0">import </span><span class="s1">shuffle</span>
<span class="s0">from </span><span class="s1">sklearn.utils._openmp_helpers </span><span class="s0">import </span><span class="s1">_openmp_effective_n_threads</span>

<span class="s1">n_threads = _openmp_effective_n_threads()</span>

<span class="s1">X_classification</span><span class="s0">, </span><span class="s1">y_classification = make_classification(random_state=</span><span class="s2">0</span><span class="s1">)</span>
<span class="s1">X_regression</span><span class="s0">, </span><span class="s1">y_regression = make_regression(random_state=</span><span class="s2">0</span><span class="s1">)</span>
<span class="s1">X_multi_classification</span><span class="s0">, </span><span class="s1">y_multi_classification = make_classification(</span>
    <span class="s1">n_classes=</span><span class="s2">3</span><span class="s0">, </span><span class="s1">n_informative=</span><span class="s2">3</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span>
<span class="s1">)</span>


<span class="s0">def </span><span class="s1">_make_dumb_dataset(n_samples):</span>
    <span class="s3">&quot;&quot;&quot;Make a dumb dataset to test early stopping.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">X_dumb = rng.randn(n_samples</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)</span>
    <span class="s1">y_dumb = (X_dumb[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] &gt; </span><span class="s2">0</span><span class="s1">).astype(</span><span class="s4">&quot;int64&quot;</span><span class="s1">)</span>
    <span class="s0">return </span><span class="s1">X_dumb</span><span class="s0">, </span><span class="s1">y_dumb</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;GradientBoosting, X, y&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">X_classification</span><span class="s0">, </span><span class="s1">y_classification)</span><span class="s0">,</span>
        <span class="s1">(HistGradientBoostingRegressor</span><span class="s0">, </span><span class="s1">X_regression</span><span class="s0">, </span><span class="s1">y_regression)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;params, err_msg&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;interaction_cst&quot;</span><span class="s1">: [</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]}</span><span class="s0">,</span>
            <span class="s4">&quot;Interaction constraints must be a sequence of tuples or lists&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;interaction_cst&quot;</span><span class="s1">: [{</span><span class="s2">0</span><span class="s0">, </span><span class="s2">9999</span><span class="s1">}]}</span><span class="s0">,</span>
            <span class="s4">r&quot;Interaction constraints must consist of integer indices in \[0,&quot;</span>
            <span class="s4">r&quot; n_features - 1\] = \[.*\], specifying the position of features,&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;interaction_cst&quot;</span><span class="s1">: [{-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">}]}</span><span class="s0">,</span>
            <span class="s4">r&quot;Interaction constraints must consist of integer indices in \[0,&quot;</span>
            <span class="s4">r&quot; n_features - 1\] = \[.*\], specifying the position of features,&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;interaction_cst&quot;</span><span class="s1">: [{</span><span class="s2">0.5</span><span class="s1">}]}</span><span class="s0">,</span>
            <span class="s4">r&quot;Interaction constraints must consist of integer indices in \[0,&quot;</span>
            <span class="s4">r&quot; n_features - 1\] = \[.*\], specifying the position of features,&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_init_parameters_validation(GradientBoosting</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">err_msg):</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">GradientBoosting(**params).fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;scoring, validation_fraction, early_stopping, n_iter_no_change, tol&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s4">&quot;neg_mean_squared_error&quot;</span><span class="s0">, </span><span class="s2">0.1</span><span class="s0">, True, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">1e-7</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># use scorer</span>
        <span class="s1">(</span><span class="s4">&quot;neg_mean_squared_error&quot;</span><span class="s0">, None, True, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">1e-1</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># use scorer on train</span>
        <span class="s1">(</span><span class="s0">None, </span><span class="s2">0.1</span><span class="s0">, True, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">1e-7</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># same with default scorer</span>
        <span class="s1">(</span><span class="s0">None, None, True, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">1e-1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s2">0.1</span><span class="s0">, True, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">1e-7</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># use loss</span>
        <span class="s1">(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, None, True, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">1e-1</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># use loss on training data</span>
        <span class="s1">(</span><span class="s0">None, None, False, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">0.0</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># no early stopping</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_early_stopping_regression(</span>
    <span class="s1">scoring</span><span class="s0">, </span><span class="s1">validation_fraction</span><span class="s0">, </span><span class="s1">early_stopping</span><span class="s0">, </span><span class="s1">n_iter_no_change</span><span class="s0">, </span><span class="s1">tol</span>
<span class="s1">):</span>
    <span class="s1">max_iter = </span><span class="s2">200</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_samples=</span><span class="s2">50</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s1">gb = HistGradientBoostingRegressor(</span>
        <span class="s1">verbose=</span><span class="s2">1</span><span class="s0">,  </span><span class="s5"># just for coverage</span>
        <span class="s1">min_samples_leaf=</span><span class="s2">5</span><span class="s0">,  </span><span class="s5"># easier to overfit fast</span>
        <span class="s1">scoring=scoring</span><span class="s0">,</span>
        <span class="s1">tol=tol</span><span class="s0">,</span>
        <span class="s1">early_stopping=early_stopping</span><span class="s0">,</span>
        <span class="s1">validation_fraction=validation_fraction</span><span class="s0">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s0">,</span>
        <span class="s1">n_iter_no_change=n_iter_no_change</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s2">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">early_stopping:</span>
        <span class="s0">assert </span><span class="s1">n_iter_no_change &lt;= gb.n_iter_ &lt; max_iter</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">gb.n_iter_ == max_iter</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;data&quot;</span><span class="s0">,</span>
    <span class="s1">(</span>
        <span class="s1">make_classification(n_samples=</span><span class="s2">30</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">make_classification(</span>
            <span class="s1">n_samples=</span><span class="s2">30</span><span class="s0">, </span><span class="s1">n_classes=</span><span class="s2">3</span><span class="s0">, </span><span class="s1">n_clusters_per_class=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">)</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;scoring, validation_fraction, early_stopping, n_iter_no_change, tol&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s4">&quot;accuracy&quot;</span><span class="s0">, </span><span class="s2">0.1</span><span class="s0">, True, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">1e-7</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># use scorer</span>
        <span class="s1">(</span><span class="s4">&quot;accuracy&quot;</span><span class="s0">, None, True, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">1e-1</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># use scorer on training data</span>
        <span class="s1">(</span><span class="s0">None, </span><span class="s2">0.1</span><span class="s0">, True, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">1e-7</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># same with default scorer</span>
        <span class="s1">(</span><span class="s0">None, None, True, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">1e-1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s2">0.1</span><span class="s0">, True, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">1e-7</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># use loss</span>
        <span class="s1">(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, None, True, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">1e-1</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># use loss on training data</span>
        <span class="s1">(</span><span class="s0">None, None, False, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">0.0</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># no early stopping</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_early_stopping_classification(</span>
    <span class="s1">data</span><span class="s0">, </span><span class="s1">scoring</span><span class="s0">, </span><span class="s1">validation_fraction</span><span class="s0">, </span><span class="s1">early_stopping</span><span class="s0">, </span><span class="s1">n_iter_no_change</span><span class="s0">, </span><span class="s1">tol</span>
<span class="s1">):</span>
    <span class="s1">max_iter = </span><span class="s2">50</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = data</span>

    <span class="s1">gb = HistGradientBoostingClassifier(</span>
        <span class="s1">verbose=</span><span class="s2">1</span><span class="s0">,  </span><span class="s5"># just for coverage</span>
        <span class="s1">min_samples_leaf=</span><span class="s2">5</span><span class="s0">,  </span><span class="s5"># easier to overfit fast</span>
        <span class="s1">scoring=scoring</span><span class="s0">,</span>
        <span class="s1">tol=tol</span><span class="s0">,</span>
        <span class="s1">early_stopping=early_stopping</span><span class="s0">,</span>
        <span class="s1">validation_fraction=validation_fraction</span><span class="s0">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s0">,</span>
        <span class="s1">n_iter_no_change=n_iter_no_change</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s2">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">early_stopping </span><span class="s0">is True</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">n_iter_no_change &lt;= gb.n_iter_ &lt; max_iter</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">gb.n_iter_ == max_iter</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;GradientBoosting, X, y&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">*_make_dumb_dataset(</span><span class="s2">10000</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">*_make_dumb_dataset(</span><span class="s2">10001</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(HistGradientBoostingRegressor</span><span class="s0">, </span><span class="s1">*_make_dumb_dataset(</span><span class="s2">10000</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(HistGradientBoostingRegressor</span><span class="s0">, </span><span class="s1">*_make_dumb_dataset(</span><span class="s2">10001</span><span class="s1">))</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_early_stopping_default(GradientBoosting</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s5"># Test that early stopping is enabled by default if and only if there</span>
    <span class="s5"># are more than 10000 samples</span>
    <span class="s1">gb = GradientBoosting(max_iter=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">n_iter_no_change=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">tol=</span><span class="s2">1e-1</span><span class="s1">)</span>
    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">if </span><span class="s1">X.shape[</span><span class="s2">0</span><span class="s1">] &gt; </span><span class="s2">10000</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">gb.n_iter_ &lt; gb.max_iter</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">gb.n_iter_ == gb.max_iter</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;scores, n_iter_no_change, tol, stopping&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">([]</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">0.001</span><span class="s0">, False</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># not enough iterations</span>
        <span class="s1">([</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">0.001</span><span class="s0">, False</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># not enough iterations</span>
        <span class="s1">([</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">0.001</span><span class="s0">, False</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># not enough iterations</span>
        <span class="s1">([</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">6</span><span class="s1">]</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">0.001</span><span class="s0">, False</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># significant improvement</span>
        <span class="s1">([</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">6</span><span class="s1">]</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">0.0</span><span class="s0">, False</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># significant improvement</span>
        <span class="s1">([</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">6</span><span class="s1">]</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">0.999</span><span class="s0">, False</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># significant improvement</span>
        <span class="s1">([</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">6</span><span class="s1">]</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">5 </span><span class="s1">- </span><span class="s2">1e-5</span><span class="s0">, False</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># significant improvement</span>
        <span class="s1">([</span><span class="s2">1</span><span class="s1">] * </span><span class="s2">6</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">0.0</span><span class="s0">, True</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># no significant improvement</span>
        <span class="s1">([</span><span class="s2">1</span><span class="s1">] * </span><span class="s2">6</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">0.001</span><span class="s0">, True</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># no significant improvement</span>
        <span class="s1">([</span><span class="s2">1</span><span class="s1">] * </span><span class="s2">6</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, </span><span class="s2">5</span><span class="s0">, True</span><span class="s1">)</span><span class="s0">,  </span><span class="s5"># no significant improvement</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_should_stop(scores</span><span class="s0">, </span><span class="s1">n_iter_no_change</span><span class="s0">, </span><span class="s1">tol</span><span class="s0">, </span><span class="s1">stopping):</span>
    <span class="s1">gbdt = HistGradientBoostingClassifier(n_iter_no_change=n_iter_no_change</span><span class="s0">, </span><span class="s1">tol=tol)</span>
    <span class="s0">assert </span><span class="s1">gbdt._should_stop(scores) == stopping</span>


<span class="s0">def </span><span class="s1">test_absolute_error():</span>
    <span class="s5"># For coverage only.</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_samples=</span><span class="s2">500</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">gbdt = HistGradientBoostingRegressor(loss=</span><span class="s4">&quot;absolute_error&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">gbdt.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">gbdt.score(X</span><span class="s0">, </span><span class="s1">y) &gt; </span><span class="s2">0.9</span>


<span class="s0">def </span><span class="s1">test_absolute_error_sample_weight():</span>
    <span class="s5"># non regression test for issue #19400</span>
    <span class="s5"># make sure no error is thrown during fit of</span>
    <span class="s5"># HistGradientBoostingRegressor with absolute_error loss function</span>
    <span class="s5"># and passing sample_weight</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s2">100</span>
    <span class="s1">X = rng.uniform(-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s1">size=(n_samples</span><span class="s0">, </span><span class="s2">2</span><span class="s1">))</span>
    <span class="s1">y = rng.uniform(-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
    <span class="s1">sample_weight = rng.uniform(</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
    <span class="s1">gbdt = HistGradientBoostingRegressor(loss=</span><span class="s4">&quot;absolute_error&quot;</span><span class="s1">)</span>
    <span class="s1">gbdt.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;y&quot;</span><span class="s0">, </span><span class="s1">[([</span><span class="s2">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.0</span><span class="s0">, </span><span class="s2">0.0</span><span class="s1">])</span><span class="s0">, </span><span class="s1">([</span><span class="s2">0.0</span><span class="s0">, </span><span class="s2">1.0</span><span class="s0">, </span><span class="s2">2.0</span><span class="s1">])])</span>
<span class="s0">def </span><span class="s1">test_gamma_y_positive(y):</span>
    <span class="s5"># Test that ValueError is raised if any y_i &lt;= 0.</span>
    <span class="s1">err_msg = </span><span class="s4">r&quot;loss='gamma' requires strictly positive y.&quot;</span>
    <span class="s1">gbdt = HistGradientBoostingRegressor(loss=</span><span class="s4">&quot;gamma&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">gbdt.fit(np.zeros(shape=(len(y)</span><span class="s0">, </span><span class="s2">1</span><span class="s1">))</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_gamma():</span>
    <span class="s5"># For a Gamma distributed target, we expect an HGBT trained with the Gamma deviance</span>
    <span class="s5"># (loss) to give better results than an HGBT with any other loss function, measured</span>
    <span class="s5"># in out-of-sample Gamma deviance as metric/score.</span>
    <span class="s5"># Note that squared error could potentially predict negative values which is</span>
    <span class="s5"># invalid (np.inf) for the Gamma deviance. A Poisson HGBT (having a log link)</span>
    <span class="s5"># does not have that defect.</span>
    <span class="s5"># Important note: It seems that a Poisson HGBT almost always has better</span>
    <span class="s5"># out-of-sample performance than the Gamma HGBT, measured in Gamma deviance.</span>
    <span class="s5"># LightGBM shows the same behaviour. Hence, we only compare to a squared error</span>
    <span class="s5"># HGBT, but not to a Poisson deviance HGBT.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">n_train</span><span class="s0">, </span><span class="s1">n_test</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s2">500</span><span class="s0">, </span><span class="s2">100</span><span class="s0">, </span><span class="s2">20</span>
    <span class="s1">X = make_low_rank_matrix(</span>
        <span class="s1">n_samples=n_train + n_test</span><span class="s0">,</span>
        <span class="s1">n_features=n_features</span><span class="s0">,</span>
        <span class="s1">random_state=rng</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s5"># We create a log-linear Gamma model. This gives y.min ~ 1e-2, y.max ~ 1e2</span>
    <span class="s1">coef = rng.uniform(low=-</span><span class="s2">10</span><span class="s0">, </span><span class="s1">high=</span><span class="s2">20</span><span class="s0">, </span><span class="s1">size=n_features)</span>
    <span class="s5"># Numpy parametrizes gamma(shape=k, scale=theta) with mean = k * theta and</span>
    <span class="s5"># variance = k * theta^2. We parametrize it instead with mean = exp(X @ coef)</span>
    <span class="s5"># and variance = dispersion * mean^2 by setting k = 1 / dispersion,</span>
    <span class="s5"># theta =  dispersion * mean.</span>
    <span class="s1">dispersion = </span><span class="s2">0.5</span>
    <span class="s1">y = rng.gamma(shape=</span><span class="s2">1 </span><span class="s1">/ dispersion</span><span class="s0">, </span><span class="s1">scale=dispersion * np.exp(X @ coef))</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">test_size=n_test</span><span class="s0">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">gbdt_gamma = HistGradientBoostingRegressor(loss=</span><span class="s4">&quot;gamma&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">123</span><span class="s1">)</span>
    <span class="s1">gbdt_mse = HistGradientBoostingRegressor(loss=</span><span class="s4">&quot;squared_error&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">123</span><span class="s1">)</span>
    <span class="s1">dummy = DummyRegressor(strategy=</span><span class="s4">&quot;mean&quot;</span><span class="s1">)</span>
    <span class="s0">for </span><span class="s1">model </span><span class="s0">in </span><span class="s1">(gbdt_gamma</span><span class="s0">, </span><span class="s1">gbdt_mse</span><span class="s0">, </span><span class="s1">dummy):</span>
        <span class="s1">model.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s0">for </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y </span><span class="s0">in </span><span class="s1">[(X_train</span><span class="s0">, </span><span class="s1">y_train)</span><span class="s0">, </span><span class="s1">(X_test</span><span class="s0">, </span><span class="s1">y_test)]:</span>
        <span class="s1">loss_gbdt_gamma = mean_gamma_deviance(y</span><span class="s0">, </span><span class="s1">gbdt_gamma.predict(X))</span>
        <span class="s5"># We restrict the squared error HGBT to predict at least the minimum seen y at</span>
        <span class="s5"># train time to make it strictly positive.</span>
        <span class="s1">loss_gbdt_mse = mean_gamma_deviance(</span>
            <span class="s1">y</span><span class="s0">, </span><span class="s1">np.maximum(np.min(y_train)</span><span class="s0">, </span><span class="s1">gbdt_mse.predict(X))</span>
        <span class="s1">)</span>
        <span class="s1">loss_dummy = mean_gamma_deviance(y</span><span class="s0">, </span><span class="s1">dummy.predict(X))</span>
        <span class="s0">assert </span><span class="s1">loss_gbdt_gamma &lt; loss_dummy</span>
        <span class="s0">assert </span><span class="s1">loss_gbdt_gamma &lt; loss_gbdt_mse</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;quantile&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">0.2</span><span class="s0">, </span><span class="s2">0.5</span><span class="s0">, </span><span class="s2">0.8</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_quantile_asymmetric_error(quantile):</span>
    <span class="s3">&quot;&quot;&quot;Test quantile regression for asymmetric distributed targets.&quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s2">10_000</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s5"># take care that X @ coef + intercept &gt; 0</span>
    <span class="s1">X = np.concatenate(</span>
        <span class="s1">(</span>
            <span class="s1">np.abs(rng.randn(n_samples)[:</span><span class="s0">, None</span><span class="s1">])</span><span class="s0">,</span>
            <span class="s1">-rng.randint(</span><span class="s2">2</span><span class="s0">, </span><span class="s1">size=(n_samples</span><span class="s0">, </span><span class="s2">1</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">axis=</span><span class="s2">1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">intercept = </span><span class="s2">1.23</span>
    <span class="s1">coef = np.array([</span><span class="s2">0.5</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2</span><span class="s1">])</span>
    <span class="s5"># For an exponential distribution with rate lambda, e.g. exp(-lambda * x),</span>
    <span class="s5"># the quantile at level q is:</span>
    <span class="s5">#   quantile(q) = - log(1 - q) / lambda</span>
    <span class="s5">#   scale = 1/lambda = -quantile(q) / log(1-q)</span>
    <span class="s1">y = rng.exponential(</span>
        <span class="s1">scale=-(X @ coef + intercept) / np.log(</span><span class="s2">1 </span><span class="s1">- quantile)</span><span class="s0">, </span><span class="s1">size=n_samples</span>
    <span class="s1">)</span>
    <span class="s1">model = HistGradientBoostingRegressor(</span>
        <span class="s1">loss=</span><span class="s4">&quot;quantile&quot;</span><span class="s0">,</span>
        <span class="s1">quantile=quantile</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s2">25</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s2">0</span><span class="s0">,</span>
        <span class="s1">max_leaf_nodes=</span><span class="s2">10</span><span class="s0">,</span>
    <span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_allclose(np.mean(model.predict(X) &gt; y)</span><span class="s0">, </span><span class="s1">quantile</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s2">1e-2</span><span class="s1">)</span>

    <span class="s1">pinball_loss = PinballLoss(quantile=quantile)</span>
    <span class="s1">loss_true_quantile = pinball_loss(y</span><span class="s0">, </span><span class="s1">X @ coef + intercept)</span>
    <span class="s1">loss_pred_quantile = pinball_loss(y</span><span class="s0">, </span><span class="s1">model.predict(X))</span>
    <span class="s5"># we are overfitting</span>
    <span class="s0">assert </span><span class="s1">loss_pred_quantile &lt;= loss_true_quantile</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;y&quot;</span><span class="s0">, </span><span class="s1">[([</span><span class="s2">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.0</span><span class="s0">, </span><span class="s2">0.0</span><span class="s1">])</span><span class="s0">, </span><span class="s1">([</span><span class="s2">0.0</span><span class="s0">, </span><span class="s2">0.0</span><span class="s0">, </span><span class="s2">0.0</span><span class="s1">])])</span>
<span class="s0">def </span><span class="s1">test_poisson_y_positive(y):</span>
    <span class="s5"># Test that ValueError is raised if either one y_i &lt; 0 or sum(y_i) &lt;= 0.</span>
    <span class="s1">err_msg = </span><span class="s4">r&quot;loss='poisson' requires non-negative y and sum\(y\) &gt; 0.&quot;</span>
    <span class="s1">gbdt = HistGradientBoostingRegressor(loss=</span><span class="s4">&quot;poisson&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">gbdt.fit(np.zeros(shape=(len(y)</span><span class="s0">, </span><span class="s2">1</span><span class="s1">))</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_poisson():</span>
    <span class="s5"># For Poisson distributed target, Poisson loss should give better results</span>
    <span class="s5"># than least squares measured in Poisson deviance as metric.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">n_train</span><span class="s0">, </span><span class="s1">n_test</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s2">500</span><span class="s0">, </span><span class="s2">100</span><span class="s0">, </span><span class="s2">100</span>
    <span class="s1">X = make_low_rank_matrix(</span>
        <span class="s1">n_samples=n_train + n_test</span><span class="s0">, </span><span class="s1">n_features=n_features</span><span class="s0">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s5"># We create a log-linear Poisson model and downscale coef as it will get</span>
    <span class="s5"># exponentiated.</span>
    <span class="s1">coef = rng.uniform(low=-</span><span class="s2">2</span><span class="s0">, </span><span class="s1">high=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">size=n_features) / np.max(X</span><span class="s0">, </span><span class="s1">axis=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">y = rng.poisson(lam=np.exp(X @ coef))</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">test_size=n_test</span><span class="s0">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">gbdt_pois = HistGradientBoostingRegressor(loss=</span><span class="s4">&quot;poisson&quot;</span><span class="s0">, </span><span class="s1">random_state=rng)</span>
    <span class="s1">gbdt_ls = HistGradientBoostingRegressor(loss=</span><span class="s4">&quot;squared_error&quot;</span><span class="s0">, </span><span class="s1">random_state=rng)</span>
    <span class="s1">gbdt_pois.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">gbdt_ls.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">dummy = DummyRegressor(strategy=</span><span class="s4">&quot;mean&quot;</span><span class="s1">).fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s0">for </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y </span><span class="s0">in </span><span class="s1">[(X_train</span><span class="s0">, </span><span class="s1">y_train)</span><span class="s0">, </span><span class="s1">(X_test</span><span class="s0">, </span><span class="s1">y_test)]:</span>
        <span class="s1">metric_pois = mean_poisson_deviance(y</span><span class="s0">, </span><span class="s1">gbdt_pois.predict(X))</span>
        <span class="s5"># squared_error might produce non-positive predictions =&gt; clip</span>
        <span class="s1">metric_ls = mean_poisson_deviance(y</span><span class="s0">, </span><span class="s1">np.clip(gbdt_ls.predict(X)</span><span class="s0">, </span><span class="s2">1e-15</span><span class="s0">, None</span><span class="s1">))</span>
        <span class="s1">metric_dummy = mean_poisson_deviance(y</span><span class="s0">, </span><span class="s1">dummy.predict(X))</span>
        <span class="s0">assert </span><span class="s1">metric_pois &lt; metric_ls</span>
        <span class="s0">assert </span><span class="s1">metric_pois &lt; metric_dummy</span>


<span class="s0">def </span><span class="s1">test_binning_train_validation_are_separated():</span>
    <span class="s5"># Make sure training and validation data are binned separately.</span>
    <span class="s5"># See issue 13926</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">validation_fraction = </span><span class="s2">0.2</span>
    <span class="s1">gb = HistGradientBoostingClassifier(</span>
        <span class="s1">early_stopping=</span><span class="s0">True, </span><span class="s1">validation_fraction=validation_fraction</span><span class="s0">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">gb.fit(X_classification</span><span class="s0">, </span><span class="s1">y_classification)</span>
    <span class="s1">mapper_training_data = gb._bin_mapper</span>

    <span class="s5"># Note that since the data is small there is no subsampling and the</span>
    <span class="s5"># random_state doesn't matter</span>
    <span class="s1">mapper_whole_data = _BinMapper(random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">mapper_whole_data.fit(X_classification)</span>

    <span class="s1">n_samples = X_classification.shape[</span><span class="s2">0</span><span class="s1">]</span>
    <span class="s0">assert </span><span class="s1">np.all(</span>
        <span class="s1">mapper_training_data.n_bins_non_missing_</span>
        <span class="s1">== int((</span><span class="s2">1 </span><span class="s1">- validation_fraction) * n_samples)</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">np.all(</span>
        <span class="s1">mapper_training_data.n_bins_non_missing_</span>
        <span class="s1">!= mapper_whole_data.n_bins_non_missing_</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_missing_values_trivial():</span>
    <span class="s5"># sanity check for missing values support. With only one feature and</span>
    <span class="s5"># y == isnan(X), the gbdt is supposed to reach perfect accuracy on the</span>
    <span class="s5"># training set.</span>

    <span class="s1">n_samples = </span><span class="s2">100</span>
    <span class="s1">n_features = </span><span class="s2">1</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s0">, </span><span class="s1">n_features))</span>
    <span class="s1">mask = rng.binomial(</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0.5</span><span class="s0">, </span><span class="s1">size=X.shape).astype(bool)</span>
    <span class="s1">X[mask] = np.nan</span>
    <span class="s1">y = mask.ravel()</span>
    <span class="s1">gb = HistGradientBoostingClassifier()</span>
    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">assert </span><span class="s1">gb.score(X</span><span class="s0">, </span><span class="s1">y) == pytest.approx(</span><span class="s2">1</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;problem&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">&quot;classification&quot;</span><span class="s0">, </span><span class="s4">&quot;regression&quot;</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s1">(</span>
        <span class="s4">&quot;missing_proportion, expected_min_score_classification, &quot;</span>
        <span class="s4">&quot;expected_min_score_regression&quot;</span>
    <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">[(</span><span class="s2">0.1</span><span class="s0">, </span><span class="s2">0.97</span><span class="s0">, </span><span class="s2">0.89</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s2">0.2</span><span class="s0">, </span><span class="s2">0.93</span><span class="s0">, </span><span class="s2">0.81</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s2">0.5</span><span class="s0">, </span><span class="s2">0.79</span><span class="s0">, </span><span class="s2">0.52</span><span class="s1">)]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_missing_values_resilience(</span>
    <span class="s1">problem</span><span class="s0">,</span>
    <span class="s1">missing_proportion</span><span class="s0">,</span>
    <span class="s1">expected_min_score_classification</span><span class="s0">,</span>
    <span class="s1">expected_min_score_regression</span><span class="s0">,</span>
<span class="s1">):</span>
    <span class="s5"># Make sure the estimators can deal with missing values and still yield</span>
    <span class="s5"># decent predictions</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s2">1000</span>
    <span class="s1">n_features = </span><span class="s2">2</span>
    <span class="s0">if </span><span class="s1">problem == </span><span class="s4">&quot;regression&quot;</span><span class="s1">:</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(</span>
            <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
            <span class="s1">n_features=n_features</span><span class="s0">,</span>
            <span class="s1">n_informative=n_features</span><span class="s0">,</span>
            <span class="s1">random_state=rng</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">gb = HistGradientBoostingRegressor()</span>
        <span class="s1">expected_min_score = expected_min_score_regression</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span>
            <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
            <span class="s1">n_features=n_features</span><span class="s0">,</span>
            <span class="s1">n_informative=n_features</span><span class="s0">,</span>
            <span class="s1">n_redundant=</span><span class="s2">0</span><span class="s0">,</span>
            <span class="s1">n_repeated=</span><span class="s2">0</span><span class="s0">,</span>
            <span class="s1">random_state=rng</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">gb = HistGradientBoostingClassifier()</span>
        <span class="s1">expected_min_score = expected_min_score_classification</span>

    <span class="s1">mask = rng.binomial(</span><span class="s2">1</span><span class="s0">, </span><span class="s1">missing_proportion</span><span class="s0">, </span><span class="s1">size=X.shape).astype(bool)</span>
    <span class="s1">X[mask] = np.nan</span>

    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">assert </span><span class="s1">gb.score(X</span><span class="s0">, </span><span class="s1">y) &gt; expected_min_score</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;data&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">make_classification(random_state=</span><span class="s2">0</span><span class="s0">, </span><span class="s1">n_classes=</span><span class="s2">2</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">make_classification(random_state=</span><span class="s2">0</span><span class="s0">, </span><span class="s1">n_classes=</span><span class="s2">3</span><span class="s0">, </span><span class="s1">n_informative=</span><span class="s2">3</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
    <span class="s1">ids=[</span><span class="s4">&quot;binary_log_loss&quot;</span><span class="s0">, </span><span class="s4">&quot;multiclass_log_loss&quot;</span><span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_zero_division_hessians(data):</span>
    <span class="s5"># non regression test for issue #14018</span>
    <span class="s5"># make sure we avoid zero division errors when computing the leaves values.</span>

    <span class="s5"># If the learning rate is too high, the raw predictions are bad and will</span>
    <span class="s5"># saturate the softmax (or sigmoid in binary classif). This leads to</span>
    <span class="s5"># probabilities being exactly 0 or 1, gradients being constant, and</span>
    <span class="s5"># hessians being zero.</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = data</span>
    <span class="s1">gb = HistGradientBoostingClassifier(learning_rate=</span><span class="s2">100</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">10</span><span class="s1">)</span>
    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_small_trainset():</span>
    <span class="s5"># Make sure that the small trainset is stratified and has the expected</span>
    <span class="s5"># length (10k samples)</span>
    <span class="s1">n_samples = </span><span class="s2">20000</span>
    <span class="s1">original_distrib = {</span><span class="s2">0</span><span class="s1">: </span><span class="s2">0.1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">: </span><span class="s2">0.2</span><span class="s0">, </span><span class="s2">2</span><span class="s1">: </span><span class="s2">0.3</span><span class="s0">, </span><span class="s2">3</span><span class="s1">: </span><span class="s2">0.4</span><span class="s1">}</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(n_samples).reshape(n_samples</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)</span>
    <span class="s1">y = [</span>
        <span class="s1">[class_] * int(prop * n_samples) </span><span class="s0">for </span><span class="s1">(class_</span><span class="s0">, </span><span class="s1">prop) </span><span class="s0">in </span><span class="s1">original_distrib.items()</span>
    <span class="s1">]</span>
    <span class="s1">y = shuffle(np.concatenate(y))</span>
    <span class="s1">gb = HistGradientBoostingClassifier()</span>

    <span class="s5"># Compute the small training set</span>
    <span class="s1">X_small</span><span class="s0">, </span><span class="s1">y_small</span><span class="s0">, </span><span class="s1">_ = gb._get_small_trainset(</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">seed=</span><span class="s2">42</span><span class="s0">, </span><span class="s1">sample_weight_train=</span><span class="s0">None</span>
    <span class="s1">)</span>

    <span class="s5"># Compute the class distribution in the small training set</span>
    <span class="s1">unique</span><span class="s0">, </span><span class="s1">counts = np.unique(y_small</span><span class="s0">, </span><span class="s1">return_counts=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">small_distrib = {class_: count / </span><span class="s2">10000 </span><span class="s0">for </span><span class="s1">(class_</span><span class="s0">, </span><span class="s1">count) </span><span class="s0">in </span><span class="s1">zip(unique</span><span class="s0">, </span><span class="s1">counts)}</span>

    <span class="s5"># Test that the small training set has the expected length</span>
    <span class="s0">assert </span><span class="s1">X_small.shape[</span><span class="s2">0</span><span class="s1">] == </span><span class="s2">10000</span>
    <span class="s0">assert </span><span class="s1">y_small.shape[</span><span class="s2">0</span><span class="s1">] == </span><span class="s2">10000</span>

    <span class="s5"># Test that the class distributions in the whole dataset and in the small</span>
    <span class="s5"># training set are identical</span>
    <span class="s0">assert </span><span class="s1">small_distrib == pytest.approx(original_distrib)</span>


<span class="s0">def </span><span class="s1">test_missing_values_minmax_imputation():</span>
    <span class="s5"># Compare the buit-in missing value handling of Histogram GBC with an</span>
    <span class="s5"># a-priori missing value imputation strategy that should yield the same</span>
    <span class="s5"># results in terms of decision function.</span>
    <span class="s5">#</span>
    <span class="s5"># Each feature (containing NaNs) is replaced by 2 features:</span>
    <span class="s5"># - one where the nans are replaced by min(feature) - 1</span>
    <span class="s5"># - one where the nans are replaced by max(feature) + 1</span>
    <span class="s5"># A split where nans go to the left has an equivalent split in the</span>
    <span class="s5"># first (min) feature, and a split where nans go to the right has an</span>
    <span class="s5"># equivalent split in the second (max) feature.</span>
    <span class="s5">#</span>
    <span class="s5"># Assuming the data is such that there is never a tie to select the best</span>
    <span class="s5"># feature to split on during training, the learned decision trees should be</span>
    <span class="s5"># strictly equivalent (learn a sequence of splits that encode the same</span>
    <span class="s5"># decision function).</span>
    <span class="s5">#</span>
    <span class="s5"># The MinMaxImputer transformer is meant to be a toy implementation of the</span>
    <span class="s5"># &quot;Missing In Attributes&quot; (MIA) missing value handling for decision trees</span>
    <span class="s5"># https://www.sciencedirect.com/science/article/abs/pii/S0167865508000305</span>
    <span class="s5"># The implementation of MIA as an imputation transformer was suggested by</span>
    <span class="s5"># &quot;Remark 3&quot; in :arxiv:'&lt;1902.06931&gt;`</span>

    <span class="s0">class </span><span class="s1">MinMaxImputer(TransformerMixin</span><span class="s0">, </span><span class="s1">BaseEstimator):</span>
        <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y=</span><span class="s0">None</span><span class="s1">):</span>
            <span class="s1">mm = MinMaxScaler().fit(X)</span>
            <span class="s1">self.data_min_ = mm.data_min_</span>
            <span class="s1">self.data_max_ = mm.data_max_</span>
            <span class="s0">return </span><span class="s1">self</span>

        <span class="s0">def </span><span class="s1">transform(self</span><span class="s0">, </span><span class="s1">X):</span>
            <span class="s1">X_min</span><span class="s0">, </span><span class="s1">X_max = X.copy()</span><span class="s0">, </span><span class="s1">X.copy()</span>

            <span class="s0">for </span><span class="s1">feature_idx </span><span class="s0">in </span><span class="s1">range(X.shape[</span><span class="s2">1</span><span class="s1">]):</span>
                <span class="s1">nan_mask = np.isnan(X[:</span><span class="s0">, </span><span class="s1">feature_idx])</span>
                <span class="s1">X_min[nan_mask</span><span class="s0">, </span><span class="s1">feature_idx] = self.data_min_[feature_idx] - </span><span class="s2">1</span>
                <span class="s1">X_max[nan_mask</span><span class="s0">, </span><span class="s1">feature_idx] = self.data_max_[feature_idx] + </span><span class="s2">1</span>

            <span class="s0">return </span><span class="s1">np.concatenate([X_min</span><span class="s0">, </span><span class="s1">X_max]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s2">1</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">make_missing_value_data(n_samples=int(</span><span class="s2">1e4</span><span class="s1">)</span><span class="s0">, </span><span class="s1">seed=</span><span class="s2">0</span><span class="s1">):</span>
        <span class="s1">rng = np.random.RandomState(seed)</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_samples=n_samples</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s2">4</span><span class="s0">, </span><span class="s1">random_state=rng)</span>

        <span class="s5"># Pre-bin the data to ensure a deterministic handling by the 2</span>
        <span class="s5"># strategies and also make it easier to insert np.nan in a structured</span>
        <span class="s5"># way:</span>
        <span class="s1">X = KBinsDiscretizer(n_bins=</span><span class="s2">42</span><span class="s0">, </span><span class="s1">encode=</span><span class="s4">&quot;ordinal&quot;</span><span class="s1">).fit_transform(X)</span>

        <span class="s5"># First feature has missing values completely at random:</span>
        <span class="s1">rnd_mask = rng.rand(X.shape[</span><span class="s2">0</span><span class="s1">]) &gt; </span><span class="s2">0.9</span>
        <span class="s1">X[rnd_mask</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] = np.nan</span>

        <span class="s5"># Second and third features have missing values for extreme values</span>
        <span class="s5"># (censoring missingness):</span>
        <span class="s1">low_mask = X[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] == </span><span class="s2">0</span>
        <span class="s1">X[low_mask</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] = np.nan</span>

        <span class="s1">high_mask = X[:</span><span class="s0">, </span><span class="s2">2</span><span class="s1">] == X[:</span><span class="s0">, </span><span class="s2">2</span><span class="s1">].max()</span>
        <span class="s1">X[high_mask</span><span class="s0">, </span><span class="s2">2</span><span class="s1">] = np.nan</span>

        <span class="s5"># Make the last feature nan pattern very informative:</span>
        <span class="s1">y_max = np.percentile(y</span><span class="s0">, </span><span class="s2">70</span><span class="s1">)</span>
        <span class="s1">y_max_mask = y &gt;= y_max</span>
        <span class="s1">y[y_max_mask] = y_max</span>
        <span class="s1">X[y_max_mask</span><span class="s0">, </span><span class="s2">3</span><span class="s1">] = np.nan</span>

        <span class="s5"># Check that there is at least one missing value in each feature:</span>
        <span class="s0">for </span><span class="s1">feature_idx </span><span class="s0">in </span><span class="s1">range(X.shape[</span><span class="s2">1</span><span class="s1">]):</span>
            <span class="s0">assert </span><span class="s1">any(np.isnan(X[:</span><span class="s0">, </span><span class="s1">feature_idx]))</span>

        <span class="s5"># Let's use a test set to check that the learned decision function is</span>
        <span class="s5"># the same as evaluated on unseen data. Otherwise it could just be the</span>
        <span class="s5"># case that we find two independent ways to overfit the training set.</span>
        <span class="s0">return </span><span class="s1">train_test_split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=rng)</span>

    <span class="s5"># n_samples need to be large enough to minimize the likelihood of having</span>
    <span class="s5"># several candidate splits with the same gain value in a given tree.</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = make_missing_value_data(</span>
        <span class="s1">n_samples=int(</span><span class="s2">1e4</span><span class="s1">)</span><span class="s0">, </span><span class="s1">seed=</span><span class="s2">0</span>
    <span class="s1">)</span>

    <span class="s5"># Use a small number of leaf nodes and iterations so as to keep</span>
    <span class="s5"># under-fitting models to minimize the likelihood of ties when training the</span>
    <span class="s5"># model.</span>
    <span class="s1">gbm1 = HistGradientBoostingRegressor(max_iter=</span><span class="s2">100</span><span class="s0">, </span><span class="s1">max_leaf_nodes=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">gbm1.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s1">gbm2 = make_pipeline(MinMaxImputer()</span><span class="s0">, </span><span class="s1">clone(gbm1))</span>
    <span class="s1">gbm2.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s5"># Check that the model reach the same score:</span>
    <span class="s0">assert </span><span class="s1">gbm1.score(X_train</span><span class="s0">, </span><span class="s1">y_train) == pytest.approx(gbm2.score(X_train</span><span class="s0">, </span><span class="s1">y_train))</span>

    <span class="s0">assert </span><span class="s1">gbm1.score(X_test</span><span class="s0">, </span><span class="s1">y_test) == pytest.approx(gbm2.score(X_test</span><span class="s0">, </span><span class="s1">y_test))</span>

    <span class="s5"># Check the individual prediction match as a finer grained</span>
    <span class="s5"># decision function check.</span>
    <span class="s1">assert_allclose(gbm1.predict(X_train)</span><span class="s0">, </span><span class="s1">gbm2.predict(X_train))</span>
    <span class="s1">assert_allclose(gbm1.predict(X_test)</span><span class="s0">, </span><span class="s1">gbm2.predict(X_test))</span>


<span class="s0">def </span><span class="s1">test_infinite_values():</span>
    <span class="s5"># Basic test for infinite values</span>

    <span class="s1">X = np.array([-np.inf</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s1">np.inf]).reshape(-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">])</span>

    <span class="s1">gbdt = HistGradientBoostingRegressor(min_samples_leaf=</span><span class="s2">1</span><span class="s1">)</span>
    <span class="s1">gbdt.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">np.testing.assert_allclose(gbdt.predict(X)</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">atol=</span><span class="s2">1e-4</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_consistent_lengths():</span>
    <span class="s1">X = np.array([-np.inf</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s1">np.inf]).reshape(-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">])</span>
    <span class="s1">sample_weight = np.array([</span><span class="s2">0.1</span><span class="s0">, </span><span class="s2">0.3</span><span class="s0">, </span><span class="s2">0.1</span><span class="s1">])</span>
    <span class="s1">gbdt = HistGradientBoostingRegressor()</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">r&quot;sample_weight.shape == \(3,\), expected&quot;</span><span class="s1">):</span>
        <span class="s1">gbdt.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">&quot;Found input variables with inconsistent number&quot;</span>
    <span class="s1">):</span>
        <span class="s1">gbdt.fit(X</span><span class="s0">, </span><span class="s1">y[</span><span class="s2">1</span><span class="s1">:])</span>


<span class="s0">def </span><span class="s1">test_infinite_values_missing_values():</span>
    <span class="s5"># High level test making sure that inf and nan values are properly handled</span>
    <span class="s5"># when both are present. This is similar to</span>
    <span class="s5"># test_split_on_nan_with_infinite_values() in test_grower.py, though we</span>
    <span class="s5"># cannot check the predictions for binned values here.</span>

    <span class="s1">X = np.asarray([-np.inf</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s1">np.inf</span><span class="s0">, </span><span class="s1">np.nan]).reshape(-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)</span>
    <span class="s1">y_isnan = np.isnan(X.ravel())</span>
    <span class="s1">y_isinf = X.ravel() == np.inf</span>

    <span class="s1">stump_clf = HistGradientBoostingClassifier(</span>
        <span class="s1">min_samples_leaf=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">learning_rate=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">max_depth=</span><span class="s2">2</span>
    <span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">stump_clf.fit(X</span><span class="s0">, </span><span class="s1">y_isinf).score(X</span><span class="s0">, </span><span class="s1">y_isinf) == </span><span class="s2">1</span>
    <span class="s0">assert </span><span class="s1">stump_clf.fit(X</span><span class="s0">, </span><span class="s1">y_isnan).score(X</span><span class="s0">, </span><span class="s1">y_isnan) == </span><span class="s2">1</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;scoring&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s4">&quot;loss&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_string_target_early_stopping(scoring):</span>
    <span class="s5"># Regression tests for #14709 where the targets need to be encoded before</span>
    <span class="s5"># to compute the score</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(</span><span class="s2">100</span><span class="s0">, </span><span class="s2">10</span><span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s4">&quot;x&quot;</span><span class="s1">] * </span><span class="s2">50 </span><span class="s1">+ [</span><span class="s4">&quot;y&quot;</span><span class="s1">] * </span><span class="s2">50</span><span class="s0">, </span><span class="s1">dtype=object)</span>
    <span class="s1">gbrt = HistGradientBoostingClassifier(n_iter_no_change=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">scoring=scoring)</span>
    <span class="s1">gbrt.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_zero_sample_weights_regression():</span>
    <span class="s5"># Make sure setting a SW to zero amounts to ignoring the corresponding</span>
    <span class="s5"># sample</span>

    <span class="s1">X = [[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span>
    <span class="s5"># ignore the first 2 training samples by setting their weight to 0</span>
    <span class="s1">sample_weight = [</span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span>
    <span class="s1">gb = HistGradientBoostingRegressor(min_samples_leaf=</span><span class="s2">1</span><span class="s1">)</span>
    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s0">assert </span><span class="s1">gb.predict([[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]])[</span><span class="s2">0</span><span class="s1">] &gt; </span><span class="s2">0.5</span>


<span class="s0">def </span><span class="s1">test_zero_sample_weights_classification():</span>
    <span class="s5"># Make sure setting a SW to zero amounts to ignoring the corresponding</span>
    <span class="s5"># sample</span>

    <span class="s1">X = [[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span>
    <span class="s5"># ignore the first 2 training samples by setting their weight to 0</span>
    <span class="s1">sample_weight = [</span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span>
    <span class="s1">gb = HistGradientBoostingClassifier(loss=</span><span class="s4">&quot;log_loss&quot;</span><span class="s0">, </span><span class="s1">min_samples_leaf=</span><span class="s2">1</span><span class="s1">)</span>
    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_array_equal(gb.predict([[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s1">])</span>

    <span class="s1">X = [[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span>
    <span class="s5"># ignore the first 2 training samples by setting their weight to 0</span>
    <span class="s1">sample_weight = [</span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span>
    <span class="s1">gb = HistGradientBoostingClassifier(loss=</span><span class="s4">&quot;log_loss&quot;</span><span class="s0">, </span><span class="s1">min_samples_leaf=</span><span class="s2">1</span><span class="s1">)</span>
    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_array_equal(gb.predict([[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;problem&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">&quot;regression&quot;</span><span class="s0">, </span><span class="s4">&quot;binary_classification&quot;</span><span class="s0">, </span><span class="s4">&quot;multiclass_classification&quot;</span><span class="s1">)</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;duplication&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">&quot;half&quot;</span><span class="s0">, </span><span class="s4">&quot;all&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_sample_weight_effect(problem</span><span class="s0">, </span><span class="s1">duplication):</span>
    <span class="s5"># High level test to make sure that duplicating a sample is equivalent to</span>
    <span class="s5"># giving it weight of 2.</span>

    <span class="s5"># fails for n_samples &gt; 255 because binning does not take sample weights</span>
    <span class="s5"># into account. Keeping n_samples &lt;= 255 makes</span>
    <span class="s5"># sure only unique values are used so SW have no effect on binning.</span>
    <span class="s1">n_samples = </span><span class="s2">255</span>
    <span class="s1">n_features = </span><span class="s2">2</span>
    <span class="s0">if </span><span class="s1">problem == </span><span class="s4">&quot;regression&quot;</span><span class="s1">:</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(</span>
            <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
            <span class="s1">n_features=n_features</span><span class="s0">,</span>
            <span class="s1">n_informative=n_features</span><span class="s0">,</span>
            <span class="s1">random_state=</span><span class="s2">0</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">Klass = HistGradientBoostingRegressor</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">n_classes = </span><span class="s2">2 </span><span class="s0">if </span><span class="s1">problem == </span><span class="s4">&quot;binary_classification&quot; </span><span class="s0">else </span><span class="s2">3</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span>
            <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
            <span class="s1">n_features=n_features</span><span class="s0">,</span>
            <span class="s1">n_informative=n_features</span><span class="s0">,</span>
            <span class="s1">n_redundant=</span><span class="s2">0</span><span class="s0">,</span>
            <span class="s1">n_clusters_per_class=</span><span class="s2">1</span><span class="s0">,</span>
            <span class="s1">n_classes=n_classes</span><span class="s0">,</span>
            <span class="s1">random_state=</span><span class="s2">0</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">Klass = HistGradientBoostingClassifier</span>

    <span class="s5"># This test can't pass if min_samples_leaf &gt; 1 because that would force 2</span>
    <span class="s5"># samples to be in the same node in est_sw, while these samples would be</span>
    <span class="s5"># free to be separate in est_dup: est_dup would just group together the</span>
    <span class="s5"># duplicated samples.</span>
    <span class="s1">est = Klass(min_samples_leaf=</span><span class="s2">1</span><span class="s1">)</span>

    <span class="s5"># Create dataset with duplicate and corresponding sample weights</span>
    <span class="s0">if </span><span class="s1">duplication == </span><span class="s4">&quot;half&quot;</span><span class="s1">:</span>
        <span class="s1">lim = n_samples // </span><span class="s2">2</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">lim = n_samples</span>
    <span class="s1">X_dup = np.r_[X</span><span class="s0">, </span><span class="s1">X[:lim]]</span>
    <span class="s1">y_dup = np.r_[y</span><span class="s0">, </span><span class="s1">y[:lim]]</span>
    <span class="s1">sample_weight = np.ones(shape=(n_samples))</span>
    <span class="s1">sample_weight[:lim] = </span><span class="s2">2</span>

    <span class="s1">est_sw = clone(est).fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">est_dup = clone(est).fit(X_dup</span><span class="s0">, </span><span class="s1">y_dup)</span>

    <span class="s5"># checking raw_predict is stricter than just predict for classification</span>
    <span class="s0">assert </span><span class="s1">np.allclose(est_sw._raw_predict(X_dup)</span><span class="s0">, </span><span class="s1">est_dup._raw_predict(X_dup))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Loss&quot;</span><span class="s0">, </span><span class="s1">(HalfSquaredError</span><span class="s0">, </span><span class="s1">AbsoluteError))</span>
<span class="s0">def </span><span class="s1">test_sum_hessians_are_sample_weight(Loss):</span>
    <span class="s5"># For losses with constant hessians, the sum_hessians field of the</span>
    <span class="s5"># histograms must be equal to the sum of the sample weight of samples at</span>
    <span class="s5"># the corresponding bin.</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s2">1000</span>
    <span class="s1">n_features = </span><span class="s2">2</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_samples=n_samples</span><span class="s0">, </span><span class="s1">n_features=n_features</span><span class="s0">, </span><span class="s1">random_state=rng)</span>
    <span class="s1">bin_mapper = _BinMapper()</span>
    <span class="s1">X_binned = bin_mapper.fit_transform(X)</span>

    <span class="s5"># While sample weights are supposed to be positive, this still works.</span>
    <span class="s1">sample_weight = rng.normal(size=n_samples)</span>

    <span class="s1">loss = Loss(sample_weight=sample_weight)</span>
    <span class="s1">gradients</span><span class="s0">, </span><span class="s1">hessians = loss.init_gradient_and_hessian(</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE</span>
    <span class="s1">)</span>
    <span class="s1">gradients</span><span class="s0">, </span><span class="s1">hessians = gradients.reshape((-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">))</span><span class="s0">, </span><span class="s1">hessians.reshape((-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">))</span>
    <span class="s1">raw_predictions = rng.normal(size=(n_samples</span><span class="s0">, </span><span class="s2">1</span><span class="s1">))</span>
    <span class="s1">loss.gradient_hessian(</span>
        <span class="s1">y_true=y</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_predictions</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">gradient_out=gradients</span><span class="s0">,</span>
        <span class="s1">hessian_out=hessians</span><span class="s0">,</span>
        <span class="s1">n_threads=n_threads</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s5"># build sum_sample_weight which contains the sum of the sample weights at</span>
    <span class="s5"># each bin (for each feature). This must be equal to the sum_hessians</span>
    <span class="s5"># field of the corresponding histogram</span>
    <span class="s1">sum_sw = np.zeros(shape=(n_features</span><span class="s0">, </span><span class="s1">bin_mapper.n_bins))</span>
    <span class="s0">for </span><span class="s1">feature_idx </span><span class="s0">in </span><span class="s1">range(n_features):</span>
        <span class="s0">for </span><span class="s1">sample_idx </span><span class="s0">in </span><span class="s1">range(n_samples):</span>
            <span class="s1">sum_sw[feature_idx</span><span class="s0">, </span><span class="s1">X_binned[sample_idx</span><span class="s0">, </span><span class="s1">feature_idx]] += sample_weight[</span>
                <span class="s1">sample_idx</span>
            <span class="s1">]</span>

    <span class="s5"># Build histogram</span>
    <span class="s1">grower = TreeGrower(</span>
        <span class="s1">X_binned</span><span class="s0">, </span><span class="s1">gradients[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">hessians[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">n_bins=bin_mapper.n_bins</span>
    <span class="s1">)</span>
    <span class="s1">histograms = grower.histogram_builder.compute_histograms_brute(</span>
        <span class="s1">grower.root.sample_indices</span>
    <span class="s1">)</span>

    <span class="s0">for </span><span class="s1">feature_idx </span><span class="s0">in </span><span class="s1">range(n_features):</span>
        <span class="s0">for </span><span class="s1">bin_idx </span><span class="s0">in </span><span class="s1">range(bin_mapper.n_bins):</span>
            <span class="s0">assert </span><span class="s1">histograms[feature_idx</span><span class="s0">, </span><span class="s1">bin_idx][</span><span class="s4">&quot;sum_hessians&quot;</span><span class="s1">] == (</span>
                <span class="s1">pytest.approx(sum_sw[feature_idx</span><span class="s0">, </span><span class="s1">bin_idx]</span><span class="s0">, </span><span class="s1">rel=</span><span class="s2">1e-5</span><span class="s1">)</span>
            <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_max_depth_max_leaf_nodes():</span>
    <span class="s5"># Non regression test for</span>
    <span class="s5"># https://github.com/scikit-learn/scikit-learn/issues/16179</span>
    <span class="s5"># there was a bug when the max_depth and the max_leaf_nodes criteria were</span>
    <span class="s5"># met at the same time, which would lead to max_leaf_nodes not being</span>
    <span class="s5"># respected.</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">est = HistGradientBoostingClassifier(max_depth=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">max_leaf_nodes=</span><span class="s2">3</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">1</span><span class="s1">).fit(</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span>
    <span class="s1">)</span>
    <span class="s1">tree = est._predictors[</span><span class="s2">0</span><span class="s1">][</span><span class="s2">0</span><span class="s1">]</span>
    <span class="s0">assert </span><span class="s1">tree.get_max_depth() == </span><span class="s2">2</span>
    <span class="s0">assert </span><span class="s1">tree.get_n_leaf_nodes() == </span><span class="s2">3  </span><span class="s5"># would be 4 prior to bug fix</span>


<span class="s0">def </span><span class="s1">test_early_stopping_on_test_set_with_warm_start():</span>
    <span class="s5"># Non regression test for #16661 where second fit fails with</span>
    <span class="s5"># warm_start=True, early_stopping is on, and no validation set</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">gb = HistGradientBoostingClassifier(</span>
        <span class="s1">max_iter=</span><span class="s2">1</span><span class="s0">,</span>
        <span class="s1">scoring=</span><span class="s4">&quot;loss&quot;</span><span class="s0">,</span>
        <span class="s1">warm_start=</span><span class="s0">True,</span>
        <span class="s1">early_stopping=</span><span class="s0">True,</span>
        <span class="s1">n_iter_no_change=</span><span class="s2">1</span><span class="s0">,</span>
        <span class="s1">validation_fraction=</span><span class="s0">None,</span>
    <span class="s1">)</span>

    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s5"># does not raise on second call</span>
    <span class="s1">gb.set_params(max_iter=</span><span class="s2">2</span><span class="s1">)</span>
    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">HistGradientBoostingRegressor)</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_single_node_trees(Est):</span>
    <span class="s5"># Make sure it's still possible to build single-node trees. In that case</span>
    <span class="s5"># the value of the root is set to 0. That's a correct value: if the tree is</span>
    <span class="s5"># single-node that's because min_gain_to_split is not respected right from</span>
    <span class="s5"># the root, so we don't want the tree to have any impact on the</span>
    <span class="s5"># predictions.</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">y[:] = </span><span class="s2">1  </span><span class="s5"># constant target will lead to a single root node</span>

    <span class="s1">est = Est(max_iter=</span><span class="s2">20</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">assert </span><span class="s1">all(len(predictor[</span><span class="s2">0</span><span class="s1">].nodes) == </span><span class="s2">1 </span><span class="s0">for </span><span class="s1">predictor </span><span class="s0">in </span><span class="s1">est._predictors)</span>
    <span class="s0">assert </span><span class="s1">all(predictor[</span><span class="s2">0</span><span class="s1">].nodes[</span><span class="s2">0</span><span class="s1">][</span><span class="s4">&quot;value&quot;</span><span class="s1">] == </span><span class="s2">0 </span><span class="s0">for </span><span class="s1">predictor </span><span class="s0">in </span><span class="s1">est._predictors)</span>
    <span class="s5"># Still gives correct predictions thanks to the baseline prediction</span>
    <span class="s1">assert_allclose(est.predict(X)</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;Est, loss, X, y&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">HistGradientBoostingClassifier</span><span class="s0">,</span>
            <span class="s1">HalfBinomialLoss(sample_weight=</span><span class="s0">None</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">X_classification</span><span class="s0">,</span>
            <span class="s1">y_classification</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">HistGradientBoostingRegressor</span><span class="s0">,</span>
            <span class="s1">HalfSquaredError(sample_weight=</span><span class="s0">None</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">X_regression</span><span class="s0">,</span>
            <span class="s1">y_regression</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_custom_loss(Est</span><span class="s0">, </span><span class="s1">loss</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s1">est = Est(loss=loss</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">20</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;HistGradientBoosting, X, y&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">X_classification</span><span class="s0">, </span><span class="s1">y_classification)</span><span class="s0">,</span>
        <span class="s1">(HistGradientBoostingRegressor</span><span class="s0">, </span><span class="s1">X_regression</span><span class="s0">, </span><span class="s1">y_regression)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">HistGradientBoostingClassifier</span><span class="s0">,</span>
            <span class="s1">X_multi_classification</span><span class="s0">,</span>
            <span class="s1">y_multi_classification</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_staged_predict(HistGradientBoosting</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s5"># Test whether staged predictor eventually gives</span>
    <span class="s5"># the same prediction.</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">test_size=</span><span class="s2">0.5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span>
    <span class="s1">)</span>
    <span class="s1">gb = HistGradientBoosting(max_iter=</span><span class="s2">10</span><span class="s1">)</span>

    <span class="s5"># test raise NotFittedError if not fitted</span>
    <span class="s0">with </span><span class="s1">pytest.raises(NotFittedError):</span>
        <span class="s1">next(gb.staged_predict(X_test))</span>

    <span class="s1">gb.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s5"># test if the staged predictions of each iteration</span>
    <span class="s5"># are equal to the corresponding predictions of the same estimator</span>
    <span class="s5"># trained from scratch.</span>
    <span class="s5"># this also test limit case when max_iter = 1</span>
    <span class="s1">method_names = (</span>
        <span class="s1">[</span><span class="s4">&quot;predict&quot;</span><span class="s1">]</span>
        <span class="s0">if </span><span class="s1">is_regressor(gb)</span>
        <span class="s0">else </span><span class="s1">[</span><span class="s4">&quot;predict&quot;</span><span class="s0">, </span><span class="s4">&quot;predict_proba&quot;</span><span class="s0">, </span><span class="s4">&quot;decision_function&quot;</span><span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s0">for </span><span class="s1">method_name </span><span class="s0">in </span><span class="s1">method_names:</span>
        <span class="s1">staged_method = getattr(gb</span><span class="s0">, </span><span class="s4">&quot;staged_&quot; </span><span class="s1">+ method_name)</span>
        <span class="s1">staged_predictions = list(staged_method(X_test))</span>
        <span class="s0">assert </span><span class="s1">len(staged_predictions) == gb.n_iter_</span>
        <span class="s0">for </span><span class="s1">n_iter</span><span class="s0">, </span><span class="s1">staged_predictions </span><span class="s0">in </span><span class="s1">enumerate(staged_method(X_test)</span><span class="s0">, </span><span class="s2">1</span><span class="s1">):</span>
            <span class="s1">aux = HistGradientBoosting(max_iter=n_iter)</span>
            <span class="s1">aux.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
            <span class="s1">pred_aux = getattr(aux</span><span class="s0">, </span><span class="s1">method_name)(X_test)</span>

            <span class="s1">assert_allclose(staged_predictions</span><span class="s0">, </span><span class="s1">pred_aux)</span>
            <span class="s0">assert </span><span class="s1">staged_predictions.shape == pred_aux.shape</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;insert_missing&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">False, True</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HistGradientBoostingRegressor</span><span class="s0">, </span><span class="s1">HistGradientBoostingClassifier)</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;bool_categorical_parameter&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;missing_value&quot;</span><span class="s0">, </span><span class="s1">[np.nan</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_unknown_categories_nan(</span>
    <span class="s1">insert_missing</span><span class="s0">, </span><span class="s1">Est</span><span class="s0">, </span><span class="s1">bool_categorical_parameter</span><span class="s0">, </span><span class="s1">missing_value</span>
<span class="s1">):</span>
    <span class="s5"># Make sure no error is raised at predict if a category wasn't seen during</span>
    <span class="s5"># fit. We also make sure they're treated as nans.</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s2">1000</span>
    <span class="s1">f1 = rng.rand(n_samples)</span>
    <span class="s1">f2 = rng.randint(</span><span class="s2">4</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
    <span class="s1">X = np.c_[f1</span><span class="s0">, </span><span class="s1">f2]</span>
    <span class="s1">y = np.zeros(shape=n_samples)</span>
    <span class="s1">y[X[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] % </span><span class="s2">2 </span><span class="s1">== </span><span class="s2">0</span><span class="s1">] = </span><span class="s2">1</span>

    <span class="s0">if </span><span class="s1">bool_categorical_parameter:</span>
        <span class="s1">categorical_features = [</span><span class="s0">False, True</span><span class="s1">]</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">categorical_features = [</span><span class="s2">1</span><span class="s1">]</span>

    <span class="s0">if </span><span class="s1">insert_missing:</span>
        <span class="s1">mask = rng.binomial(</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0.01</span><span class="s0">, </span><span class="s1">size=X.shape).astype(bool)</span>
        <span class="s0">assert </span><span class="s1">mask.sum() &gt; </span><span class="s2">0</span>
        <span class="s1">X[mask] = missing_value</span>

    <span class="s1">est = Est(max_iter=</span><span class="s2">20</span><span class="s0">, </span><span class="s1">categorical_features=categorical_features).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(est.is_categorical_</span><span class="s0">, </span><span class="s1">[</span><span class="s0">False, True</span><span class="s1">])</span>

    <span class="s5"># Make sure no error is raised on unknown categories and nans</span>
    <span class="s5"># unknown categories will be treated as nans</span>
    <span class="s1">X_test = np.zeros((</span><span class="s2">10</span><span class="s0">, </span><span class="s1">X.shape[</span><span class="s2">1</span><span class="s1">])</span><span class="s0">, </span><span class="s1">dtype=float)</span>
    <span class="s1">X_test[:</span><span class="s2">5</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] = </span><span class="s2">30</span>
    <span class="s1">X_test[</span><span class="s2">5</span><span class="s1">:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] = missing_value</span>
    <span class="s0">assert </span><span class="s1">len(np.unique(est.predict(X_test))) == </span><span class="s2">1</span>


<span class="s0">def </span><span class="s1">test_categorical_encoding_strategies():</span>
    <span class="s5"># Check native categorical handling vs different encoding strategies. We</span>
    <span class="s5"># make sure that native encoding needs only 1 split to achieve a perfect</span>
    <span class="s5"># prediction on a simple dataset. In contrast, OneHotEncoded data needs</span>
    <span class="s5"># more depth / splits, and treating categories as ordered (just using</span>
    <span class="s5"># OrdinalEncoder) requires even more depth.</span>

    <span class="s5"># dataset with one random continuous feature, and one categorical feature</span>
    <span class="s5"># with values in [0, 5], e.g. from an OrdinalEncoder.</span>
    <span class="s5"># class == 1 iff categorical value in {0, 2, 4}</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s2">10_000</span>
    <span class="s1">f1 = rng.rand(n_samples)</span>
    <span class="s1">f2 = rng.randint(</span><span class="s2">6</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
    <span class="s1">X = np.c_[f1</span><span class="s0">, </span><span class="s1">f2]</span>
    <span class="s1">y = np.zeros(shape=n_samples)</span>
    <span class="s1">y[X[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] % </span><span class="s2">2 </span><span class="s1">== </span><span class="s2">0</span><span class="s1">] = </span><span class="s2">1</span>

    <span class="s5"># make sure dataset is balanced so that the baseline_prediction doesn't</span>
    <span class="s5"># influence predictions too much with max_iter = 1</span>
    <span class="s0">assert </span><span class="s2">0.49 </span><span class="s1">&lt; y.mean() &lt; </span><span class="s2">0.51</span>

    <span class="s1">native_cat_specs = [</span>
        <span class="s1">[</span><span class="s0">False, True</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s2">1</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s0">import </span><span class="s1">pandas </span><span class="s0">as </span><span class="s1">pd</span>

        <span class="s1">X = pd.DataFrame(X</span><span class="s0">, </span><span class="s1">columns=[</span><span class="s4">&quot;f_0&quot;</span><span class="s0">, </span><span class="s4">&quot;f_1&quot;</span><span class="s1">])</span>
        <span class="s1">native_cat_specs.append([</span><span class="s4">&quot;f_1&quot;</span><span class="s1">])</span>
    <span class="s0">except </span><span class="s1">ImportError:</span>
        <span class="s0">pass</span>

    <span class="s0">for </span><span class="s1">native_cat_spec </span><span class="s0">in </span><span class="s1">native_cat_specs:</span>
        <span class="s1">clf_cat = HistGradientBoostingClassifier(</span>
            <span class="s1">max_iter=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">max_depth=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">categorical_features=native_cat_spec</span>
        <span class="s1">)</span>

        <span class="s5"># Using native categorical encoding, we get perfect predictions with just</span>
        <span class="s5"># one split</span>
        <span class="s0">assert </span><span class="s1">cross_val_score(clf_cat</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y).mean() == </span><span class="s2">1</span>

    <span class="s5"># quick sanity check for the bitset: 0, 2, 4 = 2**0 + 2**2 + 2**4 = 21</span>
    <span class="s1">expected_left_bitset = [</span><span class="s2">21</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span>
    <span class="s1">left_bitset = clf_cat.fit(X</span><span class="s0">, </span><span class="s1">y)._predictors[</span><span class="s2">0</span><span class="s1">][</span><span class="s2">0</span><span class="s1">].raw_left_cat_bitsets[</span><span class="s2">0</span><span class="s1">]</span>
    <span class="s1">assert_array_equal(left_bitset</span><span class="s0">, </span><span class="s1">expected_left_bitset)</span>

    <span class="s5"># Treating categories as ordered, we need more depth / more splits to get</span>
    <span class="s5"># the same predictions</span>
    <span class="s1">clf_no_cat = HistGradientBoostingClassifier(</span>
        <span class="s1">max_iter=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">max_depth=</span><span class="s2">4</span><span class="s0">, </span><span class="s1">categorical_features=</span><span class="s0">None</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">cross_val_score(clf_no_cat</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y).mean() &lt; </span><span class="s2">0.9</span>

    <span class="s1">clf_no_cat.set_params(max_depth=</span><span class="s2">5</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">cross_val_score(clf_no_cat</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y).mean() == </span><span class="s2">1</span>

    <span class="s5"># Using OHEd data, we need less splits than with pure OEd data, but we</span>
    <span class="s5"># still need more splits than with the native categorical splits</span>
    <span class="s1">ct = make_column_transformer(</span>
        <span class="s1">(OneHotEncoder(sparse_output=</span><span class="s0">False</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s1">])</span><span class="s0">, </span><span class="s1">remainder=</span><span class="s4">&quot;passthrough&quot;</span>
    <span class="s1">)</span>
    <span class="s1">X_ohe = ct.fit_transform(X)</span>
    <span class="s1">clf_no_cat.set_params(max_depth=</span><span class="s2">2</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">cross_val_score(clf_no_cat</span><span class="s0">, </span><span class="s1">X_ohe</span><span class="s0">, </span><span class="s1">y).mean() &lt; </span><span class="s2">0.9</span>

    <span class="s1">clf_no_cat.set_params(max_depth=</span><span class="s2">3</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">cross_val_score(clf_no_cat</span><span class="s0">, </span><span class="s1">X_ohe</span><span class="s0">, </span><span class="s1">y).mean() == </span><span class="s2">1</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">HistGradientBoostingRegressor)</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;categorical_features, monotonic_cst, expected_msg&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">[</span><span class="s6">b&quot;hello&quot;</span><span class="s0">, </span><span class="s6">b&quot;world&quot;</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s0">None,</span>
            <span class="s1">re.escape(</span>
                <span class="s4">&quot;categorical_features must be an array-like of bool, int or str, &quot;</span>
                <span class="s4">&quot;got: bytes40.&quot;</span>
            <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">np.array([</span><span class="s6">b&quot;hello&quot;</span><span class="s0">, </span><span class="s2">1.3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=object)</span><span class="s0">,</span>
            <span class="s0">None,</span>
            <span class="s1">re.escape(</span>
                <span class="s4">&quot;categorical_features must be an array-like of bool, int or str, &quot;</span>
                <span class="s4">&quot;got: bytes, float.&quot;</span>
            <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">[</span><span class="s2">0</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s0">None,</span>
            <span class="s1">re.escape(</span>
                <span class="s4">&quot;categorical_features set as integer indices must be in &quot;</span>
                <span class="s4">&quot;[0, n_features - 1]&quot;</span>
            <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">[</span><span class="s0">True, True, False, False, True</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s0">None,</span>
            <span class="s1">re.escape(</span>
                <span class="s4">&quot;categorical_features set as a boolean mask must have shape &quot;</span>
                <span class="s4">&quot;(n_features,)&quot;</span>
            <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">[</span><span class="s0">True, True, False, False</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s2">0</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s4">&quot;Categorical features cannot have monotonic constraints&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_categorical_spec_errors(</span>
    <span class="s1">Est</span><span class="s0">, </span><span class="s1">categorical_features</span><span class="s0">, </span><span class="s1">monotonic_cst</span><span class="s0">, </span><span class="s1">expected_msg</span>
<span class="s1">):</span>
    <span class="s5"># Test errors when categories are specified incorrectly</span>
    <span class="s1">n_samples = </span><span class="s2">100</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(random_state=</span><span class="s2">0</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s2">4</span><span class="s0">, </span><span class="s1">n_samples=n_samples)</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">X[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] = rng.randint(</span><span class="s2">0</span><span class="s0">, </span><span class="s2">10</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
    <span class="s1">X[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] = rng.randint(</span><span class="s2">0</span><span class="s0">, </span><span class="s2">10</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
    <span class="s1">est = Est(categorical_features=categorical_features</span><span class="s0">, </span><span class="s1">monotonic_cst=monotonic_cst)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=expected_msg):</span>
        <span class="s1">est.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">HistGradientBoostingRegressor)</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_categorical_spec_errors_with_feature_names(Est):</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s4">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s2">10</span>
    <span class="s1">X = pd.DataFrame(</span>
        <span class="s1">{</span>
            <span class="s4">&quot;f0&quot;</span><span class="s1">: range(n_samples)</span><span class="s0">,</span>
            <span class="s4">&quot;f1&quot;</span><span class="s1">: range(n_samples)</span><span class="s0">,</span>
            <span class="s4">&quot;f2&quot;</span><span class="s1">: [</span><span class="s2">1.0</span><span class="s1">] * n_samples</span><span class="s0">,</span>
        <span class="s1">}</span>
    <span class="s1">)</span>
    <span class="s1">y = [</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] * (n_samples // </span><span class="s2">2</span><span class="s1">)</span>

    <span class="s1">est = Est(categorical_features=[</span><span class="s4">&quot;f0&quot;</span><span class="s0">, </span><span class="s4">&quot;f1&quot;</span><span class="s0">, </span><span class="s4">&quot;f3&quot;</span><span class="s1">])</span>
    <span class="s1">expected_msg = re.escape(</span>
        <span class="s4">&quot;categorical_features has a item value 'f3' which is not a valid &quot;</span>
        <span class="s4">&quot;feature name of the training data.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=expected_msg):</span>
        <span class="s1">est.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">est = Est(categorical_features=[</span><span class="s4">&quot;f0&quot;</span><span class="s0">, </span><span class="s4">&quot;f1&quot;</span><span class="s1">])</span>
    <span class="s1">expected_msg = re.escape(</span>
        <span class="s4">&quot;categorical_features should be passed as an array of integers or &quot;</span>
        <span class="s4">&quot;as a boolean mask when the model is fitted on data without feature &quot;</span>
        <span class="s4">&quot;names.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=expected_msg):</span>
        <span class="s1">est.fit(X.to_numpy()</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">HistGradientBoostingRegressor)</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;categorical_features&quot;</span><span class="s0">, </span><span class="s1">([</span><span class="s0">False, False</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[]))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;as_array&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s0">True, False</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_categorical_spec_no_categories(Est</span><span class="s0">, </span><span class="s1">categorical_features</span><span class="s0">, </span><span class="s1">as_array):</span>
    <span class="s5"># Make sure we can properly detect that no categorical features are present</span>
    <span class="s5"># even if the categorical_features parameter is not None</span>
    <span class="s1">X = np.arange(</span><span class="s2">10</span><span class="s1">).reshape(</span><span class="s2">5</span><span class="s0">, </span><span class="s2">2</span><span class="s1">)</span>
    <span class="s1">y = np.arange(</span><span class="s2">5</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">as_array:</span>
        <span class="s1">categorical_features = np.asarray(categorical_features)</span>
    <span class="s1">est = Est(categorical_features=categorical_features).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">est.is_categorical_ </span><span class="s0">is None</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">HistGradientBoostingRegressor)</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;use_pandas, feature_name&quot;</span><span class="s0">, </span><span class="s1">[(</span><span class="s0">False, </span><span class="s4">&quot;at index 0&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s0">True, </span><span class="s4">&quot;'f0'&quot;</span><span class="s1">)]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_categorical_bad_encoding_errors(Est</span><span class="s0">, </span><span class="s1">use_pandas</span><span class="s0">, </span><span class="s1">feature_name):</span>
    <span class="s5"># Test errors when categories are encoded incorrectly</span>

    <span class="s1">gb = Est(categorical_features=[</span><span class="s0">True</span><span class="s1">]</span><span class="s0">, </span><span class="s1">max_bins=</span><span class="s2">2</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">use_pandas:</span>
        <span class="s1">pd = pytest.importorskip(</span><span class="s4">&quot;pandas&quot;</span><span class="s1">)</span>
        <span class="s1">X = pd.DataFrame({</span><span class="s4">&quot;f0&quot;</span><span class="s1">: [</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]})</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">X = np.array([[</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]]).T</span>
    <span class="s1">y = np.arange(</span><span class="s2">3</span><span class="s1">)</span>
    <span class="s1">msg = (</span>
        <span class="s4">f&quot;Categorical feature </span><span class="s0">{</span><span class="s1">feature_name</span><span class="s0">} </span><span class="s4">is expected to have a &quot;</span>
        <span class="s4">&quot;cardinality &lt;= 2 but actually has a cardinality of 3.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">use_pandas:</span>
        <span class="s1">X = pd.DataFrame({</span><span class="s4">&quot;f0&quot;</span><span class="s1">: [</span><span class="s2">0</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]})</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">X = np.array([[</span><span class="s2">0</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]]).T</span>
    <span class="s1">y = np.arange(</span><span class="s2">2</span><span class="s1">)</span>
    <span class="s1">msg = (</span>
        <span class="s4">f&quot;Categorical feature </span><span class="s0">{</span><span class="s1">feature_name</span><span class="s0">} </span><span class="s4">is expected to be encoded &quot;</span>
        <span class="s4">&quot;with values &lt; 2 but the largest value for the encoded categories &quot;</span>
        <span class="s4">&quot;is 2.0.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s5"># nans are ignored in the counts</span>
    <span class="s1">X = np.array([[</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s1">np.nan]]).T</span>
    <span class="s1">y = np.arange(</span><span class="s2">3</span><span class="s1">)</span>
    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">HistGradientBoostingRegressor)</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_uint8_predict(Est):</span>
    <span class="s5"># Non regression test for</span>
    <span class="s5"># https://github.com/scikit-learn/scikit-learn/issues/18408</span>
    <span class="s5"># Make sure X can be of dtype uint8 (i.e. X_BINNED_DTYPE) in predict. It</span>
    <span class="s5"># will be converted to X_DTYPE.</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s1">X = rng.randint(</span><span class="s2">0</span><span class="s0">, </span><span class="s2">100</span><span class="s0">, </span><span class="s1">size=(</span><span class="s2">10</span><span class="s0">, </span><span class="s2">2</span><span class="s1">)).astype(np.uint8)</span>
    <span class="s1">y = rng.randint(</span><span class="s2">0</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s1">size=</span><span class="s2">10</span><span class="s1">).astype(np.uint8)</span>
    <span class="s1">est = Est()</span>
    <span class="s1">est.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">est.predict(X)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;interaction_cst, n_features, result&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s0">None, </span><span class="s2">931</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">([{</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">}]</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s1">[{</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">}])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;pairwise&quot;</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s1">[{</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">}])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;pairwise&quot;</span><span class="s0">, </span><span class="s2">4</span><span class="s0">, </span><span class="s1">[{</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">}</span><span class="s0">, </span><span class="s1">{</span><span class="s2">0</span><span class="s0">, </span><span class="s2">2</span><span class="s1">}</span><span class="s0">, </span><span class="s1">{</span><span class="s2">0</span><span class="s0">, </span><span class="s2">3</span><span class="s1">}</span><span class="s0">, </span><span class="s1">{</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">}</span><span class="s0">, </span><span class="s1">{</span><span class="s2">1</span><span class="s0">, </span><span class="s2">3</span><span class="s1">}</span><span class="s0">, </span><span class="s1">{</span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">}])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;no_interactions&quot;</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s1">[{</span><span class="s2">0</span><span class="s1">}</span><span class="s0">, </span><span class="s1">{</span><span class="s2">1</span><span class="s1">}])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;no_interactions&quot;</span><span class="s0">, </span><span class="s2">4</span><span class="s0">, </span><span class="s1">[{</span><span class="s2">0</span><span class="s1">}</span><span class="s0">, </span><span class="s1">{</span><span class="s2">1</span><span class="s1">}</span><span class="s0">, </span><span class="s1">{</span><span class="s2">2</span><span class="s1">}</span><span class="s0">, </span><span class="s1">{</span><span class="s2">3</span><span class="s1">}])</span><span class="s0">,</span>
        <span class="s1">([(</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[</span><span class="s2">5</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s2">6</span><span class="s0">, </span><span class="s1">[{</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">}</span><span class="s0">, </span><span class="s1">{</span><span class="s2">1</span><span class="s0">, </span><span class="s2">5</span><span class="s1">}</span><span class="s0">, </span><span class="s1">{</span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s2">4</span><span class="s1">}])</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_check_interaction_cst(interaction_cst</span><span class="s0">, </span><span class="s1">n_features</span><span class="s0">, </span><span class="s1">result):</span>
    <span class="s3">&quot;&quot;&quot;Check that _check_interaction_cst returns the expected list of sets&quot;&quot;&quot;</span>
    <span class="s1">est = HistGradientBoostingRegressor()</span>
    <span class="s1">est.set_params(interaction_cst=interaction_cst)</span>
    <span class="s0">assert </span><span class="s1">est._check_interaction_cst(n_features) == result</span>


<span class="s0">def </span><span class="s1">test_interaction_cst_numerically():</span>
    <span class="s3">&quot;&quot;&quot;Check that interaction constraints have no forbidden interactions.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s2">1000</span>
    <span class="s1">X = rng.uniform(size=(n_samples</span><span class="s0">, </span><span class="s2">2</span><span class="s1">))</span>
    <span class="s5"># Construct y with a strong interaction term</span>
    <span class="s5"># y = x0 + x1 + 5 * x0 * x1</span>
    <span class="s1">y = np.hstack((X</span><span class="s0">, </span><span class="s2">5 </span><span class="s1">* X[:</span><span class="s0">, </span><span class="s1">[</span><span class="s2">0</span><span class="s1">]] * X[:</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s1">]])).sum(axis=</span><span class="s2">1</span><span class="s1">)</span>

    <span class="s1">est = HistGradientBoostingRegressor(random_state=</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">est_no_interactions = HistGradientBoostingRegressor(</span>
        <span class="s1">interaction_cst=[{</span><span class="s2">0</span><span class="s1">}</span><span class="s0">, </span><span class="s1">{</span><span class="s2">1</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">42</span>
    <span class="s1">)</span>
    <span class="s1">est_no_interactions.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">delta = </span><span class="s2">0.25</span>
    <span class="s5"># Make sure we do not extrapolate out of the training set as tree-based estimators</span>
    <span class="s5"># are very bad in doing so.</span>
    <span class="s1">X_test = X[(X[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] &lt; </span><span class="s2">1 </span><span class="s1">- delta) &amp; (X[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] &lt; </span><span class="s2">1 </span><span class="s1">- delta)]</span>
    <span class="s1">X_delta_d_0 = X_test + [delta</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span>
    <span class="s1">X_delta_0_d = X_test + [</span><span class="s2">0</span><span class="s0">, </span><span class="s1">delta]</span>
    <span class="s1">X_delta_d_d = X_test + [delta</span><span class="s0">, </span><span class="s1">delta]</span>

    <span class="s5"># Note: For the y from above as a function of x0 and x1, we have</span>
    <span class="s5"># y(x0+d, x1+d) = y(x0, x1) + 5 * d * (2/5 + x0 + x1) + 5 * d**2</span>
    <span class="s5"># y(x0+d, x1)   = y(x0, x1) + 5 * d * (1/5 + x1)</span>
    <span class="s5"># y(x0,   x1+d) = y(x0, x1) + 5 * d * (1/5 + x0)</span>
    <span class="s5"># Without interaction constraints, we would expect a result of 5 * d**2 for the</span>
    <span class="s5"># following expression, but zero with constraints in place.</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">est_no_interactions.predict(X_delta_d_d)</span>
        <span class="s1">+ est_no_interactions.predict(X_test)</span>
        <span class="s1">- est_no_interactions.predict(X_delta_d_0)</span>
        <span class="s1">- est_no_interactions.predict(X_delta_0_d)</span><span class="s0">,</span>
        <span class="s2">0</span><span class="s0">,</span>
        <span class="s1">atol=</span><span class="s2">1e-12</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s5"># Correct result of the expressions is 5 * delta**2. But this is hard to achieve by</span>
    <span class="s5"># a fitted tree-based model. However, with 100 iterations the expression should</span>
    <span class="s5"># at least be positive!</span>
    <span class="s0">assert </span><span class="s1">np.all(</span>
        <span class="s1">est.predict(X_delta_d_d)</span>
        <span class="s1">+ est.predict(X_test)</span>
        <span class="s1">- est.predict(X_delta_d_0)</span>
        <span class="s1">- est.predict(X_delta_0_d)</span>
        <span class="s1">&gt; </span><span class="s2">0.01</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_no_user_warning_with_scoring():</span>
    <span class="s3">&quot;&quot;&quot;Check that no UserWarning is raised when scoring is set. 
 
    Non-regression test for #22907. 
    &quot;&quot;&quot;</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s4">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_samples=</span><span class="s2">50</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">X_df = pd.DataFrame(X</span><span class="s0">, </span><span class="s1">columns=[</span><span class="s4">f&quot;col</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s4">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(X.shape[</span><span class="s2">1</span><span class="s1">])])</span>

    <span class="s1">est = HistGradientBoostingRegressor(</span>
        <span class="s1">random_state=</span><span class="s2">0</span><span class="s0">, </span><span class="s1">scoring=</span><span class="s4">&quot;neg_mean_absolute_error&quot;</span><span class="s0">, </span><span class="s1">early_stopping=</span><span class="s0">True</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s4">&quot;error&quot;</span><span class="s0">, </span><span class="s1">UserWarning)</span>
        <span class="s1">est.fit(X_df</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_class_weights():</span>
    <span class="s3">&quot;&quot;&quot;High level test to check class_weights.&quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s2">255</span>
    <span class="s1">n_features = </span><span class="s2">2</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
        <span class="s1">n_features=n_features</span><span class="s0">,</span>
        <span class="s1">n_informative=n_features</span><span class="s0">,</span>
        <span class="s1">n_redundant=</span><span class="s2">0</span><span class="s0">,</span>
        <span class="s1">n_clusters_per_class=</span><span class="s2">1</span><span class="s0">,</span>
        <span class="s1">n_classes=</span><span class="s2">2</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s2">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">y_is_1 = y == </span><span class="s2">1</span>

    <span class="s5"># class_weight is the same as sample weights with the corresponding class</span>
    <span class="s1">clf = HistGradientBoostingClassifier(</span>
        <span class="s1">min_samples_leaf=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">0</span><span class="s0">, </span><span class="s1">max_depth=</span><span class="s2">2</span>
    <span class="s1">)</span>
    <span class="s1">sample_weight = np.ones(shape=(n_samples))</span>
    <span class="s1">sample_weight[y_is_1] = </span><span class="s2">3.0</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">class_weight = {</span><span class="s2">0</span><span class="s1">: </span><span class="s2">1.0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">: </span><span class="s2">3.0</span><span class="s1">}</span>
    <span class="s1">clf_class_weighted = clone(clf).set_params(class_weight=class_weight)</span>
    <span class="s1">clf_class_weighted.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">assert_allclose(clf.decision_function(X)</span><span class="s0">, </span><span class="s1">clf_class_weighted.decision_function(X))</span>

    <span class="s5"># Check that sample_weight and class_weight are multiplicative</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight**</span><span class="s2">2</span><span class="s1">)</span>
    <span class="s1">clf_class_weighted.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_allclose(clf.decision_function(X)</span><span class="s0">, </span><span class="s1">clf_class_weighted.decision_function(X))</span>

    <span class="s5"># Make imbalanced dataset</span>
    <span class="s1">X_imb = np.concatenate((X[~y_is_1]</span><span class="s0">, </span><span class="s1">X[y_is_1][:</span><span class="s2">10</span><span class="s1">]))</span>
    <span class="s1">y_imb = np.concatenate((y[~y_is_1]</span><span class="s0">, </span><span class="s1">y[y_is_1][:</span><span class="s2">10</span><span class="s1">]))</span>

    <span class="s5"># class_weight=&quot;balanced&quot; is the same as sample_weights to be</span>
    <span class="s5"># inversely proportional to n_samples / (n_classes * np.bincount(y))</span>
    <span class="s1">clf_balanced = clone(clf).set_params(class_weight=</span><span class="s4">&quot;balanced&quot;</span><span class="s1">)</span>
    <span class="s1">clf_balanced.fit(X_imb</span><span class="s0">, </span><span class="s1">y_imb)</span>

    <span class="s1">class_weight = y_imb.shape[</span><span class="s2">0</span><span class="s1">] / (</span><span class="s2">2 </span><span class="s1">* np.bincount(y_imb))</span>
    <span class="s1">sample_weight = class_weight[y_imb]</span>
    <span class="s1">clf_sample_weight = clone(clf).set_params(class_weight=</span><span class="s0">None</span><span class="s1">)</span>
    <span class="s1">clf_sample_weight.fit(X_imb</span><span class="s0">, </span><span class="s1">y_imb</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">assert_allclose(</span>
        <span class="s1">clf_balanced.decision_function(X_imb)</span><span class="s0">,</span>
        <span class="s1">clf_sample_weight.decision_function(X_imb)</span><span class="s0">,</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_unknown_category_that_are_negative():</span>
    <span class="s3">&quot;&quot;&quot;Check that unknown categories that are negative does not error. 
 
    Non-regression test for #24274. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s2">1000</span>
    <span class="s1">X = np.c_[rng.rand(n_samples)</span><span class="s0">, </span><span class="s1">rng.randint(</span><span class="s2">4</span><span class="s0">, </span><span class="s1">size=n_samples)]</span>
    <span class="s1">y = np.zeros(shape=n_samples)</span>
    <span class="s1">y[X[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] % </span><span class="s2">2 </span><span class="s1">== </span><span class="s2">0</span><span class="s1">] = </span><span class="s2">1</span>

    <span class="s1">hist = HistGradientBoostingRegressor(</span>
        <span class="s1">random_state=</span><span class="s2">0</span><span class="s0">,</span>
        <span class="s1">categorical_features=[</span><span class="s0">False, True</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s2">10</span><span class="s0">,</span>
    <span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s5"># Check that negative values from the second column are treated like a</span>
    <span class="s5"># missing category</span>
    <span class="s1">X_test_neg = np.asarray([[</span><span class="s2">1</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">3</span><span class="s0">, </span><span class="s1">-</span><span class="s2">4</span><span class="s1">]])</span>
    <span class="s1">X_test_nan = np.asarray([[</span><span class="s2">1</span><span class="s0">, </span><span class="s1">np.nan]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">3</span><span class="s0">, </span><span class="s1">np.nan]])</span>

    <span class="s1">assert_allclose(hist.predict(X_test_neg)</span><span class="s0">, </span><span class="s1">hist.predict(X_test_nan))</span>
</pre>
</body>
</html>