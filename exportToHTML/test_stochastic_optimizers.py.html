<html>
<head>
<title>test_stochastic_optimizers.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6897bb;}
.s3 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_stochastic_optimizers.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>

<span class="s0">from </span><span class="s1">sklearn.neural_network._stochastic_optimizers </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">AdamOptimizer</span><span class="s0">,</span>
    <span class="s1">BaseOptimizer</span><span class="s0">,</span>
    <span class="s1">SGDOptimizer</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">assert_array_equal</span>

<span class="s1">shapes = [(</span><span class="s2">4</span><span class="s0">, </span><span class="s2">6</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s2">6</span><span class="s0">, </span><span class="s2">8</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s2">7</span><span class="s0">, </span><span class="s2">8</span><span class="s0">, </span><span class="s2">9</span><span class="s1">)]</span>


<span class="s0">def </span><span class="s1">test_base_optimizer():</span>
    <span class="s0">for </span><span class="s1">lr </span><span class="s0">in </span><span class="s1">[</span><span class="s2">10</span><span class="s1">**i </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(-</span><span class="s2">3</span><span class="s0">, </span><span class="s2">4</span><span class="s1">)]:</span>
        <span class="s1">optimizer = BaseOptimizer(lr)</span>
        <span class="s0">assert </span><span class="s1">optimizer.trigger_stopping(</span><span class="s3">&quot;&quot;</span><span class="s0">, False</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_sgd_optimizer_no_momentum():</span>
    <span class="s1">params = [np.zeros(shape) </span><span class="s0">for </span><span class="s1">shape </span><span class="s0">in </span><span class="s1">shapes]</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">lr </span><span class="s0">in </span><span class="s1">[</span><span class="s2">10</span><span class="s1">**i </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(-</span><span class="s2">3</span><span class="s0">, </span><span class="s2">4</span><span class="s1">)]:</span>
        <span class="s1">optimizer = SGDOptimizer(params</span><span class="s0">, </span><span class="s1">lr</span><span class="s0">, </span><span class="s1">momentum=</span><span class="s2">0</span><span class="s0">, </span><span class="s1">nesterov=</span><span class="s0">False</span><span class="s1">)</span>
        <span class="s1">grads = [rng.random_sample(shape) </span><span class="s0">for </span><span class="s1">shape </span><span class="s0">in </span><span class="s1">shapes]</span>
        <span class="s1">expected = [param - lr * grad </span><span class="s0">for </span><span class="s1">param</span><span class="s0">, </span><span class="s1">grad </span><span class="s0">in </span><span class="s1">zip(params</span><span class="s0">, </span><span class="s1">grads)]</span>
        <span class="s1">optimizer.update_params(params</span><span class="s0">, </span><span class="s1">grads)</span>

        <span class="s0">for </span><span class="s1">exp</span><span class="s0">, </span><span class="s1">param </span><span class="s0">in </span><span class="s1">zip(expected</span><span class="s0">, </span><span class="s1">params):</span>
            <span class="s1">assert_array_equal(exp</span><span class="s0">, </span><span class="s1">param)</span>


<span class="s0">def </span><span class="s1">test_sgd_optimizer_momentum():</span>
    <span class="s1">params = [np.zeros(shape) </span><span class="s0">for </span><span class="s1">shape </span><span class="s0">in </span><span class="s1">shapes]</span>
    <span class="s1">lr = </span><span class="s2">0.1</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">momentum </span><span class="s0">in </span><span class="s1">np.arange(</span><span class="s2">0.5</span><span class="s0">, </span><span class="s2">0.9</span><span class="s0">, </span><span class="s2">0.1</span><span class="s1">):</span>
        <span class="s1">optimizer = SGDOptimizer(params</span><span class="s0">, </span><span class="s1">lr</span><span class="s0">, </span><span class="s1">momentum=momentum</span><span class="s0">, </span><span class="s1">nesterov=</span><span class="s0">False</span><span class="s1">)</span>
        <span class="s1">velocities = [rng.random_sample(shape) </span><span class="s0">for </span><span class="s1">shape </span><span class="s0">in </span><span class="s1">shapes]</span>
        <span class="s1">optimizer.velocities = velocities</span>
        <span class="s1">grads = [rng.random_sample(shape) </span><span class="s0">for </span><span class="s1">shape </span><span class="s0">in </span><span class="s1">shapes]</span>
        <span class="s1">updates = [</span>
            <span class="s1">momentum * velocity - lr * grad </span><span class="s0">for </span><span class="s1">velocity</span><span class="s0">, </span><span class="s1">grad </span><span class="s0">in </span><span class="s1">zip(velocities</span><span class="s0">, </span><span class="s1">grads)</span>
        <span class="s1">]</span>
        <span class="s1">expected = [param + update </span><span class="s0">for </span><span class="s1">param</span><span class="s0">, </span><span class="s1">update </span><span class="s0">in </span><span class="s1">zip(params</span><span class="s0">, </span><span class="s1">updates)]</span>
        <span class="s1">optimizer.update_params(params</span><span class="s0">, </span><span class="s1">grads)</span>

        <span class="s0">for </span><span class="s1">exp</span><span class="s0">, </span><span class="s1">param </span><span class="s0">in </span><span class="s1">zip(expected</span><span class="s0">, </span><span class="s1">params):</span>
            <span class="s1">assert_array_equal(exp</span><span class="s0">, </span><span class="s1">param)</span>


<span class="s0">def </span><span class="s1">test_sgd_optimizer_trigger_stopping():</span>
    <span class="s1">params = [np.zeros(shape) </span><span class="s0">for </span><span class="s1">shape </span><span class="s0">in </span><span class="s1">shapes]</span>
    <span class="s1">lr = </span><span class="s2">2e-6</span>
    <span class="s1">optimizer = SGDOptimizer(params</span><span class="s0">, </span><span class="s1">lr</span><span class="s0">, </span><span class="s1">lr_schedule=</span><span class="s3">&quot;adaptive&quot;</span><span class="s1">)</span>
    <span class="s0">assert not </span><span class="s1">optimizer.trigger_stopping(</span><span class="s3">&quot;&quot;</span><span class="s0">, False</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">lr / </span><span class="s2">5 </span><span class="s1">== optimizer.learning_rate</span>
    <span class="s0">assert </span><span class="s1">optimizer.trigger_stopping(</span><span class="s3">&quot;&quot;</span><span class="s0">, False</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_sgd_optimizer_nesterovs_momentum():</span>
    <span class="s1">params = [np.zeros(shape) </span><span class="s0">for </span><span class="s1">shape </span><span class="s0">in </span><span class="s1">shapes]</span>
    <span class="s1">lr = </span><span class="s2">0.1</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">momentum </span><span class="s0">in </span><span class="s1">np.arange(</span><span class="s2">0.5</span><span class="s0">, </span><span class="s2">0.9</span><span class="s0">, </span><span class="s2">0.1</span><span class="s1">):</span>
        <span class="s1">optimizer = SGDOptimizer(params</span><span class="s0">, </span><span class="s1">lr</span><span class="s0">, </span><span class="s1">momentum=momentum</span><span class="s0">, </span><span class="s1">nesterov=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s1">velocities = [rng.random_sample(shape) </span><span class="s0">for </span><span class="s1">shape </span><span class="s0">in </span><span class="s1">shapes]</span>
        <span class="s1">optimizer.velocities = velocities</span>
        <span class="s1">grads = [rng.random_sample(shape) </span><span class="s0">for </span><span class="s1">shape </span><span class="s0">in </span><span class="s1">shapes]</span>
        <span class="s1">updates = [</span>
            <span class="s1">momentum * velocity - lr * grad </span><span class="s0">for </span><span class="s1">velocity</span><span class="s0">, </span><span class="s1">grad </span><span class="s0">in </span><span class="s1">zip(velocities</span><span class="s0">, </span><span class="s1">grads)</span>
        <span class="s1">]</span>
        <span class="s1">updates = [</span>
            <span class="s1">momentum * update - lr * grad </span><span class="s0">for </span><span class="s1">update</span><span class="s0">, </span><span class="s1">grad </span><span class="s0">in </span><span class="s1">zip(updates</span><span class="s0">, </span><span class="s1">grads)</span>
        <span class="s1">]</span>
        <span class="s1">expected = [param + update </span><span class="s0">for </span><span class="s1">param</span><span class="s0">, </span><span class="s1">update </span><span class="s0">in </span><span class="s1">zip(params</span><span class="s0">, </span><span class="s1">updates)]</span>
        <span class="s1">optimizer.update_params(params</span><span class="s0">, </span><span class="s1">grads)</span>

        <span class="s0">for </span><span class="s1">exp</span><span class="s0">, </span><span class="s1">param </span><span class="s0">in </span><span class="s1">zip(expected</span><span class="s0">, </span><span class="s1">params):</span>
            <span class="s1">assert_array_equal(exp</span><span class="s0">, </span><span class="s1">param)</span>


<span class="s0">def </span><span class="s1">test_adam_optimizer():</span>
    <span class="s1">params = [np.zeros(shape) </span><span class="s0">for </span><span class="s1">shape </span><span class="s0">in </span><span class="s1">shapes]</span>
    <span class="s1">lr = </span><span class="s2">0.001</span>
    <span class="s1">epsilon = </span><span class="s2">1e-8</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">beta_1 </span><span class="s0">in </span><span class="s1">np.arange(</span><span class="s2">0.9</span><span class="s0">, </span><span class="s2">1.0</span><span class="s0">, </span><span class="s2">0.05</span><span class="s1">):</span>
        <span class="s0">for </span><span class="s1">beta_2 </span><span class="s0">in </span><span class="s1">np.arange(</span><span class="s2">0.995</span><span class="s0">, </span><span class="s2">1.0</span><span class="s0">, </span><span class="s2">0.001</span><span class="s1">):</span>
            <span class="s1">optimizer = AdamOptimizer(params</span><span class="s0">, </span><span class="s1">lr</span><span class="s0">, </span><span class="s1">beta_1</span><span class="s0">, </span><span class="s1">beta_2</span><span class="s0">, </span><span class="s1">epsilon)</span>
            <span class="s1">ms = [rng.random_sample(shape) </span><span class="s0">for </span><span class="s1">shape </span><span class="s0">in </span><span class="s1">shapes]</span>
            <span class="s1">vs = [rng.random_sample(shape) </span><span class="s0">for </span><span class="s1">shape </span><span class="s0">in </span><span class="s1">shapes]</span>
            <span class="s1">t = </span><span class="s2">10</span>
            <span class="s1">optimizer.ms = ms</span>
            <span class="s1">optimizer.vs = vs</span>
            <span class="s1">optimizer.t = t - </span><span class="s2">1</span>
            <span class="s1">grads = [rng.random_sample(shape) </span><span class="s0">for </span><span class="s1">shape </span><span class="s0">in </span><span class="s1">shapes]</span>

            <span class="s1">ms = [beta_1 * m + (</span><span class="s2">1 </span><span class="s1">- beta_1) * grad </span><span class="s0">for </span><span class="s1">m</span><span class="s0">, </span><span class="s1">grad </span><span class="s0">in </span><span class="s1">zip(ms</span><span class="s0">, </span><span class="s1">grads)]</span>
            <span class="s1">vs = [beta_2 * v + (</span><span class="s2">1 </span><span class="s1">- beta_2) * (grad**</span><span class="s2">2</span><span class="s1">) </span><span class="s0">for </span><span class="s1">v</span><span class="s0">, </span><span class="s1">grad </span><span class="s0">in </span><span class="s1">zip(vs</span><span class="s0">, </span><span class="s1">grads)]</span>
            <span class="s1">learning_rate = lr * np.sqrt(</span><span class="s2">1 </span><span class="s1">- beta_2**t) / (</span><span class="s2">1 </span><span class="s1">- beta_1**t)</span>
            <span class="s1">updates = [</span>
                <span class="s1">-learning_rate * m / (np.sqrt(v) + epsilon) </span><span class="s0">for </span><span class="s1">m</span><span class="s0">, </span><span class="s1">v </span><span class="s0">in </span><span class="s1">zip(ms</span><span class="s0">, </span><span class="s1">vs)</span>
            <span class="s1">]</span>
            <span class="s1">expected = [param + update </span><span class="s0">for </span><span class="s1">param</span><span class="s0">, </span><span class="s1">update </span><span class="s0">in </span><span class="s1">zip(params</span><span class="s0">, </span><span class="s1">updates)]</span>

            <span class="s1">optimizer.update_params(params</span><span class="s0">, </span><span class="s1">grads)</span>
            <span class="s0">for </span><span class="s1">exp</span><span class="s0">, </span><span class="s1">param </span><span class="s0">in </span><span class="s1">zip(expected</span><span class="s0">, </span><span class="s1">params):</span>
                <span class="s1">assert_array_equal(exp</span><span class="s0">, </span><span class="s1">param)</span>
</pre>
</body>
</html>