<html>
<head>
<title>_from_model.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_from_model.py</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: Gilles Louppe, Mathieu Blondel, Maheshakya Wijewardena</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">from </span><span class="s1">copy </span><span class="s2">import </span><span class="s1">deepcopy</span>
<span class="s2">from </span><span class="s1">numbers </span><span class="s2">import </span><span class="s1">Integral</span><span class="s2">, </span><span class="s1">Real</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>

<span class="s2">from </span><span class="s1">..base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">MetaEstimatorMixin</span><span class="s2">, </span><span class="s1">_fit_context</span><span class="s2">, </span><span class="s1">clone</span>
<span class="s2">from </span><span class="s1">..exceptions </span><span class="s2">import </span><span class="s1">NotFittedError</span>
<span class="s2">from </span><span class="s1">..utils._param_validation </span><span class="s2">import </span><span class="s1">HasMethods</span><span class="s2">, </span><span class="s1">Interval</span><span class="s2">, </span><span class="s1">Options</span>
<span class="s2">from </span><span class="s1">..utils._tags </span><span class="s2">import </span><span class="s1">_safe_tags</span>
<span class="s2">from </span><span class="s1">..utils.metaestimators </span><span class="s2">import </span><span class="s1">available_if</span>
<span class="s2">from </span><span class="s1">..utils.validation </span><span class="s2">import </span><span class="s1">_num_features</span><span class="s2">, </span><span class="s1">check_is_fitted</span><span class="s2">, </span><span class="s1">check_scalar</span>
<span class="s2">from </span><span class="s1">._base </span><span class="s2">import </span><span class="s1">SelectorMixin</span><span class="s2">, </span><span class="s1">_get_feature_importances</span>


<span class="s2">def </span><span class="s1">_calculate_threshold(estimator</span><span class="s2">, </span><span class="s1">importances</span><span class="s2">, </span><span class="s1">threshold):</span>
    <span class="s3">&quot;&quot;&quot;Interpret the threshold value&quot;&quot;&quot;</span>

    <span class="s2">if </span><span class="s1">threshold </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s0"># determine default from estimator</span>
        <span class="s1">est_name = estimator.__class__.__name__</span>
        <span class="s1">is_l1_penalized = hasattr(estimator</span><span class="s2">, </span><span class="s4">&quot;penalty&quot;</span><span class="s1">) </span><span class="s2">and </span><span class="s1">estimator.penalty == </span><span class="s4">&quot;l1&quot;</span>
        <span class="s1">is_lasso = </span><span class="s4">&quot;Lasso&quot; </span><span class="s2">in </span><span class="s1">est_name</span>
        <span class="s1">is_elasticnet_l1_penalized = </span><span class="s4">&quot;ElasticNet&quot; </span><span class="s2">in </span><span class="s1">est_name </span><span class="s2">and </span><span class="s1">(</span>
            <span class="s1">(hasattr(estimator</span><span class="s2">, </span><span class="s4">&quot;l1_ratio_&quot;</span><span class="s1">) </span><span class="s2">and </span><span class="s1">np.isclose(estimator.l1_ratio_</span><span class="s2">, </span><span class="s5">1.0</span><span class="s1">))</span>
            <span class="s2">or </span><span class="s1">(hasattr(estimator</span><span class="s2">, </span><span class="s4">&quot;l1_ratio&quot;</span><span class="s1">) </span><span class="s2">and </span><span class="s1">np.isclose(estimator.l1_ratio</span><span class="s2">, </span><span class="s5">1.0</span><span class="s1">))</span>
        <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">is_l1_penalized </span><span class="s2">or </span><span class="s1">is_lasso </span><span class="s2">or </span><span class="s1">is_elasticnet_l1_penalized:</span>
            <span class="s0"># the natural default threshold is 0 when l1 penalty was used</span>
            <span class="s1">threshold = </span><span class="s5">1e-5</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">threshold = </span><span class="s4">&quot;mean&quot;</span>

    <span class="s2">if </span><span class="s1">isinstance(threshold</span><span class="s2">, </span><span class="s1">str):</span>
        <span class="s2">if </span><span class="s4">&quot;*&quot; </span><span class="s2">in </span><span class="s1">threshold:</span>
            <span class="s1">scale</span><span class="s2">, </span><span class="s1">reference = threshold.split(</span><span class="s4">&quot;*&quot;</span><span class="s1">)</span>
            <span class="s1">scale = float(scale.strip())</span>
            <span class="s1">reference = reference.strip()</span>

            <span class="s2">if </span><span class="s1">reference == </span><span class="s4">&quot;median&quot;</span><span class="s1">:</span>
                <span class="s1">reference = np.median(importances)</span>
            <span class="s2">elif </span><span class="s1">reference == </span><span class="s4">&quot;mean&quot;</span><span class="s1">:</span>
                <span class="s1">reference = np.mean(importances)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Unknown reference: &quot; </span><span class="s1">+ reference)</span>

            <span class="s1">threshold = scale * reference</span>

        <span class="s2">elif </span><span class="s1">threshold == </span><span class="s4">&quot;median&quot;</span><span class="s1">:</span>
            <span class="s1">threshold = np.median(importances)</span>

        <span class="s2">elif </span><span class="s1">threshold == </span><span class="s4">&quot;mean&quot;</span><span class="s1">:</span>
            <span class="s1">threshold = np.mean(importances)</span>

        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;Expected threshold='mean' or threshold='median' got %s&quot; </span><span class="s1">% threshold</span>
            <span class="s1">)</span>

    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">threshold = float(threshold)</span>

    <span class="s2">return </span><span class="s1">threshold</span>


<span class="s2">def </span><span class="s1">_estimator_has(attr):</span>
    <span class="s3">&quot;&quot;&quot;Check if we can delegate a method to the underlying estimator. 
 
    First, we check the fitted estimator if available, otherwise we 
    check the unfitted estimator. 
    &quot;&quot;&quot;</span>
    <span class="s2">return lambda </span><span class="s1">self: (</span>
        <span class="s1">hasattr(self.estimator_</span><span class="s2">, </span><span class="s1">attr)</span>
        <span class="s2">if </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s4">&quot;estimator_&quot;</span><span class="s1">)</span>
        <span class="s2">else </span><span class="s1">hasattr(self.estimator</span><span class="s2">, </span><span class="s1">attr)</span>
    <span class="s1">)</span>


<span class="s2">class </span><span class="s1">SelectFromModel(MetaEstimatorMixin</span><span class="s2">, </span><span class="s1">SelectorMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
    <span class="s3">&quot;&quot;&quot;Meta-transformer for selecting features based on importance weights. 
 
    .. versionadded:: 0.17 
 
    Read more in the :ref:`User Guide &lt;select_from_model&gt;`. 
 
    Parameters 
    ---------- 
    estimator : object 
        The base estimator from which the transformer is built. 
        This can be both a fitted (if ``prefit`` is set to True) 
        or a non-fitted estimator. The estimator should have a 
        ``feature_importances_`` or ``coef_`` attribute after fitting. 
        Otherwise, the ``importance_getter`` parameter should be used. 
 
    threshold : str or float, default=None 
        The threshold value to use for feature selection. Features whose 
        absolute importance value is greater or equal are kept while the others 
        are discarded. If &quot;median&quot; (resp. &quot;mean&quot;), then the ``threshold`` value 
        is the median (resp. the mean) of the feature importances. A scaling 
        factor (e.g., &quot;1.25*mean&quot;) may also be used. If None and if the 
        estimator has a parameter penalty set to l1, either explicitly 
        or implicitly (e.g, Lasso), the threshold used is 1e-5. 
        Otherwise, &quot;mean&quot; is used by default. 
 
    prefit : bool, default=False 
        Whether a prefit model is expected to be passed into the constructor 
        directly or not. 
        If `True`, `estimator` must be a fitted estimator. 
        If `False`, `estimator` is fitted and updated by calling 
        `fit` and `partial_fit`, respectively. 
 
    norm_order : non-zero int, inf, -inf, default=1 
        Order of the norm used to filter the vectors of coefficients below 
        ``threshold`` in the case where the ``coef_`` attribute of the 
        estimator is of dimension 2. 
 
    max_features : int, callable, default=None 
        The maximum number of features to select. 
 
        - If an integer, then it specifies the maximum number of features to 
          allow. 
        - If a callable, then it specifies how to calculate the maximum number of 
          features allowed by using the output of `max_features(X)`. 
        - If `None`, then all features are kept. 
 
        To only select based on ``max_features``, set ``threshold=-np.inf``. 
 
        .. versionadded:: 0.20 
        .. versionchanged:: 1.1 
           `max_features` accepts a callable. 
 
    importance_getter : str or callable, default='auto' 
        If 'auto', uses the feature importance either through a ``coef_`` 
        attribute or ``feature_importances_`` attribute of estimator. 
 
        Also accepts a string that specifies an attribute name/path 
        for extracting feature importance (implemented with `attrgetter`). 
        For example, give `regressor_.coef_` in case of 
        :class:`~sklearn.compose.TransformedTargetRegressor`  or 
        `named_steps.clf.feature_importances_` in case of 
        :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`. 
 
        If `callable`, overrides the default feature importance getter. 
        The callable is passed with the fitted estimator and it should 
        return importance for each feature. 
 
        .. versionadded:: 0.24 
 
    Attributes 
    ---------- 
    estimator_ : estimator 
        The base estimator from which the transformer is built. This attribute 
        exist only when `fit` has been called. 
 
        - If `prefit=True`, it is a deep copy of `estimator`. 
        - If `prefit=False`, it is a clone of `estimator` and fit on the data 
          passed to `fit` or `partial_fit`. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. Only defined if the 
        underlying estimator exposes such an attribute when fit. 
 
        .. versionadded:: 0.24 
 
    max_features_ : int 
        Maximum number of features calculated during :term:`fit`. Only defined 
        if the ``max_features`` is not `None`. 
 
        - If `max_features` is an `int`, then `max_features_ = max_features`. 
        - If `max_features` is a callable, then `max_features_ = max_features(X)`. 
 
        .. versionadded:: 1.1 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    threshold_ : float 
        The threshold value used for feature selection. 
 
    See Also 
    -------- 
    RFE : Recursive feature elimination based on importance weights. 
    RFECV : Recursive feature elimination with built-in cross-validated 
        selection of the best number of features. 
    SequentialFeatureSelector : Sequential cross-validation based feature 
        selection. Does not rely on importance weights. 
 
    Notes 
    ----- 
    Allows NaN/Inf in the input if the underlying estimator does as well. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.feature_selection import SelectFromModel 
    &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression 
    &gt;&gt;&gt; X = [[ 0.87, -1.34,  0.31 ], 
    ...      [-2.79, -0.02, -0.85 ], 
    ...      [-1.34, -0.48, -2.55 ], 
    ...      [ 1.92,  1.48,  0.65 ]] 
    &gt;&gt;&gt; y = [0, 1, 0, 1] 
    &gt;&gt;&gt; selector = SelectFromModel(estimator=LogisticRegression()).fit(X, y) 
    &gt;&gt;&gt; selector.estimator_.coef_ 
    array([[-0.3252302 ,  0.83462377,  0.49750423]]) 
    &gt;&gt;&gt; selector.threshold_ 
    0.55245... 
    &gt;&gt;&gt; selector.get_support() 
    array([False,  True, False]) 
    &gt;&gt;&gt; selector.transform(X) 
    array([[-1.34], 
           [-0.02], 
           [-0.48], 
           [ 1.48]]) 
 
    Using a callable to create a selector that can use no more than half 
    of the input features. 
 
    &gt;&gt;&gt; def half_callable(X): 
    ...     return round(len(X[0]) / 2) 
    &gt;&gt;&gt; half_selector = SelectFromModel(estimator=LogisticRegression(), 
    ...                                 max_features=half_callable) 
    &gt;&gt;&gt; _ = half_selector.fit(X, y) 
    &gt;&gt;&gt; half_selector.max_features_ 
    2 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;estimator&quot;</span><span class="s1">: [HasMethods(</span><span class="s4">&quot;fit&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s4">&quot;threshold&quot;</span><span class="s1">: [Interval(Real</span><span class="s2">, None, None, </span><span class="s1">closed=</span><span class="s4">&quot;both&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">str</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s4">&quot;prefit&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s4">&quot;norm_order&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Integral</span><span class="s2">, None, </span><span class="s1">-</span><span class="s5">1</span><span class="s2">, </span><span class="s1">closed=</span><span class="s4">&quot;right&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">Interval(Integral</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">Options(Real</span><span class="s2">, </span><span class="s1">{np.inf</span><span class="s2">, </span><span class="s1">-np.inf})</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s4">&quot;max_features&quot;</span><span class="s1">: [Interval(Integral</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">callable</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s4">&quot;importance_getter&quot;</span><span class="s1">: [str</span><span class="s2">, </span><span class="s1">callable]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">estimator</span><span class="s2">,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">threshold=</span><span class="s2">None,</span>
        <span class="s1">prefit=</span><span class="s2">False,</span>
        <span class="s1">norm_order=</span><span class="s5">1</span><span class="s2">,</span>
        <span class="s1">max_features=</span><span class="s2">None,</span>
        <span class="s1">importance_getter=</span><span class="s4">&quot;auto&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">self.estimator = estimator</span>
        <span class="s1">self.threshold = threshold</span>
        <span class="s1">self.prefit = prefit</span>
        <span class="s1">self.importance_getter = importance_getter</span>
        <span class="s1">self.norm_order = norm_order</span>
        <span class="s1">self.max_features = max_features</span>

    <span class="s2">def </span><span class="s1">_get_support_mask(self):</span>
        <span class="s1">estimator = getattr(self</span><span class="s2">, </span><span class="s4">&quot;estimator_&quot;</span><span class="s2">, </span><span class="s1">self.estimator)</span>
        <span class="s1">max_features = getattr(self</span><span class="s2">, </span><span class="s4">&quot;max_features_&quot;</span><span class="s2">, </span><span class="s1">self.max_features)</span>

        <span class="s2">if </span><span class="s1">self.prefit:</span>
            <span class="s2">try</span><span class="s1">:</span>
                <span class="s1">check_is_fitted(self.estimator)</span>
            <span class="s2">except </span><span class="s1">NotFittedError </span><span class="s2">as </span><span class="s1">exc:</span>
                <span class="s2">raise </span><span class="s1">NotFittedError(</span>
                    <span class="s4">&quot;When `prefit=True`, `estimator` is expected to be a fitted &quot;</span>
                    <span class="s4">&quot;estimator.&quot;</span>
                <span class="s1">) </span><span class="s2">from </span><span class="s1">exc</span>
        <span class="s2">if </span><span class="s1">callable(max_features):</span>
            <span class="s0"># This branch is executed when `transform` is called directly and thus</span>
            <span class="s0"># `max_features_` is not set and we fallback using `self.max_features`</span>
            <span class="s0"># that is not validated</span>
            <span class="s2">raise </span><span class="s1">NotFittedError(</span>
                <span class="s4">&quot;When `prefit=True` and `max_features` is a callable, call `fit` &quot;</span>
                <span class="s4">&quot;before calling `transform`.&quot;</span>
            <span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">max_features </span><span class="s2">is not None and not </span><span class="s1">isinstance(max_features</span><span class="s2">, </span><span class="s1">Integral):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s4">f&quot;`max_features` must be an integer. Got `max_features=</span><span class="s2">{</span><span class="s1">max_features</span><span class="s2">}</span><span class="s4">` &quot;</span>
                <span class="s4">&quot;instead.&quot;</span>
            <span class="s1">)</span>

        <span class="s1">scores = _get_feature_importances(</span>
            <span class="s1">estimator=estimator</span><span class="s2">,</span>
            <span class="s1">getter=self.importance_getter</span><span class="s2">,</span>
            <span class="s1">transform_func=</span><span class="s4">&quot;norm&quot;</span><span class="s2">,</span>
            <span class="s1">norm_order=self.norm_order</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">threshold = _calculate_threshold(estimator</span><span class="s2">, </span><span class="s1">scores</span><span class="s2">, </span><span class="s1">self.threshold)</span>
        <span class="s2">if </span><span class="s1">self.max_features </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">mask = np.zeros_like(scores</span><span class="s2">, </span><span class="s1">dtype=bool)</span>
            <span class="s1">candidate_indices = np.argsort(-scores</span><span class="s2">, </span><span class="s1">kind=</span><span class="s4">&quot;mergesort&quot;</span><span class="s1">)[:max_features]</span>
            <span class="s1">mask[candidate_indices] = </span><span class="s2">True</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">mask = np.ones_like(scores</span><span class="s2">, </span><span class="s1">dtype=bool)</span>
        <span class="s1">mask[scores &lt; threshold] = </span><span class="s2">False</span>
        <span class="s2">return </span><span class="s1">mask</span>

    <span class="s2">def </span><span class="s1">_check_max_features(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s2">if </span><span class="s1">self.max_features </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">n_features = _num_features(X)</span>

            <span class="s2">if </span><span class="s1">callable(self.max_features):</span>
                <span class="s1">max_features = self.max_features(X)</span>
            <span class="s2">else</span><span class="s1">:  </span><span class="s0"># int</span>
                <span class="s1">max_features = self.max_features</span>

            <span class="s1">check_scalar(</span>
                <span class="s1">max_features</span><span class="s2">,</span>
                <span class="s4">&quot;max_features&quot;</span><span class="s2">,</span>
                <span class="s1">Integral</span><span class="s2">,</span>
                <span class="s1">min_val=</span><span class="s5">0</span><span class="s2">,</span>
                <span class="s1">max_val=n_features</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">self.max_features_ = max_features</span>

    <span class="s1">@_fit_context(</span>
        <span class="s0"># SelectFromModel.estimator is not validated yet</span>
        <span class="s1">prefer_skip_nested_validation=</span><span class="s2">False</span>
    <span class="s1">)</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None, </span><span class="s1">**fit_params):</span>
        <span class="s3">&quot;&quot;&quot;Fit the SelectFromModel meta-transformer. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The training input samples. 
 
        y : array-like of shape (n_samples,), default=None 
            The target values (integers that correspond to classes in 
            classification, real numbers in regression). 
 
        **fit_params : dict 
            Other estimator specific parameters. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">self._check_max_features(X)</span>

        <span class="s2">if </span><span class="s1">self.prefit:</span>
            <span class="s2">try</span><span class="s1">:</span>
                <span class="s1">check_is_fitted(self.estimator)</span>
            <span class="s2">except </span><span class="s1">NotFittedError </span><span class="s2">as </span><span class="s1">exc:</span>
                <span class="s2">raise </span><span class="s1">NotFittedError(</span>
                    <span class="s4">&quot;When `prefit=True`, `estimator` is expected to be a fitted &quot;</span>
                    <span class="s4">&quot;estimator.&quot;</span>
                <span class="s1">) </span><span class="s2">from </span><span class="s1">exc</span>
            <span class="s1">self.estimator_ = deepcopy(self.estimator)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self.estimator_ = clone(self.estimator)</span>
            <span class="s1">self.estimator_.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">**fit_params)</span>

        <span class="s2">if </span><span class="s1">hasattr(self.estimator_</span><span class="s2">, </span><span class="s4">&quot;feature_names_in_&quot;</span><span class="s1">):</span>
            <span class="s1">self.feature_names_in_ = self.estimator_.feature_names_in_</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self._check_feature_names(X</span><span class="s2">, </span><span class="s1">reset=</span><span class="s2">True</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">self</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">threshold_(self):</span>
        <span class="s3">&quot;&quot;&quot;Threshold value used for feature selection.&quot;&quot;&quot;</span>
        <span class="s1">scores = _get_feature_importances(</span>
            <span class="s1">estimator=self.estimator_</span><span class="s2">,</span>
            <span class="s1">getter=self.importance_getter</span><span class="s2">,</span>
            <span class="s1">transform_func=</span><span class="s4">&quot;norm&quot;</span><span class="s2">,</span>
            <span class="s1">norm_order=self.norm_order</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s2">return </span><span class="s1">_calculate_threshold(self.estimator</span><span class="s2">, </span><span class="s1">scores</span><span class="s2">, </span><span class="s1">self.threshold)</span>

    <span class="s1">@available_if(_estimator_has(</span><span class="s4">&quot;partial_fit&quot;</span><span class="s1">))</span>
    <span class="s1">@_fit_context(</span>
        <span class="s0"># SelectFromModel.estimator is not validated yet</span>
        <span class="s1">prefer_skip_nested_validation=</span><span class="s2">False</span>
    <span class="s1">)</span>
    <span class="s2">def </span><span class="s1">partial_fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None, </span><span class="s1">**fit_params):</span>
        <span class="s3">&quot;&quot;&quot;Fit the SelectFromModel meta-transformer only once. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The training input samples. 
 
        y : array-like of shape (n_samples,), default=None 
            The target values (integers that correspond to classes in 
            classification, real numbers in regression). 
 
        **fit_params : dict 
            Other estimator specific parameters. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">first_call = </span><span class="s2">not </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s4">&quot;estimator_&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">first_call:</span>
            <span class="s1">self._check_max_features(X)</span>

        <span class="s2">if </span><span class="s1">self.prefit:</span>
            <span class="s2">if </span><span class="s1">first_call:</span>
                <span class="s2">try</span><span class="s1">:</span>
                    <span class="s1">check_is_fitted(self.estimator)</span>
                <span class="s2">except </span><span class="s1">NotFittedError </span><span class="s2">as </span><span class="s1">exc:</span>
                    <span class="s2">raise </span><span class="s1">NotFittedError(</span>
                        <span class="s4">&quot;When `prefit=True`, `estimator` is expected to be a fitted &quot;</span>
                        <span class="s4">&quot;estimator.&quot;</span>
                    <span class="s1">) </span><span class="s2">from </span><span class="s1">exc</span>
                <span class="s1">self.estimator_ = deepcopy(self.estimator)</span>
            <span class="s2">return </span><span class="s1">self</span>

        <span class="s2">if </span><span class="s1">first_call:</span>
            <span class="s1">self.estimator_ = clone(self.estimator)</span>
        <span class="s1">self.estimator_.partial_fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">**fit_params)</span>

        <span class="s2">if </span><span class="s1">hasattr(self.estimator_</span><span class="s2">, </span><span class="s4">&quot;feature_names_in_&quot;</span><span class="s1">):</span>
            <span class="s1">self.feature_names_in_ = self.estimator_.feature_names_in_</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self._check_feature_names(X</span><span class="s2">, </span><span class="s1">reset=first_call)</span>

        <span class="s2">return </span><span class="s1">self</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">n_features_in_(self):</span>
        <span class="s3">&quot;&quot;&quot;Number of features seen during `fit`.&quot;&quot;&quot;</span>
        <span class="s0"># For consistency with other estimators we raise a AttributeError so</span>
        <span class="s0"># that hasattr() fails if the estimator isn't fitted.</span>
        <span class="s2">try</span><span class="s1">:</span>
            <span class="s1">check_is_fitted(self)</span>
        <span class="s2">except </span><span class="s1">NotFittedError </span><span class="s2">as </span><span class="s1">nfe:</span>
            <span class="s2">raise </span><span class="s1">AttributeError(</span>
                <span class="s4">&quot;{} object has no n_features_in_ attribute.&quot;</span><span class="s1">.format(</span>
                    <span class="s1">self.__class__.__name__</span>
                <span class="s1">)</span>
            <span class="s1">) </span><span class="s2">from </span><span class="s1">nfe</span>

        <span class="s2">return </span><span class="s1">self.estimator_.n_features_in_</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s4">&quot;allow_nan&quot;</span><span class="s1">: _safe_tags(self.estimator</span><span class="s2">, </span><span class="s1">key=</span><span class="s4">&quot;allow_nan&quot;</span><span class="s1">)}</span>
</pre>
</body>
</html>