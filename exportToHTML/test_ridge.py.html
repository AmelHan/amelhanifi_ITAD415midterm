<html>
<head>
<title>test_ridge.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #6897bb;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_ridge.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">warnings</span>
<span class="s0">from </span><span class="s1">itertools </span><span class="s0">import </span><span class="s1">product</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">import </span><span class="s1">scipy.sparse </span><span class="s0">as </span><span class="s1">sp</span>
<span class="s0">from </span><span class="s1">scipy </span><span class="s0">import </span><span class="s1">linalg</span>

<span class="s0">from </span><span class="s1">sklearn </span><span class="s0">import </span><span class="s1">datasets</span>
<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">make_classification</span><span class="s0">,</span>
    <span class="s1">make_low_rank_matrix</span><span class="s0">,</span>
    <span class="s1">make_multilabel_classification</span><span class="s0">,</span>
    <span class="s1">make_regression</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.exceptions </span><span class="s0">import </span><span class="s1">ConvergenceWarning</span>
<span class="s0">from </span><span class="s1">sklearn.linear_model </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">LinearRegression</span><span class="s0">,</span>
    <span class="s1">Ridge</span><span class="s0">,</span>
    <span class="s1">RidgeClassifier</span><span class="s0">,</span>
    <span class="s1">RidgeClassifierCV</span><span class="s0">,</span>
    <span class="s1">RidgeCV</span><span class="s0">,</span>
    <span class="s1">ridge_regression</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.linear_model._ridge </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">_check_gcv_mode</span><span class="s0">,</span>
    <span class="s1">_RidgeGCV</span><span class="s0">,</span>
    <span class="s1">_solve_cholesky</span><span class="s0">,</span>
    <span class="s1">_solve_cholesky_kernel</span><span class="s0">,</span>
    <span class="s1">_solve_lbfgs</span><span class="s0">,</span>
    <span class="s1">_solve_svd</span><span class="s0">,</span>
    <span class="s1">_X_CenterStackOp</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.metrics </span><span class="s0">import </span><span class="s1">get_scorer</span><span class="s0">, </span><span class="s1">make_scorer</span><span class="s0">, </span><span class="s1">mean_squared_error</span>
<span class="s0">from </span><span class="s1">sklearn.model_selection </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">GridSearchCV</span><span class="s0">,</span>
    <span class="s1">GroupKFold</span><span class="s0">,</span>
    <span class="s1">KFold</span><span class="s0">,</span>
    <span class="s1">LeaveOneOut</span><span class="s0">,</span>
    <span class="s1">cross_val_predict</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.preprocessing </span><span class="s0">import </span><span class="s1">minmax_scale</span>
<span class="s0">from </span><span class="s1">sklearn.utils </span><span class="s0">import </span><span class="s1">_IS_32BIT</span><span class="s0">, </span><span class="s1">check_random_state</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s0">,</span>
    <span class="s1">assert_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_equal</span><span class="s0">,</span>
    <span class="s1">ignore_warnings</span><span class="s0">,</span>
<span class="s1">)</span>

<span class="s1">SOLVERS = [</span><span class="s2">&quot;svd&quot;</span><span class="s0">, </span><span class="s2">&quot;sparse_cg&quot;</span><span class="s0">, </span><span class="s2">&quot;cholesky&quot;</span><span class="s0">, </span><span class="s2">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s1">]</span>
<span class="s1">SPARSE_SOLVERS_WITH_INTERCEPT = (</span><span class="s2">&quot;sparse_cg&quot;</span><span class="s0">, </span><span class="s2">&quot;sag&quot;</span><span class="s1">)</span>
<span class="s1">SPARSE_SOLVERS_WITHOUT_INTERCEPT = (</span><span class="s2">&quot;sparse_cg&quot;</span><span class="s0">, </span><span class="s2">&quot;cholesky&quot;</span><span class="s0">, </span><span class="s2">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s1">)</span>

<span class="s1">diabetes = datasets.load_diabetes()</span>
<span class="s1">X_diabetes</span><span class="s0">, </span><span class="s1">y_diabetes = diabetes.data</span><span class="s0">, </span><span class="s1">diabetes.target</span>
<span class="s1">ind = np.arange(X_diabetes.shape[</span><span class="s3">0</span><span class="s1">])</span>
<span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
<span class="s1">rng.shuffle(ind)</span>
<span class="s1">ind = ind[:</span><span class="s3">200</span><span class="s1">]</span>
<span class="s1">X_diabetes</span><span class="s0">, </span><span class="s1">y_diabetes = X_diabetes[ind]</span><span class="s0">, </span><span class="s1">y_diabetes[ind]</span>

<span class="s1">iris = datasets.load_iris()</span>

<span class="s1">X_iris = sp.csr_matrix(iris.data)</span>
<span class="s1">y_iris = iris.target</span>


<span class="s0">def </span><span class="s1">DENSE_FILTER(X):</span>
    <span class="s0">return </span><span class="s1">X</span>


<span class="s0">def </span><span class="s1">SPARSE_FILTER(X):</span>
    <span class="s0">return </span><span class="s1">sp.csr_matrix(X)</span>


<span class="s0">def </span><span class="s1">_accuracy_callable(y_test</span><span class="s0">, </span><span class="s1">y_pred):</span>
    <span class="s0">return </span><span class="s1">np.mean(y_test == y_pred)</span>


<span class="s0">def </span><span class="s1">_mean_squared_error_callable(y_test</span><span class="s0">, </span><span class="s1">y_pred):</span>
    <span class="s0">return </span><span class="s1">((y_test - y_pred) ** </span><span class="s3">2</span><span class="s1">).mean()</span>


<span class="s1">@pytest.fixture(params=[</span><span class="s2">&quot;long&quot;</span><span class="s0">, </span><span class="s2">&quot;wide&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">ols_ridge_dataset(global_random_seed</span><span class="s0">, </span><span class="s1">request):</span>
    <span class="s4">&quot;&quot;&quot;Dataset with OLS and Ridge solutions, well conditioned X. 
 
    The construction is based on the SVD decomposition of X = U S V'. 
 
    Parameters 
    ---------- 
    type : {&quot;long&quot;, &quot;wide&quot;} 
        If &quot;long&quot;, then n_samples &gt; n_features. 
        If &quot;wide&quot;, then n_features &gt; n_samples. 
 
    For &quot;wide&quot;, we return the minimum norm solution w = X' (XX')^-1 y: 
 
        min ||w||_2 subject to X w = y 
 
    Returns 
    ------- 
    X : ndarray 
        Last column of 1, i.e. intercept. 
    y : ndarray 
    coef_ols : ndarray of shape 
        Minimum norm OLS solutions, i.e. min ||X w - y||_2_2 (with minimum ||w||_2 in 
        case of ambiguity) 
        Last coefficient is intercept. 
    coef_ridge : ndarray of shape (5,) 
        Ridge solution with alpha=1, i.e. min ||X w - y||_2_2 + ||w||_2^2. 
        Last coefficient is intercept. 
    &quot;&quot;&quot;</span>
    <span class="s5"># Make larger dim more than double as big as the smaller one.</span>
    <span class="s5"># This helps when constructing singular matrices like (X, X).</span>
    <span class="s0">if </span><span class="s1">request.param == </span><span class="s2">&quot;long&quot;</span><span class="s1">:</span>
        <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s3">12</span><span class="s0">, </span><span class="s3">4</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s3">4</span><span class="s0">, </span><span class="s3">12</span>
    <span class="s1">k = min(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">X = make_low_rank_matrix(</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">, </span><span class="s1">n_features=n_features</span><span class="s0">, </span><span class="s1">effective_rank=k</span><span class="s0">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">X[:</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">] = </span><span class="s3">1  </span><span class="s5"># last columns acts as intercept</span>
    <span class="s1">U</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">Vt = linalg.svd(X)</span>
    <span class="s0">assert </span><span class="s1">np.all(s &gt; </span><span class="s3">1e-3</span><span class="s1">)  </span><span class="s5"># to be sure</span>
    <span class="s1">U1</span><span class="s0">, </span><span class="s1">U2 = U[:</span><span class="s0">, </span><span class="s1">:k]</span><span class="s0">, </span><span class="s1">U[:</span><span class="s0">, </span><span class="s1">k:]</span>
    <span class="s1">Vt1</span><span class="s0">, </span><span class="s1">_ = Vt[:k</span><span class="s0">, </span><span class="s1">:]</span><span class="s0">, </span><span class="s1">Vt[k:</span><span class="s0">, </span><span class="s1">:]</span>

    <span class="s0">if </span><span class="s1">request.param == </span><span class="s2">&quot;long&quot;</span><span class="s1">:</span>
        <span class="s5"># Add a term that vanishes in the product X'y</span>
        <span class="s1">coef_ols = rng.uniform(low=-</span><span class="s3">10</span><span class="s0">, </span><span class="s1">high=</span><span class="s3">10</span><span class="s0">, </span><span class="s1">size=n_features)</span>
        <span class="s1">y = X @ coef_ols</span>
        <span class="s1">y += U2 @ rng.normal(size=n_samples - n_features) ** </span><span class="s3">2</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y = rng.uniform(low=-</span><span class="s3">10</span><span class="s0">, </span><span class="s1">high=</span><span class="s3">10</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
        <span class="s5"># w = X'(XX')^-1 y = V s^-1 U' y</span>
        <span class="s1">coef_ols = Vt1.T @ np.diag(</span><span class="s3">1 </span><span class="s1">/ s) @ U1.T @ y</span>

    <span class="s5"># Add penalty alpha * ||coef||_2^2 for alpha=1 and solve via normal equations.</span>
    <span class="s5"># Note that the problem is well conditioned such that we get accurate results.</span>
    <span class="s1">alpha = </span><span class="s3">1</span>
    <span class="s1">d = alpha * np.identity(n_features)</span>
    <span class="s1">d[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">] = </span><span class="s3">0  </span><span class="s5"># intercept gets no penalty</span>
    <span class="s1">coef_ridge = linalg.solve(X.T @ X + d</span><span class="s0">, </span><span class="s1">X.T @ y)</span>

    <span class="s5"># To be sure</span>
    <span class="s1">R_OLS = y - X @ coef_ols</span>
    <span class="s1">R_Ridge = y - X @ coef_ridge</span>
    <span class="s0">assert </span><span class="s1">np.linalg.norm(R_OLS) &lt; np.linalg.norm(R_Ridge)</span>

    <span class="s0">return </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">coef_ols</span><span class="s0">, </span><span class="s1">coef_ridge</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">SOLVERS)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;fit_intercept&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_ridge_regression(solver</span><span class="s0">, </span><span class="s1">fit_intercept</span><span class="s0">, </span><span class="s1">ols_ridge_dataset</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s4">&quot;&quot;&quot;Test that Ridge converges for all solvers to correct solution. 
 
    We work with a simple constructed data set with known solution. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">_</span><span class="s0">, </span><span class="s1">coef = ols_ridge_dataset</span>
    <span class="s1">alpha = </span><span class="s3">1.0  </span><span class="s5"># because ols_ridge_dataset uses this.</span>
    <span class="s1">params = dict(</span>
        <span class="s1">alpha=alpha</span><span class="s0">,</span>
        <span class="s1">fit_intercept=</span><span class="s0">True,</span>
        <span class="s1">solver=solver</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s3">1e-15 </span><span class="s0">if </span><span class="s1">solver </span><span class="s0">in </span><span class="s1">(</span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s1">) </span><span class="s0">else </span><span class="s3">1e-10</span><span class="s0">,</span>
        <span class="s1">random_state=global_random_seed</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s5"># Calculate residuals and R2.</span>
    <span class="s1">res_null = y - np.mean(y)</span>
    <span class="s1">res_Ridge = y - X @ coef</span>
    <span class="s1">R2_Ridge = </span><span class="s3">1 </span><span class="s1">- np.sum(res_Ridge**</span><span class="s3">2</span><span class="s1">) / np.sum(res_null**</span><span class="s3">2</span><span class="s1">)</span>

    <span class="s1">model = Ridge(**params)</span>
    <span class="s1">X = X[:</span><span class="s0">, </span><span class="s1">:-</span><span class="s3">1</span><span class="s1">]  </span><span class="s5"># remove intercept</span>
    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">intercept = coef[-</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">X = X - X.mean(axis=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">y = y - y.mean()</span>
        <span class="s1">intercept = </span><span class="s3">0</span>
    <span class="s1">model.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">coef = coef[:-</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s0">assert </span><span class="s1">model.intercept_ == pytest.approx(intercept)</span>
    <span class="s1">assert_allclose(model.coef_</span><span class="s0">, </span><span class="s1">coef)</span>
    <span class="s0">assert </span><span class="s1">model.score(X</span><span class="s0">, </span><span class="s1">y) == pytest.approx(R2_Ridge)</span>

    <span class="s5"># Same with sample_weight.</span>
    <span class="s1">model = Ridge(**params).fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=np.ones(X.shape[</span><span class="s3">0</span><span class="s1">]))</span>
    <span class="s0">assert </span><span class="s1">model.intercept_ == pytest.approx(intercept)</span>
    <span class="s1">assert_allclose(model.coef_</span><span class="s0">, </span><span class="s1">coef)</span>
    <span class="s0">assert </span><span class="s1">model.score(X</span><span class="s0">, </span><span class="s1">y) == pytest.approx(R2_Ridge)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">SOLVERS)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;fit_intercept&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_ridge_regression_hstacked_X(</span>
    <span class="s1">solver</span><span class="s0">, </span><span class="s1">fit_intercept</span><span class="s0">, </span><span class="s1">ols_ridge_dataset</span><span class="s0">, </span><span class="s1">global_random_seed</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Test that Ridge converges for all solvers to correct solution on hstacked data. 
 
    We work with a simple constructed data set with known solution. 
    Fit on [X] with alpha is the same as fit on [X, X]/2 with alpha/2. 
    For long X, [X, X] is a singular matrix. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">_</span><span class="s0">, </span><span class="s1">coef = ols_ridge_dataset</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">alpha = </span><span class="s3">1.0  </span><span class="s5"># because ols_ridge_dataset uses this.</span>

    <span class="s1">model = Ridge(</span>
        <span class="s1">alpha=alpha / </span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s0">,</span>
        <span class="s1">solver=solver</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s3">1e-15 </span><span class="s0">if </span><span class="s1">solver </span><span class="s0">in </span><span class="s1">(</span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s1">) </span><span class="s0">else </span><span class="s3">1e-10</span><span class="s0">,</span>
        <span class="s1">random_state=global_random_seed</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X = X[:</span><span class="s0">, </span><span class="s1">:-</span><span class="s3">1</span><span class="s1">]  </span><span class="s5"># remove intercept</span>
    <span class="s1">X = </span><span class="s3">0.5 </span><span class="s1">* np.concatenate((X</span><span class="s0">, </span><span class="s1">X)</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">np.linalg.matrix_rank(X) &lt;= min(n_samples</span><span class="s0">, </span><span class="s1">n_features - </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">intercept = coef[-</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">X = X - X.mean(axis=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">y = y - y.mean()</span>
        <span class="s1">intercept = </span><span class="s3">0</span>
    <span class="s1">model.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">coef = coef[:-</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s0">assert </span><span class="s1">model.intercept_ == pytest.approx(intercept)</span>
    <span class="s5"># coefficients are not all on the same magnitude, adding a small atol to</span>
    <span class="s5"># make this test less brittle</span>
    <span class="s1">assert_allclose(model.coef_</span><span class="s0">, </span><span class="s1">np.r_[coef</span><span class="s0">, </span><span class="s1">coef]</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-8</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">SOLVERS)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;fit_intercept&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_ridge_regression_vstacked_X(</span>
    <span class="s1">solver</span><span class="s0">, </span><span class="s1">fit_intercept</span><span class="s0">, </span><span class="s1">ols_ridge_dataset</span><span class="s0">, </span><span class="s1">global_random_seed</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Test that Ridge converges for all solvers to correct solution on vstacked data. 
 
    We work with a simple constructed data set with known solution. 
    Fit on [X] with alpha is the same as fit on [X], [y] 
                                                [X], [y] with 2 * alpha. 
    For wide X, [X', X'] is a singular matrix. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">_</span><span class="s0">, </span><span class="s1">coef = ols_ridge_dataset</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">alpha = </span><span class="s3">1.0  </span><span class="s5"># because ols_ridge_dataset uses this.</span>

    <span class="s1">model = Ridge(</span>
        <span class="s1">alpha=</span><span class="s3">2 </span><span class="s1">* alpha</span><span class="s0">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s0">,</span>
        <span class="s1">solver=solver</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s3">1e-15 </span><span class="s0">if </span><span class="s1">solver </span><span class="s0">in </span><span class="s1">(</span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s1">) </span><span class="s0">else </span><span class="s3">1e-10</span><span class="s0">,</span>
        <span class="s1">random_state=global_random_seed</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X = X[:</span><span class="s0">, </span><span class="s1">:-</span><span class="s3">1</span><span class="s1">]  </span><span class="s5"># remove intercept</span>
    <span class="s1">X = np.concatenate((X</span><span class="s0">, </span><span class="s1">X)</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">np.linalg.matrix_rank(X) &lt;= min(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">y = np.r_[y</span><span class="s0">, </span><span class="s1">y]</span>
    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">intercept = coef[-</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">X = X - X.mean(axis=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">y = y - y.mean()</span>
        <span class="s1">intercept = </span><span class="s3">0</span>
    <span class="s1">model.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">coef = coef[:-</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s0">assert </span><span class="s1">model.intercept_ == pytest.approx(intercept)</span>
    <span class="s5"># coefficients are not all on the same magnitude, adding a small atol to</span>
    <span class="s5"># make this test less brittle</span>
    <span class="s1">assert_allclose(model.coef_</span><span class="s0">, </span><span class="s1">coef</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-8</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">SOLVERS)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;fit_intercept&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_ridge_regression_unpenalized(</span>
    <span class="s1">solver</span><span class="s0">, </span><span class="s1">fit_intercept</span><span class="s0">, </span><span class="s1">ols_ridge_dataset</span><span class="s0">, </span><span class="s1">global_random_seed</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Test that unpenalized Ridge = OLS converges for all solvers to correct solution. 
 
    We work with a simple constructed data set with known solution. 
    Note: This checks the minimum norm solution for wide X, i.e. 
    n_samples &lt; n_features: 
        min ||w||_2 subject to X w = y 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">coef</span><span class="s0">, </span><span class="s1">_ = ols_ridge_dataset</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">alpha = </span><span class="s3">0  </span><span class="s5"># OLS</span>
    <span class="s1">params = dict(</span>
        <span class="s1">alpha=alpha</span><span class="s0">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s0">,</span>
        <span class="s1">solver=solver</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s3">1e-15 </span><span class="s0">if </span><span class="s1">solver </span><span class="s0">in </span><span class="s1">(</span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s1">) </span><span class="s0">else </span><span class="s3">1e-10</span><span class="s0">,</span>
        <span class="s1">random_state=global_random_seed</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">model = Ridge(**params)</span>
    <span class="s5"># Note that cholesky might give a warning: &quot;Singular matrix in solving dual</span>
    <span class="s5"># problem. Using least-squares solution instead.&quot;</span>
    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">X = X[:</span><span class="s0">, </span><span class="s1">:-</span><span class="s3">1</span><span class="s1">]  </span><span class="s5"># remove intercept</span>
        <span class="s1">intercept = coef[-</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">coef = coef[:-</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">intercept = </span><span class="s3">0</span>
    <span class="s1">model.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s5"># FIXME: `assert_allclose(model.coef_, coef)` should work for all cases but fails</span>
    <span class="s5"># for the wide/fat case with n_features &gt; n_samples. The current Ridge solvers do</span>
    <span class="s5"># NOT return the minimum norm solution with fit_intercept=True.</span>
    <span class="s0">if </span><span class="s1">n_samples &gt; n_features </span><span class="s0">or not </span><span class="s1">fit_intercept:</span>
        <span class="s0">assert </span><span class="s1">model.intercept_ == pytest.approx(intercept)</span>
        <span class="s1">assert_allclose(model.coef_</span><span class="s0">, </span><span class="s1">coef)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s5"># As it is an underdetermined problem, residuals = 0. This shows that we get</span>
        <span class="s5"># a solution to X w = y ....</span>
        <span class="s1">assert_allclose(model.predict(X)</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s1">assert_allclose(X @ coef + intercept</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s5"># But it is not the minimum norm solution. (This should be equal.)</span>
        <span class="s0">assert </span><span class="s1">np.linalg.norm(np.r_[model.intercept_</span><span class="s0">, </span><span class="s1">model.coef_]) &gt; np.linalg.norm(</span>
            <span class="s1">np.r_[intercept</span><span class="s0">, </span><span class="s1">coef]</span>
        <span class="s1">)</span>

        <span class="s1">pytest.xfail(reason=</span><span class="s2">&quot;Ridge does not provide the minimum norm solution.&quot;</span><span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">model.intercept_ == pytest.approx(intercept)</span>
        <span class="s1">assert_allclose(model.coef_</span><span class="s0">, </span><span class="s1">coef)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">SOLVERS)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;fit_intercept&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_ridge_regression_unpenalized_hstacked_X(</span>
    <span class="s1">solver</span><span class="s0">, </span><span class="s1">fit_intercept</span><span class="s0">, </span><span class="s1">ols_ridge_dataset</span><span class="s0">, </span><span class="s1">global_random_seed</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Test that unpenalized Ridge = OLS converges for all solvers to correct solution. 
 
    We work with a simple constructed data set with known solution. 
    OLS fit on [X] is the same as fit on [X, X]/2. 
    For long X, [X, X] is a singular matrix and we check against the minimum norm 
    solution: 
        min ||w||_2 subject to min ||X w - y||_2 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">coef</span><span class="s0">, </span><span class="s1">_ = ols_ridge_dataset</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">alpha = </span><span class="s3">0  </span><span class="s5"># OLS</span>

    <span class="s1">model = Ridge(</span>
        <span class="s1">alpha=alpha</span><span class="s0">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s0">,</span>
        <span class="s1">solver=solver</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s3">1e-15 </span><span class="s0">if </span><span class="s1">solver </span><span class="s0">in </span><span class="s1">(</span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s1">) </span><span class="s0">else </span><span class="s3">1e-10</span><span class="s0">,</span>
        <span class="s1">random_state=global_random_seed</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">X = X[:</span><span class="s0">, </span><span class="s1">:-</span><span class="s3">1</span><span class="s1">]  </span><span class="s5"># remove intercept</span>
        <span class="s1">intercept = coef[-</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">coef = coef[:-</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">intercept = </span><span class="s3">0</span>
    <span class="s1">X = </span><span class="s3">0.5 </span><span class="s1">* np.concatenate((X</span><span class="s0">, </span><span class="s1">X)</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">np.linalg.matrix_rank(X) &lt;= min(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">model.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">n_samples &gt; n_features </span><span class="s0">or not </span><span class="s1">fit_intercept:</span>
        <span class="s0">assert </span><span class="s1">model.intercept_ == pytest.approx(intercept)</span>
        <span class="s0">if </span><span class="s1">solver == </span><span class="s2">&quot;cholesky&quot;</span><span class="s1">:</span>
            <span class="s5"># Cholesky is a bad choice for singular X.</span>
            <span class="s1">pytest.skip()</span>
        <span class="s1">assert_allclose(model.coef_</span><span class="s0">, </span><span class="s1">np.r_[coef</span><span class="s0">, </span><span class="s1">coef])</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s5"># FIXME: Same as in test_ridge_regression_unpenalized.</span>
        <span class="s5"># As it is an underdetermined problem, residuals = 0. This shows that we get</span>
        <span class="s5"># a solution to X w = y ....</span>
        <span class="s1">assert_allclose(model.predict(X)</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s5"># But it is not the minimum norm solution. (This should be equal.)</span>
        <span class="s0">assert </span><span class="s1">np.linalg.norm(np.r_[model.intercept_</span><span class="s0">, </span><span class="s1">model.coef_]) &gt; np.linalg.norm(</span>
            <span class="s1">np.r_[intercept</span><span class="s0">, </span><span class="s1">coef</span><span class="s0">, </span><span class="s1">coef]</span>
        <span class="s1">)</span>

        <span class="s1">pytest.xfail(reason=</span><span class="s2">&quot;Ridge does not provide the minimum norm solution.&quot;</span><span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">model.intercept_ == pytest.approx(intercept)</span>
        <span class="s1">assert_allclose(model.coef_</span><span class="s0">, </span><span class="s1">np.r_[coef</span><span class="s0">, </span><span class="s1">coef])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">SOLVERS)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;fit_intercept&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_ridge_regression_unpenalized_vstacked_X(</span>
    <span class="s1">solver</span><span class="s0">, </span><span class="s1">fit_intercept</span><span class="s0">, </span><span class="s1">ols_ridge_dataset</span><span class="s0">, </span><span class="s1">global_random_seed</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Test that unpenalized Ridge = OLS converges for all solvers to correct solution. 
 
    We work with a simple constructed data set with known solution. 
    OLS fit on [X] is the same as fit on [X], [y] 
                                         [X], [y]. 
    For wide X, [X', X'] is a singular matrix and we check against the minimum norm 
    solution: 
        min ||w||_2 subject to X w = y 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">coef</span><span class="s0">, </span><span class="s1">_ = ols_ridge_dataset</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">alpha = </span><span class="s3">0  </span><span class="s5"># OLS</span>

    <span class="s1">model = Ridge(</span>
        <span class="s1">alpha=alpha</span><span class="s0">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s0">,</span>
        <span class="s1">solver=solver</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s3">1e-15 </span><span class="s0">if </span><span class="s1">solver </span><span class="s0">in </span><span class="s1">(</span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s1">) </span><span class="s0">else </span><span class="s3">1e-10</span><span class="s0">,</span>
        <span class="s1">random_state=global_random_seed</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">X = X[:</span><span class="s0">, </span><span class="s1">:-</span><span class="s3">1</span><span class="s1">]  </span><span class="s5"># remove intercept</span>
        <span class="s1">intercept = coef[-</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">coef = coef[:-</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">intercept = </span><span class="s3">0</span>
    <span class="s1">X = np.concatenate((X</span><span class="s0">, </span><span class="s1">X)</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">np.linalg.matrix_rank(X) &lt;= min(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">y = np.r_[y</span><span class="s0">, </span><span class="s1">y]</span>
    <span class="s1">model.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">if </span><span class="s1">n_samples &gt; n_features </span><span class="s0">or not </span><span class="s1">fit_intercept:</span>
        <span class="s0">assert </span><span class="s1">model.intercept_ == pytest.approx(intercept)</span>
        <span class="s1">assert_allclose(model.coef_</span><span class="s0">, </span><span class="s1">coef)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s5"># FIXME: Same as in test_ridge_regression_unpenalized.</span>
        <span class="s5"># As it is an underdetermined problem, residuals = 0. This shows that we get</span>
        <span class="s5"># a solution to X w = y ....</span>
        <span class="s1">assert_allclose(model.predict(X)</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s5"># But it is not the minimum norm solution. (This should be equal.)</span>
        <span class="s0">assert </span><span class="s1">np.linalg.norm(np.r_[model.intercept_</span><span class="s0">, </span><span class="s1">model.coef_]) &gt; np.linalg.norm(</span>
            <span class="s1">np.r_[intercept</span><span class="s0">, </span><span class="s1">coef]</span>
        <span class="s1">)</span>

        <span class="s1">pytest.xfail(reason=</span><span class="s2">&quot;Ridge does not provide the minimum norm solution.&quot;</span><span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">model.intercept_ == pytest.approx(intercept)</span>
        <span class="s1">assert_allclose(model.coef_</span><span class="s0">, </span><span class="s1">coef)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">SOLVERS)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;fit_intercept&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;sparseX&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;alpha&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1e-2</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_ridge_regression_sample_weights(</span>
    <span class="s1">solver</span><span class="s0">, </span><span class="s1">fit_intercept</span><span class="s0">, </span><span class="s1">sparseX</span><span class="s0">, </span><span class="s1">alpha</span><span class="s0">, </span><span class="s1">ols_ridge_dataset</span><span class="s0">, </span><span class="s1">global_random_seed</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Test that Ridge with sample weights gives correct results. 
 
    We use the following trick: 
        ||y - Xw||_2 = (z - Aw)' W (z - Aw) 
    for z=[y, y], A' = [X', X'] (vstacked), and W[:n/2] + W[n/2:] = 1, W=diag(W) 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">sparseX:</span>
        <span class="s0">if </span><span class="s1">fit_intercept </span><span class="s0">and </span><span class="s1">solver </span><span class="s0">not in </span><span class="s1">SPARSE_SOLVERS_WITH_INTERCEPT:</span>
            <span class="s1">pytest.skip()</span>
        <span class="s0">elif not </span><span class="s1">fit_intercept </span><span class="s0">and </span><span class="s1">solver </span><span class="s0">not in </span><span class="s1">SPARSE_SOLVERS_WITHOUT_INTERCEPT:</span>
            <span class="s1">pytest.skip()</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">_</span><span class="s0">, </span><span class="s1">coef = ols_ridge_dataset</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">sw = rng.uniform(low=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">high=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">size=n_samples)</span>

    <span class="s1">model = Ridge(</span>
        <span class="s1">alpha=alpha</span><span class="s0">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s0">,</span>
        <span class="s1">solver=solver</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s3">1e-15 </span><span class="s0">if </span><span class="s1">solver </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s1">] </span><span class="s0">else </span><span class="s3">1e-10</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">100_000</span><span class="s0">,</span>
        <span class="s1">random_state=global_random_seed</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X = X[:</span><span class="s0">, </span><span class="s1">:-</span><span class="s3">1</span><span class="s1">]  </span><span class="s5"># remove intercept</span>
    <span class="s1">X = np.concatenate((X</span><span class="s0">, </span><span class="s1">X)</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">y = np.r_[y</span><span class="s0">, </span><span class="s1">y]</span>
    <span class="s1">sw = np.r_[sw</span><span class="s0">, </span><span class="s3">1 </span><span class="s1">- sw] * alpha</span>
    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">intercept = coef[-</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">X = X - X.mean(axis=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">y = y - y.mean()</span>
        <span class="s1">intercept = </span><span class="s3">0</span>
    <span class="s0">if </span><span class="s1">sparseX:</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
    <span class="s1">model.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sw)</span>
    <span class="s1">coef = coef[:-</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s0">assert </span><span class="s1">model.intercept_ == pytest.approx(intercept)</span>
    <span class="s1">assert_allclose(model.coef_</span><span class="s0">, </span><span class="s1">coef)</span>


<span class="s0">def </span><span class="s1">test_primal_dual_relationship():</span>
    <span class="s1">y = y_diabetes.reshape(-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">coef = _solve_cholesky(X_diabetes</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">alpha=[</span><span class="s3">1e-2</span><span class="s1">])</span>
    <span class="s1">K = np.dot(X_diabetes</span><span class="s0">, </span><span class="s1">X_diabetes.T)</span>
    <span class="s1">dual_coef = _solve_cholesky_kernel(K</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">alpha=[</span><span class="s3">1e-2</span><span class="s1">])</span>
    <span class="s1">coef2 = np.dot(X_diabetes.T</span><span class="s0">, </span><span class="s1">dual_coef).T</span>
    <span class="s1">assert_array_almost_equal(coef</span><span class="s0">, </span><span class="s1">coef2)</span>


<span class="s0">def </span><span class="s1">test_ridge_regression_convergence_fail():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">y = rng.randn(</span><span class="s3">5</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(</span><span class="s3">5</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">warning_message = </span><span class="s2">r&quot;sparse_cg did not converge after&quot; r&quot; [0-9]+ iterations.&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s0">, </span><span class="s1">match=warning_message):</span>
        <span class="s1">ridge_regression(</span>
            <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">solver=</span><span class="s2">&quot;sparse_cg&quot;</span><span class="s0">, </span><span class="s1">tol=</span><span class="s3">0.0</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s0">None, </span><span class="s1">verbose=</span><span class="s3">1</span>
        <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_ridge_shapes_type():</span>
    <span class="s5"># Test shape of coef_ and intercept_</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s3">5</span><span class="s0">, </span><span class="s3">10</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">y = rng.randn(n_samples)</span>
    <span class="s1">Y1 = y[:</span><span class="s0">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">Y = np.c_[y</span><span class="s0">, </span><span class="s3">1 </span><span class="s1">+ y]</span>

    <span class="s1">ridge = Ridge()</span>

    <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">ridge.coef_.shape == (n_features</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">ridge.intercept_.shape == ()</span>
    <span class="s0">assert </span><span class="s1">isinstance(ridge.coef_</span><span class="s0">, </span><span class="s1">np.ndarray)</span>
    <span class="s0">assert </span><span class="s1">isinstance(ridge.intercept_</span><span class="s0">, </span><span class="s1">float)</span>

    <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">Y1)</span>
    <span class="s0">assert </span><span class="s1">ridge.coef_.shape == (</span><span class="s3">1</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s0">assert </span><span class="s1">ridge.intercept_.shape == (</span><span class="s3">1</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">isinstance(ridge.coef_</span><span class="s0">, </span><span class="s1">np.ndarray)</span>
    <span class="s0">assert </span><span class="s1">isinstance(ridge.intercept_</span><span class="s0">, </span><span class="s1">np.ndarray)</span>

    <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s0">assert </span><span class="s1">ridge.coef_.shape == (</span><span class="s3">2</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s0">assert </span><span class="s1">ridge.intercept_.shape == (</span><span class="s3">2</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">isinstance(ridge.coef_</span><span class="s0">, </span><span class="s1">np.ndarray)</span>
    <span class="s0">assert </span><span class="s1">isinstance(ridge.intercept_</span><span class="s0">, </span><span class="s1">np.ndarray)</span>


<span class="s0">def </span><span class="s1">test_ridge_intercept():</span>
    <span class="s5"># Test intercept with multiple targets GH issue #708</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s3">5</span><span class="s0">, </span><span class="s3">10</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">y = rng.randn(n_samples)</span>
    <span class="s1">Y = np.c_[y</span><span class="s0">, </span><span class="s3">1.0 </span><span class="s1">+ y]</span>

    <span class="s1">ridge = Ridge()</span>

    <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">intercept = ridge.intercept_</span>

    <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">assert_almost_equal(ridge.intercept_[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">intercept)</span>
    <span class="s1">assert_almost_equal(ridge.intercept_[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">intercept + </span><span class="s3">1.0</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_ridge_vs_lstsq():</span>
    <span class="s5"># On alpha=0., Ridge and OLS yield the same solution.</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s5"># we need more samples than features</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s3">5</span><span class="s0">, </span><span class="s3">4</span>
    <span class="s1">y = rng.randn(n_samples)</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>

    <span class="s1">ridge = Ridge(alpha=</span><span class="s3">0.0</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">ols = LinearRegression(fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">ols.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(ridge.coef_</span><span class="s0">, </span><span class="s1">ols.coef_)</span>

    <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">ols.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(ridge.coef_</span><span class="s0">, </span><span class="s1">ols.coef_)</span>


<span class="s0">def </span><span class="s1">test_ridge_individual_penalties():</span>
    <span class="s5"># Tests the ridge object using individual penalties</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>

    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features</span><span class="s0">, </span><span class="s1">n_targets = </span><span class="s3">20</span><span class="s0">, </span><span class="s3">10</span><span class="s0">, </span><span class="s3">5</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">y = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_targets)</span>

    <span class="s1">penalties = np.arange(n_targets)</span>

    <span class="s1">coef_cholesky = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">Ridge(alpha=alpha</span><span class="s0">, </span><span class="s1">solver=</span><span class="s2">&quot;cholesky&quot;</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">target).coef_</span>
            <span class="s0">for </span><span class="s1">alpha</span><span class="s0">, </span><span class="s1">target </span><span class="s0">in </span><span class="s1">zip(penalties</span><span class="s0">, </span><span class="s1">y.T)</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">coefs_indiv_pen = [</span>
        <span class="s1">Ridge(alpha=penalties</span><span class="s0">, </span><span class="s1">solver=solver</span><span class="s0">, </span><span class="s1">tol=</span><span class="s3">1e-12</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y).coef_</span>
        <span class="s0">for </span><span class="s1">solver </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;svd&quot;</span><span class="s0">, </span><span class="s2">&quot;sparse_cg&quot;</span><span class="s0">, </span><span class="s2">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s2">&quot;cholesky&quot;</span><span class="s0">, </span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s1">]</span>
    <span class="s1">]</span>
    <span class="s0">for </span><span class="s1">coef_indiv_pen </span><span class="s0">in </span><span class="s1">coefs_indiv_pen:</span>
        <span class="s1">assert_array_almost_equal(coef_cholesky</span><span class="s0">, </span><span class="s1">coef_indiv_pen)</span>

    <span class="s5"># Test error is raised when number of targets and penalties do not match.</span>
    <span class="s1">ridge = Ridge(alpha=penalties[:-</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">err_msg = </span><span class="s2">&quot;Number of targets and number of penalties do not correspond: 4 != 5&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;n_col&quot;</span><span class="s0">, </span><span class="s1">[()</span><span class="s0">, </span><span class="s1">(</span><span class="s3">1</span><span class="s0">,</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">3</span><span class="s0">,</span><span class="s1">)])</span>
<span class="s0">def </span><span class="s1">test_X_CenterStackOp(n_col):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(</span><span class="s3">11</span><span class="s0">, </span><span class="s3">8</span><span class="s1">)</span>
    <span class="s1">X_m = rng.randn(</span><span class="s3">8</span><span class="s1">)</span>
    <span class="s1">sqrt_sw = rng.randn(len(X))</span>
    <span class="s1">Y = rng.randn(</span><span class="s3">11</span><span class="s0">, </span><span class="s1">*n_col)</span>
    <span class="s1">A = rng.randn(</span><span class="s3">9</span><span class="s0">, </span><span class="s1">*n_col)</span>
    <span class="s1">operator = _X_CenterStackOp(sp.csr_matrix(X)</span><span class="s0">, </span><span class="s1">X_m</span><span class="s0">, </span><span class="s1">sqrt_sw)</span>
    <span class="s1">reference_operator = np.hstack([X - sqrt_sw[:</span><span class="s0">, None</span><span class="s1">] * X_m</span><span class="s0">, </span><span class="s1">sqrt_sw[:</span><span class="s0">, None</span><span class="s1">]])</span>
    <span class="s1">assert_allclose(reference_operator.dot(A)</span><span class="s0">, </span><span class="s1">operator.dot(A))</span>
    <span class="s1">assert_allclose(reference_operator.T.dot(Y)</span><span class="s0">, </span><span class="s1">operator.T.dot(Y))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;shape&quot;</span><span class="s0">, </span><span class="s1">[(</span><span class="s3">10</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">13</span><span class="s0">, </span><span class="s3">9</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">3</span><span class="s0">, </span><span class="s3">7</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">20</span><span class="s0">, </span><span class="s3">20</span><span class="s1">)])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;uniform_weights&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_compute_gram(shape</span><span class="s0">, </span><span class="s1">uniform_weights):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(*shape)</span>
    <span class="s0">if </span><span class="s1">uniform_weights:</span>
        <span class="s1">sw = np.ones(X.shape[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">sw = rng.chisquare(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">shape[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">sqrt_sw = np.sqrt(sw)</span>
    <span class="s1">X_mean = np.average(X</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">weights=sw)</span>
    <span class="s1">X_centered = (X - X_mean) * sqrt_sw[:</span><span class="s0">, None</span><span class="s1">]</span>
    <span class="s1">true_gram = X_centered.dot(X_centered.T)</span>
    <span class="s1">X_sparse = sp.csr_matrix(X * sqrt_sw[:</span><span class="s0">, None</span><span class="s1">])</span>
    <span class="s1">gcv = _RidgeGCV(fit_intercept=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">computed_gram</span><span class="s0">, </span><span class="s1">computed_mean = gcv._compute_gram(X_sparse</span><span class="s0">, </span><span class="s1">sqrt_sw)</span>
    <span class="s1">assert_allclose(X_mean</span><span class="s0">, </span><span class="s1">computed_mean)</span>
    <span class="s1">assert_allclose(true_gram</span><span class="s0">, </span><span class="s1">computed_gram)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;shape&quot;</span><span class="s0">, </span><span class="s1">[(</span><span class="s3">10</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">13</span><span class="s0">, </span><span class="s3">9</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">3</span><span class="s0">, </span><span class="s3">7</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">20</span><span class="s0">, </span><span class="s3">20</span><span class="s1">)])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;uniform_weights&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_compute_covariance(shape</span><span class="s0">, </span><span class="s1">uniform_weights):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(*shape)</span>
    <span class="s0">if </span><span class="s1">uniform_weights:</span>
        <span class="s1">sw = np.ones(X.shape[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">sw = rng.chisquare(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">shape[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">sqrt_sw = np.sqrt(sw)</span>
    <span class="s1">X_mean = np.average(X</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">weights=sw)</span>
    <span class="s1">X_centered = (X - X_mean) * sqrt_sw[:</span><span class="s0">, None</span><span class="s1">]</span>
    <span class="s1">true_covariance = X_centered.T.dot(X_centered)</span>
    <span class="s1">X_sparse = sp.csr_matrix(X * sqrt_sw[:</span><span class="s0">, None</span><span class="s1">])</span>
    <span class="s1">gcv = _RidgeGCV(fit_intercept=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">computed_cov</span><span class="s0">, </span><span class="s1">computed_mean = gcv._compute_covariance(X_sparse</span><span class="s0">, </span><span class="s1">sqrt_sw)</span>
    <span class="s1">assert_allclose(X_mean</span><span class="s0">, </span><span class="s1">computed_mean)</span>
    <span class="s1">assert_allclose(true_covariance</span><span class="s0">, </span><span class="s1">computed_cov)</span>


<span class="s0">def </span><span class="s1">_make_sparse_offset_regression(</span>
    <span class="s1">n_samples=</span><span class="s3">100</span><span class="s0">,</span>
    <span class="s1">n_features=</span><span class="s3">100</span><span class="s0">,</span>
    <span class="s1">proportion_nonzero=</span><span class="s3">0.5</span><span class="s0">,</span>
    <span class="s1">n_informative=</span><span class="s3">10</span><span class="s0">,</span>
    <span class="s1">n_targets=</span><span class="s3">1</span><span class="s0">,</span>
    <span class="s1">bias=</span><span class="s3">13.0</span><span class="s0">,</span>
    <span class="s1">X_offset=</span><span class="s3">30.0</span><span class="s0">,</span>
    <span class="s1">noise=</span><span class="s3">30.0</span><span class="s0">,</span>
    <span class="s1">shuffle=</span><span class="s0">True,</span>
    <span class="s1">coef=</span><span class="s0">False,</span>
    <span class="s1">positive=</span><span class="s0">False,</span>
    <span class="s1">random_state=</span><span class="s0">None,</span>
<span class="s1">):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">c = make_regression(</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
        <span class="s1">n_features=n_features</span><span class="s0">,</span>
        <span class="s1">n_informative=n_informative</span><span class="s0">,</span>
        <span class="s1">n_targets=n_targets</span><span class="s0">,</span>
        <span class="s1">bias=bias</span><span class="s0">,</span>
        <span class="s1">noise=noise</span><span class="s0">,</span>
        <span class="s1">shuffle=shuffle</span><span class="s0">,</span>
        <span class="s1">coef=</span><span class="s0">True,</span>
        <span class="s1">random_state=random_state</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">if </span><span class="s1">n_features == </span><span class="s3">1</span><span class="s1">:</span>
        <span class="s1">c = np.asarray([c])</span>
    <span class="s1">X += X_offset</span>
    <span class="s1">mask = (</span>
        <span class="s1">np.random.RandomState(random_state).binomial(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">proportion_nonzero</span><span class="s0">, </span><span class="s1">X.shape) &gt; </span><span class="s3">0</span>
    <span class="s1">)</span>
    <span class="s1">removed_X = X.copy()</span>
    <span class="s1">X[~mask] = </span><span class="s3">0.0</span>
    <span class="s1">removed_X[mask] = </span><span class="s3">0.0</span>
    <span class="s1">y -= removed_X.dot(c)</span>
    <span class="s0">if </span><span class="s1">positive:</span>
        <span class="s1">y += X.dot(np.abs(c) + </span><span class="s3">1 </span><span class="s1">- c)</span>
        <span class="s1">c = np.abs(c) + </span><span class="s3">1</span>
    <span class="s0">if </span><span class="s1">n_features == </span><span class="s3">1</span><span class="s1">:</span>
        <span class="s1">c = c[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s0">if </span><span class="s1">coef:</span>
        <span class="s0">return </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">c</span>
    <span class="s0">return </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;solver, sparse_X&quot;</span><span class="s0">,</span>
    <span class="s1">(</span>
        <span class="s1">(solver</span><span class="s0">, </span><span class="s1">sparse_X)</span>
        <span class="s0">for </span><span class="s1">(solver</span><span class="s0">, </span><span class="s1">sparse_X) </span><span class="s0">in </span><span class="s1">product(</span>
            <span class="s1">[</span><span class="s2">&quot;cholesky&quot;</span><span class="s0">, </span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;sparse_cg&quot;</span><span class="s0">, </span><span class="s2">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s0">, </span><span class="s2">&quot;ridgecv&quot;</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s0">False, True</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s0">if not </span><span class="s1">(sparse_X </span><span class="s0">and </span><span class="s1">solver </span><span class="s0">not in </span><span class="s1">[</span><span class="s2">&quot;sparse_cg&quot;</span><span class="s0">, </span><span class="s2">&quot;ridgecv&quot;</span><span class="s1">])</span>
    <span class="s1">)</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;n_samples,dtype,proportion_nonzero&quot;</span><span class="s0">,</span>
    <span class="s1">[(</span><span class="s3">20</span><span class="s0">, </span><span class="s2">&quot;float32&quot;</span><span class="s0">, </span><span class="s3">0.1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">40</span><span class="s0">, </span><span class="s2">&quot;float32&quot;</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">20</span><span class="s0">, </span><span class="s2">&quot;float64&quot;</span><span class="s0">, </span><span class="s3">0.2</span><span class="s1">)]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;seed&quot;</span><span class="s0">, </span><span class="s1">np.arange(</span><span class="s3">3</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_solver_consistency(</span>
    <span class="s1">solver</span><span class="s0">, </span><span class="s1">proportion_nonzero</span><span class="s0">, </span><span class="s1">n_samples</span><span class="s0">, </span><span class="s1">dtype</span><span class="s0">, </span><span class="s1">sparse_X</span><span class="s0">, </span><span class="s1">seed</span>
<span class="s1">):</span>
    <span class="s1">alpha = </span><span class="s3">1.0</span>
    <span class="s1">noise = </span><span class="s3">50.0 </span><span class="s0">if </span><span class="s1">proportion_nonzero &gt; </span><span class="s3">0.9 </span><span class="s0">else </span><span class="s3">500.0</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = _make_sparse_offset_regression(</span>
        <span class="s1">bias=</span><span class="s3">10</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s3">30</span><span class="s0">,</span>
        <span class="s1">proportion_nonzero=proportion_nonzero</span><span class="s0">,</span>
        <span class="s1">noise=noise</span><span class="s0">,</span>
        <span class="s1">random_state=seed</span><span class="s0">,</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s5"># Manually scale the data to avoid pathological cases. We use</span>
    <span class="s5"># minmax_scale to deal with the sparse case without breaking</span>
    <span class="s5"># the sparsity pattern.</span>
    <span class="s1">X = minmax_scale(X)</span>

    <span class="s1">svd_ridge = Ridge(solver=</span><span class="s2">&quot;svd&quot;</span><span class="s0">, </span><span class="s1">alpha=alpha).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">X = X.astype(dtype</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">y = y.astype(dtype</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">sparse_X:</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
    <span class="s0">if </span><span class="s1">solver == </span><span class="s2">&quot;ridgecv&quot;</span><span class="s1">:</span>
        <span class="s1">ridge = RidgeCV(alphas=[alpha])</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">ridge = Ridge(solver=solver</span><span class="s0">, </span><span class="s1">tol=</span><span class="s3">1e-10</span><span class="s0">, </span><span class="s1">alpha=alpha)</span>
    <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_allclose(ridge.coef_</span><span class="s0">, </span><span class="s1">svd_ridge.coef_</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-3</span><span class="s1">)</span>
    <span class="s1">assert_allclose(ridge.intercept_</span><span class="s0">, </span><span class="s1">svd_ridge.intercept_</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-3</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;gcv_mode&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;svd&quot;</span><span class="s0">, </span><span class="s2">&quot;eigen&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;X_constructor&quot;</span><span class="s0">, </span><span class="s1">[np.asarray</span><span class="s0">, </span><span class="s1">sp.csr_matrix])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;X_shape&quot;</span><span class="s0">, </span><span class="s1">[(</span><span class="s3">11</span><span class="s0">, </span><span class="s3">8</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">11</span><span class="s0">, </span><span class="s3">20</span><span class="s1">)])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;fit_intercept&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;y_shape, noise&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">((</span><span class="s3">11</span><span class="s0">,</span><span class="s1">)</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">((</span><span class="s3">11</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s3">30.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">((</span><span class="s3">11</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s3">150.0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_ridge_gcv_vs_ridge_loo_cv(</span>
    <span class="s1">gcv_mode</span><span class="s0">, </span><span class="s1">X_constructor</span><span class="s0">, </span><span class="s1">X_shape</span><span class="s0">, </span><span class="s1">y_shape</span><span class="s0">, </span><span class="s1">fit_intercept</span><span class="s0">, </span><span class="s1">noise</span>
<span class="s1">):</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = X_shape</span>
    <span class="s1">n_targets = y_shape[-</span><span class="s3">1</span><span class="s1">] </span><span class="s0">if </span><span class="s1">len(y_shape) == </span><span class="s3">2 </span><span class="s0">else </span><span class="s3">1</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = _make_sparse_offset_regression(</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
        <span class="s1">n_features=n_features</span><span class="s0">,</span>
        <span class="s1">n_targets=n_targets</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
        <span class="s1">noise=noise</span><span class="s0">,</span>
        <span class="s1">n_informative=</span><span class="s3">5</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">y = y.reshape(y_shape)</span>

    <span class="s1">alphas = [</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">10.0</span><span class="s0">, </span><span class="s3">1e3</span><span class="s1">]</span>
    <span class="s1">loo_ridge = RidgeCV(</span>
        <span class="s1">cv=n_samples</span><span class="s0">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s0">,</span>
        <span class="s1">alphas=alphas</span><span class="s0">,</span>
        <span class="s1">scoring=</span><span class="s2">&quot;neg_mean_squared_error&quot;</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">gcv_ridge = RidgeCV(</span>
        <span class="s1">gcv_mode=gcv_mode</span><span class="s0">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s0">,</span>
        <span class="s1">alphas=alphas</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">loo_ridge.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">X_gcv = X_constructor(X)</span>
    <span class="s1">gcv_ridge.fit(X_gcv</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">assert </span><span class="s1">gcv_ridge.alpha_ == pytest.approx(loo_ridge.alpha_)</span>
    <span class="s1">assert_allclose(gcv_ridge.coef_</span><span class="s0">, </span><span class="s1">loo_ridge.coef_</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-3</span><span class="s1">)</span>
    <span class="s1">assert_allclose(gcv_ridge.intercept_</span><span class="s0">, </span><span class="s1">loo_ridge.intercept_</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-3</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_ridge_loo_cv_asym_scoring():</span>
    <span class="s5"># checking on asymmetric scoring</span>
    <span class="s1">scoring = </span><span class="s2">&quot;explained_variance&quot;</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s3">10</span><span class="s0">, </span><span class="s3">5</span>
    <span class="s1">n_targets = </span><span class="s3">1</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = _make_sparse_offset_regression(</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
        <span class="s1">n_features=n_features</span><span class="s0">,</span>
        <span class="s1">n_targets=n_targets</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
        <span class="s1">noise=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">n_informative=</span><span class="s3">5</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">alphas = [</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">10.0</span><span class="s0">, </span><span class="s3">1e3</span><span class="s1">]</span>
    <span class="s1">loo_ridge = RidgeCV(</span>
        <span class="s1">cv=n_samples</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">True, </span><span class="s1">alphas=alphas</span><span class="s0">, </span><span class="s1">scoring=scoring</span>
    <span class="s1">)</span>

    <span class="s1">gcv_ridge = RidgeCV(fit_intercept=</span><span class="s0">True, </span><span class="s1">alphas=alphas</span><span class="s0">, </span><span class="s1">scoring=scoring)</span>

    <span class="s1">loo_ridge.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">gcv_ridge.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">assert </span><span class="s1">gcv_ridge.alpha_ == pytest.approx(loo_ridge.alpha_)</span>
    <span class="s1">assert_allclose(gcv_ridge.coef_</span><span class="s0">, </span><span class="s1">loo_ridge.coef_</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-3</span><span class="s1">)</span>
    <span class="s1">assert_allclose(gcv_ridge.intercept_</span><span class="s0">, </span><span class="s1">loo_ridge.intercept_</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-3</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;gcv_mode&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;svd&quot;</span><span class="s0">, </span><span class="s2">&quot;eigen&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;X_constructor&quot;</span><span class="s0">, </span><span class="s1">[np.asarray</span><span class="s0">, </span><span class="s1">sp.csr_matrix])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;n_features&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">8</span><span class="s0">, </span><span class="s3">20</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;y_shape, fit_intercept, noise&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">((</span><span class="s3">11</span><span class="s0">,</span><span class="s1">)</span><span class="s0">, True, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">((</span><span class="s3">11</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span><span class="s0">, True, </span><span class="s3">20.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">((</span><span class="s3">11</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span><span class="s0">, True, </span><span class="s3">150.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">((</span><span class="s3">11</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span><span class="s0">, False, </span><span class="s3">30.0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_ridge_gcv_sample_weights(</span>
    <span class="s1">gcv_mode</span><span class="s0">, </span><span class="s1">X_constructor</span><span class="s0">, </span><span class="s1">fit_intercept</span><span class="s0">, </span><span class="s1">n_features</span><span class="s0">, </span><span class="s1">y_shape</span><span class="s0">, </span><span class="s1">noise</span>
<span class="s1">):</span>
    <span class="s1">alphas = [</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">10.0</span><span class="s0">, </span><span class="s3">1e3</span><span class="s1">]</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_targets = y_shape[-</span><span class="s3">1</span><span class="s1">] </span><span class="s0">if </span><span class="s1">len(y_shape) == </span><span class="s3">2 </span><span class="s0">else </span><span class="s3">1</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = _make_sparse_offset_regression(</span>
        <span class="s1">n_samples=</span><span class="s3">11</span><span class="s0">,</span>
        <span class="s1">n_features=n_features</span><span class="s0">,</span>
        <span class="s1">n_targets=n_targets</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
        <span class="s1">noise=noise</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">y = y.reshape(y_shape)</span>

    <span class="s1">sample_weight = </span><span class="s3">3 </span><span class="s1">* rng.randn(len(X))</span>
    <span class="s1">sample_weight = (sample_weight - sample_weight.min() + </span><span class="s3">1</span><span class="s1">).astype(int)</span>
    <span class="s1">indices = np.repeat(np.arange(X.shape[</span><span class="s3">0</span><span class="s1">])</span><span class="s0">, </span><span class="s1">sample_weight)</span>
    <span class="s1">sample_weight = sample_weight.astype(float)</span>
    <span class="s1">X_tiled</span><span class="s0">, </span><span class="s1">y_tiled = X[indices]</span><span class="s0">, </span><span class="s1">y[indices]</span>

    <span class="s1">cv = GroupKFold(n_splits=X.shape[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">splits = cv.split(X_tiled</span><span class="s0">, </span><span class="s1">y_tiled</span><span class="s0">, </span><span class="s1">groups=indices)</span>
    <span class="s1">kfold = RidgeCV(</span>
        <span class="s1">alphas=alphas</span><span class="s0">,</span>
        <span class="s1">cv=splits</span><span class="s0">,</span>
        <span class="s1">scoring=</span><span class="s2">&quot;neg_mean_squared_error&quot;</span><span class="s0">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">kfold.fit(X_tiled</span><span class="s0">, </span><span class="s1">y_tiled)</span>

    <span class="s1">ridge_reg = Ridge(alpha=kfold.alpha_</span><span class="s0">, </span><span class="s1">fit_intercept=fit_intercept)</span>
    <span class="s1">splits = cv.split(X_tiled</span><span class="s0">, </span><span class="s1">y_tiled</span><span class="s0">, </span><span class="s1">groups=indices)</span>
    <span class="s1">predictions = cross_val_predict(ridge_reg</span><span class="s0">, </span><span class="s1">X_tiled</span><span class="s0">, </span><span class="s1">y_tiled</span><span class="s0">, </span><span class="s1">cv=splits)</span>
    <span class="s1">kfold_errors = (y_tiled - predictions) ** </span><span class="s3">2</span>
    <span class="s1">kfold_errors = [</span>
        <span class="s1">np.sum(kfold_errors[indices == i]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">np.arange(X.shape[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">]</span>
    <span class="s1">kfold_errors = np.asarray(kfold_errors)</span>

    <span class="s1">X_gcv = X_constructor(X)</span>
    <span class="s1">gcv_ridge = RidgeCV(</span>
        <span class="s1">alphas=alphas</span><span class="s0">,</span>
        <span class="s1">store_cv_values=</span><span class="s0">True,</span>
        <span class="s1">gcv_mode=gcv_mode</span><span class="s0">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">gcv_ridge.fit(X_gcv</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s0">if </span><span class="s1">len(y_shape) == </span><span class="s3">2</span><span class="s1">:</span>
        <span class="s1">gcv_errors = gcv_ridge.cv_values_[:</span><span class="s0">, </span><span class="s1">:</span><span class="s0">, </span><span class="s1">alphas.index(kfold.alpha_)]</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">gcv_errors = gcv_ridge.cv_values_[:</span><span class="s0">, </span><span class="s1">alphas.index(kfold.alpha_)]</span>

    <span class="s0">assert </span><span class="s1">kfold.alpha_ == pytest.approx(gcv_ridge.alpha_)</span>
    <span class="s1">assert_allclose(gcv_errors</span><span class="s0">, </span><span class="s1">kfold_errors</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-3</span><span class="s1">)</span>
    <span class="s1">assert_allclose(gcv_ridge.coef_</span><span class="s0">, </span><span class="s1">kfold.coef_</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-3</span><span class="s1">)</span>
    <span class="s1">assert_allclose(gcv_ridge.intercept_</span><span class="s0">, </span><span class="s1">kfold.intercept_</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-3</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;sparse&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;mode, mode_n_greater_than_p, mode_p_greater_than_n&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s0">None, </span><span class="s2">&quot;svd&quot;</span><span class="s0">, </span><span class="s2">&quot;eigen&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;auto&quot;</span><span class="s0">, </span><span class="s2">&quot;svd&quot;</span><span class="s0">, </span><span class="s2">&quot;eigen&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;eigen&quot;</span><span class="s0">, </span><span class="s2">&quot;eigen&quot;</span><span class="s0">, </span><span class="s2">&quot;eigen&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;svd&quot;</span><span class="s0">, </span><span class="s2">&quot;svd&quot;</span><span class="s0">, </span><span class="s2">&quot;svd&quot;</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_check_gcv_mode_choice(</span>
    <span class="s1">sparse</span><span class="s0">, </span><span class="s1">mode</span><span class="s0">, </span><span class="s1">mode_n_greater_than_p</span><span class="s0">, </span><span class="s1">mode_p_greater_than_n</span>
<span class="s1">):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">_ = make_regression(n_samples=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">sparse:</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
    <span class="s0">assert </span><span class="s1">_check_gcv_mode(X</span><span class="s0">, </span><span class="s1">mode) == mode_n_greater_than_p</span>
    <span class="s0">assert </span><span class="s1">_check_gcv_mode(X.T</span><span class="s0">, </span><span class="s1">mode) == mode_p_greater_than_n</span>


<span class="s0">def </span><span class="s1">_test_ridge_loo(filter_):</span>
    <span class="s5"># test that can work with both dense or sparse matrices</span>
    <span class="s1">n_samples = X_diabetes.shape[</span><span class="s3">0</span><span class="s1">]</span>

    <span class="s1">ret = []</span>

    <span class="s1">fit_intercept = filter_ == DENSE_FILTER</span>
    <span class="s1">ridge_gcv = _RidgeGCV(fit_intercept=fit_intercept)</span>

    <span class="s5"># check best alpha</span>
    <span class="s1">ridge_gcv.fit(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes)</span>
    <span class="s1">alpha_ = ridge_gcv.alpha_</span>
    <span class="s1">ret.append(alpha_)</span>

    <span class="s5"># check that we get same best alpha with custom loss_func</span>
    <span class="s1">f = ignore_warnings</span>
    <span class="s1">scoring = make_scorer(mean_squared_error</span><span class="s0">, </span><span class="s1">greater_is_better=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">ridge_gcv2 = RidgeCV(fit_intercept=</span><span class="s0">False, </span><span class="s1">scoring=scoring)</span>
    <span class="s1">f(ridge_gcv2.fit)(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes)</span>
    <span class="s0">assert </span><span class="s1">ridge_gcv2.alpha_ == pytest.approx(alpha_)</span>

    <span class="s5"># check that we get same best alpha with custom score_func</span>
    <span class="s0">def </span><span class="s1">func(x</span><span class="s0">, </span><span class="s1">y):</span>
        <span class="s0">return </span><span class="s1">-mean_squared_error(x</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">scoring = make_scorer(func)</span>
    <span class="s1">ridge_gcv3 = RidgeCV(fit_intercept=</span><span class="s0">False, </span><span class="s1">scoring=scoring)</span>
    <span class="s1">f(ridge_gcv3.fit)(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes)</span>
    <span class="s0">assert </span><span class="s1">ridge_gcv3.alpha_ == pytest.approx(alpha_)</span>

    <span class="s5"># check that we get same best alpha with a scorer</span>
    <span class="s1">scorer = get_scorer(</span><span class="s2">&quot;neg_mean_squared_error&quot;</span><span class="s1">)</span>
    <span class="s1">ridge_gcv4 = RidgeCV(fit_intercept=</span><span class="s0">False, </span><span class="s1">scoring=scorer)</span>
    <span class="s1">ridge_gcv4.fit(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes)</span>
    <span class="s0">assert </span><span class="s1">ridge_gcv4.alpha_ == pytest.approx(alpha_)</span>

    <span class="s5"># check that we get same best alpha with sample weights</span>
    <span class="s0">if </span><span class="s1">filter_ == DENSE_FILTER:</span>
        <span class="s1">ridge_gcv.fit(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes</span><span class="s0">, </span><span class="s1">sample_weight=np.ones(n_samples))</span>
        <span class="s0">assert </span><span class="s1">ridge_gcv.alpha_ == pytest.approx(alpha_)</span>

    <span class="s5"># simulate several responses</span>
    <span class="s1">Y = np.vstack((y_diabetes</span><span class="s0">, </span><span class="s1">y_diabetes)).T</span>

    <span class="s1">ridge_gcv.fit(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">Y_pred = ridge_gcv.predict(filter_(X_diabetes))</span>
    <span class="s1">ridge_gcv.fit(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes)</span>
    <span class="s1">y_pred = ridge_gcv.predict(filter_(X_diabetes))</span>

    <span class="s1">assert_allclose(np.vstack((y_pred</span><span class="s0">, </span><span class="s1">y_pred)).T</span><span class="s0">, </span><span class="s1">Y_pred</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-5</span><span class="s1">)</span>

    <span class="s0">return </span><span class="s1">ret</span>


<span class="s0">def </span><span class="s1">_test_ridge_cv(filter_):</span>
    <span class="s1">ridge_cv = RidgeCV()</span>
    <span class="s1">ridge_cv.fit(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes)</span>
    <span class="s1">ridge_cv.predict(filter_(X_diabetes))</span>

    <span class="s0">assert </span><span class="s1">len(ridge_cv.coef_.shape) == </span><span class="s3">1</span>
    <span class="s0">assert </span><span class="s1">type(ridge_cv.intercept_) == np.float64</span>

    <span class="s1">cv = KFold(</span><span class="s3">5</span><span class="s1">)</span>
    <span class="s1">ridge_cv.set_params(cv=cv)</span>
    <span class="s1">ridge_cv.fit(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes)</span>
    <span class="s1">ridge_cv.predict(filter_(X_diabetes))</span>

    <span class="s0">assert </span><span class="s1">len(ridge_cv.coef_.shape) == </span><span class="s3">1</span>
    <span class="s0">assert </span><span class="s1">type(ridge_cv.intercept_) == np.float64</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;ridge, make_dataset&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(RidgeCV(store_cv_values=</span><span class="s0">False</span><span class="s1">)</span><span class="s0">, </span><span class="s1">make_regression)</span><span class="s0">,</span>
        <span class="s1">(RidgeClassifierCV(store_cv_values=</span><span class="s0">False</span><span class="s1">)</span><span class="s0">, </span><span class="s1">make_classification)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_ridge_gcv_cv_values_not_stored(ridge</span><span class="s0">, </span><span class="s1">make_dataset):</span>
    <span class="s5"># Check that `cv_values_` is not stored when store_cv_values is False</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_dataset(n_samples=</span><span class="s3">6</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert not </span><span class="s1">hasattr(ridge</span><span class="s0">, </span><span class="s2">&quot;cv_values_&quot;</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;ridge, make_dataset&quot;</span><span class="s0">,</span>
    <span class="s1">[(RidgeCV()</span><span class="s0">, </span><span class="s1">make_regression)</span><span class="s0">, </span><span class="s1">(RidgeClassifierCV()</span><span class="s0">, </span><span class="s1">make_classification)]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;cv&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s3">3</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_ridge_best_score(ridge</span><span class="s0">, </span><span class="s1">make_dataset</span><span class="s0">, </span><span class="s1">cv):</span>
    <span class="s5"># check that the best_score_ is store</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_dataset(n_samples=</span><span class="s3">6</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">ridge.set_params(store_cv_values=</span><span class="s0">False, </span><span class="s1">cv=cv)</span>
    <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">hasattr(ridge</span><span class="s0">, </span><span class="s2">&quot;best_score_&quot;</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">isinstance(ridge.best_score_</span><span class="s0">, </span><span class="s1">float)</span>


<span class="s0">def </span><span class="s1">test_ridge_cv_individual_penalties():</span>
    <span class="s5"># Tests the ridge_cv object optimizing individual penalties for each target</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>

    <span class="s5"># Create random dataset with multiple targets. Each target should have</span>
    <span class="s5"># a different optimal alpha.</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features</span><span class="s0">, </span><span class="s1">n_targets = </span><span class="s3">20</span><span class="s0">, </span><span class="s3">5</span><span class="s0">, </span><span class="s3">3</span>
    <span class="s1">y = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_targets)</span>
    <span class="s1">X = (</span>
        <span class="s1">np.dot(y[:</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">np.ones((</span><span class="s3">1</span><span class="s0">, </span><span class="s1">n_features)))</span>
        <span class="s1">+ np.dot(y[:</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s3">0.05 </span><span class="s1">* np.ones((</span><span class="s3">1</span><span class="s0">, </span><span class="s1">n_features)))</span>
        <span class="s1">+ np.dot(y[:</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s1">]]</span><span class="s0">, </span><span class="s3">0.001 </span><span class="s1">* np.ones((</span><span class="s3">1</span><span class="s0">, </span><span class="s1">n_features)))</span>
        <span class="s1">+ rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">)</span>

    <span class="s1">alphas = (</span><span class="s3">1</span><span class="s0">, </span><span class="s3">100</span><span class="s0">, </span><span class="s3">1000</span><span class="s1">)</span>

    <span class="s5"># Find optimal alpha for each target</span>
    <span class="s1">optimal_alphas = [RidgeCV(alphas=alphas).fit(X</span><span class="s0">, </span><span class="s1">target).alpha_ </span><span class="s0">for </span><span class="s1">target </span><span class="s0">in </span><span class="s1">y.T]</span>

    <span class="s5"># Find optimal alphas for all targets simultaneously</span>
    <span class="s1">ridge_cv = RidgeCV(alphas=alphas</span><span class="s0">, </span><span class="s1">alpha_per_target=</span><span class="s0">True</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(optimal_alphas</span><span class="s0">, </span><span class="s1">ridge_cv.alpha_)</span>

    <span class="s5"># The resulting regression weights should incorporate the different</span>
    <span class="s5"># alpha values.</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">Ridge(alpha=ridge_cv.alpha_).fit(X</span><span class="s0">, </span><span class="s1">y).coef_</span><span class="s0">, </span><span class="s1">ridge_cv.coef_</span>
    <span class="s1">)</span>

    <span class="s5"># Test shape of alpha_ and cv_values_</span>
    <span class="s1">ridge_cv = RidgeCV(alphas=alphas</span><span class="s0">, </span><span class="s1">alpha_per_target=</span><span class="s0">True, </span><span class="s1">store_cv_values=</span><span class="s0">True</span><span class="s1">).fit(</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">ridge_cv.alpha_.shape == (n_targets</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">ridge_cv.best_score_.shape == (n_targets</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">ridge_cv.cv_values_.shape == (n_samples</span><span class="s0">, </span><span class="s1">len(alphas)</span><span class="s0">, </span><span class="s1">n_targets)</span>

    <span class="s5"># Test edge case of there being only one alpha value</span>
    <span class="s1">ridge_cv = RidgeCV(alphas=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">alpha_per_target=</span><span class="s0">True, </span><span class="s1">store_cv_values=</span><span class="s0">True</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">ridge_cv.alpha_.shape == (n_targets</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">ridge_cv.best_score_.shape == (n_targets</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">ridge_cv.cv_values_.shape == (n_samples</span><span class="s0">, </span><span class="s1">n_targets</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span>

    <span class="s5"># Test edge case of there being only one target</span>
    <span class="s1">ridge_cv = RidgeCV(alphas=alphas</span><span class="s0">, </span><span class="s1">alpha_per_target=</span><span class="s0">True, </span><span class="s1">store_cv_values=</span><span class="s0">True</span><span class="s1">).fit(</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y[:</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">np.isscalar(ridge_cv.alpha_)</span>
    <span class="s0">assert </span><span class="s1">np.isscalar(ridge_cv.best_score_)</span>
    <span class="s0">assert </span><span class="s1">ridge_cv.cv_values_.shape == (n_samples</span><span class="s0">, </span><span class="s1">len(alphas))</span>

    <span class="s5"># Try with a custom scoring function</span>
    <span class="s1">ridge_cv = RidgeCV(alphas=alphas</span><span class="s0">, </span><span class="s1">alpha_per_target=</span><span class="s0">True, </span><span class="s1">scoring=</span><span class="s2">&quot;r2&quot;</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(optimal_alphas</span><span class="s0">, </span><span class="s1">ridge_cv.alpha_)</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">Ridge(alpha=ridge_cv.alpha_).fit(X</span><span class="s0">, </span><span class="s1">y).coef_</span><span class="s0">, </span><span class="s1">ridge_cv.coef_</span>
    <span class="s1">)</span>

    <span class="s5"># Using a custom CV object should throw an error in combination with</span>
    <span class="s5"># alpha_per_target=True</span>
    <span class="s1">ridge_cv = RidgeCV(alphas=alphas</span><span class="s0">, </span><span class="s1">cv=LeaveOneOut()</span><span class="s0">, </span><span class="s1">alpha_per_target=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">msg = </span><span class="s2">&quot;cv!=None and alpha_per_target=True are incompatible&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">ridge_cv.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">ridge_cv = RidgeCV(alphas=alphas</span><span class="s0">, </span><span class="s1">cv=</span><span class="s3">6</span><span class="s0">, </span><span class="s1">alpha_per_target=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">ridge_cv.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">_test_ridge_diabetes(filter_):</span>
    <span class="s1">ridge = Ridge(fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">ridge.fit(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes)</span>
    <span class="s0">return </span><span class="s1">np.round(ridge.score(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes)</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">_test_multi_ridge_diabetes(filter_):</span>
    <span class="s5"># simulate several responses</span>
    <span class="s1">Y = np.vstack((y_diabetes</span><span class="s0">, </span><span class="s1">y_diabetes)).T</span>
    <span class="s1">n_features = X_diabetes.shape[</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">ridge = Ridge(fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">ridge.fit(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s0">assert </span><span class="s1">ridge.coef_.shape == (</span><span class="s3">2</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">Y_pred = ridge.predict(filter_(X_diabetes))</span>
    <span class="s1">ridge.fit(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes)</span>
    <span class="s1">y_pred = ridge.predict(filter_(X_diabetes))</span>
    <span class="s1">assert_array_almost_equal(np.vstack((y_pred</span><span class="s0">, </span><span class="s1">y_pred)).T</span><span class="s0">, </span><span class="s1">Y_pred</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">3</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">_test_ridge_classifiers(filter_):</span>
    <span class="s1">n_classes = np.unique(y_iris).shape[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">n_features = X_iris.shape[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s0">for </span><span class="s1">reg </span><span class="s0">in </span><span class="s1">(RidgeClassifier()</span><span class="s0">, </span><span class="s1">RidgeClassifierCV()):</span>
        <span class="s1">reg.fit(filter_(X_iris)</span><span class="s0">, </span><span class="s1">y_iris)</span>
        <span class="s0">assert </span><span class="s1">reg.coef_.shape == (n_classes</span><span class="s0">, </span><span class="s1">n_features)</span>
        <span class="s1">y_pred = reg.predict(filter_(X_iris))</span>
        <span class="s0">assert </span><span class="s1">np.mean(y_iris == y_pred) &gt; </span><span class="s3">0.79</span>

    <span class="s1">cv = KFold(</span><span class="s3">5</span><span class="s1">)</span>
    <span class="s1">reg = RidgeClassifierCV(cv=cv)</span>
    <span class="s1">reg.fit(filter_(X_iris)</span><span class="s0">, </span><span class="s1">y_iris)</span>
    <span class="s1">y_pred = reg.predict(filter_(X_iris))</span>
    <span class="s0">assert </span><span class="s1">np.mean(y_iris == y_pred) &gt;= </span><span class="s3">0.8</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;scoring&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s2">&quot;accuracy&quot;</span><span class="s0">, </span><span class="s1">_accuracy_callable])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;cv&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s1">KFold(</span><span class="s3">5</span><span class="s1">)])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;filter_&quot;</span><span class="s0">, </span><span class="s1">[DENSE_FILTER</span><span class="s0">, </span><span class="s1">SPARSE_FILTER])</span>
<span class="s0">def </span><span class="s1">test_ridge_classifier_with_scoring(filter_</span><span class="s0">, </span><span class="s1">scoring</span><span class="s0">, </span><span class="s1">cv):</span>
    <span class="s5"># non-regression test for #14672</span>
    <span class="s5"># check that RidgeClassifierCV works with all sort of scoring and</span>
    <span class="s5"># cross-validation</span>
    <span class="s1">scoring_ = make_scorer(scoring) </span><span class="s0">if </span><span class="s1">callable(scoring) </span><span class="s0">else </span><span class="s1">scoring</span>
    <span class="s1">clf = RidgeClassifierCV(scoring=scoring_</span><span class="s0">, </span><span class="s1">cv=cv)</span>
    <span class="s5"># Smoke test to check that fit/predict does not raise error</span>
    <span class="s1">clf.fit(filter_(X_iris)</span><span class="s0">, </span><span class="s1">y_iris).predict(filter_(X_iris))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;cv&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s1">KFold(</span><span class="s3">5</span><span class="s1">)])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;filter_&quot;</span><span class="s0">, </span><span class="s1">[DENSE_FILTER</span><span class="s0">, </span><span class="s1">SPARSE_FILTER])</span>
<span class="s0">def </span><span class="s1">test_ridge_regression_custom_scoring(filter_</span><span class="s0">, </span><span class="s1">cv):</span>
    <span class="s5"># check that custom scoring is working as expected</span>
    <span class="s5"># check the tie breaking strategy (keep the first alpha tried)</span>

    <span class="s0">def </span><span class="s1">_dummy_score(y_test</span><span class="s0">, </span><span class="s1">y_pred):</span>
        <span class="s0">return </span><span class="s3">0.42</span>

    <span class="s1">alphas = np.logspace(-</span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">num=</span><span class="s3">5</span><span class="s1">)</span>
    <span class="s1">clf = RidgeClassifierCV(alphas=alphas</span><span class="s0">, </span><span class="s1">scoring=make_scorer(_dummy_score)</span><span class="s0">, </span><span class="s1">cv=cv)</span>
    <span class="s1">clf.fit(filter_(X_iris)</span><span class="s0">, </span><span class="s1">y_iris)</span>
    <span class="s0">assert </span><span class="s1">clf.best_score_ == pytest.approx(</span><span class="s3">0.42</span><span class="s1">)</span>
    <span class="s5"># In case of tie score, the first alphas will be kept</span>
    <span class="s0">assert </span><span class="s1">clf.alpha_ == pytest.approx(alphas[</span><span class="s3">0</span><span class="s1">])</span>


<span class="s0">def </span><span class="s1">_test_tolerance(filter_):</span>
    <span class="s1">ridge = Ridge(tol=</span><span class="s3">1e-5</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">ridge.fit(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes)</span>
    <span class="s1">score = ridge.score(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes)</span>

    <span class="s1">ridge2 = Ridge(tol=</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">ridge2.fit(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes)</span>
    <span class="s1">score2 = ridge2.score(filter_(X_diabetes)</span><span class="s0">, </span><span class="s1">y_diabetes)</span>

    <span class="s0">assert </span><span class="s1">score &gt;= score2</span>


<span class="s0">def </span><span class="s1">check_dense_sparse(test_func):</span>
    <span class="s5"># test dense matrix</span>
    <span class="s1">ret_dense = test_func(DENSE_FILTER)</span>
    <span class="s5"># test sparse matrix</span>
    <span class="s1">ret_sparse = test_func(SPARSE_FILTER)</span>
    <span class="s5"># test that the outputs are the same</span>
    <span class="s0">if </span><span class="s1">ret_dense </span><span class="s0">is not None and </span><span class="s1">ret_sparse </span><span class="s0">is not None</span><span class="s1">:</span>
        <span class="s1">assert_array_almost_equal(ret_dense</span><span class="s0">, </span><span class="s1">ret_sparse</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">3</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;test_func&quot;</span><span class="s0">,</span>
    <span class="s1">(</span>
        <span class="s1">_test_ridge_loo</span><span class="s0">,</span>
        <span class="s1">_test_ridge_cv</span><span class="s0">,</span>
        <span class="s1">_test_ridge_diabetes</span><span class="s0">,</span>
        <span class="s1">_test_multi_ridge_diabetes</span><span class="s0">,</span>
        <span class="s1">_test_ridge_classifiers</span><span class="s0">,</span>
        <span class="s1">_test_tolerance</span><span class="s0">,</span>
    <span class="s1">)</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_dense_sparse(test_func):</span>
    <span class="s1">check_dense_sparse(test_func)</span>


<span class="s0">def </span><span class="s1">test_class_weights():</span>
    <span class="s5"># Test class weights.</span>
    <span class="s1">X = np.array([[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">0.8</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]])</span>
    <span class="s1">y = [</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">reg = RidgeClassifier(class_weight=</span><span class="s0">None</span><span class="s1">)</span>
    <span class="s1">reg.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(reg.predict([[</span><span class="s3">0.2</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s3">1</span><span class="s1">]))</span>

    <span class="s5"># we give a small weights to class 1</span>
    <span class="s1">reg = RidgeClassifier(class_weight={</span><span class="s3">1</span><span class="s1">: </span><span class="s3">0.001</span><span class="s1">})</span>
    <span class="s1">reg.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s5"># now the hyperplane should rotate clock-wise and</span>
    <span class="s5"># the prediction on this point should shift</span>
    <span class="s1">assert_array_equal(reg.predict([[</span><span class="s3">0.2</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">1</span><span class="s1">]))</span>

    <span class="s5"># check if class_weight = 'balanced' can handle negative labels.</span>
    <span class="s1">reg = RidgeClassifier(class_weight=</span><span class="s2">&quot;balanced&quot;</span><span class="s1">)</span>
    <span class="s1">reg.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(reg.predict([[</span><span class="s3">0.2</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s3">1</span><span class="s1">]))</span>

    <span class="s5"># class_weight = 'balanced', and class_weight = None should return</span>
    <span class="s5"># same values when y has equal number of all labels</span>
    <span class="s1">X = np.array([[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">0.8</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">]])</span>
    <span class="s1">y = [</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">reg = RidgeClassifier(class_weight=</span><span class="s0">None</span><span class="s1">)</span>
    <span class="s1">reg.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">rega = RidgeClassifier(class_weight=</span><span class="s2">&quot;balanced&quot;</span><span class="s1">)</span>
    <span class="s1">rega.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">len(rega.classes_) == </span><span class="s3">2</span>
    <span class="s1">assert_array_almost_equal(reg.coef_</span><span class="s0">, </span><span class="s1">rega.coef_)</span>
    <span class="s1">assert_array_almost_equal(reg.intercept_</span><span class="s0">, </span><span class="s1">rega.intercept_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;reg&quot;</span><span class="s0">, </span><span class="s1">(RidgeClassifier</span><span class="s0">, </span><span class="s1">RidgeClassifierCV))</span>
<span class="s0">def </span><span class="s1">test_class_weight_vs_sample_weight(reg):</span>
    <span class="s4">&quot;&quot;&quot;Check class_weights resemble sample_weights behavior.&quot;&quot;&quot;</span>

    <span class="s5"># Iris is balanced, so no effect expected for using 'balanced' weights</span>
    <span class="s1">reg1 = reg()</span>
    <span class="s1">reg1.fit(iris.data</span><span class="s0">, </span><span class="s1">iris.target)</span>
    <span class="s1">reg2 = reg(class_weight=</span><span class="s2">&quot;balanced&quot;</span><span class="s1">)</span>
    <span class="s1">reg2.fit(iris.data</span><span class="s0">, </span><span class="s1">iris.target)</span>
    <span class="s1">assert_almost_equal(reg1.coef_</span><span class="s0">, </span><span class="s1">reg2.coef_)</span>

    <span class="s5"># Inflate importance of class 1, check against user-defined weights</span>
    <span class="s1">sample_weight = np.ones(iris.target.shape)</span>
    <span class="s1">sample_weight[iris.target == </span><span class="s3">1</span><span class="s1">] *= </span><span class="s3">100</span>
    <span class="s1">class_weight = {</span><span class="s3">0</span><span class="s1">: </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">: </span><span class="s3">100.0</span><span class="s0">, </span><span class="s3">2</span><span class="s1">: </span><span class="s3">1.0</span><span class="s1">}</span>
    <span class="s1">reg1 = reg()</span>
    <span class="s1">reg1.fit(iris.data</span><span class="s0">, </span><span class="s1">iris.target</span><span class="s0">, </span><span class="s1">sample_weight)</span>
    <span class="s1">reg2 = reg(class_weight=class_weight)</span>
    <span class="s1">reg2.fit(iris.data</span><span class="s0">, </span><span class="s1">iris.target)</span>
    <span class="s1">assert_almost_equal(reg1.coef_</span><span class="s0">, </span><span class="s1">reg2.coef_)</span>

    <span class="s5"># Check that sample_weight and class_weight are multiplicative</span>
    <span class="s1">reg1 = reg()</span>
    <span class="s1">reg1.fit(iris.data</span><span class="s0">, </span><span class="s1">iris.target</span><span class="s0">, </span><span class="s1">sample_weight**</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">reg2 = reg(class_weight=class_weight)</span>
    <span class="s1">reg2.fit(iris.data</span><span class="s0">, </span><span class="s1">iris.target</span><span class="s0">, </span><span class="s1">sample_weight)</span>
    <span class="s1">assert_almost_equal(reg1.coef_</span><span class="s0">, </span><span class="s1">reg2.coef_)</span>


<span class="s0">def </span><span class="s1">test_class_weights_cv():</span>
    <span class="s5"># Test class weights for cross validated ridge classifier.</span>
    <span class="s1">X = np.array([[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">0.8</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]])</span>
    <span class="s1">y = [</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">reg = RidgeClassifierCV(class_weight=</span><span class="s0">None, </span><span class="s1">alphas=[</span><span class="s3">0.01</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">reg.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s5"># we give a small weights to class 1</span>
    <span class="s1">reg = RidgeClassifierCV(class_weight={</span><span class="s3">1</span><span class="s1">: </span><span class="s3">0.001</span><span class="s1">}</span><span class="s0">, </span><span class="s1">alphas=[</span><span class="s3">0.01</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">10</span><span class="s1">])</span>
    <span class="s1">reg.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">assert_array_equal(reg.predict([[-</span><span class="s3">0.2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">1</span><span class="s1">]))</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;scoring&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s2">&quot;neg_mean_squared_error&quot;</span><span class="s0">, </span><span class="s1">_mean_squared_error_callable]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_ridgecv_store_cv_values(scoring):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>

    <span class="s1">n_samples = </span><span class="s3">8</span>
    <span class="s1">n_features = </span><span class="s3">5</span>
    <span class="s1">x = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">alphas = [</span><span class="s3">1e-1</span><span class="s0">, </span><span class="s3">1e0</span><span class="s0">, </span><span class="s3">1e1</span><span class="s1">]</span>
    <span class="s1">n_alphas = len(alphas)</span>

    <span class="s1">scoring_ = make_scorer(scoring) </span><span class="s0">if </span><span class="s1">callable(scoring) </span><span class="s0">else </span><span class="s1">scoring</span>

    <span class="s1">r = RidgeCV(alphas=alphas</span><span class="s0">, </span><span class="s1">cv=</span><span class="s0">None, </span><span class="s1">store_cv_values=</span><span class="s0">True, </span><span class="s1">scoring=scoring_)</span>

    <span class="s5"># with len(y.shape) == 1</span>
    <span class="s1">y = rng.randn(n_samples)</span>
    <span class="s1">r.fit(x</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">r.cv_values_.shape == (n_samples</span><span class="s0">, </span><span class="s1">n_alphas)</span>

    <span class="s5"># with len(y.shape) == 2</span>
    <span class="s1">n_targets = </span><span class="s3">3</span>
    <span class="s1">y = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_targets)</span>
    <span class="s1">r.fit(x</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">r.cv_values_.shape == (n_samples</span><span class="s0">, </span><span class="s1">n_targets</span><span class="s0">, </span><span class="s1">n_alphas)</span>

    <span class="s1">r = RidgeCV(cv=</span><span class="s3">3</span><span class="s0">, </span><span class="s1">store_cv_values=</span><span class="s0">True, </span><span class="s1">scoring=scoring)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;cv!=None and store_cv_values&quot;</span><span class="s1">):</span>
        <span class="s1">r.fit(x</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;scoring&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s2">&quot;accuracy&quot;</span><span class="s0">, </span><span class="s1">_accuracy_callable])</span>
<span class="s0">def </span><span class="s1">test_ridge_classifier_cv_store_cv_values(scoring):</span>
    <span class="s1">x = np.array([[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">0.8</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">])</span>

    <span class="s1">n_samples = x.shape[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">alphas = [</span><span class="s3">1e-1</span><span class="s0">, </span><span class="s3">1e0</span><span class="s0">, </span><span class="s3">1e1</span><span class="s1">]</span>
    <span class="s1">n_alphas = len(alphas)</span>

    <span class="s1">scoring_ = make_scorer(scoring) </span><span class="s0">if </span><span class="s1">callable(scoring) </span><span class="s0">else </span><span class="s1">scoring</span>

    <span class="s1">r = RidgeClassifierCV(</span>
        <span class="s1">alphas=alphas</span><span class="s0">, </span><span class="s1">cv=</span><span class="s0">None, </span><span class="s1">store_cv_values=</span><span class="s0">True, </span><span class="s1">scoring=scoring_</span>
    <span class="s1">)</span>

    <span class="s5"># with len(y.shape) == 1</span>
    <span class="s1">n_targets = </span><span class="s3">1</span>
    <span class="s1">r.fit(x</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">r.cv_values_.shape == (n_samples</span><span class="s0">, </span><span class="s1">n_targets</span><span class="s0">, </span><span class="s1">n_alphas)</span>

    <span class="s5"># with len(y.shape) == 2</span>
    <span class="s1">y = np.array(</span>
        <span class="s1">[[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]]</span>
    <span class="s1">).transpose()</span>
    <span class="s1">n_targets = y.shape[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">r.fit(x</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">r.cv_values_.shape == (n_samples</span><span class="s0">, </span><span class="s1">n_targets</span><span class="s0">, </span><span class="s1">n_alphas)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s1">[RidgeCV</span><span class="s0">, </span><span class="s1">RidgeClassifierCV])</span>
<span class="s0">def </span><span class="s1">test_ridgecv_alphas_conversion(Estimator):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">alphas = (</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">10.0</span><span class="s1">)</span>

    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s3">5</span><span class="s0">, </span><span class="s3">5</span>
    <span class="s0">if </span><span class="s1">Estimator </span><span class="s0">is </span><span class="s1">RidgeCV:</span>
        <span class="s1">y = rng.randn(n_samples)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y = rng.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">n_samples)</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>

    <span class="s1">ridge_est = Estimator(alphas=alphas)</span>
    <span class="s0">assert </span><span class="s1">(</span>
        <span class="s1">ridge_est.alphas </span><span class="s0">is </span><span class="s1">alphas</span>
    <span class="s1">)</span><span class="s0">, </span><span class="s2">f&quot;`alphas` was mutated in `</span><span class="s0">{</span><span class="s1">Estimator.__name__</span><span class="s0">}</span><span class="s2">.__init__`&quot;</span>

    <span class="s1">ridge_est.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(ridge_est.alphas</span><span class="s0">, </span><span class="s1">np.asarray(alphas))</span>


<span class="s0">def </span><span class="s1">test_ridgecv_sample_weight():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">alphas = (</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">10.0</span><span class="s1">)</span>

    <span class="s5"># There are different algorithms for n_samples &gt; n_features</span>
    <span class="s5"># and the opposite, so test them both.</span>
    <span class="s0">for </span><span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features </span><span class="s0">in </span><span class="s1">((</span><span class="s3">6</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">5</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)):</span>
        <span class="s1">y = rng.randn(n_samples)</span>
        <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
        <span class="s1">sample_weight = </span><span class="s3">1.0 </span><span class="s1">+ rng.rand(n_samples)</span>

        <span class="s1">cv = KFold(</span><span class="s3">5</span><span class="s1">)</span>
        <span class="s1">ridgecv = RidgeCV(alphas=alphas</span><span class="s0">, </span><span class="s1">cv=cv)</span>
        <span class="s1">ridgecv.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>

        <span class="s5"># Check using GridSearchCV directly</span>
        <span class="s1">parameters = {</span><span class="s2">&quot;alpha&quot;</span><span class="s1">: alphas}</span>
        <span class="s1">gs = GridSearchCV(Ridge()</span><span class="s0">, </span><span class="s1">parameters</span><span class="s0">, </span><span class="s1">cv=cv)</span>
        <span class="s1">gs.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>

        <span class="s0">assert </span><span class="s1">ridgecv.alpha_ == gs.best_estimator_.alpha</span>
        <span class="s1">assert_array_almost_equal(ridgecv.coef_</span><span class="s0">, </span><span class="s1">gs.best_estimator_.coef_)</span>


<span class="s0">def </span><span class="s1">test_raises_value_error_if_sample_weights_greater_than_1d():</span>
    <span class="s5"># Sample weights must be either scalar or 1D</span>

    <span class="s1">n_sampless = [</span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s1">]</span>
    <span class="s1">n_featuress = [</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features </span><span class="s0">in </span><span class="s1">zip(n_sampless</span><span class="s0">, </span><span class="s1">n_featuress):</span>
        <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
        <span class="s1">y = rng.randn(n_samples)</span>
        <span class="s1">sample_weights_OK = rng.randn(n_samples) ** </span><span class="s3">2 </span><span class="s1">+ </span><span class="s3">1</span>
        <span class="s1">sample_weights_OK_1 = </span><span class="s3">1.0</span>
        <span class="s1">sample_weights_OK_2 = </span><span class="s3">2.0</span>
        <span class="s1">sample_weights_not_OK = sample_weights_OK[:</span><span class="s0">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">sample_weights_not_OK_2 = sample_weights_OK[np.newaxis</span><span class="s0">, </span><span class="s1">:]</span>

        <span class="s1">ridge = Ridge(alpha=</span><span class="s3">1</span><span class="s1">)</span>

        <span class="s5"># make sure the &quot;OK&quot; sample weights actually work</span>
        <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weights_OK)</span>
        <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weights_OK_1)</span>
        <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weights_OK_2)</span>

        <span class="s0">def </span><span class="s1">fit_ridge_not_ok():</span>
            <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weights_not_OK)</span>

        <span class="s0">def </span><span class="s1">fit_ridge_not_ok_2():</span>
            <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weights_not_OK_2)</span>

        <span class="s1">err_msg = </span><span class="s2">&quot;Sample weights must be 1D array or scalar&quot;</span>
        <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
            <span class="s1">fit_ridge_not_ok()</span>

        <span class="s1">err_msg = </span><span class="s2">&quot;Sample weights must be 1D array or scalar&quot;</span>
        <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
            <span class="s1">fit_ridge_not_ok_2()</span>


<span class="s0">def </span><span class="s1">test_sparse_design_with_sample_weights():</span>
    <span class="s5"># Sample weights must work with sparse matrices</span>

    <span class="s1">n_sampless = [</span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s1">]</span>
    <span class="s1">n_featuress = [</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>

    <span class="s1">sparse_matrix_converters = [</span>
        <span class="s1">sp.coo_matrix</span><span class="s0">,</span>
        <span class="s1">sp.csr_matrix</span><span class="s0">,</span>
        <span class="s1">sp.csc_matrix</span><span class="s0">,</span>
        <span class="s1">sp.lil_matrix</span><span class="s0">,</span>
        <span class="s1">sp.dok_matrix</span><span class="s0">,</span>
    <span class="s1">]</span>

    <span class="s1">sparse_ridge = Ridge(alpha=</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">dense_ridge = Ridge(alpha=</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features </span><span class="s0">in </span><span class="s1">zip(n_sampless</span><span class="s0">, </span><span class="s1">n_featuress):</span>
        <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
        <span class="s1">y = rng.randn(n_samples)</span>
        <span class="s1">sample_weights = rng.randn(n_samples) ** </span><span class="s3">2 </span><span class="s1">+ </span><span class="s3">1</span>
        <span class="s0">for </span><span class="s1">sparse_converter </span><span class="s0">in </span><span class="s1">sparse_matrix_converters:</span>
            <span class="s1">X_sparse = sparse_converter(X)</span>
            <span class="s1">sparse_ridge.fit(X_sparse</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weights)</span>
            <span class="s1">dense_ridge.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weights)</span>

            <span class="s1">assert_array_almost_equal(sparse_ridge.coef_</span><span class="s0">, </span><span class="s1">dense_ridge.coef_</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">6</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_ridgecv_int_alphas():</span>
    <span class="s1">X = np.array([[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">0.8</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]])</span>
    <span class="s1">y = [</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s5"># Integers</span>
    <span class="s1">ridge = RidgeCV(alphas=(</span><span class="s3">1</span><span class="s0">, </span><span class="s3">10</span><span class="s0">, </span><span class="s3">100</span><span class="s1">))</span>
    <span class="s1">ridge.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s1">[RidgeCV</span><span class="s0">, </span><span class="s1">RidgeClassifierCV])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;params, err_type, err_msg&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">({</span><span class="s2">&quot;alphas&quot;</span><span class="s1">: (</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">100</span><span class="s1">)}</span><span class="s0">, </span><span class="s1">ValueError</span><span class="s0">, </span><span class="s2">r&quot;alphas\[1\] == -1, must be &gt; 0.0&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s2">&quot;alphas&quot;</span><span class="s1">: (-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">10.0</span><span class="s1">)}</span><span class="s0">,</span>
            <span class="s1">ValueError</span><span class="s0">,</span>
            <span class="s2">r&quot;alphas\[0\] == -0.1, must be &gt; 0.0&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s2">&quot;alphas&quot;</span><span class="s1">: (</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s2">&quot;1&quot;</span><span class="s1">)}</span><span class="s0">,</span>
            <span class="s1">TypeError</span><span class="s0">,</span>
            <span class="s2">r&quot;alphas\[2\] must be an instance of float, not str&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_ridgecv_alphas_validation(Estimator</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">err_type</span><span class="s0">, </span><span class="s1">err_msg):</span>
    <span class="s4">&quot;&quot;&quot;Check the `alphas` validation in RidgeCV and RidgeClassifierCV.&quot;&quot;&quot;</span>

    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s3">5</span><span class="s0">, </span><span class="s3">5</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">y = rng.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">n_samples)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(err_type</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">Estimator(**params).fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s1">[RidgeCV</span><span class="s0">, </span><span class="s1">RidgeClassifierCV])</span>
<span class="s0">def </span><span class="s1">test_ridgecv_alphas_scalar(Estimator):</span>
    <span class="s4">&quot;&quot;&quot;Check the case when `alphas` is a scalar. 
    This case was supported in the past when `alphas` where converted 
    into array in `__init__`. 
    We add this test to ensure backward compatibility. 
    &quot;&quot;&quot;</span>

    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s3">5</span><span class="s0">, </span><span class="s3">5</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s0">if </span><span class="s1">Estimator </span><span class="s0">is </span><span class="s1">RidgeCV:</span>
        <span class="s1">y = rng.randn(n_samples)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y = rng.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">n_samples)</span>

    <span class="s1">Estimator(alphas=</span><span class="s3">1</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_raises_value_error_if_solver_not_supported():</span>
    <span class="s5"># Tests whether a ValueError is raised if a non-identified solver</span>
    <span class="s5"># is passed to ridge_regression</span>

    <span class="s1">wrong_solver = </span><span class="s2">&quot;This is not a solver (MagritteSolveCV QuantumBitcoin)&quot;</span>

    <span class="s1">exception = ValueError</span>
    <span class="s1">message = (</span>
        <span class="s2">&quot;Known solvers are 'sparse_cg', 'cholesky', 'svd'&quot;</span>
        <span class="s2">&quot; 'lsqr', 'sag' or 'saga'. Got %s.&quot; </span><span class="s1">% wrong_solver</span>
    <span class="s1">)</span>

    <span class="s0">def </span><span class="s1">func():</span>
        <span class="s1">X = np.eye(</span><span class="s3">3</span><span class="s1">)</span>
        <span class="s1">y = np.ones(</span><span class="s3">3</span><span class="s1">)</span>
        <span class="s1">ridge_regression(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">solver=wrong_solver)</span>

        <span class="s0">with </span><span class="s1">pytest.raises(exception</span><span class="s0">, </span><span class="s1">match=message):</span>
            <span class="s1">func()</span>


<span class="s0">def </span><span class="s1">test_sparse_cg_max_iter():</span>
    <span class="s1">reg = Ridge(solver=</span><span class="s2">&quot;sparse_cg&quot;</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">reg.fit(X_diabetes</span><span class="s0">, </span><span class="s1">y_diabetes)</span>
    <span class="s0">assert </span><span class="s1">reg.coef_.shape[</span><span class="s3">0</span><span class="s1">] == X_diabetes.shape[</span><span class="s3">1</span><span class="s1">]</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">test_n_iter():</span>
    <span class="s5"># Test that self.n_iter_ is correct.</span>
    <span class="s1">n_targets = </span><span class="s3">2</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = X_diabetes</span><span class="s0">, </span><span class="s1">y_diabetes</span>
    <span class="s1">y_n = np.tile(y</span><span class="s0">, </span><span class="s1">(n_targets</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)).T</span>

    <span class="s0">for </span><span class="s1">max_iter </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">1</span><span class="s0">, </span><span class="s3">4</span><span class="s1">):</span>
        <span class="s0">for </span><span class="s1">solver </span><span class="s0">in </span><span class="s1">(</span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s0">, </span><span class="s2">&quot;lsqr&quot;</span><span class="s1">):</span>
            <span class="s1">reg = Ridge(solver=solver</span><span class="s0">, </span><span class="s1">max_iter=max_iter</span><span class="s0">, </span><span class="s1">tol=</span><span class="s3">1e-12</span><span class="s1">)</span>
            <span class="s1">reg.fit(X</span><span class="s0">, </span><span class="s1">y_n)</span>
            <span class="s1">assert_array_equal(reg.n_iter_</span><span class="s0">, </span><span class="s1">np.tile(max_iter</span><span class="s0">, </span><span class="s1">n_targets))</span>

    <span class="s0">for </span><span class="s1">solver </span><span class="s0">in </span><span class="s1">(</span><span class="s2">&quot;sparse_cg&quot;</span><span class="s0">, </span><span class="s2">&quot;svd&quot;</span><span class="s0">, </span><span class="s2">&quot;cholesky&quot;</span><span class="s1">):</span>
        <span class="s1">reg = Ridge(solver=solver</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">tol=</span><span class="s3">1e-1</span><span class="s1">)</span>
        <span class="s1">reg.fit(X</span><span class="s0">, </span><span class="s1">y_n)</span>
        <span class="s0">assert </span><span class="s1">reg.n_iter_ </span><span class="s0">is None</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s2">&quot;sparse_cg&quot;</span><span class="s0">, </span><span class="s2">&quot;lbfgs&quot;</span><span class="s0">, </span><span class="s2">&quot;auto&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;with_sample_weight&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_ridge_fit_intercept_sparse(solver</span><span class="s0">, </span><span class="s1">with_sample_weight</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s4">&quot;&quot;&quot;Check that ridge finds the same coefs and intercept on dense and sparse input 
    in the presence of sample weights. 
 
    For now only sparse_cg and lbfgs can correctly fit an intercept 
    with sparse X with default tol and max_iter. 
    'sag' is tested separately in test_ridge_fit_intercept_sparse_sag because it 
    requires more iterations and should raise a warning if default max_iter is used. 
    Other solvers raise an exception, as checked in 
    test_ridge_fit_intercept_sparse_error 
    &quot;&quot;&quot;</span>
    <span class="s1">positive = solver == </span><span class="s2">&quot;lbfgs&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = _make_sparse_offset_regression(</span>
        <span class="s1">n_features=</span><span class="s3">20</span><span class="s0">, </span><span class="s1">random_state=global_random_seed</span><span class="s0">, </span><span class="s1">positive=positive</span>
    <span class="s1">)</span>

    <span class="s1">sample_weight = </span><span class="s0">None</span>
    <span class="s0">if </span><span class="s1">with_sample_weight:</span>
        <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
        <span class="s1">sample_weight = </span><span class="s3">1.0 </span><span class="s1">+ rng.uniform(size=X.shape[</span><span class="s3">0</span><span class="s1">])</span>

    <span class="s5"># &quot;auto&quot; should switch to &quot;sparse_cg&quot; when X is sparse</span>
    <span class="s5"># so the reference we use for both (&quot;auto&quot; and &quot;sparse_cg&quot;) is</span>
    <span class="s5"># Ridge(solver=&quot;sparse_cg&quot;), fitted using the dense representation (note</span>
    <span class="s5"># that &quot;sparse_cg&quot; can fit sparse or dense data)</span>
    <span class="s1">dense_solver = </span><span class="s2">&quot;sparse_cg&quot; </span><span class="s0">if </span><span class="s1">solver == </span><span class="s2">&quot;auto&quot; </span><span class="s0">else </span><span class="s1">solver</span>
    <span class="s1">dense_ridge = Ridge(solver=dense_solver</span><span class="s0">, </span><span class="s1">tol=</span><span class="s3">1e-12</span><span class="s0">, </span><span class="s1">positive=positive)</span>
    <span class="s1">sparse_ridge = Ridge(solver=solver</span><span class="s0">, </span><span class="s1">tol=</span><span class="s3">1e-12</span><span class="s0">, </span><span class="s1">positive=positive)</span>

    <span class="s1">dense_ridge.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">sparse_ridge.fit(sp.csr_matrix(X)</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">assert_allclose(dense_ridge.intercept_</span><span class="s0">, </span><span class="s1">sparse_ridge.intercept_)</span>
    <span class="s1">assert_allclose(dense_ridge.coef_</span><span class="s0">, </span><span class="s1">sparse_ridge.coef_</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">5e-7</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;saga&quot;</span><span class="s0">, </span><span class="s2">&quot;svd&quot;</span><span class="s0">, </span><span class="s2">&quot;cholesky&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_ridge_fit_intercept_sparse_error(solver):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = _make_sparse_offset_regression(n_features=</span><span class="s3">20</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X_csr = sp.csr_matrix(X)</span>
    <span class="s1">sparse_ridge = Ridge(solver=solver)</span>
    <span class="s1">err_msg = </span><span class="s2">&quot;solver='{}' does not support&quot;</span><span class="s1">.format(solver)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">sparse_ridge.fit(X_csr</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;with_sample_weight&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_ridge_fit_intercept_sparse_sag(with_sample_weight</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = _make_sparse_offset_regression(</span>
        <span class="s1">n_features=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">n_samples=</span><span class="s3">20</span><span class="s0">, </span><span class="s1">random_state=global_random_seed</span><span class="s0">, </span><span class="s1">X_offset=</span><span class="s3">5.0</span>
    <span class="s1">)</span>
    <span class="s0">if </span><span class="s1">with_sample_weight:</span>
        <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
        <span class="s1">sample_weight = </span><span class="s3">1.0 </span><span class="s1">+ rng.uniform(size=X.shape[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">sample_weight = </span><span class="s0">None</span>
    <span class="s1">X_csr = sp.csr_matrix(X)</span>

    <span class="s1">params = dict(</span>
        <span class="s1">alpha=</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">solver=</span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">True, </span><span class="s1">tol=</span><span class="s3">1e-10</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">100000</span>
    <span class="s1">)</span>
    <span class="s1">dense_ridge = Ridge(**params)</span>
    <span class="s1">sparse_ridge = Ridge(**params)</span>
    <span class="s1">dense_ridge.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s0">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;error&quot;</span><span class="s0">, </span><span class="s1">UserWarning)</span>
        <span class="s1">sparse_ridge.fit(X_csr</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_allclose(dense_ridge.intercept_</span><span class="s0">, </span><span class="s1">sparse_ridge.intercept_</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-4</span><span class="s1">)</span>
    <span class="s1">assert_allclose(dense_ridge.coef_</span><span class="s0">, </span><span class="s1">sparse_ridge.coef_</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-4</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">'&quot;sag&quot; solver requires.*'</span><span class="s1">):</span>
        <span class="s1">Ridge(solver=</span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">True, </span><span class="s1">tol=</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s0">None</span><span class="s1">).fit(X_csr</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;return_intercept&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">False, True</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;sample_weight&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s1">np.ones(</span><span class="s3">1000</span><span class="s1">)])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;arr_type&quot;</span><span class="s0">, </span><span class="s1">[np.array</span><span class="s0">, </span><span class="s1">sp.csr_matrix])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;auto&quot;</span><span class="s0">, </span><span class="s2">&quot;sparse_cg&quot;</span><span class="s0">, </span><span class="s2">&quot;cholesky&quot;</span><span class="s0">, </span><span class="s2">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s0">, </span><span class="s2">&quot;lbfgs&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_ridge_regression_check_arguments_validity(</span>
    <span class="s1">return_intercept</span><span class="s0">, </span><span class="s1">sample_weight</span><span class="s0">, </span><span class="s1">arr_type</span><span class="s0">, </span><span class="s1">solver</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;check if all combinations of arguments give valid estimations&quot;&quot;&quot;</span>

    <span class="s5"># test excludes 'svd' solver because it raises exception for sparse inputs</span>

    <span class="s1">rng = check_random_state(</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">X = rng.rand(</span><span class="s3">1000</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">true_coefs = [</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">0.1</span><span class="s1">]</span>
    <span class="s1">y = np.dot(X</span><span class="s0">, </span><span class="s1">true_coefs)</span>
    <span class="s1">true_intercept = </span><span class="s3">0.0</span>
    <span class="s0">if </span><span class="s1">return_intercept:</span>
        <span class="s1">true_intercept = </span><span class="s3">10000.0</span>
    <span class="s1">y += true_intercept</span>
    <span class="s1">X_testing = arr_type(X)</span>

    <span class="s1">alpha</span><span class="s0">, </span><span class="s1">tol = </span><span class="s3">1e-3</span><span class="s0">, </span><span class="s3">1e-6</span>
    <span class="s1">atol = </span><span class="s3">1e-3 </span><span class="s0">if </span><span class="s1">_IS_32BIT </span><span class="s0">else </span><span class="s3">1e-4</span>

    <span class="s1">positive = solver == </span><span class="s2">&quot;lbfgs&quot;</span>

    <span class="s0">if </span><span class="s1">solver </span><span class="s0">not in </span><span class="s1">[</span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;auto&quot;</span><span class="s1">] </span><span class="s0">and </span><span class="s1">return_intercept:</span>
        <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;In Ridge, only 'sag' solver&quot;</span><span class="s1">):</span>
            <span class="s1">ridge_regression(</span>
                <span class="s1">X_testing</span><span class="s0">,</span>
                <span class="s1">y</span><span class="s0">,</span>
                <span class="s1">alpha=alpha</span><span class="s0">,</span>
                <span class="s1">solver=solver</span><span class="s0">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
                <span class="s1">return_intercept=return_intercept</span><span class="s0">,</span>
                <span class="s1">positive=positive</span><span class="s0">,</span>
                <span class="s1">tol=tol</span><span class="s0">,</span>
            <span class="s1">)</span>
        <span class="s0">return</span>

    <span class="s1">out = ridge_regression(</span>
        <span class="s1">X_testing</span><span class="s0">,</span>
        <span class="s1">y</span><span class="s0">,</span>
        <span class="s1">alpha=alpha</span><span class="s0">,</span>
        <span class="s1">solver=solver</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">positive=positive</span><span class="s0">,</span>
        <span class="s1">return_intercept=return_intercept</span><span class="s0">,</span>
        <span class="s1">tol=tol</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">if </span><span class="s1">return_intercept:</span>
        <span class="s1">coef</span><span class="s0">, </span><span class="s1">intercept = out</span>
        <span class="s1">assert_allclose(coef</span><span class="s0">, </span><span class="s1">true_coefs</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">atol=atol)</span>
        <span class="s1">assert_allclose(intercept</span><span class="s0">, </span><span class="s1">true_intercept</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">atol=atol)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">assert_allclose(out</span><span class="s0">, </span><span class="s1">true_coefs</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">atol=atol)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;svd&quot;</span><span class="s0">, </span><span class="s2">&quot;sparse_cg&quot;</span><span class="s0">, </span><span class="s2">&quot;cholesky&quot;</span><span class="s0">, </span><span class="s2">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s0">, </span><span class="s2">&quot;lbfgs&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_dtype_match(solver):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">alpha = </span><span class="s3">1.0</span>
    <span class="s1">positive = solver == </span><span class="s2">&quot;lbfgs&quot;</span>

    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s3">6</span><span class="s0">, </span><span class="s3">5</span>
    <span class="s1">X_64 = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">y_64 = rng.randn(n_samples)</span>
    <span class="s1">X_32 = X_64.astype(np.float32)</span>
    <span class="s1">y_32 = y_64.astype(np.float32)</span>

    <span class="s1">tol = </span><span class="s3">2 </span><span class="s1">* np.finfo(np.float32).resolution</span>
    <span class="s5"># Check type consistency 32bits</span>
    <span class="s1">ridge_32 = Ridge(</span>
        <span class="s1">alpha=alpha</span><span class="s0">, </span><span class="s1">solver=solver</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">500</span><span class="s0">, </span><span class="s1">tol=tol</span><span class="s0">, </span><span class="s1">positive=positive</span>
    <span class="s1">)</span>
    <span class="s1">ridge_32.fit(X_32</span><span class="s0">, </span><span class="s1">y_32)</span>
    <span class="s1">coef_32 = ridge_32.coef_</span>

    <span class="s5"># Check type consistency 64 bits</span>
    <span class="s1">ridge_64 = Ridge(</span>
        <span class="s1">alpha=alpha</span><span class="s0">, </span><span class="s1">solver=solver</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">500</span><span class="s0">, </span><span class="s1">tol=tol</span><span class="s0">, </span><span class="s1">positive=positive</span>
    <span class="s1">)</span>
    <span class="s1">ridge_64.fit(X_64</span><span class="s0">, </span><span class="s1">y_64)</span>
    <span class="s1">coef_64 = ridge_64.coef_</span>

    <span class="s5"># Do the actual checks at once for easier debug</span>
    <span class="s0">assert </span><span class="s1">coef_32.dtype == X_32.dtype</span>
    <span class="s0">assert </span><span class="s1">coef_64.dtype == X_64.dtype</span>
    <span class="s0">assert </span><span class="s1">ridge_32.predict(X_32).dtype == X_32.dtype</span>
    <span class="s0">assert </span><span class="s1">ridge_64.predict(X_64).dtype == X_64.dtype</span>
    <span class="s1">assert_allclose(ridge_32.coef_</span><span class="s0">, </span><span class="s1">ridge_64.coef_</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-4</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">5e-4</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_dtype_match_cholesky():</span>
    <span class="s5"># Test different alphas in cholesky solver to ensure full coverage.</span>
    <span class="s5"># This test is separated from test_dtype_match for clarity.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">alpha = np.array([</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.5</span><span class="s1">])</span>

    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features</span><span class="s0">, </span><span class="s1">n_target = </span><span class="s3">6</span><span class="s0">, </span><span class="s3">7</span><span class="s0">, </span><span class="s3">2</span>
    <span class="s1">X_64 = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">y_64 = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_target)</span>
    <span class="s1">X_32 = X_64.astype(np.float32)</span>
    <span class="s1">y_32 = y_64.astype(np.float32)</span>

    <span class="s5"># Check type consistency 32bits</span>
    <span class="s1">ridge_32 = Ridge(alpha=alpha</span><span class="s0">, </span><span class="s1">solver=</span><span class="s2">&quot;cholesky&quot;</span><span class="s1">)</span>
    <span class="s1">ridge_32.fit(X_32</span><span class="s0">, </span><span class="s1">y_32)</span>
    <span class="s1">coef_32 = ridge_32.coef_</span>

    <span class="s5"># Check type consistency 64 bits</span>
    <span class="s1">ridge_64 = Ridge(alpha=alpha</span><span class="s0">, </span><span class="s1">solver=</span><span class="s2">&quot;cholesky&quot;</span><span class="s1">)</span>
    <span class="s1">ridge_64.fit(X_64</span><span class="s0">, </span><span class="s1">y_64)</span>
    <span class="s1">coef_64 = ridge_64.coef_</span>

    <span class="s5"># Do all the checks at once, like this is easier to debug</span>
    <span class="s0">assert </span><span class="s1">coef_32.dtype == X_32.dtype</span>
    <span class="s0">assert </span><span class="s1">coef_64.dtype == X_64.dtype</span>
    <span class="s0">assert </span><span class="s1">ridge_32.predict(X_32).dtype == X_32.dtype</span>
    <span class="s0">assert </span><span class="s1">ridge_64.predict(X_64).dtype == X_64.dtype</span>
    <span class="s1">assert_almost_equal(ridge_32.coef_</span><span class="s0">, </span><span class="s1">ridge_64.coef_</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">5</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;svd&quot;</span><span class="s0">, </span><span class="s2">&quot;cholesky&quot;</span><span class="s0">, </span><span class="s2">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s2">&quot;sparse_cg&quot;</span><span class="s0">, </span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s0">, </span><span class="s2">&quot;lbfgs&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;seed&quot;</span><span class="s0">, </span><span class="s1">range(</span><span class="s3">1</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_ridge_regression_dtype_stability(solver</span><span class="s0">, </span><span class="s1">seed):</span>
    <span class="s1">random_state = np.random.RandomState(seed)</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s3">6</span><span class="s0">, </span><span class="s3">5</span>
    <span class="s1">X = random_state.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">coef = random_state.randn(n_features)</span>
    <span class="s1">y = np.dot(X</span><span class="s0">, </span><span class="s1">coef) + </span><span class="s3">0.01 </span><span class="s1">* random_state.randn(n_samples)</span>
    <span class="s1">alpha = </span><span class="s3">1.0</span>
    <span class="s1">positive = solver == </span><span class="s2">&quot;lbfgs&quot;</span>
    <span class="s1">results = dict()</span>
    <span class="s5"># XXX: Sparse CG seems to be far less numerically stable than the</span>
    <span class="s5"># others, maybe we should not enable float32 for this one.</span>
    <span class="s1">atol = </span><span class="s3">1e-3 </span><span class="s0">if </span><span class="s1">solver == </span><span class="s2">&quot;sparse_cg&quot; </span><span class="s0">else </span><span class="s3">1e-5</span>
    <span class="s0">for </span><span class="s1">current_dtype </span><span class="s0">in </span><span class="s1">(np.float32</span><span class="s0">, </span><span class="s1">np.float64):</span>
        <span class="s1">results[current_dtype] = ridge_regression(</span>
            <span class="s1">X.astype(current_dtype)</span><span class="s0">,</span>
            <span class="s1">y.astype(current_dtype)</span><span class="s0">,</span>
            <span class="s1">alpha=alpha</span><span class="s0">,</span>
            <span class="s1">solver=solver</span><span class="s0">,</span>
            <span class="s1">random_state=random_state</span><span class="s0">,</span>
            <span class="s1">sample_weight=</span><span class="s0">None,</span>
            <span class="s1">positive=positive</span><span class="s0">,</span>
            <span class="s1">max_iter=</span><span class="s3">500</span><span class="s0">,</span>
            <span class="s1">tol=</span><span class="s3">1e-10</span><span class="s0">,</span>
            <span class="s1">return_n_iter=</span><span class="s0">False,</span>
            <span class="s1">return_intercept=</span><span class="s0">False,</span>
        <span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">results[np.float32].dtype == np.float32</span>
    <span class="s0">assert </span><span class="s1">results[np.float64].dtype == np.float64</span>
    <span class="s1">assert_allclose(results[np.float32]</span><span class="s0">, </span><span class="s1">results[np.float64]</span><span class="s0">, </span><span class="s1">atol=atol)</span>


<span class="s0">def </span><span class="s1">test_ridge_sag_with_X_fortran():</span>
    <span class="s5"># check that Fortran array are converted when using SAG solver</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(random_state=</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s5"># for the order of X and y to not be C-ordered arrays</span>
    <span class="s1">X = np.asfortranarray(X)</span>
    <span class="s1">X = X[::</span><span class="s3">2</span><span class="s0">, </span><span class="s1">:]</span>
    <span class="s1">y = y[::</span><span class="s3">2</span><span class="s1">]</span>
    <span class="s1">Ridge(solver=</span><span class="s2">&quot;sag&quot;</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;Classifier, params&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(RidgeClassifier</span><span class="s0">, </span><span class="s1">{})</span><span class="s0">,</span>
        <span class="s1">(RidgeClassifierCV</span><span class="s0">, </span><span class="s1">{</span><span class="s2">&quot;cv&quot;</span><span class="s1">: </span><span class="s0">None</span><span class="s1">})</span><span class="s0">,</span>
        <span class="s1">(RidgeClassifierCV</span><span class="s0">, </span><span class="s1">{</span><span class="s2">&quot;cv&quot;</span><span class="s1">: </span><span class="s3">3</span><span class="s1">})</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_ridgeclassifier_multilabel(Classifier</span><span class="s0">, </span><span class="s1">params):</span>
    <span class="s4">&quot;&quot;&quot;Check that multilabel classification is supported and give meaningful 
    results.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_multilabel_classification(n_classes=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">y = y.reshape(-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">Y = np.concatenate([y</span><span class="s0">, </span><span class="s1">y]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">clf = Classifier(**params).fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">Y_pred = clf.predict(X)</span>

    <span class="s0">assert </span><span class="s1">Y_pred.shape == Y.shape</span>
    <span class="s1">assert_array_equal(Y_pred[:</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">Y_pred[:</span><span class="s0">, </span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">Ridge(solver=</span><span class="s2">&quot;sag&quot;</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;auto&quot;</span><span class="s0">, </span><span class="s2">&quot;lbfgs&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;fit_intercept&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;alpha&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s3">1e-2</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_ridge_positive_regression_test(solver</span><span class="s0">, </span><span class="s1">fit_intercept</span><span class="s0">, </span><span class="s1">alpha):</span>
    <span class="s4">&quot;&quot;&quot;Test that positive Ridge finds true positive coefficients.&quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">3</span><span class="s0">, </span><span class="s3">4</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">5</span><span class="s0">, </span><span class="s3">6</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">7</span><span class="s0">, </span><span class="s3">8</span><span class="s1">]])</span>
    <span class="s1">coef = np.array([</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">10</span><span class="s1">])</span>
    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">intercept = </span><span class="s3">20</span>
        <span class="s1">y = X.dot(coef) + intercept</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y = X.dot(coef)</span>

    <span class="s1">model = Ridge(</span>
        <span class="s1">alpha=alpha</span><span class="s0">, </span><span class="s1">positive=</span><span class="s0">True, </span><span class="s1">solver=solver</span><span class="s0">, </span><span class="s1">fit_intercept=fit_intercept</span>
    <span class="s1">)</span>
    <span class="s1">model.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">np.all(model.coef_ &gt;= </span><span class="s3">0</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;fit_intercept&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;alpha&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s3">1e-2</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_ridge_ground_truth_positive_test(fit_intercept</span><span class="s0">, </span><span class="s1">alpha):</span>
    <span class="s4">&quot;&quot;&quot;Test that Ridge w/wo positive converges to the same solution. 
 
    Ridge with positive=True and positive=False must give the same 
    when the ground truth coefs are all positive. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(</span><span class="s3">300</span><span class="s0">, </span><span class="s3">100</span><span class="s1">)</span>
    <span class="s1">coef = rng.uniform(</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">size=X.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">intercept = </span><span class="s3">1</span>
        <span class="s1">y = X @ coef + intercept</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y = X @ coef</span>
    <span class="s1">y += rng.normal(size=X.shape[</span><span class="s3">0</span><span class="s1">]) * </span><span class="s3">0.01</span>

    <span class="s1">results = []</span>
    <span class="s0">for </span><span class="s1">positive </span><span class="s0">in </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">]:</span>
        <span class="s1">model = Ridge(</span>
            <span class="s1">alpha=alpha</span><span class="s0">, </span><span class="s1">positive=positive</span><span class="s0">, </span><span class="s1">fit_intercept=fit_intercept</span><span class="s0">, </span><span class="s1">tol=</span><span class="s3">1e-10</span>
        <span class="s1">)</span>
        <span class="s1">results.append(model.fit(X</span><span class="s0">, </span><span class="s1">y).coef_)</span>
    <span class="s1">assert_allclose(*results</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;svd&quot;</span><span class="s0">, </span><span class="s2">&quot;cholesky&quot;</span><span class="s0">, </span><span class="s2">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s2">&quot;sparse_cg&quot;</span><span class="s0">, </span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_ridge_positive_error_test(solver):</span>
    <span class="s4">&quot;&quot;&quot;Test input validation for positive argument in Ridge.&quot;&quot;&quot;</span>
    <span class="s1">alpha = </span><span class="s3">0.1</span>
    <span class="s1">X = np.array([[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">3</span><span class="s0">, </span><span class="s3">4</span><span class="s1">]])</span>
    <span class="s1">coef = np.array([</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">y = X @ coef</span>

    <span class="s1">model = Ridge(alpha=alpha</span><span class="s0">, </span><span class="s1">positive=</span><span class="s0">True, </span><span class="s1">solver=solver</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;does not support positive&quot;</span><span class="s1">):</span>
        <span class="s1">model.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;only 'lbfgs' solver can be used&quot;</span><span class="s1">):</span>
        <span class="s1">_</span><span class="s0">, </span><span class="s1">_ = ridge_regression(</span>
            <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">alpha</span><span class="s0">, </span><span class="s1">positive=</span><span class="s0">True, </span><span class="s1">solver=solver</span><span class="s0">, </span><span class="s1">return_intercept=</span><span class="s0">False</span>
        <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;alpha&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s3">1e-2</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_positive_ridge_loss(alpha):</span>
    <span class="s4">&quot;&quot;&quot;Check ridge loss consistency when positive argument is enabled.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_samples=</span><span class="s3">300</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s3">300</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">alpha = </span><span class="s3">0.10</span>
    <span class="s1">n_checks = </span><span class="s3">100</span>

    <span class="s0">def </span><span class="s1">ridge_loss(model</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s0">None, </span><span class="s1">noise_scale=</span><span class="s3">1e-8</span><span class="s1">):</span>
        <span class="s1">intercept = model.intercept_</span>
        <span class="s0">if </span><span class="s1">random_state </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">rng = np.random.RandomState(random_state)</span>
            <span class="s1">coef = model.coef_ + rng.uniform(</span><span class="s3">0</span><span class="s0">, </span><span class="s1">noise_scale</span><span class="s0">, </span><span class="s1">size=model.coef_.shape)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">coef = model.coef_</span>

        <span class="s0">return </span><span class="s3">0.5 </span><span class="s1">* np.sum((y - X @ coef - intercept) ** </span><span class="s3">2</span><span class="s1">) + </span><span class="s3">0.5 </span><span class="s1">* alpha * np.sum(</span>
            <span class="s1">coef**</span><span class="s3">2</span>
        <span class="s1">)</span>

    <span class="s1">model = Ridge(alpha=alpha).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">model_positive = Ridge(alpha=alpha</span><span class="s0">, </span><span class="s1">positive=</span><span class="s0">True</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s5"># Check 1:</span>
    <span class="s5">#   Loss for solution found by Ridge(positive=False)</span>
    <span class="s5">#   is lower than that for solution found by Ridge(positive=True)</span>
    <span class="s1">loss = ridge_loss(model)</span>
    <span class="s1">loss_positive = ridge_loss(model_positive)</span>
    <span class="s0">assert </span><span class="s1">loss &lt;= loss_positive</span>

    <span class="s5"># Check 2:</span>
    <span class="s5">#   Loss for solution found by Ridge(positive=True)</span>
    <span class="s5">#   is lower than that for small random positive perturbation</span>
    <span class="s5">#   of the positive solution.</span>
    <span class="s0">for </span><span class="s1">random_state </span><span class="s0">in </span><span class="s1">range(n_checks):</span>
        <span class="s1">loss_perturbed = ridge_loss(model_positive</span><span class="s0">, </span><span class="s1">random_state=random_state)</span>
        <span class="s0">assert </span><span class="s1">loss_positive &lt;= loss_perturbed</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;alpha&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s3">1e-2</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_lbfgs_solver_consistency(alpha):</span>
    <span class="s4">&quot;&quot;&quot;Test that LBGFS gets almost the same coef of svd when positive=False.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_samples=</span><span class="s3">300</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s3">300</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">y = np.expand_dims(y</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">alpha = np.asarray([alpha])</span>
    <span class="s1">config = {</span>
        <span class="s2">&quot;positive&quot;</span><span class="s1">: </span><span class="s0">False,</span>
        <span class="s2">&quot;tol&quot;</span><span class="s1">: </span><span class="s3">1e-16</span><span class="s0">,</span>
        <span class="s2">&quot;max_iter&quot;</span><span class="s1">: </span><span class="s3">500000</span><span class="s0">,</span>
    <span class="s1">}</span>

    <span class="s1">coef_lbfgs = _solve_lbfgs(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">alpha</span><span class="s0">, </span><span class="s1">**config)</span>
    <span class="s1">coef_cholesky = _solve_svd(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">alpha)</span>
    <span class="s1">assert_allclose(coef_lbfgs</span><span class="s0">, </span><span class="s1">coef_cholesky</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-4</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_lbfgs_solver_error():</span>
    <span class="s4">&quot;&quot;&quot;Test that LBFGS solver raises ConvergenceWarning.&quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]])</span>
    <span class="s1">y = np.array([-</span><span class="s3">1e10</span><span class="s0">, </span><span class="s3">1e10</span><span class="s1">])</span>

    <span class="s1">model = Ridge(</span>
        <span class="s1">alpha=</span><span class="s3">0.01</span><span class="s0">,</span>
        <span class="s1">solver=</span><span class="s2">&quot;lbfgs&quot;</span><span class="s0">,</span>
        <span class="s1">fit_intercept=</span><span class="s0">False,</span>
        <span class="s1">tol=</span><span class="s3">1e-12</span><span class="s0">,</span>
        <span class="s1">positive=</span><span class="s0">True,</span>
        <span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;lbfgs solver did not converge&quot;</span><span class="s1">):</span>
        <span class="s1">model.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;fit_intercept&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">False, True</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;sparseX&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">False, True</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;data&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;tall&quot;</span><span class="s0">, </span><span class="s2">&quot;wide&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">SOLVERS + [</span><span class="s2">&quot;lbfgs&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_ridge_sample_weight_consistency(</span>
    <span class="s1">fit_intercept</span><span class="s0">, </span><span class="s1">sparseX</span><span class="s0">, </span><span class="s1">data</span><span class="s0">, </span><span class="s1">solver</span><span class="s0">, </span><span class="s1">global_random_seed</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Test that the impact of sample_weight is consistent. 
 
    Note that this test is stricter than the common test 
    check_sample_weights_invariance alone. 
    &quot;&quot;&quot;</span>
    <span class="s5"># filter out solver that do not support sparse input</span>
    <span class="s0">if </span><span class="s1">sparseX:</span>
        <span class="s0">if </span><span class="s1">solver == </span><span class="s2">&quot;svd&quot; </span><span class="s0">or </span><span class="s1">(solver </span><span class="s0">in </span><span class="s1">(</span><span class="s2">&quot;cholesky&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s1">) </span><span class="s0">and </span><span class="s1">fit_intercept):</span>
            <span class="s1">pytest.skip(</span><span class="s2">&quot;unsupported configuration&quot;</span><span class="s1">)</span>

    <span class="s5"># XXX: this test is quite sensitive to the seed used to generate the data:</span>
    <span class="s5"># ideally we would like the test to pass for any global_random_seed but this is not</span>
    <span class="s5"># the case at the moment.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s3">12</span>
    <span class="s0">if </span><span class="s1">data == </span><span class="s2">&quot;tall&quot;</span><span class="s1">:</span>
        <span class="s1">n_features = n_samples // </span><span class="s3">2</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">n_features = n_samples * </span><span class="s3">2</span>

    <span class="s1">X = rng.rand(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">y = rng.rand(n_samples)</span>
    <span class="s0">if </span><span class="s1">sparseX:</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
    <span class="s1">params = dict(</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s0">,</span>
        <span class="s1">alpha=</span><span class="s3">1.0</span><span class="s0">,</span>
        <span class="s1">solver=solver</span><span class="s0">,</span>
        <span class="s1">positive=(solver == </span><span class="s2">&quot;lbfgs&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">random_state=global_random_seed</span><span class="s0">,  </span><span class="s5"># for sag/saga</span>
        <span class="s1">tol=</span><span class="s3">1e-12</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s5"># 1) sample_weight=np.ones(..) should be equivalent to sample_weight=None</span>
    <span class="s5"># same check as check_sample_weights_invariance(name, reg, kind=&quot;ones&quot;), but we also</span>
    <span class="s5"># test with sparse input.</span>
    <span class="s1">reg = Ridge(**params).fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=</span><span class="s0">None</span><span class="s1">)</span>
    <span class="s1">coef = reg.coef_.copy()</span>
    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">intercept = reg.intercept_</span>
    <span class="s1">sample_weight = np.ones_like(y)</span>
    <span class="s1">reg.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_allclose(reg.coef_</span><span class="s0">, </span><span class="s1">coef</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-6</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">assert_allclose(reg.intercept_</span><span class="s0">, </span><span class="s1">intercept)</span>

    <span class="s5"># 2) setting elements of sample_weight to 0 is equivalent to removing these samples</span>
    <span class="s5"># same check as check_sample_weights_invariance(name, reg, kind=&quot;zeros&quot;), but we</span>
    <span class="s5"># also test with sparse input</span>
    <span class="s1">sample_weight = rng.uniform(low=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">high=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">size=X.shape[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">sample_weight[-</span><span class="s3">5</span><span class="s1">:] = </span><span class="s3">0</span>
    <span class="s1">y[-</span><span class="s3">5</span><span class="s1">:] *= </span><span class="s3">1000  </span><span class="s5"># to make excluding those samples important</span>
    <span class="s1">reg.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">coef = reg.coef_.copy()</span>
    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">intercept = reg.intercept_</span>
    <span class="s1">reg.fit(X[:-</span><span class="s3">5</span><span class="s0">, </span><span class="s1">:]</span><span class="s0">, </span><span class="s1">y[:-</span><span class="s3">5</span><span class="s1">]</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight[:-</span><span class="s3">5</span><span class="s1">])</span>
    <span class="s1">assert_allclose(reg.coef_</span><span class="s0">, </span><span class="s1">coef</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-6</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">assert_allclose(reg.intercept_</span><span class="s0">, </span><span class="s1">intercept)</span>

    <span class="s5"># 3) scaling of sample_weight should have no effect</span>
    <span class="s5"># Note: For models with penalty, scaling the penalty term might work.</span>
    <span class="s1">reg2 = Ridge(**params).set_params(alpha=np.pi * params[</span><span class="s2">&quot;alpha&quot;</span><span class="s1">])</span>
    <span class="s1">reg2.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=np.pi * sample_weight)</span>
    <span class="s0">if </span><span class="s1">solver </span><span class="s0">in </span><span class="s1">(</span><span class="s2">&quot;sag&quot;</span><span class="s0">, </span><span class="s2">&quot;saga&quot;</span><span class="s1">) </span><span class="s0">and not </span><span class="s1">fit_intercept:</span>
        <span class="s1">pytest.xfail(</span><span class="s2">f&quot;Solver </span><span class="s0">{</span><span class="s1">solver</span><span class="s0">} </span><span class="s2">does fail test for scaling of sample_weight.&quot;</span><span class="s1">)</span>
    <span class="s1">assert_allclose(reg2.coef_</span><span class="s0">, </span><span class="s1">coef</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-6</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">assert_allclose(reg2.intercept_</span><span class="s0">, </span><span class="s1">intercept)</span>

    <span class="s5"># 4) check that multiplying sample_weight by 2 is equivalent</span>
    <span class="s5"># to repeating corresponding samples twice</span>
    <span class="s0">if </span><span class="s1">sparseX:</span>
        <span class="s1">X = X.toarray()</span>
    <span class="s1">X2 = np.concatenate([X</span><span class="s0">, </span><span class="s1">X[: n_samples // </span><span class="s3">2</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">y2 = np.concatenate([y</span><span class="s0">, </span><span class="s1">y[: n_samples // </span><span class="s3">2</span><span class="s1">]])</span>
    <span class="s1">sample_weight_1 = sample_weight.copy()</span>
    <span class="s1">sample_weight_1[: n_samples // </span><span class="s3">2</span><span class="s1">] *= </span><span class="s3">2</span>
    <span class="s1">sample_weight_2 = np.concatenate(</span>
        <span class="s1">[sample_weight</span><span class="s0">, </span><span class="s1">sample_weight[: n_samples // </span><span class="s3">2</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">0</span>
    <span class="s1">)</span>
    <span class="s0">if </span><span class="s1">sparseX:</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
        <span class="s1">X2 = sp.csr_matrix(X2)</span>
    <span class="s1">reg1 = Ridge(**params).fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight_1)</span>
    <span class="s1">reg2 = Ridge(**params).fit(X2</span><span class="s0">, </span><span class="s1">y2</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight_2)</span>
    <span class="s1">assert_allclose(reg1.coef_</span><span class="s0">, </span><span class="s1">reg2.coef_)</span>
    <span class="s0">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">assert_allclose(reg1.intercept_</span><span class="s0">, </span><span class="s1">reg2.intercept_)</span>
</pre>
</body>
</html>