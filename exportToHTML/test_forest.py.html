<html>
<head>
<title>test_forest.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_forest.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Testing for the forest module (sklearn.ensemble.forest). 
&quot;&quot;&quot;</span>

<span class="s2"># Authors: Gilles Louppe,</span>
<span class="s2">#          Brian Holt,</span>
<span class="s2">#          Andreas Mueller,</span>
<span class="s2">#          Arnaud Joly</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">itertools</span>
<span class="s3">import </span><span class="s1">math</span>
<span class="s3">import </span><span class="s1">pickle</span>
<span class="s3">from </span><span class="s1">collections </span><span class="s3">import </span><span class="s1">defaultdict</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">from </span><span class="s1">itertools </span><span class="s3">import </span><span class="s1">combinations</span><span class="s3">, </span><span class="s1">product</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Any</span><span class="s3">, </span><span class="s1">Dict</span>
<span class="s3">from </span><span class="s1">unittest.mock </span><span class="s3">import </span><span class="s1">patch</span>

<span class="s3">import </span><span class="s1">joblib</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">pytest</span>
<span class="s3">from </span><span class="s1">scipy.sparse </span><span class="s3">import </span><span class="s1">coo_matrix</span><span class="s3">, </span><span class="s1">csc_matrix</span><span class="s3">, </span><span class="s1">csr_matrix</span>
<span class="s3">from </span><span class="s1">scipy.special </span><span class="s3">import </span><span class="s1">comb</span>

<span class="s3">import </span><span class="s1">sklearn</span>
<span class="s3">from </span><span class="s1">sklearn </span><span class="s3">import </span><span class="s1">datasets</span>
<span class="s3">from </span><span class="s1">sklearn.datasets </span><span class="s3">import </span><span class="s1">make_classification</span>
<span class="s3">from </span><span class="s1">sklearn.decomposition </span><span class="s3">import </span><span class="s1">TruncatedSVD</span>
<span class="s3">from </span><span class="s1">sklearn.dummy </span><span class="s3">import </span><span class="s1">DummyRegressor</span>
<span class="s3">from </span><span class="s1">sklearn.ensemble </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">ExtraTreesClassifier</span><span class="s3">,</span>
    <span class="s1">ExtraTreesRegressor</span><span class="s3">,</span>
    <span class="s1">RandomForestClassifier</span><span class="s3">,</span>
    <span class="s1">RandomForestRegressor</span><span class="s3">,</span>
    <span class="s1">RandomTreesEmbedding</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">sklearn.exceptions </span><span class="s3">import </span><span class="s1">NotFittedError</span>
<span class="s3">from </span><span class="s1">sklearn.metrics </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">explained_variance_score</span><span class="s3">,</span>
    <span class="s1">f1_score</span><span class="s3">,</span>
    <span class="s1">mean_poisson_deviance</span><span class="s3">,</span>
    <span class="s1">mean_squared_error</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">sklearn.model_selection </span><span class="s3">import </span><span class="s1">GridSearchCV</span><span class="s3">, </span><span class="s1">cross_val_score</span><span class="s3">, </span><span class="s1">train_test_split</span>
<span class="s3">from </span><span class="s1">sklearn.svm </span><span class="s3">import </span><span class="s1">LinearSVC</span>
<span class="s3">from </span><span class="s1">sklearn.tree._classes </span><span class="s3">import </span><span class="s1">SPARSE_SPLITTERS</span>
<span class="s3">from </span><span class="s1">sklearn.utils._testing </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">_convert_container</span><span class="s3">,</span>
    <span class="s1">assert_almost_equal</span><span class="s3">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s3">,</span>
    <span class="s1">assert_array_equal</span><span class="s3">,</span>
    <span class="s1">ignore_warnings</span><span class="s3">,</span>
    <span class="s1">skip_if_no_parallel</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">sklearn.utils.parallel </span><span class="s3">import </span><span class="s1">Parallel</span>
<span class="s3">from </span><span class="s1">sklearn.utils.validation </span><span class="s3">import </span><span class="s1">check_random_state</span>

<span class="s2"># toy sample</span>
<span class="s1">X = [[-</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]]</span>
<span class="s1">y = [-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span>
<span class="s1">T = [[-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">3</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]]</span>
<span class="s1">true_result = [-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span>

<span class="s2"># Larger classification sample used for testing feature importances</span>
<span class="s1">X_large</span><span class="s3">, </span><span class="s1">y_large = datasets.make_classification(</span>
    <span class="s1">n_samples=</span><span class="s4">500</span><span class="s3">,</span>
    <span class="s1">n_features=</span><span class="s4">10</span><span class="s3">,</span>
    <span class="s1">n_informative=</span><span class="s4">3</span><span class="s3">,</span>
    <span class="s1">n_redundant=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">n_repeated=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">shuffle=</span><span class="s3">False,</span>
    <span class="s1">random_state=</span><span class="s4">0</span><span class="s3">,</span>
<span class="s1">)</span>

<span class="s2"># also load the iris dataset</span>
<span class="s2"># and randomly permute it</span>
<span class="s1">iris = datasets.load_iris()</span>
<span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
<span class="s1">perm = rng.permutation(iris.target.size)</span>
<span class="s1">iris.data = iris.data[perm]</span>
<span class="s1">iris.target = iris.target[perm]</span>

<span class="s2"># Make regression dataset</span>
<span class="s1">X_reg</span><span class="s3">, </span><span class="s1">y_reg = datasets.make_regression(n_samples=</span><span class="s4">500</span><span class="s3">, </span><span class="s1">n_features=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>

<span class="s2"># also make a hastie_10_2 dataset</span>
<span class="s1">hastie_X</span><span class="s3">, </span><span class="s1">hastie_y = datasets.make_hastie_10_2(n_samples=</span><span class="s4">20</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">hastie_X = hastie_X.astype(np.float32)</span>

<span class="s2"># Get the default backend in joblib to test parallelism and interaction with</span>
<span class="s2"># different backends</span>
<span class="s1">DEFAULT_JOBLIB_BACKEND = joblib.parallel.get_active_backend()[</span><span class="s4">0</span><span class="s1">].__class__</span>

<span class="s1">FOREST_CLASSIFIERS = {</span>
    <span class="s5">&quot;ExtraTreesClassifier&quot;</span><span class="s1">: ExtraTreesClassifier</span><span class="s3">,</span>
    <span class="s5">&quot;RandomForestClassifier&quot;</span><span class="s1">: RandomForestClassifier</span><span class="s3">,</span>
<span class="s1">}</span>

<span class="s1">FOREST_REGRESSORS = {</span>
    <span class="s5">&quot;ExtraTreesRegressor&quot;</span><span class="s1">: ExtraTreesRegressor</span><span class="s3">,</span>
    <span class="s5">&quot;RandomForestRegressor&quot;</span><span class="s1">: RandomForestRegressor</span><span class="s3">,</span>
<span class="s1">}</span>

<span class="s1">FOREST_TRANSFORMERS = {</span>
    <span class="s5">&quot;RandomTreesEmbedding&quot;</span><span class="s1">: RandomTreesEmbedding</span><span class="s3">,</span>
<span class="s1">}</span>

<span class="s1">FOREST_ESTIMATORS: Dict[str</span><span class="s3">, </span><span class="s1">Any] = dict()</span>
<span class="s1">FOREST_ESTIMATORS.update(FOREST_CLASSIFIERS)</span>
<span class="s1">FOREST_ESTIMATORS.update(FOREST_REGRESSORS)</span>
<span class="s1">FOREST_ESTIMATORS.update(FOREST_TRANSFORMERS)</span>

<span class="s1">FOREST_CLASSIFIERS_REGRESSORS: Dict[str</span><span class="s3">, </span><span class="s1">Any] = FOREST_CLASSIFIERS.copy()</span>
<span class="s1">FOREST_CLASSIFIERS_REGRESSORS.update(FOREST_REGRESSORS)</span>


<span class="s3">def </span><span class="s1">check_classification_toy(name):</span>
    <span class="s0">&quot;&quot;&quot;Check classification on a toy dataset.&quot;&quot;&quot;</span>
    <span class="s1">ForestClassifier = FOREST_CLASSIFIERS[name]</span>

    <span class="s1">clf = ForestClassifier(n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(clf.predict(T)</span><span class="s3">, </span><span class="s1">true_result)</span>
    <span class="s3">assert </span><span class="s4">10 </span><span class="s1">== len(clf)</span>

    <span class="s1">clf = ForestClassifier(n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">max_features=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(clf.predict(T)</span><span class="s3">, </span><span class="s1">true_result)</span>
    <span class="s3">assert </span><span class="s4">10 </span><span class="s1">== len(clf)</span>

    <span class="s2"># also test apply</span>
    <span class="s1">leaf_indices = clf.apply(X)</span>
    <span class="s3">assert </span><span class="s1">leaf_indices.shape == (len(X)</span><span class="s3">, </span><span class="s1">clf.n_estimators)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS)</span>
<span class="s3">def </span><span class="s1">test_classification_toy(name):</span>
    <span class="s1">check_classification_toy(name)</span>


<span class="s3">def </span><span class="s1">check_iris_criterion(name</span><span class="s3">, </span><span class="s1">criterion):</span>
    <span class="s2"># Check consistency on dataset iris.</span>
    <span class="s1">ForestClassifier = FOREST_CLASSIFIERS[name]</span>

    <span class="s1">clf = ForestClassifier(n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">criterion=criterion</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf.fit(iris.data</span><span class="s3">, </span><span class="s1">iris.target)</span>
    <span class="s1">score = clf.score(iris.data</span><span class="s3">, </span><span class="s1">iris.target)</span>
    <span class="s3">assert </span><span class="s1">score &gt; </span><span class="s4">0.9</span><span class="s3">, </span><span class="s5">&quot;Failed with criterion %s and score = %f&quot; </span><span class="s1">% (criterion</span><span class="s3">, </span><span class="s1">score)</span>

    <span class="s1">clf = ForestClassifier(</span>
        <span class="s1">n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">criterion=criterion</span><span class="s3">, </span><span class="s1">max_features=</span><span class="s4">2</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(iris.data</span><span class="s3">, </span><span class="s1">iris.target)</span>
    <span class="s1">score = clf.score(iris.data</span><span class="s3">, </span><span class="s1">iris.target)</span>
    <span class="s3">assert </span><span class="s1">score &gt; </span><span class="s4">0.5</span><span class="s3">, </span><span class="s5">&quot;Failed with criterion %s and score = %f&quot; </span><span class="s1">% (criterion</span><span class="s3">, </span><span class="s1">score)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;criterion&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">&quot;gini&quot;</span><span class="s3">, </span><span class="s5">&quot;log_loss&quot;</span><span class="s1">))</span>
<span class="s3">def </span><span class="s1">test_iris(name</span><span class="s3">, </span><span class="s1">criterion):</span>
    <span class="s1">check_iris_criterion(name</span><span class="s3">, </span><span class="s1">criterion)</span>


<span class="s3">def </span><span class="s1">check_regression_criterion(name</span><span class="s3">, </span><span class="s1">criterion):</span>
    <span class="s2"># Check consistency on regression dataset.</span>
    <span class="s1">ForestRegressor = FOREST_REGRESSORS[name]</span>

    <span class="s1">reg = ForestRegressor(n_estimators=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">criterion=criterion</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">reg.fit(X_reg</span><span class="s3">, </span><span class="s1">y_reg)</span>
    <span class="s1">score = reg.score(X_reg</span><span class="s3">, </span><span class="s1">y_reg)</span>
    <span class="s3">assert </span><span class="s1">(</span>
        <span class="s1">score &gt; </span><span class="s4">0.93</span>
    <span class="s1">)</span><span class="s3">, </span><span class="s5">&quot;Failed with max_features=None, criterion %s and score = %f&quot; </span><span class="s1">% (</span>
        <span class="s1">criterion</span><span class="s3">,</span>
        <span class="s1">score</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s1">reg = ForestRegressor(</span>
        <span class="s1">n_estimators=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">criterion=criterion</span><span class="s3">, </span><span class="s1">max_features=</span><span class="s4">6</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span>
    <span class="s1">)</span>
    <span class="s1">reg.fit(X_reg</span><span class="s3">, </span><span class="s1">y_reg)</span>
    <span class="s1">score = reg.score(X_reg</span><span class="s3">, </span><span class="s1">y_reg)</span>
    <span class="s3">assert </span><span class="s1">score &gt; </span><span class="s4">0.92</span><span class="s3">, </span><span class="s5">&quot;Failed with max_features=6, criterion %s and score = %f&quot; </span><span class="s1">% (</span>
        <span class="s1">criterion</span><span class="s3">,</span>
        <span class="s1">score</span><span class="s3">,</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_REGRESSORS)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;criterion&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">&quot;squared_error&quot;</span><span class="s3">, </span><span class="s5">&quot;absolute_error&quot;</span><span class="s3">, </span><span class="s5">&quot;friedman_mse&quot;</span><span class="s1">)</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">test_regression(name</span><span class="s3">, </span><span class="s1">criterion):</span>
    <span class="s1">check_regression_criterion(name</span><span class="s3">, </span><span class="s1">criterion)</span>


<span class="s3">def </span><span class="s1">test_poisson_vs_mse():</span>
    <span class="s0">&quot;&quot;&quot;Test that random forest with poisson criterion performs better than 
    mse for a poisson target. 
 
    There is a similar test for DecisionTreeRegressor. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">n_train</span><span class="s3">, </span><span class="s1">n_test</span><span class="s3">, </span><span class="s1">n_features = </span><span class="s4">500</span><span class="s3">, </span><span class="s4">500</span><span class="s3">, </span><span class="s4">10</span>
    <span class="s1">X = datasets.make_low_rank_matrix(</span>
        <span class="s1">n_samples=n_train + n_test</span><span class="s3">, </span><span class="s1">n_features=n_features</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s2">#  We create a log-linear Poisson model and downscale coef as it will get</span>
    <span class="s2"># exponentiated.</span>
    <span class="s1">coef = rng.uniform(low=-</span><span class="s4">2</span><span class="s3">, </span><span class="s1">high=</span><span class="s4">2</span><span class="s3">, </span><span class="s1">size=n_features) / np.max(X</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">y = rng.poisson(lam=np.exp(X @ coef))</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">test_size=n_test</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s2"># We prevent some overfitting by setting min_samples_split=10.</span>
    <span class="s1">forest_poi = RandomForestRegressor(</span>
        <span class="s1">criterion=</span><span class="s5">&quot;poisson&quot;</span><span class="s3">, </span><span class="s1">min_samples_leaf=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">max_features=</span><span class="s5">&quot;sqrt&quot;</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">forest_mse = RandomForestRegressor(</span>
        <span class="s1">criterion=</span><span class="s5">&quot;squared_error&quot;</span><span class="s3">,</span>
        <span class="s1">min_samples_leaf=</span><span class="s4">10</span><span class="s3">,</span>
        <span class="s1">max_features=</span><span class="s5">&quot;sqrt&quot;</span><span class="s3">,</span>
        <span class="s1">random_state=rng</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s1">forest_poi.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s1">forest_mse.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s1">dummy = DummyRegressor(strategy=</span><span class="s5">&quot;mean&quot;</span><span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s3">for </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">data_name </span><span class="s3">in </span><span class="s1">[(X_train</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s5">&quot;train&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(X_test</span><span class="s3">, </span><span class="s1">y_test</span><span class="s3">, </span><span class="s5">&quot;test&quot;</span><span class="s1">)]:</span>
        <span class="s1">metric_poi = mean_poisson_deviance(y</span><span class="s3">, </span><span class="s1">forest_poi.predict(X))</span>
        <span class="s2"># squared_error forest might produce non-positive predictions =&gt; clip</span>
        <span class="s2"># If y = 0 for those, the poisson deviance gets too good.</span>
        <span class="s2"># If we drew more samples, we would eventually get y &gt; 0 and the</span>
        <span class="s2"># poisson deviance would explode, i.e. be undefined. Therefore, we do</span>
        <span class="s2"># not clip to a tiny value like 1e-15, but to 1e-6. This acts like a</span>
        <span class="s2"># small penalty to the non-positive predictions.</span>
        <span class="s1">metric_mse = mean_poisson_deviance(</span>
            <span class="s1">y</span><span class="s3">, </span><span class="s1">np.clip(forest_mse.predict(X)</span><span class="s3">, </span><span class="s4">1e-6</span><span class="s3">, None</span><span class="s1">)</span>
        <span class="s1">)</span>
        <span class="s1">metric_dummy = mean_poisson_deviance(y</span><span class="s3">, </span><span class="s1">dummy.predict(X))</span>
        <span class="s2"># As squared_error might correctly predict 0 in train set, its train</span>
        <span class="s2"># score can be better than Poisson. This is no longer the case for the</span>
        <span class="s2"># test set. But keep the above comment for clipping in mind.</span>
        <span class="s3">if </span><span class="s1">data_name == </span><span class="s5">&quot;test&quot;</span><span class="s1">:</span>
            <span class="s3">assert </span><span class="s1">metric_poi &lt; metric_mse</span>
        <span class="s3">assert </span><span class="s1">metric_poi &lt; </span><span class="s4">0.8 </span><span class="s1">* metric_dummy</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;criterion&quot;</span><span class="s3">, </span><span class="s1">(</span><span class="s5">&quot;poisson&quot;</span><span class="s3">, </span><span class="s5">&quot;squared_error&quot;</span><span class="s1">))</span>
<span class="s3">def </span><span class="s1">test_balance_property_random_forest(criterion):</span>
    <span class="s0">&quot;&quot;&quot; &quot;Test that sum(y_pred)==sum(y_true) on the training set.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">n_train</span><span class="s3">, </span><span class="s1">n_test</span><span class="s3">, </span><span class="s1">n_features = </span><span class="s4">500</span><span class="s3">, </span><span class="s4">500</span><span class="s3">, </span><span class="s4">10</span>
    <span class="s1">X = datasets.make_low_rank_matrix(</span>
        <span class="s1">n_samples=n_train + n_test</span><span class="s3">, </span><span class="s1">n_features=n_features</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>

    <span class="s1">coef = rng.uniform(low=-</span><span class="s4">2</span><span class="s3">, </span><span class="s1">high=</span><span class="s4">2</span><span class="s3">, </span><span class="s1">size=n_features) / np.max(X</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">y = rng.poisson(lam=np.exp(X @ coef))</span>

    <span class="s1">reg = RandomForestRegressor(</span>
        <span class="s1">criterion=criterion</span><span class="s3">, </span><span class="s1">n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">bootstrap=</span><span class="s3">False, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">reg.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s3">assert </span><span class="s1">np.sum(reg.predict(X)) == pytest.approx(np.sum(y))</span>


<span class="s3">def </span><span class="s1">check_regressor_attributes(name):</span>
    <span class="s2"># Regression models should not have a classes_ attribute.</span>
    <span class="s1">r = FOREST_REGRESSORS[name](random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s3">assert not </span><span class="s1">hasattr(r</span><span class="s3">, </span><span class="s5">&quot;classes_&quot;</span><span class="s1">)</span>
    <span class="s3">assert not </span><span class="s1">hasattr(r</span><span class="s3">, </span><span class="s5">&quot;n_classes_&quot;</span><span class="s1">)</span>

    <span class="s1">r.fit([[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">4</span><span class="s3">, </span><span class="s4">5</span><span class="s3">, </span><span class="s4">6</span><span class="s1">]]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">])</span>
    <span class="s3">assert not </span><span class="s1">hasattr(r</span><span class="s3">, </span><span class="s5">&quot;classes_&quot;</span><span class="s1">)</span>
    <span class="s3">assert not </span><span class="s1">hasattr(r</span><span class="s3">, </span><span class="s5">&quot;n_classes_&quot;</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_REGRESSORS)</span>
<span class="s3">def </span><span class="s1">test_regressor_attributes(name):</span>
    <span class="s1">check_regressor_attributes(name)</span>


<span class="s3">def </span><span class="s1">check_probability(name):</span>
    <span class="s2"># Predict probabilities.</span>
    <span class="s1">ForestClassifier = FOREST_CLASSIFIERS[name]</span>
    <span class="s3">with </span><span class="s1">np.errstate(divide=</span><span class="s5">&quot;ignore&quot;</span><span class="s1">):</span>
        <span class="s1">clf = ForestClassifier(</span>
            <span class="s1">n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">max_features=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">max_depth=</span><span class="s4">1</span>
        <span class="s1">)</span>
        <span class="s1">clf.fit(iris.data</span><span class="s3">, </span><span class="s1">iris.target)</span>
        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">np.sum(clf.predict_proba(iris.data)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.ones(iris.data.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">)</span>
        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">clf.predict_proba(iris.data)</span><span class="s3">, </span><span class="s1">np.exp(clf.predict_log_proba(iris.data))</span>
        <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS)</span>
<span class="s3">def </span><span class="s1">test_probability(name):</span>
    <span class="s1">check_probability(name)</span>


<span class="s3">def </span><span class="s1">check_importances(name</span><span class="s3">, </span><span class="s1">criterion</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">, </span><span class="s1">tolerance):</span>
    <span class="s2"># cast as dype</span>
    <span class="s1">X = X_large.astype(dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">y = y_large.astype(dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>

    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>

    <span class="s1">est = ForestEstimator(n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">criterion=criterion</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">importances = est.feature_importances_</span>

    <span class="s2"># The forest estimator can detect that only the first 3 features of the</span>
    <span class="s2"># dataset are informative:</span>
    <span class="s1">n_important = np.sum(importances &gt; </span><span class="s4">0.1</span><span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">importances.shape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">10</span>
    <span class="s3">assert </span><span class="s1">n_important == </span><span class="s4">3</span>
    <span class="s3">assert </span><span class="s1">np.all(importances[:</span><span class="s4">3</span><span class="s1">] &gt; </span><span class="s4">0.1</span><span class="s1">)</span>

    <span class="s2"># Check with parallel</span>
    <span class="s1">importances = est.feature_importances_</span>
    <span class="s1">est.set_params(n_jobs=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">importances_parallel = est.feature_importances_</span>
    <span class="s1">assert_array_almost_equal(importances</span><span class="s3">, </span><span class="s1">importances_parallel)</span>

    <span class="s2"># Check with sample weights</span>
    <span class="s1">sample_weight = check_random_state(</span><span class="s4">0</span><span class="s1">).randint(</span><span class="s4">1</span><span class="s3">, </span><span class="s4">10</span><span class="s3">, </span><span class="s1">len(X))</span>
    <span class="s1">est = ForestEstimator(n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">criterion=criterion)</span>
    <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">importances = est.feature_importances_</span>
    <span class="s3">assert </span><span class="s1">np.all(importances &gt;= </span><span class="s4">0.0</span><span class="s1">)</span>

    <span class="s3">for </span><span class="s1">scale </span><span class="s3">in </span><span class="s1">[</span><span class="s4">0.5</span><span class="s3">, </span><span class="s4">100</span><span class="s1">]:</span>
        <span class="s1">est = ForestEstimator(n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">criterion=criterion)</span>
        <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=scale * sample_weight)</span>
        <span class="s1">importances_bis = est.feature_importances_</span>
        <span class="s3">assert </span><span class="s1">np.abs(importances - importances_bis).mean() &lt; tolerance</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;dtype&quot;</span><span class="s3">, </span><span class="s1">(np.float64</span><span class="s3">, </span><span class="s1">np.float32))</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;name, criterion&quot;</span><span class="s3">,</span>
    <span class="s1">itertools.chain(</span>
        <span class="s1">product(FOREST_CLASSIFIERS</span><span class="s3">, </span><span class="s1">[</span><span class="s5">&quot;gini&quot;</span><span class="s3">, </span><span class="s5">&quot;log_loss&quot;</span><span class="s1">])</span><span class="s3">,</span>
        <span class="s1">product(FOREST_REGRESSORS</span><span class="s3">, </span><span class="s1">[</span><span class="s5">&quot;squared_error&quot;</span><span class="s3">, </span><span class="s5">&quot;friedman_mse&quot;</span><span class="s3">, </span><span class="s5">&quot;absolute_error&quot;</span><span class="s1">])</span><span class="s3">,</span>
    <span class="s1">)</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">test_importances(dtype</span><span class="s3">, </span><span class="s1">name</span><span class="s3">, </span><span class="s1">criterion):</span>
    <span class="s1">tolerance = </span><span class="s4">0.01</span>
    <span class="s3">if </span><span class="s1">name </span><span class="s3">in </span><span class="s1">FOREST_REGRESSORS </span><span class="s3">and </span><span class="s1">criterion == </span><span class="s5">&quot;absolute_error&quot;</span><span class="s1">:</span>
        <span class="s1">tolerance = </span><span class="s4">0.05</span>
    <span class="s1">check_importances(name</span><span class="s3">, </span><span class="s1">criterion</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">, </span><span class="s1">tolerance)</span>


<span class="s3">def </span><span class="s1">test_importances_asymptotic():</span>
    <span class="s2"># Check whether variable importances of totally randomized trees</span>
    <span class="s2"># converge towards their theoretical values (See Louppe et al,</span>
    <span class="s2"># Understanding variable importances in forests of randomized trees, 2013).</span>

    <span class="s3">def </span><span class="s1">binomial(k</span><span class="s3">, </span><span class="s1">n):</span>
        <span class="s3">return </span><span class="s4">0 </span><span class="s3">if </span><span class="s1">k &lt; </span><span class="s4">0 </span><span class="s3">or </span><span class="s1">k &gt; n </span><span class="s3">else </span><span class="s1">comb(int(n)</span><span class="s3">, </span><span class="s1">int(k)</span><span class="s3">, </span><span class="s1">exact=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">entropy(samples):</span>
        <span class="s1">n_samples = len(samples)</span>
        <span class="s1">entropy = </span><span class="s4">0.0</span>

        <span class="s3">for </span><span class="s1">count </span><span class="s3">in </span><span class="s1">np.bincount(samples):</span>
            <span class="s1">p = </span><span class="s4">1.0 </span><span class="s1">* count / n_samples</span>
            <span class="s3">if </span><span class="s1">p &gt; </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s1">entropy -= p * np.log2(p)</span>

        <span class="s3">return </span><span class="s1">entropy</span>

    <span class="s3">def </span><span class="s1">mdi_importance(X_m</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>

        <span class="s1">features = list(range(n_features))</span>
        <span class="s1">features.pop(X_m)</span>
        <span class="s1">values = [np.unique(X[:</span><span class="s3">, </span><span class="s1">i]) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_features)]</span>

        <span class="s1">imp = </span><span class="s4">0.0</span>

        <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range(n_features):</span>
            <span class="s2"># Weight of each B of size k</span>
            <span class="s1">coef = </span><span class="s4">1.0 </span><span class="s1">/ (binomial(k</span><span class="s3">, </span><span class="s1">n_features) * (n_features - k))</span>

            <span class="s2"># For all B of size k</span>
            <span class="s3">for </span><span class="s1">B </span><span class="s3">in </span><span class="s1">combinations(features</span><span class="s3">, </span><span class="s1">k):</span>
                <span class="s2"># For all values B=b</span>
                <span class="s3">for </span><span class="s1">b </span><span class="s3">in </span><span class="s1">product(*[values[B[j]] </span><span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(k)]):</span>
                    <span class="s1">mask_b = np.ones(n_samples</span><span class="s3">, </span><span class="s1">dtype=bool)</span>

                    <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(k):</span>
                        <span class="s1">mask_b &amp;= X[:</span><span class="s3">, </span><span class="s1">B[j]] == b[j]</span>

                    <span class="s1">X_</span><span class="s3">, </span><span class="s1">y_ = X[mask_b</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">y[mask_b]</span>
                    <span class="s1">n_samples_b = len(X_)</span>

                    <span class="s3">if </span><span class="s1">n_samples_b &gt; </span><span class="s4">0</span><span class="s1">:</span>
                        <span class="s1">children = []</span>

                        <span class="s3">for </span><span class="s1">xi </span><span class="s3">in </span><span class="s1">values[X_m]:</span>
                            <span class="s1">mask_xi = X_[:</span><span class="s3">, </span><span class="s1">X_m] == xi</span>
                            <span class="s1">children.append(y_[mask_xi])</span>

                        <span class="s1">imp += (</span>
                            <span class="s1">coef</span>
                            <span class="s1">* (</span><span class="s4">1.0 </span><span class="s1">* n_samples_b / n_samples)  </span><span class="s2"># P(B=b)</span>
                            <span class="s1">* (</span>
                                <span class="s1">entropy(y_)</span>
                                <span class="s1">- sum(</span>
                                    <span class="s1">[</span>
                                        <span class="s1">entropy(c) * len(c) / n_samples_b</span>
                                        <span class="s3">for </span><span class="s1">c </span><span class="s3">in </span><span class="s1">children</span>
                                    <span class="s1">]</span>
                                <span class="s1">)</span>
                            <span class="s1">)</span>
                        <span class="s1">)</span>

        <span class="s3">return </span><span class="s1">imp</span>

    <span class="s1">data = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">3</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">4</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">5</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">6</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">7</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">8</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">9</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = np.array(data[:</span><span class="s3">, </span><span class="s1">:</span><span class="s4">7</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=bool)</span><span class="s3">, </span><span class="s1">data[:</span><span class="s3">, </span><span class="s4">7</span><span class="s1">]</span>
    <span class="s1">n_features = X.shape[</span><span class="s4">1</span><span class="s1">]</span>

    <span class="s2"># Compute true importances</span>
    <span class="s1">true_importances = np.zeros(n_features)</span>

    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_features):</span>
        <span class="s1">true_importances[i] = mdi_importance(i</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s2"># Estimate importances with totally randomized trees</span>
    <span class="s1">clf = ExtraTreesClassifier(</span>
        <span class="s1">n_estimators=</span><span class="s4">500</span><span class="s3">, </span><span class="s1">max_features=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">criterion=</span><span class="s5">&quot;log_loss&quot;</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">importances = (</span>
        <span class="s1">sum(</span>
            <span class="s1">tree.tree_.compute_feature_importances(normalize=</span><span class="s3">False</span><span class="s1">)</span>
            <span class="s3">for </span><span class="s1">tree </span><span class="s3">in </span><span class="s1">clf.estimators_</span>
        <span class="s1">)</span>
        <span class="s1">/ clf.n_estimators</span>
    <span class="s1">)</span>

    <span class="s2"># Check correctness</span>
    <span class="s1">assert_almost_equal(entropy(y)</span><span class="s3">, </span><span class="s1">sum(importances))</span>
    <span class="s3">assert </span><span class="s1">np.abs(true_importances - importances).mean() &lt; </span><span class="s4">0.01</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_ESTIMATORS)</span>
<span class="s3">def </span><span class="s1">test_unfitted_feature_importances(name):</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">&quot;This {} instance is not fitted yet. Call 'fit' with &quot;</span>
        <span class="s5">&quot;appropriate arguments before using this estimator.&quot;</span><span class="s1">.format(name)</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s3">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">getattr(FOREST_ESTIMATORS[name]()</span><span class="s3">, </span><span class="s5">&quot;feature_importances_&quot;</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;ForestClassifier&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS.values())</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;X_type&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s5">&quot;array&quot;</span><span class="s3">, </span><span class="s5">&quot;sparse_csr&quot;</span><span class="s3">, </span><span class="s5">&quot;sparse_csc&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;X, y, lower_bound_accuracy&quot;</span><span class="s3">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">*datasets.make_classification(n_samples=</span><span class="s4">300</span><span class="s3">, </span><span class="s1">n_classes=</span><span class="s4">2</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s4">0.9</span><span class="s3">,</span>
        <span class="s1">)</span><span class="s3">,</span>
        <span class="s1">(</span>
            <span class="s1">*datasets.make_classification(</span>
                <span class="s1">n_samples=</span><span class="s4">1000</span><span class="s3">, </span><span class="s1">n_classes=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">n_informative=</span><span class="s4">6</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s4">0.65</span><span class="s3">,</span>
        <span class="s1">)</span><span class="s3">,</span>
        <span class="s1">(</span>
            <span class="s1">iris.data</span><span class="s3">,</span>
            <span class="s1">iris.target * </span><span class="s4">2 </span><span class="s1">+ </span><span class="s4">1</span><span class="s3">,</span>
            <span class="s4">0.65</span><span class="s3">,</span>
        <span class="s1">)</span><span class="s3">,</span>
        <span class="s1">(</span>
            <span class="s1">*datasets.make_multilabel_classification(n_samples=</span><span class="s4">300</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s4">0.18</span><span class="s3">,</span>
        <span class="s1">)</span><span class="s3">,</span>
    <span class="s1">]</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;oob_score&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s3">True, </span><span class="s1">partial(f1_score</span><span class="s3">, </span><span class="s1">average=</span><span class="s5">&quot;micro&quot;</span><span class="s1">)])</span>
<span class="s3">def </span><span class="s1">test_forest_classifier_oob(</span>
    <span class="s1">ForestClassifier</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">X_type</span><span class="s3">, </span><span class="s1">lower_bound_accuracy</span><span class="s3">, </span><span class="s1">oob_score</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Check that OOB score is close to score on a test set.&quot;&quot;&quot;</span>
    <span class="s1">X = _convert_container(X</span><span class="s3">, </span><span class="s1">constructor_name=X_type)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">X</span><span class="s3">,</span>
        <span class="s1">y</span><span class="s3">,</span>
        <span class="s1">test_size=</span><span class="s4">0.5</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s1">classifier = ForestClassifier(</span>
        <span class="s1">n_estimators=</span><span class="s4">40</span><span class="s3">,</span>
        <span class="s1">bootstrap=</span><span class="s3">True,</span>
        <span class="s1">oob_score=oob_score</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s3">assert not </span><span class="s1">hasattr(classifier</span><span class="s3">, </span><span class="s5">&quot;oob_score_&quot;</span><span class="s1">)</span>
    <span class="s3">assert not </span><span class="s1">hasattr(classifier</span><span class="s3">, </span><span class="s5">&quot;oob_decision_function_&quot;</span><span class="s1">)</span>

    <span class="s1">classifier.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s3">if </span><span class="s1">callable(oob_score):</span>
        <span class="s1">test_score = oob_score(y_test</span><span class="s3">, </span><span class="s1">classifier.predict(X_test))</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">test_score = classifier.score(X_test</span><span class="s3">, </span><span class="s1">y_test)</span>
        <span class="s3">assert </span><span class="s1">classifier.oob_score_ &gt;= lower_bound_accuracy</span>

    <span class="s3">assert </span><span class="s1">abs(test_score - classifier.oob_score_) &lt;= </span><span class="s4">0.1</span>

    <span class="s3">assert </span><span class="s1">hasattr(classifier</span><span class="s3">, </span><span class="s5">&quot;oob_score_&quot;</span><span class="s1">)</span>
    <span class="s3">assert not </span><span class="s1">hasattr(classifier</span><span class="s3">, </span><span class="s5">&quot;oob_prediction_&quot;</span><span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">hasattr(classifier</span><span class="s3">, </span><span class="s5">&quot;oob_decision_function_&quot;</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">y.ndim == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">expected_shape = (X_train.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">len(set(y)))</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">expected_shape = (X_train.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">len(set(y[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]))</span><span class="s3">, </span><span class="s1">y.shape[</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s3">assert </span><span class="s1">classifier.oob_decision_function_.shape == expected_shape</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;ForestRegressor&quot;</span><span class="s3">, </span><span class="s1">FOREST_REGRESSORS.values())</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;X_type&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s5">&quot;array&quot;</span><span class="s3">, </span><span class="s5">&quot;sparse_csr&quot;</span><span class="s3">, </span><span class="s5">&quot;sparse_csc&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;X, y, lower_bound_r2&quot;</span><span class="s3">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">*datasets.make_regression(</span>
                <span class="s1">n_samples=</span><span class="s4">500</span><span class="s3">, </span><span class="s1">n_features=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">n_targets=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s4">0.7</span><span class="s3">,</span>
        <span class="s1">)</span><span class="s3">,</span>
        <span class="s1">(</span>
            <span class="s1">*datasets.make_regression(</span>
                <span class="s1">n_samples=</span><span class="s4">500</span><span class="s3">, </span><span class="s1">n_features=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">n_targets=</span><span class="s4">2</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s4">0.55</span><span class="s3">,</span>
        <span class="s1">)</span><span class="s3">,</span>
    <span class="s1">]</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;oob_score&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s3">True, </span><span class="s1">explained_variance_score])</span>
<span class="s3">def </span><span class="s1">test_forest_regressor_oob(ForestRegressor</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">X_type</span><span class="s3">, </span><span class="s1">lower_bound_r2</span><span class="s3">, </span><span class="s1">oob_score):</span>
    <span class="s0">&quot;&quot;&quot;Check that forest-based regressor provide an OOB score close to the 
    score on a test set.&quot;&quot;&quot;</span>
    <span class="s1">X = _convert_container(X</span><span class="s3">, </span><span class="s1">constructor_name=X_type)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">X</span><span class="s3">,</span>
        <span class="s1">y</span><span class="s3">,</span>
        <span class="s1">test_size=</span><span class="s4">0.5</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s1">regressor = ForestRegressor(</span>
        <span class="s1">n_estimators=</span><span class="s4">50</span><span class="s3">,</span>
        <span class="s1">bootstrap=</span><span class="s3">True,</span>
        <span class="s1">oob_score=oob_score</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s3">assert not </span><span class="s1">hasattr(regressor</span><span class="s3">, </span><span class="s5">&quot;oob_score_&quot;</span><span class="s1">)</span>
    <span class="s3">assert not </span><span class="s1">hasattr(regressor</span><span class="s3">, </span><span class="s5">&quot;oob_prediction_&quot;</span><span class="s1">)</span>

    <span class="s1">regressor.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s3">if </span><span class="s1">callable(oob_score):</span>
        <span class="s1">test_score = oob_score(y_test</span><span class="s3">, </span><span class="s1">regressor.predict(X_test))</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">test_score = regressor.score(X_test</span><span class="s3">, </span><span class="s1">y_test)</span>
        <span class="s3">assert </span><span class="s1">regressor.oob_score_ &gt;= lower_bound_r2</span>

    <span class="s3">assert </span><span class="s1">abs(test_score - regressor.oob_score_) &lt;= </span><span class="s4">0.1</span>

    <span class="s3">assert </span><span class="s1">hasattr(regressor</span><span class="s3">, </span><span class="s5">&quot;oob_score_&quot;</span><span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">hasattr(regressor</span><span class="s3">, </span><span class="s5">&quot;oob_prediction_&quot;</span><span class="s1">)</span>
    <span class="s3">assert not </span><span class="s1">hasattr(regressor</span><span class="s3">, </span><span class="s5">&quot;oob_decision_function_&quot;</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">y.ndim == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">expected_shape = (X_train.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">,</span><span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">expected_shape = (X_train.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">y.ndim)</span>
    <span class="s3">assert </span><span class="s1">regressor.oob_prediction_.shape == expected_shape</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;ForestEstimator&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS_REGRESSORS.values())</span>
<span class="s3">def </span><span class="s1">test_forest_oob_warning(ForestEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Check that a warning is raised when not enough estimator and the OOB 
    estimates will be inaccurate.&quot;&quot;&quot;</span>
    <span class="s1">estimator = ForestEstimator(</span>
        <span class="s1">n_estimators=</span><span class="s4">1</span><span class="s3">,</span>
        <span class="s1">oob_score=</span><span class="s3">True,</span>
        <span class="s1">bootstrap=</span><span class="s3">True,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s3">, </span><span class="s1">match=</span><span class="s5">&quot;Some inputs do not have OOB scores&quot;</span><span class="s1">):</span>
        <span class="s1">estimator.fit(iris.data</span><span class="s3">, </span><span class="s1">iris.target)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;ForestEstimator&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS_REGRESSORS.values())</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;X, y, params, err_msg&quot;</span><span class="s3">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">iris.data</span><span class="s3">,</span>
            <span class="s1">iris.target</span><span class="s3">,</span>
            <span class="s1">{</span><span class="s5">&quot;oob_score&quot;</span><span class="s1">: </span><span class="s3">True, </span><span class="s5">&quot;bootstrap&quot;</span><span class="s1">: </span><span class="s3">False</span><span class="s1">}</span><span class="s3">,</span>
            <span class="s5">&quot;Out of bag estimation only available if bootstrap=True&quot;</span><span class="s3">,</span>
        <span class="s1">)</span><span class="s3">,</span>
        <span class="s1">(</span>
            <span class="s1">iris.data</span><span class="s3">,</span>
            <span class="s1">rng.randint(low=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">high=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">size=(iris.data.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s4">2</span><span class="s1">))</span><span class="s3">,</span>
            <span class="s1">{</span><span class="s5">&quot;oob_score&quot;</span><span class="s1">: </span><span class="s3">True, </span><span class="s5">&quot;bootstrap&quot;</span><span class="s1">: </span><span class="s3">True</span><span class="s1">}</span><span class="s3">,</span>
            <span class="s5">&quot;The type of target cannot be used to compute OOB estimates&quot;</span><span class="s3">,</span>
        <span class="s1">)</span><span class="s3">,</span>
    <span class="s1">]</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">test_forest_oob_error(ForestEstimator</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">err_msg):</span>
    <span class="s1">estimator = ForestEstimator(**params)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError</span><span class="s3">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">estimator.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;oob_score&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s3">True, False</span><span class="s1">])</span>
<span class="s3">def </span><span class="s1">test_random_trees_embedding_raise_error_oob(oob_score):</span>
    <span class="s3">with </span><span class="s1">pytest.raises(TypeError</span><span class="s3">, </span><span class="s1">match=</span><span class="s5">&quot;got an unexpected keyword argument&quot;</span><span class="s1">):</span>
        <span class="s1">RandomTreesEmbedding(oob_score=oob_score)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(NotImplementedError</span><span class="s3">, </span><span class="s1">match=</span><span class="s5">&quot;OOB score not supported&quot;</span><span class="s1">):</span>
        <span class="s1">RandomTreesEmbedding()._set_oob_score_and_attributes(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">check_gridsearch(name):</span>
    <span class="s1">forest = FOREST_CLASSIFIERS[name]()</span>
    <span class="s1">clf = GridSearchCV(forest</span><span class="s3">, </span><span class="s1">{</span><span class="s5">&quot;n_estimators&quot;</span><span class="s1">: (</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span><span class="s3">, </span><span class="s5">&quot;max_depth&quot;</span><span class="s1">: (</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)})</span>
    <span class="s1">clf.fit(iris.data</span><span class="s3">, </span><span class="s1">iris.target)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS)</span>
<span class="s3">def </span><span class="s1">test_gridsearch(name):</span>
    <span class="s2"># Check that base trees can be grid-searched.</span>
    <span class="s1">check_gridsearch(name)</span>


<span class="s3">def </span><span class="s1">check_parallel(name</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
    <span class="s0">&quot;&quot;&quot;Check parallel computations in classification&quot;&quot;&quot;</span>
    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>
    <span class="s1">forest = ForestEstimator(n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">forest.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">len(forest) == </span><span class="s4">10</span>

    <span class="s1">forest.set_params(n_jobs=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">y1 = forest.predict(X)</span>
    <span class="s1">forest.set_params(n_jobs=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">y2 = forest.predict(X)</span>
    <span class="s1">assert_array_almost_equal(y1</span><span class="s3">, </span><span class="s1">y2</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS_REGRESSORS)</span>
<span class="s3">def </span><span class="s1">test_parallel(name):</span>
    <span class="s3">if </span><span class="s1">name </span><span class="s3">in </span><span class="s1">FOREST_CLASSIFIERS:</span>
        <span class="s1">X = iris.data</span>
        <span class="s1">y = iris.target</span>
    <span class="s3">elif </span><span class="s1">name </span><span class="s3">in </span><span class="s1">FOREST_REGRESSORS:</span>
        <span class="s1">X = X_reg</span>
        <span class="s1">y = y_reg</span>

    <span class="s1">check_parallel(name</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">check_pickle(name</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
    <span class="s2"># Check pickability.</span>

    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>
    <span class="s1">obj = ForestEstimator(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">obj.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">score = obj.score(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">pickle_object = pickle.dumps(obj)</span>

    <span class="s1">obj2 = pickle.loads(pickle_object)</span>
    <span class="s3">assert </span><span class="s1">type(obj2) == obj.__class__</span>
    <span class="s1">score2 = obj2.score(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">score == score2</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS_REGRESSORS)</span>
<span class="s3">def </span><span class="s1">test_pickle(name):</span>
    <span class="s3">if </span><span class="s1">name </span><span class="s3">in </span><span class="s1">FOREST_CLASSIFIERS:</span>
        <span class="s1">X = iris.data</span>
        <span class="s1">y = iris.target</span>
    <span class="s3">elif </span><span class="s1">name </span><span class="s3">in </span><span class="s1">FOREST_REGRESSORS:</span>
        <span class="s1">X = X_reg</span>
        <span class="s1">y = y_reg</span>

    <span class="s1">check_pickle(name</span><span class="s3">, </span><span class="s1">X[::</span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">y[::</span><span class="s4">2</span><span class="s1">])</span>


<span class="s3">def </span><span class="s1">check_multioutput(name):</span>
    <span class="s2"># Check estimators on multi-output problems.</span>

    <span class="s1">X_train = [</span>
        <span class="s1">[-</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">2</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">]</span>
    <span class="s1">y_train = [</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">3</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">3</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">3</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">]</span>
    <span class="s1">X_test = [[-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]]</span>
    <span class="s1">y_test = [[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">3</span><span class="s1">]]</span>

    <span class="s1">est = FOREST_ESTIMATORS[name](random_state=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">bootstrap=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">y_pred = est.fit(X_train</span><span class="s3">, </span><span class="s1">y_train).predict(X_test)</span>
    <span class="s1">assert_array_almost_equal(y_pred</span><span class="s3">, </span><span class="s1">y_test)</span>

    <span class="s3">if </span><span class="s1">name </span><span class="s3">in </span><span class="s1">FOREST_CLASSIFIERS:</span>
        <span class="s3">with </span><span class="s1">np.errstate(divide=</span><span class="s5">&quot;ignore&quot;</span><span class="s1">):</span>
            <span class="s1">proba = est.predict_proba(X_test)</span>
            <span class="s3">assert </span><span class="s1">len(proba) == </span><span class="s4">2</span>
            <span class="s3">assert </span><span class="s1">proba[</span><span class="s4">0</span><span class="s1">].shape == (</span><span class="s4">4</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span>
            <span class="s3">assert </span><span class="s1">proba[</span><span class="s4">1</span><span class="s1">].shape == (</span><span class="s4">4</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span>

            <span class="s1">log_proba = est.predict_log_proba(X_test)</span>
            <span class="s3">assert </span><span class="s1">len(log_proba) == </span><span class="s4">2</span>
            <span class="s3">assert </span><span class="s1">log_proba[</span><span class="s4">0</span><span class="s1">].shape == (</span><span class="s4">4</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span>
            <span class="s3">assert </span><span class="s1">log_proba[</span><span class="s4">1</span><span class="s1">].shape == (</span><span class="s4">4</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS_REGRESSORS)</span>
<span class="s3">def </span><span class="s1">test_multioutput(name):</span>
    <span class="s1">check_multioutput(name)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS)</span>
<span class="s3">def </span><span class="s1">test_multioutput_string(name):</span>
    <span class="s2"># Check estimators on multi-output problems with string outputs.</span>

    <span class="s1">X_train = [</span>
        <span class="s1">[-</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">2</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">]</span>
    <span class="s1">y_train = [</span>
        <span class="s1">[</span><span class="s5">&quot;red&quot;</span><span class="s3">, </span><span class="s5">&quot;blue&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;red&quot;</span><span class="s3">, </span><span class="s5">&quot;blue&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;red&quot;</span><span class="s3">, </span><span class="s5">&quot;blue&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;green&quot;</span><span class="s3">, </span><span class="s5">&quot;green&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;green&quot;</span><span class="s3">, </span><span class="s5">&quot;green&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;green&quot;</span><span class="s3">, </span><span class="s5">&quot;green&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;red&quot;</span><span class="s3">, </span><span class="s5">&quot;purple&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;red&quot;</span><span class="s3">, </span><span class="s5">&quot;purple&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;red&quot;</span><span class="s3">, </span><span class="s5">&quot;purple&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;green&quot;</span><span class="s3">, </span><span class="s5">&quot;yellow&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;green&quot;</span><span class="s3">, </span><span class="s5">&quot;yellow&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;green&quot;</span><span class="s3">, </span><span class="s5">&quot;yellow&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">]</span>
    <span class="s1">X_test = [[-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]]</span>
    <span class="s1">y_test = [</span>
        <span class="s1">[</span><span class="s5">&quot;red&quot;</span><span class="s3">, </span><span class="s5">&quot;blue&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;green&quot;</span><span class="s3">, </span><span class="s5">&quot;green&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;red&quot;</span><span class="s3">, </span><span class="s5">&quot;purple&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;green&quot;</span><span class="s3">, </span><span class="s5">&quot;yellow&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">]</span>

    <span class="s1">est = FOREST_ESTIMATORS[name](random_state=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">bootstrap=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">y_pred = est.fit(X_train</span><span class="s3">, </span><span class="s1">y_train).predict(X_test)</span>
    <span class="s1">assert_array_equal(y_pred</span><span class="s3">, </span><span class="s1">y_test)</span>

    <span class="s3">with </span><span class="s1">np.errstate(divide=</span><span class="s5">&quot;ignore&quot;</span><span class="s1">):</span>
        <span class="s1">proba = est.predict_proba(X_test)</span>
        <span class="s3">assert </span><span class="s1">len(proba) == </span><span class="s4">2</span>
        <span class="s3">assert </span><span class="s1">proba[</span><span class="s4">0</span><span class="s1">].shape == (</span><span class="s4">4</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s3">assert </span><span class="s1">proba[</span><span class="s4">1</span><span class="s1">].shape == (</span><span class="s4">4</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span>

        <span class="s1">log_proba = est.predict_log_proba(X_test)</span>
        <span class="s3">assert </span><span class="s1">len(log_proba) == </span><span class="s4">2</span>
        <span class="s3">assert </span><span class="s1">log_proba[</span><span class="s4">0</span><span class="s1">].shape == (</span><span class="s4">4</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s3">assert </span><span class="s1">log_proba[</span><span class="s4">1</span><span class="s1">].shape == (</span><span class="s4">4</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">check_classes_shape(name):</span>
    <span class="s2"># Test that n_classes_ and classes_ have proper shape.</span>
    <span class="s1">ForestClassifier = FOREST_CLASSIFIERS[name]</span>

    <span class="s2"># Classification, single output</span>
    <span class="s1">clf = ForestClassifier(random_state=</span><span class="s4">0</span><span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s3">assert </span><span class="s1">clf.n_classes_ == </span><span class="s4">2</span>
    <span class="s1">assert_array_equal(clf.classes_</span><span class="s3">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">])</span>

    <span class="s2"># Classification, multi-output</span>
    <span class="s1">_y = np.vstack((y</span><span class="s3">, </span><span class="s1">np.array(y) * </span><span class="s4">2</span><span class="s1">)).T</span>
    <span class="s1">clf = ForestClassifier(random_state=</span><span class="s4">0</span><span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">_y)</span>

    <span class="s1">assert_array_equal(clf.n_classes_</span><span class="s3">, </span><span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(clf.classes_</span><span class="s3">, </span><span class="s1">[[-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[-</span><span class="s4">2</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS)</span>
<span class="s3">def </span><span class="s1">test_classes_shape(name):</span>
    <span class="s1">check_classes_shape(name)</span>


<span class="s3">def </span><span class="s1">test_random_trees_dense_type():</span>
    <span class="s2"># Test that the `sparse_output` parameter of RandomTreesEmbedding</span>
    <span class="s2"># works by returning a dense array.</span>

    <span class="s2"># Create the RTE with sparse=False</span>
    <span class="s1">hasher = RandomTreesEmbedding(n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">sparse_output=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = datasets.make_circles(factor=</span><span class="s4">0.5</span><span class="s1">)</span>
    <span class="s1">X_transformed = hasher.fit_transform(X)</span>

    <span class="s2"># Assert that type is ndarray, not scipy.sparse.csr_matrix</span>
    <span class="s3">assert </span><span class="s1">type(X_transformed) == np.ndarray</span>


<span class="s3">def </span><span class="s1">test_random_trees_dense_equal():</span>
    <span class="s2"># Test that the `sparse_output` parameter of RandomTreesEmbedding</span>
    <span class="s2"># works by returning the same array for both argument values.</span>

    <span class="s2"># Create the RTEs</span>
    <span class="s1">hasher_dense = RandomTreesEmbedding(</span>
        <span class="s1">n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">sparse_output=</span><span class="s3">False, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">hasher_sparse = RandomTreesEmbedding(</span>
        <span class="s1">n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">sparse_output=</span><span class="s3">True, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = datasets.make_circles(factor=</span><span class="s4">0.5</span><span class="s1">)</span>
    <span class="s1">X_transformed_dense = hasher_dense.fit_transform(X)</span>
    <span class="s1">X_transformed_sparse = hasher_sparse.fit_transform(X)</span>

    <span class="s2"># Assert that dense and sparse hashers have same array.</span>
    <span class="s1">assert_array_equal(X_transformed_sparse.toarray()</span><span class="s3">, </span><span class="s1">X_transformed_dense)</span>


<span class="s2"># Ignore warnings from switching to more power iterations in randomized_svd</span>
<span class="s1">@ignore_warnings</span>
<span class="s3">def </span><span class="s1">test_random_hasher():</span>
    <span class="s2"># test random forest hashing on circles dataset</span>
    <span class="s2"># make sure that it is linearly separable.</span>
    <span class="s2"># even after projected to two SVD dimensions</span>
    <span class="s2"># Note: Not all random_states produce perfect results.</span>
    <span class="s1">hasher = RandomTreesEmbedding(n_estimators=</span><span class="s4">30</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = datasets.make_circles(factor=</span><span class="s4">0.5</span><span class="s1">)</span>
    <span class="s1">X_transformed = hasher.fit_transform(X)</span>

    <span class="s2"># test fit and transform:</span>
    <span class="s1">hasher = RandomTreesEmbedding(n_estimators=</span><span class="s4">30</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">assert_array_equal(hasher.fit(X).transform(X).toarray()</span><span class="s3">, </span><span class="s1">X_transformed.toarray())</span>

    <span class="s2"># one leaf active per data point per forest</span>
    <span class="s3">assert </span><span class="s1">X_transformed.shape[</span><span class="s4">0</span><span class="s1">] == X.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">assert_array_equal(X_transformed.sum(axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">hasher.n_estimators)</span>
    <span class="s1">svd = TruncatedSVD(n_components=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">X_reduced = svd.fit_transform(X_transformed)</span>
    <span class="s1">linear_clf = LinearSVC()</span>
    <span class="s1">linear_clf.fit(X_reduced</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">linear_clf.score(X_reduced</span><span class="s3">, </span><span class="s1">y) == </span><span class="s4">1.0</span>


<span class="s3">def </span><span class="s1">test_random_hasher_sparse_data():</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = datasets.make_multilabel_classification(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">hasher = RandomTreesEmbedding(n_estimators=</span><span class="s4">30</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">X_transformed = hasher.fit_transform(X)</span>
    <span class="s1">X_transformed_sparse = hasher.fit_transform(csc_matrix(X))</span>
    <span class="s1">assert_array_equal(X_transformed_sparse.toarray()</span><span class="s3">, </span><span class="s1">X_transformed.toarray())</span>


<span class="s3">def </span><span class="s1">test_parallel_train():</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">12321</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = </span><span class="s4">80</span><span class="s3">, </span><span class="s4">30</span>
    <span class="s1">X_train = rng.randn(n_samples</span><span class="s3">, </span><span class="s1">n_features)</span>
    <span class="s1">y_train = rng.randint(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s1">n_samples)</span>

    <span class="s1">clfs = [</span>
        <span class="s1">RandomForestClassifier(n_estimators=</span><span class="s4">20</span><span class="s3">, </span><span class="s1">n_jobs=n_jobs</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">12345</span><span class="s1">).fit(</span>
            <span class="s1">X_train</span><span class="s3">, </span><span class="s1">y_train</span>
        <span class="s1">)</span>
        <span class="s3">for </span><span class="s1">n_jobs </span><span class="s3">in </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, </span><span class="s4">8</span><span class="s3">, </span><span class="s4">16</span><span class="s3">, </span><span class="s4">32</span><span class="s1">]</span>
    <span class="s1">]</span>

    <span class="s1">X_test = rng.randn(n_samples</span><span class="s3">, </span><span class="s1">n_features)</span>
    <span class="s1">probas = [clf.predict_proba(X_test) </span><span class="s3">for </span><span class="s1">clf </span><span class="s3">in </span><span class="s1">clfs]</span>
    <span class="s3">for </span><span class="s1">proba1</span><span class="s3">, </span><span class="s1">proba2 </span><span class="s3">in </span><span class="s1">zip(probas</span><span class="s3">, </span><span class="s1">probas[</span><span class="s4">1</span><span class="s1">:]):</span>
        <span class="s1">assert_array_almost_equal(proba1</span><span class="s3">, </span><span class="s1">proba2)</span>


<span class="s3">def </span><span class="s1">test_distribution():</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">12321</span><span class="s1">)</span>

    <span class="s2"># Single variable with 4 values</span>
    <span class="s1">X = rng.randint(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">4</span><span class="s3">, </span><span class="s1">size=(</span><span class="s4">1000</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">y = rng.rand(</span><span class="s4">1000</span><span class="s1">)</span>
    <span class="s1">n_trees = </span><span class="s4">500</span>

    <span class="s1">reg = ExtraTreesRegressor(n_estimators=n_trees</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">uniques = defaultdict(int)</span>
    <span class="s3">for </span><span class="s1">tree </span><span class="s3">in </span><span class="s1">reg.estimators_:</span>
        <span class="s1">tree = </span><span class="s5">&quot;&quot;</span><span class="s1">.join(</span>
            <span class="s1">(</span><span class="s5">&quot;%d,%d/&quot; </span><span class="s1">% (f</span><span class="s3">, </span><span class="s1">int(t)) </span><span class="s3">if </span><span class="s1">f &gt;= </span><span class="s4">0 </span><span class="s3">else </span><span class="s5">&quot;-&quot;</span><span class="s1">)</span>
            <span class="s3">for </span><span class="s1">f</span><span class="s3">, </span><span class="s1">t </span><span class="s3">in </span><span class="s1">zip(tree.tree_.feature</span><span class="s3">, </span><span class="s1">tree.tree_.threshold)</span>
        <span class="s1">)</span>

        <span class="s1">uniques[tree] += </span><span class="s4">1</span>

    <span class="s1">uniques = sorted([(</span><span class="s4">1.0 </span><span class="s1">* count / n_trees</span><span class="s3">, </span><span class="s1">tree) </span><span class="s3">for </span><span class="s1">tree</span><span class="s3">, </span><span class="s1">count </span><span class="s3">in </span><span class="s1">uniques.items()])</span>

    <span class="s2"># On a single variable problem where X_0 has 4 equiprobable values, there</span>
    <span class="s2"># are 5 ways to build a random tree. The more compact (0,1/0,0/--0,2/--) of</span>
    <span class="s2"># them has probability 1/3 while the 4 others have probability 1/6.</span>

    <span class="s3">assert </span><span class="s1">len(uniques) == </span><span class="s4">5</span>
    <span class="s3">assert </span><span class="s4">0.20 </span><span class="s1">&gt; uniques[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]  </span><span class="s2"># Rough approximation of 1/6.</span>
    <span class="s3">assert </span><span class="s4">0.20 </span><span class="s1">&gt; uniques[</span><span class="s4">1</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s3">assert </span><span class="s4">0.20 </span><span class="s1">&gt; uniques[</span><span class="s4">2</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s3">assert </span><span class="s4">0.20 </span><span class="s1">&gt; uniques[</span><span class="s4">3</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s3">assert </span><span class="s1">uniques[</span><span class="s4">4</span><span class="s1">][</span><span class="s4">0</span><span class="s1">] &gt; </span><span class="s4">0.3</span>
    <span class="s3">assert </span><span class="s1">uniques[</span><span class="s4">4</span><span class="s1">][</span><span class="s4">1</span><span class="s1">] == </span><span class="s5">&quot;0,1/0,0/--0,2/--&quot;</span>

    <span class="s2"># Two variables, one with 2 values, one with 3 values</span>
    <span class="s1">X = np.empty((</span><span class="s4">1000</span><span class="s3">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">X[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">] = np.random.randint(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">1000</span><span class="s1">)</span>
    <span class="s1">X[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">] = np.random.randint(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, </span><span class="s4">1000</span><span class="s1">)</span>
    <span class="s1">y = rng.rand(</span><span class="s4">1000</span><span class="s1">)</span>

    <span class="s1">reg = ExtraTreesRegressor(max_features=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">uniques = defaultdict(int)</span>
    <span class="s3">for </span><span class="s1">tree </span><span class="s3">in </span><span class="s1">reg.estimators_:</span>
        <span class="s1">tree = </span><span class="s5">&quot;&quot;</span><span class="s1">.join(</span>
            <span class="s1">(</span><span class="s5">&quot;%d,%d/&quot; </span><span class="s1">% (f</span><span class="s3">, </span><span class="s1">int(t)) </span><span class="s3">if </span><span class="s1">f &gt;= </span><span class="s4">0 </span><span class="s3">else </span><span class="s5">&quot;-&quot;</span><span class="s1">)</span>
            <span class="s3">for </span><span class="s1">f</span><span class="s3">, </span><span class="s1">t </span><span class="s3">in </span><span class="s1">zip(tree.tree_.feature</span><span class="s3">, </span><span class="s1">tree.tree_.threshold)</span>
        <span class="s1">)</span>

        <span class="s1">uniques[tree] += </span><span class="s4">1</span>

    <span class="s1">uniques = [(count</span><span class="s3">, </span><span class="s1">tree) </span><span class="s3">for </span><span class="s1">tree</span><span class="s3">, </span><span class="s1">count </span><span class="s3">in </span><span class="s1">uniques.items()]</span>
    <span class="s3">assert </span><span class="s1">len(uniques) == </span><span class="s4">8</span>


<span class="s3">def </span><span class="s1">check_max_leaf_nodes_max_depth(name):</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = hastie_X</span><span class="s3">, </span><span class="s1">hastie_y</span>

    <span class="s2"># Test precedence of max_leaf_nodes over max_depth.</span>
    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>
    <span class="s1">est = ForestEstimator(</span>
        <span class="s1">max_depth=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">max_leaf_nodes=</span><span class="s4">4</span><span class="s3">, </span><span class="s1">n_estimators=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">est.estimators_[</span><span class="s4">0</span><span class="s1">].get_depth() == </span><span class="s4">1</span>

    <span class="s1">est = ForestEstimator(max_depth=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">n_estimators=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">est.estimators_[</span><span class="s4">0</span><span class="s1">].get_depth() == </span><span class="s4">1</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_ESTIMATORS)</span>
<span class="s3">def </span><span class="s1">test_max_leaf_nodes_max_depth(name):</span>
    <span class="s1">check_max_leaf_nodes_max_depth(name)</span>


<span class="s3">def </span><span class="s1">check_min_samples_split(name):</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = hastie_X</span><span class="s3">, </span><span class="s1">hastie_y</span>
    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>

    <span class="s1">est = ForestEstimator(min_samples_split=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">n_estimators=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">node_idx = est.estimators_[</span><span class="s4">0</span><span class="s1">].tree_.children_left != -</span><span class="s4">1</span>
    <span class="s1">node_samples = est.estimators_[</span><span class="s4">0</span><span class="s1">].tree_.n_node_samples[node_idx]</span>

    <span class="s3">assert </span><span class="s1">np.min(node_samples) &gt; len(X) * </span><span class="s4">0.5 </span><span class="s1">- </span><span class="s4">1</span><span class="s3">, </span><span class="s5">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span>

    <span class="s1">est = ForestEstimator(min_samples_split=</span><span class="s4">0.5</span><span class="s3">, </span><span class="s1">n_estimators=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">node_idx = est.estimators_[</span><span class="s4">0</span><span class="s1">].tree_.children_left != -</span><span class="s4">1</span>
    <span class="s1">node_samples = est.estimators_[</span><span class="s4">0</span><span class="s1">].tree_.n_node_samples[node_idx]</span>

    <span class="s3">assert </span><span class="s1">np.min(node_samples) &gt; len(X) * </span><span class="s4">0.5 </span><span class="s1">- </span><span class="s4">1</span><span class="s3">, </span><span class="s5">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_ESTIMATORS)</span>
<span class="s3">def </span><span class="s1">test_min_samples_split(name):</span>
    <span class="s1">check_min_samples_split(name)</span>


<span class="s3">def </span><span class="s1">check_min_samples_leaf(name):</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = hastie_X</span><span class="s3">, </span><span class="s1">hastie_y</span>

    <span class="s2"># Test if leaves contain more than leaf_count training examples</span>
    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>

    <span class="s1">est = ForestEstimator(min_samples_leaf=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">n_estimators=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">out = est.estimators_[</span><span class="s4">0</span><span class="s1">].tree_.apply(X)</span>
    <span class="s1">node_counts = np.bincount(out)</span>
    <span class="s2"># drop inner nodes</span>
    <span class="s1">leaf_count = node_counts[node_counts != </span><span class="s4">0</span><span class="s1">]</span>
    <span class="s3">assert </span><span class="s1">np.min(leaf_count) &gt; </span><span class="s4">4</span><span class="s3">, </span><span class="s5">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span>

    <span class="s1">est = ForestEstimator(min_samples_leaf=</span><span class="s4">0.25</span><span class="s3">, </span><span class="s1">n_estimators=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">out = est.estimators_[</span><span class="s4">0</span><span class="s1">].tree_.apply(X)</span>
    <span class="s1">node_counts = np.bincount(out)</span>
    <span class="s2"># drop inner nodes</span>
    <span class="s1">leaf_count = node_counts[node_counts != </span><span class="s4">0</span><span class="s1">]</span>
    <span class="s3">assert </span><span class="s1">np.min(leaf_count) &gt; len(X) * </span><span class="s4">0.25 </span><span class="s1">- </span><span class="s4">1</span><span class="s3">, </span><span class="s5">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_ESTIMATORS)</span>
<span class="s3">def </span><span class="s1">test_min_samples_leaf(name):</span>
    <span class="s1">check_min_samples_leaf(name)</span>


<span class="s3">def </span><span class="s1">check_min_weight_fraction_leaf(name):</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = hastie_X</span><span class="s3">, </span><span class="s1">hastie_y</span>

    <span class="s2"># Test if leaves contain at least min_weight_fraction_leaf of the</span>
    <span class="s2"># training set</span>
    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">weights = rng.rand(X.shape[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">total_weight = np.sum(weights)</span>

    <span class="s2"># test both DepthFirstTreeBuilder and BestFirstTreeBuilder</span>
    <span class="s2"># by setting max_leaf_nodes</span>
    <span class="s3">for </span><span class="s1">frac </span><span class="s3">in </span><span class="s1">np.linspace(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">0.5</span><span class="s3">, </span><span class="s4">6</span><span class="s1">):</span>
        <span class="s1">est = ForestEstimator(</span>
            <span class="s1">min_weight_fraction_leaf=frac</span><span class="s3">, </span><span class="s1">n_estimators=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
        <span class="s1">)</span>
        <span class="s3">if </span><span class="s5">&quot;RandomForest&quot; </span><span class="s3">in </span><span class="s1">name:</span>
            <span class="s1">est.bootstrap = </span><span class="s3">False</span>

        <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=weights)</span>
        <span class="s1">out = est.estimators_[</span><span class="s4">0</span><span class="s1">].tree_.apply(X)</span>
        <span class="s1">node_weights = np.bincount(out</span><span class="s3">, </span><span class="s1">weights=weights)</span>
        <span class="s2"># drop inner nodes</span>
        <span class="s1">leaf_weights = node_weights[node_weights != </span><span class="s4">0</span><span class="s1">]</span>
        <span class="s3">assert </span><span class="s1">(</span>
            <span class="s1">np.min(leaf_weights) &gt;= total_weight * est.min_weight_fraction_leaf</span>
        <span class="s1">)</span><span class="s3">, </span><span class="s5">&quot;Failed with {0} min_weight_fraction_leaf={1}&quot;</span><span class="s1">.format(</span>
            <span class="s1">name</span><span class="s3">, </span><span class="s1">est.min_weight_fraction_leaf</span>
        <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_ESTIMATORS)</span>
<span class="s3">def </span><span class="s1">test_min_weight_fraction_leaf(name):</span>
    <span class="s1">check_min_weight_fraction_leaf(name)</span>


<span class="s3">def </span><span class="s1">check_sparse_input(name</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">X_sparse</span><span class="s3">, </span><span class="s1">y):</span>
    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>

    <span class="s1">dense = ForestEstimator(random_state=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">max_depth=</span><span class="s4">2</span><span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">sparse = ForestEstimator(random_state=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">max_depth=</span><span class="s4">2</span><span class="s1">).fit(X_sparse</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">assert_array_almost_equal(sparse.apply(X)</span><span class="s3">, </span><span class="s1">dense.apply(X))</span>

    <span class="s3">if </span><span class="s1">name </span><span class="s3">in </span><span class="s1">FOREST_CLASSIFIERS </span><span class="s3">or </span><span class="s1">name </span><span class="s3">in </span><span class="s1">FOREST_REGRESSORS:</span>
        <span class="s1">assert_array_almost_equal(sparse.predict(X)</span><span class="s3">, </span><span class="s1">dense.predict(X))</span>
        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">sparse.feature_importances_</span><span class="s3">, </span><span class="s1">dense.feature_importances_</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">name </span><span class="s3">in </span><span class="s1">FOREST_CLASSIFIERS:</span>
        <span class="s1">assert_array_almost_equal(sparse.predict_proba(X)</span><span class="s3">, </span><span class="s1">dense.predict_proba(X))</span>
        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">sparse.predict_log_proba(X)</span><span class="s3">, </span><span class="s1">dense.predict_log_proba(X)</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">name </span><span class="s3">in </span><span class="s1">FOREST_TRANSFORMERS:</span>
        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">sparse.transform(X).toarray()</span><span class="s3">, </span><span class="s1">dense.transform(X).toarray()</span>
        <span class="s1">)</span>
        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">sparse.fit_transform(X).toarray()</span><span class="s3">, </span><span class="s1">dense.fit_transform(X).toarray()</span>
        <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_ESTIMATORS)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;sparse_matrix&quot;</span><span class="s3">, </span><span class="s1">(csr_matrix</span><span class="s3">, </span><span class="s1">csc_matrix</span><span class="s3">, </span><span class="s1">coo_matrix))</span>
<span class="s3">def </span><span class="s1">test_sparse_input(name</span><span class="s3">, </span><span class="s1">sparse_matrix):</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = datasets.make_multilabel_classification(random_state=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">n_samples=</span><span class="s4">50</span><span class="s1">)</span>

    <span class="s1">check_sparse_input(name</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">sparse_matrix(X)</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">check_memory_layout(name</span><span class="s3">, </span><span class="s1">dtype):</span>
    <span class="s2"># Check that it works no matter the memory layout</span>

    <span class="s1">est = FOREST_ESTIMATORS[name](random_state=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">bootstrap=</span><span class="s3">False</span><span class="s1">)</span>

    <span class="s2"># Nothing</span>
    <span class="s1">X = np.asarray(iris.data</span><span class="s3">, </span><span class="s1">dtype=dtype)</span>
    <span class="s1">y = iris.target</span>
    <span class="s1">assert_array_almost_equal(est.fit(X</span><span class="s3">, </span><span class="s1">y).predict(X)</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s2"># C-order</span>
    <span class="s1">X = np.asarray(iris.data</span><span class="s3">, </span><span class="s1">order=</span><span class="s5">&quot;C&quot;</span><span class="s3">, </span><span class="s1">dtype=dtype)</span>
    <span class="s1">y = iris.target</span>
    <span class="s1">assert_array_almost_equal(est.fit(X</span><span class="s3">, </span><span class="s1">y).predict(X)</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s2"># F-order</span>
    <span class="s1">X = np.asarray(iris.data</span><span class="s3">, </span><span class="s1">order=</span><span class="s5">&quot;F&quot;</span><span class="s3">, </span><span class="s1">dtype=dtype)</span>
    <span class="s1">y = iris.target</span>
    <span class="s1">assert_array_almost_equal(est.fit(X</span><span class="s3">, </span><span class="s1">y).predict(X)</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s2"># Contiguous</span>
    <span class="s1">X = np.ascontiguousarray(iris.data</span><span class="s3">, </span><span class="s1">dtype=dtype)</span>
    <span class="s1">y = iris.target</span>
    <span class="s1">assert_array_almost_equal(est.fit(X</span><span class="s3">, </span><span class="s1">y).predict(X)</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s3">if </span><span class="s1">est.estimator.splitter </span><span class="s3">in </span><span class="s1">SPARSE_SPLITTERS:</span>
        <span class="s2"># csr matrix</span>
        <span class="s1">X = csr_matrix(iris.data</span><span class="s3">, </span><span class="s1">dtype=dtype)</span>
        <span class="s1">y = iris.target</span>
        <span class="s1">assert_array_almost_equal(est.fit(X</span><span class="s3">, </span><span class="s1">y).predict(X)</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s2"># csc_matrix</span>
        <span class="s1">X = csc_matrix(iris.data</span><span class="s3">, </span><span class="s1">dtype=dtype)</span>
        <span class="s1">y = iris.target</span>
        <span class="s1">assert_array_almost_equal(est.fit(X</span><span class="s3">, </span><span class="s1">y).predict(X)</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s2"># coo_matrix</span>
        <span class="s1">X = coo_matrix(iris.data</span><span class="s3">, </span><span class="s1">dtype=dtype)</span>
        <span class="s1">y = iris.target</span>
        <span class="s1">assert_array_almost_equal(est.fit(X</span><span class="s3">, </span><span class="s1">y).predict(X)</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s2"># Strided</span>
    <span class="s1">X = np.asarray(iris.data[::</span><span class="s4">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=dtype)</span>
    <span class="s1">y = iris.target[::</span><span class="s4">3</span><span class="s1">]</span>
    <span class="s1">assert_array_almost_equal(est.fit(X</span><span class="s3">, </span><span class="s1">y).predict(X)</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS_REGRESSORS)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;dtype&quot;</span><span class="s3">, </span><span class="s1">(np.float64</span><span class="s3">, </span><span class="s1">np.float32))</span>
<span class="s3">def </span><span class="s1">test_memory_layout(name</span><span class="s3">, </span><span class="s1">dtype):</span>
    <span class="s1">check_memory_layout(name</span><span class="s3">, </span><span class="s1">dtype)</span>


<span class="s1">@ignore_warnings</span>
<span class="s3">def </span><span class="s1">check_1d_input(name</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">X_2d</span><span class="s3">, </span><span class="s1">y):</span>
    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">ForestEstimator(n_estimators=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">est = ForestEstimator(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">est.fit(X_2d</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s3">if </span><span class="s1">name </span><span class="s3">in </span><span class="s1">FOREST_CLASSIFIERS </span><span class="s3">or </span><span class="s1">name </span><span class="s3">in </span><span class="s1">FOREST_REGRESSORS:</span>
        <span class="s3">with </span><span class="s1">pytest.raises(ValueError):</span>
            <span class="s1">est.predict(X)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_ESTIMATORS)</span>
<span class="s3">def </span><span class="s1">test_1d_input(name):</span>
    <span class="s1">X = iris.data[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">X_2d = iris.data[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">].reshape((-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">y = iris.target</span>

    <span class="s3">with </span><span class="s1">ignore_warnings():</span>
        <span class="s1">check_1d_input(name</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">X_2d</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">check_class_weights(name):</span>
    <span class="s2"># Check class_weights resemble sample_weights behavior.</span>
    <span class="s1">ForestClassifier = FOREST_CLASSIFIERS[name]</span>

    <span class="s2"># Iris is balanced, so no effect expected for using 'balanced' weights</span>
    <span class="s1">clf1 = ForestClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf1.fit(iris.data</span><span class="s3">, </span><span class="s1">iris.target)</span>
    <span class="s1">clf2 = ForestClassifier(class_weight=</span><span class="s5">&quot;balanced&quot;</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf2.fit(iris.data</span><span class="s3">, </span><span class="s1">iris.target)</span>
    <span class="s1">assert_almost_equal(clf1.feature_importances_</span><span class="s3">, </span><span class="s1">clf2.feature_importances_)</span>

    <span class="s2"># Make a multi-output problem with three copies of Iris</span>
    <span class="s1">iris_multi = np.vstack((iris.target</span><span class="s3">, </span><span class="s1">iris.target</span><span class="s3">, </span><span class="s1">iris.target)).T</span>
    <span class="s2"># Create user-defined weights that should balance over the outputs</span>
    <span class="s1">clf3 = ForestClassifier(</span>
        <span class="s1">class_weight=[</span>
            <span class="s1">{</span><span class="s4">0</span><span class="s1">: </span><span class="s4">2.0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">: </span><span class="s4">2.0</span><span class="s3">, </span><span class="s4">2</span><span class="s1">: </span><span class="s4">1.0</span><span class="s1">}</span><span class="s3">,</span>
            <span class="s1">{</span><span class="s4">0</span><span class="s1">: </span><span class="s4">2.0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">: </span><span class="s4">1.0</span><span class="s3">, </span><span class="s4">2</span><span class="s1">: </span><span class="s4">2.0</span><span class="s1">}</span><span class="s3">,</span>
            <span class="s1">{</span><span class="s4">0</span><span class="s1">: </span><span class="s4">1.0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">: </span><span class="s4">2.0</span><span class="s3">, </span><span class="s4">2</span><span class="s1">: </span><span class="s4">2.0</span><span class="s1">}</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s1">clf3.fit(iris.data</span><span class="s3">, </span><span class="s1">iris_multi)</span>
    <span class="s1">assert_almost_equal(clf2.feature_importances_</span><span class="s3">, </span><span class="s1">clf3.feature_importances_)</span>
    <span class="s2"># Check against multi-output &quot;balanced&quot; which should also have no effect</span>
    <span class="s1">clf4 = ForestClassifier(class_weight=</span><span class="s5">&quot;balanced&quot;</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf4.fit(iris.data</span><span class="s3">, </span><span class="s1">iris_multi)</span>
    <span class="s1">assert_almost_equal(clf3.feature_importances_</span><span class="s3">, </span><span class="s1">clf4.feature_importances_)</span>

    <span class="s2"># Inflate importance of class 1, check against user-defined weights</span>
    <span class="s1">sample_weight = np.ones(iris.target.shape)</span>
    <span class="s1">sample_weight[iris.target == </span><span class="s4">1</span><span class="s1">] *= </span><span class="s4">100</span>
    <span class="s1">class_weight = {</span><span class="s4">0</span><span class="s1">: </span><span class="s4">1.0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">: </span><span class="s4">100.0</span><span class="s3">, </span><span class="s4">2</span><span class="s1">: </span><span class="s4">1.0</span><span class="s1">}</span>
    <span class="s1">clf1 = ForestClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf1.fit(iris.data</span><span class="s3">, </span><span class="s1">iris.target</span><span class="s3">, </span><span class="s1">sample_weight)</span>
    <span class="s1">clf2 = ForestClassifier(class_weight=class_weight</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf2.fit(iris.data</span><span class="s3">, </span><span class="s1">iris.target)</span>
    <span class="s1">assert_almost_equal(clf1.feature_importances_</span><span class="s3">, </span><span class="s1">clf2.feature_importances_)</span>

    <span class="s2"># Check that sample_weight and class_weight are multiplicative</span>
    <span class="s1">clf1 = ForestClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf1.fit(iris.data</span><span class="s3">, </span><span class="s1">iris.target</span><span class="s3">, </span><span class="s1">sample_weight**</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">clf2 = ForestClassifier(class_weight=class_weight</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf2.fit(iris.data</span><span class="s3">, </span><span class="s1">iris.target</span><span class="s3">, </span><span class="s1">sample_weight)</span>
    <span class="s1">assert_almost_equal(clf1.feature_importances_</span><span class="s3">, </span><span class="s1">clf2.feature_importances_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS)</span>
<span class="s3">def </span><span class="s1">test_class_weights(name):</span>
    <span class="s1">check_class_weights(name)</span>


<span class="s3">def </span><span class="s1">check_class_weight_balanced_and_bootstrap_multi_output(name):</span>
    <span class="s2"># Test class_weight works for multi-output&quot;&quot;&quot;</span>
    <span class="s1">ForestClassifier = FOREST_CLASSIFIERS[name]</span>
    <span class="s1">_y = np.vstack((y</span><span class="s3">, </span><span class="s1">np.array(y) * </span><span class="s4">2</span><span class="s1">)).T</span>
    <span class="s1">clf = ForestClassifier(class_weight=</span><span class="s5">&quot;balanced&quot;</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">_y)</span>
    <span class="s1">clf = ForestClassifier(</span>
        <span class="s1">class_weight=[{-</span><span class="s4">1</span><span class="s1">: </span><span class="s4">0.5</span><span class="s3">, </span><span class="s4">1</span><span class="s1">: </span><span class="s4">1.0</span><span class="s1">}</span><span class="s3">, </span><span class="s1">{-</span><span class="s4">2</span><span class="s1">: </span><span class="s4">1.0</span><span class="s3">, </span><span class="s4">2</span><span class="s1">: </span><span class="s4">1.0</span><span class="s1">}]</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">_y)</span>
    <span class="s2"># smoke test for balanced subsample</span>
    <span class="s1">clf = ForestClassifier(class_weight=</span><span class="s5">&quot;balanced_subsample&quot;</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">_y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS)</span>
<span class="s3">def </span><span class="s1">test_class_weight_balanced_and_bootstrap_multi_output(name):</span>
    <span class="s1">check_class_weight_balanced_and_bootstrap_multi_output(name)</span>


<span class="s3">def </span><span class="s1">check_class_weight_errors(name):</span>
    <span class="s2"># Test if class_weight raises errors and warnings when expected.</span>
    <span class="s1">ForestClassifier = FOREST_CLASSIFIERS[name]</span>
    <span class="s1">_y = np.vstack((y</span><span class="s3">, </span><span class="s1">np.array(y) * </span><span class="s4">2</span><span class="s1">)).T</span>

    <span class="s2"># Warning warm_start with preset</span>
    <span class="s1">clf = ForestClassifier(class_weight=</span><span class="s5">&quot;balanced&quot;</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">warn_msg = (</span>
        <span class="s5">&quot;Warm-start fitting without increasing n_estimators does not fit new trees.&quot;</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s3">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">_y)</span>

    <span class="s2"># Incorrect length list for multi-output</span>
    <span class="s1">clf = ForestClassifier(class_weight=[{-</span><span class="s4">1</span><span class="s1">: </span><span class="s4">0.5</span><span class="s3">, </span><span class="s4">1</span><span class="s1">: </span><span class="s4">1.0</span><span class="s1">}]</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">_y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS)</span>
<span class="s3">def </span><span class="s1">test_class_weight_errors(name):</span>
    <span class="s1">check_class_weight_errors(name)</span>


<span class="s3">def </span><span class="s1">check_warm_start(name</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">):</span>
    <span class="s2"># Test if fitting incrementally with warm start gives a forest of the</span>
    <span class="s2"># right size and the same results as a normal fit.</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = hastie_X</span><span class="s3">, </span><span class="s1">hastie_y</span>
    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>
    <span class="s1">est_ws = </span><span class="s3">None</span>
    <span class="s3">for </span><span class="s1">n_estimators </span><span class="s3">in </span><span class="s1">[</span><span class="s4">5</span><span class="s3">, </span><span class="s4">10</span><span class="s1">]:</span>
        <span class="s3">if </span><span class="s1">est_ws </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">est_ws = ForestEstimator(</span>
                <span class="s1">n_estimators=n_estimators</span><span class="s3">, </span><span class="s1">random_state=random_state</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True</span>
            <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">est_ws.set_params(n_estimators=n_estimators)</span>
        <span class="s1">est_ws.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s3">assert </span><span class="s1">len(est_ws) == n_estimators</span>

    <span class="s1">est_no_ws = ForestEstimator(</span>
        <span class="s1">n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">random_state=random_state</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">False</span>
    <span class="s1">)</span>
    <span class="s1">est_no_ws.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s3">assert </span><span class="s1">set([tree.random_state </span><span class="s3">for </span><span class="s1">tree </span><span class="s3">in </span><span class="s1">est_ws]) == set(</span>
        <span class="s1">[tree.random_state </span><span class="s3">for </span><span class="s1">tree </span><span class="s3">in </span><span class="s1">est_no_ws]</span>
    <span class="s1">)</span>

    <span class="s1">assert_array_equal(</span>
        <span class="s1">est_ws.apply(X)</span><span class="s3">, </span><span class="s1">est_no_ws.apply(X)</span><span class="s3">, </span><span class="s1">err_msg=</span><span class="s5">&quot;Failed with {0}&quot;</span><span class="s1">.format(name)</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_ESTIMATORS)</span>
<span class="s3">def </span><span class="s1">test_warm_start(name):</span>
    <span class="s1">check_warm_start(name)</span>


<span class="s3">def </span><span class="s1">check_warm_start_clear(name):</span>
    <span class="s2"># Test if fit clears state and grows a new forest when warm_start==False.</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = hastie_X</span><span class="s3">, </span><span class="s1">hastie_y</span>
    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>
    <span class="s1">est = ForestEstimator(n_estimators=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">max_depth=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">False, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">est_2 = ForestEstimator(</span>
        <span class="s1">n_estimators=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">max_depth=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True, </span><span class="s1">random_state=</span><span class="s4">2</span>
    <span class="s1">)</span>
    <span class="s1">est_2.fit(X</span><span class="s3">, </span><span class="s1">y)  </span><span class="s2"># inits state</span>
    <span class="s1">est_2.set_params(warm_start=</span><span class="s3">False, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">est_2.fit(X</span><span class="s3">, </span><span class="s1">y)  </span><span class="s2"># clears old state and equals est</span>

    <span class="s1">assert_array_almost_equal(est_2.apply(X)</span><span class="s3">, </span><span class="s1">est.apply(X))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_ESTIMATORS)</span>
<span class="s3">def </span><span class="s1">test_warm_start_clear(name):</span>
    <span class="s1">check_warm_start_clear(name)</span>


<span class="s3">def </span><span class="s1">check_warm_start_smaller_n_estimators(name):</span>
    <span class="s2"># Test if warm start second fit with smaller n_estimators raises error.</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = hastie_X</span><span class="s3">, </span><span class="s1">hastie_y</span>
    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>
    <span class="s1">est = ForestEstimator(n_estimators=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">max_depth=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">est.set_params(n_estimators=</span><span class="s4">4</span><span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_ESTIMATORS)</span>
<span class="s3">def </span><span class="s1">test_warm_start_smaller_n_estimators(name):</span>
    <span class="s1">check_warm_start_smaller_n_estimators(name)</span>


<span class="s3">def </span><span class="s1">check_warm_start_equal_n_estimators(name):</span>
    <span class="s2"># Test if warm start with equal n_estimators does nothing and returns the</span>
    <span class="s2"># same forest and raises a warning.</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = hastie_X</span><span class="s3">, </span><span class="s1">hastie_y</span>
    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>
    <span class="s1">est = ForestEstimator(n_estimators=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">max_depth=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">est_2 = ForestEstimator(</span>
        <span class="s1">n_estimators=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">max_depth=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True, </span><span class="s1">random_state=</span><span class="s4">1</span>
    <span class="s1">)</span>
    <span class="s1">est_2.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s2"># Now est_2 equals est.</span>

    <span class="s1">est_2.set_params(random_state=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">warn_msg = (</span>
        <span class="s5">&quot;Warm-start fitting without increasing n_estimators does not fit new trees.&quot;</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s3">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">est_2.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s2"># If we had fit the trees again we would have got a different forest as we</span>
    <span class="s2"># changed the random state.</span>
    <span class="s1">assert_array_equal(est.apply(X)</span><span class="s3">, </span><span class="s1">est_2.apply(X))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_ESTIMATORS)</span>
<span class="s3">def </span><span class="s1">test_warm_start_equal_n_estimators(name):</span>
    <span class="s1">check_warm_start_equal_n_estimators(name)</span>


<span class="s3">def </span><span class="s1">check_warm_start_oob(name):</span>
    <span class="s2"># Test that the warm start computes oob score when asked.</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = hastie_X</span><span class="s3">, </span><span class="s1">hastie_y</span>
    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>
    <span class="s2"># Use 15 estimators to avoid 'some inputs do not have OOB scores' warning.</span>
    <span class="s1">est = ForestEstimator(</span>
        <span class="s1">n_estimators=</span><span class="s4">15</span><span class="s3">,</span>
        <span class="s1">max_depth=</span><span class="s4">3</span><span class="s3">,</span>
        <span class="s1">warm_start=</span><span class="s3">False,</span>
        <span class="s1">random_state=</span><span class="s4">1</span><span class="s3">,</span>
        <span class="s1">bootstrap=</span><span class="s3">True,</span>
        <span class="s1">oob_score=</span><span class="s3">True,</span>
    <span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">est_2 = ForestEstimator(</span>
        <span class="s1">n_estimators=</span><span class="s4">5</span><span class="s3">,</span>
        <span class="s1">max_depth=</span><span class="s4">3</span><span class="s3">,</span>
        <span class="s1">warm_start=</span><span class="s3">False,</span>
        <span class="s1">random_state=</span><span class="s4">1</span><span class="s3">,</span>
        <span class="s1">bootstrap=</span><span class="s3">True,</span>
        <span class="s1">oob_score=</span><span class="s3">False,</span>
    <span class="s1">)</span>
    <span class="s1">est_2.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">est_2.set_params(warm_start=</span><span class="s3">True, </span><span class="s1">oob_score=</span><span class="s3">True, </span><span class="s1">n_estimators=</span><span class="s4">15</span><span class="s1">)</span>
    <span class="s1">est_2.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s3">assert </span><span class="s1">hasattr(est_2</span><span class="s3">, </span><span class="s5">&quot;oob_score_&quot;</span><span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">est.oob_score_ == est_2.oob_score_</span>

    <span class="s2"># Test that oob_score is computed even if we don't need to train</span>
    <span class="s2"># additional trees.</span>
    <span class="s1">est_3 = ForestEstimator(</span>
        <span class="s1">n_estimators=</span><span class="s4">15</span><span class="s3">,</span>
        <span class="s1">max_depth=</span><span class="s4">3</span><span class="s3">,</span>
        <span class="s1">warm_start=</span><span class="s3">True,</span>
        <span class="s1">random_state=</span><span class="s4">1</span><span class="s3">,</span>
        <span class="s1">bootstrap=</span><span class="s3">True,</span>
        <span class="s1">oob_score=</span><span class="s3">False,</span>
    <span class="s1">)</span>
    <span class="s1">est_3.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert not </span><span class="s1">hasattr(est_3</span><span class="s3">, </span><span class="s5">&quot;oob_score_&quot;</span><span class="s1">)</span>

    <span class="s1">est_3.set_params(oob_score=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">ignore_warnings(est_3.fit)(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s3">assert </span><span class="s1">est.oob_score_ == est_3.oob_score_</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS_REGRESSORS)</span>
<span class="s3">def </span><span class="s1">test_warm_start_oob(name):</span>
    <span class="s1">check_warm_start_oob(name)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS_REGRESSORS)</span>
<span class="s3">def </span><span class="s1">test_oob_not_computed_twice(name):</span>
    <span class="s2"># Check that oob_score is not computed twice when warm_start=True.</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = hastie_X</span><span class="s3">, </span><span class="s1">hastie_y</span>
    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>

    <span class="s1">est = ForestEstimator(</span>
        <span class="s1">n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True, </span><span class="s1">bootstrap=</span><span class="s3">True, </span><span class="s1">oob_score=</span><span class="s3">True</span>
    <span class="s1">)</span>

    <span class="s3">with </span><span class="s1">patch.object(</span>
        <span class="s1">est</span><span class="s3">, </span><span class="s5">&quot;_set_oob_score_and_attributes&quot;</span><span class="s3">, </span><span class="s1">wraps=est._set_oob_score_and_attributes</span>
    <span class="s1">) </span><span class="s3">as </span><span class="s1">mock_set_oob_score_and_attributes:</span>
        <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s3">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s3">, </span><span class="s1">match=</span><span class="s5">&quot;Warm-start fitting without increasing&quot;</span><span class="s1">):</span>
            <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s1">mock_set_oob_score_and_attributes.assert_called_once()</span>


<span class="s3">def </span><span class="s1">test_dtype_convert(n_classes=</span><span class="s4">15</span><span class="s1">):</span>
    <span class="s1">classifier = RandomForestClassifier(random_state=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">bootstrap=</span><span class="s3">False</span><span class="s1">)</span>

    <span class="s1">X = np.eye(n_classes)</span>
    <span class="s1">y = [ch </span><span class="s3">for </span><span class="s1">ch </span><span class="s3">in </span><span class="s5">&quot;ABCDEFGHIJKLMNOPQRSTU&quot;</span><span class="s1">[:n_classes]]</span>

    <span class="s1">result = classifier.fit(X</span><span class="s3">, </span><span class="s1">y).predict(X)</span>
    <span class="s1">assert_array_equal(classifier.classes_</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(result</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">check_decision_path(name):</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = hastie_X</span><span class="s3">, </span><span class="s1">hastie_y</span>
    <span class="s1">n_samples = X.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">ForestEstimator = FOREST_ESTIMATORS[name]</span>
    <span class="s1">est = ForestEstimator(n_estimators=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">max_depth=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">False, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">indicator</span><span class="s3">, </span><span class="s1">n_nodes_ptr = est.decision_path(X)</span>

    <span class="s3">assert </span><span class="s1">indicator.shape[</span><span class="s4">1</span><span class="s1">] == n_nodes_ptr[-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s3">assert </span><span class="s1">indicator.shape[</span><span class="s4">0</span><span class="s1">] == n_samples</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">np.diff(n_nodes_ptr)</span><span class="s3">, </span><span class="s1">[e.tree_.node_count </span><span class="s3">for </span><span class="s1">e </span><span class="s3">in </span><span class="s1">est.estimators_]</span>
    <span class="s1">)</span>

    <span class="s2"># Assert that leaves index are correct</span>
    <span class="s1">leaves = est.apply(X)</span>
    <span class="s3">for </span><span class="s1">est_id </span><span class="s3">in </span><span class="s1">range(leaves.shape[</span><span class="s4">1</span><span class="s1">]):</span>
        <span class="s1">leave_indicator = [</span>
            <span class="s1">indicator[i</span><span class="s3">, </span><span class="s1">n_nodes_ptr[est_id] + j]</span>
            <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">j </span><span class="s3">in </span><span class="s1">enumerate(leaves[:</span><span class="s3">, </span><span class="s1">est_id])</span>
        <span class="s1">]</span>
        <span class="s1">assert_array_almost_equal(leave_indicator</span><span class="s3">, </span><span class="s1">np.ones(shape=n_samples))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS_REGRESSORS)</span>
<span class="s3">def </span><span class="s1">test_decision_path(name):</span>
    <span class="s1">check_decision_path(name)</span>


<span class="s3">def </span><span class="s1">test_min_impurity_decrease():</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = datasets.make_hastie_10_2(n_samples=</span><span class="s4">100</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">all_estimators = [</span>
        <span class="s1">RandomForestClassifier</span><span class="s3">,</span>
        <span class="s1">RandomForestRegressor</span><span class="s3">,</span>
        <span class="s1">ExtraTreesClassifier</span><span class="s3">,</span>
        <span class="s1">ExtraTreesRegressor</span><span class="s3">,</span>
    <span class="s1">]</span>

    <span class="s3">for </span><span class="s1">Estimator </span><span class="s3">in </span><span class="s1">all_estimators:</span>
        <span class="s1">est = Estimator(min_impurity_decrease=</span><span class="s4">0.1</span><span class="s1">)</span>
        <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s3">for </span><span class="s1">tree </span><span class="s3">in </span><span class="s1">est.estimators_:</span>
            <span class="s2"># Simply check if the parameter is passed on correctly. Tree tests</span>
            <span class="s2"># will suffice for the actual working of this param</span>
            <span class="s3">assert </span><span class="s1">tree.min_impurity_decrease == </span><span class="s4">0.1</span>


<span class="s3">def </span><span class="s1">test_poisson_y_positive_check():</span>
    <span class="s1">est = RandomForestRegressor(criterion=</span><span class="s5">&quot;poisson&quot;</span><span class="s1">)</span>
    <span class="s1">X = np.zeros((</span><span class="s4">3</span><span class="s3">, </span><span class="s4">3</span><span class="s1">))</span>

    <span class="s1">y = [-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">3</span><span class="s1">]</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;Some value\(s\) of y are negative which is &quot;</span>
        <span class="s5">r&quot;not allowed for Poisson regression.&quot;</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError</span><span class="s3">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">y = [</span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;Sum of y is not strictly positive which &quot;</span>
        <span class="s5">r&quot;is necessary for Poisson regression.&quot;</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError</span><span class="s3">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s2"># mypy error: Variable &quot;DEFAULT_JOBLIB_BACKEND&quot; is not valid type</span>
<span class="s3">class </span><span class="s1">MyBackend(DEFAULT_JOBLIB_BACKEND):  </span><span class="s2"># type: ignore</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s1">self.count = </span><span class="s4">0</span>
        <span class="s1">super().__init__(*args</span><span class="s3">, </span><span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">start_call(self):</span>
        <span class="s1">self.count += </span><span class="s4">1</span>
        <span class="s3">return </span><span class="s1">super().start_call()</span>


<span class="s1">joblib.register_parallel_backend(</span><span class="s5">&quot;testing&quot;</span><span class="s3">, </span><span class="s1">MyBackend)</span>


<span class="s1">@skip_if_no_parallel</span>
<span class="s3">def </span><span class="s1">test_backend_respected():</span>
    <span class="s1">clf = RandomForestClassifier(n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s3">with </span><span class="s1">joblib.parallel_backend(</span><span class="s5">&quot;testing&quot;</span><span class="s1">) </span><span class="s3">as </span><span class="s1">(ba</span><span class="s3">, </span><span class="s1">n_jobs):</span>
        <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s3">assert </span><span class="s1">ba.count &gt; </span><span class="s4">0</span>

    <span class="s2"># predict_proba requires shared memory. Ensure that's honored.</span>
    <span class="s3">with </span><span class="s1">joblib.parallel_backend(</span><span class="s5">&quot;testing&quot;</span><span class="s1">) </span><span class="s3">as </span><span class="s1">(ba</span><span class="s3">, </span><span class="s1">_):</span>
        <span class="s1">clf.predict_proba(X)</span>

    <span class="s3">assert </span><span class="s1">ba.count == </span><span class="s4">0</span>


<span class="s3">def </span><span class="s1">test_forest_feature_importances_sum():</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">15</span><span class="s3">, </span><span class="s1">n_informative=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">n_classes=</span><span class="s4">3</span>
    <span class="s1">)</span>
    <span class="s1">clf = RandomForestClassifier(</span>
        <span class="s1">min_samples_leaf=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s3">, </span><span class="s1">n_estimators=</span><span class="s4">200</span>
    <span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">math.isclose(</span><span class="s4">1</span><span class="s3">, </span><span class="s1">clf.feature_importances_.sum()</span><span class="s3">, </span><span class="s1">abs_tol=</span><span class="s4">1e-7</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_forest_degenerate_feature_importances():</span>
    <span class="s2"># build a forest of single node trees. See #13636</span>
    <span class="s1">X = np.zeros((</span><span class="s4">10</span><span class="s3">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">y = np.ones((</span><span class="s4">10</span><span class="s3">,</span><span class="s1">))</span>
    <span class="s1">gbr = RandomForestRegressor(n_estimators=</span><span class="s4">10</span><span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(gbr.feature_importances_</span><span class="s3">, </span><span class="s1">np.zeros(</span><span class="s4">10</span><span class="s3">, </span><span class="s1">dtype=np.float64))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS_REGRESSORS)</span>
<span class="s3">def </span><span class="s1">test_max_samples_bootstrap(name):</span>
    <span class="s2"># Check invalid `max_samples` values</span>
    <span class="s1">est = FOREST_CLASSIFIERS_REGRESSORS[name](bootstrap=</span><span class="s3">False, </span><span class="s1">max_samples=</span><span class="s4">0.5</span><span class="s1">)</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;`max_sample` cannot be set if `bootstrap=False`. &quot;</span>
        <span class="s5">r&quot;Either switch to `bootstrap=True` or set &quot;</span>
        <span class="s5">r&quot;`max_sample=None`.&quot;</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError</span><span class="s3">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS_REGRESSORS)</span>
<span class="s3">def </span><span class="s1">test_large_max_samples_exception(name):</span>
    <span class="s2"># Check invalid `max_samples`</span>
    <span class="s1">est = FOREST_CLASSIFIERS_REGRESSORS[name](bootstrap=</span><span class="s3">True, </span><span class="s1">max_samples=int(</span><span class="s4">1e9</span><span class="s1">))</span>
    <span class="s1">match = </span><span class="s5">&quot;`max_samples` must be &lt;= n_samples=6 but got value 1000000000&quot;</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError</span><span class="s3">, </span><span class="s1">match=match):</span>
        <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_REGRESSORS)</span>
<span class="s3">def </span><span class="s1">test_max_samples_boundary_regressors(name):</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">X_reg</span><span class="s3">, </span><span class="s1">y_reg</span><span class="s3">, </span><span class="s1">train_size=</span><span class="s4">0.7</span><span class="s3">, </span><span class="s1">test_size=</span><span class="s4">0.3</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>

    <span class="s1">ms_1_model = FOREST_REGRESSORS[name](</span>
        <span class="s1">bootstrap=</span><span class="s3">True, </span><span class="s1">max_samples=</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">ms_1_predict = ms_1_model.fit(X_train</span><span class="s3">, </span><span class="s1">y_train).predict(X_test)</span>

    <span class="s1">ms_None_model = FOREST_REGRESSORS[name](</span>
        <span class="s1">bootstrap=</span><span class="s3">True, </span><span class="s1">max_samples=</span><span class="s3">None, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">ms_None_predict = ms_None_model.fit(X_train</span><span class="s3">, </span><span class="s1">y_train).predict(X_test)</span>

    <span class="s1">ms_1_ms = mean_squared_error(ms_1_predict</span><span class="s3">, </span><span class="s1">y_test)</span>
    <span class="s1">ms_None_ms = mean_squared_error(ms_None_predict</span><span class="s3">, </span><span class="s1">y_test)</span>

    <span class="s3">assert </span><span class="s1">ms_1_ms == pytest.approx(ms_None_ms)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;name&quot;</span><span class="s3">, </span><span class="s1">FOREST_CLASSIFIERS)</span>
<span class="s3">def </span><span class="s1">test_max_samples_boundary_classifiers(name):</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">_ = train_test_split(</span>
        <span class="s1">X_large</span><span class="s3">, </span><span class="s1">y_large</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">stratify=y_large</span>
    <span class="s1">)</span>

    <span class="s1">ms_1_model = FOREST_CLASSIFIERS[name](</span>
        <span class="s1">bootstrap=</span><span class="s3">True, </span><span class="s1">max_samples=</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">ms_1_proba = ms_1_model.fit(X_train</span><span class="s3">, </span><span class="s1">y_train).predict_proba(X_test)</span>

    <span class="s1">ms_None_model = FOREST_CLASSIFIERS[name](</span>
        <span class="s1">bootstrap=</span><span class="s3">True, </span><span class="s1">max_samples=</span><span class="s3">None, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">ms_None_proba = ms_None_model.fit(X_train</span><span class="s3">, </span><span class="s1">y_train).predict_proba(X_test)</span>

    <span class="s1">np.testing.assert_allclose(ms_1_proba</span><span class="s3">, </span><span class="s1">ms_None_proba)</span>


<span class="s3">def </span><span class="s1">test_forest_y_sparse():</span>
    <span class="s1">X = [[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">3</span><span class="s1">]]</span>
    <span class="s1">y = csr_matrix([</span><span class="s4">4</span><span class="s3">, </span><span class="s4">5</span><span class="s3">, </span><span class="s4">6</span><span class="s1">])</span>
    <span class="s1">est = RandomForestClassifier()</span>
    <span class="s1">msg = </span><span class="s5">&quot;sparse multilabel-indicator for y is not supported.&quot;</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError</span><span class="s3">, </span><span class="s1">match=msg):</span>
        <span class="s1">est.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;ForestClass&quot;</span><span class="s3">, </span><span class="s1">[RandomForestClassifier</span><span class="s3">, </span><span class="s1">RandomForestRegressor])</span>
<span class="s3">def </span><span class="s1">test_little_tree_with_small_max_samples(ForestClass):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">1</span><span class="s1">)</span>

    <span class="s1">X = rng.randn(</span><span class="s4">10000</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">y = rng.randn(</span><span class="s4">10000</span><span class="s1">) &gt; </span><span class="s4">0</span>

    <span class="s2"># First fit with no restriction on max samples</span>
    <span class="s1">est1 = ForestClass(</span>
        <span class="s1">n_estimators=</span><span class="s4">1</span><span class="s3">,</span>
        <span class="s1">random_state=rng</span><span class="s3">,</span>
        <span class="s1">max_samples=</span><span class="s3">None,</span>
    <span class="s1">)</span>

    <span class="s2"># Second fit with max samples restricted to just 2</span>
    <span class="s1">est2 = ForestClass(</span>
        <span class="s1">n_estimators=</span><span class="s4">1</span><span class="s3">,</span>
        <span class="s1">random_state=rng</span><span class="s3">,</span>
        <span class="s1">max_samples=</span><span class="s4">2</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s1">est1.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">est2.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">tree1 = est1.estimators_[</span><span class="s4">0</span><span class="s1">].tree_</span>
    <span class="s1">tree2 = est2.estimators_[</span><span class="s4">0</span><span class="s1">].tree_</span>

    <span class="s1">msg = </span><span class="s5">&quot;Tree without `max_samples` restriction should have more nodes&quot;</span>
    <span class="s3">assert </span><span class="s1">tree1.node_count &gt; tree2.node_count</span><span class="s3">, </span><span class="s1">msg</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;Forest&quot;</span><span class="s3">, </span><span class="s1">FOREST_REGRESSORS)</span>
<span class="s3">def </span><span class="s1">test_mse_criterion_object_segfault_smoke_test(Forest):</span>
    <span class="s2"># This is a smoke test to ensure that passing a mutable criterion</span>
    <span class="s2"># does not cause a segfault when fitting with concurrent threads.</span>
    <span class="s2"># Non-regression test for:</span>
    <span class="s2"># https://github.com/scikit-learn/scikit-learn/issues/12623</span>
    <span class="s3">from </span><span class="s1">sklearn.tree._criterion </span><span class="s3">import </span><span class="s1">MSE</span>

    <span class="s1">y = y_reg.reshape(-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_outputs = y.shape</span>
    <span class="s1">mse_criterion = MSE(n_outputs</span><span class="s3">, </span><span class="s1">n_samples)</span>
    <span class="s1">est = FOREST_REGRESSORS[Forest](n_estimators=</span><span class="s4">2</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">2</span><span class="s3">, </span><span class="s1">criterion=mse_criterion)</span>

    <span class="s1">est.fit(X_reg</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">test_random_trees_embedding_feature_names_out():</span>
    <span class="s0">&quot;&quot;&quot;Check feature names out for Random Trees Embedding.&quot;&quot;&quot;</span>
    <span class="s1">random_state = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = np.abs(random_state.randn(</span><span class="s4">100</span><span class="s3">, </span><span class="s4">4</span><span class="s1">))</span>
    <span class="s1">hasher = RandomTreesEmbedding(</span>
        <span class="s1">n_estimators=</span><span class="s4">2</span><span class="s3">, </span><span class="s1">max_depth=</span><span class="s4">2</span><span class="s3">, </span><span class="s1">sparse_output=</span><span class="s3">False, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">).fit(X)</span>
    <span class="s1">names = hasher.get_feature_names_out()</span>
    <span class="s1">expected_names = [</span>
        <span class="s5">f&quot;randomtreesembedding_</span><span class="s3">{</span><span class="s1">tree</span><span class="s3">}</span><span class="s5">_</span><span class="s3">{</span><span class="s1">leaf</span><span class="s3">}</span><span class="s5">&quot;</span>
        <span class="s2"># Note: nodes with indices 0, 1 and 4 are internal split nodes and</span>
        <span class="s2"># therefore do not appear in the expected output feature names.</span>
        <span class="s3">for </span><span class="s1">tree</span><span class="s3">, </span><span class="s1">leaf </span><span class="s3">in </span><span class="s1">[</span>
            <span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">5</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">6</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s4">1</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s4">1</span><span class="s3">, </span><span class="s4">5</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s4">1</span><span class="s3">, </span><span class="s4">6</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">]</span>
    <span class="s1">]</span>
    <span class="s1">assert_array_equal(expected_names</span><span class="s3">, </span><span class="s1">names)</span>


<span class="s2"># TODO(1.4): remove in 1.4</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;name&quot;</span><span class="s3">,</span>
    <span class="s1">FOREST_ESTIMATORS</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">test_base_estimator_property_deprecated(name):</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">3</span><span class="s3">, </span><span class="s4">4</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">model = FOREST_ESTIMATORS[name]()</span>
    <span class="s1">model.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">warn_msg = (</span>
        <span class="s5">&quot;Attribute `base_estimator_` was deprecated in version 1.2 and &quot;</span>
        <span class="s5">&quot;will be removed in 1.4. Use `estimator_` instead.&quot;</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s3">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">model.base_estimator_</span>


<span class="s3">def </span><span class="s1">test_read_only_buffer(monkeypatch):</span>
    <span class="s0">&quot;&quot;&quot;RandomForestClassifier must work on readonly sparse data. 
 
    Non-regression test for: https://github.com/scikit-learn/scikit-learn/issues/25333 
    &quot;&quot;&quot;</span>
    <span class="s1">monkeypatch.setattr(</span>
        <span class="s1">sklearn.ensemble._forest</span><span class="s3">,</span>
        <span class="s5">&quot;Parallel&quot;</span><span class="s3">,</span>
        <span class="s1">partial(Parallel</span><span class="s3">, </span><span class="s1">max_nbytes=</span><span class="s4">100</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s1">rng = np.random.RandomState(seed=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">100</span><span class="s3">, </span><span class="s1">n_features=</span><span class="s4">200</span><span class="s3">, </span><span class="s1">random_state=rng)</span>
    <span class="s1">X = csr_matrix(X</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">clf = RandomForestClassifier(n_jobs=</span><span class="s4">2</span><span class="s3">, </span><span class="s1">random_state=rng)</span>
    <span class="s1">cross_val_score(clf</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">cv=</span><span class="s4">2</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;class_weight&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s5">&quot;balanced_subsample&quot;</span><span class="s3">, None</span><span class="s1">])</span>
<span class="s3">def </span><span class="s1">test_round_samples_to_one_when_samples_too_low(class_weight):</span>
    <span class="s0">&quot;&quot;&quot;Check low max_samples works and is rounded to one. 
 
    Non-regression test for gh-24037. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = datasets.load_wine(return_X_y=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">forest = RandomForestClassifier(</span>
        <span class="s1">n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">max_samples=</span><span class="s4">1e-4</span><span class="s3">, </span><span class="s1">class_weight=class_weight</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">forest.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
</pre>
</body>
</html>