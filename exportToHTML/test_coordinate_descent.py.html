<html>
<head>
<title>test_coordinate_descent.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_coordinate_descent.py</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span>
<span class="s0">#          Alexandre Gramfort &lt;alexandre.gramfort@inria.fr&gt;</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">copy </span><span class="s2">import </span><span class="s1">deepcopy</span>

<span class="s2">import </span><span class="s1">joblib</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">interpolate</span><span class="s2">, </span><span class="s1">sparse</span>

<span class="s2">from </span><span class="s1">sklearn.base </span><span class="s2">import </span><span class="s1">clone</span><span class="s2">, </span><span class="s1">is_classifier</span>
<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">load_diabetes</span><span class="s2">, </span><span class="s1">make_regression</span>
<span class="s2">from </span><span class="s1">sklearn.exceptions </span><span class="s2">import </span><span class="s1">ConvergenceWarning</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">ElasticNet</span><span class="s2">,</span>
    <span class="s1">ElasticNetCV</span><span class="s2">,</span>
    <span class="s1">Lars</span><span class="s2">,</span>
    <span class="s1">Lasso</span><span class="s2">,</span>
    <span class="s1">LassoCV</span><span class="s2">,</span>
    <span class="s1">LassoLars</span><span class="s2">,</span>
    <span class="s1">LassoLarsCV</span><span class="s2">,</span>
    <span class="s1">LassoLarsIC</span><span class="s2">,</span>
    <span class="s1">LinearRegression</span><span class="s2">,</span>
    <span class="s1">MultiTaskElasticNet</span><span class="s2">,</span>
    <span class="s1">MultiTaskElasticNetCV</span><span class="s2">,</span>
    <span class="s1">MultiTaskLasso</span><span class="s2">,</span>
    <span class="s1">MultiTaskLassoCV</span><span class="s2">,</span>
    <span class="s1">OrthogonalMatchingPursuit</span><span class="s2">,</span>
    <span class="s1">Ridge</span><span class="s2">,</span>
    <span class="s1">RidgeClassifier</span><span class="s2">,</span>
    <span class="s1">RidgeClassifierCV</span><span class="s2">,</span>
    <span class="s1">RidgeCV</span><span class="s2">,</span>
    <span class="s1">enet_path</span><span class="s2">,</span>
    <span class="s1">lars_path</span><span class="s2">,</span>
    <span class="s1">lasso_path</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model._coordinate_descent </span><span class="s2">import </span><span class="s1">_set_order</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">GridSearchCV</span><span class="s2">,</span>
    <span class="s1">LeaveOneGroupOut</span><span class="s2">,</span>
    <span class="s1">train_test_split</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.pipeline </span><span class="s2">import </span><span class="s1">make_pipeline</span>
<span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">StandardScaler</span>
<span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">check_array</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">TempMemmap</span><span class="s2">,</span>
    <span class="s1">assert_allclose</span><span class="s2">,</span>
    <span class="s1">assert_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_equal</span><span class="s2">,</span>
    <span class="s1">ignore_warnings</span><span class="s2">,</span>
<span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;order&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;C&quot;</span><span class="s2">, </span><span class="s3">&quot;F&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;input_order&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;C&quot;</span><span class="s2">, </span><span class="s3">&quot;F&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_set_order_dense(order</span><span class="s2">, </span><span class="s1">input_order):</span>
    <span class="s4">&quot;&quot;&quot;Check that _set_order returns arrays with promised order.&quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">order=input_order)</span>
    <span class="s1">y = np.array([</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">order=input_order)</span>
    <span class="s1">X2</span><span class="s2">, </span><span class="s1">y2 = _set_order(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">order=order)</span>
    <span class="s2">if </span><span class="s1">order == </span><span class="s3">&quot;C&quot;</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s1">X2.flags[</span><span class="s3">&quot;C_CONTIGUOUS&quot;</span><span class="s1">]</span>
        <span class="s2">assert </span><span class="s1">y2.flags[</span><span class="s3">&quot;C_CONTIGUOUS&quot;</span><span class="s1">]</span>
    <span class="s2">elif </span><span class="s1">order == </span><span class="s3">&quot;F&quot;</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s1">X2.flags[</span><span class="s3">&quot;F_CONTIGUOUS&quot;</span><span class="s1">]</span>
        <span class="s2">assert </span><span class="s1">y2.flags[</span><span class="s3">&quot;F_CONTIGUOUS&quot;</span><span class="s1">]</span>

    <span class="s2">if </span><span class="s1">order == input_order:</span>
        <span class="s2">assert </span><span class="s1">X </span><span class="s2">is </span><span class="s1">X2</span>
        <span class="s2">assert </span><span class="s1">y </span><span class="s2">is </span><span class="s1">y2</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;order&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;C&quot;</span><span class="s2">, </span><span class="s3">&quot;F&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;input_order&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;C&quot;</span><span class="s2">, </span><span class="s3">&quot;F&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_set_order_sparse(order</span><span class="s2">, </span><span class="s1">input_order):</span>
    <span class="s4">&quot;&quot;&quot;Check that _set_order returns sparse matrices in promised format.&quot;&quot;&quot;</span>
    <span class="s1">X = sparse.coo_matrix(np.array([[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]]))</span>
    <span class="s1">y = sparse.coo_matrix(np.array([</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]))</span>
    <span class="s1">sparse_format = </span><span class="s3">&quot;csc&quot; </span><span class="s2">if </span><span class="s1">input_order == </span><span class="s3">&quot;F&quot; </span><span class="s2">else </span><span class="s3">&quot;csr&quot;</span>
    <span class="s1">X = X.asformat(sparse_format)</span>
    <span class="s1">y = X.asformat(sparse_format)</span>
    <span class="s1">X2</span><span class="s2">, </span><span class="s1">y2 = _set_order(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">order=order)</span>

    <span class="s1">format = </span><span class="s3">&quot;csc&quot; </span><span class="s2">if </span><span class="s1">order == </span><span class="s3">&quot;F&quot; </span><span class="s2">else </span><span class="s3">&quot;csr&quot;</span>
    <span class="s2">assert </span><span class="s1">sparse.issparse(X2) </span><span class="s2">and </span><span class="s1">X2.format == format</span>
    <span class="s2">assert </span><span class="s1">sparse.issparse(y2) </span><span class="s2">and </span><span class="s1">y2.format == format</span>


<span class="s2">def </span><span class="s1">test_lasso_zero():</span>
    <span class="s0"># Check that the lasso can handle zero data without crashing</span>
    <span class="s1">X = [[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">clf = Lasso(alpha=</span><span class="s5">0.1</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">pred = clf.predict([[</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">3</span><span class="s1">]])</span>
    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(pred</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">assert_almost_equal(clf.dual_gap_</span><span class="s2">, </span><span class="s5">0</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_enet_nonfinite_params():</span>
    <span class="s0"># Check ElasticNet throws ValueError when dealing with non-finite parameter</span>
    <span class="s0"># values</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s5">10</span>
    <span class="s1">fmax = np.finfo(np.float64).max</span>
    <span class="s1">X = fmax * rng.uniform(size=(n_samples</span><span class="s2">, </span><span class="s5">2</span><span class="s1">))</span>
    <span class="s1">y = rng.randint(</span><span class="s5">0</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s1">size=n_samples)</span>

    <span class="s1">clf = ElasticNet(alpha=</span><span class="s5">0.1</span><span class="s1">)</span>
    <span class="s1">msg = </span><span class="s3">&quot;Coordinate descent iterations resulted in non-finite parameter values&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_lasso_toy():</span>
    <span class="s0"># Test Lasso on a toy example for various values of alpha.</span>
    <span class="s0"># When validating this against glmnet notice that glmnet divides it</span>
    <span class="s0"># against nobs.</span>

    <span class="s1">X = [[-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">]]</span>
    <span class="s1">Y = [-</span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]  </span><span class="s0"># just a straight line</span>
    <span class="s1">T = [[</span><span class="s5">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">4</span><span class="s1">]]  </span><span class="s0"># test sample</span>

    <span class="s1">clf = Lasso(alpha=</span><span class="s5">1e-8</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">pred = clf.predict(T)</span>
    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(pred</span><span class="s2">, </span><span class="s1">[</span><span class="s5">2</span><span class="s2">, </span><span class="s5">3</span><span class="s2">, </span><span class="s5">4</span><span class="s1">])</span>
    <span class="s1">assert_almost_equal(clf.dual_gap_</span><span class="s2">, </span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">clf = Lasso(alpha=</span><span class="s5">0.1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">pred = clf.predict(T)</span>
    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.85</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(pred</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1.7</span><span class="s2">, </span><span class="s5">2.55</span><span class="s2">, </span><span class="s5">3.4</span><span class="s1">])</span>
    <span class="s1">assert_almost_equal(clf.dual_gap_</span><span class="s2">, </span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">clf = Lasso(alpha=</span><span class="s5">0.5</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">pred = clf.predict(T)</span>
    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.25</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(pred</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0.75</span><span class="s2">, </span><span class="s5">1.0</span><span class="s1">])</span>
    <span class="s1">assert_almost_equal(clf.dual_gap_</span><span class="s2">, </span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">clf = Lasso(alpha=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">pred = clf.predict(T)</span>
    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.0</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(pred</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">assert_almost_equal(clf.dual_gap_</span><span class="s2">, </span><span class="s5">0</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_enet_toy():</span>
    <span class="s0"># Test ElasticNet for various parameters of alpha and l1_ratio.</span>
    <span class="s0"># Actually, the parameters alpha = 0 should not be allowed. However,</span>
    <span class="s0"># we test it as a border case.</span>
    <span class="s0"># ElasticNet is tested with and without precomputed Gram matrix</span>

    <span class="s1">X = np.array([[-</span><span class="s5">1.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1.0</span><span class="s1">]])</span>
    <span class="s1">Y = [-</span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]  </span><span class="s0"># just a straight line</span>
    <span class="s1">T = [[</span><span class="s5">2.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">3.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">4.0</span><span class="s1">]]  </span><span class="s0"># test sample</span>

    <span class="s0"># this should be the same as lasso</span>
    <span class="s1">clf = ElasticNet(alpha=</span><span class="s5">1e-8</span><span class="s2">, </span><span class="s1">l1_ratio=</span><span class="s5">1.0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">pred = clf.predict(T)</span>
    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(pred</span><span class="s2">, </span><span class="s1">[</span><span class="s5">2</span><span class="s2">, </span><span class="s5">3</span><span class="s2">, </span><span class="s5">4</span><span class="s1">])</span>
    <span class="s1">assert_almost_equal(clf.dual_gap_</span><span class="s2">, </span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">clf = ElasticNet(alpha=</span><span class="s5">0.5</span><span class="s2">, </span><span class="s1">l1_ratio=</span><span class="s5">0.3</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s5">100</span><span class="s2">, </span><span class="s1">precompute=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">pred = clf.predict(T)</span>
    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.50819</span><span class="s1">]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(pred</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1.0163</span><span class="s2">, </span><span class="s5">1.5245</span><span class="s2">, </span><span class="s5">2.0327</span><span class="s1">]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf.dual_gap_</span><span class="s2">, </span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">clf.set_params(max_iter=</span><span class="s5">100</span><span class="s2">, </span><span class="s1">precompute=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)  </span><span class="s0"># with Gram</span>
    <span class="s1">pred = clf.predict(T)</span>
    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.50819</span><span class="s1">]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(pred</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1.0163</span><span class="s2">, </span><span class="s5">1.5245</span><span class="s2">, </span><span class="s5">2.0327</span><span class="s1">]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf.dual_gap_</span><span class="s2">, </span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">clf.set_params(max_iter=</span><span class="s5">100</span><span class="s2">, </span><span class="s1">precompute=np.dot(X.T</span><span class="s2">, </span><span class="s1">X))</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)  </span><span class="s0"># with Gram</span>
    <span class="s1">pred = clf.predict(T)</span>
    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.50819</span><span class="s1">]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(pred</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1.0163</span><span class="s2">, </span><span class="s5">1.5245</span><span class="s2">, </span><span class="s5">2.0327</span><span class="s1">]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf.dual_gap_</span><span class="s2">, </span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">clf = ElasticNet(alpha=</span><span class="s5">0.5</span><span class="s2">, </span><span class="s1">l1_ratio=</span><span class="s5">0.5</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">pred = clf.predict(T)</span>
    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.45454</span><span class="s1">]</span><span class="s2">, </span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(pred</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.9090</span><span class="s2">, </span><span class="s5">1.3636</span><span class="s2">, </span><span class="s5">1.8181</span><span class="s1">]</span><span class="s2">, </span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf.dual_gap_</span><span class="s2">, </span><span class="s5">0</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_lasso_dual_gap():</span>
    <span class="s4">&quot;&quot;&quot; 
    Check that Lasso.dual_gap_ matches its objective formulation, with the 
    datafit normalized by n_samples 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_samples=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">30</span><span class="s1">)</span>
    <span class="s1">n_samples = len(y)</span>
    <span class="s1">alpha = </span><span class="s5">0.01 </span><span class="s1">* np.max(np.abs(X.T @ y)) / n_samples</span>
    <span class="s1">clf = Lasso(alpha=alpha</span><span class="s2">, </span><span class="s1">fit_intercept=</span><span class="s2">False</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">w = clf.coef_</span>
    <span class="s1">R = y - X @ w</span>
    <span class="s1">primal = </span><span class="s5">0.5 </span><span class="s1">* np.mean(R**</span><span class="s5">2</span><span class="s1">) + clf.alpha * np.sum(np.abs(w))</span>
    <span class="s0"># dual pt: R / n_samples, dual constraint: norm(X.T @ theta, inf) &lt;= alpha</span>
    <span class="s1">R /= np.max(np.abs(X.T @ R) / (n_samples * alpha))</span>
    <span class="s1">dual = </span><span class="s5">0.5 </span><span class="s1">* (np.mean(y**</span><span class="s5">2</span><span class="s1">) - np.mean((y - R) ** </span><span class="s5">2</span><span class="s1">))</span>
    <span class="s1">assert_allclose(clf.dual_gap_</span><span class="s2">, </span><span class="s1">primal - dual)</span>


<span class="s2">def </span><span class="s1">build_dataset(n_samples=</span><span class="s5">50</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">200</span><span class="s2">, </span><span class="s1">n_informative_features=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">n_targets=</span><span class="s5">1</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot; 
    build an ill-posed linear regression problem with many noisy features and 
    comparatively few samples 
    &quot;&quot;&quot;</span>
    <span class="s1">random_state = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">n_targets &gt; </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">w = random_state.randn(n_features</span><span class="s2">, </span><span class="s1">n_targets)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">w = random_state.randn(n_features)</span>
    <span class="s1">w[n_informative_features:] = </span><span class="s5">0.0</span>
    <span class="s1">X = random_state.randn(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">y = np.dot(X</span><span class="s2">, </span><span class="s1">w)</span>
    <span class="s1">X_test = random_state.randn(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">y_test = np.dot(X_test</span><span class="s2">, </span><span class="s1">w)</span>
    <span class="s2">return </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test</span>


<span class="s2">def </span><span class="s1">test_lasso_cv():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test = build_dataset()</span>
    <span class="s1">max_iter = </span><span class="s5">150</span>
    <span class="s1">clf = LassoCV(n_alphas=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">1e-3</span><span class="s2">, </span><span class="s1">max_iter=max_iter</span><span class="s2">, </span><span class="s1">cv=</span><span class="s5">3</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(clf.alpha_</span><span class="s2">, </span><span class="s5">0.056</span><span class="s2">, </span><span class="s5">2</span><span class="s1">)</span>

    <span class="s1">clf = LassoCV(n_alphas=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">1e-3</span><span class="s2">, </span><span class="s1">max_iter=max_iter</span><span class="s2">, </span><span class="s1">precompute=</span><span class="s2">True, </span><span class="s1">cv=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(clf.alpha_</span><span class="s2">, </span><span class="s5">0.056</span><span class="s2">, </span><span class="s5">2</span><span class="s1">)</span>

    <span class="s0"># Check that the lars and the coordinate descent implementation</span>
    <span class="s0"># select a similar alpha</span>
    <span class="s1">lars = LassoLarsCV(max_iter=</span><span class="s5">30</span><span class="s2">, </span><span class="s1">cv=</span><span class="s5">3</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s0"># for this we check that they don't fall in the grid of</span>
    <span class="s0"># clf.alphas further than 1</span>
    <span class="s2">assert </span><span class="s1">(</span>
        <span class="s1">np.abs(</span>
            <span class="s1">np.searchsorted(clf.alphas_[::-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">lars.alpha_)</span>
            <span class="s1">- np.searchsorted(clf.alphas_[::-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">clf.alpha_)</span>
        <span class="s1">)</span>
        <span class="s1">&lt;= </span><span class="s5">1</span>
    <span class="s1">)</span>
    <span class="s0"># check that they also give a similar MSE</span>
    <span class="s1">mse_lars = interpolate.interp1d(lars.cv_alphas_</span><span class="s2">, </span><span class="s1">lars.mse_path_.T)</span>
    <span class="s1">np.testing.assert_approx_equal(</span>
        <span class="s1">mse_lars(clf.alphas_[</span><span class="s5">5</span><span class="s1">]).mean()</span><span class="s2">, </span><span class="s1">clf.mse_path_[</span><span class="s5">5</span><span class="s1">].mean()</span><span class="s2">, </span><span class="s1">significant=</span><span class="s5">2</span>
    <span class="s1">)</span>

    <span class="s0"># test set</span>
    <span class="s2">assert </span><span class="s1">clf.score(X_test</span><span class="s2">, </span><span class="s1">y_test) &gt; </span><span class="s5">0.99</span>


<span class="s2">def </span><span class="s1">test_lasso_cv_with_some_model_selection():</span>
    <span class="s2">from </span><span class="s1">sklearn </span><span class="s2">import </span><span class="s1">datasets</span>
    <span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">ShuffleSplit</span>

    <span class="s1">diabetes = datasets.load_diabetes()</span>
    <span class="s1">X = diabetes.data</span>
    <span class="s1">y = diabetes.target</span>

    <span class="s1">pipe = make_pipeline(StandardScaler()</span><span class="s2">, </span><span class="s1">LassoCV(cv=ShuffleSplit(random_state=</span><span class="s5">0</span><span class="s1">)))</span>
    <span class="s1">pipe.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_lasso_cv_positive_constraint():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test = build_dataset()</span>
    <span class="s1">max_iter = </span><span class="s5">500</span>

    <span class="s0"># Ensure the unconstrained fit has a negative coefficient</span>
    <span class="s1">clf_unconstrained = LassoCV(n_alphas=</span><span class="s5">3</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">1e-1</span><span class="s2">, </span><span class="s1">max_iter=max_iter</span><span class="s2">, </span><span class="s1">cv=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">n_jobs=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">clf_unconstrained.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">min(clf_unconstrained.coef_) &lt; </span><span class="s5">0</span>

    <span class="s0"># On same data, constrained fit has non-negative coefficients</span>
    <span class="s1">clf_constrained = LassoCV(</span>
        <span class="s1">n_alphas=</span><span class="s5">3</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">1e-1</span><span class="s2">, </span><span class="s1">max_iter=max_iter</span><span class="s2">, </span><span class="s1">positive=</span><span class="s2">True, </span><span class="s1">cv=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">n_jobs=</span><span class="s5">1</span>
    <span class="s1">)</span>
    <span class="s1">clf_constrained.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">min(clf_constrained.coef_) &gt;= </span><span class="s5">0</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;alphas, err_type, err_msg&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">((</span><span class="s5">1</span><span class="s2">, </span><span class="s1">-</span><span class="s5">1</span><span class="s2">, </span><span class="s1">-</span><span class="s5">100</span><span class="s1">)</span><span class="s2">, </span><span class="s1">ValueError</span><span class="s2">, </span><span class="s3">r&quot;alphas\[1\] == -1, must be &gt;= 0.0.&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">(-</span><span class="s5">0.1</span><span class="s2">, </span><span class="s1">-</span><span class="s5">1.0</span><span class="s2">, </span><span class="s1">-</span><span class="s5">10.0</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">ValueError</span><span class="s2">,</span>
            <span class="s3">r&quot;alphas\[0\] == -0.1, must be &gt;= 0.0.&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">(</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1.0</span><span class="s2">, </span><span class="s3">&quot;1&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">TypeError</span><span class="s2">,</span>
            <span class="s3">r&quot;alphas\[2\] must be an instance of float, not str&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_lassocv_alphas_validation(alphas</span><span class="s2">, </span><span class="s1">err_type</span><span class="s2">, </span><span class="s1">err_msg):</span>
    <span class="s4">&quot;&quot;&quot;Check the `alphas` validation in LassoCV.&quot;&quot;&quot;</span>

    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s5">5</span><span class="s2">, </span><span class="s5">5</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">y = rng.randint(</span><span class="s5">0</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s1">n_samples)</span>
    <span class="s1">lassocv = LassoCV(alphas=alphas)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(err_type</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">lassocv.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">_scale_alpha_inplace(estimator</span><span class="s2">, </span><span class="s1">n_samples):</span>
    <span class="s4">&quot;&quot;&quot;Rescale the parameter alpha from when the estimator is evoked with 
    normalize set to True as if it were evoked in a Pipeline with normalize set 
    to False and with a StandardScaler. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">(</span><span class="s3">&quot;alpha&quot; </span><span class="s2">not in </span><span class="s1">estimator.get_params()) </span><span class="s2">and </span><span class="s1">(</span>
        <span class="s3">&quot;alphas&quot; </span><span class="s2">not in </span><span class="s1">estimator.get_params()</span>
    <span class="s1">):</span>
        <span class="s2">return</span>

    <span class="s2">if </span><span class="s1">isinstance(estimator</span><span class="s2">, </span><span class="s1">(RidgeCV</span><span class="s2">, </span><span class="s1">RidgeClassifierCV)):</span>
        <span class="s0"># alphas is not validated at this point and can be a list.</span>
        <span class="s0"># We convert it to a np.ndarray to make sure broadcasting</span>
        <span class="s0"># is used.</span>
        <span class="s1">alphas = np.asarray(estimator.alphas) * n_samples</span>
        <span class="s2">return </span><span class="s1">estimator.set_params(alphas=alphas)</span>
    <span class="s2">if </span><span class="s1">isinstance(estimator</span><span class="s2">, </span><span class="s1">(Lasso</span><span class="s2">, </span><span class="s1">LassoLars</span><span class="s2">, </span><span class="s1">MultiTaskLasso)):</span>
        <span class="s1">alpha = estimator.alpha * np.sqrt(n_samples)</span>
    <span class="s2">if </span><span class="s1">isinstance(estimator</span><span class="s2">, </span><span class="s1">(Ridge</span><span class="s2">, </span><span class="s1">RidgeClassifier)):</span>
        <span class="s1">alpha = estimator.alpha * n_samples</span>
    <span class="s2">if </span><span class="s1">isinstance(estimator</span><span class="s2">, </span><span class="s1">(ElasticNet</span><span class="s2">, </span><span class="s1">MultiTaskElasticNet)):</span>
        <span class="s2">if </span><span class="s1">estimator.l1_ratio == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">alpha = estimator.alpha * np.sqrt(n_samples)</span>
        <span class="s2">elif </span><span class="s1">estimator.l1_ratio == </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">alpha = estimator.alpha * n_samples</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s0"># To avoid silent errors in case of refactoring</span>
            <span class="s2">raise </span><span class="s1">NotImplementedError</span>

    <span class="s1">estimator.set_params(alpha=alpha)</span>


<span class="s0"># TODO(1.4): remove 'normalize'</span>
<span class="s1">@pytest.mark.filterwarnings(</span><span class="s3">&quot;ignore:'normalize' was deprecated&quot;</span><span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;LinearModel, params&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(LassoLars</span><span class="s2">, </span><span class="s1">{</span><span class="s3">&quot;alpha&quot;</span><span class="s1">: </span><span class="s5">0.1</span><span class="s1">})</span><span class="s2">,</span>
        <span class="s1">(OrthogonalMatchingPursuit</span><span class="s2">, </span><span class="s1">{})</span><span class="s2">,</span>
        <span class="s1">(Lars</span><span class="s2">, </span><span class="s1">{})</span><span class="s2">,</span>
        <span class="s1">(LassoLarsIC</span><span class="s2">, </span><span class="s1">{})</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_model_pipeline_same_as_normalize_true(LinearModel</span><span class="s2">, </span><span class="s1">params):</span>
    <span class="s0"># Test that linear models (LinearModel) set with normalize set to True are</span>
    <span class="s0"># doing the same as the same linear model preceded by StandardScaler</span>
    <span class="s0"># in the pipeline and with normalize set to False</span>

    <span class="s0"># normalize is True</span>
    <span class="s1">model_normalize = LinearModel(normalize=</span><span class="s2">True, </span><span class="s1">fit_intercept=</span><span class="s2">True, </span><span class="s1">**params)</span>

    <span class="s1">pipeline = make_pipeline(</span>
        <span class="s1">StandardScaler()</span><span class="s2">, </span><span class="s1">LinearModel(normalize=</span><span class="s2">False, </span><span class="s1">fit_intercept=</span><span class="s2">True, </span><span class="s1">**params)</span>
    <span class="s1">)</span>

    <span class="s1">is_multitask = model_normalize._get_tags()[</span><span class="s3">&quot;multioutput_only&quot;</span><span class="s1">]</span>

    <span class="s0"># prepare the data</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s5">100</span><span class="s2">, </span><span class="s5">2</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">w = rng.randn(n_features)</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">X += </span><span class="s5">20  </span><span class="s0"># make features non-zero mean</span>
    <span class="s1">y = X.dot(w)</span>

    <span class="s0"># make classes out of regression</span>
    <span class="s2">if </span><span class="s1">is_classifier(model_normalize):</span>
        <span class="s1">y[y &gt; np.mean(y)] = -</span><span class="s5">1</span>
        <span class="s1">y[y &gt; </span><span class="s5">0</span><span class="s1">] = </span><span class="s5">1</span>
    <span class="s2">if </span><span class="s1">is_multitask:</span>
        <span class="s1">y = np.stack((y</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test = train_test_split(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">42</span><span class="s1">)</span>

    <span class="s1">_scale_alpha_inplace(pipeline[</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X_train.shape[</span><span class="s5">0</span><span class="s1">])</span>

    <span class="s1">model_normalize.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s1">y_pred_normalize = model_normalize.predict(X_test)</span>

    <span class="s1">pipeline.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s1">y_pred_standardize = pipeline.predict(X_test)</span>

    <span class="s1">assert_allclose(model_normalize.coef_ * pipeline[</span><span class="s5">0</span><span class="s1">].scale_</span><span class="s2">, </span><span class="s1">pipeline[</span><span class="s5">1</span><span class="s1">].coef_)</span>
    <span class="s2">assert </span><span class="s1">pipeline[</span><span class="s5">1</span><span class="s1">].intercept_ == pytest.approx(y_train.mean())</span>
    <span class="s2">assert </span><span class="s1">model_normalize.intercept_ == pytest.approx(</span>
        <span class="s1">y_train.mean() - model_normalize.coef_.dot(X_train.mean(</span><span class="s5">0</span><span class="s1">))</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(y_pred_normalize</span><span class="s2">, </span><span class="s1">y_pred_standardize)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;LinearModel, params&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(Lasso</span><span class="s2">, </span><span class="s1">{</span><span class="s3">&quot;tol&quot;</span><span class="s1">: </span><span class="s5">1e-16</span><span class="s2">, </span><span class="s3">&quot;alpha&quot;</span><span class="s1">: </span><span class="s5">0.1</span><span class="s1">})</span><span class="s2">,</span>
        <span class="s1">(LassoCV</span><span class="s2">, </span><span class="s1">{</span><span class="s3">&quot;tol&quot;</span><span class="s1">: </span><span class="s5">1e-16</span><span class="s1">})</span><span class="s2">,</span>
        <span class="s1">(ElasticNetCV</span><span class="s2">, </span><span class="s1">{})</span><span class="s2">,</span>
        <span class="s1">(RidgeClassifier</span><span class="s2">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;sparse_cg&quot;</span><span class="s2">, </span><span class="s3">&quot;alpha&quot;</span><span class="s1">: </span><span class="s5">0.1</span><span class="s1">})</span><span class="s2">,</span>
        <span class="s1">(ElasticNet</span><span class="s2">, </span><span class="s1">{</span><span class="s3">&quot;tol&quot;</span><span class="s1">: </span><span class="s5">1e-16</span><span class="s2">, </span><span class="s3">&quot;l1_ratio&quot;</span><span class="s1">: </span><span class="s5">1</span><span class="s2">, </span><span class="s3">&quot;alpha&quot;</span><span class="s1">: </span><span class="s5">0.01</span><span class="s1">})</span><span class="s2">,</span>
        <span class="s1">(ElasticNet</span><span class="s2">, </span><span class="s1">{</span><span class="s3">&quot;tol&quot;</span><span class="s1">: </span><span class="s5">1e-16</span><span class="s2">, </span><span class="s3">&quot;l1_ratio&quot;</span><span class="s1">: </span><span class="s5">0</span><span class="s2">, </span><span class="s3">&quot;alpha&quot;</span><span class="s1">: </span><span class="s5">0.01</span><span class="s1">})</span><span class="s2">,</span>
        <span class="s1">(Ridge</span><span class="s2">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;sparse_cg&quot;</span><span class="s2">, </span><span class="s3">&quot;tol&quot;</span><span class="s1">: </span><span class="s5">1e-12</span><span class="s2">, </span><span class="s3">&quot;alpha&quot;</span><span class="s1">: </span><span class="s5">0.1</span><span class="s1">})</span><span class="s2">,</span>
        <span class="s1">(LinearRegression</span><span class="s2">, </span><span class="s1">{})</span><span class="s2">,</span>
        <span class="s1">(RidgeCV</span><span class="s2">, </span><span class="s1">{})</span><span class="s2">,</span>
        <span class="s1">(RidgeClassifierCV</span><span class="s2">, </span><span class="s1">{})</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_model_pipeline_same_dense_and_sparse(LinearModel</span><span class="s2">, </span><span class="s1">params):</span>
    <span class="s0"># Test that linear model preceded by StandardScaler in the pipeline and</span>
    <span class="s0"># with normalize set to False gives the same y_pred and the same .coef_</span>
    <span class="s0"># given X sparse or dense</span>

    <span class="s1">model_dense = make_pipeline(StandardScaler(with_mean=</span><span class="s2">False</span><span class="s1">)</span><span class="s2">, </span><span class="s1">LinearModel(**params))</span>

    <span class="s1">model_sparse = make_pipeline(StandardScaler(with_mean=</span><span class="s2">False</span><span class="s1">)</span><span class="s2">, </span><span class="s1">LinearModel(**params))</span>

    <span class="s0"># prepare the data</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s5">200</span>
    <span class="s1">n_features = </span><span class="s5">2</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">X[X &lt; </span><span class="s5">0.1</span><span class="s1">] = </span><span class="s5">0.0</span>

    <span class="s1">X_sparse = sparse.csr_matrix(X)</span>
    <span class="s1">y = rng.rand(n_samples)</span>

    <span class="s2">if </span><span class="s1">is_classifier(model_dense):</span>
        <span class="s1">y = np.sign(y)</span>

    <span class="s1">model_dense.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">model_sparse.fit(X_sparse</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">assert_allclose(model_sparse[</span><span class="s5">1</span><span class="s1">].coef_</span><span class="s2">, </span><span class="s1">model_dense[</span><span class="s5">1</span><span class="s1">].coef_)</span>
    <span class="s1">y_pred_dense = model_dense.predict(X)</span>
    <span class="s1">y_pred_sparse = model_sparse.predict(X_sparse)</span>
    <span class="s1">assert_allclose(y_pred_dense</span><span class="s2">, </span><span class="s1">y_pred_sparse)</span>

    <span class="s1">assert_allclose(model_dense[</span><span class="s5">1</span><span class="s1">].intercept_</span><span class="s2">, </span><span class="s1">model_sparse[</span><span class="s5">1</span><span class="s1">].intercept_)</span>


<span class="s2">def </span><span class="s1">test_lasso_path_return_models_vs_new_return_gives_same_coefficients():</span>
    <span class="s0"># Test that lasso_path with lars_path style output gives the</span>
    <span class="s0"># same result</span>

    <span class="s0"># Some toy data</span>
    <span class="s1">X = np.array([[</span><span class="s5">1</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s5">3.1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">2.3</span><span class="s2">, </span><span class="s5">5.4</span><span class="s2">, </span><span class="s5">4.3</span><span class="s1">]]).T</span>
    <span class="s1">y = np.array([</span><span class="s5">1</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s5">3.1</span><span class="s1">])</span>
    <span class="s1">alphas = [</span><span class="s5">5.0</span><span class="s2">, </span><span class="s5">1.0</span><span class="s2">, </span><span class="s5">0.5</span><span class="s1">]</span>

    <span class="s0"># Use lars_path and lasso_path(new output) with 1D linear interpolation</span>
    <span class="s0"># to compute the same path</span>
    <span class="s1">alphas_lars</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">coef_path_lars = lars_path(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">method=</span><span class="s3">&quot;lasso&quot;</span><span class="s1">)</span>
    <span class="s1">coef_path_cont_lars = interpolate.interp1d(</span>
        <span class="s1">alphas_lars[::-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">coef_path_lars[:</span><span class="s2">, </span><span class="s1">::-</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">alphas_lasso2</span><span class="s2">, </span><span class="s1">coef_path_lasso2</span><span class="s2">, </span><span class="s1">_ = lasso_path(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">alphas=alphas)</span>
    <span class="s1">coef_path_cont_lasso = interpolate.interp1d(</span>
        <span class="s1">alphas_lasso2[::-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">coef_path_lasso2[:</span><span class="s2">, </span><span class="s1">::-</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">coef_path_cont_lasso(alphas)</span><span class="s2">, </span><span class="s1">coef_path_cont_lars(alphas)</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">1</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_enet_path():</span>
    <span class="s0"># We use a large number of samples and of informative features so that</span>
    <span class="s0"># the l1_ratio selected is more toward ridge than lasso</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test = build_dataset(</span>
        <span class="s1">n_samples=</span><span class="s5">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">100</span><span class="s2">, </span><span class="s1">n_informative_features=</span><span class="s5">100</span>
    <span class="s1">)</span>
    <span class="s1">max_iter = </span><span class="s5">150</span>

    <span class="s0"># Here we have a small number of iterations, and thus the</span>
    <span class="s0"># ElasticNet might not converge. This is to speed up tests</span>
    <span class="s1">clf = ElasticNetCV(</span>
        <span class="s1">alphas=[</span><span class="s5">0.01</span><span class="s2">, </span><span class="s5">0.05</span><span class="s2">, </span><span class="s5">0.1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">2e-3</span><span class="s2">, </span><span class="s1">l1_ratio=[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0.7</span><span class="s1">]</span><span class="s2">, </span><span class="s1">cv=</span><span class="s5">3</span><span class="s2">, </span><span class="s1">max_iter=max_iter</span>
    <span class="s1">)</span>
    <span class="s1">ignore_warnings(clf.fit)(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s0"># Well-conditioned settings, we should have selected our</span>
    <span class="s0"># smallest penalty</span>
    <span class="s1">assert_almost_equal(clf.alpha_</span><span class="s2">, </span><span class="s1">min(clf.alphas_))</span>
    <span class="s0"># Non-sparse ground truth: we should have selected an elastic-net</span>
    <span class="s0"># that is closer to ridge than to lasso</span>
    <span class="s2">assert </span><span class="s1">clf.l1_ratio_ == min(clf.l1_ratio)</span>

    <span class="s1">clf = ElasticNetCV(</span>
        <span class="s1">alphas=[</span><span class="s5">0.01</span><span class="s2">, </span><span class="s5">0.05</span><span class="s2">, </span><span class="s5">0.1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">eps=</span><span class="s5">2e-3</span><span class="s2">,</span>
        <span class="s1">l1_ratio=[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0.7</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">cv=</span><span class="s5">3</span><span class="s2">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
        <span class="s1">precompute=</span><span class="s2">True,</span>
    <span class="s1">)</span>
    <span class="s1">ignore_warnings(clf.fit)(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s0"># Well-conditioned settings, we should have selected our</span>
    <span class="s0"># smallest penalty</span>
    <span class="s1">assert_almost_equal(clf.alpha_</span><span class="s2">, </span><span class="s1">min(clf.alphas_))</span>
    <span class="s0"># Non-sparse ground truth: we should have selected an elastic-net</span>
    <span class="s0"># that is closer to ridge than to lasso</span>
    <span class="s2">assert </span><span class="s1">clf.l1_ratio_ == min(clf.l1_ratio)</span>

    <span class="s0"># We are in well-conditioned settings with low noise: we should</span>
    <span class="s0"># have a good test-set performance</span>
    <span class="s2">assert </span><span class="s1">clf.score(X_test</span><span class="s2">, </span><span class="s1">y_test) &gt; </span><span class="s5">0.99</span>

    <span class="s0"># Multi-output/target case</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test = build_dataset(n_features=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">n_targets=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">clf = MultiTaskElasticNetCV(</span>
        <span class="s1">n_alphas=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">2e-3</span><span class="s2">, </span><span class="s1">l1_ratio=[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0.7</span><span class="s1">]</span><span class="s2">, </span><span class="s1">cv=</span><span class="s5">3</span><span class="s2">, </span><span class="s1">max_iter=max_iter</span>
    <span class="s1">)</span>
    <span class="s1">ignore_warnings(clf.fit)(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s0"># We are in well-conditioned settings with low noise: we should</span>
    <span class="s0"># have a good test-set performance</span>
    <span class="s2">assert </span><span class="s1">clf.score(X_test</span><span class="s2">, </span><span class="s1">y_test) &gt; </span><span class="s5">0.99</span>
    <span class="s2">assert </span><span class="s1">clf.coef_.shape == (</span><span class="s5">3</span><span class="s2">, </span><span class="s5">10</span><span class="s1">)</span>

    <span class="s0"># Mono-output should have same cross-validated alpha_ and l1_ratio_</span>
    <span class="s0"># in both cases.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_features=</span><span class="s5">10</span><span class="s1">)</span>
    <span class="s1">clf1 = ElasticNetCV(n_alphas=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">2e-3</span><span class="s2">, </span><span class="s1">l1_ratio=[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0.7</span><span class="s1">])</span>
    <span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf2 = MultiTaskElasticNetCV(n_alphas=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">2e-3</span><span class="s2">, </span><span class="s1">l1_ratio=[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0.7</span><span class="s1">])</span>
    <span class="s1">clf2.fit(X</span><span class="s2">, </span><span class="s1">y[:</span><span class="s2">, </span><span class="s1">np.newaxis])</span>
    <span class="s1">assert_almost_equal(clf1.l1_ratio_</span><span class="s2">, </span><span class="s1">clf2.l1_ratio_)</span>
    <span class="s1">assert_almost_equal(clf1.alpha_</span><span class="s2">, </span><span class="s1">clf2.alpha_)</span>


<span class="s2">def </span><span class="s1">test_path_parameters():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset()</span>
    <span class="s1">max_iter = </span><span class="s5">100</span>

    <span class="s1">clf = ElasticNetCV(n_alphas=</span><span class="s5">50</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">1e-3</span><span class="s2">, </span><span class="s1">max_iter=max_iter</span><span class="s2">, </span><span class="s1">l1_ratio=</span><span class="s5">0.5</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-3</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)  </span><span class="s0"># new params</span>
    <span class="s1">assert_almost_equal(</span><span class="s5">0.5</span><span class="s2">, </span><span class="s1">clf.l1_ratio)</span>
    <span class="s2">assert </span><span class="s5">50 </span><span class="s1">== clf.n_alphas</span>
    <span class="s2">assert </span><span class="s5">50 </span><span class="s1">== len(clf.alphas_)</span>


<span class="s2">def </span><span class="s1">test_warm_start():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset()</span>
    <span class="s1">clf = ElasticNet(alpha=</span><span class="s5">0.1</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">warm_start=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">ignore_warnings(clf.fit)(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">ignore_warnings(clf.fit)(X</span><span class="s2">, </span><span class="s1">y)  </span><span class="s0"># do a second round with 5 iterations</span>

    <span class="s1">clf2 = ElasticNet(alpha=</span><span class="s5">0.1</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s5">10</span><span class="s1">)</span>
    <span class="s1">ignore_warnings(clf2.fit)(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(clf2.coef_</span><span class="s2">, </span><span class="s1">clf.coef_)</span>


<span class="s2">def </span><span class="s1">test_lasso_alpha_warning():</span>
    <span class="s1">X = [[-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">]]</span>
    <span class="s1">Y = [-</span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]  </span><span class="s0"># just a straight line</span>

    <span class="s1">clf = Lasso(alpha=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">warning_message = (</span>
        <span class="s3">&quot;With alpha=0, this algorithm does not &quot;</span>
        <span class="s3">&quot;converge well. You are advised to use the &quot;</span>
        <span class="s3">&quot;LinearRegression estimator&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s2">, </span><span class="s1">match=warning_message):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>


<span class="s2">def </span><span class="s1">test_lasso_positive_constraint():</span>
    <span class="s1">X = [[-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">]  </span><span class="s0"># just a straight line with negative slope</span>

    <span class="s1">lasso = Lasso(alpha=</span><span class="s5">0.1</span><span class="s2">, </span><span class="s1">positive=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">lasso.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">min(lasso.coef_) &gt;= </span><span class="s5">0</span>

    <span class="s1">lasso = Lasso(alpha=</span><span class="s5">0.1</span><span class="s2">, </span><span class="s1">precompute=</span><span class="s2">True, </span><span class="s1">positive=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">lasso.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">min(lasso.coef_) &gt;= </span><span class="s5">0</span>


<span class="s2">def </span><span class="s1">test_enet_positive_constraint():</span>
    <span class="s1">X = [[-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">]  </span><span class="s0"># just a straight line with negative slope</span>

    <span class="s1">enet = ElasticNet(alpha=</span><span class="s5">0.1</span><span class="s2">, </span><span class="s1">positive=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">enet.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">min(enet.coef_) &gt;= </span><span class="s5">0</span>


<span class="s2">def </span><span class="s1">test_enet_cv_positive_constraint():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test = build_dataset()</span>
    <span class="s1">max_iter = </span><span class="s5">500</span>

    <span class="s0"># Ensure the unconstrained fit has a negative coefficient</span>
    <span class="s1">enetcv_unconstrained = ElasticNetCV(</span>
        <span class="s1">n_alphas=</span><span class="s5">3</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">1e-1</span><span class="s2">, </span><span class="s1">max_iter=max_iter</span><span class="s2">, </span><span class="s1">cv=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">n_jobs=</span><span class="s5">1</span>
    <span class="s1">)</span>
    <span class="s1">enetcv_unconstrained.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">min(enetcv_unconstrained.coef_) &lt; </span><span class="s5">0</span>

    <span class="s0"># On same data, constrained fit has non-negative coefficients</span>
    <span class="s1">enetcv_constrained = ElasticNetCV(</span>
        <span class="s1">n_alphas=</span><span class="s5">3</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">1e-1</span><span class="s2">, </span><span class="s1">max_iter=max_iter</span><span class="s2">, </span><span class="s1">cv=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">positive=</span><span class="s2">True, </span><span class="s1">n_jobs=</span><span class="s5">1</span>
    <span class="s1">)</span>
    <span class="s1">enetcv_constrained.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">min(enetcv_constrained.coef_) &gt;= </span><span class="s5">0</span>


<span class="s2">def </span><span class="s1">test_uniform_targets():</span>
    <span class="s1">enet = ElasticNetCV(n_alphas=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">m_enet = MultiTaskElasticNetCV(n_alphas=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">lasso = LassoCV(n_alphas=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">m_lasso = MultiTaskLassoCV(n_alphas=</span><span class="s5">3</span><span class="s1">)</span>

    <span class="s1">models_single_task = (enet</span><span class="s2">, </span><span class="s1">lasso)</span>
    <span class="s1">models_multi_task = (m_enet</span><span class="s2">, </span><span class="s1">m_lasso)</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">X_train = rng.random_sample(size=(</span><span class="s5">10</span><span class="s2">, </span><span class="s5">3</span><span class="s1">))</span>
    <span class="s1">X_test = rng.random_sample(size=(</span><span class="s5">10</span><span class="s2">, </span><span class="s5">3</span><span class="s1">))</span>

    <span class="s1">y1 = np.empty(</span><span class="s5">10</span><span class="s1">)</span>
    <span class="s1">y2 = np.empty((</span><span class="s5">10</span><span class="s2">, </span><span class="s5">2</span><span class="s1">))</span>

    <span class="s2">for </span><span class="s1">model </span><span class="s2">in </span><span class="s1">models_single_task:</span>
        <span class="s2">for </span><span class="s1">y_values </span><span class="s2">in </span><span class="s1">(</span><span class="s5">0</span><span class="s2">, </span><span class="s5">5</span><span class="s1">):</span>
            <span class="s1">y1.fill(y_values)</span>
            <span class="s1">assert_array_equal(model.fit(X_train</span><span class="s2">, </span><span class="s1">y1).predict(X_test)</span><span class="s2">, </span><span class="s1">y1)</span>
            <span class="s1">assert_array_equal(model.alphas_</span><span class="s2">, </span><span class="s1">[np.finfo(float).resolution] * </span><span class="s5">3</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">model </span><span class="s2">in </span><span class="s1">models_multi_task:</span>
        <span class="s2">for </span><span class="s1">y_values </span><span class="s2">in </span><span class="s1">(</span><span class="s5">0</span><span class="s2">, </span><span class="s5">5</span><span class="s1">):</span>
            <span class="s1">y2[:</span><span class="s2">, </span><span class="s5">0</span><span class="s1">].fill(y_values)</span>
            <span class="s1">y2[:</span><span class="s2">, </span><span class="s5">1</span><span class="s1">].fill(</span><span class="s5">2 </span><span class="s1">* y_values)</span>
            <span class="s1">assert_array_equal(model.fit(X_train</span><span class="s2">, </span><span class="s1">y2).predict(X_test)</span><span class="s2">, </span><span class="s1">y2)</span>
            <span class="s1">assert_array_equal(model.alphas_</span><span class="s2">, </span><span class="s1">[np.finfo(float).resolution] * </span><span class="s5">3</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_multi_task_lasso_and_enet():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test = build_dataset()</span>
    <span class="s1">Y = np.c_[y</span><span class="s2">, </span><span class="s1">y]</span>
    <span class="s0"># Y_test = np.c_[y_test, y_test]</span>
    <span class="s1">clf = MultiTaskLasso(alpha=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s2">assert </span><span class="s5">0 </span><span class="s1">&lt; clf.dual_gap_ &lt; </span><span class="s5">1e-5</span>
    <span class="s1">assert_array_almost_equal(clf.coef_[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">clf.coef_[</span><span class="s5">1</span><span class="s1">])</span>

    <span class="s1">clf = MultiTaskElasticNet(alpha=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s2">assert </span><span class="s5">0 </span><span class="s1">&lt; clf.dual_gap_ &lt; </span><span class="s5">1e-5</span>
    <span class="s1">assert_array_almost_equal(clf.coef_[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">clf.coef_[</span><span class="s5">1</span><span class="s1">])</span>

    <span class="s1">clf = MultiTaskElasticNet(alpha=</span><span class="s5">1.0</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">warning_message = (</span>
        <span class="s3">&quot;Objective did not converge. You might want to &quot;</span>
        <span class="s3">&quot;increase the number of iterations.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s2">, </span><span class="s1">match=warning_message):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>


<span class="s2">def </span><span class="s1">test_lasso_readonly_data():</span>
    <span class="s1">X = np.array([[-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">]])</span>
    <span class="s1">Y = np.array([-</span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s1">])  </span><span class="s0"># just a straight line</span>
    <span class="s1">T = np.array([[</span><span class="s5">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">4</span><span class="s1">]])  </span><span class="s0"># test sample</span>
    <span class="s2">with </span><span class="s1">TempMemmap((X</span><span class="s2">, </span><span class="s1">Y)) </span><span class="s2">as </span><span class="s1">(X</span><span class="s2">, </span><span class="s1">Y):</span>
        <span class="s1">clf = Lasso(alpha=</span><span class="s5">0.5</span><span class="s1">)</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
        <span class="s1">pred = clf.predict(T)</span>
        <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.25</span><span class="s1">])</span>
        <span class="s1">assert_array_almost_equal(pred</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0.75</span><span class="s2">, </span><span class="s5">1.0</span><span class="s1">])</span>
        <span class="s1">assert_almost_equal(clf.dual_gap_</span><span class="s2">, </span><span class="s5">0</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_multi_task_lasso_readonly_data():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test = build_dataset()</span>
    <span class="s1">Y = np.c_[y</span><span class="s2">, </span><span class="s1">y]</span>
    <span class="s2">with </span><span class="s1">TempMemmap((X</span><span class="s2">, </span><span class="s1">Y)) </span><span class="s2">as </span><span class="s1">(X</span><span class="s2">, </span><span class="s1">Y):</span>
        <span class="s1">Y = np.c_[y</span><span class="s2">, </span><span class="s1">y]</span>
        <span class="s1">clf = MultiTaskLasso(alpha=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
        <span class="s2">assert </span><span class="s5">0 </span><span class="s1">&lt; clf.dual_gap_ &lt; </span><span class="s5">1e-5</span>
        <span class="s1">assert_array_almost_equal(clf.coef_[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">clf.coef_[</span><span class="s5">1</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_enet_multitarget():</span>
    <span class="s1">n_targets = </span><span class="s5">3</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(</span>
        <span class="s1">n_samples=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">8</span><span class="s2">, </span><span class="s1">n_informative_features=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">n_targets=n_targets</span>
    <span class="s1">)</span>
    <span class="s1">estimator = ElasticNet(alpha=</span><span class="s5">0.01</span><span class="s1">)</span>
    <span class="s1">estimator.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">coef</span><span class="s2">, </span><span class="s1">intercept</span><span class="s2">, </span><span class="s1">dual_gap = (</span>
        <span class="s1">estimator.coef_</span><span class="s2">,</span>
        <span class="s1">estimator.intercept_</span><span class="s2">,</span>
        <span class="s1">estimator.dual_gap_</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(n_targets):</span>
        <span class="s1">estimator.fit(X</span><span class="s2">, </span><span class="s1">y[:</span><span class="s2">, </span><span class="s1">k])</span>
        <span class="s1">assert_array_almost_equal(coef[k</span><span class="s2">, </span><span class="s1">:]</span><span class="s2">, </span><span class="s1">estimator.coef_)</span>
        <span class="s1">assert_array_almost_equal(intercept[k]</span><span class="s2">, </span><span class="s1">estimator.intercept_)</span>
        <span class="s1">assert_array_almost_equal(dual_gap[k]</span><span class="s2">, </span><span class="s1">estimator.dual_gap_)</span>


<span class="s2">def </span><span class="s1">test_multioutput_enetcv_error():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(</span><span class="s5">10</span><span class="s2">, </span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">y = rng.randn(</span><span class="s5">10</span><span class="s2">, </span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">clf = ElasticNetCV()</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_multitask_enet_and_lasso_cv():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_features=</span><span class="s5">50</span><span class="s2">, </span><span class="s1">n_targets=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">clf = MultiTaskElasticNetCV(cv=</span><span class="s5">3</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(clf.alpha_</span><span class="s2">, </span><span class="s5">0.00556</span><span class="s2">, </span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">clf = MultiTaskLassoCV(cv=</span><span class="s5">3</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(clf.alpha_</span><span class="s2">, </span><span class="s5">0.00278</span><span class="s2">, </span><span class="s5">3</span><span class="s1">)</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_targets=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">clf = MultiTaskElasticNetCV(</span>
        <span class="s1">n_alphas=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">1e-3</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s5">100</span><span class="s2">, </span><span class="s1">l1_ratio=[</span><span class="s5">0.3</span><span class="s2">, </span><span class="s5">0.5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-3</span><span class="s2">, </span><span class="s1">cv=</span><span class="s5">3</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s5">0.5 </span><span class="s1">== clf.l1_ratio_</span>
    <span class="s2">assert </span><span class="s1">(</span><span class="s5">3</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">]) == clf.coef_.shape</span>
    <span class="s2">assert </span><span class="s1">(</span><span class="s5">3</span><span class="s2">,</span><span class="s1">) == clf.intercept_.shape</span>
    <span class="s2">assert </span><span class="s1">(</span><span class="s5">2</span><span class="s2">, </span><span class="s5">10</span><span class="s2">, </span><span class="s5">3</span><span class="s1">) == clf.mse_path_.shape</span>
    <span class="s2">assert </span><span class="s1">(</span><span class="s5">2</span><span class="s2">, </span><span class="s5">10</span><span class="s1">) == clf.alphas_.shape</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_targets=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">clf = MultiTaskLassoCV(n_alphas=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">1e-3</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s5">100</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-3</span><span class="s2">, </span><span class="s1">cv=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">(</span><span class="s5">3</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">]) == clf.coef_.shape</span>
    <span class="s2">assert </span><span class="s1">(</span><span class="s5">3</span><span class="s2">,</span><span class="s1">) == clf.intercept_.shape</span>
    <span class="s2">assert </span><span class="s1">(</span><span class="s5">10</span><span class="s2">, </span><span class="s5">3</span><span class="s1">) == clf.mse_path_.shape</span>
    <span class="s2">assert </span><span class="s5">10 </span><span class="s1">== len(clf.alphas_)</span>


<span class="s2">def </span><span class="s1">test_1d_multioutput_enet_and_multitask_enet_cv():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_features=</span><span class="s5">10</span><span class="s1">)</span>
    <span class="s1">y = y[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">clf = ElasticNetCV(n_alphas=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">2e-3</span><span class="s2">, </span><span class="s1">l1_ratio=[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0.7</span><span class="s1">])</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y[:</span><span class="s2">, </span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">clf1 = MultiTaskElasticNetCV(n_alphas=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">2e-3</span><span class="s2">, </span><span class="s1">l1_ratio=[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0.7</span><span class="s1">])</span>
    <span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(clf.l1_ratio_</span><span class="s2">, </span><span class="s1">clf1.l1_ratio_)</span>
    <span class="s1">assert_almost_equal(clf.alpha_</span><span class="s2">, </span><span class="s1">clf1.alpha_)</span>
    <span class="s1">assert_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">clf1.coef_[</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">assert_almost_equal(clf.intercept_</span><span class="s2">, </span><span class="s1">clf1.intercept_[</span><span class="s5">0</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_1d_multioutput_lasso_and_multitask_lasso_cv():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_features=</span><span class="s5">10</span><span class="s1">)</span>
    <span class="s1">y = y[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">clf = LassoCV(n_alphas=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">2e-3</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y[:</span><span class="s2">, </span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">clf1 = MultiTaskLassoCV(n_alphas=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">eps=</span><span class="s5">2e-3</span><span class="s1">)</span>
    <span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(clf.alpha_</span><span class="s2">, </span><span class="s1">clf1.alpha_)</span>
    <span class="s1">assert_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">clf1.coef_[</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">assert_almost_equal(clf.intercept_</span><span class="s2">, </span><span class="s1">clf1.intercept_[</span><span class="s5">0</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_sparse_input_dtype_enet_and_lassocv():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_features=</span><span class="s5">10</span><span class="s1">)</span>
    <span class="s1">clf = ElasticNetCV(n_alphas=</span><span class="s5">5</span><span class="s1">)</span>
    <span class="s1">clf.fit(sparse.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf1 = ElasticNetCV(n_alphas=</span><span class="s5">5</span><span class="s1">)</span>
    <span class="s1">clf1.fit(sparse.csr_matrix(X</span><span class="s2">, </span><span class="s1">dtype=np.float32)</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(clf.alpha_</span><span class="s2">, </span><span class="s1">clf1.alpha_</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">6</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">clf1.coef_</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">6</span><span class="s1">)</span>

    <span class="s1">clf = LassoCV(n_alphas=</span><span class="s5">5</span><span class="s1">)</span>
    <span class="s1">clf.fit(sparse.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf1 = LassoCV(n_alphas=</span><span class="s5">5</span><span class="s1">)</span>
    <span class="s1">clf1.fit(sparse.csr_matrix(X</span><span class="s2">, </span><span class="s1">dtype=np.float32)</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(clf.alpha_</span><span class="s2">, </span><span class="s1">clf1.alpha_</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">6</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">clf1.coef_</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">6</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_elasticnet_precompute_incorrect_gram():</span>
    <span class="s0"># check that passing an invalid precomputed Gram matrix will raise an</span>
    <span class="s0"># error.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset()</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">X_centered = X - np.average(X</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">garbage = rng.standard_normal(X.shape)</span>
    <span class="s1">precompute = np.dot(garbage.T</span><span class="s2">, </span><span class="s1">garbage)</span>

    <span class="s1">clf = ElasticNet(alpha=</span><span class="s5">0.01</span><span class="s2">, </span><span class="s1">precompute=precompute)</span>
    <span class="s1">msg = </span><span class="s3">&quot;Gram matrix.*did not pass validation.*&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.fit(X_centered</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_elasticnet_precompute_gram_weighted_samples():</span>
    <span class="s0"># check the equivalence between passing a precomputed Gram matrix and</span>
    <span class="s0"># internal computation using sample weights.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset()</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">sample_weight = rng.lognormal(size=y.shape)</span>

    <span class="s1">w_norm = sample_weight * (y.shape / np.sum(sample_weight))</span>
    <span class="s1">X_c = X - np.average(X</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">weights=w_norm)</span>
    <span class="s1">X_r = X_c * np.sqrt(w_norm)[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">gram = np.dot(X_r.T</span><span class="s2">, </span><span class="s1">X_r)</span>

    <span class="s1">clf1 = ElasticNet(alpha=</span><span class="s5">0.01</span><span class="s2">, </span><span class="s1">precompute=gram)</span>
    <span class="s1">clf1.fit(X_c</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">clf2 = ElasticNet(alpha=</span><span class="s5">0.01</span><span class="s2">, </span><span class="s1">precompute=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">clf2.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">assert_allclose(clf1.coef_</span><span class="s2">, </span><span class="s1">clf2.coef_)</span>


<span class="s2">def </span><span class="s1">test_elasticnet_precompute_gram():</span>
    <span class="s0"># Check the dtype-aware check for a precomputed Gram matrix</span>
    <span class="s0"># (see https://github.com/scikit-learn/scikit-learn/pull/22059</span>
    <span class="s0"># and https://github.com/scikit-learn/scikit-learn/issues/21997).</span>
    <span class="s0"># Here: (X_c.T, X_c)[2, 3] is not equal to np.dot(X_c[:, 2], X_c[:, 3])</span>
    <span class="s0"># but within tolerance for np.float32</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s5">58</span><span class="s1">)</span>
    <span class="s1">X = rng.binomial(</span><span class="s5">1</span><span class="s2">, </span><span class="s5">0.25</span><span class="s2">, </span><span class="s1">(</span><span class="s5">1000</span><span class="s2">, </span><span class="s5">4</span><span class="s1">)).astype(np.float32)</span>
    <span class="s1">y = rng.rand(</span><span class="s5">1000</span><span class="s1">).astype(np.float32)</span>

    <span class="s1">X_c = X - np.average(X</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">gram = np.dot(X_c.T</span><span class="s2">, </span><span class="s1">X_c)</span>

    <span class="s1">clf1 = ElasticNet(alpha=</span><span class="s5">0.01</span><span class="s2">, </span><span class="s1">precompute=gram)</span>
    <span class="s1">clf1.fit(X_c</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">clf2 = ElasticNet(alpha=</span><span class="s5">0.01</span><span class="s2">, </span><span class="s1">precompute=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">clf2.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">assert_allclose(clf1.coef_</span><span class="s2">, </span><span class="s1">clf2.coef_)</span>


<span class="s2">def </span><span class="s1">test_warm_start_convergence():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset()</span>
    <span class="s1">model = ElasticNet(alpha=</span><span class="s5">1e-3</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-3</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">n_iter_reference = model.n_iter_</span>

    <span class="s0"># This dataset is not trivial enough for the model to converge in one pass.</span>
    <span class="s2">assert </span><span class="s1">n_iter_reference &gt; </span><span class="s5">2</span>

    <span class="s0"># Check that n_iter_ is invariant to multiple calls to fit</span>
    <span class="s0"># when warm_start=False, all else being equal.</span>
    <span class="s1">model.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">n_iter_cold_start = model.n_iter_</span>
    <span class="s2">assert </span><span class="s1">n_iter_cold_start == n_iter_reference</span>

    <span class="s0"># Fit the same model again, using a warm start: the optimizer just performs</span>
    <span class="s0"># a single pass before checking that it has already converged</span>
    <span class="s1">model.set_params(warm_start=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">model.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">n_iter_warm_start = model.n_iter_</span>
    <span class="s2">assert </span><span class="s1">n_iter_warm_start == </span><span class="s5">1</span>


<span class="s2">def </span><span class="s1">test_warm_start_convergence_with_regularizer_decrement():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = load_diabetes(return_X_y=</span><span class="s2">True</span><span class="s1">)</span>

    <span class="s0"># Train a model to converge on a lightly regularized problem</span>
    <span class="s1">final_alpha = </span><span class="s5">1e-5</span>
    <span class="s1">low_reg_model = ElasticNet(alpha=final_alpha).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s0"># Fitting a new model on a more regularized version of the same problem.</span>
    <span class="s0"># Fitting with high regularization is easier it should converge faster</span>
    <span class="s0"># in general.</span>
    <span class="s1">high_reg_model = ElasticNet(alpha=final_alpha * </span><span class="s5">10</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">low_reg_model.n_iter_ &gt; high_reg_model.n_iter_</span>

    <span class="s0"># Fit the solution to the original, less regularized version of the</span>
    <span class="s0"># problem but from the solution of the highly regularized variant of</span>
    <span class="s0"># the problem as a better starting point. This should also converge</span>
    <span class="s0"># faster than the original model that starts from zero.</span>
    <span class="s1">warm_low_reg_model = deepcopy(high_reg_model)</span>
    <span class="s1">warm_low_reg_model.set_params(warm_start=</span><span class="s2">True, </span><span class="s1">alpha=final_alpha)</span>
    <span class="s1">warm_low_reg_model.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">low_reg_model.n_iter_ &gt; warm_low_reg_model.n_iter_</span>


<span class="s2">def </span><span class="s1">test_random_descent():</span>
    <span class="s0"># Test that both random and cyclic selection give the same results.</span>
    <span class="s0"># Ensure that the test models fully converge and check a wide</span>
    <span class="s0"># range of conditions.</span>

    <span class="s0"># This uses the coordinate descent algo using the gram trick.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_samples=</span><span class="s5">50</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">20</span><span class="s1">)</span>
    <span class="s1">clf_cyclic = ElasticNet(selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s1">)</span>
    <span class="s1">clf_cyclic.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf_random = ElasticNet(selection=</span><span class="s3">&quot;random&quot;</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">42</span><span class="s1">)</span>
    <span class="s1">clf_random.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(clf_cyclic.coef_</span><span class="s2">, </span><span class="s1">clf_random.coef_)</span>
    <span class="s1">assert_almost_equal(clf_cyclic.intercept_</span><span class="s2">, </span><span class="s1">clf_random.intercept_)</span>

    <span class="s0"># This uses the descent algo without the gram trick</span>
    <span class="s1">clf_cyclic = ElasticNet(selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s1">)</span>
    <span class="s1">clf_cyclic.fit(X.T</span><span class="s2">, </span><span class="s1">y[:</span><span class="s5">20</span><span class="s1">])</span>
    <span class="s1">clf_random = ElasticNet(selection=</span><span class="s3">&quot;random&quot;</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">42</span><span class="s1">)</span>
    <span class="s1">clf_random.fit(X.T</span><span class="s2">, </span><span class="s1">y[:</span><span class="s5">20</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(clf_cyclic.coef_</span><span class="s2">, </span><span class="s1">clf_random.coef_)</span>
    <span class="s1">assert_almost_equal(clf_cyclic.intercept_</span><span class="s2">, </span><span class="s1">clf_random.intercept_)</span>

    <span class="s0"># Sparse Case</span>
    <span class="s1">clf_cyclic = ElasticNet(selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s1">)</span>
    <span class="s1">clf_cyclic.fit(sparse.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf_random = ElasticNet(selection=</span><span class="s3">&quot;random&quot;</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">42</span><span class="s1">)</span>
    <span class="s1">clf_random.fit(sparse.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(clf_cyclic.coef_</span><span class="s2">, </span><span class="s1">clf_random.coef_)</span>
    <span class="s1">assert_almost_equal(clf_cyclic.intercept_</span><span class="s2">, </span><span class="s1">clf_random.intercept_)</span>

    <span class="s0"># Multioutput case.</span>
    <span class="s1">new_y = np.hstack((y[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span><span class="s2">, </span><span class="s1">y[:</span><span class="s2">, </span><span class="s1">np.newaxis]))</span>
    <span class="s1">clf_cyclic = MultiTaskElasticNet(selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s1">)</span>
    <span class="s1">clf_cyclic.fit(X</span><span class="s2">, </span><span class="s1">new_y)</span>
    <span class="s1">clf_random = MultiTaskElasticNet(selection=</span><span class="s3">&quot;random&quot;</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">42</span><span class="s1">)</span>
    <span class="s1">clf_random.fit(X</span><span class="s2">, </span><span class="s1">new_y)</span>
    <span class="s1">assert_array_almost_equal(clf_cyclic.coef_</span><span class="s2">, </span><span class="s1">clf_random.coef_)</span>
    <span class="s1">assert_almost_equal(clf_cyclic.intercept_</span><span class="s2">, </span><span class="s1">clf_random.intercept_)</span>


<span class="s2">def </span><span class="s1">test_enet_path_positive():</span>
    <span class="s0"># Test positive parameter</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_samples=</span><span class="s5">50</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">50</span><span class="s2">, </span><span class="s1">n_targets=</span><span class="s5">2</span><span class="s1">)</span>

    <span class="s0"># For mono output</span>
    <span class="s0"># Test that the coefs returned by positive=True in enet_path are positive</span>
    <span class="s2">for </span><span class="s1">path </span><span class="s2">in </span><span class="s1">[enet_path</span><span class="s2">, </span><span class="s1">lasso_path]:</span>
        <span class="s1">pos_path_coef = path(X</span><span class="s2">, </span><span class="s1">Y[:</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">positive=</span><span class="s2">True</span><span class="s1">)[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s2">assert </span><span class="s1">np.all(pos_path_coef &gt;= </span><span class="s5">0</span><span class="s1">)</span>

    <span class="s0"># For multi output, positive parameter is not allowed</span>
    <span class="s0"># Test that an error is raised</span>
    <span class="s2">for </span><span class="s1">path </span><span class="s2">in </span><span class="s1">[enet_path</span><span class="s2">, </span><span class="s1">lasso_path]:</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
            <span class="s1">path(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">positive=</span><span class="s2">True</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_sparse_dense_descent_paths():</span>
    <span class="s0"># Test that dense and sparse input give the same input for descent paths.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_samples=</span><span class="s5">50</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">20</span><span class="s1">)</span>
    <span class="s1">csr = sparse.csr_matrix(X)</span>
    <span class="s2">for </span><span class="s1">path </span><span class="s2">in </span><span class="s1">[enet_path</span><span class="s2">, </span><span class="s1">lasso_path]:</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">coefs</span><span class="s2">, </span><span class="s1">_ = path(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">sparse_coefs</span><span class="s2">, </span><span class="s1">_ = path(csr</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">assert_array_almost_equal(coefs</span><span class="s2">, </span><span class="s1">sparse_coefs)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;path_func&quot;</span><span class="s2">, </span><span class="s1">[enet_path</span><span class="s2">, </span><span class="s1">lasso_path])</span>
<span class="s2">def </span><span class="s1">test_path_unknown_parameter(path_func):</span>
    <span class="s4">&quot;&quot;&quot;Check that passing parameter not used by the coordinate descent solver 
    will raise an error.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_samples=</span><span class="s5">50</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">20</span><span class="s1">)</span>
    <span class="s1">err_msg = </span><span class="s3">&quot;Unexpected parameters in params&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">path_func(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">normalize=</span><span class="s2">True, </span><span class="s1">fit_intercept=</span><span class="s2">True</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_check_input_false():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_samples=</span><span class="s5">20</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">10</span><span class="s1">)</span>
    <span class="s1">X = check_array(X</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s2">, </span><span class="s1">dtype=</span><span class="s3">&quot;float64&quot;</span><span class="s1">)</span>
    <span class="s1">y = check_array(X</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s2">, </span><span class="s1">dtype=</span><span class="s3">&quot;float64&quot;</span><span class="s1">)</span>
    <span class="s1">clf = ElasticNet(selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s1">)</span>
    <span class="s0"># Check that no error is raised if data is provided in the right format</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">check_input=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s0"># With check_input=False, an exhaustive check is not made on y but its</span>
    <span class="s0"># dtype is still cast in _preprocess_data to X's dtype. So the test should</span>
    <span class="s0"># pass anyway</span>
    <span class="s1">X = check_array(X</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s2">, </span><span class="s1">dtype=</span><span class="s3">&quot;float32&quot;</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">check_input=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s0"># With no input checking, providing X in C order should result in false</span>
    <span class="s0"># computation</span>
    <span class="s1">X = check_array(X</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;C&quot;</span><span class="s2">, </span><span class="s1">dtype=</span><span class="s3">&quot;float64&quot;</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">check_input=</span><span class="s2">False</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;check_input&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_enet_copy_X_True(check_input):</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset()</span>
    <span class="s1">X = X.copy(order=</span><span class="s3">&quot;F&quot;</span><span class="s1">)</span>

    <span class="s1">original_X = X.copy()</span>
    <span class="s1">enet = ElasticNet(copy_X=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">enet.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">check_input=check_input)</span>

    <span class="s1">assert_array_equal(original_X</span><span class="s2">, </span><span class="s1">X)</span>


<span class="s2">def </span><span class="s1">test_enet_copy_X_False_check_input_False():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset()</span>
    <span class="s1">X = X.copy(order=</span><span class="s3">&quot;F&quot;</span><span class="s1">)</span>

    <span class="s1">original_X = X.copy()</span>
    <span class="s1">enet = ElasticNet(copy_X=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">enet.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">check_input=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s0"># No copying, X is overwritten</span>
    <span class="s2">assert </span><span class="s1">np.any(np.not_equal(original_X</span><span class="s2">, </span><span class="s1">X))</span>


<span class="s2">def </span><span class="s1">test_overrided_gram_matrix():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_samples=</span><span class="s5">20</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">10</span><span class="s1">)</span>
    <span class="s1">Gram = X.T.dot(X)</span>
    <span class="s1">clf = ElasticNet(selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-8</span><span class="s2">, </span><span class="s1">precompute=Gram)</span>
    <span class="s1">warning_message = (</span>
        <span class="s3">&quot;Gram matrix was provided but X was centered&quot;</span>
        <span class="s3">&quot; to fit intercept, &quot;</span>
        <span class="s3">&quot;or X was normalized : recomputing Gram matrix.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s2">, </span><span class="s1">match=warning_message):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;model&quot;</span><span class="s2">, </span><span class="s1">[ElasticNet</span><span class="s2">, </span><span class="s1">Lasso])</span>
<span class="s2">def </span><span class="s1">test_lasso_non_float_y(model):</span>
    <span class="s1">X = [[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s5">1</span><span class="s2">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">2</span><span class="s1">]</span>
    <span class="s1">y_float = [</span><span class="s5">0.0</span><span class="s2">, </span><span class="s5">1.0</span><span class="s2">, </span><span class="s5">2.0</span><span class="s1">]</span>

    <span class="s1">clf = model(fit_intercept=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf_float = model(fit_intercept=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">clf_float.fit(X</span><span class="s2">, </span><span class="s1">y_float)</span>
    <span class="s1">assert_array_equal(clf.coef_</span><span class="s2">, </span><span class="s1">clf_float.coef_)</span>


<span class="s2">def </span><span class="s1">test_enet_float_precision():</span>
    <span class="s0"># Generate dataset</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test = build_dataset(n_samples=</span><span class="s5">20</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">10</span><span class="s1">)</span>
    <span class="s0"># Here we have a small number of iterations, and thus the</span>
    <span class="s0"># ElasticNet might not converge. This is to speed up tests</span>

    <span class="s2">for </span><span class="s1">fit_intercept </span><span class="s2">in </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">]:</span>
        <span class="s1">coef = {}</span>
        <span class="s1">intercept = {}</span>
        <span class="s2">for </span><span class="s1">dtype </span><span class="s2">in </span><span class="s1">[np.float64</span><span class="s2">, </span><span class="s1">np.float32]:</span>
            <span class="s1">clf = ElasticNet(</span>
                <span class="s1">alpha=</span><span class="s5">0.5</span><span class="s2">,</span>
                <span class="s1">max_iter=</span><span class="s5">100</span><span class="s2">,</span>
                <span class="s1">precompute=</span><span class="s2">False,</span>
                <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
            <span class="s1">)</span>

            <span class="s1">X = dtype(X)</span>
            <span class="s1">y = dtype(y)</span>
            <span class="s1">ignore_warnings(clf.fit)(X</span><span class="s2">, </span><span class="s1">y)</span>

            <span class="s1">coef[(</span><span class="s3">&quot;simple&quot;</span><span class="s2">, </span><span class="s1">dtype)] = clf.coef_</span>
            <span class="s1">intercept[(</span><span class="s3">&quot;simple&quot;</span><span class="s2">, </span><span class="s1">dtype)] = clf.intercept_</span>

            <span class="s2">assert </span><span class="s1">clf.coef_.dtype == dtype</span>

            <span class="s0"># test precompute Gram array</span>
            <span class="s1">Gram = X.T.dot(X)</span>
            <span class="s1">clf_precompute = ElasticNet(</span>
                <span class="s1">alpha=</span><span class="s5">0.5</span><span class="s2">,</span>
                <span class="s1">max_iter=</span><span class="s5">100</span><span class="s2">,</span>
                <span class="s1">precompute=Gram</span><span class="s2">,</span>
                <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">ignore_warnings(clf_precompute.fit)(X</span><span class="s2">, </span><span class="s1">y)</span>
            <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s2">, </span><span class="s1">clf_precompute.coef_)</span>
            <span class="s1">assert_array_almost_equal(clf.intercept_</span><span class="s2">, </span><span class="s1">clf_precompute.intercept_)</span>

            <span class="s0"># test multi task enet</span>
            <span class="s1">multi_y = np.hstack((y[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span><span class="s2">, </span><span class="s1">y[:</span><span class="s2">, </span><span class="s1">np.newaxis]))</span>
            <span class="s1">clf_multioutput = MultiTaskElasticNet(</span>
                <span class="s1">alpha=</span><span class="s5">0.5</span><span class="s2">,</span>
                <span class="s1">max_iter=</span><span class="s5">100</span><span class="s2">,</span>
                <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">clf_multioutput.fit(X</span><span class="s2">, </span><span class="s1">multi_y)</span>
            <span class="s1">coef[(</span><span class="s3">&quot;multi&quot;</span><span class="s2">, </span><span class="s1">dtype)] = clf_multioutput.coef_</span>
            <span class="s1">intercept[(</span><span class="s3">&quot;multi&quot;</span><span class="s2">, </span><span class="s1">dtype)] = clf_multioutput.intercept_</span>
            <span class="s2">assert </span><span class="s1">clf.coef_.dtype == dtype</span>

        <span class="s2">for </span><span class="s1">v </span><span class="s2">in </span><span class="s1">[</span><span class="s3">&quot;simple&quot;</span><span class="s2">, </span><span class="s3">&quot;multi&quot;</span><span class="s1">]:</span>
            <span class="s1">assert_array_almost_equal(</span>
                <span class="s1">coef[(v</span><span class="s2">, </span><span class="s1">np.float32)]</span><span class="s2">, </span><span class="s1">coef[(v</span><span class="s2">, </span><span class="s1">np.float64)]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">4</span>
            <span class="s1">)</span>
            <span class="s1">assert_array_almost_equal(</span>
                <span class="s1">intercept[(v</span><span class="s2">, </span><span class="s1">np.float32)]</span><span class="s2">, </span><span class="s1">intercept[(v</span><span class="s2">, </span><span class="s1">np.float64)]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">4</span>
            <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_enet_l1_ratio():</span>
    <span class="s0"># Test that an error message is raised if an estimator that</span>
    <span class="s0"># uses _alpha_grid is called with l1_ratio=0</span>
    <span class="s1">msg = (</span>
        <span class="s3">&quot;Automatic alpha grid generation is not supported for l1_ratio=0. &quot;</span>
        <span class="s3">&quot;Please supply a grid by providing your estimator with the &quot;</span>
        <span class="s3">&quot;appropriate `alphas=` argument.&quot;</span>
    <span class="s1">)</span>
    <span class="s1">X = np.array([[</span><span class="s5">1</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s5">4</span><span class="s2">, </span><span class="s5">5</span><span class="s2">, </span><span class="s5">8</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">3</span><span class="s2">, </span><span class="s5">5</span><span class="s2">, </span><span class="s5">7</span><span class="s2">, </span><span class="s5">7</span><span class="s2">, </span><span class="s5">8</span><span class="s1">]]).T</span>
    <span class="s1">y = np.array([</span><span class="s5">12</span><span class="s2">, </span><span class="s5">10</span><span class="s2">, </span><span class="s5">11</span><span class="s2">, </span><span class="s5">21</span><span class="s2">, </span><span class="s5">5</span><span class="s1">])</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">ElasticNetCV(l1_ratio=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">42</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">MultiTaskElasticNetCV(l1_ratio=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">42</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y[:</span><span class="s2">, None</span><span class="s1">])</span>

    <span class="s0"># Test that l1_ratio=0 with alpha&gt;0 produces user warning</span>
    <span class="s1">warning_message = (</span>
        <span class="s3">&quot;Coordinate descent without L1 regularization may &quot;</span>
        <span class="s3">&quot;lead to unexpected results and is discouraged. &quot;</span>
        <span class="s3">&quot;Set l1_ratio &gt; 0 to add L1 regularization.&quot;</span>
    <span class="s1">)</span>
    <span class="s1">est = ElasticNetCV(l1_ratio=[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">alphas=[</span><span class="s5">1</span><span class="s1">])</span>
    <span class="s2">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s2">, </span><span class="s1">match=warning_message):</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s0"># Test that l1_ratio=0 is allowed if we supply a grid manually</span>
    <span class="s1">alphas = [</span><span class="s5">0.1</span><span class="s2">, </span><span class="s5">10</span><span class="s1">]</span>
    <span class="s1">estkwds = {</span><span class="s3">&quot;alphas&quot;</span><span class="s1">: alphas</span><span class="s2">, </span><span class="s3">&quot;random_state&quot;</span><span class="s1">: </span><span class="s5">42</span><span class="s1">}</span>
    <span class="s1">est_desired = ElasticNetCV(l1_ratio=</span><span class="s5">0.00001</span><span class="s2">, </span><span class="s1">**estkwds)</span>
    <span class="s1">est = ElasticNetCV(l1_ratio=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">**estkwds)</span>
    <span class="s2">with </span><span class="s1">ignore_warnings():</span>
        <span class="s1">est_desired.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(est.coef_</span><span class="s2">, </span><span class="s1">est_desired.coef_</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">5</span><span class="s1">)</span>

    <span class="s1">est_desired = MultiTaskElasticNetCV(l1_ratio=</span><span class="s5">0.00001</span><span class="s2">, </span><span class="s1">**estkwds)</span>
    <span class="s1">est = MultiTaskElasticNetCV(l1_ratio=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">**estkwds)</span>
    <span class="s2">with </span><span class="s1">ignore_warnings():</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y[:</span><span class="s2">, None</span><span class="s1">])</span>
        <span class="s1">est_desired.fit(X</span><span class="s2">, </span><span class="s1">y[:</span><span class="s2">, None</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(est.coef_</span><span class="s2">, </span><span class="s1">est_desired.coef_</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">5</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_coef_shape_not_zero():</span>
    <span class="s1">est_no_intercept = Lasso(fit_intercept=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">est_no_intercept.fit(np.c_[np.ones(</span><span class="s5">3</span><span class="s1">)]</span><span class="s2">, </span><span class="s1">np.ones(</span><span class="s5">3</span><span class="s1">))</span>
    <span class="s2">assert </span><span class="s1">est_no_intercept.coef_.shape == (</span><span class="s5">1</span><span class="s2">,</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_warm_start_multitask_lasso():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test = build_dataset()</span>
    <span class="s1">Y = np.c_[y</span><span class="s2">, </span><span class="s1">y]</span>
    <span class="s1">clf = MultiTaskLasso(alpha=</span><span class="s5">0.1</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">warm_start=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">ignore_warnings(clf.fit)(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">ignore_warnings(clf.fit)(X</span><span class="s2">, </span><span class="s1">Y)  </span><span class="s0"># do a second round with 5 iterations</span>

    <span class="s1">clf2 = MultiTaskLasso(alpha=</span><span class="s5">0.1</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s5">10</span><span class="s1">)</span>
    <span class="s1">ignore_warnings(clf2.fit)(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">assert_array_almost_equal(clf2.coef_</span><span class="s2">, </span><span class="s1">clf.coef_)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;klass, n_classes, kwargs&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(Lasso</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s1">dict(precompute=</span><span class="s2">True</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">(Lasso</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s1">dict(precompute=</span><span class="s2">False</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">(MultiTaskLasso</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s1">dict())</span><span class="s2">,</span>
        <span class="s1">(MultiTaskLasso</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s1">dict())</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_enet_coordinate_descent(klass</span><span class="s2">, </span><span class="s1">n_classes</span><span class="s2">, </span><span class="s1">kwargs):</span>
    <span class="s4">&quot;&quot;&quot;Test that a warning is issued if model does not converge&quot;&quot;&quot;</span>
    <span class="s1">clf = klass(max_iter=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">**kwargs)</span>
    <span class="s1">n_samples = </span><span class="s5">5</span>
    <span class="s1">n_features = </span><span class="s5">2</span>
    <span class="s1">X = np.ones((n_samples</span><span class="s2">, </span><span class="s1">n_features)) * </span><span class="s5">1e50</span>
    <span class="s1">y = np.ones((n_samples</span><span class="s2">, </span><span class="s1">n_classes))</span>
    <span class="s2">if </span><span class="s1">klass == Lasso:</span>
        <span class="s1">y = y.ravel()</span>
    <span class="s1">warning_message = (</span>
        <span class="s3">&quot;Objective did not converge. You might want to&quot;</span>
        <span class="s3">&quot; increase the number of iterations.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s2">, </span><span class="s1">match=warning_message):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_convergence_warnings():</span>
    <span class="s1">random_state = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">X = random_state.standard_normal((</span><span class="s5">1000</span><span class="s2">, </span><span class="s5">500</span><span class="s1">))</span>
    <span class="s1">y = random_state.standard_normal((</span><span class="s5">1000</span><span class="s2">, </span><span class="s5">3</span><span class="s1">))</span>

    <span class="s0"># check that the model converges w/o convergence warnings</span>
    <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s3">&quot;error&quot;</span><span class="s2">, </span><span class="s1">ConvergenceWarning)</span>
        <span class="s1">MultiTaskElasticNet().fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_sparse_input_convergence_warning():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset(n_samples=</span><span class="s5">1000</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">500</span><span class="s1">)</span>

    <span class="s2">with </span><span class="s1">pytest.warns(ConvergenceWarning):</span>
        <span class="s1">ElasticNet(max_iter=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">0</span><span class="s1">).fit(sparse.csr_matrix(X</span><span class="s2">, </span><span class="s1">dtype=np.float32)</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s0"># check that the model converges w/o convergence warnings</span>
    <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s3">&quot;error&quot;</span><span class="s2">, </span><span class="s1">ConvergenceWarning)</span>
        <span class="s1">Lasso().fit(sparse.csr_matrix(X</span><span class="s2">, </span><span class="s1">dtype=np.float32)</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;precompute, inner_precompute&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s2">True, True</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s3">&quot;auto&quot;</span><span class="s2">, False</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s2">False, False</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_lassoCV_does_not_set_precompute(monkeypatch</span><span class="s2">, </span><span class="s1">precompute</span><span class="s2">, </span><span class="s1">inner_precompute):</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = build_dataset()</span>
    <span class="s1">calls = </span><span class="s5">0</span>

    <span class="s2">class </span><span class="s1">LassoMock(Lasso):</span>
        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
            <span class="s1">super().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
            <span class="s2">nonlocal </span><span class="s1">calls</span>
            <span class="s1">calls += </span><span class="s5">1</span>
            <span class="s2">assert </span><span class="s1">self.precompute == inner_precompute</span>

    <span class="s1">monkeypatch.setattr(</span><span class="s3">&quot;sklearn.linear_model._coordinate_descent.Lasso&quot;</span><span class="s2">, </span><span class="s1">LassoMock)</span>
    <span class="s1">clf = LassoCV(precompute=precompute)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">calls &gt; </span><span class="s5">0</span>


<span class="s2">def </span><span class="s1">test_multi_task_lasso_cv_dtype():</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s5">10</span><span class="s2">, </span><span class="s5">3</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s5">42</span><span class="s1">)</span>
    <span class="s1">X = rng.binomial(</span><span class="s5">1</span><span class="s2">, </span><span class="s5">0.5</span><span class="s2">, </span><span class="s1">size=(n_samples</span><span class="s2">, </span><span class="s1">n_features))</span>
    <span class="s1">X = X.astype(int)  </span><span class="s0"># make it explicit that X is int</span>
    <span class="s1">y = X[:</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]].copy()</span>
    <span class="s1">est = MultiTaskLassoCV(n_alphas=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">fit_intercept=</span><span class="s2">True</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(est.coef_</span><span class="s2">, </span><span class="s1">[[</span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]] * </span><span class="s5">2</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s5">3</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;fit_intercept&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;alpha&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.01</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;precompute&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">False, True</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;sparseX&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">False, True</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_enet_sample_weight_consistency(</span>
    <span class="s1">fit_intercept</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">precompute</span><span class="s2">, </span><span class="s1">sparseX</span><span class="s2">, </span><span class="s1">global_random_seed</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Test that the impact of sample_weight is consistent. 
 
    Note that this test is stricter than the common test 
    check_sample_weights_invariance alone and also tests sparse X. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s5">10</span><span class="s2">, </span><span class="s5">5</span>

    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">y = rng.rand(n_samples)</span>
    <span class="s2">if </span><span class="s1">sparseX:</span>
        <span class="s1">X = sparse.csc_matrix(X)</span>
    <span class="s1">params = dict(</span>
        <span class="s1">alpha=alpha</span><span class="s2">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">precompute=precompute</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s5">1e-6</span><span class="s2">,</span>
        <span class="s1">l1_ratio=</span><span class="s5">0.5</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">reg = ElasticNet(**params).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">coef = reg.coef_.copy()</span>
    <span class="s2">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">intercept = reg.intercept_</span>

    <span class="s0"># 1) sample_weight=np.ones(..) should be equivalent to sample_weight=None</span>
    <span class="s1">sample_weight = np.ones_like(y)</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_allclose(reg.coef_</span><span class="s2">, </span><span class="s1">coef</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-6</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">assert_allclose(reg.intercept_</span><span class="s2">, </span><span class="s1">intercept)</span>

    <span class="s0"># 2) sample_weight=None should be equivalent to sample_weight = number</span>
    <span class="s1">sample_weight = </span><span class="s5">123.0</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_allclose(reg.coef_</span><span class="s2">, </span><span class="s1">coef</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-6</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">assert_allclose(reg.intercept_</span><span class="s2">, </span><span class="s1">intercept)</span>

    <span class="s0"># 3) scaling of sample_weight should have no effect, cf. np.average()</span>
    <span class="s1">sample_weight = rng.uniform(low=</span><span class="s5">0.01</span><span class="s2">, </span><span class="s1">high=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">size=X.shape[</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">reg = reg.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">coef = reg.coef_.copy()</span>
    <span class="s2">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">intercept = reg.intercept_</span>

    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=np.pi * sample_weight)</span>
    <span class="s1">assert_allclose(reg.coef_</span><span class="s2">, </span><span class="s1">coef</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-6</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">assert_allclose(reg.intercept_</span><span class="s2">, </span><span class="s1">intercept)</span>

    <span class="s0"># 4) setting elements of sample_weight to 0 is equivalent to removing these samples</span>
    <span class="s1">sample_weight_0 = sample_weight.copy()</span>
    <span class="s1">sample_weight_0[-</span><span class="s5">5</span><span class="s1">:] = </span><span class="s5">0</span>
    <span class="s1">y[-</span><span class="s5">5</span><span class="s1">:] *= </span><span class="s5">1000  </span><span class="s0"># to make excluding those samples important</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight_0)</span>
    <span class="s1">coef_0 = reg.coef_.copy()</span>
    <span class="s2">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">intercept_0 = reg.intercept_</span>
    <span class="s1">reg.fit(X[:-</span><span class="s5">5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y[:-</span><span class="s5">5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight[:-</span><span class="s5">5</span><span class="s1">])</span>
    <span class="s1">assert_allclose(reg.coef_</span><span class="s2">, </span><span class="s1">coef_0</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-6</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">assert_allclose(reg.intercept_</span><span class="s2">, </span><span class="s1">intercept_0)</span>

    <span class="s0"># 5) check that multiplying sample_weight by 2 is equivalent to repeating</span>
    <span class="s0"># corresponding samples twice</span>
    <span class="s2">if </span><span class="s1">sparseX:</span>
        <span class="s1">X2 = sparse.vstack([X</span><span class="s2">, </span><span class="s1">X[: n_samples // </span><span class="s5">2</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">format=</span><span class="s3">&quot;csc&quot;</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">X2 = np.concatenate([X</span><span class="s2">, </span><span class="s1">X[: n_samples // </span><span class="s5">2</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">y2 = np.concatenate([y</span><span class="s2">, </span><span class="s1">y[: n_samples // </span><span class="s5">2</span><span class="s1">]])</span>
    <span class="s1">sample_weight_1 = sample_weight.copy()</span>
    <span class="s1">sample_weight_1[: n_samples // </span><span class="s5">2</span><span class="s1">] *= </span><span class="s5">2</span>
    <span class="s1">sample_weight_2 = np.concatenate(</span>
        <span class="s1">[sample_weight</span><span class="s2">, </span><span class="s1">sample_weight[: n_samples // </span><span class="s5">2</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span>
    <span class="s1">)</span>

    <span class="s1">reg1 = ElasticNet(**params).fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight_1)</span>
    <span class="s1">reg2 = ElasticNet(**params).fit(X2</span><span class="s2">, </span><span class="s1">y2</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight_2)</span>
    <span class="s1">assert_allclose(reg1.coef_</span><span class="s2">, </span><span class="s1">reg2.coef_</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-6</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;fit_intercept&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;sparseX&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">False, True</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_enet_cv_sample_weight_correctness(fit_intercept</span><span class="s2">, </span><span class="s1">sparseX):</span>
    <span class="s4">&quot;&quot;&quot;Test that ElasticNetCV with sample weights gives correct results.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s5">42</span><span class="s1">)</span>
    <span class="s1">n_splits</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s5">3</span><span class="s2">, </span><span class="s5">10</span><span class="s2">, </span><span class="s5">5</span>
    <span class="s1">X = rng.rand(n_splits * n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">beta = rng.rand(n_features)</span>
    <span class="s1">beta[</span><span class="s5">0</span><span class="s1">:</span><span class="s5">2</span><span class="s1">] = </span><span class="s5">0</span>
    <span class="s1">y = X @ beta + rng.rand(n_splits * n_samples)</span>
    <span class="s1">sw = np.ones_like(y)</span>
    <span class="s2">if </span><span class="s1">sparseX:</span>
        <span class="s1">X = sparse.csc_matrix(X)</span>
    <span class="s1">params = dict(tol=</span><span class="s5">1e-6</span><span class="s1">)</span>

    <span class="s0"># Set alphas, otherwise the two cv models might use different ones.</span>
    <span class="s2">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">alphas = np.linspace(</span><span class="s5">0.001</span><span class="s2">, </span><span class="s5">0.01</span><span class="s2">, </span><span class="s1">num=</span><span class="s5">91</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">alphas = np.linspace(</span><span class="s5">0.01</span><span class="s2">, </span><span class="s5">0.1</span><span class="s2">, </span><span class="s1">num=</span><span class="s5">91</span><span class="s1">)</span>

    <span class="s0"># We weight the first fold 2 times more.</span>
    <span class="s1">sw[:n_samples] = </span><span class="s5">2</span>
    <span class="s1">groups_sw = np.r_[</span>
        <span class="s1">np.full(n_samples</span><span class="s2">, </span><span class="s5">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.full(n_samples</span><span class="s2">, </span><span class="s5">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.full(n_samples</span><span class="s2">, </span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">]</span>
    <span class="s1">splits_sw = list(LeaveOneGroupOut().split(X</span><span class="s2">, </span><span class="s1">groups=groups_sw))</span>
    <span class="s1">reg_sw = ElasticNetCV(</span>
        <span class="s1">alphas=alphas</span><span class="s2">, </span><span class="s1">cv=splits_sw</span><span class="s2">, </span><span class="s1">fit_intercept=fit_intercept</span><span class="s2">, </span><span class="s1">**params</span>
    <span class="s1">)</span>
    <span class="s1">reg_sw.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sw)</span>

    <span class="s0"># We repeat the first fold 2 times and provide splits ourselves</span>
    <span class="s2">if </span><span class="s1">sparseX:</span>
        <span class="s1">X = X.toarray()</span>
    <span class="s1">X = np.r_[X[:n_samples]</span><span class="s2">, </span><span class="s1">X]</span>
    <span class="s2">if </span><span class="s1">sparseX:</span>
        <span class="s1">X = sparse.csc_matrix(X)</span>
    <span class="s1">y = np.r_[y[:n_samples]</span><span class="s2">, </span><span class="s1">y]</span>
    <span class="s1">groups = np.r_[</span>
        <span class="s1">np.full(</span><span class="s5">2 </span><span class="s1">* n_samples</span><span class="s2">, </span><span class="s5">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.full(n_samples</span><span class="s2">, </span><span class="s5">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.full(n_samples</span><span class="s2">, </span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">]</span>
    <span class="s1">splits = list(LeaveOneGroupOut().split(X</span><span class="s2">, </span><span class="s1">groups=groups))</span>
    <span class="s1">reg = ElasticNetCV(alphas=alphas</span><span class="s2">, </span><span class="s1">cv=splits</span><span class="s2">, </span><span class="s1">fit_intercept=fit_intercept</span><span class="s2">, </span><span class="s1">**params)</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s0"># ensure that we chose meaningful alphas, i.e. not boundaries</span>
    <span class="s2">assert </span><span class="s1">alphas[</span><span class="s5">0</span><span class="s1">] &lt; reg.alpha_ &lt; alphas[-</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s2">assert </span><span class="s1">reg_sw.alpha_ == reg.alpha_</span>
    <span class="s1">assert_allclose(reg_sw.coef_</span><span class="s2">, </span><span class="s1">reg.coef_)</span>
    <span class="s2">assert </span><span class="s1">reg_sw.intercept_ == pytest.approx(reg.intercept_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;sample_weight&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">False, True</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_enet_cv_grid_search(sample_weight):</span>
    <span class="s4">&quot;&quot;&quot;Test that ElasticNetCV gives same result as GridSearchCV.&quot;&quot;&quot;</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s5">200</span><span class="s2">, </span><span class="s5">10</span>
    <span class="s1">cv = </span><span class="s5">5</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">n_samples=n_samples</span><span class="s2">,</span>
        <span class="s1">n_features=n_features</span><span class="s2">,</span>
        <span class="s1">effective_rank=</span><span class="s5">10</span><span class="s2">,</span>
        <span class="s1">n_informative=n_features - </span><span class="s5">4</span><span class="s2">,</span>
        <span class="s1">noise=</span><span class="s5">10</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s5">0</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">if </span><span class="s1">sample_weight:</span>
        <span class="s1">sample_weight = np.linspace(</span><span class="s5">1</span><span class="s2">, </span><span class="s5">5</span><span class="s2">, </span><span class="s1">num=n_samples)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">sample_weight = </span><span class="s2">None</span>

    <span class="s1">alphas = np.logspace(np.log10(</span><span class="s5">1e-5</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.log10(</span><span class="s5">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">num=</span><span class="s5">10</span><span class="s1">)</span>
    <span class="s1">l1_ratios = [</span><span class="s5">0.1</span><span class="s2">, </span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0.9</span><span class="s1">]</span>
    <span class="s1">reg = ElasticNetCV(cv=cv</span><span class="s2">, </span><span class="s1">alphas=alphas</span><span class="s2">, </span><span class="s1">l1_ratio=l1_ratios)</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">param = {</span><span class="s3">&quot;alpha&quot;</span><span class="s1">: alphas</span><span class="s2">, </span><span class="s3">&quot;l1_ratio&quot;</span><span class="s1">: l1_ratios}</span>
    <span class="s1">gs = GridSearchCV(</span>
        <span class="s1">estimator=ElasticNet()</span><span class="s2">,</span>
        <span class="s1">param_grid=param</span><span class="s2">,</span>
        <span class="s1">cv=cv</span><span class="s2">,</span>
        <span class="s1">scoring=</span><span class="s3">&quot;neg_mean_squared_error&quot;</span><span class="s2">,</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s2">assert </span><span class="s1">reg.l1_ratio_ == pytest.approx(gs.best_params_[</span><span class="s3">&quot;l1_ratio&quot;</span><span class="s1">])</span>
    <span class="s2">assert </span><span class="s1">reg.alpha_ == pytest.approx(gs.best_params_[</span><span class="s3">&quot;alpha&quot;</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;fit_intercept&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;l1_ratio&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">1</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;precompute&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">False, True</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;sparseX&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">False, True</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_enet_cv_sample_weight_consistency(</span>
    <span class="s1">fit_intercept</span><span class="s2">, </span><span class="s1">l1_ratio</span><span class="s2">, </span><span class="s1">precompute</span><span class="s2">, </span><span class="s1">sparseX</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Test that the impact of sample_weight is consistent.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s5">10</span><span class="s2">, </span><span class="s5">5</span>

    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">y = X.sum(axis=</span><span class="s5">1</span><span class="s1">) + rng.rand(n_samples)</span>
    <span class="s1">params = dict(</span>
        <span class="s1">l1_ratio=l1_ratio</span><span class="s2">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">precompute=precompute</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s5">1e-6</span><span class="s2">,</span>
        <span class="s1">cv=</span><span class="s5">3</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">if </span><span class="s1">sparseX:</span>
        <span class="s1">X = sparse.csc_matrix(X)</span>

    <span class="s2">if </span><span class="s1">l1_ratio == </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s1">params.pop(</span><span class="s3">&quot;l1_ratio&quot;</span><span class="s2">, None</span><span class="s1">)</span>
        <span class="s1">reg = LassoCV(**params).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">reg = ElasticNetCV(**params).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">coef = reg.coef_.copy()</span>
    <span class="s2">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">intercept = reg.intercept_</span>

    <span class="s0"># sample_weight=np.ones(..) should be equivalent to sample_weight=None</span>
    <span class="s1">sample_weight = np.ones_like(y)</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_allclose(reg.coef_</span><span class="s2">, </span><span class="s1">coef</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-6</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">assert_allclose(reg.intercept_</span><span class="s2">, </span><span class="s1">intercept)</span>

    <span class="s0"># sample_weight=None should be equivalent to sample_weight = number</span>
    <span class="s1">sample_weight = </span><span class="s5">123.0</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_allclose(reg.coef_</span><span class="s2">, </span><span class="s1">coef</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-6</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">assert_allclose(reg.intercept_</span><span class="s2">, </span><span class="s1">intercept)</span>

    <span class="s0"># scaling of sample_weight should have no effect, cf. np.average()</span>
    <span class="s1">sample_weight = </span><span class="s5">2 </span><span class="s1">* np.ones_like(y)</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_allclose(reg.coef_</span><span class="s2">, </span><span class="s1">coef</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-6</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">assert_allclose(reg.intercept_</span><span class="s2">, </span><span class="s1">intercept)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;estimator&quot;</span><span class="s2">, </span><span class="s1">[ElasticNetCV</span><span class="s2">, </span><span class="s1">LassoCV])</span>
<span class="s2">def </span><span class="s1">test_linear_models_cv_fit_with_loky(estimator):</span>
    <span class="s0"># LinearModelsCV.fit performs inplace operations on fancy-indexed memmapped</span>
    <span class="s0"># data when using the loky backend, causing an error due to unexpected</span>
    <span class="s0"># behavior of fancy indexing of read-only memmaps (cf. numpy#14132).</span>

    <span class="s0"># Create a problem sufficiently large to cause memmapping (1MB).</span>
    <span class="s0"># Unfortunately the scikit-learn and joblib APIs do not make it possible to</span>
    <span class="s0"># change the max_nbyte of the inner Parallel call.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(int(</span><span class="s5">1e6</span><span class="s1">) // </span><span class="s5">8 </span><span class="s1">+ </span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">X.nbytes &gt; </span><span class="s5">1e6  </span><span class="s0"># 1 MB</span>
    <span class="s2">with </span><span class="s1">joblib.parallel_backend(</span><span class="s3">&quot;loky&quot;</span><span class="s1">):</span>
        <span class="s1">estimator(n_jobs=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">cv=</span><span class="s5">3</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;check_input&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_enet_sample_weight_does_not_overwrite_sample_weight(check_input):</span>
    <span class="s4">&quot;&quot;&quot;Check that ElasticNet does not overwrite sample_weights.&quot;&quot;&quot;</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s5">10</span><span class="s2">, </span><span class="s5">5</span>

    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">y = rng.rand(n_samples)</span>

    <span class="s1">sample_weight_1_25 = </span><span class="s5">1.25 </span><span class="s1">* np.ones_like(y)</span>
    <span class="s1">sample_weight = sample_weight_1_25.copy()</span>

    <span class="s1">reg = ElasticNet()</span>
    <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight</span><span class="s2">, </span><span class="s1">check_input=check_input)</span>

    <span class="s1">assert_array_equal(sample_weight</span><span class="s2">, </span><span class="s1">sample_weight_1_25)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;ridge_alpha&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1e-1</span><span class="s2">, </span><span class="s5">1.0</span><span class="s2">, </span><span class="s5">1e6</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_enet_ridge_consistency(ridge_alpha):</span>
    <span class="s0"># Check that ElasticNet(l1_ratio=0) converges to the same solution as Ridge</span>
    <span class="s0"># provided that the value of alpha is adapted.</span>
    <span class="s0">#</span>
    <span class="s0"># XXX: this test does not pass for weaker regularization (lower values of</span>
    <span class="s0"># ridge_alpha): it could be either a problem of ElasticNet or Ridge (less</span>
    <span class="s0"># likely) and depends on the dataset statistics: lower values for</span>
    <span class="s0"># effective_rank are more problematic in particular.</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s5">42</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s5">300</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">n_samples=n_samples</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s5">100</span><span class="s2">,</span>
        <span class="s1">effective_rank=</span><span class="s5">10</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s5">50</span><span class="s2">,</span>
        <span class="s1">random_state=rng</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">sw = rng.uniform(low=</span><span class="s5">0.01</span><span class="s2">, </span><span class="s1">high=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">size=X.shape[</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">alpha = </span><span class="s5">1.0</span>
    <span class="s1">common_params = dict(</span>
        <span class="s1">tol=</span><span class="s5">1e-12</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">ridge = Ridge(alpha=alpha</span><span class="s2">, </span><span class="s1">**common_params).fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sw)</span>

    <span class="s1">alpha_enet = alpha / sw.sum()</span>
    <span class="s1">enet = ElasticNet(alpha=alpha_enet</span><span class="s2">, </span><span class="s1">l1_ratio=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">**common_params).fit(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sw</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(ridge.coef_</span><span class="s2">, </span><span class="s1">enet.coef_)</span>
    <span class="s1">assert_allclose(ridge.intercept_</span><span class="s2">, </span><span class="s1">enet.intercept_)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;estimator&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">Lasso(alpha=</span><span class="s5">1.0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">ElasticNet(alpha=</span><span class="s5">1.0</span><span class="s2">, </span><span class="s1">l1_ratio=</span><span class="s5">0.1</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_sample_weight_invariance(estimator):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s5">42</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">n_samples=</span><span class="s5">100</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s5">300</span><span class="s2">,</span>
        <span class="s1">effective_rank=</span><span class="s5">10</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s5">50</span><span class="s2">,</span>
        <span class="s1">random_state=rng</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">sw = rng.uniform(low=</span><span class="s5">0.01</span><span class="s2">, </span><span class="s1">high=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">size=X.shape[</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">params = dict(tol=</span><span class="s5">1e-12</span><span class="s1">)</span>

    <span class="s0"># Check that setting some weights to 0 is equivalent to trimming the</span>
    <span class="s0"># samples:</span>
    <span class="s1">cutoff = X.shape[</span><span class="s5">0</span><span class="s1">] // </span><span class="s5">3</span>
    <span class="s1">sw_with_null = sw.copy()</span>
    <span class="s1">sw_with_null[:cutoff] = </span><span class="s5">0.0</span>
    <span class="s1">X_trimmed</span><span class="s2">, </span><span class="s1">y_trimmed = X[cutoff:</span><span class="s2">, </span><span class="s1">:]</span><span class="s2">, </span><span class="s1">y[cutoff:]</span>
    <span class="s1">sw_trimmed = sw[cutoff:]</span>

    <span class="s1">reg_trimmed = (</span>
        <span class="s1">clone(estimator)</span>
        <span class="s1">.set_params(**params)</span>
        <span class="s1">.fit(X_trimmed</span><span class="s2">, </span><span class="s1">y_trimmed</span><span class="s2">, </span><span class="s1">sample_weight=sw_trimmed)</span>
    <span class="s1">)</span>
    <span class="s1">reg_null_weighted = (</span>
        <span class="s1">clone(estimator).set_params(**params).fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sw_with_null)</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(reg_null_weighted.coef_</span><span class="s2">, </span><span class="s1">reg_trimmed.coef_)</span>
    <span class="s1">assert_allclose(reg_null_weighted.intercept_</span><span class="s2">, </span><span class="s1">reg_trimmed.intercept_)</span>

    <span class="s0"># Check that duplicating the training dataset is equivalent to multiplying</span>
    <span class="s0"># the weights by 2:</span>
    <span class="s1">X_dup = np.concatenate([X</span><span class="s2">, </span><span class="s1">X]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">y_dup = np.concatenate([y</span><span class="s2">, </span><span class="s1">y]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">sw_dup = np.concatenate([sw</span><span class="s2">, </span><span class="s1">sw]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">reg_2sw = clone(estimator).set_params(**params).fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s5">2 </span><span class="s1">* sw)</span>
    <span class="s1">reg_dup = (</span>
        <span class="s1">clone(estimator).set_params(**params).fit(X_dup</span><span class="s2">, </span><span class="s1">y_dup</span><span class="s2">, </span><span class="s1">sample_weight=sw_dup)</span>
    <span class="s1">)</span>

    <span class="s1">assert_allclose(reg_2sw.coef_</span><span class="s2">, </span><span class="s1">reg_dup.coef_)</span>
    <span class="s1">assert_allclose(reg_2sw.intercept_</span><span class="s2">, </span><span class="s1">reg_dup.intercept_)</span>


<span class="s2">def </span><span class="s1">test_read_only_buffer():</span>
    <span class="s4">&quot;&quot;&quot;Test that sparse coordinate descent works for read-only buffers&quot;&quot;&quot;</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">clf = ElasticNet(alpha=</span><span class="s5">0.1</span><span class="s2">, </span><span class="s1">copy_X=</span><span class="s2">True, </span><span class="s1">random_state=rng)</span>
    <span class="s1">X = np.asfortranarray(rng.uniform(size=(</span><span class="s5">100</span><span class="s2">, </span><span class="s5">10</span><span class="s1">)))</span>
    <span class="s1">X.setflags(write=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s1">y = rng.rand(</span><span class="s5">100</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
</pre>
</body>
</html>