<html>
<head>
<title>test_quantile.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_quantile.py</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: David Dale &lt;dale.david@mail.ru&gt;</span>
<span class="s0">#          Christian Lorentzen &lt;lorentzen.ch@gmail.com&gt;</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">pytest </span><span class="s2">import </span><span class="s1">approx</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">sparse</span>
<span class="s2">from </span><span class="s1">scipy.optimize </span><span class="s2">import </span><span class="s1">minimize</span>

<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">make_regression</span>
<span class="s2">from </span><span class="s1">sklearn.exceptions </span><span class="s2">import </span><span class="s1">ConvergenceWarning</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">HuberRegressor</span><span class="s2">, </span><span class="s1">QuantileRegressor</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">mean_pinball_loss</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">assert_allclose</span><span class="s2">, </span><span class="s1">skip_if_32bit</span>
<span class="s2">from </span><span class="s1">sklearn.utils.fixes </span><span class="s2">import </span><span class="s1">parse_version</span><span class="s2">, </span><span class="s1">sp_version</span>


<span class="s1">@pytest.fixture</span>
<span class="s2">def </span><span class="s1">X_y_data():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(n_samples=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">noise=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span>


<span class="s1">@pytest.fixture</span>
<span class="s2">def </span><span class="s1">default_solver():</span>
    <span class="s2">return </span><span class="s4">&quot;highs&quot; </span><span class="s2">if </span><span class="s1">sp_version &gt;= parse_version(</span><span class="s4">&quot;1.6.0&quot;</span><span class="s1">) </span><span class="s2">else </span><span class="s4">&quot;interior-point&quot;</span>


<span class="s1">@pytest.mark.skipif(</span>
    <span class="s1">parse_version(sp_version.base_version) &gt;= parse_version(</span><span class="s4">&quot;1.11&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">reason=</span><span class="s4">&quot;interior-point solver is not available in SciPy 1.11&quot;</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;solver&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;interior-point&quot;</span><span class="s2">, </span><span class="s4">&quot;revised simplex&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_incompatible_solver_for_sparse_input(X_y_data</span><span class="s2">, </span><span class="s1">solver):</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = X_y_data</span>
    <span class="s1">X_sparse = sparse.csc_matrix(X)</span>
    <span class="s1">err_msg = (</span>
        <span class="s4">f&quot;Solver </span><span class="s2">{</span><span class="s1">solver</span><span class="s2">} </span><span class="s4">does not support sparse X. Use solver 'highs' for example.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">QuantileRegressor(solver=solver).fit(X_sparse</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;solver&quot;</span><span class="s2">, </span><span class="s1">(</span><span class="s4">&quot;highs-ds&quot;</span><span class="s2">, </span><span class="s4">&quot;highs-ipm&quot;</span><span class="s2">, </span><span class="s4">&quot;highs&quot;</span><span class="s1">))</span>
<span class="s1">@pytest.mark.skipif(</span>
    <span class="s1">sp_version &gt;= parse_version(</span><span class="s4">&quot;1.6.0&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">reason=</span><span class="s4">&quot;Solvers are available as of scipy 1.6.0&quot;</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_too_new_solver_methods_raise_error(X_y_data</span><span class="s2">, </span><span class="s1">solver):</span>
    <span class="s5">&quot;&quot;&quot;Test that highs solver raises for scipy&lt;1.6.0.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = X_y_data</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s4">&quot;scipy&gt;=1.6.0&quot;</span><span class="s1">):</span>
        <span class="s1">QuantileRegressor(solver=solver).fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;quantile, alpha, intercept, coef&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s0"># for 50% quantile w/o regularization, any slope in [1, 10] is okay</span>
        <span class="s1">[</span><span class="s3">0.5</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s0"># if positive error costs more, the slope is maximal</span>
        <span class="s1">[</span><span class="s3">0.51</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">10</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s0"># if negative error costs more, the slope is minimal</span>
        <span class="s1">[</span><span class="s3">0.49</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s0"># for a small lasso penalty, the slope is also minimal</span>
        <span class="s1">[</span><span class="s3">0.5</span><span class="s2">, </span><span class="s3">0.01</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s0"># for a large lasso penalty, the model predicts the constant median</span>
        <span class="s1">[</span><span class="s3">0.5</span><span class="s2">, </span><span class="s3">100</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_quantile_toy_example(quantile</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">intercept</span><span class="s2">, </span><span class="s1">coef</span><span class="s2">, </span><span class="s1">default_solver):</span>
    <span class="s0"># test how different parameters affect a small intuitive example</span>
    <span class="s1">X = [[</span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">1</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">11</span><span class="s1">]</span>
    <span class="s1">model = QuantileRegressor(</span>
        <span class="s1">quantile=quantile</span><span class="s2">, </span><span class="s1">alpha=alpha</span><span class="s2">, </span><span class="s1">solver=default_solver</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_allclose(model.intercept_</span><span class="s2">, </span><span class="s1">intercept</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">1e-2</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">coef </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">assert_allclose(model.coef_[</span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">coef</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">1e-2</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">alpha &lt; </span><span class="s3">100</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s1">model.coef_[</span><span class="s3">0</span><span class="s1">] &gt;= </span><span class="s3">1</span>
    <span class="s2">assert </span><span class="s1">model.coef_[</span><span class="s3">0</span><span class="s1">] &lt;= </span><span class="s3">10</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;fit_intercept&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_quantile_equals_huber_for_low_epsilon(fit_intercept</span><span class="s2">, </span><span class="s1">default_solver):</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(n_samples=</span><span class="s3">100</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">20</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">noise=</span><span class="s3">1.0</span><span class="s1">)</span>
    <span class="s1">alpha = </span><span class="s3">1e-4</span>
    <span class="s1">huber = HuberRegressor(</span>
        <span class="s1">epsilon=</span><span class="s3">1 </span><span class="s1">+ </span><span class="s3">1e-4</span><span class="s2">, </span><span class="s1">alpha=alpha</span><span class="s2">, </span><span class="s1">fit_intercept=fit_intercept</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">quant = QuantileRegressor(</span>
        <span class="s1">alpha=alpha</span><span class="s2">, </span><span class="s1">fit_intercept=fit_intercept</span><span class="s2">, </span><span class="s1">solver=default_solver</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_allclose(huber.coef_</span><span class="s2">, </span><span class="s1">quant.coef_</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">1e-1</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept:</span>
        <span class="s2">assert </span><span class="s1">huber.intercept_ == approx(quant.intercept_</span><span class="s2">, </span><span class="s1">abs=</span><span class="s3">1e-1</span><span class="s1">)</span>
        <span class="s0"># check that we still predict fraction</span>
        <span class="s2">assert </span><span class="s1">np.mean(y &lt; quant.predict(X)) == approx(</span><span class="s3">0.5</span><span class="s2">, </span><span class="s1">abs=</span><span class="s3">1e-1</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;q&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0.5</span><span class="s2">, </span><span class="s3">0.9</span><span class="s2">, </span><span class="s3">0.05</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_quantile_estimates_calibration(q</span><span class="s2">, </span><span class="s1">default_solver):</span>
    <span class="s0"># Test that model estimates percentage of points below the prediction</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(n_samples=</span><span class="s3">1000</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">20</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">noise=</span><span class="s3">1.0</span><span class="s1">)</span>
    <span class="s1">quant = QuantileRegressor(</span>
        <span class="s1">quantile=q</span><span class="s2">,</span>
        <span class="s1">alpha=</span><span class="s3">0</span><span class="s2">,</span>
        <span class="s1">solver=default_solver</span><span class="s2">,</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">np.mean(y &lt; quant.predict(X)) == approx(q</span><span class="s2">, </span><span class="s1">abs=</span><span class="s3">1e-2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_quantile_sample_weight(default_solver):</span>
    <span class="s0"># test that with unequal sample weights we still estimate weighted fraction</span>
    <span class="s1">n = </span><span class="s3">1000</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(n_samples=n</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">5</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">noise=</span><span class="s3">10.0</span><span class="s1">)</span>
    <span class="s1">weight = np.ones(n)</span>
    <span class="s0"># when we increase weight of upper observations,</span>
    <span class="s0"># estimate of quantile should go up</span>
    <span class="s1">weight[y &gt; y.mean()] = </span><span class="s3">100</span>
    <span class="s1">quant = QuantileRegressor(quantile=</span><span class="s3">0.5</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s3">1e-8</span><span class="s2">, </span><span class="s1">solver=default_solver)</span>
    <span class="s1">quant.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=weight)</span>
    <span class="s1">fraction_below = np.mean(y &lt; quant.predict(X))</span>
    <span class="s2">assert </span><span class="s1">fraction_below &gt; </span><span class="s3">0.5</span>
    <span class="s1">weighted_fraction_below = np.average(y &lt; quant.predict(X)</span><span class="s2">, </span><span class="s1">weights=weight)</span>
    <span class="s2">assert </span><span class="s1">weighted_fraction_below == approx(</span><span class="s3">0.5</span><span class="s2">, </span><span class="s1">abs=</span><span class="s3">3e-2</span><span class="s1">)</span>


<span class="s1">@pytest.mark.skipif(</span>
    <span class="s1">sp_version &lt; parse_version(</span><span class="s4">&quot;1.6.0&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">reason=</span><span class="s4">&quot;The `highs` solver is available from the 1.6.0 scipy version&quot;</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;quantile&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0.2</span><span class="s2">, </span><span class="s3">0.5</span><span class="s2">, </span><span class="s3">0.8</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_asymmetric_error(quantile</span><span class="s2">, </span><span class="s1">default_solver):</span>
    <span class="s5">&quot;&quot;&quot;Test quantile regression for asymmetric distributed targets.&quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s3">1000</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">X = np.concatenate(</span>
        <span class="s1">(</span>
            <span class="s1">np.abs(rng.randn(n_samples)[:</span><span class="s2">, None</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">-rng.randint(</span><span class="s3">2</span><span class="s2">, </span><span class="s1">size=(n_samples</span><span class="s2">, </span><span class="s3">1</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">axis=</span><span class="s3">1</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">intercept = </span><span class="s3">1.23</span>
    <span class="s1">coef = np.array([</span><span class="s3">0.5</span><span class="s2">, </span><span class="s1">-</span><span class="s3">2</span><span class="s1">])</span>
    <span class="s0">#  Take care that X @ coef + intercept &gt; 0</span>
    <span class="s2">assert </span><span class="s1">np.min(X @ coef + intercept) &gt; </span><span class="s3">0</span>
    <span class="s0"># For an exponential distribution with rate lambda, e.g. exp(-lambda * x),</span>
    <span class="s0"># the quantile at level q is:</span>
    <span class="s0">#   quantile(q) = - log(1 - q) / lambda</span>
    <span class="s0">#   scale = 1/lambda = -quantile(q) / log(1 - q)</span>
    <span class="s1">y = rng.exponential(</span>
        <span class="s1">scale=-(X @ coef + intercept) / np.log(</span><span class="s3">1 </span><span class="s1">- quantile)</span><span class="s2">, </span><span class="s1">size=n_samples</span>
    <span class="s1">)</span>
    <span class="s1">model = QuantileRegressor(</span>
        <span class="s1">quantile=quantile</span><span class="s2">,</span>
        <span class="s1">alpha=</span><span class="s3">0</span><span class="s2">,</span>
        <span class="s1">solver=default_solver</span><span class="s2">,</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s0"># This test can be made to pass with any solver but in the interest</span>
    <span class="s0"># of sparing continuous integration resources, the test is performed</span>
    <span class="s0"># with the fastest solver only.</span>

    <span class="s2">assert </span><span class="s1">model.intercept_ == approx(intercept</span><span class="s2">, </span><span class="s1">rel=</span><span class="s3">0.2</span><span class="s1">)</span>
    <span class="s1">assert_allclose(model.coef_</span><span class="s2">, </span><span class="s1">coef</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s3">0.6</span><span class="s1">)</span>
    <span class="s1">assert_allclose(np.mean(model.predict(X) &gt; y)</span><span class="s2">, </span><span class="s1">quantile</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">1e-2</span><span class="s1">)</span>

    <span class="s0"># Now compare to Nelder-Mead optimization with L1 penalty</span>
    <span class="s1">alpha = </span><span class="s3">0.01</span>
    <span class="s1">model.set_params(alpha=alpha).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">model_coef = np.r_[model.intercept_</span><span class="s2">, </span><span class="s1">model.coef_]</span>

    <span class="s2">def </span><span class="s1">func(coef):</span>
        <span class="s1">loss = mean_pinball_loss(y</span><span class="s2">, </span><span class="s1">X @ coef[</span><span class="s3">1</span><span class="s1">:] + coef[</span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">alpha=quantile)</span>
        <span class="s1">L1 = np.sum(np.abs(coef[</span><span class="s3">1</span><span class="s1">:]))</span>
        <span class="s2">return </span><span class="s1">loss + alpha * L1</span>

    <span class="s1">res = minimize(</span>
        <span class="s1">fun=func</span><span class="s2">,</span>
        <span class="s1">x0=[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">method=</span><span class="s4">&quot;Nelder-Mead&quot;</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s3">1e-12</span><span class="s2">,</span>
        <span class="s1">options={</span><span class="s4">&quot;maxiter&quot;</span><span class="s1">: </span><span class="s3">2000</span><span class="s1">}</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s2">assert </span><span class="s1">func(model_coef) == approx(func(res.x))</span>
    <span class="s1">assert_allclose(model.intercept_</span><span class="s2">, </span><span class="s1">res.x[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">assert_allclose(model.coef_</span><span class="s2">, </span><span class="s1">res.x[</span><span class="s3">1</span><span class="s1">:])</span>
    <span class="s1">assert_allclose(np.mean(model.predict(X) &gt; y)</span><span class="s2">, </span><span class="s1">quantile</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">1e-2</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;quantile&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0.2</span><span class="s2">, </span><span class="s3">0.5</span><span class="s2">, </span><span class="s3">0.8</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_equivariance(quantile</span><span class="s2">, </span><span class="s1">default_solver):</span>
    <span class="s5">&quot;&quot;&quot;Test equivariace of quantile regression. 
 
    See Koenker (2005) Quantile Regression, Chapter 2.2.3. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s3">100</span><span class="s2">, </span><span class="s3">5</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">n_samples=n_samples</span><span class="s2">,</span>
        <span class="s1">n_features=n_features</span><span class="s2">,</span>
        <span class="s1">n_informative=n_features</span><span class="s2">,</span>
        <span class="s1">noise=</span><span class="s3">0</span><span class="s2">,</span>
        <span class="s1">random_state=rng</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
    <span class="s1">)</span>
    <span class="s0"># make y asymmetric</span>
    <span class="s1">y += rng.exponential(scale=</span><span class="s3">100</span><span class="s2">, </span><span class="s1">size=y.shape)</span>
    <span class="s1">params = dict(alpha=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">solver=default_solver)</span>
    <span class="s1">model1 = QuantileRegressor(quantile=quantile</span><span class="s2">, </span><span class="s1">**params).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s0"># coef(q; a*y, X) = a * coef(q; y, X)</span>
    <span class="s1">a = </span><span class="s3">2.5</span>
    <span class="s1">model2 = QuantileRegressor(quantile=quantile</span><span class="s2">, </span><span class="s1">**params).fit(X</span><span class="s2">, </span><span class="s1">a * y)</span>
    <span class="s2">assert </span><span class="s1">model2.intercept_ == approx(a * model1.intercept_</span><span class="s2">, </span><span class="s1">rel=</span><span class="s3">1e-5</span><span class="s1">)</span>
    <span class="s1">assert_allclose(model2.coef_</span><span class="s2">, </span><span class="s1">a * model1.coef_</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s3">1e-5</span><span class="s1">)</span>

    <span class="s0"># coef(1-q; -a*y, X) = -a * coef(q; y, X)</span>
    <span class="s1">model2 = QuantileRegressor(quantile=</span><span class="s3">1 </span><span class="s1">- quantile</span><span class="s2">, </span><span class="s1">**params).fit(X</span><span class="s2">, </span><span class="s1">-a * y)</span>
    <span class="s2">assert </span><span class="s1">model2.intercept_ == approx(-a * model1.intercept_</span><span class="s2">, </span><span class="s1">rel=</span><span class="s3">1e-5</span><span class="s1">)</span>
    <span class="s1">assert_allclose(model2.coef_</span><span class="s2">, </span><span class="s1">-a * model1.coef_</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s3">1e-5</span><span class="s1">)</span>

    <span class="s0"># coef(q; y + X @ g, X) = coef(q; y, X) + g</span>
    <span class="s1">g_intercept</span><span class="s2">, </span><span class="s1">g_coef = rng.randn()</span><span class="s2">, </span><span class="s1">rng.randn(n_features)</span>
    <span class="s1">model2 = QuantileRegressor(quantile=quantile</span><span class="s2">, </span><span class="s1">**params)</span>
    <span class="s1">model2.fit(X</span><span class="s2">, </span><span class="s1">y + X @ g_coef + g_intercept)</span>
    <span class="s2">assert </span><span class="s1">model2.intercept_ == approx(model1.intercept_ + g_intercept)</span>
    <span class="s1">assert_allclose(model2.coef_</span><span class="s2">, </span><span class="s1">model1.coef_ + g_coef</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s3">1e-6</span><span class="s1">)</span>

    <span class="s0"># coef(q; y, X @ A) = A^-1 @ coef(q; y, X)</span>
    <span class="s1">A = rng.randn(n_features</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">model2 = QuantileRegressor(quantile=quantile</span><span class="s2">, </span><span class="s1">**params)</span>
    <span class="s1">model2.fit(X @ A</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">model2.intercept_ == approx(model1.intercept_</span><span class="s2">, </span><span class="s1">rel=</span><span class="s3">1e-5</span><span class="s1">)</span>
    <span class="s1">assert_allclose(model2.coef_</span><span class="s2">, </span><span class="s1">np.linalg.solve(A</span><span class="s2">, </span><span class="s1">model1.coef_)</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s3">1e-5</span><span class="s1">)</span>


<span class="s1">@pytest.mark.skipif(</span>
    <span class="s1">parse_version(sp_version.base_version) &gt;= parse_version(</span><span class="s4">&quot;1.11&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">reason=</span><span class="s4">&quot;interior-point solver is not available in SciPy 1.11&quot;</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.filterwarnings(</span><span class="s4">&quot;ignore:`method='interior-point'` is deprecated&quot;</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_linprog_failure():</span>
    <span class="s5">&quot;&quot;&quot;Test that linprog fails.&quot;&quot;&quot;</span>
    <span class="s1">X = np.linspace(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">10</span><span class="s2">, </span><span class="s1">num=</span><span class="s3">10</span><span class="s1">).reshape(-</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">y = np.linspace(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">10</span><span class="s2">, </span><span class="s1">num=</span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">reg = QuantileRegressor(</span>
        <span class="s1">alpha=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">solver=</span><span class="s4">&quot;interior-point&quot;</span><span class="s2">, </span><span class="s1">solver_options={</span><span class="s4">&quot;maxiter&quot;</span><span class="s1">: </span><span class="s3">1</span><span class="s1">}</span>
    <span class="s1">)</span>

    <span class="s1">msg = </span><span class="s4">&quot;Linear programming for QuantileRegressor did not succeed.&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">reg.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s1">@skip_if_32bit</span>
<span class="s1">@pytest.mark.skipif(</span>
    <span class="s1">sp_version &lt;= parse_version(</span><span class="s4">&quot;1.6.0&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">reason=</span><span class="s4">&quot;Solvers are available as of scipy 1.6.0&quot;</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;sparse_format&quot;</span><span class="s2">, </span><span class="s1">[sparse.csc_matrix</span><span class="s2">, </span><span class="s1">sparse.csr_matrix</span><span class="s2">, </span><span class="s1">sparse.coo_matrix]</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;solver&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;highs&quot;</span><span class="s2">, </span><span class="s4">&quot;highs-ds&quot;</span><span class="s2">, </span><span class="s4">&quot;highs-ipm&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;fit_intercept&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_sparse_input(sparse_format</span><span class="s2">, </span><span class="s1">solver</span><span class="s2">, </span><span class="s1">fit_intercept</span><span class="s2">, </span><span class="s1">default_solver):</span>
    <span class="s5">&quot;&quot;&quot;Test that sparse and dense X give same results.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(n_samples=</span><span class="s3">100</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">20</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">noise=</span><span class="s3">1.0</span><span class="s1">)</span>
    <span class="s1">X_sparse = sparse_format(X)</span>
    <span class="s1">alpha = </span><span class="s3">1e-4</span>
    <span class="s1">quant_dense = QuantileRegressor(</span>
        <span class="s1">alpha=alpha</span><span class="s2">, </span><span class="s1">fit_intercept=fit_intercept</span><span class="s2">, </span><span class="s1">solver=default_solver</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">quant_sparse = QuantileRegressor(</span>
        <span class="s1">alpha=alpha</span><span class="s2">, </span><span class="s1">fit_intercept=fit_intercept</span><span class="s2">, </span><span class="s1">solver=solver</span>
    <span class="s1">).fit(X_sparse</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_allclose(quant_sparse.coef_</span><span class="s2">, </span><span class="s1">quant_dense.coef_</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s3">1e-2</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept:</span>
        <span class="s2">assert </span><span class="s1">quant_sparse.intercept_ == approx(quant_dense.intercept_)</span>
        <span class="s0"># check that we still predict fraction</span>
        <span class="s2">assert </span><span class="s3">0.45 </span><span class="s1">&lt;= np.mean(y &lt; quant_sparse.predict(X_sparse)) &lt;= </span><span class="s3">0.57</span>


<span class="s0"># TODO (1.4): remove this test in 1.4</span>
<span class="s1">@pytest.mark.skipif(</span>
    <span class="s1">parse_version(sp_version.base_version) &gt;= parse_version(</span><span class="s4">&quot;1.11&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">reason=</span><span class="s4">&quot;interior-point solver is not available in SciPy 1.11&quot;</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_warning_new_default(X_y_data):</span>
    <span class="s5">&quot;&quot;&quot;Check that we warn about the new default solver.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = X_y_data</span>
    <span class="s1">model = QuantileRegressor()</span>
    <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s2">, </span><span class="s1">match=</span><span class="s4">&quot;The default solver will change&quot;</span><span class="s1">):</span>
        <span class="s1">model.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_error_interior_point_future(X_y_data</span><span class="s2">, </span><span class="s1">monkeypatch):</span>
    <span class="s5">&quot;&quot;&quot;Check that we will raise a proper error when requesting 
    `solver='interior-point'` in SciPy &gt;= 1.11. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = X_y_data</span>
    <span class="s2">import </span><span class="s1">sklearn.linear_model._quantile</span>

    <span class="s2">with </span><span class="s1">monkeypatch.context() </span><span class="s2">as </span><span class="s1">m:</span>
        <span class="s1">m.setattr(sklearn.linear_model._quantile</span><span class="s2">, </span><span class="s4">&quot;sp_version&quot;</span><span class="s2">, </span><span class="s1">parse_version(</span><span class="s4">&quot;1.11.0&quot;</span><span class="s1">))</span>
        <span class="s1">err_msg = </span><span class="s4">&quot;Solver interior-point is not anymore available in SciPy &gt;= 1.11.0.&quot;</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
            <span class="s1">QuantileRegressor(solver=</span><span class="s4">&quot;interior-point&quot;</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
</pre>
</body>
</html>