<html>
<head>
<title>kernridgeregress_class.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
kernridgeregress_class.py</font>
</center></td></tr></table>
<pre><span class="s0">'''Kernel Ridge Regression for local non-parametric regression'''</span>


<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">spatial </span><span class="s2">as </span><span class="s1">ssp</span>
<span class="s2">import </span><span class="s1">matplotlib.pylab </span><span class="s2">as </span><span class="s1">plt</span>


<span class="s2">def </span><span class="s1">kernel_rbf(x</span><span class="s2">,</span><span class="s1">y</span><span class="s2">,</span><span class="s1">scale=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">**kwds):</span>
    <span class="s4">#scale = kwds.get('scale',1)</span>
    <span class="s1">dist = ssp.minkowski_distance_p(x[:</span><span class="s2">,</span><span class="s1">np.newaxis</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">,</span><span class="s1">y[np.newaxis</span><span class="s2">,</span><span class="s1">:</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">,</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">np.exp(-</span><span class="s3">0.5</span><span class="s1">/scale*(dist))</span>


<span class="s2">def </span><span class="s1">kernel_euclid(x</span><span class="s2">,</span><span class="s1">y</span><span class="s2">,</span><span class="s1">p=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">**kwds):</span>
    <span class="s2">return </span><span class="s1">ssp.minkowski_distance(x[:</span><span class="s2">,</span><span class="s1">np.newaxis</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">,</span><span class="s1">y[np.newaxis</span><span class="s2">,</span><span class="s1">:</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">,</span><span class="s1">p)</span>


<span class="s2">class </span><span class="s1">GaussProcess:</span>
    <span class="s0">'''class to perform kernel ridge regression (gaussian process) 
 
    Warning: this class is memory intensive, it creates nobs x nobs distance 
    matrix and its inverse, where nobs is the number of rows (observations). 
    See sparse version for larger number of observations 
 
 
    Notes 
    ----- 
 
    Todo: 
    * normalize multidimensional x array on demand, either by var or cov 
    * add confidence band 
    * automatic selection or proposal of smoothing parameters 
 
    Note: this is different from kernel smoothing regression, 
       see for example https://en.wikipedia.org/wiki/Kernel_smoother 
 
    In this version of the kernel ridge regression, the training points 
    are fitted exactly. 
    Needs a fast version for leave-one-out regression, for fitting each 
    observation on all the other points. 
    This version could be numerically improved for the calculation for many 
    different values of the ridge coefficient. see also short summary by 
    Isabelle Guyon (ETHZ) in a manuscript KernelRidge.pdf 
 
    Needs verification and possibly additional statistical results or 
    summary statistics for interpretation, but this is a problem with 
    non-parametric, non-linear methods. 
 
    Reference 
    --------- 
 
    Rasmussen, C.E. and C.K.I. Williams, 2006, Gaussian Processes for Machine 
    Learning, the MIT Press, www.GaussianProcess.org/gpal, chapter 2 
 
    a short summary of the kernel ridge regression is at 
    http://www.ics.uci.edu/~welling/teaching/KernelsICS273B/Kernel-Ridge.pdf 
    '''</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None, </span><span class="s1">kernel=kernel_rbf</span><span class="s2">,</span>
                 <span class="s1">scale=</span><span class="s3">0.5</span><span class="s2">, </span><span class="s1">ridgecoeff = </span><span class="s3">1e-10</span><span class="s2">, </span><span class="s1">**kwds ):</span>
        <span class="s0">''' 
        Parameters 
        ---------- 
        x : 2d array (N,K) 
           data array of explanatory variables, columns represent variables 
           rows represent observations 
        y : 2d array (N,1) (optional) 
           endogenous variable that should be fitted or predicted 
           can alternatively be specified as parameter to fit method 
        kernel : function, default: kernel_rbf 
           kernel: (x1,x2)-&gt;kernel matrix is a function that takes as parameter 
           two column arrays and return the kernel or distance matrix 
        scale : float (optional) 
           smoothing parameter for the rbf kernel 
        ridgecoeff : float (optional) 
           coefficient that is multiplied with the identity matrix in the 
           ridge regression 
 
        Notes 
        ----- 
        After initialization, kernel matrix is calculated and if y is given 
        as parameter then also the linear regression parameter and the 
        fitted or estimated y values, yest, are calculated. yest is available 
        as an attribute in this case. 
 
        Both scale and the ridge coefficient smooth the fitted curve. 
 
        '''</span>

        <span class="s1">self.x = x</span>
        <span class="s1">self.kernel = kernel</span>
        <span class="s1">self.scale = scale</span>
        <span class="s1">self.ridgecoeff = ridgecoeff</span>
        <span class="s1">self.distxsample = kernel(x</span><span class="s2">,</span><span class="s1">x</span><span class="s2">,</span><span class="s1">scale=scale)</span>
        <span class="s1">self.Kinv = np.linalg.inv(self.distxsample +</span>
                             <span class="s1">np.eye(*self.distxsample.shape)*ridgecoeff)</span>
        <span class="s2">if </span><span class="s1">y </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">self.y = y</span>
            <span class="s1">self.yest = self.fit(y)</span>


    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">,</span><span class="s1">y):</span>
        <span class="s0">'''fit the training explanatory variables to a sample ouput variable'''</span>
        <span class="s1">self.parest = np.dot(self.Kinv</span><span class="s2">, </span><span class="s1">y) </span><span class="s4">#self.kernel(y,y,scale=self.scale))</span>
        <span class="s1">yhat = np.dot(self.distxsample</span><span class="s2">,</span><span class="s1">self.parest)</span>
        <span class="s2">return </span><span class="s1">yhat</span>

<span class="s4">##        print ds33.shape</span>
<span class="s4">##        ds33_2 = kernel(x,x[::k,:],scale=scale)</span>
<span class="s4">##        dsinv = np.linalg.inv(ds33+np.eye(*distxsample.shape)*ridgecoeff)</span>
<span class="s4">##        B = np.dot(dsinv,y[::k,:])</span>
    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">,</span><span class="s1">x):</span>
        <span class="s0">'''predict new y values for a given array of explanatory variables'''</span>
        <span class="s1">self.xpredict = x</span>
        <span class="s1">distxpredict = self.kernel(x</span><span class="s2">, </span><span class="s1">self.x</span><span class="s2">, </span><span class="s1">scale=self.scale)</span>
        <span class="s1">self.ypredict = np.dot(distxpredict</span><span class="s2">, </span><span class="s1">self.parest)</span>
        <span class="s2">return </span><span class="s1">self.ypredict</span>

    <span class="s2">def </span><span class="s1">plot(self</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">plt=plt ):</span>
        <span class="s0">'''some basic plots'''</span>
        <span class="s4">#todo return proper graph handles</span>
        <span class="s1">plt.figure()</span>
        <span class="s1">plt.plot(self.x</span><span class="s2">,</span><span class="s1">self.y</span><span class="s2">, </span><span class="s5">'bo-'</span><span class="s2">, </span><span class="s1">self.x</span><span class="s2">, </span><span class="s1">self.yest</span><span class="s2">, </span><span class="s5">'r.-'</span><span class="s1">)</span>
        <span class="s1">plt.title(</span><span class="s5">'sample (training) points'</span><span class="s1">)</span>
        <span class="s1">plt.figure()</span>
        <span class="s1">plt.plot(self.xpredict</span><span class="s2">,</span><span class="s1">y</span><span class="s2">,</span><span class="s5">'bo-'</span><span class="s2">,</span><span class="s1">self.xpredict</span><span class="s2">,</span><span class="s1">self.ypredict</span><span class="s2">,</span><span class="s5">'r.-'</span><span class="s1">)</span>
        <span class="s1">plt.title(</span><span class="s5">'all points'</span><span class="s1">)</span>



<span class="s2">def </span><span class="s1">example1():</span>
    <span class="s1">m</span><span class="s2">,</span><span class="s1">k = </span><span class="s3">500</span><span class="s2">,</span><span class="s3">4</span>
    <span class="s1">upper = </span><span class="s3">6</span>
    <span class="s1">scale=</span><span class="s3">10</span>
    <span class="s1">xs1a = np.linspace(</span><span class="s3">1</span><span class="s2">,</span><span class="s1">upper</span><span class="s2">,</span><span class="s1">m)[:</span><span class="s2">,</span><span class="s1">np.newaxis]</span>
    <span class="s1">xs1 = xs1a*np.ones((</span><span class="s3">1</span><span class="s2">,</span><span class="s3">4</span><span class="s1">)) + </span><span class="s3">1</span><span class="s1">/(</span><span class="s3">1.0</span><span class="s1">+np.exp(np.random.randn(m</span><span class="s2">,</span><span class="s1">k)))</span>
    <span class="s1">xs1 /= np.std(xs1[::k</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">,</span><span class="s3">0</span><span class="s1">)   </span><span class="s4"># normalize scale, could use cov to normalize</span>
    <span class="s1">y1true = np.sum(np.sin(xs1)+np.sqrt(xs1)</span><span class="s2">,</span><span class="s3">1</span><span class="s1">)[:</span><span class="s2">,</span><span class="s1">np.newaxis]</span>
    <span class="s1">y1 = y1true + </span><span class="s3">0.250 </span><span class="s1">* np.random.randn(m</span><span class="s2">,</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">stride = </span><span class="s3">2 </span><span class="s4">#use only some points as trainig points e.g 2 means every 2nd</span>
    <span class="s1">gp1 = GaussProcess(xs1[::stride</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">,</span><span class="s1">y1[::stride</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">kernel=kernel_euclid</span><span class="s2">,</span>
                       <span class="s1">ridgecoeff=</span><span class="s3">1e-10</span><span class="s1">)</span>
    <span class="s1">yhatr1 = gp1.predict(xs1)</span>
    <span class="s1">plt.figure()</span>
    <span class="s1">plt.plot(y1true</span><span class="s2">, </span><span class="s1">y1</span><span class="s2">,</span><span class="s5">'bo'</span><span class="s2">,</span><span class="s1">y1true</span><span class="s2">, </span><span class="s1">yhatr1</span><span class="s2">,</span><span class="s5">'r.'</span><span class="s1">)</span>
    <span class="s1">plt.title(</span><span class="s5">'euclid kernel: true y versus noisy y and estimated y'</span><span class="s1">)</span>
    <span class="s1">plt.figure()</span>
    <span class="s1">plt.plot(y1</span><span class="s2">,</span><span class="s5">'bo-'</span><span class="s2">,</span><span class="s1">y1true</span><span class="s2">,</span><span class="s5">'go-'</span><span class="s2">,</span><span class="s1">yhatr1</span><span class="s2">,</span><span class="s5">'r.-'</span><span class="s1">)</span>
    <span class="s1">plt.title(</span><span class="s5">'euclid kernel: true (green), noisy (blue) and estimated (red) '</span><span class="s1">+</span>
              <span class="s5">'observations'</span><span class="s1">)</span>

    <span class="s1">gp2 = GaussProcess(xs1[::stride</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">,</span><span class="s1">y1[::stride</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">kernel=kernel_rbf</span><span class="s2">,</span>
                       <span class="s1">scale=scale</span><span class="s2">, </span><span class="s1">ridgecoeff=</span><span class="s3">1e-1</span><span class="s1">)</span>
    <span class="s1">yhatr2 = gp2.predict(xs1)</span>
    <span class="s1">plt.figure()</span>
    <span class="s1">plt.plot(y1true</span><span class="s2">, </span><span class="s1">y1</span><span class="s2">,</span><span class="s5">'bo'</span><span class="s2">,</span><span class="s1">y1true</span><span class="s2">, </span><span class="s1">yhatr2</span><span class="s2">,</span><span class="s5">'r.'</span><span class="s1">)</span>
    <span class="s1">plt.title(</span><span class="s5">'rbf kernel: true versus noisy (blue) and estimated (red) observations'</span><span class="s1">)</span>
    <span class="s1">plt.figure()</span>
    <span class="s1">plt.plot(y1</span><span class="s2">,</span><span class="s5">'bo-'</span><span class="s2">,</span><span class="s1">y1true</span><span class="s2">,</span><span class="s5">'go-'</span><span class="s2">,</span><span class="s1">yhatr2</span><span class="s2">,</span><span class="s5">'r.-'</span><span class="s1">)</span>
    <span class="s1">plt.title(</span><span class="s5">'rbf kernel: true (green), noisy (blue) and estimated (red) '</span><span class="s1">+</span>
              <span class="s5">'observations'</span><span class="s1">)</span>
    <span class="s4">#gp2.plot(y1)</span>


<span class="s2">def </span><span class="s1">example2(m=</span><span class="s3">100</span><span class="s2">, </span><span class="s1">scale=</span><span class="s3">0.01</span><span class="s2">, </span><span class="s1">stride=</span><span class="s3">2</span><span class="s1">):</span>
    <span class="s4">#m,k = 100,1</span>
    <span class="s1">upper = </span><span class="s3">6</span>
    <span class="s1">xs1 = np.linspace(</span><span class="s3">1</span><span class="s2">,</span><span class="s1">upper</span><span class="s2">,</span><span class="s1">m)[:</span><span class="s2">,</span><span class="s1">np.newaxis]</span>
    <span class="s1">y1true = np.sum(np.sin(xs1**</span><span class="s3">2</span><span class="s1">)</span><span class="s2">,</span><span class="s3">1</span><span class="s1">)[:</span><span class="s2">,</span><span class="s1">np.newaxis]/xs1</span>
    <span class="s1">y1 = y1true + </span><span class="s3">0.05</span><span class="s1">*np.random.randn(m</span><span class="s2">,</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">ridgecoeff = </span><span class="s3">1e-10</span>
    <span class="s4">#stride = 2 #use only some points as trainig points e.g 2 means every 2nd</span>
    <span class="s1">gp1 = GaussProcess(xs1[::stride</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">,</span><span class="s1">y1[::stride</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">kernel=kernel_euclid</span><span class="s2">,</span>
                       <span class="s1">ridgecoeff=</span><span class="s3">1e-10</span><span class="s1">)</span>
    <span class="s1">yhatr1 = gp1.predict(xs1)</span>
    <span class="s1">plt.figure()</span>
    <span class="s1">plt.plot(y1true</span><span class="s2">, </span><span class="s1">y1</span><span class="s2">,</span><span class="s5">'bo'</span><span class="s2">,</span><span class="s1">y1true</span><span class="s2">, </span><span class="s1">yhatr1</span><span class="s2">,</span><span class="s5">'r.'</span><span class="s1">)</span>
    <span class="s1">plt.title(</span><span class="s5">'euclid kernel: true versus noisy (blue) and estimated (red) observations'</span><span class="s1">)</span>
    <span class="s1">plt.figure()</span>
    <span class="s1">plt.plot(y1</span><span class="s2">,</span><span class="s5">'bo-'</span><span class="s2">,</span><span class="s1">y1true</span><span class="s2">,</span><span class="s5">'go-'</span><span class="s2">,</span><span class="s1">yhatr1</span><span class="s2">,</span><span class="s5">'r.-'</span><span class="s1">)</span>
    <span class="s1">plt.title(</span><span class="s5">'euclid kernel: true (green), noisy (blue) and estimated (red) '</span><span class="s1">+</span>
              <span class="s5">'observations'</span><span class="s1">)</span>

    <span class="s1">gp2 = GaussProcess(xs1[::stride</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">,</span><span class="s1">y1[::stride</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">kernel=kernel_rbf</span><span class="s2">,</span>
                       <span class="s1">scale=scale</span><span class="s2">, </span><span class="s1">ridgecoeff=</span><span class="s3">1e-2</span><span class="s1">)</span>
    <span class="s1">yhatr2 = gp2.predict(xs1)</span>
    <span class="s1">plt.figure()</span>
    <span class="s1">plt.plot(y1true</span><span class="s2">, </span><span class="s1">y1</span><span class="s2">,</span><span class="s5">'bo'</span><span class="s2">,</span><span class="s1">y1true</span><span class="s2">, </span><span class="s1">yhatr2</span><span class="s2">,</span><span class="s5">'r.'</span><span class="s1">)</span>
    <span class="s1">plt.title(</span><span class="s5">'rbf kernel: true versus noisy (blue) and estimated (red) observations'</span><span class="s1">)</span>
    <span class="s1">plt.figure()</span>
    <span class="s1">plt.plot(y1</span><span class="s2">,</span><span class="s5">'bo-'</span><span class="s2">,</span><span class="s1">y1true</span><span class="s2">,</span><span class="s5">'go-'</span><span class="s2">,</span><span class="s1">yhatr2</span><span class="s2">,</span><span class="s5">'r.-'</span><span class="s1">)</span>
    <span class="s1">plt.title(</span><span class="s5">'rbf kernel: true (green), noisy (blue) and estimated (red) '</span><span class="s1">+</span>
              <span class="s5">'observations'</span><span class="s1">)</span>
    <span class="s4">#gp2.plot(y1)</span>

<span class="s2">if </span><span class="s1">__name__ == </span><span class="s5">'__main__'</span><span class="s1">:</span>
    <span class="s1">example2()</span>
    <span class="s4">#example2(m=1000, scale=0.01)</span>
    <span class="s4">#example2(m=100, scale=0.5)   # oversmoothing</span>
    <span class="s4">#example2(m=2000, scale=0.005) # this looks good for rbf, zoom in</span>
    <span class="s4">#example2(m=200, scale=0.01,stride=4)</span>
    <span class="s1">example1()</span>
    <span class="s4">#plt.show()</span>
</pre>
</body>
</html>