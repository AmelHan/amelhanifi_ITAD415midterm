<html>
<head>
<title>test_estimator_checks.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_estimator_checks.py</font>
</center></td></tr></table>
<pre><span class="s0"># We can not use pytest here, because we run</span>
<span class="s0"># build_tools/azure/test_pytest_soft_dependency.sh on these</span>
<span class="s0"># tests to make sure estimator_checks works without pytest.</span>

<span class="s2">import </span><span class="s1">importlib</span>
<span class="s2">import </span><span class="s1">sys</span>
<span class="s2">import </span><span class="s1">unittest</span>
<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">numbers </span><span class="s2">import </span><span class="s1">Integral</span><span class="s2">, </span><span class="s1">Real</span>

<span class="s2">import </span><span class="s1">joblib</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">scipy.sparse </span><span class="s2">as </span><span class="s1">sp</span>

<span class="s2">from </span><span class="s1">sklearn </span><span class="s2">import </span><span class="s1">config_context</span><span class="s2">, </span><span class="s1">get_config</span>
<span class="s2">from </span><span class="s1">sklearn.base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">ClassifierMixin</span><span class="s2">, </span><span class="s1">OutlierMixin</span>
<span class="s2">from </span><span class="s1">sklearn.cluster </span><span class="s2">import </span><span class="s1">MiniBatchKMeans</span>
<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">make_multilabel_classification</span>
<span class="s2">from </span><span class="s1">sklearn.decomposition </span><span class="s2">import </span><span class="s1">PCA</span>
<span class="s2">from </span><span class="s1">sklearn.ensemble </span><span class="s2">import </span><span class="s1">ExtraTreesClassifier</span>
<span class="s2">from </span><span class="s1">sklearn.exceptions </span><span class="s2">import </span><span class="s1">ConvergenceWarning</span><span class="s2">, </span><span class="s1">SkipTestWarning</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">LinearRegression</span><span class="s2">,</span>
    <span class="s1">LogisticRegression</span><span class="s2">,</span>
    <span class="s1">MultiTaskElasticNet</span><span class="s2">,</span>
    <span class="s1">SGDClassifier</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.mixture </span><span class="s2">import </span><span class="s1">GaussianMixture</span>
<span class="s2">from </span><span class="s1">sklearn.neighbors </span><span class="s2">import </span><span class="s1">KNeighborsRegressor</span>
<span class="s2">from </span><span class="s1">sklearn.svm </span><span class="s2">import </span><span class="s1">SVC</span><span class="s2">, </span><span class="s1">NuSVC</span>
<span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">_array_api</span><span class="s2">, </span><span class="s1">all_estimators</span><span class="s2">, </span><span class="s1">deprecated</span>
<span class="s2">from </span><span class="s1">sklearn.utils._param_validation </span><span class="s2">import </span><span class="s1">Interval</span><span class="s2">, </span><span class="s1">StrOptions</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">MinimalClassifier</span><span class="s2">,</span>
    <span class="s1">MinimalRegressor</span><span class="s2">,</span>
    <span class="s1">MinimalTransformer</span><span class="s2">,</span>
    <span class="s1">SkipTest</span><span class="s2">,</span>
    <span class="s1">ignore_warnings</span><span class="s2">,</span>
    <span class="s1">raises</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.utils.estimator_checks </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">_NotAnArray</span><span class="s2">,</span>
    <span class="s1">_set_checking_parameters</span><span class="s2">,</span>
    <span class="s1">_yield_all_checks</span><span class="s2">,</span>
    <span class="s1">check_array_api_input</span><span class="s2">,</span>
    <span class="s1">check_class_weight_balanced_linear_classifier</span><span class="s2">,</span>
    <span class="s1">check_classifier_data_not_an_array</span><span class="s2">,</span>
    <span class="s1">check_classifiers_multilabel_output_format_decision_function</span><span class="s2">,</span>
    <span class="s1">check_classifiers_multilabel_output_format_predict</span><span class="s2">,</span>
    <span class="s1">check_classifiers_multilabel_output_format_predict_proba</span><span class="s2">,</span>
    <span class="s1">check_dataframe_column_names_consistency</span><span class="s2">,</span>
    <span class="s1">check_decision_proba_consistency</span><span class="s2">,</span>
    <span class="s1">check_estimator</span><span class="s2">,</span>
    <span class="s1">check_estimator_get_tags_default_keys</span><span class="s2">,</span>
    <span class="s1">check_estimators_unfitted</span><span class="s2">,</span>
    <span class="s1">check_fit_check_is_fitted</span><span class="s2">,</span>
    <span class="s1">check_fit_score_takes_y</span><span class="s2">,</span>
    <span class="s1">check_methods_sample_order_invariance</span><span class="s2">,</span>
    <span class="s1">check_methods_subset_invariance</span><span class="s2">,</span>
    <span class="s1">check_no_attributes_set_in_init</span><span class="s2">,</span>
    <span class="s1">check_outlier_contamination</span><span class="s2">,</span>
    <span class="s1">check_outlier_corruption</span><span class="s2">,</span>
    <span class="s1">check_regressor_data_not_an_array</span><span class="s2">,</span>
    <span class="s1">check_requires_y_none</span><span class="s2">,</span>
    <span class="s1">set_random_state</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.utils.metaestimators </span><span class="s2">import </span><span class="s1">available_if</span>
<span class="s2">from </span><span class="s1">sklearn.utils.validation </span><span class="s2">import </span><span class="s1">check_array</span><span class="s2">, </span><span class="s1">check_is_fitted</span><span class="s2">, </span><span class="s1">check_X_y</span>


<span class="s2">class </span><span class="s1">CorrectNotFittedError(ValueError):</span>
    <span class="s3">&quot;&quot;&quot;Exception class to raise if estimator is used before fitting. 
 
    Like NotFittedError, it inherits from ValueError, but not from 
    AttributeError. Used for testing only. 
    &quot;&quot;&quot;</span>


<span class="s2">class </span><span class="s1">BaseBadClassifier(ClassifierMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s2">return </span><span class="s1">np.ones(X.shape[</span><span class="s4">0</span><span class="s1">])</span>


<span class="s2">class </span><span class="s1">ChangesDict(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">key=</span><span class="s4">0</span><span class="s1">):</span>
        <span class="s1">self.key = key</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s1">X = check_array(X)</span>
        <span class="s1">self.key = </span><span class="s4">1000</span>
        <span class="s2">return </span><span class="s1">np.ones(X.shape[</span><span class="s4">0</span><span class="s1">])</span>


<span class="s2">class </span><span class="s1">SetsWrongAttribute(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">acceptable_key=</span><span class="s4">0</span><span class="s1">):</span>
        <span class="s1">self.acceptable_key = acceptable_key</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self.wrong_attribute = </span><span class="s4">0</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">return </span><span class="s1">self</span>


<span class="s2">class </span><span class="s1">ChangesWrongAttribute(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">wrong_attribute=</span><span class="s4">0</span><span class="s1">):</span>
        <span class="s1">self.wrong_attribute = wrong_attribute</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self.wrong_attribute = </span><span class="s4">1</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">return </span><span class="s1">self</span>


<span class="s2">class </span><span class="s1">ChangesUnderscoreAttribute(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self._good_attribute = </span><span class="s4">1</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">return </span><span class="s1">self</span>


<span class="s2">class </span><span class="s1">RaisesErrorInSetParams(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">p=</span><span class="s4">0</span><span class="s1">):</span>
        <span class="s1">self.p = p</span>

    <span class="s2">def </span><span class="s1">set_params(self</span><span class="s2">, </span><span class="s1">**kwargs):</span>
        <span class="s2">if </span><span class="s5">&quot;p&quot; </span><span class="s2">in </span><span class="s1">kwargs:</span>
            <span class="s1">p = kwargs.pop(</span><span class="s5">&quot;p&quot;</span><span class="s1">)</span>
            <span class="s2">if </span><span class="s1">p &lt; </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;p can't be less than 0&quot;</span><span class="s1">)</span>
            <span class="s1">self.p = p</span>
        <span class="s2">return </span><span class="s1">super().set_params(**kwargs)</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">return </span><span class="s1">self</span>


<span class="s2">class </span><span class="s1">HasMutableParameters(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">p=object()):</span>
        <span class="s1">self.p = p</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">return </span><span class="s1">self</span>


<span class="s2">class </span><span class="s1">HasImmutableParameters(BaseEstimator):</span>
    <span class="s0"># Note that object is an uninitialized class, thus immutable.</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">p=</span><span class="s4">42</span><span class="s2">, </span><span class="s1">q=np.int32(</span><span class="s4">42</span><span class="s1">)</span><span class="s2">, </span><span class="s1">r=object):</span>
        <span class="s1">self.p = p</span>
        <span class="s1">self.q = q</span>
        <span class="s1">self.r = r</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">return </span><span class="s1">self</span>


<span class="s2">class </span><span class="s1">ModifiesValueInsteadOfRaisingError(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">p=</span><span class="s4">0</span><span class="s1">):</span>
        <span class="s1">self.p = p</span>

    <span class="s2">def </span><span class="s1">set_params(self</span><span class="s2">, </span><span class="s1">**kwargs):</span>
        <span class="s2">if </span><span class="s5">&quot;p&quot; </span><span class="s2">in </span><span class="s1">kwargs:</span>
            <span class="s1">p = kwargs.pop(</span><span class="s5">&quot;p&quot;</span><span class="s1">)</span>
            <span class="s2">if </span><span class="s1">p &lt; </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s1">p = </span><span class="s4">0</span>
            <span class="s1">self.p = p</span>
        <span class="s2">return </span><span class="s1">super().set_params(**kwargs)</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">return </span><span class="s1">self</span>


<span class="s2">class </span><span class="s1">ModifiesAnotherValue(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">a=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">b=</span><span class="s5">&quot;method1&quot;</span><span class="s1">):</span>
        <span class="s1">self.a = a</span>
        <span class="s1">self.b = b</span>

    <span class="s2">def </span><span class="s1">set_params(self</span><span class="s2">, </span><span class="s1">**kwargs):</span>
        <span class="s2">if </span><span class="s5">&quot;a&quot; </span><span class="s2">in </span><span class="s1">kwargs:</span>
            <span class="s1">a = kwargs.pop(</span><span class="s5">&quot;a&quot;</span><span class="s1">)</span>
            <span class="s1">self.a = a</span>
            <span class="s2">if </span><span class="s1">a </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s1">kwargs.pop(</span><span class="s5">&quot;b&quot;</span><span class="s1">)</span>
                <span class="s1">self.b = </span><span class="s5">&quot;method2&quot;</span>
        <span class="s2">return </span><span class="s1">super().set_params(**kwargs)</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">return </span><span class="s1">self</span>


<span class="s2">class </span><span class="s1">NoCheckinPredict(BaseBadClassifier):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">return </span><span class="s1">self</span>


<span class="s2">class </span><span class="s1">NoSparseClassifier(BaseBadClassifier):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">accept_sparse=[</span><span class="s5">&quot;csr&quot;</span><span class="s2">, </span><span class="s5">&quot;csc&quot;</span><span class="s1">])</span>
        <span class="s2">if </span><span class="s1">sp.issparse(X):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Nonsensical Error&quot;</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s1">X = check_array(X)</span>
        <span class="s2">return </span><span class="s1">np.ones(X.shape[</span><span class="s4">0</span><span class="s1">])</span>


<span class="s2">class </span><span class="s1">CorrectNotFittedErrorClassifier(BaseBadClassifier):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">self.coef_ = np.ones(X.shape[</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">X = check_array(X)</span>
        <span class="s2">return </span><span class="s1">np.ones(X.shape[</span><span class="s4">0</span><span class="s1">])</span>


<span class="s2">class </span><span class="s1">NoSampleWeightPandasSeriesType(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0"># Convert data</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">accept_sparse=(</span><span class="s5">&quot;csr&quot;</span><span class="s2">, </span><span class="s5">&quot;csc&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">multi_output=</span><span class="s2">True, </span><span class="s1">y_numeric=</span><span class="s2">True</span>
        <span class="s1">)</span>
        <span class="s0"># Function is only called after we verify that pandas is installed</span>
        <span class="s2">from </span><span class="s1">pandas </span><span class="s2">import </span><span class="s1">Series</span>

        <span class="s2">if </span><span class="s1">isinstance(sample_weight</span><span class="s2">, </span><span class="s1">Series):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s5">&quot;Estimator does not accept 'sample_weight'of type pandas.Series&quot;</span>
            <span class="s1">)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s1">X = check_array(X)</span>
        <span class="s2">return </span><span class="s1">np.ones(X.shape[</span><span class="s4">0</span><span class="s1">])</span>


<span class="s2">class </span><span class="s1">BadBalancedWeightsClassifier(BaseBadClassifier):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">class_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self.class_weight = class_weight</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">LabelEncoder</span>
        <span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">compute_class_weight</span>

        <span class="s1">label_encoder = LabelEncoder().fit(y)</span>
        <span class="s1">classes = label_encoder.classes_</span>
        <span class="s1">class_weight = compute_class_weight(self.class_weight</span><span class="s2">, </span><span class="s1">classes=classes</span><span class="s2">, </span><span class="s1">y=y)</span>

        <span class="s0"># Intentionally modify the balanced class_weight</span>
        <span class="s0"># to simulate a bug and raise an exception</span>
        <span class="s2">if </span><span class="s1">self.class_weight == </span><span class="s5">&quot;balanced&quot;</span><span class="s1">:</span>
            <span class="s1">class_weight += </span><span class="s4">1.0</span>

        <span class="s0"># Simply assigning coef_ to the class_weight</span>
        <span class="s1">self.coef_ = class_weight</span>
        <span class="s2">return </span><span class="s1">self</span>


<span class="s2">class </span><span class="s1">BadTransformerWithoutMixin(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">X = self._validate_data(X)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">transform(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s1">X = check_array(X)</span>
        <span class="s2">return </span><span class="s1">X</span>


<span class="s2">class </span><span class="s1">NotInvariantPredict(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s0"># Convert data</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">accept_sparse=(</span><span class="s5">&quot;csr&quot;</span><span class="s2">, </span><span class="s5">&quot;csc&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">multi_output=</span><span class="s2">True, </span><span class="s1">y_numeric=</span><span class="s2">True</span>
        <span class="s1">)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s0"># return 1 if X has more than one element else return 0</span>
        <span class="s1">X = check_array(X)</span>
        <span class="s2">if </span><span class="s1">X.shape[</span><span class="s4">0</span><span class="s1">] &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">np.ones(X.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s2">return </span><span class="s1">np.zeros(X.shape[</span><span class="s4">0</span><span class="s1">])</span>


<span class="s2">class </span><span class="s1">NotInvariantSampleOrder(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">accept_sparse=(</span><span class="s5">&quot;csr&quot;</span><span class="s2">, </span><span class="s5">&quot;csc&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">multi_output=</span><span class="s2">True, </span><span class="s1">y_numeric=</span><span class="s2">True</span>
        <span class="s1">)</span>
        <span class="s0"># store the original X to check for sample order later</span>
        <span class="s1">self._X = X</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s1">X = check_array(X)</span>
        <span class="s0"># if the input contains the same elements but different sample order,</span>
        <span class="s0"># then just return zeros.</span>
        <span class="s2">if </span><span class="s1">(</span>
            <span class="s1">np.array_equiv(np.sort(X</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.sort(self._X</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">))</span>
            <span class="s2">and </span><span class="s1">(X != self._X).any()</span>
        <span class="s1">):</span>
            <span class="s2">return </span><span class="s1">np.zeros(X.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s2">return </span><span class="s1">X[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>


<span class="s2">class </span><span class="s1">OneClassSampleErrorClassifier(BaseBadClassifier):</span>
    <span class="s3">&quot;&quot;&quot;Classifier allowing to trigger different behaviors when `sample_weight` reduces 
    the number of classes to 1.&quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">raise_when_single_class=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s1">self.raise_when_single_class = raise_when_single_class</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = check_X_y(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">accept_sparse=(</span><span class="s5">&quot;csr&quot;</span><span class="s2">, </span><span class="s5">&quot;csc&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">multi_output=</span><span class="s2">True, </span><span class="s1">y_numeric=</span><span class="s2">True</span>
        <span class="s1">)</span>

        <span class="s1">self.has_single_class_ = </span><span class="s2">False</span>
        <span class="s1">self.classes_</span><span class="s2">, </span><span class="s1">y = np.unique(y</span><span class="s2">, </span><span class="s1">return_inverse=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">n_classes_ = self.classes_.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s2">if </span><span class="s1">n_classes_ &lt; </span><span class="s4">2 </span><span class="s2">and </span><span class="s1">self.raise_when_single_class:</span>
            <span class="s1">self.has_single_class_ = </span><span class="s2">True</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;normal class error&quot;</span><span class="s1">)</span>

        <span class="s0"># find the number of class after trimming</span>
        <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">isinstance(sample_weight</span><span class="s2">, </span><span class="s1">np.ndarray) </span><span class="s2">and </span><span class="s1">len(sample_weight) &gt; </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s1">n_classes_ = np.count_nonzero(np.bincount(y</span><span class="s2">, </span><span class="s1">sample_weight))</span>
            <span class="s2">if </span><span class="s1">n_classes_ &lt; </span><span class="s4">2</span><span class="s1">:</span>
                <span class="s1">self.has_single_class_ = </span><span class="s2">True</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Nonsensical Error&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">X = check_array(X)</span>
        <span class="s2">if </span><span class="s1">self.has_single_class_:</span>
            <span class="s2">return </span><span class="s1">np.zeros(X.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s2">return </span><span class="s1">np.ones(X.shape[</span><span class="s4">0</span><span class="s1">])</span>


<span class="s2">class </span><span class="s1">LargeSparseNotSupportedClassifier(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">y</span><span class="s2">,</span>
            <span class="s1">accept_sparse=(</span><span class="s5">&quot;csr&quot;</span><span class="s2">, </span><span class="s5">&quot;csc&quot;</span><span class="s2">, </span><span class="s5">&quot;coo&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">accept_large_sparse=</span><span class="s2">True,</span>
            <span class="s1">multi_output=</span><span class="s2">True,</span>
            <span class="s1">y_numeric=</span><span class="s2">True,</span>
        <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">sp.issparse(X):</span>
            <span class="s2">if </span><span class="s1">X.getformat() == </span><span class="s5">&quot;coo&quot;</span><span class="s1">:</span>
                <span class="s2">if </span><span class="s1">X.row.dtype == </span><span class="s5">&quot;int64&quot; </span><span class="s2">or </span><span class="s1">X.col.dtype == </span><span class="s5">&quot;int64&quot;</span><span class="s1">:</span>
                    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Estimator doesn't support 64-bit indices&quot;</span><span class="s1">)</span>
            <span class="s2">elif </span><span class="s1">X.getformat() </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;csc&quot;</span><span class="s2">, </span><span class="s5">&quot;csr&quot;</span><span class="s1">]:</span>
                <span class="s2">assert </span><span class="s5">&quot;int64&quot; </span><span class="s2">not in </span><span class="s1">(</span>
                    <span class="s1">X.indices.dtype</span><span class="s2">,</span>
                    <span class="s1">X.indptr.dtype</span><span class="s2">,</span>
                <span class="s1">)</span><span class="s2">, </span><span class="s5">&quot;Estimator doesn't support 64-bit indices&quot;</span>

        <span class="s2">return </span><span class="s1">self</span>


<span class="s2">class </span><span class="s1">SparseTransformer(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self.X_shape_ = self._validate_data(X).shape</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">fit_transform(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">self.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>

    <span class="s2">def </span><span class="s1">transform(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s1">X = check_array(X)</span>
        <span class="s2">if </span><span class="s1">X.shape[</span><span class="s4">1</span><span class="s1">] != self.X_shape_[</span><span class="s4">1</span><span class="s1">]:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Bad number of features&quot;</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">sp.csr_matrix(X)</span>


<span class="s2">class </span><span class="s1">EstimatorInconsistentForPandas(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">try</span><span class="s1">:</span>
            <span class="s2">from </span><span class="s1">pandas </span><span class="s2">import </span><span class="s1">DataFrame</span>

            <span class="s2">if </span><span class="s1">isinstance(X</span><span class="s2">, </span><span class="s1">DataFrame):</span>
                <span class="s1">self.value_ = X.iloc[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">X = check_array(X)</span>
                <span class="s1">self.value_ = X[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>
            <span class="s2">return </span><span class="s1">self</span>

        <span class="s2">except </span><span class="s1">ImportError:</span>
            <span class="s1">X = check_array(X)</span>
            <span class="s1">self.value_ = X[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>
            <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s1">X = check_array(X)</span>
        <span class="s2">return </span><span class="s1">np.array([self.value_] * X.shape[</span><span class="s4">0</span><span class="s1">])</span>


<span class="s2">class </span><span class="s1">UntaggedBinaryClassifier(SGDClassifier):</span>
    <span class="s0"># Toy classifier that only supports binary classification, will fail tests.</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">coef_init=</span><span class="s2">None, </span><span class="s1">intercept_init=</span><span class="s2">None, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">coef_init</span><span class="s2">, </span><span class="s1">intercept_init</span><span class="s2">, </span><span class="s1">sample_weight)</span>
        <span class="s2">if </span><span class="s1">len(self.classes_) &gt; </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Only 2 classes are supported&quot;</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">partial_fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">classes=</span><span class="s2">None, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().partial_fit(X=X</span><span class="s2">, </span><span class="s1">y=y</span><span class="s2">, </span><span class="s1">classes=classes</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
        <span class="s2">if </span><span class="s1">len(self.classes_) &gt; </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Only 2 classes are supported&quot;</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">self</span>


<span class="s2">class </span><span class="s1">TaggedBinaryClassifier(UntaggedBinaryClassifier):</span>
    <span class="s0"># Toy classifier that only supports binary classification.</span>
    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s5">&quot;binary_only&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>


<span class="s2">class </span><span class="s1">EstimatorMissingDefaultTags(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">_get_tags(self):</span>
        <span class="s1">tags = super()._get_tags().copy()</span>
        <span class="s2">del </span><span class="s1">tags[</span><span class="s5">&quot;allow_nan&quot;</span><span class="s1">]</span>
        <span class="s2">return </span><span class="s1">tags</span>


<span class="s2">class </span><span class="s1">RequiresPositiveXRegressor(LinearRegression):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">multi_output=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">(X &lt; </span><span class="s4">0</span><span class="s1">).any():</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;negative X values not supported!&quot;</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">super().fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s5">&quot;requires_positive_X&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>


<span class="s2">class </span><span class="s1">RequiresPositiveYRegressor(LinearRegression):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">multi_output=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">(y &lt;= </span><span class="s4">0</span><span class="s1">).any():</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;negative y values not supported!&quot;</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">super().fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s5">&quot;requires_positive_y&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>


<span class="s2">class </span><span class="s1">PoorScoreLogisticRegression(LogisticRegression):</span>
    <span class="s2">def </span><span class="s1">decision_function(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s2">return </span><span class="s1">super().decision_function(X) + </span><span class="s4">1</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s5">&quot;poor_score&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>


<span class="s2">class </span><span class="s1">PartialFitChecksName(BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s1">self._validate_data(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">partial_fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s1">reset = </span><span class="s2">not </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s5">&quot;_fitted&quot;</span><span class="s1">)</span>
        <span class="s1">self._validate_data(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">reset=reset)</span>
        <span class="s1">self._fitted = </span><span class="s2">True</span>
        <span class="s2">return </span><span class="s1">self</span>


<span class="s2">class </span><span class="s1">BrokenArrayAPI(BaseEstimator):</span>
    <span class="s3">&quot;&quot;&quot;Make different predictions when using Numpy and the Array API&quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s1">enabled = get_config()[</span><span class="s5">&quot;array_api_dispatch&quot;</span><span class="s1">]</span>
        <span class="s1">xp</span><span class="s2">, </span><span class="s1">_ = _array_api.get_namespace(X)</span>
        <span class="s2">if </span><span class="s1">enabled:</span>
            <span class="s2">return </span><span class="s1">xp.asarray([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">np.array([</span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_check_array_api_input():</span>
    <span class="s2">try</span><span class="s1">:</span>
        <span class="s1">importlib.import_module(</span><span class="s5">&quot;array_api_compat&quot;</span><span class="s1">)</span>
    <span class="s2">except </span><span class="s1">ModuleNotFoundError:</span>
        <span class="s2">raise </span><span class="s1">SkipTest(</span><span class="s5">&quot;array_api_compat is required to run this test&quot;</span><span class="s1">)</span>
    <span class="s2">try</span><span class="s1">:</span>
        <span class="s1">importlib.import_module(</span><span class="s5">&quot;numpy.array_api&quot;</span><span class="s1">)</span>
    <span class="s2">except </span><span class="s1">ModuleNotFoundError:  </span><span class="s0"># pragma: nocover</span>
        <span class="s2">raise </span><span class="s1">SkipTest(</span><span class="s5">&quot;numpy.array_api is required to run this test&quot;</span><span class="s1">)</span>

    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;Not equal to tolerance&quot;</span><span class="s1">):</span>
        <span class="s1">check_array_api_input(</span>
            <span class="s5">&quot;BrokenArrayAPI&quot;</span><span class="s2">, </span><span class="s1">BrokenArrayAPI()</span><span class="s2">, </span><span class="s1">array_namespace=</span><span class="s5">&quot;numpy.array_api&quot;</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_not_an_array_array_function():</span>
    <span class="s1">not_array = _NotAnArray(np.ones(</span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">msg = </span><span class="s5">&quot;Don't want to call array_function sum!&quot;</span>
    <span class="s2">with </span><span class="s1">raises(TypeError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">np.sum(not_array)</span>
    <span class="s0"># always returns True</span>
    <span class="s2">assert </span><span class="s1">np.may_share_memory(not_array</span><span class="s2">, None</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_check_fit_score_takes_y_works_on_deprecated_fit():</span>
    <span class="s0"># Tests that check_fit_score_takes_y works on a class with</span>
    <span class="s0"># a deprecated fit method</span>

    <span class="s2">class </span><span class="s1">TestEstimatorWithDeprecatedFitMethod(BaseEstimator):</span>
        <span class="s1">@deprecated(</span><span class="s5">&quot;Deprecated for the purpose of testing check_fit_score_takes_y&quot;</span><span class="s1">)</span>
        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
            <span class="s2">return </span><span class="s1">self</span>

    <span class="s1">check_fit_score_takes_y(</span><span class="s5">&quot;test&quot;</span><span class="s2">, </span><span class="s1">TestEstimatorWithDeprecatedFitMethod())</span>


<span class="s2">def </span><span class="s1">test_check_estimator():</span>
    <span class="s0"># tests that the estimator actually fails on &quot;bad&quot; estimators.</span>
    <span class="s0"># not a complete test of all checks, which are very extensive.</span>

    <span class="s0"># check that we have a set_params and can clone</span>
    <span class="s1">msg = </span><span class="s5">&quot;Passing a class was deprecated&quot;</span>
    <span class="s2">with </span><span class="s1">raises(TypeError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(object)</span>
    <span class="s1">msg = (</span>
        <span class="s5">&quot;Parameter 'p' of estimator 'HasMutableParameters' is of type &quot;</span>
        <span class="s5">&quot;object which is not allowed&quot;</span>
    <span class="s1">)</span>
    <span class="s0"># check that the &quot;default_constructible&quot; test checks for mutable parameters</span>
    <span class="s1">check_estimator(HasImmutableParameters())  </span><span class="s0"># should pass</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(HasMutableParameters())</span>
    <span class="s0"># check that values returned by get_params match set_params</span>
    <span class="s1">msg = </span><span class="s5">&quot;get_params result does not match what was passed to set_params&quot;</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(ModifiesValueInsteadOfRaisingError())</span>
    <span class="s2">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s2">True</span><span class="s1">) </span><span class="s2">as </span><span class="s1">records:</span>
        <span class="s1">check_estimator(RaisesErrorInSetParams())</span>
    <span class="s2">assert </span><span class="s1">UserWarning </span><span class="s2">in </span><span class="s1">[rec.category </span><span class="s2">for </span><span class="s1">rec </span><span class="s2">in </span><span class="s1">records]</span>

    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(ModifiesAnotherValue())</span>
    <span class="s0"># check that we have a fit method</span>
    <span class="s1">msg = </span><span class="s5">&quot;object has no attribute 'fit'&quot;</span>
    <span class="s2">with </span><span class="s1">raises(AttributeError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(BaseEstimator())</span>
    <span class="s0"># check that fit does input validation</span>
    <span class="s1">msg = </span><span class="s5">&quot;Did not raise&quot;</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(BaseBadClassifier())</span>
    <span class="s0"># check that sample_weights in fit accepts pandas.Series type</span>
    <span class="s2">try</span><span class="s1">:</span>
        <span class="s2">from </span><span class="s1">pandas </span><span class="s2">import </span><span class="s1">Series  </span><span class="s0"># noqa</span>

        <span class="s1">msg = (</span>
            <span class="s5">&quot;Estimator NoSampleWeightPandasSeriesType raises error if &quot;</span>
            <span class="s5">&quot;'sample_weight' parameter is of type pandas.Series&quot;</span>
        <span class="s1">)</span>
        <span class="s2">with </span><span class="s1">raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
            <span class="s1">check_estimator(NoSampleWeightPandasSeriesType())</span>
    <span class="s2">except </span><span class="s1">ImportError:</span>
        <span class="s2">pass</span>
    <span class="s0"># check that predict does input validation (doesn't accept dicts in input)</span>
    <span class="s1">msg = </span><span class="s5">&quot;Estimator NoCheckinPredict doesn't check for NaN and inf in predict&quot;</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(NoCheckinPredict())</span>
    <span class="s0"># check that estimator state does not change</span>
    <span class="s0"># at transform/predict/predict_proba time</span>
    <span class="s1">msg = </span><span class="s5">&quot;Estimator changes __dict__ during predict&quot;</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(ChangesDict())</span>
    <span class="s0"># check that `fit` only changes attributes that</span>
    <span class="s0"># are private (start with an _ or end with a _).</span>
    <span class="s1">msg = (</span>
        <span class="s5">&quot;Estimator ChangesWrongAttribute should not change or mutate  &quot;</span>
        <span class="s5">&quot;the parameter wrong_attribute from 0 to 1 during fit.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(ChangesWrongAttribute())</span>
    <span class="s1">check_estimator(ChangesUnderscoreAttribute())</span>
    <span class="s0"># check that `fit` doesn't add any public attribute</span>
    <span class="s1">msg = (</span>
        <span class="s5">r&quot;Estimator adds public attribute\(s\) during the fit method.&quot;</span>
        <span class="s5">&quot; Estimators are only allowed to add private attributes&quot;</span>
        <span class="s5">&quot; either started with _ or ended&quot;</span>
        <span class="s5">&quot; with _ but wrong_attribute added&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(SetsWrongAttribute())</span>
    <span class="s0"># check for sample order invariance</span>
    <span class="s1">name = NotInvariantSampleOrder.__name__</span>
    <span class="s1">method = </span><span class="s5">&quot;predict&quot;</span>
    <span class="s1">msg = (</span>
        <span class="s5">&quot;{method} of {name} is not invariant when applied to a dataset&quot;</span>
        <span class="s5">&quot;with different sample order.&quot;</span>
    <span class="s1">).format(method=method</span><span class="s2">, </span><span class="s1">name=name)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(NotInvariantSampleOrder())</span>
    <span class="s0"># check for invariant method</span>
    <span class="s1">name = NotInvariantPredict.__name__</span>
    <span class="s1">method = </span><span class="s5">&quot;predict&quot;</span>
    <span class="s1">msg = (</span><span class="s5">&quot;{method} of {name} is not invariant when applied to a subset.&quot;</span><span class="s1">).format(</span>
        <span class="s1">method=method</span><span class="s2">, </span><span class="s1">name=name</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(NotInvariantPredict())</span>
    <span class="s0"># check for sparse matrix input handling</span>
    <span class="s1">name = NoSparseClassifier.__name__</span>
    <span class="s1">msg = </span><span class="s5">&quot;Estimator %s doesn't seem to fail gracefully on sparse data&quot; </span><span class="s1">% name</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(NoSparseClassifier())</span>

    <span class="s0"># check for classifiers reducing to less than two classes via sample weights</span>
    <span class="s1">name = OneClassSampleErrorClassifier.__name__</span>
    <span class="s1">msg = (</span>
        <span class="s5">f&quot;</span><span class="s2">{</span><span class="s1">name</span><span class="s2">} </span><span class="s5">failed when fitted on one label after sample_weight &quot;</span>
        <span class="s5">&quot;trimming. Error message is not explicit, it should have &quot;</span>
        <span class="s5">&quot;'class'.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(OneClassSampleErrorClassifier())</span>

    <span class="s0"># Large indices test on bad estimator</span>
    <span class="s1">msg = (</span>
        <span class="s5">&quot;Estimator LargeSparseNotSupportedClassifier doesn't seem to &quot;</span>
        <span class="s5">r&quot;support \S{3}_64 matrix, and is not failing gracefully.*&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(LargeSparseNotSupportedClassifier())</span>

    <span class="s0"># does error on binary_only untagged estimator</span>
    <span class="s1">msg = </span><span class="s5">&quot;Only 2 classes are supported&quot;</span>
    <span class="s2">with </span><span class="s1">raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(UntaggedBinaryClassifier())</span>

    <span class="s0"># non-regression test for estimators transforming to sparse data</span>
    <span class="s1">check_estimator(SparseTransformer())</span>

    <span class="s0"># doesn't error on actual estimator</span>
    <span class="s1">check_estimator(LogisticRegression())</span>
    <span class="s1">check_estimator(LogisticRegression(C=</span><span class="s4">0.01</span><span class="s1">))</span>
    <span class="s1">check_estimator(MultiTaskElasticNet())</span>

    <span class="s0"># doesn't error on binary_only tagged estimator</span>
    <span class="s1">check_estimator(TaggedBinaryClassifier())</span>
    <span class="s1">check_estimator(RequiresPositiveXRegressor())</span>

    <span class="s0"># Check regressor with requires_positive_y estimator tag</span>
    <span class="s1">msg = </span><span class="s5">&quot;negative y values not supported!&quot;</span>
    <span class="s2">with </span><span class="s1">raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimator(RequiresPositiveYRegressor())</span>

    <span class="s0"># Does not raise error on classifier with poor_score tag</span>
    <span class="s1">check_estimator(PoorScoreLogisticRegression())</span>


<span class="s2">def </span><span class="s1">test_check_outlier_corruption():</span>
    <span class="s0"># should raise AssertionError</span>
    <span class="s1">decision = np.array([</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">1.5</span><span class="s2">, </span><span class="s4">2.0</span><span class="s1">])</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError):</span>
        <span class="s1">check_outlier_corruption(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s1">decision)</span>
    <span class="s0"># should pass</span>
    <span class="s1">decision = np.array([</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">2.0</span><span class="s1">])</span>
    <span class="s1">check_outlier_corruption(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s1">decision)</span>


<span class="s2">def </span><span class="s1">test_check_estimator_transformer_no_mixin():</span>
    <span class="s0"># check that TransformerMixin is not required for transformer tests to run</span>
    <span class="s2">with </span><span class="s1">raises(AttributeError</span><span class="s2">, </span><span class="s5">&quot;.*fit_transform.*&quot;</span><span class="s1">):</span>
        <span class="s1">check_estimator(BadTransformerWithoutMixin())</span>


<span class="s2">def </span><span class="s1">test_check_estimator_clones():</span>
    <span class="s0"># check that check_estimator doesn't modify the estimator it receives</span>
    <span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">load_iris</span>

    <span class="s1">iris = load_iris()</span>

    <span class="s2">for </span><span class="s1">Estimator </span><span class="s2">in </span><span class="s1">[</span>
        <span class="s1">GaussianMixture</span><span class="s2">,</span>
        <span class="s1">LinearRegression</span><span class="s2">,</span>
        <span class="s1">SGDClassifier</span><span class="s2">,</span>
        <span class="s1">PCA</span><span class="s2">,</span>
        <span class="s1">ExtraTreesClassifier</span><span class="s2">,</span>
        <span class="s1">MiniBatchKMeans</span><span class="s2">,</span>
    <span class="s1">]:</span>
        <span class="s0"># without fitting</span>
        <span class="s2">with </span><span class="s1">ignore_warnings(category=ConvergenceWarning):</span>
            <span class="s1">est = Estimator()</span>
            <span class="s1">_set_checking_parameters(est)</span>
            <span class="s1">set_random_state(est)</span>
            <span class="s1">old_hash = joblib.hash(est)</span>
            <span class="s1">check_estimator(est)</span>
        <span class="s2">assert </span><span class="s1">old_hash == joblib.hash(est)</span>

        <span class="s0"># with fitting</span>
        <span class="s2">with </span><span class="s1">ignore_warnings(category=ConvergenceWarning):</span>
            <span class="s1">est = Estimator()</span>
            <span class="s1">_set_checking_parameters(est)</span>
            <span class="s1">set_random_state(est)</span>
            <span class="s1">est.fit(iris.data + </span><span class="s4">10</span><span class="s2">, </span><span class="s1">iris.target)</span>
            <span class="s1">old_hash = joblib.hash(est)</span>
            <span class="s1">check_estimator(est)</span>
        <span class="s2">assert </span><span class="s1">old_hash == joblib.hash(est)</span>


<span class="s2">def </span><span class="s1">test_check_estimators_unfitted():</span>
    <span class="s0"># check that a ValueError/AttributeError is raised when calling predict</span>
    <span class="s0"># on an unfitted estimator</span>
    <span class="s1">msg = </span><span class="s5">&quot;Did not raise&quot;</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_estimators_unfitted(</span><span class="s5">&quot;estimator&quot;</span><span class="s2">, </span><span class="s1">NoSparseClassifier())</span>

    <span class="s0"># check that CorrectNotFittedError inherit from either ValueError</span>
    <span class="s0"># or AttributeError</span>
    <span class="s1">check_estimators_unfitted(</span><span class="s5">&quot;estimator&quot;</span><span class="s2">, </span><span class="s1">CorrectNotFittedErrorClassifier())</span>


<span class="s2">def </span><span class="s1">test_check_no_attributes_set_in_init():</span>
    <span class="s2">class </span><span class="s1">NonConformantEstimatorPrivateSet(BaseEstimator):</span>
        <span class="s2">def </span><span class="s1">__init__(self):</span>
            <span class="s1">self.you_should_not_set_this_ = </span><span class="s2">None</span>

    <span class="s2">class </span><span class="s1">NonConformantEstimatorNoParamSet(BaseEstimator):</span>
        <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">you_should_set_this_=</span><span class="s2">None</span><span class="s1">):</span>
            <span class="s2">pass</span>

    <span class="s2">class </span><span class="s1">ConformantEstimatorClassAttribute(BaseEstimator):</span>
        <span class="s0"># making sure our __metadata_request__* class attributes are okay!</span>
        <span class="s1">__metadata_request__fit = {</span><span class="s5">&quot;foo&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>

    <span class="s1">msg = (</span>
        <span class="s5">&quot;Estimator estimator_name should not set any&quot;</span>
        <span class="s5">&quot; attribute apart from parameters during init.&quot;</span>
        <span class="s5">r&quot; Found attributes \['you_should_not_set_this_'\].&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_no_attributes_set_in_init(</span>
            <span class="s5">&quot;estimator_name&quot;</span><span class="s2">, </span><span class="s1">NonConformantEstimatorPrivateSet()</span>
        <span class="s1">)</span>

    <span class="s1">msg = (</span>
        <span class="s5">&quot;Estimator estimator_name should store all parameters as an attribute&quot;</span>
        <span class="s5">&quot; during init&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AttributeError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_no_attributes_set_in_init(</span>
            <span class="s5">&quot;estimator_name&quot;</span><span class="s2">, </span><span class="s1">NonConformantEstimatorNoParamSet()</span>
        <span class="s1">)</span>

    <span class="s0"># a private class attribute is okay!</span>
    <span class="s1">check_no_attributes_set_in_init(</span>
        <span class="s5">&quot;estimator_name&quot;</span><span class="s2">, </span><span class="s1">ConformantEstimatorClassAttribute()</span>
    <span class="s1">)</span>
    <span class="s0"># also check if cloning an estimator which has non-default set requests is</span>
    <span class="s0"># fine. Setting a non-default value via `set_{method}_request` sets the</span>
    <span class="s0"># private _metadata_request instance attribute which is copied in `clone`.</span>
    <span class="s2">with </span><span class="s1">config_context(enable_metadata_routing=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s1">check_no_attributes_set_in_init(</span>
            <span class="s5">&quot;estimator_name&quot;</span><span class="s2">,</span>
            <span class="s1">ConformantEstimatorClassAttribute().set_fit_request(foo=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_check_estimator_pairwise():</span>
    <span class="s0"># check that check_estimator() works on estimator with _pairwise</span>
    <span class="s0"># kernel or metric</span>

    <span class="s0"># test precomputed kernel</span>
    <span class="s1">est = SVC(kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s1">)</span>
    <span class="s1">check_estimator(est)</span>

    <span class="s0"># test precomputed metric</span>
    <span class="s1">est = KNeighborsRegressor(metric=</span><span class="s5">&quot;precomputed&quot;</span><span class="s1">)</span>
    <span class="s1">check_estimator(est)</span>


<span class="s2">def </span><span class="s1">test_check_classifier_data_not_an_array():</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;Not equal to tolerance&quot;</span><span class="s1">):</span>
        <span class="s1">check_classifier_data_not_an_array(</span>
            <span class="s5">&quot;estimator_name&quot;</span><span class="s2">, </span><span class="s1">EstimatorInconsistentForPandas()</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_check_regressor_data_not_an_array():</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;Not equal to tolerance&quot;</span><span class="s1">):</span>
        <span class="s1">check_regressor_data_not_an_array(</span>
            <span class="s5">&quot;estimator_name&quot;</span><span class="s2">, </span><span class="s1">EstimatorInconsistentForPandas()</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_check_estimator_get_tags_default_keys():</span>
    <span class="s1">estimator = EstimatorMissingDefaultTags()</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;EstimatorMissingDefaultTags._get_tags\(\) is missing entries&quot;</span>
        <span class="s5">r&quot; for the following default tags: {'allow_nan'}&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_estimator_get_tags_default_keys(estimator.__class__.__name__</span><span class="s2">, </span><span class="s1">estimator)</span>

    <span class="s0"># noop check when _get_tags is not available</span>
    <span class="s1">estimator = MinimalTransformer()</span>
    <span class="s1">check_estimator_get_tags_default_keys(estimator.__class__.__name__</span><span class="s2">, </span><span class="s1">estimator)</span>


<span class="s2">def </span><span class="s1">test_check_dataframe_column_names_consistency():</span>
    <span class="s1">err_msg = </span><span class="s5">&quot;Estimator does not have a feature_names_in_&quot;</span>
    <span class="s2">with </span><span class="s1">raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_dataframe_column_names_consistency(</span><span class="s5">&quot;estimator_name&quot;</span><span class="s2">, </span><span class="s1">BaseBadClassifier())</span>
    <span class="s1">check_dataframe_column_names_consistency(</span><span class="s5">&quot;estimator_name&quot;</span><span class="s2">, </span><span class="s1">PartialFitChecksName())</span>

    <span class="s1">lr = LogisticRegression()</span>
    <span class="s1">check_dataframe_column_names_consistency(lr.__class__.__name__</span><span class="s2">, </span><span class="s1">lr)</span>
    <span class="s1">lr.__doc__ = </span><span class="s5">&quot;Docstring that does not document the estimator's attributes&quot;</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">&quot;Estimator LogisticRegression does not document its feature_names_in_ attribute&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_dataframe_column_names_consistency(lr.__class__.__name__</span><span class="s2">, </span><span class="s1">lr)</span>


<span class="s2">class </span><span class="s1">_BaseMultiLabelClassifierMock(ClassifierMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">response_output):</span>
        <span class="s1">self.response_output = response_output</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s5">&quot;multilabel&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>


<span class="s2">def </span><span class="s1">test_check_classifiers_multilabel_output_format_predict():</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">test_size</span><span class="s2">, </span><span class="s1">n_outputs = </span><span class="s4">100</span><span class="s2">, </span><span class="s4">25</span><span class="s2">, </span><span class="s4">5</span>
    <span class="s1">_</span><span class="s2">, </span><span class="s1">y = make_multilabel_classification(</span>
        <span class="s1">n_samples=n_samples</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">2</span><span class="s2">,</span>
        <span class="s1">n_classes=n_outputs</span><span class="s2">,</span>
        <span class="s1">n_labels=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">length=</span><span class="s4">50</span><span class="s2">,</span>
        <span class="s1">allow_unlabeled=</span><span class="s2">True,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">y_test = y[-test_size:]</span>

    <span class="s2">class </span><span class="s1">MultiLabelClassifierPredict(_BaseMultiLabelClassifierMock):</span>
        <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
            <span class="s2">return </span><span class="s1">self.response_output</span>

    <span class="s0"># 1. inconsistent array type</span>
    <span class="s1">clf = MultiLabelClassifierPredict(response_output=y_test.tolist())</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;MultiLabelClassifierPredict.predict is expected to output a &quot;</span>
        <span class="s5">r&quot;NumPy array. Got &lt;class 'list'&gt; instead.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_classifiers_multilabel_output_format_predict(clf.__class__.__name__</span><span class="s2">, </span><span class="s1">clf)</span>
    <span class="s0"># 2. inconsistent shape</span>
    <span class="s1">clf = MultiLabelClassifierPredict(response_output=y_test[:</span><span class="s2">, </span><span class="s1">:-</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;MultiLabelClassifierPredict.predict outputs a NumPy array of &quot;</span>
        <span class="s5">r&quot;shape \(25, 4\) instead of \(25, 5\).&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_classifiers_multilabel_output_format_predict(clf.__class__.__name__</span><span class="s2">, </span><span class="s1">clf)</span>
    <span class="s0"># 3. inconsistent dtype</span>
    <span class="s1">clf = MultiLabelClassifierPredict(response_output=y_test.astype(np.float64))</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;MultiLabelClassifierPredict.predict does not output the same &quot;</span>
        <span class="s5">r&quot;dtype than the targets.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_classifiers_multilabel_output_format_predict(clf.__class__.__name__</span><span class="s2">, </span><span class="s1">clf)</span>


<span class="s2">def </span><span class="s1">test_check_classifiers_multilabel_output_format_predict_proba():</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">test_size</span><span class="s2">, </span><span class="s1">n_outputs = </span><span class="s4">100</span><span class="s2">, </span><span class="s4">25</span><span class="s2">, </span><span class="s4">5</span>
    <span class="s1">_</span><span class="s2">, </span><span class="s1">y = make_multilabel_classification(</span>
        <span class="s1">n_samples=n_samples</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">2</span><span class="s2">,</span>
        <span class="s1">n_classes=n_outputs</span><span class="s2">,</span>
        <span class="s1">n_labels=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">length=</span><span class="s4">50</span><span class="s2">,</span>
        <span class="s1">allow_unlabeled=</span><span class="s2">True,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">y_test = y[-test_size:]</span>

    <span class="s2">class </span><span class="s1">MultiLabelClassifierPredictProba(_BaseMultiLabelClassifierMock):</span>
        <span class="s2">def </span><span class="s1">predict_proba(self</span><span class="s2">, </span><span class="s1">X):</span>
            <span class="s2">return </span><span class="s1">self.response_output</span>

    <span class="s0"># 1. unknown output type</span>
    <span class="s1">clf = MultiLabelClassifierPredictProba(response_output=sp.csr_matrix(y_test))</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">&quot;Unknown returned type .*csr_matrix.* by &quot;</span>
        <span class="s5">r&quot;MultiLabelClassifierPredictProba.predict_proba. A list or a Numpy &quot;</span>
        <span class="s5">r&quot;array is expected.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_classifiers_multilabel_output_format_predict_proba(</span>
            <span class="s1">clf.__class__.__name__</span><span class="s2">,</span>
            <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">)</span>
    <span class="s0"># 2. for list output</span>
    <span class="s0"># 2.1. inconsistent length</span>
    <span class="s1">clf = MultiLabelClassifierPredictProba(response_output=y_test.tolist())</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">&quot;When MultiLabelClassifierPredictProba.predict_proba returns a list, &quot;</span>
        <span class="s5">&quot;the list should be of length n_outputs and contain NumPy arrays. Got &quot;</span>
        <span class="s5">f&quot;length of </span><span class="s2">{</span><span class="s1">test_size</span><span class="s2">} </span><span class="s5">instead of </span><span class="s2">{</span><span class="s1">n_outputs</span><span class="s2">}</span><span class="s5">.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_classifiers_multilabel_output_format_predict_proba(</span>
            <span class="s1">clf.__class__.__name__</span><span class="s2">,</span>
            <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">)</span>
    <span class="s0"># 2.2. array of inconsistent shape</span>
    <span class="s1">response_output = [np.ones_like(y_test) </span><span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(n_outputs)]</span>
    <span class="s1">clf = MultiLabelClassifierPredictProba(response_output=response_output)</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;When MultiLabelClassifierPredictProba.predict_proba returns a list, &quot;</span>
        <span class="s5">r&quot;this list should contain NumPy arrays of shape \(n_samples, 2\). Got &quot;</span>
        <span class="s5">r&quot;NumPy arrays of shape \(25, 5\) instead of \(25, 2\).&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_classifiers_multilabel_output_format_predict_proba(</span>
            <span class="s1">clf.__class__.__name__</span><span class="s2">,</span>
            <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">)</span>
    <span class="s0"># 2.3. array of inconsistent dtype</span>
    <span class="s1">response_output = [</span>
        <span class="s1">np.ones(shape=(y_test.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype=np.int64) </span><span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(n_outputs)</span>
    <span class="s1">]</span>
    <span class="s1">clf = MultiLabelClassifierPredictProba(response_output=response_output)</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">&quot;When MultiLabelClassifierPredictProba.predict_proba returns a list, &quot;</span>
        <span class="s5">&quot;it should contain NumPy arrays with floating dtype.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_classifiers_multilabel_output_format_predict_proba(</span>
            <span class="s1">clf.__class__.__name__</span><span class="s2">,</span>
            <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">)</span>
    <span class="s0"># 2.4. array does not contain probability (each row should sum to 1)</span>
    <span class="s1">response_output = [</span>
        <span class="s1">np.ones(shape=(y_test.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype=np.float64) </span><span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(n_outputs)</span>
    <span class="s1">]</span>
    <span class="s1">clf = MultiLabelClassifierPredictProba(response_output=response_output)</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;When MultiLabelClassifierPredictProba.predict_proba returns a list, &quot;</span>
        <span class="s5">r&quot;each NumPy array should contain probabilities for each class and &quot;</span>
        <span class="s5">r&quot;thus each row should sum to 1&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_classifiers_multilabel_output_format_predict_proba(</span>
            <span class="s1">clf.__class__.__name__</span><span class="s2">,</span>
            <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">)</span>
    <span class="s0"># 3 for array output</span>
    <span class="s0"># 3.1. array of inconsistent shape</span>
    <span class="s1">clf = MultiLabelClassifierPredictProba(response_output=y_test[:</span><span class="s2">, </span><span class="s1">:-</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;When MultiLabelClassifierPredictProba.predict_proba returns a NumPy &quot;</span>
        <span class="s5">r&quot;array, the expected shape is \(n_samples, n_outputs\). Got \(25, 4\)&quot;</span>
        <span class="s5">r&quot; instead of \(25, 5\).&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_classifiers_multilabel_output_format_predict_proba(</span>
            <span class="s1">clf.__class__.__name__</span><span class="s2">,</span>
            <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">)</span>
    <span class="s0"># 3.2. array of inconsistent dtype</span>
    <span class="s1">response_output = np.zeros_like(y_test</span><span class="s2">, </span><span class="s1">dtype=np.int64)</span>
    <span class="s1">clf = MultiLabelClassifierPredictProba(response_output=response_output)</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;When MultiLabelClassifierPredictProba.predict_proba returns a NumPy &quot;</span>
        <span class="s5">r&quot;array, the expected data type is floating.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_classifiers_multilabel_output_format_predict_proba(</span>
            <span class="s1">clf.__class__.__name__</span><span class="s2">,</span>
            <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">)</span>
    <span class="s0"># 4. array does not contain probabilities</span>
    <span class="s1">clf = MultiLabelClassifierPredictProba(response_output=y_test * </span><span class="s4">2.0</span><span class="s1">)</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;When MultiLabelClassifierPredictProba.predict_proba returns a NumPy &quot;</span>
        <span class="s5">r&quot;array, this array is expected to provide probabilities of the &quot;</span>
        <span class="s5">r&quot;positive class and should therefore contain values between 0 and 1.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_classifiers_multilabel_output_format_predict_proba(</span>
            <span class="s1">clf.__class__.__name__</span><span class="s2">,</span>
            <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_check_classifiers_multilabel_output_format_decision_function():</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">test_size</span><span class="s2">, </span><span class="s1">n_outputs = </span><span class="s4">100</span><span class="s2">, </span><span class="s4">25</span><span class="s2">, </span><span class="s4">5</span>
    <span class="s1">_</span><span class="s2">, </span><span class="s1">y = make_multilabel_classification(</span>
        <span class="s1">n_samples=n_samples</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">2</span><span class="s2">,</span>
        <span class="s1">n_classes=n_outputs</span><span class="s2">,</span>
        <span class="s1">n_labels=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">length=</span><span class="s4">50</span><span class="s2">,</span>
        <span class="s1">allow_unlabeled=</span><span class="s2">True,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">y_test = y[-test_size:]</span>

    <span class="s2">class </span><span class="s1">MultiLabelClassifierDecisionFunction(_BaseMultiLabelClassifierMock):</span>
        <span class="s2">def </span><span class="s1">decision_function(self</span><span class="s2">, </span><span class="s1">X):</span>
            <span class="s2">return </span><span class="s1">self.response_output</span>

    <span class="s0"># 1. inconsistent array type</span>
    <span class="s1">clf = MultiLabelClassifierDecisionFunction(response_output=y_test.tolist())</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;MultiLabelClassifierDecisionFunction.decision_function is expected &quot;</span>
        <span class="s5">r&quot;to output a NumPy array. Got &lt;class 'list'&gt; instead.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_classifiers_multilabel_output_format_decision_function(</span>
            <span class="s1">clf.__class__.__name__</span><span class="s2">,</span>
            <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">)</span>
    <span class="s0"># 2. inconsistent shape</span>
    <span class="s1">clf = MultiLabelClassifierDecisionFunction(response_output=y_test[:</span><span class="s2">, </span><span class="s1">:-</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;MultiLabelClassifierDecisionFunction.decision_function is expected &quot;</span>
        <span class="s5">r&quot;to provide a NumPy array of shape \(n_samples, n_outputs\). Got &quot;</span>
        <span class="s5">r&quot;\(25, 4\) instead of \(25, 5\)&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_classifiers_multilabel_output_format_decision_function(</span>
            <span class="s1">clf.__class__.__name__</span><span class="s2">,</span>
            <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">)</span>
    <span class="s0"># 3. inconsistent dtype</span>
    <span class="s1">clf = MultiLabelClassifierDecisionFunction(response_output=y_test)</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">r&quot;MultiLabelClassifierDecisionFunction.decision_function is expected &quot;</span>
        <span class="s5">r&quot;to output a floating dtype.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_classifiers_multilabel_output_format_decision_function(</span>
            <span class="s1">clf.__class__.__name__</span><span class="s2">,</span>
            <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">run_tests_without_pytest():</span>
    <span class="s3">&quot;&quot;&quot;Runs the tests in this file without using pytest.&quot;&quot;&quot;</span>
    <span class="s1">main_module = sys.modules[</span><span class="s5">&quot;__main__&quot;</span><span class="s1">]</span>
    <span class="s1">test_functions = [</span>
        <span class="s1">getattr(main_module</span><span class="s2">, </span><span class="s1">name)</span>
        <span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">dir(main_module)</span>
        <span class="s2">if </span><span class="s1">name.startswith(</span><span class="s5">&quot;test_&quot;</span><span class="s1">)</span>
    <span class="s1">]</span>
    <span class="s1">test_cases = [unittest.FunctionTestCase(fn) </span><span class="s2">for </span><span class="s1">fn </span><span class="s2">in </span><span class="s1">test_functions]</span>
    <span class="s1">suite = unittest.TestSuite()</span>
    <span class="s1">suite.addTests(test_cases)</span>
    <span class="s1">runner = unittest.TextTestRunner()</span>
    <span class="s1">runner.run(suite)</span>


<span class="s2">def </span><span class="s1">test_check_class_weight_balanced_linear_classifier():</span>
    <span class="s0"># check that ill-computed balanced weights raises an exception</span>
    <span class="s1">msg = </span><span class="s5">&quot;Classifier estimator_name is not computing class_weight=balanced properly&quot;</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">check_class_weight_balanced_linear_classifier(</span>
            <span class="s5">&quot;estimator_name&quot;</span><span class="s2">, </span><span class="s1">BadBalancedWeightsClassifier</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_all_estimators_all_public():</span>
    <span class="s0"># all_estimator should not fail when pytest is not installed and return</span>
    <span class="s0"># only public estimators</span>
    <span class="s2">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s2">True</span><span class="s1">) </span><span class="s2">as </span><span class="s1">record:</span>
        <span class="s1">estimators = all_estimators()</span>
    <span class="s0"># no warnings are raised</span>
    <span class="s2">assert not </span><span class="s1">record</span>
    <span class="s2">for </span><span class="s1">est </span><span class="s2">in </span><span class="s1">estimators:</span>
        <span class="s2">assert not </span><span class="s1">est.__class__.__name__.startswith(</span><span class="s5">&quot;_&quot;</span><span class="s1">)</span>


<span class="s2">if </span><span class="s1">__name__ == </span><span class="s5">&quot;__main__&quot;</span><span class="s1">:</span>
    <span class="s0"># This module is run as a script to check that we have no dependency on</span>
    <span class="s0"># pytest for estimator checks.</span>
    <span class="s1">run_tests_without_pytest()</span>


<span class="s2">def </span><span class="s1">test_xfail_ignored_in_check_estimator():</span>
    <span class="s0"># Make sure checks marked as xfail are just ignored and not run by</span>
    <span class="s0"># check_estimator(), but still raise a warning.</span>
    <span class="s2">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s2">True</span><span class="s1">) </span><span class="s2">as </span><span class="s1">records:</span>
        <span class="s1">check_estimator(NuSVC())</span>
    <span class="s2">assert </span><span class="s1">SkipTestWarning </span><span class="s2">in </span><span class="s1">[rec.category </span><span class="s2">for </span><span class="s1">rec </span><span class="s2">in </span><span class="s1">records]</span>


<span class="s0"># FIXME: this test should be uncommented when the checks will be granular</span>
<span class="s0"># enough. In 0.24, these tests fail due to low estimator performance.</span>
<span class="s2">def </span><span class="s1">test_minimal_class_implementation_checks():</span>
    <span class="s0"># Check that third-party library can run tests without inheriting from</span>
    <span class="s0"># BaseEstimator.</span>
    <span class="s0"># FIXME</span>
    <span class="s2">raise </span><span class="s1">SkipTest</span>
    <span class="s1">minimal_estimators = [MinimalTransformer()</span><span class="s2">, </span><span class="s1">MinimalRegressor()</span><span class="s2">, </span><span class="s1">MinimalClassifier()]</span>
    <span class="s2">for </span><span class="s1">estimator </span><span class="s2">in </span><span class="s1">minimal_estimators:</span>
        <span class="s1">check_estimator(estimator)</span>


<span class="s2">def </span><span class="s1">test_check_fit_check_is_fitted():</span>
    <span class="s2">class </span><span class="s1">Estimator(BaseEstimator):</span>
        <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">behavior=</span><span class="s5">&quot;attribute&quot;</span><span class="s1">):</span>
            <span class="s1">self.behavior = behavior</span>

        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">**kwargs):</span>
            <span class="s2">if </span><span class="s1">self.behavior == </span><span class="s5">&quot;attribute&quot;</span><span class="s1">:</span>
                <span class="s1">self.is_fitted_ = </span><span class="s2">True</span>
            <span class="s2">elif </span><span class="s1">self.behavior == </span><span class="s5">&quot;method&quot;</span><span class="s1">:</span>
                <span class="s1">self._is_fitted = </span><span class="s2">True</span>
            <span class="s2">return </span><span class="s1">self</span>

        <span class="s1">@available_if(</span><span class="s2">lambda </span><span class="s1">self: self.behavior </span><span class="s2">in </span><span class="s1">{</span><span class="s5">&quot;method&quot;</span><span class="s2">, </span><span class="s5">&quot;always-true&quot;</span><span class="s1">})</span>
        <span class="s2">def </span><span class="s1">__sklearn_is_fitted__(self):</span>
            <span class="s2">if </span><span class="s1">self.behavior == </span><span class="s5">&quot;always-true&quot;</span><span class="s1">:</span>
                <span class="s2">return True</span>
            <span class="s2">return </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s5">&quot;_is_fitted&quot;</span><span class="s1">)</span>

    <span class="s2">with </span><span class="s1">raises(Exception</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;passes check_is_fitted before being fit&quot;</span><span class="s1">):</span>
        <span class="s1">check_fit_check_is_fitted(</span><span class="s5">&quot;estimator&quot;</span><span class="s2">, </span><span class="s1">Estimator(behavior=</span><span class="s5">&quot;always-true&quot;</span><span class="s1">))</span>

    <span class="s1">check_fit_check_is_fitted(</span><span class="s5">&quot;estimator&quot;</span><span class="s2">, </span><span class="s1">Estimator(behavior=</span><span class="s5">&quot;method&quot;</span><span class="s1">))</span>
    <span class="s1">check_fit_check_is_fitted(</span><span class="s5">&quot;estimator&quot;</span><span class="s2">, </span><span class="s1">Estimator(behavior=</span><span class="s5">&quot;attribute&quot;</span><span class="s1">))</span>


<span class="s2">def </span><span class="s1">test_check_requires_y_none():</span>
    <span class="s2">class </span><span class="s1">Estimator(BaseEstimator):</span>
        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y = check_X_y(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s2">True</span><span class="s1">) </span><span class="s2">as </span><span class="s1">record:</span>
        <span class="s1">check_requires_y_none(</span><span class="s5">&quot;estimator&quot;</span><span class="s2">, </span><span class="s1">Estimator())</span>

    <span class="s0"># no warnings are raised</span>
    <span class="s2">assert not </span><span class="s1">[r.message </span><span class="s2">for </span><span class="s1">r </span><span class="s2">in </span><span class="s1">record]</span>


<span class="s2">def </span><span class="s1">test_non_deterministic_estimator_skip_tests():</span>
    <span class="s0"># check estimators with non_deterministic tag set to True</span>
    <span class="s0"># will skip certain tests, refer to issue #22313 for details</span>
    <span class="s2">for </span><span class="s1">est </span><span class="s2">in </span><span class="s1">[MinimalTransformer</span><span class="s2">, </span><span class="s1">MinimalRegressor</span><span class="s2">, </span><span class="s1">MinimalClassifier]:</span>
        <span class="s1">all_tests = list(_yield_all_checks(est()))</span>
        <span class="s2">assert </span><span class="s1">check_methods_sample_order_invariance </span><span class="s2">in </span><span class="s1">all_tests</span>
        <span class="s2">assert </span><span class="s1">check_methods_subset_invariance </span><span class="s2">in </span><span class="s1">all_tests</span>

        <span class="s2">class </span><span class="s1">Estimator(est):</span>
            <span class="s2">def </span><span class="s1">_more_tags(self):</span>
                <span class="s2">return </span><span class="s1">{</span><span class="s5">&quot;non_deterministic&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>

        <span class="s1">all_tests = list(_yield_all_checks(Estimator()))</span>
        <span class="s2">assert </span><span class="s1">check_methods_sample_order_invariance </span><span class="s2">not in </span><span class="s1">all_tests</span>
        <span class="s2">assert </span><span class="s1">check_methods_subset_invariance </span><span class="s2">not in </span><span class="s1">all_tests</span>


<span class="s2">def </span><span class="s1">test_check_outlier_contamination():</span>
    <span class="s3">&quot;&quot;&quot;Check the test for the contamination parameter in the outlier detectors.&quot;&quot;&quot;</span>

    <span class="s0"># Without any parameter constraints, the estimator will early exit the test by</span>
    <span class="s0"># returning None.</span>
    <span class="s2">class </span><span class="s1">OutlierDetectorWithoutConstraint(OutlierMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
        <span class="s3">&quot;&quot;&quot;Outlier detector without parameter validation.&quot;&quot;&quot;</span>

        <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">contamination=</span><span class="s4">0.1</span><span class="s1">):</span>
            <span class="s1">self.contamination = contamination</span>

        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
            <span class="s2">return </span><span class="s1">self  </span><span class="s0"># pragma: no cover</span>

        <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
            <span class="s2">return </span><span class="s1">np.ones(X.shape[</span><span class="s4">0</span><span class="s1">])</span>

    <span class="s1">detector = OutlierDetectorWithoutConstraint()</span>
    <span class="s2">assert </span><span class="s1">check_outlier_contamination(detector.__class__.__name__</span><span class="s2">, </span><span class="s1">detector) </span><span class="s2">is None</span>

    <span class="s0"># Now, we check that with the parameter constraints, the test should only be valid</span>
    <span class="s0"># if an Interval constraint with bound in [0, 1] is provided.</span>
    <span class="s2">class </span><span class="s1">OutlierDetectorWithConstraint(OutlierDetectorWithoutConstraint):</span>
        <span class="s1">_parameter_constraints = {</span><span class="s5">&quot;contamination&quot;</span><span class="s1">: [StrOptions({</span><span class="s5">&quot;auto&quot;</span><span class="s1">})]}</span>

    <span class="s1">detector = OutlierDetectorWithConstraint()</span>
    <span class="s1">err_msg = </span><span class="s5">&quot;contamination constraints should contain a Real Interval constraint.&quot;</span>
    <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">check_outlier_contamination(detector.__class__.__name__</span><span class="s2">, </span><span class="s1">detector)</span>

    <span class="s0"># Add a correct interval constraint and check that the test passes.</span>
    <span class="s1">OutlierDetectorWithConstraint._parameter_constraints[</span><span class="s5">&quot;contamination&quot;</span><span class="s1">] = [</span>
        <span class="s1">Interval(Real</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s1">closed=</span><span class="s5">&quot;right&quot;</span><span class="s1">)</span>
    <span class="s1">]</span>
    <span class="s1">detector = OutlierDetectorWithConstraint()</span>
    <span class="s1">check_outlier_contamination(detector.__class__.__name__</span><span class="s2">, </span><span class="s1">detector)</span>

    <span class="s1">incorrect_intervals = [</span>
        <span class="s1">Interval(Integral</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">closed=</span><span class="s5">&quot;right&quot;</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># not an integral interval</span>
        <span class="s1">Interval(Real</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">closed=</span><span class="s5">&quot;right&quot;</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># lower bound is negative</span>
        <span class="s1">Interval(Real</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s1">closed=</span><span class="s5">&quot;right&quot;</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># upper bound is greater than 1</span>
        <span class="s1">Interval(Real</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># lower bound include 0</span>
    <span class="s1">]</span>

    <span class="s1">err_msg = </span><span class="s5">r&quot;contamination constraint should be an interval in \(0, 0.5\]&quot;</span>
    <span class="s2">for </span><span class="s1">interval </span><span class="s2">in </span><span class="s1">incorrect_intervals:</span>
        <span class="s1">OutlierDetectorWithConstraint._parameter_constraints[</span><span class="s5">&quot;contamination&quot;</span><span class="s1">] = [</span>
            <span class="s1">interval</span>
        <span class="s1">]</span>
        <span class="s1">detector = OutlierDetectorWithConstraint()</span>
        <span class="s2">with </span><span class="s1">raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
            <span class="s1">check_outlier_contamination(detector.__class__.__name__</span><span class="s2">, </span><span class="s1">detector)</span>


<span class="s2">def </span><span class="s1">test_decision_proba_tie_ranking():</span>
    <span class="s3">&quot;&quot;&quot;Check that in case with some probabilities ties, we relax the 
    ranking comparison with the decision function. 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/24025 
    &quot;&quot;&quot;</span>
    <span class="s1">estimator = SGDClassifier(loss=</span><span class="s5">&quot;log_loss&quot;</span><span class="s1">)</span>
    <span class="s1">check_decision_proba_consistency(</span><span class="s5">&quot;SGDClassifier&quot;</span><span class="s2">, </span><span class="s1">estimator)</span>
</pre>
</body>
</html>