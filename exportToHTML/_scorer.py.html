<html>
<head>
<title>_scorer.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_scorer.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
The :mod:`sklearn.metrics.scorer` submodule implements a flexible 
interface for model selection and evaluation using 
arbitrary score functions. 
 
A scorer object is a callable that can be passed to 
:class:`~sklearn.model_selection.GridSearchCV` or 
:func:`sklearn.model_selection.cross_val_score` as the ``scoring`` 
parameter, to specify how a model should be evaluated. 
 
The signature of the call is ``(estimator, X, y)`` where ``estimator`` 
is the model to be evaluated, ``X`` is the test data and ``y`` is the 
ground truth labeling (or ``None`` in the case of unsupervised models). 
&quot;&quot;&quot;</span>

<span class="s2"># Authors: Andreas Mueller &lt;amueller@ais.uni-bonn.de&gt;</span>
<span class="s2">#          Lars Buitinck</span>
<span class="s2">#          Arnaud Joly &lt;arnaud.v.joly@gmail.com&gt;</span>
<span class="s2"># License: Simplified BSD</span>

<span class="s3">import </span><span class="s1">copy</span>
<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">collections </span><span class="s3">import </span><span class="s1">Counter</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">from </span><span class="s1">inspect </span><span class="s3">import </span><span class="s1">signature</span>
<span class="s3">from </span><span class="s1">traceback </span><span class="s3">import </span><span class="s1">format_exc</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">is_regressor</span>
<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">Bunch</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">HasMethods</span><span class="s3">, </span><span class="s1">StrOptions</span><span class="s3">, </span><span class="s1">validate_params</span>
<span class="s3">from </span><span class="s1">..utils._response </span><span class="s3">import </span><span class="s1">_get_response_values</span>
<span class="s3">from </span><span class="s1">..utils.metadata_routing </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">MetadataRequest</span><span class="s3">,</span>
    <span class="s1">MetadataRouter</span><span class="s3">,</span>
    <span class="s1">_MetadataRequester</span><span class="s3">,</span>
    <span class="s1">_routing_enabled</span><span class="s3">,</span>
    <span class="s1">get_routing_for_object</span><span class="s3">,</span>
    <span class="s1">process_routing</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">..utils.multiclass </span><span class="s3">import </span><span class="s1">type_of_target</span>
<span class="s3">from </span><span class="s1">. </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">accuracy_score</span><span class="s3">,</span>
    <span class="s1">average_precision_score</span><span class="s3">,</span>
    <span class="s1">balanced_accuracy_score</span><span class="s3">,</span>
    <span class="s1">brier_score_loss</span><span class="s3">,</span>
    <span class="s1">class_likelihood_ratios</span><span class="s3">,</span>
    <span class="s1">explained_variance_score</span><span class="s3">,</span>
    <span class="s1">f1_score</span><span class="s3">,</span>
    <span class="s1">jaccard_score</span><span class="s3">,</span>
    <span class="s1">log_loss</span><span class="s3">,</span>
    <span class="s1">matthews_corrcoef</span><span class="s3">,</span>
    <span class="s1">max_error</span><span class="s3">,</span>
    <span class="s1">mean_absolute_error</span><span class="s3">,</span>
    <span class="s1">mean_absolute_percentage_error</span><span class="s3">,</span>
    <span class="s1">mean_gamma_deviance</span><span class="s3">,</span>
    <span class="s1">mean_poisson_deviance</span><span class="s3">,</span>
    <span class="s1">mean_squared_error</span><span class="s3">,</span>
    <span class="s1">mean_squared_log_error</span><span class="s3">,</span>
    <span class="s1">median_absolute_error</span><span class="s3">,</span>
    <span class="s1">precision_score</span><span class="s3">,</span>
    <span class="s1">r2_score</span><span class="s3">,</span>
    <span class="s1">recall_score</span><span class="s3">,</span>
    <span class="s1">roc_auc_score</span><span class="s3">,</span>
    <span class="s1">top_k_accuracy_score</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">.cluster </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">adjusted_mutual_info_score</span><span class="s3">,</span>
    <span class="s1">adjusted_rand_score</span><span class="s3">,</span>
    <span class="s1">completeness_score</span><span class="s3">,</span>
    <span class="s1">fowlkes_mallows_score</span><span class="s3">,</span>
    <span class="s1">homogeneity_score</span><span class="s3">,</span>
    <span class="s1">mutual_info_score</span><span class="s3">,</span>
    <span class="s1">normalized_mutual_info_score</span><span class="s3">,</span>
    <span class="s1">rand_score</span><span class="s3">,</span>
    <span class="s1">v_measure_score</span><span class="s3">,</span>
<span class="s1">)</span>


<span class="s3">def </span><span class="s1">_cached_call(cache</span><span class="s3">, </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">response_method</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s0">&quot;&quot;&quot;Call estimator with method and args and kwargs.&quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">cache </span><span class="s3">is not None and </span><span class="s1">response_method </span><span class="s3">in </span><span class="s1">cache:</span>
        <span class="s3">return </span><span class="s1">cache[response_method]</span>

    <span class="s1">result</span><span class="s3">, </span><span class="s1">_ = _get_response_values(</span>
        <span class="s1">estimator</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">response_method=response_method</span><span class="s3">, </span><span class="s1">**kwargs</span>
    <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">cache </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">cache[response_method] = result</span>

    <span class="s3">return </span><span class="s1">result</span>


<span class="s3">class </span><span class="s1">_MultimetricScorer:</span>
    <span class="s0">&quot;&quot;&quot;Callable for multimetric scoring used to avoid repeated calls 
    to `predict_proba`, `predict`, and `decision_function`. 
 
    `_MultimetricScorer` will return a dictionary of scores corresponding to 
    the scorers in the dictionary. Note that `_MultimetricScorer` can be 
    created with a dictionary with one key  (i.e. only one actual scorer). 
 
    Parameters 
    ---------- 
    scorers : dict 
        Dictionary mapping names to callable scorers. 
 
    raise_exc : bool, default=True 
        Whether to raise the exception in `__call__` or not. If set to `False` 
        a formatted string of the exception details is passed as result of 
        the failing scorer. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">scorers</span><span class="s3">, </span><span class="s1">raise_exc=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s1">self._scorers = scorers</span>
        <span class="s1">self._raise_exc = raise_exc</span>

    <span class="s3">def </span><span class="s1">__call__(self</span><span class="s3">, </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s0">&quot;&quot;&quot;Evaluate predicted target values.&quot;&quot;&quot;</span>
        <span class="s1">scores = {}</span>
        <span class="s1">cache = {} </span><span class="s3">if </span><span class="s1">self._use_cache(estimator) </span><span class="s3">else None</span>
        <span class="s1">cached_call = partial(_cached_call</span><span class="s3">, </span><span class="s1">cache)</span>

        <span class="s3">if </span><span class="s1">_routing_enabled():</span>
            <span class="s1">routed_params = process_routing(self</span><span class="s3">, </span><span class="s4">&quot;score&quot;</span><span class="s3">, </span><span class="s1">kwargs)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># they all get the same args, and they all get them all</span>
            <span class="s1">routed_params = Bunch(</span>
                <span class="s1">**{name: Bunch(score=kwargs) </span><span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">self._scorers}</span>
            <span class="s1">)</span>

        <span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">scorer </span><span class="s3">in </span><span class="s1">self._scorers.items():</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s3">if </span><span class="s1">isinstance(scorer</span><span class="s3">, </span><span class="s1">_BaseScorer):</span>
                    <span class="s1">score = scorer._score(</span>
                        <span class="s1">cached_call</span><span class="s3">, </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**routed_params.get(name).score</span>
                    <span class="s1">)</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">score = scorer(estimator</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**routed_params.get(name).score)</span>
                <span class="s1">scores[name] = score</span>
            <span class="s3">except </span><span class="s1">Exception </span><span class="s3">as </span><span class="s1">e:</span>
                <span class="s3">if </span><span class="s1">self._raise_exc:</span>
                    <span class="s3">raise </span><span class="s1">e</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">scores[name] = format_exc()</span>
        <span class="s3">return </span><span class="s1">scores</span>

    <span class="s3">def </span><span class="s1">_use_cache(self</span><span class="s3">, </span><span class="s1">estimator):</span>
        <span class="s0">&quot;&quot;&quot;Return True if using a cache is beneficial. 
 
        Caching may be beneficial when one of these conditions holds: 
          - `_ProbaScorer` will be called twice. 
          - `_PredictScorer` will be called twice. 
          - `_ThresholdScorer` will be called twice. 
          - `_ThresholdScorer` and `_PredictScorer` are called and 
             estimator is a regressor. 
          - `_ThresholdScorer` and `_ProbaScorer` are called and 
             estimator does not have a `decision_function` attribute. 
 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">len(self._scorers) == </span><span class="s5">1</span><span class="s1">:  </span><span class="s2"># Only one scorer</span>
            <span class="s3">return False</span>

        <span class="s1">counter = Counter([type(v) </span><span class="s3">for </span><span class="s1">v </span><span class="s3">in </span><span class="s1">self._scorers.values()])</span>

        <span class="s3">if </span><span class="s1">any(</span>
            <span class="s1">counter[known_type] &gt; </span><span class="s5">1</span>
            <span class="s3">for </span><span class="s1">known_type </span><span class="s3">in </span><span class="s1">[_PredictScorer</span><span class="s3">, </span><span class="s1">_ProbaScorer</span><span class="s3">, </span><span class="s1">_ThresholdScorer]</span>
        <span class="s1">):</span>
            <span class="s3">return True</span>

        <span class="s3">if </span><span class="s1">counter[_ThresholdScorer]:</span>
            <span class="s3">if </span><span class="s1">is_regressor(estimator) </span><span class="s3">and </span><span class="s1">counter[_PredictScorer]:</span>
                <span class="s3">return True</span>
            <span class="s3">elif </span><span class="s1">counter[_ProbaScorer] </span><span class="s3">and not </span><span class="s1">hasattr(estimator</span><span class="s3">, </span><span class="s4">&quot;decision_function&quot;</span><span class="s1">):</span>
                <span class="s3">return True</span>
        <span class="s3">return False</span>

    <span class="s3">def </span><span class="s1">get_metadata_routing(self):</span>
        <span class="s0">&quot;&quot;&quot;Get metadata routing of this object. 
 
        Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing 
        mechanism works. 
 
        .. versionadded:: 1.3 
 
        Returns 
        ------- 
        routing : MetadataRouter 
            A :class:`~utils.metadata_routing.MetadataRouter` encapsulating 
            routing information. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">MetadataRouter(owner=self.__class__.__name__).add(</span>
            <span class="s1">**self._scorers</span><span class="s3">, </span><span class="s1">method_mapping=</span><span class="s4">&quot;score&quot;</span>
        <span class="s1">)</span>


<span class="s3">class </span><span class="s1">_BaseScorer(_MetadataRequester):</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">score_func</span><span class="s3">, </span><span class="s1">sign</span><span class="s3">, </span><span class="s1">kwargs):</span>
        <span class="s1">self._kwargs = kwargs</span>
        <span class="s1">self._score_func = score_func</span>
        <span class="s1">self._sign = sign</span>

    <span class="s3">def </span><span class="s1">_get_pos_label(self):</span>
        <span class="s3">if </span><span class="s4">&quot;pos_label&quot; </span><span class="s3">in </span><span class="s1">self._kwargs:</span>
            <span class="s3">return </span><span class="s1">self._kwargs[</span><span class="s4">&quot;pos_label&quot;</span><span class="s1">]</span>
        <span class="s1">score_func_params = signature(self._score_func).parameters</span>
        <span class="s3">if </span><span class="s4">&quot;pos_label&quot; </span><span class="s3">in </span><span class="s1">score_func_params:</span>
            <span class="s3">return </span><span class="s1">score_func_params[</span><span class="s4">&quot;pos_label&quot;</span><span class="s1">].default</span>
        <span class="s3">return None</span>

    <span class="s3">def </span><span class="s1">__repr__(self):</span>
        <span class="s1">kwargs_string = </span><span class="s4">&quot;&quot;</span><span class="s1">.join(</span>
            <span class="s1">[</span><span class="s4">&quot;, %s=%s&quot; </span><span class="s1">% (str(k)</span><span class="s3">, </span><span class="s1">str(v)) </span><span class="s3">for </span><span class="s1">k</span><span class="s3">, </span><span class="s1">v </span><span class="s3">in </span><span class="s1">self._kwargs.items()]</span>
        <span class="s1">)</span>
        <span class="s3">return </span><span class="s4">&quot;make_scorer(%s%s%s%s)&quot; </span><span class="s1">% (</span>
            <span class="s1">self._score_func.__name__</span><span class="s3">,</span>
            <span class="s4">&quot;&quot; </span><span class="s3">if </span><span class="s1">self._sign &gt; </span><span class="s5">0 </span><span class="s3">else </span><span class="s4">&quot;, greater_is_better=False&quot;</span><span class="s3">,</span>
            <span class="s1">self._factory_args()</span><span class="s3">,</span>
            <span class="s1">kwargs_string</span><span class="s3">,</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">__call__(self</span><span class="s3">, </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y_true</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s0">&quot;&quot;&quot;Evaluate predicted target values for X relative to y_true. 
 
        Parameters 
        ---------- 
        estimator : object 
            Trained estimator to use for scoring. Must have a predict_proba 
            method; the output of that is used to compute the score. 
 
        X : {array-like, sparse matrix} 
            Test data that will be fed to estimator.predict. 
 
        y_true : array-like 
            Gold standard target values for X. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. 
 
        **kwargs : dict 
            Other parameters passed to the scorer. Refer to 
            :func:`set_score_request` for more details. 
 
            Only available if `enable_metadata_routing=True`. See the 
            :ref:`User Guide &lt;metadata_routing&gt;`. 
 
            .. versionadded:: 1.3 
 
        Returns 
        ------- 
        score : float 
            Score function applied to prediction of estimator on X. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">kwargs </span><span class="s3">and not </span><span class="s1">_routing_enabled():</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;kwargs is only supported if enable_metadata_routing=True. See&quot;</span>
                <span class="s4">&quot; the User Guide for more information.&quot;</span>
            <span class="s1">)</span>

        <span class="s1">_kwargs = copy.deepcopy(kwargs)</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">_kwargs[</span><span class="s4">&quot;sample_weight&quot;</span><span class="s1">] = sample_weight</span>

        <span class="s3">return </span><span class="s1">self._score(partial(_cached_call</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">, </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y_true</span><span class="s3">, </span><span class="s1">**_kwargs)</span>

    <span class="s3">def </span><span class="s1">_factory_args(self):</span>
        <span class="s0">&quot;&quot;&quot;Return non-default make_scorer arguments for repr.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s4">&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">_warn_overlap(self</span><span class="s3">, </span><span class="s1">message</span><span class="s3">, </span><span class="s1">kwargs):</span>
        <span class="s0">&quot;&quot;&quot;Warn if there is any overlap between ``self._kwargs`` and ``kwargs``. 
 
        This method is intended to be used to check for overlap between 
        ``self._kwargs`` and ``kwargs`` passed as metadata. 
        &quot;&quot;&quot;</span>
        <span class="s1">_kwargs = set() </span><span class="s3">if </span><span class="s1">self._kwargs </span><span class="s3">is None else </span><span class="s1">set(self._kwargs.keys())</span>
        <span class="s1">overlap = _kwargs.intersection(kwargs.keys())</span>
        <span class="s3">if </span><span class="s1">overlap:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">message</span><span class="s3">} </span><span class="s4">Overlapping parameters are: </span><span class="s3">{</span><span class="s1">overlap</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s3">, </span><span class="s1">UserWarning</span>
            <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">set_score_request(self</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s0">&quot;&quot;&quot;Set requested parameters by the scorer. 
 
        Please see :ref:`User Guide &lt;metadata_routing&gt;` on how the routing 
        mechanism works. 
 
        .. versionadded:: 1.3 
 
        Parameters 
        ---------- 
        kwargs : dict 
            Arguments should be of the form ``param_name=alias``, and `alias` 
            can be one of ``{True, False, None, str}``. 
        &quot;&quot;&quot;</span>
        <span class="s1">self._warn_overlap(</span>
            <span class="s1">message=(</span>
                <span class="s4">&quot;You are setting metadata request for parameters which are &quot;</span>
                <span class="s4">&quot;already set as kwargs for this metric. These set values will be &quot;</span>
                <span class="s4">&quot;overridden by passed metadata if provided. Please pass them either &quot;</span>
                <span class="s4">&quot;as metadata or kwargs to `make_scorer`.&quot;</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">kwargs=kwargs</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">self._metadata_request = MetadataRequest(owner=self.__class__.__name__)</span>
        <span class="s3">for </span><span class="s1">param</span><span class="s3">, </span><span class="s1">alias </span><span class="s3">in </span><span class="s1">kwargs.items():</span>
            <span class="s1">self._metadata_request.score.add_request(param=param</span><span class="s3">, </span><span class="s1">alias=alias)</span>
        <span class="s3">return </span><span class="s1">self</span>


<span class="s3">class </span><span class="s1">_PredictScorer(_BaseScorer):</span>
    <span class="s3">def </span><span class="s1">_score(self</span><span class="s3">, </span><span class="s1">method_caller</span><span class="s3">, </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y_true</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s0">&quot;&quot;&quot;Evaluate predicted target values for X relative to y_true. 
 
        Parameters 
        ---------- 
        method_caller : callable 
            Returns predictions given an estimator, method name, and other 
            arguments, potentially caching results. 
 
        estimator : object 
            Trained estimator to use for scoring. Must have a `predict` 
            method; the output of that is used to compute the score. 
 
        X : {array-like, sparse matrix} 
            Test data that will be fed to estimator.predict. 
 
        y_true : array-like 
            Gold standard target values for X. 
 
        **kwargs : dict 
            Other parameters passed to the scorer. Refer to 
            :func:`set_score_request` for more details. 
 
            .. versionadded:: 1.3 
 
        Returns 
        ------- 
        score : float 
            Score function applied to prediction of estimator on X. 
        &quot;&quot;&quot;</span>
        <span class="s1">self._warn_overlap(</span>
            <span class="s1">message=(</span>
                <span class="s4">&quot;There is an overlap between set kwargs of this scorer instance and&quot;</span>
                <span class="s4">&quot; passed metadata. Please pass them either as kwargs to `make_scorer`&quot;</span>
                <span class="s4">&quot; or metadata, but not both.&quot;</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">kwargs=kwargs</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">y_pred = method_caller(estimator</span><span class="s3">, </span><span class="s4">&quot;predict&quot;</span><span class="s3">, </span><span class="s1">X)</span>
        <span class="s1">scoring_kwargs = {**self._kwargs</span><span class="s3">, </span><span class="s1">**kwargs}</span>
        <span class="s3">return </span><span class="s1">self._sign * self._score_func(y_true</span><span class="s3">, </span><span class="s1">y_pred</span><span class="s3">, </span><span class="s1">**scoring_kwargs)</span>


<span class="s3">class </span><span class="s1">_ProbaScorer(_BaseScorer):</span>
    <span class="s3">def </span><span class="s1">_score(self</span><span class="s3">, </span><span class="s1">method_caller</span><span class="s3">, </span><span class="s1">clf</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s0">&quot;&quot;&quot;Evaluate predicted probabilities for X relative to y_true. 
 
        Parameters 
        ---------- 
        method_caller : callable 
            Returns predictions given an estimator, method name, and other 
            arguments, potentially caching results. 
 
        clf : object 
            Trained classifier to use for scoring. Must have a `predict_proba` 
            method; the output of that is used to compute the score. 
 
        X : {array-like, sparse matrix} 
            Test data that will be fed to clf.predict_proba. 
 
        y : array-like 
            Gold standard target values for X. These must be class labels, 
            not probabilities. 
 
        **kwargs : dict 
            Other parameters passed to the scorer. Refer to 
            :func:`set_score_request` for more details. 
 
            .. versionadded:: 1.3 
 
        Returns 
        ------- 
        score : float 
            Score function applied to prediction of estimator on X. 
        &quot;&quot;&quot;</span>
        <span class="s1">self._warn_overlap(</span>
            <span class="s1">message=(</span>
                <span class="s4">&quot;There is an overlap between set kwargs of this scorer instance and&quot;</span>
                <span class="s4">&quot; passed metadata. Please pass them either as kwargs to `make_scorer`&quot;</span>
                <span class="s4">&quot; or metadata, but not both.&quot;</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">kwargs=kwargs</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s1">y_pred = method_caller(clf</span><span class="s3">, </span><span class="s4">&quot;predict_proba&quot;</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">pos_label=self._get_pos_label())</span>
        <span class="s1">scoring_kwargs = {**self._kwargs</span><span class="s3">, </span><span class="s1">**kwargs}</span>
        <span class="s3">return </span><span class="s1">self._sign * self._score_func(y</span><span class="s3">, </span><span class="s1">y_pred</span><span class="s3">, </span><span class="s1">**scoring_kwargs)</span>

    <span class="s3">def </span><span class="s1">_factory_args(self):</span>
        <span class="s3">return </span><span class="s4">&quot;, needs_proba=True&quot;</span>


<span class="s3">class </span><span class="s1">_ThresholdScorer(_BaseScorer):</span>
    <span class="s3">def </span><span class="s1">_score(self</span><span class="s3">, </span><span class="s1">method_caller</span><span class="s3">, </span><span class="s1">clf</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s0">&quot;&quot;&quot;Evaluate decision function output for X relative to y_true. 
 
        Parameters 
        ---------- 
        method_caller : callable 
            Returns predictions given an estimator, method name, and other 
            arguments, potentially caching results. 
 
        clf : object 
            Trained classifier to use for scoring. Must have either a 
            decision_function method or a predict_proba method; the output of 
            that is used to compute the score. 
 
        X : {array-like, sparse matrix} 
            Test data that will be fed to clf.decision_function or 
            clf.predict_proba. 
 
        y : array-like 
            Gold standard target values for X. These must be class labels, 
            not decision function values. 
 
        **kwargs : dict 
            Other parameters passed to the scorer. Refer to 
            :func:`set_score_request` for more details. 
 
            .. versionadded:: 1.3 
 
        Returns 
        ------- 
        score : float 
            Score function applied to prediction of estimator on X. 
        &quot;&quot;&quot;</span>
        <span class="s1">self._warn_overlap(</span>
            <span class="s1">message=(</span>
                <span class="s4">&quot;There is an overlap between set kwargs of this scorer instance and&quot;</span>
                <span class="s4">&quot; passed metadata. Please pass them either as kwargs to `make_scorer`&quot;</span>
                <span class="s4">&quot; or metadata, but not both.&quot;</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">kwargs=kwargs</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s1">y_type = type_of_target(y)</span>
        <span class="s3">if </span><span class="s1">y_type </span><span class="s3">not in </span><span class="s1">(</span><span class="s4">&quot;binary&quot;</span><span class="s3">, </span><span class="s4">&quot;multilabel-indicator&quot;</span><span class="s1">):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;{0} format is not supported&quot;</span><span class="s1">.format(y_type))</span>

        <span class="s3">if </span><span class="s1">is_regressor(clf):</span>
            <span class="s1">y_pred = method_caller(clf</span><span class="s3">, </span><span class="s4">&quot;predict&quot;</span><span class="s3">, </span><span class="s1">X)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">pos_label = self._get_pos_label()</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s1">y_pred = method_caller(clf</span><span class="s3">, </span><span class="s4">&quot;decision_function&quot;</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">pos_label=pos_label)</span>

                <span class="s3">if </span><span class="s1">isinstance(y_pred</span><span class="s3">, </span><span class="s1">list):</span>
                    <span class="s2"># For multi-output multi-class estimator</span>
                    <span class="s1">y_pred = np.vstack([p </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">y_pred]).T</span>

            <span class="s3">except </span><span class="s1">(NotImplementedError</span><span class="s3">, </span><span class="s1">AttributeError):</span>
                <span class="s1">y_pred = method_caller(clf</span><span class="s3">, </span><span class="s4">&quot;predict_proba&quot;</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">pos_label=pos_label)</span>
                <span class="s3">if </span><span class="s1">isinstance(y_pred</span><span class="s3">, </span><span class="s1">list):</span>
                    <span class="s1">y_pred = np.vstack([p[:</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">] </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">y_pred]).T</span>

        <span class="s1">scoring_kwargs = {**self._kwargs</span><span class="s3">, </span><span class="s1">**kwargs}</span>
        <span class="s3">return </span><span class="s1">self._sign * self._score_func(y</span><span class="s3">, </span><span class="s1">y_pred</span><span class="s3">, </span><span class="s1">**scoring_kwargs)</span>

    <span class="s3">def </span><span class="s1">_factory_args(self):</span>
        <span class="s3">return </span><span class="s4">&quot;, needs_threshold=True&quot;</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s4">&quot;scoring&quot;</span><span class="s1">: [str</span><span class="s3">, </span><span class="s1">callable</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">True,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">get_scorer(scoring):</span>
    <span class="s0">&quot;&quot;&quot;Get a scorer from string. 
 
    Read more in the :ref:`User Guide &lt;scoring_parameter&gt;`. 
    :func:`~sklearn.metrics.get_scorer_names` can be used to retrieve the names 
    of all available scorers. 
 
    Parameters 
    ---------- 
    scoring : str, callable or None 
        Scoring method as string. If callable it is returned as is. 
        If None, returns None. 
 
    Returns 
    ------- 
    scorer : callable 
        The scorer. 
 
    Notes 
    ----- 
    When passed a string, this function always returns a copy of the scorer 
    object. Calling `get_scorer` twice for the same scorer results in two 
    separate scorer objects. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">isinstance(scoring</span><span class="s3">, </span><span class="s1">str):</span>
        <span class="s3">try</span><span class="s1">:</span>
            <span class="s1">scorer = copy.deepcopy(_SCORERS[scoring])</span>
        <span class="s3">except </span><span class="s1">KeyError:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;%r is not a valid scoring value. &quot;</span>
                <span class="s4">&quot;Use sklearn.metrics.get_scorer_names() &quot;</span>
                <span class="s4">&quot;to get valid options.&quot; </span><span class="s1">% scoring</span>
            <span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">scorer = scoring</span>
    <span class="s3">return </span><span class="s1">scorer</span>


<span class="s3">class </span><span class="s1">_PassthroughScorer:</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">estimator):</span>
        <span class="s1">self._estimator = estimator</span>

    <span class="s3">def </span><span class="s1">__call__(self</span><span class="s3">, </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s0">&quot;&quot;&quot;Method that wraps estimator.score&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">estimator.score(*args</span><span class="s3">, </span><span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">get_metadata_routing(self):</span>
        <span class="s0">&quot;&quot;&quot;Get requested data properties. 
 
        Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing 
        mechanism works. 
 
        .. versionadded:: 1.3 
 
        Returns 
        ------- 
        routing : MetadataRouter 
            A :class:`~utils.metadata_routing.MetadataRouter` encapsulating 
            routing information. 
        &quot;&quot;&quot;</span>
        <span class="s2"># This scorer doesn't do any validation or routing, it only exposes the</span>
        <span class="s2"># score requests to the parent object. This object behaves as a</span>
        <span class="s2"># consumer rather than a router.</span>
        <span class="s1">res = MetadataRequest(owner=self._estimator.__class__.__name__)</span>
        <span class="s1">res.score = get_routing_for_object(self._estimator).score</span>
        <span class="s3">return </span><span class="s1">res</span>


<span class="s3">def </span><span class="s1">_check_multimetric_scoring(estimator</span><span class="s3">, </span><span class="s1">scoring):</span>
    <span class="s0">&quot;&quot;&quot;Check the scoring parameter in cases when multiple metrics are allowed. 
 
    Parameters 
    ---------- 
    estimator : sklearn estimator instance 
        The estimator for which the scoring will be applied. 
 
    scoring : list, tuple or dict 
        Strategy to evaluate the performance of the cross-validated model on 
        the test set. 
 
        The possibilities are: 
 
        - a list or tuple of unique strings; 
        - a callable returning a dictionary where they keys are the metric 
          names and the values are the metric scores; 
        - a dictionary with metric names as keys and callables a values. 
 
        See :ref:`multimetric_grid_search` for an example. 
 
    Returns 
    ------- 
    scorers_dict : dict 
        A dict mapping each scorer name to its validated scorer. 
    &quot;&quot;&quot;</span>
    <span class="s1">err_msg_generic = (</span>
        <span class="s4">f&quot;scoring is invalid (got </span><span class="s3">{</span><span class="s1">scoring</span><span class="s3">!r}</span><span class="s4">). Refer to the &quot;</span>
        <span class="s4">&quot;scoring glossary for details: &quot;</span>
        <span class="s4">&quot;https://scikit-learn.org/stable/glossary.html#term-scoring&quot;</span>
    <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">isinstance(scoring</span><span class="s3">, </span><span class="s1">(list</span><span class="s3">, </span><span class="s1">tuple</span><span class="s3">, </span><span class="s1">set)):</span>
        <span class="s1">err_msg = (</span>
            <span class="s4">&quot;The list/tuple elements must be unique strings of predefined scorers. &quot;</span>
        <span class="s1">)</span>
        <span class="s3">try</span><span class="s1">:</span>
            <span class="s1">keys = set(scoring)</span>
        <span class="s3">except </span><span class="s1">TypeError </span><span class="s3">as </span><span class="s1">e:</span>
            <span class="s3">raise </span><span class="s1">ValueError(err_msg) </span><span class="s3">from </span><span class="s1">e</span>

        <span class="s3">if </span><span class="s1">len(keys) != len(scoring):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">err_msg</span><span class="s3">} </span><span class="s4">Duplicate elements were found in&quot;</span>
                <span class="s4">f&quot; the given list. </span><span class="s3">{</span><span class="s1">scoring</span><span class="s3">!r}</span><span class="s4">&quot;</span>
            <span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">len(keys) &gt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s3">if not </span><span class="s1">all(isinstance(k</span><span class="s3">, </span><span class="s1">str) </span><span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">keys):</span>
                <span class="s3">if </span><span class="s1">any(callable(k) </span><span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">keys):</span>
                    <span class="s3">raise </span><span class="s1">ValueError(</span>
                        <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">err_msg</span><span class="s3">} </span><span class="s4">One or more of the elements &quot;</span>
                        <span class="s4">&quot;were callables. Use a dict of score &quot;</span>
                        <span class="s4">&quot;name mapped to the scorer callable. &quot;</span>
                        <span class="s4">f&quot;Got </span><span class="s3">{</span><span class="s1">scoring</span><span class="s3">!r}</span><span class="s4">&quot;</span>
                    <span class="s1">)</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s3">raise </span><span class="s1">ValueError(</span>
                        <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">err_msg</span><span class="s3">} </span><span class="s4">Non-string types were found &quot;</span>
                        <span class="s4">f&quot;in the given list. Got </span><span class="s3">{</span><span class="s1">scoring</span><span class="s3">!r}</span><span class="s4">&quot;</span>
                    <span class="s1">)</span>
            <span class="s1">scorers = {</span>
                <span class="s1">scorer: check_scoring(estimator</span><span class="s3">, </span><span class="s1">scoring=scorer) </span><span class="s3">for </span><span class="s1">scorer </span><span class="s3">in </span><span class="s1">scoring</span>
            <span class="s1">}</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">err_msg</span><span class="s3">} </span><span class="s4">Empty list was given. </span><span class="s3">{</span><span class="s1">scoring</span><span class="s3">!r}</span><span class="s4">&quot;</span><span class="s1">)</span>

    <span class="s3">elif </span><span class="s1">isinstance(scoring</span><span class="s3">, </span><span class="s1">dict):</span>
        <span class="s1">keys = set(scoring)</span>
        <span class="s3">if not </span><span class="s1">all(isinstance(k</span><span class="s3">, </span><span class="s1">str) </span><span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">keys):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;Non-string types were found in the keys of &quot;</span>
                <span class="s4">f&quot;the given dict. scoring=</span><span class="s3">{</span><span class="s1">scoring</span><span class="s3">!r}</span><span class="s4">&quot;</span>
            <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">len(keys) == </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">f&quot;An empty dict was passed. </span><span class="s3">{</span><span class="s1">scoring</span><span class="s3">!r}</span><span class="s4">&quot;</span><span class="s1">)</span>
        <span class="s1">scorers = {</span>
            <span class="s1">key: check_scoring(estimator</span><span class="s3">, </span><span class="s1">scoring=scorer)</span>
            <span class="s3">for </span><span class="s1">key</span><span class="s3">, </span><span class="s1">scorer </span><span class="s3">in </span><span class="s1">scoring.items()</span>
        <span class="s1">}</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(err_msg_generic)</span>
    <span class="s3">return </span><span class="s1">scorers</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s4">&quot;score_func&quot;</span><span class="s1">: [callable]</span><span class="s3">,</span>
        <span class="s4">&quot;greater_is_better&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;needs_proba&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;needs_threshold&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">True,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">make_scorer(</span>
    <span class="s1">score_func</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">greater_is_better=</span><span class="s3">True,</span>
    <span class="s1">needs_proba=</span><span class="s3">False,</span>
    <span class="s1">needs_threshold=</span><span class="s3">False,</span>
    <span class="s1">**kwargs</span><span class="s3">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Make a scorer from a performance metric or loss function. 
 
    This factory function wraps scoring functions for use in 
    :class:`~sklearn.model_selection.GridSearchCV` and 
    :func:`~sklearn.model_selection.cross_val_score`. 
    It takes a score function, such as :func:`~sklearn.metrics.accuracy_score`, 
    :func:`~sklearn.metrics.mean_squared_error`, 
    :func:`~sklearn.metrics.adjusted_rand_score` or 
    :func:`~sklearn.metrics.average_precision_score` 
    and returns a callable that scores an estimator's output. 
    The signature of the call is `(estimator, X, y)` where `estimator` 
    is the model to be evaluated, `X` is the data and `y` is the 
    ground truth labeling (or `None` in the case of unsupervised models). 
 
    Read more in the :ref:`User Guide &lt;scoring&gt;`. 
 
    Parameters 
    ---------- 
    score_func : callable 
        Score function (or loss function) with signature 
        ``score_func(y, y_pred, **kwargs)``. 
 
    greater_is_better : bool, default=True 
        Whether `score_func` is a score function (default), meaning high is 
        good, or a loss function, meaning low is good. In the latter case, the 
        scorer object will sign-flip the outcome of the `score_func`. 
 
    needs_proba : bool, default=False 
        Whether `score_func` requires `predict_proba` to get probability 
        estimates out of a classifier. 
 
        If True, for binary `y_true`, the score function is supposed to accept 
        a 1D `y_pred` (i.e., probability of the positive class, shape 
        `(n_samples,)`). 
 
    needs_threshold : bool, default=False 
        Whether `score_func` takes a continuous decision certainty. 
        This only works for binary classification using estimators that 
        have either a `decision_function` or `predict_proba` method. 
 
        If True, for binary `y_true`, the score function is supposed to accept 
        a 1D `y_pred` (i.e., probability of the positive class or the decision 
        function, shape `(n_samples,)`). 
 
        For example `average_precision` or the area under the roc curve 
        can not be computed using discrete predictions alone. 
 
    **kwargs : additional arguments 
        Additional parameters to be passed to `score_func`. 
 
    Returns 
    ------- 
    scorer : callable 
        Callable object that returns a scalar score; greater is better. 
 
    Notes 
    ----- 
    If `needs_proba=False` and `needs_threshold=False`, the score 
    function is supposed to accept the output of :term:`predict`. If 
    `needs_proba=True`, the score function is supposed to accept the 
    output of :term:`predict_proba` (For binary `y_true`, the score function is 
    supposed to accept probability of the positive class). If 
    `needs_threshold=True`, the score function is supposed to accept the 
    output of :term:`decision_function` or :term:`predict_proba` when 
    :term:`decision_function` is not present. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.metrics import fbeta_score, make_scorer 
    &gt;&gt;&gt; ftwo_scorer = make_scorer(fbeta_score, beta=2) 
    &gt;&gt;&gt; ftwo_scorer 
    make_scorer(fbeta_score, beta=2) 
    &gt;&gt;&gt; from sklearn.model_selection import GridSearchCV 
    &gt;&gt;&gt; from sklearn.svm import LinearSVC 
    &gt;&gt;&gt; grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, 
    ...                     scoring=ftwo_scorer) 
    &quot;&quot;&quot;</span>
    <span class="s1">sign = </span><span class="s5">1 </span><span class="s3">if </span><span class="s1">greater_is_better </span><span class="s3">else </span><span class="s1">-</span><span class="s5">1</span>
    <span class="s3">if </span><span class="s1">needs_proba </span><span class="s3">and </span><span class="s1">needs_threshold:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;Set either needs_proba or needs_threshold to True, but not both.&quot;</span>
        <span class="s1">)</span>
    <span class="s3">if </span><span class="s1">needs_proba:</span>
        <span class="s1">cls = _ProbaScorer</span>
    <span class="s3">elif </span><span class="s1">needs_threshold:</span>
        <span class="s1">cls = _ThresholdScorer</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">cls = _PredictScorer</span>
    <span class="s3">return </span><span class="s1">cls(score_func</span><span class="s3">, </span><span class="s1">sign</span><span class="s3">, </span><span class="s1">kwargs)</span>


<span class="s2"># Standard regression scores</span>
<span class="s1">explained_variance_scorer = make_scorer(explained_variance_score)</span>
<span class="s1">r2_scorer = make_scorer(r2_score)</span>
<span class="s1">max_error_scorer = make_scorer(max_error</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">False</span><span class="s1">)</span>
<span class="s1">neg_mean_squared_error_scorer = make_scorer(mean_squared_error</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">False</span><span class="s1">)</span>
<span class="s1">neg_mean_squared_log_error_scorer = make_scorer(</span>
    <span class="s1">mean_squared_log_error</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">False</span>
<span class="s1">)</span>
<span class="s1">neg_mean_absolute_error_scorer = make_scorer(</span>
    <span class="s1">mean_absolute_error</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">False</span>
<span class="s1">)</span>
<span class="s1">neg_mean_absolute_percentage_error_scorer = make_scorer(</span>
    <span class="s1">mean_absolute_percentage_error</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">False</span>
<span class="s1">)</span>
<span class="s1">neg_median_absolute_error_scorer = make_scorer(</span>
    <span class="s1">median_absolute_error</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">False</span>
<span class="s1">)</span>
<span class="s1">neg_root_mean_squared_error_scorer = make_scorer(</span>
    <span class="s1">mean_squared_error</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">False, </span><span class="s1">squared=</span><span class="s3">False</span>
<span class="s1">)</span>
<span class="s1">neg_mean_poisson_deviance_scorer = make_scorer(</span>
    <span class="s1">mean_poisson_deviance</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">False</span>
<span class="s1">)</span>

<span class="s1">neg_mean_gamma_deviance_scorer = make_scorer(</span>
    <span class="s1">mean_gamma_deviance</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">False</span>
<span class="s1">)</span>

<span class="s2"># Standard Classification Scores</span>
<span class="s1">accuracy_scorer = make_scorer(accuracy_score)</span>
<span class="s1">balanced_accuracy_scorer = make_scorer(balanced_accuracy_score)</span>
<span class="s1">matthews_corrcoef_scorer = make_scorer(matthews_corrcoef)</span>


<span class="s3">def </span><span class="s1">positive_likelihood_ratio(y_true</span><span class="s3">, </span><span class="s1">y_pred):</span>
    <span class="s3">return </span><span class="s1">class_likelihood_ratios(y_true</span><span class="s3">, </span><span class="s1">y_pred)[</span><span class="s5">0</span><span class="s1">]</span>


<span class="s3">def </span><span class="s1">negative_likelihood_ratio(y_true</span><span class="s3">, </span><span class="s1">y_pred):</span>
    <span class="s3">return </span><span class="s1">class_likelihood_ratios(y_true</span><span class="s3">, </span><span class="s1">y_pred)[</span><span class="s5">1</span><span class="s1">]</span>


<span class="s1">positive_likelihood_ratio_scorer = make_scorer(positive_likelihood_ratio)</span>
<span class="s1">neg_negative_likelihood_ratio_scorer = make_scorer(</span>
    <span class="s1">negative_likelihood_ratio</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">False</span>
<span class="s1">)</span>

<span class="s2"># Score functions that need decision values</span>
<span class="s1">top_k_accuracy_scorer = make_scorer(</span>
    <span class="s1">top_k_accuracy_score</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">True, </span><span class="s1">needs_threshold=</span><span class="s3">True</span>
<span class="s1">)</span>
<span class="s1">roc_auc_scorer = make_scorer(</span>
    <span class="s1">roc_auc_score</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">True, </span><span class="s1">needs_threshold=</span><span class="s3">True</span>
<span class="s1">)</span>
<span class="s1">average_precision_scorer = make_scorer(average_precision_score</span><span class="s3">, </span><span class="s1">needs_threshold=</span><span class="s3">True</span><span class="s1">)</span>
<span class="s1">roc_auc_ovo_scorer = make_scorer(roc_auc_score</span><span class="s3">, </span><span class="s1">needs_proba=</span><span class="s3">True, </span><span class="s1">multi_class=</span><span class="s4">&quot;ovo&quot;</span><span class="s1">)</span>
<span class="s1">roc_auc_ovo_weighted_scorer = make_scorer(</span>
    <span class="s1">roc_auc_score</span><span class="s3">, </span><span class="s1">needs_proba=</span><span class="s3">True, </span><span class="s1">multi_class=</span><span class="s4">&quot;ovo&quot;</span><span class="s3">, </span><span class="s1">average=</span><span class="s4">&quot;weighted&quot;</span>
<span class="s1">)</span>
<span class="s1">roc_auc_ovr_scorer = make_scorer(roc_auc_score</span><span class="s3">, </span><span class="s1">needs_proba=</span><span class="s3">True, </span><span class="s1">multi_class=</span><span class="s4">&quot;ovr&quot;</span><span class="s1">)</span>
<span class="s1">roc_auc_ovr_weighted_scorer = make_scorer(</span>
    <span class="s1">roc_auc_score</span><span class="s3">, </span><span class="s1">needs_proba=</span><span class="s3">True, </span><span class="s1">multi_class=</span><span class="s4">&quot;ovr&quot;</span><span class="s3">, </span><span class="s1">average=</span><span class="s4">&quot;weighted&quot;</span>
<span class="s1">)</span>

<span class="s2"># Score function for probabilistic classification</span>
<span class="s1">neg_log_loss_scorer = make_scorer(log_loss</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">False, </span><span class="s1">needs_proba=</span><span class="s3">True</span><span class="s1">)</span>
<span class="s1">neg_brier_score_scorer = make_scorer(</span>
    <span class="s1">brier_score_loss</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">False, </span><span class="s1">needs_proba=</span><span class="s3">True</span>
<span class="s1">)</span>
<span class="s1">brier_score_loss_scorer = make_scorer(</span>
    <span class="s1">brier_score_loss</span><span class="s3">, </span><span class="s1">greater_is_better=</span><span class="s3">False, </span><span class="s1">needs_proba=</span><span class="s3">True</span>
<span class="s1">)</span>


<span class="s2"># Clustering scores</span>
<span class="s1">adjusted_rand_scorer = make_scorer(adjusted_rand_score)</span>
<span class="s1">rand_scorer = make_scorer(rand_score)</span>
<span class="s1">homogeneity_scorer = make_scorer(homogeneity_score)</span>
<span class="s1">completeness_scorer = make_scorer(completeness_score)</span>
<span class="s1">v_measure_scorer = make_scorer(v_measure_score)</span>
<span class="s1">mutual_info_scorer = make_scorer(mutual_info_score)</span>
<span class="s1">adjusted_mutual_info_scorer = make_scorer(adjusted_mutual_info_score)</span>
<span class="s1">normalized_mutual_info_scorer = make_scorer(normalized_mutual_info_score)</span>
<span class="s1">fowlkes_mallows_scorer = make_scorer(fowlkes_mallows_score)</span>


<span class="s1">_SCORERS = dict(</span>
    <span class="s1">explained_variance=explained_variance_scorer</span><span class="s3">,</span>
    <span class="s1">r2=r2_scorer</span><span class="s3">,</span>
    <span class="s1">max_error=max_error_scorer</span><span class="s3">,</span>
    <span class="s1">matthews_corrcoef=matthews_corrcoef_scorer</span><span class="s3">,</span>
    <span class="s1">neg_median_absolute_error=neg_median_absolute_error_scorer</span><span class="s3">,</span>
    <span class="s1">neg_mean_absolute_error=neg_mean_absolute_error_scorer</span><span class="s3">,</span>
    <span class="s1">neg_mean_absolute_percentage_error=neg_mean_absolute_percentage_error_scorer</span><span class="s3">,  </span><span class="s2"># noqa</span>
    <span class="s1">neg_mean_squared_error=neg_mean_squared_error_scorer</span><span class="s3">,</span>
    <span class="s1">neg_mean_squared_log_error=neg_mean_squared_log_error_scorer</span><span class="s3">,</span>
    <span class="s1">neg_root_mean_squared_error=neg_root_mean_squared_error_scorer</span><span class="s3">,</span>
    <span class="s1">neg_mean_poisson_deviance=neg_mean_poisson_deviance_scorer</span><span class="s3">,</span>
    <span class="s1">neg_mean_gamma_deviance=neg_mean_gamma_deviance_scorer</span><span class="s3">,</span>
    <span class="s1">accuracy=accuracy_scorer</span><span class="s3">,</span>
    <span class="s1">top_k_accuracy=top_k_accuracy_scorer</span><span class="s3">,</span>
    <span class="s1">roc_auc=roc_auc_scorer</span><span class="s3">,</span>
    <span class="s1">roc_auc_ovr=roc_auc_ovr_scorer</span><span class="s3">,</span>
    <span class="s1">roc_auc_ovo=roc_auc_ovo_scorer</span><span class="s3">,</span>
    <span class="s1">roc_auc_ovr_weighted=roc_auc_ovr_weighted_scorer</span><span class="s3">,</span>
    <span class="s1">roc_auc_ovo_weighted=roc_auc_ovo_weighted_scorer</span><span class="s3">,</span>
    <span class="s1">balanced_accuracy=balanced_accuracy_scorer</span><span class="s3">,</span>
    <span class="s1">average_precision=average_precision_scorer</span><span class="s3">,</span>
    <span class="s1">neg_log_loss=neg_log_loss_scorer</span><span class="s3">,</span>
    <span class="s1">neg_brier_score=neg_brier_score_scorer</span><span class="s3">,</span>
    <span class="s1">positive_likelihood_ratio=positive_likelihood_ratio_scorer</span><span class="s3">,</span>
    <span class="s1">neg_negative_likelihood_ratio=neg_negative_likelihood_ratio_scorer</span><span class="s3">,</span>
    <span class="s2"># Cluster metrics that use supervised evaluation</span>
    <span class="s1">adjusted_rand_score=adjusted_rand_scorer</span><span class="s3">,</span>
    <span class="s1">rand_score=rand_scorer</span><span class="s3">,</span>
    <span class="s1">homogeneity_score=homogeneity_scorer</span><span class="s3">,</span>
    <span class="s1">completeness_score=completeness_scorer</span><span class="s3">,</span>
    <span class="s1">v_measure_score=v_measure_scorer</span><span class="s3">,</span>
    <span class="s1">mutual_info_score=mutual_info_scorer</span><span class="s3">,</span>
    <span class="s1">adjusted_mutual_info_score=adjusted_mutual_info_scorer</span><span class="s3">,</span>
    <span class="s1">normalized_mutual_info_score=normalized_mutual_info_scorer</span><span class="s3">,</span>
    <span class="s1">fowlkes_mallows_score=fowlkes_mallows_scorer</span><span class="s3">,</span>
<span class="s1">)</span>


<span class="s3">def </span><span class="s1">get_scorer_names():</span>
    <span class="s0">&quot;&quot;&quot;Get the names of all available scorers. 
 
    These names can be passed to :func:`~sklearn.metrics.get_scorer` to 
    retrieve the scorer object. 
 
    Returns 
    ------- 
    list of str 
        Names of all available scorers. 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">sorted(_SCORERS.keys())</span>


<span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">metric </span><span class="s3">in </span><span class="s1">[</span>
    <span class="s1">(</span><span class="s4">&quot;precision&quot;</span><span class="s3">, </span><span class="s1">precision_score)</span><span class="s3">,</span>
    <span class="s1">(</span><span class="s4">&quot;recall&quot;</span><span class="s3">, </span><span class="s1">recall_score)</span><span class="s3">,</span>
    <span class="s1">(</span><span class="s4">&quot;f1&quot;</span><span class="s3">, </span><span class="s1">f1_score)</span><span class="s3">,</span>
    <span class="s1">(</span><span class="s4">&quot;jaccard&quot;</span><span class="s3">, </span><span class="s1">jaccard_score)</span><span class="s3">,</span>
<span class="s1">]:</span>
    <span class="s1">_SCORERS[name] = make_scorer(metric</span><span class="s3">, </span><span class="s1">average=</span><span class="s4">&quot;binary&quot;</span><span class="s1">)</span>
    <span class="s3">for </span><span class="s1">average </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;macro&quot;</span><span class="s3">, </span><span class="s4">&quot;micro&quot;</span><span class="s3">, </span><span class="s4">&quot;samples&quot;</span><span class="s3">, </span><span class="s4">&quot;weighted&quot;</span><span class="s1">]:</span>
        <span class="s1">qualified_name = </span><span class="s4">&quot;{0}_{1}&quot;</span><span class="s1">.format(name</span><span class="s3">, </span><span class="s1">average)</span>
        <span class="s1">_SCORERS[qualified_name] = make_scorer(metric</span><span class="s3">, </span><span class="s1">pos_label=</span><span class="s3">None, </span><span class="s1">average=average)</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s4">&quot;estimator&quot;</span><span class="s1">: [HasMethods(</span><span class="s4">&quot;fit&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;scoring&quot;</span><span class="s1">: [StrOptions(set(get_scorer_names()))</span><span class="s3">, </span><span class="s1">callable</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;allow_none&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">True,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">check_scoring(estimator</span><span class="s3">, </span><span class="s1">scoring=</span><span class="s3">None, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">allow_none=</span><span class="s3">False</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Determine scorer from user options. 
 
    A TypeError will be thrown if the estimator cannot be scored. 
 
    Parameters 
    ---------- 
    estimator : estimator object implementing 'fit' 
        The object to use to fit the data. 
 
    scoring : str or callable, default=None 
        A string (see model evaluation documentation) or 
        a scorer callable object / function with signature 
        ``scorer(estimator, X, y)``. 
        If None, the provided estimator object's `score` method is used. 
 
    allow_none : bool, default=False 
        If no scoring is specified and the estimator has no score function, we 
        can either return None or raise an exception. 
 
    Returns 
    ------- 
    scoring : callable 
        A scorer callable object / function with signature 
        ``scorer(estimator, X, y)``. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">isinstance(scoring</span><span class="s3">, </span><span class="s1">str):</span>
        <span class="s3">return </span><span class="s1">get_scorer(scoring)</span>
    <span class="s3">if </span><span class="s1">callable(scoring):</span>
        <span class="s2"># Heuristic to ensure user has not passed a metric</span>
        <span class="s1">module = getattr(scoring</span><span class="s3">, </span><span class="s4">&quot;__module__&quot;</span><span class="s3">, None</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">(</span>
            <span class="s1">hasattr(module</span><span class="s3">, </span><span class="s4">&quot;startswith&quot;</span><span class="s1">)</span>
            <span class="s3">and </span><span class="s1">module.startswith(</span><span class="s4">&quot;sklearn.metrics.&quot;</span><span class="s1">)</span>
            <span class="s3">and not </span><span class="s1">module.startswith(</span><span class="s4">&quot;sklearn.metrics._scorer&quot;</span><span class="s1">)</span>
            <span class="s3">and not </span><span class="s1">module.startswith(</span><span class="s4">&quot;sklearn.metrics.tests.&quot;</span><span class="s1">)</span>
        <span class="s1">):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;scoring value %r looks like it is a metric &quot;</span>
                <span class="s4">&quot;function rather than a scorer. A scorer should &quot;</span>
                <span class="s4">&quot;require an estimator as its first parameter. &quot;</span>
                <span class="s4">&quot;Please use `make_scorer` to convert a metric &quot;</span>
                <span class="s4">&quot;to a scorer.&quot; </span><span class="s1">% scoring</span>
            <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">get_scorer(scoring)</span>
    <span class="s3">if </span><span class="s1">scoring </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">hasattr(estimator</span><span class="s3">, </span><span class="s4">&quot;score&quot;</span><span class="s1">):</span>
            <span class="s3">return </span><span class="s1">_PassthroughScorer(estimator)</span>
        <span class="s3">elif </span><span class="s1">allow_none:</span>
            <span class="s3">return None</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">TypeError(</span>
                <span class="s4">&quot;If no scoring is specified, the estimator passed should &quot;</span>
                <span class="s4">&quot;have a 'score' method. The estimator %r does not.&quot; </span><span class="s1">% estimator</span>
            <span class="s1">)</span>
</pre>
</body>
</html>