<html>
<head>
<title>robust_linear_model.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
robust_linear_model.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Robust linear models with support for the M-estimators  listed under 
:ref:`norms &lt;norms&gt;`. 
 
References 
---------- 
PJ Huber.  'Robust Statistics' John Wiley and Sons, Inc., New York.  1981. 
 
PJ Huber.  1973,  'The 1972 Wald Memorial Lectures: Robust Regression: 
    Asymptotics, Conjectures, and Monte Carlo.'  The Annals of Statistics, 
    1.5, 799-821. 
 
R Venables, B Ripley. 'Modern Applied Statistics in S'  Springer, New York, 
    2002. 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">scipy.stats </span><span class="s2">as </span><span class="s1">stats</span>

<span class="s2">import </span><span class="s1">statsmodels.base.model </span><span class="s2">as </span><span class="s1">base</span>
<span class="s2">import </span><span class="s1">statsmodels.base.wrapper </span><span class="s2">as </span><span class="s1">wrap</span>
<span class="s2">import </span><span class="s1">statsmodels.regression._tools </span><span class="s2">as </span><span class="s1">reg_tools</span>
<span class="s2">import </span><span class="s1">statsmodels.regression.linear_model </span><span class="s2">as </span><span class="s1">lm</span>
<span class="s2">import </span><span class="s1">statsmodels.robust.norms </span><span class="s2">as </span><span class="s1">norms</span>
<span class="s2">import </span><span class="s1">statsmodels.robust.scale </span><span class="s2">as </span><span class="s1">scale</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.decorators </span><span class="s2">import </span><span class="s1">cache_readonly</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s2">import </span><span class="s1">ConvergenceWarning</span>

<span class="s1">__all__ = [</span><span class="s3">'RLM'</span><span class="s1">]</span>


<span class="s2">def </span><span class="s1">_check_convergence(criterion</span><span class="s2">, </span><span class="s1">iteration</span><span class="s2">, </span><span class="s1">tol</span><span class="s2">, </span><span class="s1">maxiter):</span>
    <span class="s1">cond = np.abs(criterion[iteration] - criterion[iteration - </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s2">return not </span><span class="s1">(np.any(cond &gt; tol) </span><span class="s2">and </span><span class="s1">iteration &lt; maxiter)</span>


<span class="s2">class </span><span class="s1">RLM(base.LikelihoodModel):</span>
    <span class="s1">__doc__ = </span><span class="s3">&quot;&quot;&quot; 
    Robust Linear Model 
 
    Estimate a robust linear model via iteratively reweighted least squares 
    given a robust criterion estimator. 
 
    %(params)s 
    M : statsmodels.robust.norms.RobustNorm, optional 
        The robust criterion function for downweighting outliers. 
        The current options are LeastSquares, HuberT, RamsayE, AndrewWave, 
        TrimmedMean, Hampel, and TukeyBiweight.  The default is HuberT(). 
        See statsmodels.robust.norms for more information. 
    %(extra_params)s 
 
    Attributes 
    ---------- 
 
    df_model : float 
        The degrees of freedom of the model.  The number of regressors p less 
        one for the intercept.  Note that the reported model degrees 
        of freedom does not count the intercept as a regressor, though 
        the model is assumed to have an intercept. 
    df_resid : float 
        The residual degrees of freedom.  The number of observations n 
        less the number of regressors p.  Note that here p does include 
        the intercept as using a degree of freedom. 
    endog : ndarray 
        See above.  Note that endog is a reference to the data so that if 
        data is already an array and it is changed, then `endog` changes 
        as well. 
    exog : ndarray 
        See above.  Note that endog is a reference to the data so that if 
        data is already an array and it is changed, then `endog` changes 
        as well. 
    M : statsmodels.robust.norms.RobustNorm 
         See above.  Robust estimator instance instantiated. 
    nobs : float 
        The number of observations n 
    pinv_wexog : ndarray 
        The pseudoinverse of the design / exogenous data array.  Note that 
        RLM has no whiten method, so this is just the pseudo inverse of the 
        design. 
    normalized_cov_params : ndarray 
        The p x p normalized covariance of the design / exogenous data. 
        This is approximately equal to (X.T X)^(-1) 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import statsmodels.api as sm 
    &gt;&gt;&gt; data = sm.datasets.stackloss.load() 
    &gt;&gt;&gt; data.exog = sm.add_constant(data.exog) 
    &gt;&gt;&gt; rlm_model = sm.RLM(data.endog, data.exog, </span><span class="s2">\ 
                           </span><span class="s3">M=sm.robust.norms.HuberT()) 
 
    &gt;&gt;&gt; rlm_results = rlm_model.fit() 
    &gt;&gt;&gt; rlm_results.params 
    array([  0.82938433,   0.92606597,  -0.12784672, -41.02649835]) 
    &gt;&gt;&gt; rlm_results.bse 
    array([ 0.11100521,  0.30293016,  0.12864961,  9.79189854]) 
    &gt;&gt;&gt; rlm_results_HC2 = rlm_model.fit(cov=&quot;H2&quot;) 
    &gt;&gt;&gt; rlm_results_HC2.params 
    array([  0.82938433,   0.92606597,  -0.12784672, -41.02649835]) 
    &gt;&gt;&gt; rlm_results_HC2.bse 
    array([ 0.11945975,  0.32235497,  0.11796313,  9.08950419]) 
    &gt;&gt;&gt; mod = sm.RLM(data.endog, data.exog, M=sm.robust.norms.Hampel()) 
    &gt;&gt;&gt; rlm_hamp_hub = mod.fit(scale_est=sm.robust.scale.HuberScale()) 
    &gt;&gt;&gt; rlm_hamp_hub.params 
    array([  0.73175452,   1.25082038,  -0.14794399, -40.27122257]) 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s3">'params'</span><span class="s1">: base._model_params_doc</span><span class="s2">,</span>
           <span class="s3">'extra_params'</span><span class="s1">: base._missing_param_doc}</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">M=</span><span class="s2">None, </span><span class="s1">missing=</span><span class="s3">'none'</span><span class="s2">,</span>
                 <span class="s1">**kwargs):</span>
        <span class="s1">self._check_kwargs(kwargs)</span>
        <span class="s1">self.M = M </span><span class="s2">if </span><span class="s1">M </span><span class="s2">is not None else </span><span class="s1">norms.HuberT()</span>
        <span class="s1">super(base.LikelihoodModel</span><span class="s2">, </span><span class="s1">self).__init__(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">,</span>
                                                   <span class="s1">missing=missing</span><span class="s2">, </span><span class="s1">**kwargs)</span>
        <span class="s1">self._initialize()</span>
        <span class="s5"># things to remove_data</span>
        <span class="s1">self._data_attr.extend([</span><span class="s3">'weights'</span><span class="s2">, </span><span class="s3">'pinv_wexog'</span><span class="s1">])</span>

    <span class="s2">def </span><span class="s1">_initialize(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Initializes the model for the IRLS fit. 
 
        Resets the history and number of iterations. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.pinv_wexog = np.linalg.pinv(self.exog)</span>
        <span class="s1">self.normalized_cov_params = np.dot(self.pinv_wexog</span><span class="s2">,</span>
                                            <span class="s1">np.transpose(self.pinv_wexog))</span>
        <span class="s1">self.df_resid = (float(self.exog.shape[</span><span class="s4">0</span><span class="s1">] -</span>
                               <span class="s1">np.linalg.matrix_rank(self.exog)))</span>
        <span class="s1">self.df_model = float(np.linalg.matrix_rank(self.exog) - </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">self.nobs = float(self.endog.shape[</span><span class="s4">0</span><span class="s1">])</span>

    <span class="s2">def </span><span class="s1">score(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError</span>

    <span class="s2">def </span><span class="s1">information(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">exog=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return linear predicted values from a design matrix. 
 
        Parameters 
        ---------- 
        params : array_like 
            Parameters of a linear model 
        exog : array_like, optional. 
            Design / exogenous data. Model exog is used if None. 
 
        Returns 
        ------- 
        An array of fitted values 
        &quot;&quot;&quot;</span>
        <span class="s5"># copied from linear_model  # TODO: then is it needed?</span>
        <span class="s2">if </span><span class="s1">exog </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>
        <span class="s2">return </span><span class="s1">np.dot(exog</span><span class="s2">, </span><span class="s1">params)</span>

    <span class="s2">def </span><span class="s1">loglike(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError</span>

    <span class="s2">def </span><span class="s1">deviance(self</span><span class="s2">, </span><span class="s1">tmp_results):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the (unnormalized) log-likelihood from the M estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">tmp_resid = self.endog - tmp_results.fittedvalues</span>
        <span class="s2">return </span><span class="s1">self.M(tmp_resid / tmp_results.scale).sum()</span>

    <span class="s2">def </span><span class="s1">_update_history(self</span><span class="s2">, </span><span class="s1">tmp_results</span><span class="s2">, </span><span class="s1">history</span><span class="s2">, </span><span class="s1">conv):</span>
        <span class="s1">history[</span><span class="s3">'params'</span><span class="s1">].append(tmp_results.params)</span>
        <span class="s1">history[</span><span class="s3">'scale'</span><span class="s1">].append(tmp_results.scale)</span>
        <span class="s2">if </span><span class="s1">conv == </span><span class="s3">'dev'</span><span class="s1">:</span>
            <span class="s1">history[</span><span class="s3">'deviance'</span><span class="s1">].append(self.deviance(tmp_results))</span>
        <span class="s2">elif </span><span class="s1">conv == </span><span class="s3">'sresid'</span><span class="s1">:</span>
            <span class="s1">history[</span><span class="s3">'sresid'</span><span class="s1">].append(tmp_results.resid / tmp_results.scale)</span>
        <span class="s2">elif </span><span class="s1">conv == </span><span class="s3">'weights'</span><span class="s1">:</span>
            <span class="s1">history[</span><span class="s3">'weights'</span><span class="s1">].append(tmp_results.model.weights)</span>
        <span class="s2">return </span><span class="s1">history</span>

    <span class="s2">def </span><span class="s1">_estimate_scale(self</span><span class="s2">, </span><span class="s1">resid):</span>
        <span class="s0">&quot;&quot;&quot; 
        Estimates the scale based on the option provided to the fit method. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">isinstance(self.scale_est</span><span class="s2">, </span><span class="s1">str):</span>
            <span class="s2">if </span><span class="s1">self.scale_est.lower() == </span><span class="s3">'mad'</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">scale.mad(resid</span><span class="s2">, </span><span class="s1">center=</span><span class="s4">0</span><span class="s1">)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Option %s for scale_est not understood&quot; </span><span class="s1">%</span>
                                 <span class="s1">self.scale_est)</span>
        <span class="s2">elif </span><span class="s1">isinstance(self.scale_est</span><span class="s2">, </span><span class="s1">scale.HuberScale):</span>
            <span class="s2">return </span><span class="s1">self.scale_est(self.df_resid</span><span class="s2">, </span><span class="s1">self.nobs</span><span class="s2">, </span><span class="s1">resid)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">scale.scale_est(self</span><span class="s2">, </span><span class="s1">resid) ** </span><span class="s4">2</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">tol=</span><span class="s4">1e-8</span><span class="s2">, </span><span class="s1">scale_est=</span><span class="s3">'mad'</span><span class="s2">, </span><span class="s1">init=</span><span class="s2">None, </span><span class="s1">cov=</span><span class="s3">'H1'</span><span class="s2">,</span>
            <span class="s1">update_scale=</span><span class="s2">True, </span><span class="s1">conv=</span><span class="s3">'dev'</span><span class="s2">, </span><span class="s1">start_params=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fits the model using iteratively reweighted least squares. 
 
        The IRLS routine runs until the specified objective converges to `tol` 
        or `maxiter` has been reached. 
 
        Parameters 
        ---------- 
        conv : str 
            Indicates the convergence criteria. 
            Available options are &quot;coefs&quot; (the coefficients), &quot;weights&quot; (the 
            weights in the iteration), &quot;sresid&quot; (the standardized residuals), 
            and &quot;dev&quot; (the un-normalized log-likelihood for the M 
            estimator).  The default is &quot;dev&quot;. 
        cov : str, optional 
            'H1', 'H2', or 'H3' 
            Indicates how the covariance matrix is estimated.  Default is 'H1'. 
            See rlm.RLMResults for more information. 
        init : str 
            Specifies method for the initial estimates of the parameters. 
            Default is None, which means that the least squares estimate 
            is used.  Currently it is the only available choice. 
        maxiter : int 
            The maximum number of iterations to try. Default is 50. 
        scale_est : str or HuberScale() 
            'mad' or HuberScale() 
            Indicates the estimate to use for scaling the weights in the IRLS. 
            The default is 'mad' (median absolute deviation.  Other options are 
            'HuberScale' for Huber's proposal 2. Huber's proposal 2 has 
            optional keyword arguments d, tol, and maxiter for specifying the 
            tuning constant, the convergence tolerance, and the maximum number 
            of iterations. See statsmodels.robust.scale for more information. 
        tol : float 
            The convergence tolerance of the estimate.  Default is 1e-8. 
        update_scale : Bool 
            If `update_scale` is False then the scale estimate for the 
            weights is held constant over the iteration.  Otherwise, it 
            is updated for each fit in the iteration.  Default is True. 
        start_params : array_like, optional 
            Initial guess of the solution of the optimizer. If not provided, 
            the initial parameters are computed using OLS. 
 
        Returns 
        ------- 
        results : statsmodels.rlm.RLMresults 
            Results instance 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">cov.upper() </span><span class="s2">not in </span><span class="s1">[</span><span class="s3">&quot;H1&quot;</span><span class="s2">, </span><span class="s3">&quot;H2&quot;</span><span class="s2">, </span><span class="s3">&quot;H3&quot;</span><span class="s1">]:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Covariance matrix %s not understood&quot; </span><span class="s1">% cov)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self.cov = cov.upper()</span>
        <span class="s1">conv = conv.lower()</span>
        <span class="s2">if </span><span class="s1">conv </span><span class="s2">not in </span><span class="s1">[</span><span class="s3">&quot;weights&quot;</span><span class="s2">, </span><span class="s3">&quot;coefs&quot;</span><span class="s2">, </span><span class="s3">&quot;dev&quot;</span><span class="s2">, </span><span class="s3">&quot;sresid&quot;</span><span class="s1">]:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Convergence argument %s not understood&quot; </span><span class="s1">% conv)</span>
        <span class="s1">self.scale_est = scale_est</span>

        <span class="s2">if </span><span class="s1">start_params </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">wls_results = lm.WLS(self.endog</span><span class="s2">, </span><span class="s1">self.exog).fit()</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">start_params = np.asarray(start_params</span><span class="s2">, </span><span class="s1">dtype=np.double).squeeze()</span>
            <span class="s2">if </span><span class="s1">(start_params.shape[</span><span class="s4">0</span><span class="s1">] != self.exog.shape[</span><span class="s4">1</span><span class="s1">] </span><span class="s2">or</span>
                    <span class="s1">start_params.ndim != </span><span class="s4">1</span><span class="s1">):</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">'start_params must by a 1-d array with {0} '</span>
                                 <span class="s3">'values'</span><span class="s1">.format(self.exog.shape[</span><span class="s4">1</span><span class="s1">]))</span>
            <span class="s1">fake_wls = reg_tools._MinimalWLS(self.endog</span><span class="s2">, </span><span class="s1">self.exog</span><span class="s2">,</span>
                                             <span class="s1">weights=np.ones_like(self.endog)</span><span class="s2">,</span>
                                             <span class="s1">check_weights=</span><span class="s2">False</span><span class="s1">)</span>
            <span class="s1">wls_results = fake_wls.results(start_params)</span>

        <span class="s2">if not </span><span class="s1">init:</span>
            <span class="s1">self.scale = self._estimate_scale(wls_results.resid)</span>

        <span class="s1">history = dict(params=[np.inf]</span><span class="s2">, </span><span class="s1">scale=[])</span>
        <span class="s2">if </span><span class="s1">conv == </span><span class="s3">'coefs'</span><span class="s1">:</span>
            <span class="s1">criterion = history[</span><span class="s3">'params'</span><span class="s1">]</span>
        <span class="s2">elif </span><span class="s1">conv == </span><span class="s3">'dev'</span><span class="s1">:</span>
            <span class="s1">history.update(dict(deviance=[np.inf]))</span>
            <span class="s1">criterion = history[</span><span class="s3">'deviance'</span><span class="s1">]</span>
        <span class="s2">elif </span><span class="s1">conv == </span><span class="s3">'sresid'</span><span class="s1">:</span>
            <span class="s1">history.update(dict(sresid=[np.inf]))</span>
            <span class="s1">criterion = history[</span><span class="s3">'sresid'</span><span class="s1">]</span>
        <span class="s2">elif </span><span class="s1">conv == </span><span class="s3">'weights'</span><span class="s1">:</span>
            <span class="s1">history.update(dict(weights=[np.inf]))</span>
            <span class="s1">criterion = history[</span><span class="s3">'weights'</span><span class="s1">]</span>

        <span class="s5"># done one iteration so update</span>
        <span class="s1">history = self._update_history(wls_results</span><span class="s2">, </span><span class="s1">history</span><span class="s2">, </span><span class="s1">conv)</span>
        <span class="s1">iteration = </span><span class="s4">1</span>
        <span class="s1">converged = </span><span class="s4">0</span>
        <span class="s2">while not </span><span class="s1">converged:</span>
            <span class="s2">if </span><span class="s1">self.scale == </span><span class="s4">0.0</span><span class="s1">:</span>
                <span class="s2">import </span><span class="s1">warnings</span>
                <span class="s1">warnings.warn(</span><span class="s3">'Estimated scale is 0.0 indicating that the most'</span>
                              <span class="s3">' last iteration produced a perfect fit of the '</span>
                              <span class="s3">'weighted data.'</span><span class="s2">, </span><span class="s1">ConvergenceWarning)</span>
                <span class="s2">break</span>
            <span class="s1">self.weights = self.M.weights(wls_results.resid / self.scale)</span>
            <span class="s1">wls_results = reg_tools._MinimalWLS(self.endog</span><span class="s2">, </span><span class="s1">self.exog</span><span class="s2">,</span>
                                                <span class="s1">weights=self.weights</span><span class="s2">,</span>
                                                <span class="s1">check_weights=</span><span class="s2">True</span><span class="s1">).fit()</span>
            <span class="s2">if </span><span class="s1">update_scale </span><span class="s2">is True</span><span class="s1">:</span>
                <span class="s1">self.scale = self._estimate_scale(wls_results.resid)</span>
            <span class="s1">history = self._update_history(wls_results</span><span class="s2">, </span><span class="s1">history</span><span class="s2">, </span><span class="s1">conv)</span>
            <span class="s1">iteration += </span><span class="s4">1</span>
            <span class="s1">converged = _check_convergence(criterion</span><span class="s2">, </span><span class="s1">iteration</span><span class="s2">, </span><span class="s1">tol</span><span class="s2">, </span><span class="s1">maxiter)</span>
        <span class="s1">results = RLMResults(self</span><span class="s2">, </span><span class="s1">wls_results.params</span><span class="s2">,</span>
                             <span class="s1">self.normalized_cov_params</span><span class="s2">, </span><span class="s1">self.scale)</span>

        <span class="s1">history[</span><span class="s3">'iteration'</span><span class="s1">] = iteration</span>
        <span class="s1">results.fit_history = history</span>
        <span class="s1">results.fit_options = dict(cov=cov.upper()</span><span class="s2">, </span><span class="s1">scale_est=scale_est</span><span class="s2">,</span>
                                   <span class="s1">norm=self.M.__class__.__name__</span><span class="s2">, </span><span class="s1">conv=conv)</span>
        <span class="s5"># norm is not changed in fit, no old state</span>

        <span class="s5"># doing the next causes exception</span>
        <span class="s5"># self.cov = self.scale_est = None #reset for additional fits</span>
        <span class="s5"># iteration and history could contain wrong state with repeated fit</span>
        <span class="s2">return </span><span class="s1">RLMResultsWrapper(results)</span>


<span class="s2">class </span><span class="s1">RLMResults(base.LikelihoodModelResults):</span>
    <span class="s0">&quot;&quot;&quot; 
    Class to contain RLM results 
 
    Attributes 
    ---------- 
 
    bcov_scaled : ndarray 
        p x p scaled covariance matrix specified in the model fit method. 
        The default is H1. H1 is defined as 
        ``k**2 * (1/df_resid*sum(M.psi(sresid)**2)*scale**2)/ 
        ((1/nobs*sum(M.psi_deriv(sresid)))**2) * (X.T X)^(-1)`` 
 
        where ``k = 1 + (df_model +1)/nobs * var_psiprime/m**2`` 
        where ``m = mean(M.psi_deriv(sresid))`` and 
        ``var_psiprime = var(M.psi_deriv(sresid))`` 
 
        H2 is defined as 
        ``k * (1/df_resid) * sum(M.psi(sresid)**2) *scale**2/ 
        ((1/nobs)*sum(M.psi_deriv(sresid)))*W_inv`` 
 
        H3 is defined as 
        ``1/k * (1/df_resid * sum(M.psi(sresid)**2)*scale**2 * 
        (W_inv X.T X W_inv))`` 
 
        where `k` is defined as above and 
        ``W_inv = (M.psi_deriv(sresid) exog.T exog)^(-1)`` 
 
        See the technical documentation for cleaner formulae. 
    bcov_unscaled : ndarray 
        The usual p x p covariance matrix with scale set equal to 1.  It 
        is then just equivalent to normalized_cov_params. 
    bse : ndarray 
        An array of the standard errors of the parameters.  The standard 
        errors are taken from the robust covariance matrix specified in the 
        argument to fit. 
    chisq : ndarray 
        An array of the chi-squared values of the parameter estimates. 
    df_model 
        See RLM.df_model 
    df_resid 
        See RLM.df_resid 
    fit_history : dict 
        Contains information about the iterations. Its keys are `deviance`, 
        `params`, `iteration` and the convergence criteria specified in 
        `RLM.fit`, if different from `deviance` or `params`. 
    fit_options : dict 
        Contains the options given to fit. 
    fittedvalues : ndarray 
        The linear predicted values.  dot(exog, params) 
    model : statsmodels.rlm.RLM 
        A reference to the model instance 
    nobs : float 
        The number of observations n 
    normalized_cov_params : ndarray 
        See RLM.normalized_cov_params 
    params : ndarray 
        The coefficients of the fitted model 
    pinv_wexog : ndarray 
        See RLM.pinv_wexog 
    pvalues : ndarray 
        The p values associated with `tvalues`. Note that `tvalues` are assumed 
        to be distributed standard normal rather than Student's t. 
    resid : ndarray 
        The residuals of the fitted model.  endog - fittedvalues 
    scale : float 
        The type of scale is determined in the arguments to the fit method in 
        RLM.  The reported scale is taken from the residuals of the weighted 
        least squares in the last IRLS iteration if update_scale is True.  If 
        update_scale is False, then it is the scale given by the first OLS 
        fit before the IRLS iterations. 
    sresid : ndarray 
        The scaled residuals. 
    tvalues : ndarray 
        The &quot;t-statistics&quot; of params. These are defined as params/bse where 
        bse are taken from the robust covariance matrix specified in the 
        argument to fit. 
    weights : ndarray 
        The reported weights are determined by passing the scaled residuals 
        from the last weighted least squares fit in the IRLS algorithm. 
 
    See Also 
    -------- 
    statsmodels.base.model.LikelihoodModelResults 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">model</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">normalized_cov_params</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s1">super(RLMResults</span><span class="s2">, </span><span class="s1">self).__init__(model</span><span class="s2">, </span><span class="s1">params</span><span class="s2">,</span>
                                         <span class="s1">normalized_cov_params</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s1">self.model = model</span>
        <span class="s1">self.df_model = model.df_model</span>
        <span class="s1">self.df_resid = model.df_resid</span>
        <span class="s1">self.nobs = model.nobs</span>
        <span class="s1">self._cache = {}</span>
        <span class="s5"># for remove_data</span>
        <span class="s1">self._data_in_cache.extend([</span><span class="s3">'sresid'</span><span class="s1">])</span>

        <span class="s1">self.cov_params_default = self.bcov_scaled</span>
        <span class="s5"># TODO: &quot;pvals&quot; should come from chisq on bse?</span>

    <span class="s1">@cache_readonly</span>
    <span class="s2">def </span><span class="s1">fittedvalues(self):</span>
        <span class="s2">return </span><span class="s1">np.dot(self.model.exog</span><span class="s2">, </span><span class="s1">self.params)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s2">def </span><span class="s1">resid(self):</span>
        <span class="s2">return </span><span class="s1">self.model.endog - self.fittedvalues  </span><span class="s5"># before bcov</span>

    <span class="s1">@cache_readonly</span>
    <span class="s2">def </span><span class="s1">sresid(self):</span>
        <span class="s2">if </span><span class="s1">self.scale == </span><span class="s4">0.0</span><span class="s1">:</span>
            <span class="s1">sresid = self.resid.copy()</span>
            <span class="s1">sresid[:] = </span><span class="s4">0.0</span>
            <span class="s2">return </span><span class="s1">sresid</span>
        <span class="s2">return </span><span class="s1">self.resid / self.scale</span>

    <span class="s1">@cache_readonly</span>
    <span class="s2">def </span><span class="s1">bcov_unscaled(self):</span>
        <span class="s2">return </span><span class="s1">self.normalized_cov_params</span>

    <span class="s1">@cache_readonly</span>
    <span class="s2">def </span><span class="s1">weights(self):</span>
        <span class="s2">return </span><span class="s1">self.model.weights</span>

    <span class="s1">@cache_readonly</span>
    <span class="s2">def </span><span class="s1">bcov_scaled(self):</span>
        <span class="s1">model = self.model</span>
        <span class="s1">m = np.mean(model.M.psi_deriv(self.sresid))</span>
        <span class="s1">var_psiprime = np.var(model.M.psi_deriv(self.sresid))</span>
        <span class="s1">k = </span><span class="s4">1 </span><span class="s1">+ (self.df_model + </span><span class="s4">1</span><span class="s1">) / self.nobs * var_psiprime / m ** </span><span class="s4">2</span>

        <span class="s2">if </span><span class="s1">model.cov == </span><span class="s3">&quot;H1&quot;</span><span class="s1">:</span>
            <span class="s1">ss_psi = np.sum(model.M.psi(self.sresid) ** </span><span class="s4">2</span><span class="s1">)</span>
            <span class="s1">s_psi_deriv = np.sum(model.M.psi_deriv(self.sresid))</span>
            <span class="s2">return </span><span class="s1">k ** </span><span class="s4">2 </span><span class="s1">* (</span><span class="s4">1 </span><span class="s1">/ self.df_resid * ss_psi * self.scale ** </span><span class="s4">2</span><span class="s1">) /\</span>
                <span class="s1">((</span><span class="s4">1 </span><span class="s1">/ self.nobs * s_psi_deriv) ** </span><span class="s4">2</span><span class="s1">) *\</span>
                <span class="s1">model.normalized_cov_params</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">W = np.dot(model.M.psi_deriv(self.sresid) * model.exog.T</span><span class="s2">,</span>
                       <span class="s1">model.exog)</span>
            <span class="s1">W_inv = np.linalg.inv(W)</span>
            <span class="s5"># [W_jk]^-1 = [SUM(psi_deriv(Sr_i)*x_ij*x_jk)]^-1</span>
            <span class="s5"># where Sr are the standardized residuals</span>
            <span class="s2">if </span><span class="s1">model.cov == </span><span class="s3">&quot;H2&quot;</span><span class="s1">:</span>
                <span class="s5"># These are correct, based on Huber (1973) 8.13</span>
                <span class="s2">return </span><span class="s1">k * (</span><span class="s4">1 </span><span class="s1">/ self.df_resid) * np.sum(</span>
                    <span class="s1">model.M.psi(self.sresid) ** </span><span class="s4">2</span><span class="s1">) * self.scale ** </span><span class="s4">2 </span><span class="s1">\</span>
                       <span class="s1">/ ((</span><span class="s4">1 </span><span class="s1">/ self.nobs) *</span>
                          <span class="s1">np.sum(model.M.psi_deriv(self.sresid))) * W_inv</span>
            <span class="s2">elif </span><span class="s1">model.cov == </span><span class="s3">&quot;H3&quot;</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">k ** -</span><span class="s4">1 </span><span class="s1">* </span><span class="s4">1 </span><span class="s1">/ self.df_resid * np.sum(</span>
                    <span class="s1">model.M.psi(self.sresid) ** </span><span class="s4">2</span><span class="s1">) * self.scale ** </span><span class="s4">2 </span><span class="s1">\</span>
                       <span class="s1">* np.dot(</span>
                    <span class="s1">np.dot(W_inv</span><span class="s2">, </span><span class="s1">np.dot(model.exog.T</span><span class="s2">, </span><span class="s1">model.exog))</span><span class="s2">,</span>
                    <span class="s1">W_inv)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s2">def </span><span class="s1">pvalues(self):</span>
        <span class="s2">return </span><span class="s1">stats.norm.sf(np.abs(self.tvalues)) * </span><span class="s4">2</span>

    <span class="s1">@cache_readonly</span>
    <span class="s2">def </span><span class="s1">bse(self):</span>
        <span class="s2">return </span><span class="s1">np.sqrt(np.diag(self.bcov_scaled))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s2">def </span><span class="s1">chisq(self):</span>
        <span class="s2">return </span><span class="s1">(self.params / self.bse) ** </span><span class="s4">2</span>

    <span class="s2">def </span><span class="s1">summary(self</span><span class="s2">, </span><span class="s1">yname=</span><span class="s2">None, </span><span class="s1">xname=</span><span class="s2">None, </span><span class="s1">title=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s4">.05</span><span class="s2">,</span>
                <span class="s1">return_fmt=</span><span class="s3">'text'</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        This is for testing the new summary setup 
        &quot;&quot;&quot;</span>
        <span class="s1">top_left = [(</span><span class="s3">'Dep. Variable:'</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s3">'Model:'</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s3">'Method:'</span><span class="s2">, </span><span class="s1">[</span><span class="s3">'IRLS'</span><span class="s1">])</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s3">'Norm:'</span><span class="s2">, </span><span class="s1">[self.fit_options[</span><span class="s3">'norm'</span><span class="s1">]])</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s3">'Scale Est.:'</span><span class="s2">, </span><span class="s1">[self.fit_options[</span><span class="s3">'scale_est'</span><span class="s1">]])</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s3">'Cov Type:'</span><span class="s2">, </span><span class="s1">[self.fit_options[</span><span class="s3">'cov'</span><span class="s1">]])</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s3">'Date:'</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s3">'Time:'</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s3">'No. Iterations:'</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;%d&quot; </span><span class="s1">% self.fit_history[</span><span class="s3">'iteration'</span><span class="s1">]])</span>
                    <span class="s1">]</span>
        <span class="s1">top_right = [(</span><span class="s3">'No. Observations:'</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">,</span>
                     <span class="s1">(</span><span class="s3">'Df Residuals:'</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">,</span>
                     <span class="s1">(</span><span class="s3">'Df Model:'</span><span class="s2">, None</span><span class="s1">)</span>
                     <span class="s1">]</span>

        <span class="s2">if </span><span class="s1">title </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">title = </span><span class="s3">&quot;Robust linear Model Regression Results&quot;</span>

        <span class="s5"># boiler plate</span>
        <span class="s2">from </span><span class="s1">statsmodels.iolib.summary </span><span class="s2">import </span><span class="s1">Summary</span>
        <span class="s1">smry = Summary()</span>
        <span class="s1">smry.add_table_2cols(self</span><span class="s2">, </span><span class="s1">gleft=top_left</span><span class="s2">, </span><span class="s1">gright=top_right</span><span class="s2">,</span>
                             <span class="s1">yname=yname</span><span class="s2">, </span><span class="s1">xname=xname</span><span class="s2">, </span><span class="s1">title=title)</span>
        <span class="s1">smry.add_table_params(self</span><span class="s2">, </span><span class="s1">yname=yname</span><span class="s2">, </span><span class="s1">xname=xname</span><span class="s2">, </span><span class="s1">alpha=alpha</span><span class="s2">,</span>
                              <span class="s1">use_t=self.use_t)</span>

        <span class="s5"># add warnings/notes, added to text format only</span>
        <span class="s1">etext = []</span>
        <span class="s1">wstr = (</span><span class="s3">&quot;If the model instance has been used for another fit with &quot;</span>
                <span class="s3">&quot;different fit parameters, then the fit options might not be &quot;</span>
                <span class="s3">&quot;the correct ones anymore .&quot;</span><span class="s1">)</span>
        <span class="s1">etext.append(wstr)</span>

        <span class="s2">if </span><span class="s1">etext:</span>
            <span class="s1">smry.add_extra_txt(etext)</span>

        <span class="s2">return </span><span class="s1">smry</span>

    <span class="s2">def </span><span class="s1">summary2(self</span><span class="s2">, </span><span class="s1">xname=</span><span class="s2">None, </span><span class="s1">yname=</span><span class="s2">None, </span><span class="s1">title=</span><span class="s2">None, </span><span class="s1">alpha=</span><span class="s4">.05</span><span class="s2">,</span>
                 <span class="s1">float_format=</span><span class="s3">&quot;%.4f&quot;</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Experimental summary function for regression results 
 
        Parameters 
        ---------- 
        yname : str 
            Name of the dependent variable (optional) 
        xname : list[str], optional 
            Names for the exogenous variables. Default is `var_##` for ## in 
            the number of regressors. Must match the number of parameters 
            in the model 
        title : str, optional 
            Title for the top table. If not None, then this replaces the 
            default title 
        alpha : float 
            significance level for the confidence intervals 
        float_format : str 
            print format for floats in parameters summary 
 
        Returns 
        ------- 
        smry : Summary instance 
            this holds the summary tables and text, which can be printed or 
            converted to various output formats. 
 
        See Also 
        -------- 
        statsmodels.iolib.summary2.Summary : class to hold summary results 
        &quot;&quot;&quot;</span>
        <span class="s2">from </span><span class="s1">statsmodels.iolib </span><span class="s2">import </span><span class="s1">summary2</span>
        <span class="s1">smry = summary2.Summary()</span>
        <span class="s1">smry.add_base(results=self</span><span class="s2">, </span><span class="s1">alpha=alpha</span><span class="s2">, </span><span class="s1">float_format=float_format</span><span class="s2">,</span>
                      <span class="s1">xname=xname</span><span class="s2">, </span><span class="s1">yname=yname</span><span class="s2">, </span><span class="s1">title=title)</span>

        <span class="s2">return </span><span class="s1">smry</span>


<span class="s2">class </span><span class="s1">RLMResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s2">pass</span>


<span class="s1">wrap.populate_wrapper(RLMResultsWrapper</span><span class="s2">, </span><span class="s1">RLMResults)  </span><span class="s5"># noqa:E305</span>
</pre>
</body>
</html>