<html>
<head>
<title>_lilliefors.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_lilliefors.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Implements Lilliefors corrected Kolmogorov-Smirnov tests for normal and 
exponential distributions. 
 
`kstest_fit` is provided as a top-level function to access both tests. 
`kstest_normal` and `kstest_exponential` are provided as convenience functions 
with the appropriate test as the default. 
`lilliefors` is provided as an alias for `kstest_fit`. 
 
Created on Sat Oct 01 13:16:49 2011 
 
Author: Josef Perktold 
License: BSD-3 
 
pvalues for Lilliefors test are based on formula and table in 
 
An Analytic Approximation to the Distribution of Lilliefors's Test Statistic 
for Normality 
Author(s): Gerard E. Dallal and Leland WilkinsonSource: The American 
Statistician, Vol. 40, No. 4 (Nov., 1986), pp. 294-296 
Published by: American Statistical Association 
Stable URL: http://www.jstor.org/stable/2684607 . 
 
On the Kolmogorov-Smirnov Test for Normality with Mean and Variance Unknown 
Hubert W. Lilliefors 
Journal of the American Statistical Association, Vol. 62, No. 318. 
(Jun., 1967), pp. 399-402. 
 
--- 
 
Updated 2017-07-23 
Jacob C. Kimmel 
 
Ref: 
Lilliefors, H.W. 
On the Kolmogorov-Smirnov test for the exponential distribution with mean 
unknown. Journal of the American Statistical Association, Vol 64, No. 325. 
(1969), pp. 387â€“389. 
&quot;&quot;&quot;</span>
<span class="s2">from </span><span class="s1">functools </span><span class="s2">import </span><span class="s1">partial</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">stats</span>

<span class="s2">from </span><span class="s1">statsmodels.tools.validation </span><span class="s2">import </span><span class="s1">string_like</span>
<span class="s2">from </span><span class="s1">._lilliefors_critical_values </span><span class="s2">import </span><span class="s1">(critical_values</span><span class="s2">,</span>
                                          <span class="s1">asymp_critical_values</span><span class="s2">,</span>
                                          <span class="s1">PERCENTILES)</span>
<span class="s2">from </span><span class="s1">.tabledist </span><span class="s2">import </span><span class="s1">TableDist</span>


<span class="s2">def </span><span class="s1">_make_asymptotic_function(params):</span>
    <span class="s0">&quot;&quot;&quot; 
    Generates an asymptotic distribution callable from a param matrix 
 
    Polynomial is a[0] * x**(-1/2) + a[1] * x**(-1) + a[2] * x**(-3/2) 
 
    Parameters 
    ---------- 
    params : ndarray 
        Array with shape (nalpha, 3) where nalpha is the number of 
        significance levels 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">f(n):</span>
        <span class="s1">poly = np.array([</span><span class="s3">1</span><span class="s2">, </span><span class="s1">np.log(n)</span><span class="s2">, </span><span class="s1">np.log(n) ** </span><span class="s3">2</span><span class="s1">])</span>
        <span class="s2">return </span><span class="s1">np.exp(poly.dot(params.T))</span>

    <span class="s2">return </span><span class="s1">f</span>


<span class="s2">def </span><span class="s1">ksstat(x</span><span class="s2">, </span><span class="s1">cdf</span><span class="s2">, </span><span class="s1">alternative=</span><span class="s4">'two_sided'</span><span class="s2">, </span><span class="s1">args=()):</span>
    <span class="s0">&quot;&quot;&quot; 
    Calculate statistic for the Kolmogorov-Smirnov test for goodness of fit 
 
    This calculates the test statistic for a test of the distribution G(x) of 
    an observed variable against a given distribution F(x). Under the null 
    hypothesis the two distributions are identical, G(x)=F(x). The 
    alternative hypothesis can be either 'two_sided' (default), 'less' 
    or 'greater'. The KS test is only valid for continuous distributions. 
 
    Parameters 
    ---------- 
    x : array_like, 1d 
        array of observations 
    cdf : str or callable 
        string: name of a distribution in scipy.stats 
        callable: function to evaluate cdf 
    alternative : 'two_sided' (default), 'less' or 'greater' 
        defines the alternative hypothesis (see explanation) 
    args : tuple, sequence 
        distribution parameters for call to cdf 
 
 
    Returns 
    ------- 
    D : float 
        KS test statistic, either D, D+ or D- 
 
    See Also 
    -------- 
    scipy.stats.kstest 
 
    Notes 
    ----- 
 
    In the one-sided test, the alternative is that the empirical 
    cumulative distribution function of the random variable is &quot;less&quot; 
    or &quot;greater&quot; than the cumulative distribution function F(x) of the 
    hypothesis, G(x)&lt;=F(x), resp. G(x)&gt;=F(x). 
 
    In contrast to scipy.stats.kstest, this function only calculates the 
    statistic which can be used either as distance measure or to implement 
    case specific p-values. 
    &quot;&quot;&quot;</span>
    <span class="s1">nobs = float(len(x))</span>

    <span class="s2">if </span><span class="s1">isinstance(cdf</span><span class="s2">, </span><span class="s1">str):</span>
        <span class="s1">cdf = getattr(stats.distributions</span><span class="s2">, </span><span class="s1">cdf).cdf</span>
    <span class="s2">elif </span><span class="s1">hasattr(cdf</span><span class="s2">, </span><span class="s4">'cdf'</span><span class="s1">):</span>
        <span class="s1">cdf = getattr(cdf</span><span class="s2">, </span><span class="s4">'cdf'</span><span class="s1">)</span>

    <span class="s1">x = np.sort(x)</span>
    <span class="s1">cdfvals = cdf(x</span><span class="s2">, </span><span class="s1">*args)</span>

    <span class="s1">d_plus = (np.arange(</span><span class="s3">1.0</span><span class="s2">, </span><span class="s1">nobs + </span><span class="s3">1</span><span class="s1">) / nobs - cdfvals).max()</span>
    <span class="s1">d_min = (cdfvals - np.arange(</span><span class="s3">0.0</span><span class="s2">, </span><span class="s1">nobs) / nobs).max()</span>
    <span class="s2">if </span><span class="s1">alternative == </span><span class="s4">'greater'</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">d_plus</span>
    <span class="s2">elif </span><span class="s1">alternative == </span><span class="s4">'less'</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">d_min</span>

    <span class="s2">return </span><span class="s1">np.max([d_plus</span><span class="s2">, </span><span class="s1">d_min])</span>


<span class="s2">def </span><span class="s1">get_lilliefors_table(dist=</span><span class="s4">'norm'</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Generates tables for significance levels of Lilliefors test statistics 
 
    Tables for available normal and exponential distribution testing, 
    as specified in Lilliefors references above 
 
    Parameters 
    ---------- 
    dist : str 
        distribution being tested in set {'norm', 'exp'}. 
 
    Returns 
    ------- 
    lf : TableDist object. 
        table of critical values 
    &quot;&quot;&quot;</span>
    <span class="s5"># function just to keep things together</span>
    <span class="s5"># for this test alpha is sf probability, i.e. right tail probability</span>

    <span class="s1">alpha = </span><span class="s3">1 </span><span class="s1">- np.array(PERCENTILES) / </span><span class="s3">100.0</span>
    <span class="s1">alpha = alpha[::-</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">dist = </span><span class="s4">'normal' </span><span class="s2">if </span><span class="s1">dist == </span><span class="s4">'norm' </span><span class="s2">else </span><span class="s1">dist</span>
    <span class="s2">if </span><span class="s1">dist </span><span class="s2">not in </span><span class="s1">critical_values:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Invalid dist parameter. Must be 'norm' or 'exp'&quot;</span><span class="s1">)</span>
    <span class="s1">cv_data = critical_values[dist]</span>
    <span class="s1">acv_data = asymp_critical_values[dist]</span>

    <span class="s1">size = np.array(sorted(cv_data)</span><span class="s2">, </span><span class="s1">dtype=float)</span>
    <span class="s1">crit_lf = np.array([cv_data[key] </span><span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">sorted(cv_data)])</span>
    <span class="s1">crit_lf = crit_lf[:</span><span class="s2">, </span><span class="s1">::-</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">asym_params = np.array([acv_data[key] </span><span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">sorted(acv_data)])</span>
    <span class="s1">asymp_fn = _make_asymptotic_function((asym_params[::-</span><span class="s3">1</span><span class="s1">]))</span>

    <span class="s1">lf = TableDist(alpha</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">crit_lf</span><span class="s2">, </span><span class="s1">asymptotic=asymp_fn)</span>
    <span class="s2">return </span><span class="s1">lf</span>


<span class="s1">lilliefors_table_norm = get_lilliefors_table(dist=</span><span class="s4">'norm'</span><span class="s1">)</span>
<span class="s1">lilliefors_table_expon = get_lilliefors_table(dist=</span><span class="s4">'exp'</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">pval_lf(d_max</span><span class="s2">, </span><span class="s1">n):</span>
    <span class="s0">&quot;&quot;&quot; 
    Approximate pvalues for Lilliefors test 
 
    This is only valid for pvalues smaller than 0.1 which is not checked in 
    this function. 
 
    Parameters 
    ---------- 
    d_max : array_like 
        two-sided Kolmogorov-Smirnov test statistic 
    n : int or float 
        sample size 
 
    Returns 
    ------- 
    p-value : float or ndarray 
        pvalue according to approximation formula of Dallal and Wilkinson. 
 
    Notes 
    ----- 
    This is mainly a helper function where the calling code should dispatch 
    on bound violations. Therefore it does not check whether the pvalue is in 
    the valid range. 
 
    Precision for the pvalues is around 2 to 3 decimals. This approximation is 
    also used by other statistical packages (e.g. R:fBasics) but might not be 
    the most precise available. 
 
    References 
    ---------- 
    DallalWilkinson1986 
    &quot;&quot;&quot;</span>
    <span class="s5"># todo: check boundaries, valid range for n and Dmax</span>
    <span class="s2">if </span><span class="s1">n &gt; </span><span class="s3">100</span><span class="s1">:</span>
        <span class="s1">d_max *= (n / </span><span class="s3">100.</span><span class="s1">) ** </span><span class="s3">0.49</span>
        <span class="s1">n = </span><span class="s3">100</span>
    <span class="s1">pval = np.exp(-</span><span class="s3">7.01256 </span><span class="s1">* d_max ** </span><span class="s3">2 </span><span class="s1">* (n + </span><span class="s3">2.78019</span><span class="s1">)</span>
                  <span class="s1">+ </span><span class="s3">2.99587 </span><span class="s1">* d_max * np.sqrt(n + </span><span class="s3">2.78019</span><span class="s1">) - </span><span class="s3">0.122119</span>
                  <span class="s1">+ </span><span class="s3">0.974598 </span><span class="s1">/ np.sqrt(n) + </span><span class="s3">1.67997 </span><span class="s1">/ n)</span>
    <span class="s2">return </span><span class="s1">pval</span>


<span class="s2">def </span><span class="s1">kstest_fit(x</span><span class="s2">, </span><span class="s1">dist=</span><span class="s4">'norm'</span><span class="s2">, </span><span class="s1">pvalmethod=</span><span class="s4">&quot;table&quot;</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Test assumed normal or exponential distribution using Lilliefors' test. 
 
    Lilliefors' test is a Kolmogorov-Smirnov test with estimated parameters. 
 
    Parameters 
    ---------- 
    x : array_like, 1d 
        Data to test. 
    dist : {'norm', 'exp'}, optional 
        The assumed distribution. 
    pvalmethod : {'approx', 'table'}, optional 
        The method used to compute the p-value of the test statistic. In 
        general, 'table' is preferred and makes use of a very large simulation. 
        'approx' is only valid for normality. if `dist = 'exp'` `table` is 
        always used. 'approx' uses the approximation formula of Dalal and 
        Wilkinson, valid for pvalues &lt; 0.1. If the pvalue is larger than 0.1, 
        then the result of `table` is returned. 
 
    Returns 
    ------- 
    ksstat : float 
        Kolmogorov-Smirnov test statistic with estimated mean and variance. 
    pvalue : float 
        If the pvalue is lower than some threshold, e.g. 0.05, then we can 
        reject the Null hypothesis that the sample comes from a normal 
        distribution. 
 
    Notes 
    ----- 
    'table' uses an improved table based on 10,000,000 simulations. The 
    critical values are approximated using 
    log(cv_alpha) = b_alpha + c[0] log(n) + c[1] log(n)**2 
    where cv_alpha is the critical value for a test with size alpha, 
    b_alpha is an alpha-specific intercept term and c[1] and c[2] are 
    coefficients that are shared all alphas. 
    Values in the table are linearly interpolated. Values outside the 
    range are be returned as bounds, 0.990 for large and 0.001 for small 
    pvalues. 
 
    For implementation details, see  lilliefors_critical_value_simulation.py in 
    the test directory. 
    &quot;&quot;&quot;</span>
    <span class="s1">pvalmethod = string_like(pvalmethod</span><span class="s2">,</span>
                             <span class="s4">&quot;pvalmethod&quot;</span><span class="s2">,</span>
                             <span class="s1">options=(</span><span class="s4">&quot;approx&quot;</span><span class="s2">, </span><span class="s4">&quot;table&quot;</span><span class="s1">))</span>
    <span class="s1">x = np.asarray(x)</span>
    <span class="s2">if </span><span class="s1">x.ndim == </span><span class="s3">2 </span><span class="s2">and </span><span class="s1">x.shape[</span><span class="s3">1</span><span class="s1">] == </span><span class="s3">1</span><span class="s1">:</span>
        <span class="s1">x = x[:</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]</span>
    <span class="s2">elif </span><span class="s1">x.ndim != </span><span class="s3">1</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Invalid parameter `x`: must be a one-dimensional&quot;</span>
                         <span class="s4">&quot; array-like or a single-column DataFrame&quot;</span><span class="s1">)</span>

    <span class="s1">nobs = len(x)</span>

    <span class="s2">if </span><span class="s1">dist == </span><span class="s4">'norm'</span><span class="s1">:</span>
        <span class="s1">z = (x - x.mean()) / x.std(ddof=</span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">test_d = stats.norm.cdf</span>
        <span class="s1">lilliefors_table = lilliefors_table_norm</span>
    <span class="s2">elif </span><span class="s1">dist == </span><span class="s4">'exp'</span><span class="s1">:</span>
        <span class="s1">z = x / x.mean()</span>
        <span class="s1">test_d = stats.expon.cdf</span>
        <span class="s1">lilliefors_table = lilliefors_table_expon</span>
        <span class="s1">pvalmethod = </span><span class="s4">'table'</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Invalid dist parameter, must be 'norm' or 'exp'&quot;</span><span class="s1">)</span>

    <span class="s1">min_nobs = </span><span class="s3">4 </span><span class="s2">if </span><span class="s1">dist == </span><span class="s4">'norm' </span><span class="s2">else </span><span class="s3">3</span>
    <span class="s2">if </span><span class="s1">nobs &lt; min_nobs:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">'Test for distribution {0} requires at least {1} '</span>
                         <span class="s4">'observations'</span><span class="s1">.format(dist</span><span class="s2">, </span><span class="s1">min_nobs))</span>

    <span class="s1">d_ks = ksstat(z</span><span class="s2">, </span><span class="s1">test_d</span><span class="s2">, </span><span class="s1">alternative=</span><span class="s4">'two_sided'</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">pvalmethod == </span><span class="s4">'approx'</span><span class="s1">:</span>
        <span class="s1">pval = pval_lf(d_ks</span><span class="s2">, </span><span class="s1">nobs)</span>
        <span class="s5"># check pval is in desired range</span>
        <span class="s2">if </span><span class="s1">pval &gt; </span><span class="s3">0.1</span><span class="s1">:</span>
            <span class="s1">pval = lilliefors_table.prob(d_ks</span><span class="s2">, </span><span class="s1">nobs)</span>
    <span class="s2">else</span><span class="s1">:  </span><span class="s5"># pvalmethod == 'table'</span>
        <span class="s1">pval = lilliefors_table.prob(d_ks</span><span class="s2">, </span><span class="s1">nobs)</span>

    <span class="s2">return </span><span class="s1">d_ks</span><span class="s2">, </span><span class="s1">pval</span>


<span class="s1">lilliefors = kstest_fit</span>
<span class="s1">kstest_normal = kstest_fit</span>
<span class="s1">kstest_exponential = partial(kstest_fit</span><span class="s2">, </span><span class="s1">dist=</span><span class="s4">'exp'</span><span class="s1">)</span>
</pre>
</body>
</html>