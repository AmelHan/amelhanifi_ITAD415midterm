<html>
<head>
<title>example_wls.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
example_wls.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Example: scikits.statsmodels.WLS 
 
example is extended to look at the meaning of rsquared in WLS, 
at outliers, compares with RLM and a short bootstrap 
 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">scikits.statsmodels </span><span class="s2">as </span><span class="s1">sm</span>
<span class="s2">import </span><span class="s1">matplotlib.pyplot </span><span class="s2">as </span><span class="s1">plt</span>

<span class="s1">data = sm.datasets.ccard.Load()</span>
<span class="s1">data.exog = sm.add_constant(data.exog)</span>
<span class="s1">ols_fit = sm.OLS(data.endog</span><span class="s2">, </span><span class="s1">data.exog).fit()</span>

<span class="s3"># perhaps the residuals from this fit depend on the square of income</span>
<span class="s1">incomesq = data.exog[:</span><span class="s2">,</span><span class="s4">2</span><span class="s1">]</span>
<span class="s1">plt.scatter(incomesq</span><span class="s2">, </span><span class="s1">ols_fit.resid)</span>
<span class="s1">plt.grid()</span>


<span class="s3"># If we think that the variance is proportional to income**2</span>
<span class="s3"># we would want to weight the regression by income</span>
<span class="s3"># the weights argument in WLS weights the regression by its square root</span>
<span class="s3"># and since income enters the equation, if we have income/income</span>
<span class="s3"># it becomes the constant, so we would want to perform</span>
<span class="s3"># this type of regression without an explicit constant in the design</span>

<span class="s3">#data.exog = data.exog[:,:-1]</span>
<span class="s1">wls_fit = sm.WLS(data.endog</span><span class="s2">, </span><span class="s1">data.exog[:</span><span class="s2">,</span><span class="s1">:-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">weights=</span><span class="s4">1</span><span class="s1">/incomesq).fit()</span>

<span class="s3"># This however, leads to difficulties in interpreting the post-estimation</span>
<span class="s3"># statistics.  Statsmodels does not yet handle this elegantly, but</span>
<span class="s3"># the following may be more appropriate</span>

<span class="s3"># explained sum of squares</span>
<span class="s1">ess = wls_fit.uncentered_tss - wls_fit.ssr</span>
<span class="s3"># rsquared</span>
<span class="s1">rsquared = ess/wls_fit.uncentered_tss</span>
<span class="s3"># mean squared error of the model</span>
<span class="s1">mse_model = ess/(wls_fit.df_model + </span><span class="s4">1</span><span class="s1">) </span><span class="s3"># add back the dof of the constant</span>
<span class="s3"># f statistic</span>
<span class="s1">fvalue = mse_model/wls_fit.mse_resid</span>
<span class="s3"># adjusted r-squared</span>
<span class="s1">rsquared_adj = </span><span class="s4">1 </span><span class="s1">-(wls_fit.nobs)/(wls_fit.df_resid)*(</span><span class="s4">1</span><span class="s1">-rsquared)</span>



<span class="s3">#Trying to figure out what's going on in this example</span>
<span class="s3">#----------------------------------------------------</span>

<span class="s3">#JP: I need to look at this again. Even if I exclude the weight variable</span>
<span class="s3"># from the regressors and keep the constant in then the reported rsquared</span>
<span class="s3"># stays small. Below also compared using squared or sqrt of weight variable.</span>
<span class="s3"># TODO: need to add 45 degree line to graphs</span>
<span class="s1">wls_fit3 = sm.WLS(data.endog</span><span class="s2">, </span><span class="s1">data.exog[:</span><span class="s2">,</span><span class="s1">(</span><span class="s4">0</span><span class="s2">,</span><span class="s4">1</span><span class="s2">,</span><span class="s4">3</span><span class="s2">,</span><span class="s4">4</span><span class="s1">)]</span><span class="s2">, </span><span class="s1">weights=</span><span class="s4">1</span><span class="s1">/incomesq).fit()</span>
<span class="s1">print wls_fit3.summary()</span>
<span class="s1">print </span><span class="s5">'corrected rsquared'</span><span class="s2">, </span>
<span class="s1">print (wls_fit3.uncentered_tss - wls_fit3.ssr)/wls_fit3.uncentered_tss</span>
<span class="s1">plt.figure()</span>
<span class="s1">plt.title(</span><span class="s5">'WLS dropping heteroscedasticity variable from regressors'</span><span class="s1">)</span>
<span class="s1">plt.plot(data.endog</span><span class="s2">, </span><span class="s1">wls_fit3.fittedvalues</span><span class="s2">, </span><span class="s5">'o'</span><span class="s1">)</span>
<span class="s1">plt.xlim([</span><span class="s4">0</span><span class="s2">,</span><span class="s4">2000</span><span class="s1">])</span>
<span class="s1">plt.ylim([</span><span class="s4">0</span><span class="s2">,</span><span class="s4">2000</span><span class="s1">])</span>
<span class="s1">print </span><span class="s5">'raw correlation of endog and fittedvalues'</span>
<span class="s1">print np.corrcoef(data.endog</span><span class="s2">, </span><span class="s1">wls_fit.fittedvalues)</span>
<span class="s1">print </span><span class="s5">'raw correlation coefficient of endog and fittedvalues squared'</span>
<span class="s1">print np.corrcoef(data.endog</span><span class="s2">, </span><span class="s1">wls_fit.fittedvalues)[</span><span class="s4">0</span><span class="s2">,</span><span class="s4">1</span><span class="s1">]**</span><span class="s4">2</span>

<span class="s3"># compare with robust regression,</span>
<span class="s3"># heteroscedasticity correction downweights the outliers</span>
<span class="s1">rlm_fit = sm.RLM(data.endog</span><span class="s2">, </span><span class="s1">data.exog).fit()</span>
<span class="s1">plt.figure()</span>
<span class="s1">plt.title(</span><span class="s5">'using robust for comparison'</span><span class="s1">)</span>
<span class="s1">plt.plot(data.endog</span><span class="s2">, </span><span class="s1">rlm_fit.fittedvalues</span><span class="s2">, </span><span class="s5">'o'</span><span class="s1">)</span>
<span class="s1">plt.xlim([</span><span class="s4">0</span><span class="s2">,</span><span class="s4">2000</span><span class="s1">])</span>
<span class="s1">plt.ylim([</span><span class="s4">0</span><span class="s2">,</span><span class="s4">2000</span><span class="s1">])</span>

<span class="s3">#What is going on? A more systematic look at the data</span>
<span class="s3">#----------------------------------------------------</span>

<span class="s3"># two helper functions</span>

<span class="s2">def </span><span class="s1">getrsq(fitresult):</span>
    <span class="s0">'''calculates rsquared residual, total and explained sums of squares 
 
    Parameters 
    ---------- 
    fitresult : instance of Regression Result class, or tuple of (resid, endog) arrays 
        regression residuals and endogenous variable 
         
    Returns 
    ------- 
    rsquared 
    residual sum of squares 
    (centered) total sum of squares 
    explained sum of squares (for centered) 
    '''</span>
    <span class="s2">if </span><span class="s1">hasattr(fitresult</span><span class="s2">, </span><span class="s5">'resid'</span><span class="s1">) </span><span class="s2">and </span><span class="s1">hasattr(fitresult</span><span class="s2">, </span><span class="s5">'model'</span><span class="s1">):</span>
        <span class="s1">resid = fitresult.resid</span>
        <span class="s1">endog = fitresult.model.endog</span>
        <span class="s1">nobs = fitresult.nobs</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">resid = fitresult[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">endog = fitresult[</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">nobs = resid.shape[</span><span class="s4">0</span><span class="s1">]</span>
        
    
    <span class="s1">rss = np.dot(resid</span><span class="s2">, </span><span class="s1">resid)</span>
    <span class="s1">tss = np.var(endog)*nobs</span>
    <span class="s2">return </span><span class="s4">1</span><span class="s1">-rss/tss</span><span class="s2">, </span><span class="s1">rss</span><span class="s2">, </span><span class="s1">tss</span><span class="s2">, </span><span class="s1">tss-rss</span>


<span class="s2">def </span><span class="s1">index_trim_outlier(resid</span><span class="s2">, </span><span class="s1">k):</span>
    <span class="s0">'''returns indices to residual array with k outliers removed 
 
    Parameters 
    ---------- 
    resid : array_like, 1d 
        data vector, usually residuals of a regression 
    k : int 
        number of outliers to remove 
 
    Returns 
    ------- 
    trimmed_index : array, 1d 
        index array with k outliers removed 
    outlier_index : array, 1d 
        index array of k outliers 
 
    Notes 
    ----- 
 
    Outliers are defined as the k observations with the largest 
    absolute values. 
     
    '''</span>
    <span class="s1">sort_index = np.argsort(np.abs(resid))</span>
    <span class="s3"># index of non-outlier</span>
    <span class="s1">trimmed_index = np.sort(sort_index[:-k])</span>
    <span class="s1">outlier_index = np.sort(sort_index[-k:])</span>
    <span class="s2">return </span><span class="s1">trimmed_index</span><span class="s2">, </span><span class="s1">outlier_index</span>


<span class="s3">#Comparing estimation results for ols, rlm and wls with and without outliers</span>
<span class="s3">#---------------------------------------------------------------------------</span>
 
<span class="s3">#ols_test_fit = sm.OLS(data.endog, data.exog).fit()</span>
<span class="s1">olskeep</span><span class="s2">, </span><span class="s1">olsoutl = index_trim_outlier(ols_fit.resid</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
<span class="s1">print </span><span class="s5">'ols outliers'</span><span class="s2">, </span><span class="s1">olsoutl</span><span class="s2">, </span><span class="s1">ols_fit.resid[olsoutl]</span>
<span class="s1">ols_fit_rm2 = sm.OLS(data.endog[olskeep]</span><span class="s2">, </span><span class="s1">data.exog[olskeep</span><span class="s2">,</span><span class="s1">:]).fit()</span>
<span class="s1">rlm_fit_rm2 = sm.RLM(data.endog[olskeep]</span><span class="s2">, </span><span class="s1">data.exog[olskeep</span><span class="s2">,</span><span class="s1">:]).fit()</span>
<span class="s3">#weights = 1/incomesq</span>

<span class="s1">results = [ols_fit</span><span class="s2">, </span><span class="s1">ols_fit_rm2</span><span class="s2">, </span><span class="s1">rlm_fit</span><span class="s2">, </span><span class="s1">rlm_fit_rm2]</span>
<span class="s3">#Note: I think incomesq is already square</span>
<span class="s2">for </span><span class="s1">weights </span><span class="s2">in </span><span class="s1">[</span><span class="s4">1</span><span class="s1">/incomesq</span><span class="s2">, </span><span class="s4">1</span><span class="s1">/incomesq**</span><span class="s4">2</span><span class="s2">, </span><span class="s1">np.sqrt(incomesq)]:</span>
    <span class="s1">print </span><span class="s5">'</span><span class="s2">\n</span><span class="s5">Comparison OLS and WLS with and without outliers'</span>
    <span class="s1">wls_fit0 = sm.WLS(data.endog</span><span class="s2">, </span><span class="s1">data.exog</span><span class="s2">, </span><span class="s1">weights=weights).fit()</span>
    <span class="s1">wls_fit_rm2 = sm.WLS(data.endog[olskeep]</span><span class="s2">, </span><span class="s1">data.exog[olskeep</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">,</span>
                         <span class="s1">weights=weights[olskeep]).fit()</span>
    <span class="s1">wlskeep</span><span class="s2">, </span><span class="s1">wlsoutl = index_trim_outlier(ols_fit.resid</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">print </span><span class="s5">'2 outliers candidates and residuals'</span>
    <span class="s1">print wlsoutl</span><span class="s2">, </span><span class="s1">wls_fit.resid[olsoutl]</span>
    <span class="s3"># redundant because ols and wls outliers are the same:</span>
    <span class="s3">##wls_fit_rm2_ = sm.WLS(data.endog[wlskeep], data.exog[wlskeep,:],</span>
    <span class="s3">##                     weights=1/incomesq[wlskeep]).fit()</span>

    <span class="s1">print </span><span class="s5">'outliers ols, wls:'</span><span class="s2">, </span><span class="s1">olsoutl</span><span class="s2">, </span><span class="s1">wlsoutl</span>

    <span class="s1">print </span><span class="s5">'rsquared'</span>
    <span class="s1">print </span><span class="s5">'ols vs ols rm2'</span><span class="s2">, </span><span class="s1">ols_fit.rsquared</span><span class="s2">, </span><span class="s1">ols_fit_rm2.rsquared</span>
    <span class="s1">print </span><span class="s5">'wls vs wls rm2'</span><span class="s2">, </span><span class="s1">wls_fit0.rsquared</span><span class="s2">, </span><span class="s1">wls_fit_rm2.rsquared </span><span class="s3">#, wls_fit_rm2_.rsquared</span>
    <span class="s1">print </span><span class="s5">'compare R2_resid  versus  R2_wresid'</span>
    <span class="s1">print </span><span class="s5">'ols minus 2'</span><span class="s2">, </span><span class="s1">getrsq(ols_fit_rm2)[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">print getrsq((ols_fit_rm2.wresid</span><span class="s2">, </span><span class="s1">ols_fit_rm2.model.wendog))[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">print </span><span class="s5">'wls        '</span><span class="s2">, </span><span class="s1">getrsq(wls_fit)[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">print getrsq((wls_fit.wresid</span><span class="s2">, </span><span class="s1">wls_fit.model.wendog))[</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s1">print </span><span class="s5">'wls minus 2'</span><span class="s2">, </span><span class="s1">getrsq(wls_fit_rm2)[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s3"># next is same as wls_fit_rm2.rsquared for cross checking</span>
    <span class="s1">print getrsq((wls_fit_rm2.wresid</span><span class="s2">, </span><span class="s1">wls_fit_rm2.model.wendog))[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s3">#print getrsq(wls_fit_rm2_)[0],</span>
    <span class="s3">#print getrsq((wls_fit_rm2_.wresid, wls_fit_rm2_.model.wendog))[0]</span>
    <span class="s1">results.extend([wls_fit0</span><span class="s2">, </span><span class="s1">wls_fit_rm2])</span>

<span class="s1">print </span><span class="s5">'     ols             ols_rm2       rlm           rlm_rm2     wls (lin)    wls_rm2 (lin)   wls (squ)   wls_rm2 (squ)  wls (sqrt)   wls_rm2 (sqrt)'</span>
<span class="s1">print </span><span class="s5">'Parameter estimates'</span>
<span class="s1">print np.column_stack([r.params </span><span class="s2">for </span><span class="s1">r </span><span class="s2">in </span><span class="s1">results])</span>
<span class="s1">print </span><span class="s5">'R2 original data, next line R2 weighted data'</span>
<span class="s1">print np.column_stack([getattr(r</span><span class="s2">, </span><span class="s5">'rsquared'</span><span class="s2">, None</span><span class="s1">) </span><span class="s2">for </span><span class="s1">r </span><span class="s2">in </span><span class="s1">results])</span>

<span class="s1">print </span><span class="s5">'Standard errors'</span>
<span class="s1">print np.column_stack([getattr(r</span><span class="s2">, </span><span class="s5">'bse'</span><span class="s2">, None</span><span class="s1">) </span><span class="s2">for </span><span class="s1">r </span><span class="s2">in </span><span class="s1">results])</span>
<span class="s1">print </span><span class="s5">'Heteroscedasticity robust standard errors (with ols)'</span>
<span class="s1">print </span><span class="s5">'with outliers'</span>
<span class="s1">print np.column_stack([getattr(ols_fit</span><span class="s2">, </span><span class="s1">se</span><span class="s2">, None</span><span class="s1">) </span><span class="s2">for </span><span class="s1">se </span><span class="s2">in </span><span class="s1">[</span><span class="s5">'HC0_se'</span><span class="s2">, </span><span class="s5">'HC1_se'</span><span class="s2">, </span><span class="s5">'HC2_se'</span><span class="s2">, </span><span class="s5">'HC3_se'</span><span class="s1">]])</span>
<span class="s5">''' 
 
     ols             ols_rm2       rlm           rlm_rm2     wls (lin)    wls_rm2 (lin)   wls (squ)   wls_rm2 (squ)  wls (sqrt)   wls_rm2 (sqrt) 
Parameter estimates 
[[  -3.08181404   -5.06103843   -4.98510966   -5.34410309   -2.69418516    -3.1305703    -1.43815462   -1.58893054   -3.57074829   -6.80053364] 
 [ 234.34702702  115.08753715  129.85391456  109.01433492  158.42697752   128.38182357   60.95113284  100.25000841  254.82166855  103.75834726] 
 [ -14.99684418   -5.77558429   -6.46204829   -4.77409191   -7.24928987    -7.41228893    6.84943071   -3.34972494  -16.40524256   -4.5924465 ] 
 [  27.94090839   85.46566835   89.91389709   95.85086459   60.44877369    79.7759146    55.9884469    60.97199734   -3.8085159    84.69170048] 
 [-237.1465136    39.51639838  -15.50014814   31.39771833 -114.10886935   -40.04207242   -6.41976501  -38.83583228 -260.72084271  117.20540179]] 
 
R2 original data, next line R2 weighted data 
[[   0.24357792    0.31745994    0.19220308    0.30527648    0.22861236     0.3112333     0.06573949    0.29366904    0.24114325    0.31218669]] 
[[   0.24357791    0.31745994    None          None          0.05936888     0.0679071     0.06661848    0.12769654    0.35326686    0.54681225]] 
 
-&gt; R2 with weighted data is jumping all over 
 
standard errors 
[[   5.51471653    3.31028758    2.61580069    2.39537089    3.80730631     2.90027255    2.71141739    2.46959477    6.37593755    3.39477842] 
 [  80.36595035   49.35949263   38.12005692   35.71722666   76.39115431    58.35231328   87.18452039   80.30086861   86.99568216   47.58202096] 
 [   7.46933695    4.55366113    3.54293763    3.29509357    9.72433732     7.41259156   15.15205888   14.10674821    7.18302629    3.91640711] 
 [  82.92232357   50.54681754   39.33262384   36.57639175   58.55088753    44.82218676   43.11017757   39.31097542   96.4077482    52.57314209] 
 [ 199.35166485  122.1287718    94.55866295   88.3741058   139.68749646   106.89445525  115.79258539  105.99258363  239.38105863  130.32619908]] 
 
robust standard errors (with ols) 
with outliers 
      HC0_se         HC1_se       HC2_se        HC3_se' 
[[   3.30166123    3.42264107    3.4477148     3.60462409] 
 [  88.86635165   92.12260235   92.08368378   95.48159869] 
 [   6.94456348    7.19902694    7.19953754    7.47634779] 
 [  92.18777672   95.56573144   95.67211143   99.31427277] 
 [ 212.9905298   220.79495237  221.08892661  229.57434782]] 
 
removing 2 outliers 
[[   2.57840843    2.67574088    2.68958007    2.80968452] 
 [  36.21720995   37.58437497   37.69555106   39.51362437] 
 [   3.1156149     3.23322638    3.27353882    3.49104794] 
 [  50.09789409   51.98904166   51.89530067   53.79478834] 
 [  94.27094886   97.82958699   98.25588281  102.60375381]] 
 
 
'''</span>

<span class="s3"># a quick bootstrap analysis</span>
<span class="s3"># --------------------------</span>
<span class="s3">#</span>
<span class="s3">#(I didn't check whether this is fully correct statistically)</span>

<span class="s1">nobs</span><span class="s2">, </span><span class="s1">nvar = data.exog.shape</span>
<span class="s1">niter = </span><span class="s4">2000</span>
<span class="s1">bootres = np.zeros((niter</span><span class="s2">, </span><span class="s1">nvar*</span><span class="s4">2</span><span class="s1">))</span>

<span class="s2">for </span><span class="s1">it </span><span class="s2">in </span><span class="s1">range(niter):</span>
    <span class="s1">rind = np.random.randint(nobs</span><span class="s2">, </span><span class="s1">size=nobs)</span>
    <span class="s1">endog = data.endog[rind]</span>
    <span class="s1">exog = data.exog[rind</span><span class="s2">,</span><span class="s1">:]</span>
    <span class="s1">res = sm.OLS(endog</span><span class="s2">, </span><span class="s1">exog).fit()</span>
    <span class="s1">bootres[it</span><span class="s2">, </span><span class="s1">:nvar] = res.params</span>
    <span class="s1">bootres[it</span><span class="s2">, </span><span class="s1">nvar:] = res.bse</span>

<span class="s1">np.set_printoptions(linewidth=</span><span class="s4">200</span><span class="s1">)</span>
<span class="s1">print </span><span class="s5">'Bootstrap Results of parameters and parameter standard deviation  OLS'</span>
<span class="s1">print </span><span class="s5">'Parameter estimates'</span>
<span class="s1">print </span><span class="s5">'median'</span><span class="s2">, </span><span class="s1">np.median(bootres[:</span><span class="s2">,</span><span class="s1">:</span><span class="s4">5</span><span class="s1">]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
<span class="s1">print </span><span class="s5">'mean  '</span><span class="s2">, </span><span class="s1">np.mean(bootres[:</span><span class="s2">,</span><span class="s1">:</span><span class="s4">5</span><span class="s1">]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
<span class="s1">print </span><span class="s5">'std   '</span><span class="s2">, </span><span class="s1">np.std(bootres[:</span><span class="s2">,</span><span class="s1">:</span><span class="s4">5</span><span class="s1">]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

<span class="s1">print </span><span class="s5">'Standard deviation of parameter estimates'</span>
<span class="s1">print </span><span class="s5">'median'</span><span class="s2">, </span><span class="s1">np.median(bootres[:</span><span class="s2">,</span><span class="s4">5</span><span class="s1">:]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
<span class="s1">print </span><span class="s5">'mean  '</span><span class="s2">, </span><span class="s1">np.mean(bootres[:</span><span class="s2">,</span><span class="s4">5</span><span class="s1">:]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
<span class="s1">print </span><span class="s5">'std   '</span><span class="s2">, </span><span class="s1">np.std(bootres[:</span><span class="s2">,</span><span class="s4">5</span><span class="s1">:]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

<span class="s1">plt.figure()</span>
<span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">4</span><span class="s1">):</span>
    <span class="s1">plt.subplot(</span><span class="s4">2</span><span class="s2">,</span><span class="s4">2</span><span class="s2">,</span><span class="s1">i+</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">plt.hist(bootres[:</span><span class="s2">,</span><span class="s1">i]</span><span class="s2">,</span><span class="s4">50</span><span class="s1">)</span>
    <span class="s1">plt.title(</span><span class="s5">'var%d'</span><span class="s1">%i)</span>
<span class="s1">plt.figtext(</span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0.935</span><span class="s2">,  </span><span class="s5">'OLS Bootstrap'</span><span class="s2">,</span>
               <span class="s1">ha=</span><span class="s5">'center'</span><span class="s2">, </span><span class="s1">color=</span><span class="s5">'black'</span><span class="s2">, </span><span class="s1">weight=</span><span class="s5">'bold'</span><span class="s2">, </span><span class="s1">size=</span><span class="s5">'large'</span><span class="s1">)    </span>

<span class="s1">data_endog = data.endog[olskeep]</span>
<span class="s1">data_exog = data.exog[olskeep</span><span class="s2">,</span><span class="s1">:]</span>
<span class="s1">incomesq_rm2 = incomesq[olskeep]</span>

<span class="s1">nobs</span><span class="s2">, </span><span class="s1">nvar = data_exog.shape</span>
<span class="s1">niter = </span><span class="s4">500  </span><span class="s3"># a bit slow</span>
<span class="s1">bootreswls = np.zeros((niter</span><span class="s2">, </span><span class="s1">nvar*</span><span class="s4">2</span><span class="s1">))</span>

<span class="s2">for </span><span class="s1">it </span><span class="s2">in </span><span class="s1">range(niter):</span>
    <span class="s1">rind = np.random.randint(nobs</span><span class="s2">, </span><span class="s1">size=nobs)</span>
    <span class="s1">endog = data_endog[rind]</span>
    <span class="s1">exog = data_exog[rind</span><span class="s2">,</span><span class="s1">:]</span>
    <span class="s1">res = sm.WLS(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">weights=</span><span class="s4">1</span><span class="s1">/incomesq[rind</span><span class="s2">,</span><span class="s1">:]).fit()</span>
    <span class="s1">bootreswls[it</span><span class="s2">, </span><span class="s1">:nvar] = res.params</span>
    <span class="s1">bootreswls[it</span><span class="s2">, </span><span class="s1">nvar:] = res.bse</span>

<span class="s1">print </span><span class="s5">'Bootstrap Results of parameters and parameter standard deviation'</span><span class="s2">,</span>
<span class="s1">print </span><span class="s5">'WLS removed 2 outliers from sample'</span>
<span class="s1">print </span><span class="s5">'Parameter estimates'</span>
<span class="s1">print </span><span class="s5">'median'</span><span class="s2">, </span><span class="s1">np.median(bootreswls[:</span><span class="s2">,</span><span class="s1">:</span><span class="s4">5</span><span class="s1">]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
<span class="s1">print </span><span class="s5">'mean  '</span><span class="s2">, </span><span class="s1">np.mean(bootreswls[:</span><span class="s2">,</span><span class="s1">:</span><span class="s4">5</span><span class="s1">]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
<span class="s1">print </span><span class="s5">'std   '</span><span class="s2">, </span><span class="s1">np.std(bootreswls[:</span><span class="s2">,</span><span class="s1">:</span><span class="s4">5</span><span class="s1">]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

<span class="s1">print </span><span class="s5">'Standard deviation of parameter estimates'</span>
<span class="s1">print </span><span class="s5">'median'</span><span class="s2">, </span><span class="s1">np.median(bootreswls[:</span><span class="s2">,</span><span class="s4">5</span><span class="s1">:]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
<span class="s1">print </span><span class="s5">'mean  '</span><span class="s2">, </span><span class="s1">np.mean(bootreswls[:</span><span class="s2">,</span><span class="s4">5</span><span class="s1">:]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
<span class="s1">print </span><span class="s5">'std   '</span><span class="s2">, </span><span class="s1">np.std(bootreswls[:</span><span class="s2">,</span><span class="s4">5</span><span class="s1">:]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

<span class="s1">plt.figure()</span>
<span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">4</span><span class="s1">):</span>
    <span class="s1">plt.subplot(</span><span class="s4">2</span><span class="s2">,</span><span class="s4">2</span><span class="s2">,</span><span class="s1">i+</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">plt.hist(bootreswls[:</span><span class="s2">,</span><span class="s1">i]</span><span class="s2">,</span><span class="s4">50</span><span class="s1">)</span>
    <span class="s1">plt.title(</span><span class="s5">'var%d'</span><span class="s1">%i)</span>
<span class="s1">plt.figtext(</span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0.935</span><span class="s2">,  </span><span class="s5">'WLS rm2 Bootstrap'</span><span class="s2">,</span>
               <span class="s1">ha=</span><span class="s5">'center'</span><span class="s2">, </span><span class="s1">color=</span><span class="s5">'black'</span><span class="s2">, </span><span class="s1">weight=</span><span class="s5">'bold'</span><span class="s2">, </span><span class="s1">size=</span><span class="s5">'large'</span><span class="s1">)</span>


<span class="s3">#plt.show()</span>
<span class="s3">#plt.close('all')</span>

<span class="s5">''' 
The following a random variables not fixed by a seed 
 
Bootstrap Results of parameters and parameter standard deviation 
OLS 
 
Parameter estimates 
median [  -3.26216383  228.52546429  -14.57239967   34.27155426 -227.02816597] 
mean   [  -2.89855173  234.37139359  -14.98726881   27.96375666 -243.18361746] 
std    [   3.78704907   97.35797802    9.16316538   94.65031973  221.79444244] 
 
Standard deviation of parameter estimates 
median [   5.44701033   81.96921398    7.58642431   80.64906783  200.19167735] 
mean   [   5.44840542   86.02554883    8.56750041   80.41864084  201.81196849] 
std    [   1.43425083   29.74806562    4.22063268   19.14973277   55.34848348] 
 
Bootstrap Results of parameters and parameter standard deviation 
WLS removed 2 outliers from sample 
 
Parameter estimates 
median [  -3.95876112  137.10419042   -9.29131131   88.40265447  -44.21091869] 
mean   [  -3.67485724  135.42681207   -8.7499235    89.74703443  -46.38622848] 
std    [   2.96908679   56.36648967    7.03870751   48.51201918  106.92466097] 
 
Standard deviation of parameter estimates 
median [   2.89349748   59.19454402    6.70583332   45.40987953  119.05241283] 
mean   [   2.97600894   60.14540249    6.92102065   45.66077486  121.35519673] 
std    [   0.55378808   11.77831934    1.69289179    7.4911526    23.72821085] 
 
 
 
Conclusion: problem with outliers and possibly heteroscedasticity 
----------------------------------------------------------------- 
 
in bootstrap results 
* bse in OLS underestimates the standard deviation of the parameters 
  compared to standard deviation in bootstrap 
* OLS heteroscedasticity corrected standard errors for the original 
  data (above) are close to bootstrap std 
* using WLS with 2 outliers removed has a relatively good match between 
  the mean or median bse and the std of the parameter estimates in the 
  bootstrap 
 
We could also include rsquared in bootstrap, and do it also for RLM. 
The problems could also mean that the linearity assumption is violated, 
e.g. try non-linear transformation of exog variables, but linear 
in parameters. 
 
 
for statsmodels 
 * In this case rsquared for original data looks less random/arbitrary. 
 * Don't change definition of rsquared from centered tss to uncentered 
   tss when calculating rsquared in WLS if the original exog contains 
   a constant. The increase in rsquared because of a change in definition 
   will be very misleading. 
 * Whether there is a constant in the transformed exog, wexog, or not, 
   might affect also the degrees of freedom calculation, but I haven't 
   checked this. I would guess that the df_model should stay the same, 
   but needs to be verified with a textbook. 
 * df_model has to be adjusted if the original data does not have a 
   constant, e.g. when regressing an endog on a single exog variable 
   without constant. This case might require also a redefinition of 
   the rsquare and f statistic for the regression anova to use the 
   uncentered tss. 
   This can be done through keyword parameter to model.__init__ or 
   through autodedection with hasconst = (exog.var(0)&lt;1e-10).any() 
   I'm not sure about fixed effects with a full dummy set but 
   without a constant. In this case autodedection wouldn't work this 
   way. Also, I'm not sure whether a ddof keyword parameter can also 
   handle the hasconst case. 
    
 
'''</span>

</pre>
</body>
</html>