<html>
<head>
<title>_voting.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_voting.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Soft Voting/Majority Rule classifier and Voting regressor. 
 
This module contains: 
 - A Soft Voting/Majority Rule classifier for classification estimators. 
 - A Voting regressor for regression estimators. 
&quot;&quot;&quot;</span>

<span class="s2"># Authors: Sebastian Raschka &lt;se.raschka@gmail.com&gt;,</span>
<span class="s2">#          Gilles Louppe &lt;g.louppe@gmail.com&gt;,</span>
<span class="s2">#          Ramil Nugmanov &lt;stsouko@live.ru&gt;</span>
<span class="s2">#          Mohamed Ali Jamaoui &lt;m.ali.jamaoui@gmail.com&gt;</span>
<span class="s2">#</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">from </span><span class="s1">abc </span><span class="s3">import </span><span class="s1">abstractmethod</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">ClassifierMixin</span><span class="s3">,</span>
    <span class="s1">RegressorMixin</span><span class="s3">,</span>
    <span class="s1">TransformerMixin</span><span class="s3">,</span>
    <span class="s1">_fit_context</span><span class="s3">,</span>
    <span class="s1">clone</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">..exceptions </span><span class="s3">import </span><span class="s1">NotFittedError</span>
<span class="s3">from </span><span class="s1">..preprocessing </span><span class="s3">import </span><span class="s1">LabelEncoder</span>
<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">Bunch</span>
<span class="s3">from </span><span class="s1">..utils._estimator_html_repr </span><span class="s3">import </span><span class="s1">_VisualBlock</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">StrOptions</span>
<span class="s3">from </span><span class="s1">..utils.metaestimators </span><span class="s3">import </span><span class="s1">available_if</span>
<span class="s3">from </span><span class="s1">..utils.multiclass </span><span class="s3">import </span><span class="s1">check_classification_targets</span>
<span class="s3">from </span><span class="s1">..utils.parallel </span><span class="s3">import </span><span class="s1">Parallel</span><span class="s3">, </span><span class="s1">delayed</span>
<span class="s3">from </span><span class="s1">..utils.validation </span><span class="s3">import </span><span class="s1">_check_feature_names_in</span><span class="s3">, </span><span class="s1">check_is_fitted</span><span class="s3">, </span><span class="s1">column_or_1d</span>
<span class="s3">from </span><span class="s1">._base </span><span class="s3">import </span><span class="s1">_BaseHeterogeneousEnsemble</span><span class="s3">, </span><span class="s1">_fit_single_estimator</span>


<span class="s3">class </span><span class="s1">_BaseVoting(TransformerMixin</span><span class="s3">, </span><span class="s1">_BaseHeterogeneousEnsemble):</span>
    <span class="s0">&quot;&quot;&quot;Base class for voting. 
 
    Warning: This class should not be used directly. Use derived classes 
    instead. 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;estimators&quot;</span><span class="s1">: [list]</span><span class="s3">,</span>
        <span class="s4">&quot;weights&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_jobs&quot;</span><span class="s1">: [</span><span class="s3">None, </span><span class="s1">Integral]</span><span class="s3">,</span>
        <span class="s4">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s4">&quot;verbose&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">_log_message(self</span><span class="s3">, </span><span class="s1">name</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">total):</span>
        <span class="s3">if not </span><span class="s1">self.verbose:</span>
            <span class="s3">return None</span>
        <span class="s3">return </span><span class="s4">f&quot;(</span><span class="s3">{</span><span class="s1">idx</span><span class="s3">} </span><span class="s4">of </span><span class="s3">{</span><span class="s1">total</span><span class="s3">}</span><span class="s4">) Processing </span><span class="s3">{</span><span class="s1">name</span><span class="s3">}</span><span class="s4">&quot;</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">_weights_not_none(self):</span>
        <span class="s0">&quot;&quot;&quot;Get the weights of not `None` estimators.&quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self.weights </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">return None</span>
        <span class="s3">return </span><span class="s1">[w </span><span class="s3">for </span><span class="s1">est</span><span class="s3">, </span><span class="s1">w </span><span class="s3">in </span><span class="s1">zip(self.estimators</span><span class="s3">, </span><span class="s1">self.weights) </span><span class="s3">if </span><span class="s1">est[</span><span class="s5">1</span><span class="s1">] != </span><span class="s4">&quot;drop&quot;</span><span class="s1">]</span>

    <span class="s3">def </span><span class="s1">_predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Collect results from clf.predict calls.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.asarray([est.predict(X) </span><span class="s3">for </span><span class="s1">est </span><span class="s3">in </span><span class="s1">self.estimators_]).T</span>

    <span class="s1">@abstractmethod</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Get common fit operations.&quot;&quot;&quot;</span>
        <span class="s1">names</span><span class="s3">, </span><span class="s1">clfs = self._validate_estimators()</span>

        <span class="s3">if </span><span class="s1">self.weights </span><span class="s3">is not None and </span><span class="s1">len(self.weights) != len(self.estimators):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;Number of `estimators` and weights must be equal; got&quot;</span>
                <span class="s4">f&quot; </span><span class="s3">{</span><span class="s1">len(self.weights)</span><span class="s3">} </span><span class="s4">weights, </span><span class="s3">{</span><span class="s1">len(self.estimators)</span><span class="s3">} </span><span class="s4">estimators&quot;</span>
            <span class="s1">)</span>

        <span class="s1">self.estimators_ = Parallel(n_jobs=self.n_jobs)(</span>
            <span class="s1">delayed(_fit_single_estimator)(</span>
                <span class="s1">clone(clf)</span><span class="s3">,</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">y</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
                <span class="s1">message_clsname=</span><span class="s4">&quot;Voting&quot;</span><span class="s3">,</span>
                <span class="s1">message=self._log_message(names[idx]</span><span class="s3">, </span><span class="s1">idx + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">len(clfs))</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">for </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">clf </span><span class="s3">in </span><span class="s1">enumerate(clfs)</span>
            <span class="s3">if </span><span class="s1">clf != </span><span class="s4">&quot;drop&quot;</span>
        <span class="s1">)</span>

        <span class="s1">self.named_estimators_ = Bunch()</span>

        <span class="s2"># Uses 'drop' as placeholder for dropped estimators</span>
        <span class="s1">est_iter = iter(self.estimators_)</span>
        <span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">est </span><span class="s3">in </span><span class="s1">self.estimators:</span>
            <span class="s1">current_est = est </span><span class="s3">if </span><span class="s1">est == </span><span class="s4">&quot;drop&quot; </span><span class="s3">else </span><span class="s1">next(est_iter)</span>
            <span class="s1">self.named_estimators_[name] = current_est</span>

            <span class="s3">if </span><span class="s1">hasattr(current_est</span><span class="s3">, </span><span class="s4">&quot;feature_names_in_&quot;</span><span class="s1">):</span>
                <span class="s1">self.feature_names_in_ = current_est.feature_names_in_</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">fit_transform(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None, </span><span class="s1">**fit_params):</span>
        <span class="s0">&quot;&quot;&quot;Return class labels or probabilities for each estimator. 
 
        Return predictions for X for each estimator. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix, dataframe} of shape \ 
                (n_samples, n_features) 
            Input samples. 
 
        y : ndarray of shape (n_samples,), default=None 
            Target values (None for unsupervised transformations). 
 
        **fit_params : dict 
            Additional fit parameters. 
 
        Returns 
        ------- 
        X_new : ndarray array of shape (n_samples, n_features_new) 
            Transformed array. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">super().fit_transform(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">**fit_params)</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">n_features_in_(self):</span>
        <span class="s0">&quot;&quot;&quot;Number of features seen during :term:`fit`.&quot;&quot;&quot;</span>
        <span class="s2"># For consistency with other estimators we raise a AttributeError so</span>
        <span class="s2"># that hasattr() fails if the estimator isn't fitted.</span>
        <span class="s3">try</span><span class="s1">:</span>
            <span class="s1">check_is_fitted(self)</span>
        <span class="s3">except </span><span class="s1">NotFittedError </span><span class="s3">as </span><span class="s1">nfe:</span>
            <span class="s3">raise </span><span class="s1">AttributeError(</span>
                <span class="s4">&quot;{} object has no n_features_in_ attribute.&quot;</span><span class="s1">.format(</span>
                    <span class="s1">self.__class__.__name__</span>
                <span class="s1">)</span>
            <span class="s1">) </span><span class="s3">from </span><span class="s1">nfe</span>

        <span class="s3">return </span><span class="s1">self.estimators_[</span><span class="s5">0</span><span class="s1">].n_features_in_</span>

    <span class="s3">def </span><span class="s1">_sk_visual_block_(self):</span>
        <span class="s1">names</span><span class="s3">, </span><span class="s1">estimators = zip(*self.estimators)</span>
        <span class="s3">return </span><span class="s1">_VisualBlock(</span><span class="s4">&quot;parallel&quot;</span><span class="s3">, </span><span class="s1">estimators</span><span class="s3">, </span><span class="s1">names=names)</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span><span class="s4">&quot;preserves_dtype&quot;</span><span class="s1">: []}</span>


<span class="s3">class </span><span class="s1">VotingClassifier(ClassifierMixin</span><span class="s3">, </span><span class="s1">_BaseVoting):</span>
    <span class="s0">&quot;&quot;&quot;Soft Voting/Majority Rule classifier for unfitted estimators. 
 
    Read more in the :ref:`User Guide &lt;voting_classifier&gt;`. 
 
    .. versionadded:: 0.17 
 
    Parameters 
    ---------- 
    estimators : list of (str, estimator) tuples 
        Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones 
        of those original estimators that will be stored in the class attribute 
        ``self.estimators_``. An estimator can be set to ``'drop'`` using 
        :meth:`set_params`. 
 
        .. versionchanged:: 0.21 
            ``'drop'`` is accepted. Using None was deprecated in 0.22 and 
            support was removed in 0.24. 
 
    voting : {'hard', 'soft'}, default='hard' 
        If 'hard', uses predicted class labels for majority rule voting. 
        Else if 'soft', predicts the class label based on the argmax of 
        the sums of the predicted probabilities, which is recommended for 
        an ensemble of well-calibrated classifiers. 
 
    weights : array-like of shape (n_classifiers,), default=None 
        Sequence of weights (`float` or `int`) to weight the occurrences of 
        predicted class labels (`hard` voting) or class probabilities 
        before averaging (`soft` voting). Uses uniform weights if `None`. 
 
    n_jobs : int, default=None 
        The number of jobs to run in parallel for ``fit``. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
        .. versionadded:: 0.18 
 
    flatten_transform : bool, default=True 
        Affects shape of transform output only when voting='soft' 
        If voting='soft' and flatten_transform=True, transform method returns 
        matrix with shape (n_samples, n_classifiers * n_classes). If 
        flatten_transform=False, it returns 
        (n_classifiers, n_samples, n_classes). 
 
    verbose : bool, default=False 
        If True, the time elapsed while fitting will be printed as it 
        is completed. 
 
        .. versionadded:: 0.23 
 
    Attributes 
    ---------- 
    estimators_ : list of classifiers 
        The collection of fitted sub-estimators as defined in ``estimators`` 
        that are not 'drop'. 
 
    named_estimators_ : :class:`~sklearn.utils.Bunch` 
        Attribute to access any fitted sub-estimators by name. 
 
        .. versionadded:: 0.20 
 
    le_ : :class:`~sklearn.preprocessing.LabelEncoder` 
        Transformer used to encode the labels during fit and decode during 
        prediction. 
 
    classes_ : ndarray of shape (n_classes,) 
        The classes labels. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. Only defined if the 
        underlying classifier exposes such an attribute when fit. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Only defined if the 
        underlying estimators expose such an attribute when fit. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    VotingRegressor : Prediction voting regressor. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression 
    &gt;&gt;&gt; from sklearn.naive_bayes import GaussianNB 
    &gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier, VotingClassifier 
    &gt;&gt;&gt; clf1 = LogisticRegression(multi_class='multinomial', random_state=1) 
    &gt;&gt;&gt; clf2 = RandomForestClassifier(n_estimators=50, random_state=1) 
    &gt;&gt;&gt; clf3 = GaussianNB() 
    &gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) 
    &gt;&gt;&gt; y = np.array([1, 1, 1, 2, 2, 2]) 
    &gt;&gt;&gt; eclf1 = VotingClassifier(estimators=[ 
    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard') 
    &gt;&gt;&gt; eclf1 = eclf1.fit(X, y) 
    &gt;&gt;&gt; print(eclf1.predict(X)) 
    [1 1 1 2 2 2] 
    &gt;&gt;&gt; np.array_equal(eclf1.named_estimators_.lr.predict(X), 
    ...                eclf1.named_estimators_['lr'].predict(X)) 
    True 
    &gt;&gt;&gt; eclf2 = VotingClassifier(estimators=[ 
    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], 
    ...         voting='soft') 
    &gt;&gt;&gt; eclf2 = eclf2.fit(X, y) 
    &gt;&gt;&gt; print(eclf2.predict(X)) 
    [1 1 1 2 2 2] 
 
    To drop an estimator, :meth:`set_params` can be used to remove it. Here we 
    dropped one of the estimators, resulting in 2 fitted estimators: 
 
    &gt;&gt;&gt; eclf2 = eclf2.set_params(lr='drop') 
    &gt;&gt;&gt; eclf2 = eclf2.fit(X, y) 
    &gt;&gt;&gt; len(eclf2.estimators_) 
    2 
 
    Setting `flatten_transform=True` with `voting='soft'` flattens output shape of 
    `transform`: 
 
    &gt;&gt;&gt; eclf3 = VotingClassifier(estimators=[ 
    ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)], 
    ...        voting='soft', weights=[2,1,1], 
    ...        flatten_transform=True) 
    &gt;&gt;&gt; eclf3 = eclf3.fit(X, y) 
    &gt;&gt;&gt; print(eclf3.predict(X)) 
    [1 1 1 2 2 2] 
    &gt;&gt;&gt; print(eclf3.transform(X).shape) 
    (6, 6) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**_BaseVoting._parameter_constraints</span><span class="s3">,</span>
        <span class="s4">&quot;voting&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;hard&quot;</span><span class="s3">, </span><span class="s4">&quot;soft&quot;</span><span class="s1">})]</span><span class="s3">,</span>
        <span class="s4">&quot;flatten_transform&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">estimators</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">voting=</span><span class="s4">&quot;hard&quot;</span><span class="s3">,</span>
        <span class="s1">weights=</span><span class="s3">None,</span>
        <span class="s1">n_jobs=</span><span class="s3">None,</span>
        <span class="s1">flatten_transform=</span><span class="s3">True,</span>
        <span class="s1">verbose=</span><span class="s3">False,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(estimators=estimators)</span>
        <span class="s1">self.voting = voting</span>
        <span class="s1">self.weights = weights</span>
        <span class="s1">self.n_jobs = n_jobs</span>
        <span class="s1">self.flatten_transform = flatten_transform</span>
        <span class="s1">self.verbose = verbose</span>

    <span class="s1">@_fit_context(</span>
        <span class="s2"># estimators in VotingClassifier.estimators are not validated yet</span>
        <span class="s1">prefer_skip_nested_validation=</span><span class="s3">False</span>
    <span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the estimators. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training vectors, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
 
        y : array-like of shape (n_samples,) 
            Target values. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. If None, then samples are equally weighted. 
            Note that this is supported only if all underlying estimators 
            support sample weights. 
 
            .. versionadded:: 0.18 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_classification_targets(y)</span>
        <span class="s3">if </span><span class="s1">isinstance(y</span><span class="s3">, </span><span class="s1">np.ndarray) </span><span class="s3">and </span><span class="s1">len(y.shape) &gt; </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">y.shape[</span><span class="s5">1</span><span class="s1">] &gt; </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
                <span class="s4">&quot;Multilabel and multi-output classification is not supported.&quot;</span>
            <span class="s1">)</span>

        <span class="s1">self.le_ = LabelEncoder().fit(y)</span>
        <span class="s1">self.classes_ = self.le_.classes_</span>
        <span class="s1">transformed_y = self.le_.transform(y)</span>

        <span class="s3">return </span><span class="s1">super().fit(X</span><span class="s3">, </span><span class="s1">transformed_y</span><span class="s3">, </span><span class="s1">sample_weight)</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict class labels for X. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The input samples. 
 
        Returns 
        ------- 
        maj : array-like of shape (n_samples,) 
            Predicted class labels. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">if </span><span class="s1">self.voting == </span><span class="s4">&quot;soft&quot;</span><span class="s1">:</span>
            <span class="s1">maj = np.argmax(self.predict_proba(X)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>

        <span class="s3">else</span><span class="s1">:  </span><span class="s2"># 'hard' voting</span>
            <span class="s1">predictions = self._predict(X)</span>
            <span class="s1">maj = np.apply_along_axis(</span>
                <span class="s3">lambda </span><span class="s1">x: np.argmax(np.bincount(x</span><span class="s3">, </span><span class="s1">weights=self._weights_not_none))</span><span class="s3">,</span>
                <span class="s1">axis=</span><span class="s5">1</span><span class="s3">,</span>
                <span class="s1">arr=predictions</span><span class="s3">,</span>
            <span class="s1">)</span>

        <span class="s1">maj = self.le_.inverse_transform(maj)</span>

        <span class="s3">return </span><span class="s1">maj</span>

    <span class="s3">def </span><span class="s1">_collect_probas(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Collect results from clf.predict calls.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.asarray([clf.predict_proba(X) </span><span class="s3">for </span><span class="s1">clf </span><span class="s3">in </span><span class="s1">self.estimators_])</span>

    <span class="s3">def </span><span class="s1">_check_voting(self):</span>
        <span class="s3">if </span><span class="s1">self.voting == </span><span class="s4">&quot;hard&quot;</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">AttributeError(</span>
                <span class="s4">f&quot;predict_proba is not available when voting=</span><span class="s3">{</span><span class="s1">repr(self.voting)</span><span class="s3">}</span><span class="s4">&quot;</span>
            <span class="s1">)</span>
        <span class="s3">return True</span>

    <span class="s1">@available_if(_check_voting)</span>
    <span class="s3">def </span><span class="s1">predict_proba(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Compute probabilities of possible outcomes for samples in X. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The input samples. 
 
        Returns 
        ------- 
        avg : array-like of shape (n_samples, n_classes) 
            Weighted average probability for each class per sample. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">avg = np.average(</span>
            <span class="s1">self._collect_probas(X)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">weights=self._weights_not_none</span>
        <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">avg</span>

    <span class="s3">def </span><span class="s1">transform(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Return class labels or probabilities for X for each estimator. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training vectors, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
 
        Returns 
        ------- 
        probabilities_or_labels 
            If `voting='soft'` and `flatten_transform=True`: 
                returns ndarray of shape (n_samples, n_classifiers * n_classes), 
                being class probabilities calculated by each classifier. 
            If `voting='soft' and `flatten_transform=False`: 
                ndarray of shape (n_classifiers, n_samples, n_classes) 
            If `voting='hard'`: 
                ndarray of shape (n_samples, n_classifiers), being 
                class labels predicted by each classifier. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s3">if </span><span class="s1">self.voting == </span><span class="s4">&quot;soft&quot;</span><span class="s1">:</span>
            <span class="s1">probas = self._collect_probas(X)</span>
            <span class="s3">if not </span><span class="s1">self.flatten_transform:</span>
                <span class="s3">return </span><span class="s1">probas</span>
            <span class="s3">return </span><span class="s1">np.hstack(probas)</span>

        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">self._predict(X)</span>

    <span class="s3">def </span><span class="s1">get_feature_names_out(self</span><span class="s3">, </span><span class="s1">input_features=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Get output feature names for transformation. 
 
        Parameters 
        ---------- 
        input_features : array-like of str or None, default=None 
            Not used, present here for API consistency by convention. 
 
        Returns 
        ------- 
        feature_names_out : ndarray of str objects 
            Transformed feature names. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self</span><span class="s3">, </span><span class="s4">&quot;n_features_in_&quot;</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">self.voting == </span><span class="s4">&quot;soft&quot; </span><span class="s3">and not </span><span class="s1">self.flatten_transform:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;get_feature_names_out is not supported when `voting='soft'` and &quot;</span>
                <span class="s4">&quot;`flatten_transform=False`&quot;</span>
            <span class="s1">)</span>

        <span class="s1">_check_feature_names_in(self</span><span class="s3">, </span><span class="s1">input_features</span><span class="s3">, </span><span class="s1">generate_names=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">class_name = self.__class__.__name__.lower()</span>

        <span class="s1">active_names = [name </span><span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">est </span><span class="s3">in </span><span class="s1">self.estimators </span><span class="s3">if </span><span class="s1">est != </span><span class="s4">&quot;drop&quot;</span><span class="s1">]</span>

        <span class="s3">if </span><span class="s1">self.voting == </span><span class="s4">&quot;hard&quot;</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">np.asarray(</span>
                <span class="s1">[</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">class_name</span><span class="s3">}</span><span class="s4">_</span><span class="s3">{</span><span class="s1">name</span><span class="s3">}</span><span class="s4">&quot; </span><span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">active_names]</span><span class="s3">, </span><span class="s1">dtype=object</span>
            <span class="s1">)</span>

        <span class="s2"># voting == &quot;soft&quot;</span>
        <span class="s1">n_classes = len(self.classes_)</span>
        <span class="s1">names_out = [</span>
            <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">class_name</span><span class="s3">}</span><span class="s4">_</span><span class="s3">{</span><span class="s1">name</span><span class="s3">}{</span><span class="s1">i</span><span class="s3">}</span><span class="s4">&quot; </span><span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">active_names </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_classes)</span>
        <span class="s1">]</span>
        <span class="s3">return </span><span class="s1">np.asarray(names_out</span><span class="s3">, </span><span class="s1">dtype=object)</span>


<span class="s3">class </span><span class="s1">VotingRegressor(RegressorMixin</span><span class="s3">, </span><span class="s1">_BaseVoting):</span>
    <span class="s0">&quot;&quot;&quot;Prediction voting regressor for unfitted estimators. 
 
    A voting regressor is an ensemble meta-estimator that fits several base 
    regressors, each on the whole dataset. Then it averages the individual 
    predictions to form a final prediction. 
 
    Read more in the :ref:`User Guide &lt;voting_regressor&gt;`. 
 
    .. versionadded:: 0.21 
 
    Parameters 
    ---------- 
    estimators : list of (str, estimator) tuples 
        Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones 
        of those original estimators that will be stored in the class attribute 
        ``self.estimators_``. An estimator can be set to ``'drop'`` using 
        :meth:`set_params`. 
 
        .. versionchanged:: 0.21 
            ``'drop'`` is accepted. Using None was deprecated in 0.22 and 
            support was removed in 0.24. 
 
    weights : array-like of shape (n_regressors,), default=None 
        Sequence of weights (`float` or `int`) to weight the occurrences of 
        predicted values before averaging. Uses uniform weights if `None`. 
 
    n_jobs : int, default=None 
        The number of jobs to run in parallel for ``fit``. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
    verbose : bool, default=False 
        If True, the time elapsed while fitting will be printed as it 
        is completed. 
 
        .. versionadded:: 0.23 
 
    Attributes 
    ---------- 
    estimators_ : list of regressors 
        The collection of fitted sub-estimators as defined in ``estimators`` 
        that are not 'drop'. 
 
    named_estimators_ : :class:`~sklearn.utils.Bunch` 
        Attribute to access any fitted sub-estimators by name. 
 
        .. versionadded:: 0.20 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. Only defined if the 
        underlying regressor exposes such an attribute when fit. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Only defined if the 
        underlying estimators expose such an attribute when fit. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    VotingClassifier : Soft Voting/Majority Rule classifier. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.linear_model import LinearRegression 
    &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor 
    &gt;&gt;&gt; from sklearn.ensemble import VotingRegressor 
    &gt;&gt;&gt; from sklearn.neighbors import KNeighborsRegressor 
    &gt;&gt;&gt; r1 = LinearRegression() 
    &gt;&gt;&gt; r2 = RandomForestRegressor(n_estimators=10, random_state=1) 
    &gt;&gt;&gt; r3 = KNeighborsRegressor() 
    &gt;&gt;&gt; X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]]) 
    &gt;&gt;&gt; y = np.array([2, 6, 12, 20, 30, 42]) 
    &gt;&gt;&gt; er = VotingRegressor([('lr', r1), ('rf', r2), ('r3', r3)]) 
    &gt;&gt;&gt; print(er.fit(X, y).predict(X)) 
    [ 6.8...  8.4... 12.5... 17.8... 26...  34...] 
 
    In the following example, we drop the `'lr'` estimator with 
    :meth:`~VotingRegressor.set_params` and fit the remaining two estimators: 
 
    &gt;&gt;&gt; er = er.set_params(lr='drop') 
    &gt;&gt;&gt; er = er.fit(X, y) 
    &gt;&gt;&gt; len(er.estimators_) 
    2 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">estimators</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">weights=</span><span class="s3">None, </span><span class="s1">n_jobs=</span><span class="s3">None, </span><span class="s1">verbose=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s1">super().__init__(estimators=estimators)</span>
        <span class="s1">self.weights = weights</span>
        <span class="s1">self.n_jobs = n_jobs</span>
        <span class="s1">self.verbose = verbose</span>

    <span class="s1">@_fit_context(</span>
        <span class="s2"># estimators in VotingRegressor.estimators are not validated yet</span>
        <span class="s1">prefer_skip_nested_validation=</span><span class="s3">False</span>
    <span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the estimators. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training vectors, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
 
        y : array-like of shape (n_samples,) 
            Target values. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. If None, then samples are equally weighted. 
            Note that this is supported only if all underlying estimators 
            support sample weights. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">y = column_or_1d(y</span><span class="s3">, </span><span class="s1">warn=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">super().fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight)</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict regression target for X. 
 
        The predicted regression target of an input sample is computed as the 
        mean predicted regression targets of the estimators in the ensemble. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The input samples. 
 
        Returns 
        ------- 
        y : ndarray of shape (n_samples,) 
            The predicted values. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">return </span><span class="s1">np.average(self._predict(X)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">weights=self._weights_not_none)</span>

    <span class="s3">def </span><span class="s1">transform(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Return predictions for X for each estimator. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The input samples. 
 
        Returns 
        ------- 
        predictions : ndarray of shape (n_samples, n_classifiers) 
            Values predicted by each regressor. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">return </span><span class="s1">self._predict(X)</span>

    <span class="s3">def </span><span class="s1">get_feature_names_out(self</span><span class="s3">, </span><span class="s1">input_features=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Get output feature names for transformation. 
 
        Parameters 
        ---------- 
        input_features : array-like of str or None, default=None 
            Not used, present here for API consistency by convention. 
 
        Returns 
        ------- 
        feature_names_out : ndarray of str objects 
            Transformed feature names. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self</span><span class="s3">, </span><span class="s4">&quot;n_features_in_&quot;</span><span class="s1">)</span>
        <span class="s1">_check_feature_names_in(self</span><span class="s3">, </span><span class="s1">input_features</span><span class="s3">, </span><span class="s1">generate_names=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">class_name = self.__class__.__name__.lower()</span>
        <span class="s3">return </span><span class="s1">np.asarray(</span>
            <span class="s1">[</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">class_name</span><span class="s3">}</span><span class="s4">_</span><span class="s3">{</span><span class="s1">name</span><span class="s3">}</span><span class="s4">&quot; </span><span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">est </span><span class="s3">in </span><span class="s1">self.estimators </span><span class="s3">if </span><span class="s1">est != </span><span class="s4">&quot;drop&quot;</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">dtype=object</span><span class="s3">,</span>
        <span class="s1">)</span>
</pre>
</body>
</html>