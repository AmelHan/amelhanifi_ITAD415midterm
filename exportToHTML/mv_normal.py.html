<html>
<head>
<title>mv_normal.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
mv_normal.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot;Multivariate Normal and t distributions 
 
 
 
Created on Sat May 28 15:38:23 2011 
 
@author: Josef Perktold 
 
TODO: 
* renaming, 
    - after adding t distribution, cov does not make sense for Sigma    DONE 
    - should mean also be renamed to mu, if there will be distributions 
      with mean != mu 
* not sure about corner cases 
    - behavior with (almost) singular sigma or transforms 
    - df &lt;= 2, is everything correct if variance is not finite or defined ? 
* check to return possibly univariate distribution for marginals or conditional 
    distributions, does univariate special case work? seems ok for conditional 
* are all the extra transformation methods useful outside of testing ? 
  - looks like I have some mixup in definitions of standardize, normalize 
* new methods marginal, conditional, ... just added, typos ? 
  - largely tested for MVNormal, not yet for MVT   DONE 
* conditional: reusing, vectorizing, should we reuse a projection matrix or 
  allow for a vectorized, conditional_mean similar to OLS.predict 
* add additional things similar to LikelihoodModelResults? quadratic forms, 
  F distribution, and others ??? 
* add Delta method for nonlinear functions here, current function is hidden 
  somewhere in miscmodels 
* raise ValueErrors for wrong input shapes, currently only partially checked 
 
* quantile method (ppf for equal bounds for multiple testing) is missing 
  http://svitsrv25.epfl.ch/R-doc/library/mvtnorm/html/qmvt.html seems to use 
  just a root finder for inversion of cdf 
 
* normalize has ambiguous definition, and mixing it up in different versions 
  std from sigma or std from cov ? 
  I would like to get what I need for mvt-cdf, or not 
  univariate standard t distribution has scale=1 but std&gt;1 
  FIXED: add std_sigma, and normalize uses std_sigma 
 
* more work: bivariate distributions, 
  inherit from multivariate but overwrite some methods for better efficiency, 
  e.g. cdf and expect 
 
I kept the original MVNormal0 class as reference, can be deleted 
 
 
See Also 
-------- 
sandbox/examples/ex_mvelliptical.py 
 
Examples 
-------- 
 
Note, several parts of these examples are random and the numbers will not be 
(exactly) the same. 
 
&gt;&gt;&gt; import numpy as np 
&gt;&gt;&gt; import statsmodels.sandbox.distributions.mv_normal as mvd 
&gt;&gt;&gt; 
&gt;&gt;&gt; from numpy.testing import assert_array_almost_equal 
&gt;&gt;&gt; 
&gt;&gt;&gt; cov3 = np.array([[ 1.  ,  0.5 ,  0.75], 
...                    [ 0.5 ,  1.5 ,  0.6 ], 
...                    [ 0.75,  0.6 ,  2.  ]]) 
 
&gt;&gt;&gt; mu = np.array([-1, 0.0, 2.0]) 
 
multivariate normal distribution 
-------------------------------- 
 
&gt;&gt;&gt; mvn3 = mvd.MVNormal(mu, cov3) 
&gt;&gt;&gt; mvn3.rvs(size=3) 
array([[-0.08559948, -1.0319881 ,  1.76073533], 
       [ 0.30079522,  0.55859618,  4.16538667], 
       [-1.36540091, -1.50152847,  3.87571161]]) 
 
&gt;&gt;&gt; mvn3.std 
array([ 1.        ,  1.22474487,  1.41421356]) 
&gt;&gt;&gt; a = [0.0, 1.0, 1.5] 
&gt;&gt;&gt; mvn3.pdf(a) 
0.013867410439318712 
&gt;&gt;&gt; mvn3.cdf(a) 
0.31163181123730122 
 
Monte Carlo integration 
 
&gt;&gt;&gt; mvn3.expect_mc(lambda x: (x&lt;a).all(-1), size=100000) 
0.30958999999999998 
&gt;&gt;&gt; mvn3.expect_mc(lambda x: (x&lt;a).all(-1), size=1000000) 
0.31197399999999997 
 
multivariate t distribution 
--------------------------- 
 
&gt;&gt;&gt; mvt3 = mvd.MVT(mu, cov3, 4) 
&gt;&gt;&gt; mvt3.rvs(size=4) 
array([[-0.94185437,  0.3933273 ,  2.40005487], 
       [ 0.07563648,  0.06655433,  7.90752238], 
       [ 1.06596474,  0.32701158,  2.03482886], 
       [ 3.80529746,  7.0192967 ,  8.41899229]]) 
 
&gt;&gt;&gt; mvt3.pdf(a) 
0.010402959362646937 
&gt;&gt;&gt; mvt3.cdf(a) 
0.30269483623249821 
&gt;&gt;&gt; mvt3.expect_mc(lambda x: (x&lt;a).all(-1), size=1000000) 
0.30271199999999998 
 
&gt;&gt;&gt; mvt3.cov 
array([[ 2. ,  1. ,  1.5], 
       [ 1. ,  3. ,  1.2], 
       [ 1.5,  1.2,  4. ]]) 
&gt;&gt;&gt; mvt3.corr 
array([[ 1.        ,  0.40824829,  0.53033009], 
       [ 0.40824829,  1.        ,  0.34641016], 
       [ 0.53033009,  0.34641016,  1.        ]]) 
 
get normalized distribution 
 
&gt;&gt;&gt; mvt3n = mvt3.normalized() 
&gt;&gt;&gt; mvt3n.sigma 
array([[ 1.        ,  0.40824829,  0.53033009], 
       [ 0.40824829,  1.        ,  0.34641016], 
       [ 0.53033009,  0.34641016,  1.        ]]) 
&gt;&gt;&gt; mvt3n.cov 
array([[ 2.        ,  0.81649658,  1.06066017], 
       [ 0.81649658,  2.        ,  0.69282032], 
       [ 1.06066017,  0.69282032,  2.        ]]) 
 
What's currently there? 
 
&gt;&gt;&gt; [i for i in dir(mvn3) if not i[0]=='_'] 
['affine_transformed', 'cdf', 'cholsigmainv', 'conditional', 'corr', 'cov', 
'expect_mc', 'extra_args', 'logdetsigma', 'logpdf', 'marginal', 'mean', 
'normalize', 'normalized', 'normalized2', 'nvars', 'pdf', 'rvs', 'sigma', 
'sigmainv', 'standardize', 'standardized', 'std', 'std_sigma', 'whiten'] 
 
&gt;&gt;&gt; [i for i in dir(mvt3) if not i[0]=='_'] 
['affine_transformed', 'cdf', 'cholsigmainv', 'corr', 'cov', 'df', 'expect_mc', 
'extra_args', 'logdetsigma', 'logpdf', 'marginal', 'mean', 'normalize', 
'normalized', 'normalized2', 'nvars', 'pdf', 'rvs', 'sigma', 'sigmainv', 
'standardize', 'standardized', 'std', 'std_sigma', 'whiten'] 
 
&quot;&quot;&quot;</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">special</span>

<span class="s3">from </span><span class="s1">statsmodels.sandbox.distributions.multivariate </span><span class="s3">import </span><span class="s1">mvstdtprob</span>
<span class="s3">from </span><span class="s1">.extras </span><span class="s3">import </span><span class="s1">mvnormcdf</span>


<span class="s3">def </span><span class="s1">expect_mc(dist</span><span class="s3">, </span><span class="s1">func=</span><span class="s3">lambda </span><span class="s1">x: </span><span class="s4">1</span><span class="s3">, </span><span class="s1">size=</span><span class="s4">50000</span><span class="s1">):</span>
    <span class="s2">'''calculate expected value of function by Monte Carlo integration 
 
    Parameters 
    ---------- 
    dist : distribution instance 
        needs to have rvs defined as a method for drawing random numbers 
    func : callable 
        function for which expectation is calculated, this function needs to 
        be vectorized, integration is over axis=0 
    size : int 
        number of random samples to use in the Monte Carlo integration, 
 
 
    Notes 
    ----- 
    this does not batch 
 
    Returns 
    ------- 
    expected value : ndarray 
        return of function func integrated over axis=0 by MonteCarlo, this will 
        have the same shape as the return of func without axis=0 
 
    Examples 
    -------- 
 
    integrate probability that both observations are negative 
 
    &gt;&gt;&gt; mvn = mve.MVNormal([0,0],2.) 
    &gt;&gt;&gt; mve.expect_mc(mvn, lambda x: (x&lt;np.array([0,0])).all(-1), size=100000) 
    0.25306000000000001 
 
    get tail probabilities of marginal distribution (should be 0.1) 
 
    &gt;&gt;&gt; c = stats.norm.isf(0.05, scale=np.sqrt(2.)) 
    &gt;&gt;&gt; expect_mc(mvn, lambda x: (np.abs(x)&gt;np.array([c, c])), size=100000) 
    array([ 0.09969,  0.0986 ]) 
 
    or calling the method 
 
    &gt;&gt;&gt; mvn.expect_mc(lambda x: (np.abs(x)&gt;np.array([c, c])), size=100000) 
    array([ 0.09937,  0.10075]) 
 
 
    '''</span>
    <span class="s3">def </span><span class="s1">fun(x):</span>
        <span class="s3">return </span><span class="s1">func(x) </span><span class="s0"># * dist.pdf(x)</span>
    <span class="s1">rvs = dist.rvs(size=size)</span>
    <span class="s3">return </span><span class="s1">fun(rvs).mean(</span><span class="s4">0</span><span class="s1">)</span>

<span class="s3">def </span><span class="s1">expect_mc_bounds(dist</span><span class="s3">, </span><span class="s1">func=</span><span class="s3">lambda </span><span class="s1">x: </span><span class="s4">1</span><span class="s3">, </span><span class="s1">size=</span><span class="s4">50000</span><span class="s3">, </span><span class="s1">lower=</span><span class="s3">None, </span><span class="s1">upper=</span><span class="s3">None,</span>
                     <span class="s1">conditional=</span><span class="s3">False, </span><span class="s1">overfact=</span><span class="s4">1.2</span><span class="s1">):</span>
    <span class="s2">'''calculate expected value of function by Monte Carlo integration 
 
    Parameters 
    ---------- 
    dist : distribution instance 
        needs to have rvs defined as a method for drawing random numbers 
    func : callable 
        function for which expectation is calculated, this function needs to 
        be vectorized, integration is over axis=0 
    size : int 
        minimum number of random samples to use in the Monte Carlo integration, 
        the actual number used can be larger because of oversampling. 
    lower : None or array_like 
        lower integration bounds, if None, then it is set to -inf 
    upper : None or array_like 
        upper integration bounds, if None, then it is set to +inf 
    conditional : bool 
        If true, then the expectation is conditional on being in within 
        [lower, upper] bounds, otherwise it is unconditional 
    overfact : float 
        oversampling factor, the actual number of random variables drawn in 
        each attempt are overfact * remaining draws. Extra draws are also 
        used in the integration. 
 
 
    Notes 
    ----- 
    this does not batch 
 
    Returns 
    ------- 
    expected value : ndarray 
        return of function func integrated over axis=0 by MonteCarlo, this will 
        have the same shape as the return of func without axis=0 
 
    Examples 
    -------- 
    &gt;&gt;&gt; mvn = mve.MVNormal([0,0],2.) 
    &gt;&gt;&gt; mve.expect_mc_bounds(mvn, lambda x: np.ones(x.shape[0]), 
                                lower=[-10,-10],upper=[0,0]) 
    0.24990416666666668 
 
    get 3 marginal moments with one integration 
 
    &gt;&gt;&gt; mvn = mve.MVNormal([0,0],1.) 
    &gt;&gt;&gt; mve.expect_mc_bounds(mvn, lambda x: np.dstack([x, x**2, x**3, x**4]), 
        lower=[-np.inf,-np.inf], upper=[np.inf,np.inf]) 
    array([[  2.88629497e-03,   9.96706297e-01,  -2.51005344e-03, 
              2.95240921e+00], 
           [ -5.48020088e-03,   9.96004409e-01,  -2.23803072e-02, 
              2.96289203e+00]]) 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; [stats.norm.moment(i) for i in [1,2,3,4]] 
    [0.0, 1.0, 0.0, 3.0] 
 
 
    '''</span>
    <span class="s0">#call rvs once to find length of random vector</span>
    <span class="s1">rvsdim = dist.rvs(size=</span><span class="s4">1</span><span class="s1">).shape[-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s3">if </span><span class="s1">lower </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">lower = -np.inf * np.ones(rvsdim)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">lower = np.asarray(lower)</span>
    <span class="s3">if </span><span class="s1">upper </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">upper = np.inf * np.ones(rvsdim)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">upper = np.asarray(upper)</span>

    <span class="s3">def </span><span class="s1">fun(x):</span>
        <span class="s3">return </span><span class="s1">func(x) </span><span class="s0"># * dist.pdf(x)</span>

    <span class="s1">rvsli = []</span>
    <span class="s1">used = </span><span class="s4">0 </span><span class="s0">#remain = size  #inplace changes size</span>
    <span class="s1">total = </span><span class="s4">0</span>
    <span class="s3">while True</span><span class="s1">:</span>
        <span class="s1">remain = size - used  </span><span class="s0">#just a temp variable</span>
        <span class="s1">rvs = dist.rvs(size=int(remain * overfact))</span>
        <span class="s1">total += int(size * overfact)</span>

        <span class="s1">rvsok = rvs[((rvs &gt;= lower) &amp; (rvs &lt;= upper)).all(-</span><span class="s4">1</span><span class="s1">)]</span>
        <span class="s0">#if rvsok.ndim == 1: #possible shape problems if only 1 random vector</span>
        <span class="s1">rvsok = np.atleast_2d(rvsok)</span>
        <span class="s1">used += rvsok.shape[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s1">rvsli.append(rvsok)   </span><span class="s0">#[:remain]) use extras instead</span>
        <span class="s1">print(used)</span>
        <span class="s3">if </span><span class="s1">used &gt;= size:</span>
            <span class="s3">break</span>
    <span class="s1">rvs = np.vstack(rvsli)</span>
    <span class="s1">print(rvs.shape)</span>
    <span class="s3">assert </span><span class="s1">used == rvs.shape[</span><span class="s4">0</span><span class="s1">] </span><span class="s0">#saftey check</span>
    <span class="s1">mean_conditional = fun(rvs).mean(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">conditional:</span>
        <span class="s3">return </span><span class="s1">mean_conditional</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">mean_conditional * (used * </span><span class="s4">1. </span><span class="s1">/ total)</span>


<span class="s3">def </span><span class="s1">bivariate_normal(x</span><span class="s3">, </span><span class="s1">mu</span><span class="s3">, </span><span class="s1">cov):</span>
    <span class="s2">&quot;&quot;&quot; 
    Bivariate Gaussian distribution for equal shape *X*, *Y*. 
 
    See `bivariate normal 
    &lt;http://mathworld.wolfram.com/BivariateNormalDistribution.html&gt;`_ 
    at mathworld. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">Y = np.transpose(x)</span>
    <span class="s1">mux</span><span class="s3">, </span><span class="s1">muy = mu</span>
    <span class="s1">sigmax</span><span class="s3">, </span><span class="s1">sigmaxy</span><span class="s3">, </span><span class="s1">tmp</span><span class="s3">, </span><span class="s1">sigmay = np.ravel(cov)</span>
    <span class="s1">sigmax</span><span class="s3">, </span><span class="s1">sigmay = np.sqrt(sigmax)</span><span class="s3">, </span><span class="s1">np.sqrt(sigmay)</span>
    <span class="s1">Xmu = X-mux</span>
    <span class="s1">Ymu = Y-muy</span>

    <span class="s1">rho = sigmaxy/(sigmax*sigmay)</span>
    <span class="s1">z = Xmu**</span><span class="s4">2</span><span class="s1">/sigmax**</span><span class="s4">2 </span><span class="s1">+ Ymu**</span><span class="s4">2</span><span class="s1">/sigmay**</span><span class="s4">2 </span><span class="s1">- </span><span class="s4">2</span><span class="s1">*rho*Xmu*Ymu/(sigmax*sigmay)</span>
    <span class="s1">denom = </span><span class="s4">2</span><span class="s1">*np.pi*sigmax*sigmay*np.sqrt(</span><span class="s4">1</span><span class="s1">-rho**</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">np.exp( -z/(</span><span class="s4">2</span><span class="s1">*(</span><span class="s4">1</span><span class="s1">-rho**</span><span class="s4">2</span><span class="s1">))) / denom</span>



<span class="s3">class </span><span class="s1">BivariateNormal:</span>


    <span class="s0">#TODO: make integration limits more flexible</span>
    <span class="s0">#      or normalize before integration</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">mean</span><span class="s3">, </span><span class="s1">cov):</span>
        <span class="s1">self.mean = mu</span>
        <span class="s1">self.cov = cov</span>
        <span class="s1">self.sigmax</span><span class="s3">, </span><span class="s1">self.sigmaxy</span><span class="s3">, </span><span class="s1">tmp</span><span class="s3">, </span><span class="s1">self.sigmay = np.ravel(cov)</span>
        <span class="s1">self.nvars = </span><span class="s4">2</span>

    <span class="s3">def </span><span class="s1">rvs(self</span><span class="s3">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s3">return </span><span class="s1">np.random.multivariate_normal(self.mean</span><span class="s3">, </span><span class="s1">self.cov</span><span class="s3">, </span><span class="s1">size=size)</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s3">return </span><span class="s1">bivariate_normal(x</span><span class="s3">, </span><span class="s1">self.mean</span><span class="s3">, </span><span class="s1">self.cov)</span>

    <span class="s3">def </span><span class="s1">logpdf(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s0">#TODO: replace this</span>
        <span class="s3">return </span><span class="s1">np.log(self.pdf(x))</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s3">return </span><span class="s1">self.expect(upper=x)</span>

    <span class="s3">def </span><span class="s1">expect(self</span><span class="s3">, </span><span class="s1">func=</span><span class="s3">lambda </span><span class="s1">x: </span><span class="s4">1</span><span class="s3">, </span><span class="s1">lower=(-</span><span class="s4">10</span><span class="s3">,</span><span class="s1">-</span><span class="s4">10</span><span class="s1">)</span><span class="s3">, </span><span class="s1">upper=(</span><span class="s4">10</span><span class="s3">,</span><span class="s4">10</span><span class="s1">)):</span>
        <span class="s3">def </span><span class="s1">fun(x</span><span class="s3">, </span><span class="s1">y):</span>
            <span class="s1">x = np.column_stack((x</span><span class="s3">,</span><span class="s1">y))</span>
            <span class="s3">return </span><span class="s1">func(x) * self.pdf(x)</span>
        <span class="s3">from </span><span class="s1">scipy.integrate </span><span class="s3">import </span><span class="s1">dblquad</span>
        <span class="s3">return </span><span class="s1">dblquad(fun</span><span class="s3">, </span><span class="s1">lower[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">upper[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, lambda </span><span class="s1">y: lower[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
                       <span class="s3">lambda </span><span class="s1">y: upper[</span><span class="s4">1</span><span class="s1">])</span>

    <span class="s3">def </span><span class="s1">kl(self</span><span class="s3">, </span><span class="s1">other):</span>
        <span class="s2">'''Kullback-Leibler divergence between this and another distribution 
 
        int f(x) (log f(x) - log g(x)) dx 
 
        where f is the pdf of self, and g is the pdf of other 
 
        uses double integration with scipy.integrate.dblquad 
 
        limits currently hardcoded 
 
        '''</span>
        <span class="s1">fun = </span><span class="s3">lambda </span><span class="s1">x : self.logpdf(x) - other.logpdf(x)</span>
        <span class="s3">return </span><span class="s1">self.expect(fun)</span>

    <span class="s3">def </span><span class="s1">kl_mc(self</span><span class="s3">, </span><span class="s1">other</span><span class="s3">, </span><span class="s1">size=</span><span class="s4">500000</span><span class="s1">):</span>
        <span class="s1">fun = </span><span class="s3">lambda </span><span class="s1">x : self.logpdf(x) - other.logpdf(x)</span>
        <span class="s1">rvs = self.rvs(size=size)</span>
        <span class="s3">return </span><span class="s1">fun(rvs).mean()</span>

<span class="s3">class </span><span class="s1">MVElliptical:</span>
    <span class="s2">'''Base Class for multivariate elliptical distributions, normal and t 
 
    contains common initialization, and some common methods 
    subclass needs to implement at least rvs and logpdf methods 
 
    '''</span>
    <span class="s0">#getting common things between normal and t distribution</span>


    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">mean</span><span class="s3">, </span><span class="s1">sigma</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwds):</span>
        <span class="s2">'''initialize instance 
 
        Parameters 
        ---------- 
        mean : array_like 
            parameter mu (might be renamed), for symmetric distributions this 
            is the mean 
        sigma : array_like, 2d 
            dispersion matrix, covariance matrix in normal distribution, but 
            only proportional to covariance matrix in t distribution 
        args : list 
            distribution specific arguments, e.g. df for t distribution 
        kwds : dict 
            currently not used 
 
        '''</span>

        <span class="s1">self.extra_args = []</span>
        <span class="s1">self.mean = np.asarray(mean)</span>
        <span class="s1">self.sigma = sigma = np.asarray(sigma)</span>
        <span class="s1">sigma = np.squeeze(sigma)</span>
        <span class="s1">self.nvars = nvars = len(mean)</span>
        <span class="s0">#self.covchol = np.linalg.cholesky(sigma)</span>


        <span class="s0">#in the following sigma is original, self.sigma is full matrix</span>
        <span class="s3">if </span><span class="s1">sigma.shape == ():</span>
            <span class="s0">#iid</span>
            <span class="s1">self.sigma = np.eye(nvars) * sigma</span>
            <span class="s1">self.sigmainv = np.eye(nvars) / sigma</span>
            <span class="s1">self.cholsigmainv = np.eye(nvars) / np.sqrt(sigma)</span>
        <span class="s3">elif </span><span class="s1">(sigma.ndim == </span><span class="s4">1</span><span class="s1">) </span><span class="s3">and </span><span class="s1">(len(sigma) == nvars):</span>
            <span class="s0">#independent heteroskedastic</span>
            <span class="s1">self.sigma = np.diag(sigma)</span>
            <span class="s1">self.sigmainv = np.diag(</span><span class="s4">1. </span><span class="s1">/ sigma)</span>
            <span class="s1">self.cholsigmainv = np.diag( </span><span class="s4">1. </span><span class="s1">/ np.sqrt(sigma))</span>
        <span class="s3">elif </span><span class="s1">sigma.shape == (nvars</span><span class="s3">, </span><span class="s1">nvars): </span><span class="s0">#python tuple comparison</span>
            <span class="s0">#general</span>
            <span class="s1">self.sigmainv = np.linalg.pinv(sigma)</span>
            <span class="s1">self.cholsigmainv = np.linalg.cholesky(self.sigmainv).T</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'sigma has invalid shape'</span><span class="s1">)</span>

        <span class="s0">#store logdetsigma for logpdf</span>
        <span class="s1">self.logdetsigma = np.log(np.linalg.det(self.sigma))</span>

    <span class="s3">def </span><span class="s1">rvs(self</span><span class="s3">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s2">'''random variable 
 
        Parameters 
        ---------- 
        size : int or tuple 
            the number and shape of random variables to draw. 
 
        Returns 
        ------- 
        rvs : ndarray 
            the returned random variables with shape given by size and the 
            dimension of the multivariate random vector as additional last 
            dimension 
 
 
        '''</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">def </span><span class="s1">logpdf(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">'''logarithm of probability density function 
 
        Parameters 
        ---------- 
        x : array_like 
            can be 1d or 2d, if 2d, then each row is taken as independent 
            multivariate random vector 
 
        Returns 
        ------- 
        logpdf : float or array 
            probability density value of each random vector 
 
 
        this should be made to work with 2d x, 
        with multivariate normal vector in each row and iid across rows 
        does not work now because of dot in whiten 
 
        '''</span>


        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">**kwds):</span>
        <span class="s2">'''cumulative distribution function 
 
        Parameters 
        ---------- 
        x : array_like 
            can be 1d or 2d, if 2d, then each row is taken as independent 
            multivariate random vector 
        kwds : dict 
            contains options for the numerical calculation of the cdf 
 
        Returns 
        ------- 
        cdf : float or array 
            probability density value of each random vector 
 
        '''</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>


    <span class="s3">def </span><span class="s1">affine_transformed(self</span><span class="s3">, </span><span class="s1">shift</span><span class="s3">, </span><span class="s1">scale_matrix):</span>
        <span class="s2">'''affine transformation define in subclass because of distribution 
        specific restrictions'''</span>
        <span class="s0">#implemented in subclass at least for now</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">def </span><span class="s1">whiten(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">&quot;&quot;&quot; 
        whiten the data by linear transformation 
 
        Parameters 
        ---------- 
        x : array_like, 1d or 2d 
            Data to be whitened, if 2d then each row contains an independent 
            sample of the multivariate random vector 
 
        Returns 
        ------- 
        np.dot(x, self.cholsigmainv.T) 
 
        Notes 
        ----- 
        This only does rescaling, it does not subtract the mean, use standardize 
        for this instead 
 
        See Also 
        -------- 
        standardize : subtract mean and rescale to standardized random variable. 
        &quot;&quot;&quot;</span>
        <span class="s1">x = np.asarray(x)</span>
        <span class="s3">return </span><span class="s1">np.dot(x</span><span class="s3">, </span><span class="s1">self.cholsigmainv.T)</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">'''probability density function 
 
        Parameters 
        ---------- 
        x : array_like 
            can be 1d or 2d, if 2d, then each row is taken as independent 
            multivariate random vector 
 
        Returns 
        ------- 
        pdf : float or array 
            probability density value of each random vector 
 
        '''</span>
        <span class="s3">return </span><span class="s1">np.exp(self.logpdf(x))</span>

    <span class="s3">def </span><span class="s1">standardize(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">'''standardize the random variable, i.e. subtract mean and whiten 
 
        Parameters 
        ---------- 
        x : array_like, 1d or 2d 
            Data to be whitened, if 2d then each row contains an independent 
            sample of the multivariate random vector 
 
        Returns 
        ------- 
        np.dot(x - self.mean, self.cholsigmainv.T) 
 
        Notes 
        ----- 
 
 
        See Also 
        -------- 
        whiten : rescale random variable, standardize without subtracting mean. 
 
 
        '''</span>
        <span class="s3">return </span><span class="s1">self.whiten(x - self.mean)</span>

    <span class="s3">def </span><span class="s1">standardized(self):</span>
        <span class="s2">'''return new standardized MVNormal instance 
        '''</span>
        <span class="s3">return </span><span class="s1">self.affine_transformed(-self.mean</span><span class="s3">, </span><span class="s1">self.cholsigmainv)</span>


    <span class="s3">def </span><span class="s1">normalize(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">'''normalize the random variable, i.e. subtract mean and rescale 
 
        The distribution will have zero mean and sigma equal to correlation 
 
        Parameters 
        ---------- 
        x : array_like, 1d or 2d 
            Data to be whitened, if 2d then each row contains an independent 
            sample of the multivariate random vector 
 
        Returns 
        ------- 
        (x - self.mean)/std_sigma 
 
        Notes 
        ----- 
 
 
        See Also 
        -------- 
        whiten : rescale random variable, standardize without subtracting mean. 
 
 
        '''</span>
        <span class="s1">std_ = np.atleast_2d(self.std_sigma)</span>
        <span class="s3">return </span><span class="s1">(x - self.mean)/std_ </span><span class="s0">#/std_.T</span>

    <span class="s3">def </span><span class="s1">normalized(self</span><span class="s3">, </span><span class="s1">demeaned=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s2">'''return a normalized distribution where sigma=corr 
 
        if demeaned is True, then mean will be set to zero 
 
        '''</span>
        <span class="s3">if </span><span class="s1">demeaned:</span>
            <span class="s1">mean_new = np.zeros_like(self.mean)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">mean_new = self.mean / self.std_sigma</span>
        <span class="s1">sigma_new = self.corr</span>
        <span class="s1">args = [getattr(self</span><span class="s3">, </span><span class="s1">ea) </span><span class="s3">for </span><span class="s1">ea </span><span class="s3">in </span><span class="s1">self.extra_args]</span>
        <span class="s3">return </span><span class="s1">self.__class__(mean_new</span><span class="s3">, </span><span class="s1">sigma_new</span><span class="s3">, </span><span class="s1">*args)</span>

    <span class="s3">def </span><span class="s1">normalized2(self</span><span class="s3">, </span><span class="s1">demeaned=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s2">'''return a normalized distribution where sigma=corr 
 
 
 
        second implementation for testing affine transformation 
        '''</span>
        <span class="s3">if </span><span class="s1">demeaned:</span>
            <span class="s1">shift = -self.mean</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">shift = self.mean * (</span><span class="s4">1. </span><span class="s1">/ self.std_sigma - </span><span class="s4">1.</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">self.affine_transformed(shift</span><span class="s3">, </span><span class="s1">np.diag(</span><span class="s4">1. </span><span class="s1">/ self.std_sigma))</span>
        <span class="s0">#the following &quot;standardizes&quot; cov instead</span>
        <span class="s0">#return self.affine_transformed(shift, self.cholsigmainv)</span>



    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">std(self):</span>
        <span class="s2">'''standard deviation, square root of diagonal elements of cov 
        '''</span>
        <span class="s3">return </span><span class="s1">np.sqrt(np.diag(self.cov))</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">std_sigma(self):</span>
        <span class="s2">'''standard deviation, square root of diagonal elements of sigma 
        '''</span>
        <span class="s3">return </span><span class="s1">np.sqrt(np.diag(self.sigma))</span>


    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">corr(self):</span>
        <span class="s2">'''correlation matrix'''</span>
        <span class="s3">return </span><span class="s1">self.cov / np.outer(self.std</span><span class="s3">, </span><span class="s1">self.std)</span>

    <span class="s1">expect_mc = expect_mc</span>

    <span class="s3">def </span><span class="s1">marginal(self</span><span class="s3">, </span><span class="s1">indices):</span>
        <span class="s2">'''return marginal distribution for variables given by indices 
 
        this should be correct for normal and t distribution 
 
        Parameters 
        ---------- 
        indices : array_like, int 
            list of indices of variables in the marginal distribution 
 
        Returns 
        ------- 
        mvdist : instance 
            new instance of the same multivariate distribution class that 
            contains the marginal distribution of the variables given in 
            indices 
 
        '''</span>
        <span class="s1">indices = np.asarray(indices)</span>
        <span class="s1">mean_new = self.mean[indices]</span>
        <span class="s1">sigma_new = self.sigma[indices[:</span><span class="s3">,None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">indices]</span>
        <span class="s1">args = [getattr(self</span><span class="s3">, </span><span class="s1">ea) </span><span class="s3">for </span><span class="s1">ea </span><span class="s3">in </span><span class="s1">self.extra_args]</span>
        <span class="s3">return </span><span class="s1">self.__class__(mean_new</span><span class="s3">, </span><span class="s1">sigma_new</span><span class="s3">, </span><span class="s1">*args)</span>


<span class="s0">#parts taken from linear_model, but heavy adjustments</span>
<span class="s3">class </span><span class="s1">MVNormal0:</span>
    <span class="s2">'''Class for Multivariate Normal Distribution 
 
    original full version, kept for testing, new version inherits from 
    MVElliptical 
 
    uses Cholesky decomposition of covariance matrix for the transformation 
    of the data 
 
    '''</span>


    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">mean</span><span class="s3">, </span><span class="s1">cov):</span>
        <span class="s1">self.mean = mean</span>
        <span class="s1">self.cov = cov = np.asarray(cov)</span>
        <span class="s1">cov = np.squeeze(cov)</span>
        <span class="s1">self.nvars = nvars = len(mean)</span>


        <span class="s0">#in the following cov is original, self.cov is full matrix</span>
        <span class="s3">if </span><span class="s1">cov.shape == ():</span>
            <span class="s0">#iid</span>
            <span class="s1">self.cov = np.eye(nvars) * cov</span>
            <span class="s1">self.covinv = np.eye(nvars) / cov</span>
            <span class="s1">self.cholcovinv = np.eye(nvars) / np.sqrt(cov)</span>
        <span class="s3">elif </span><span class="s1">(cov.ndim == </span><span class="s4">1</span><span class="s1">) </span><span class="s3">and </span><span class="s1">(len(cov) == nvars):</span>
            <span class="s0">#independent heteroskedastic</span>
            <span class="s1">self.cov = np.diag(cov)</span>
            <span class="s1">self.covinv = np.diag(</span><span class="s4">1. </span><span class="s1">/ cov)</span>
            <span class="s1">self.cholcovinv = np.diag( </span><span class="s4">1. </span><span class="s1">/ np.sqrt(cov))</span>
        <span class="s3">elif </span><span class="s1">cov.shape == (nvars</span><span class="s3">, </span><span class="s1">nvars): </span><span class="s0">#python tuple comparison</span>
            <span class="s0">#general</span>
            <span class="s1">self.covinv = np.linalg.pinv(cov)</span>
            <span class="s1">self.cholcovinv = np.linalg.cholesky(self.covinv).T</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'cov has invalid shape'</span><span class="s1">)</span>

        <span class="s0">#store logdetcov for logpdf</span>
        <span class="s1">self.logdetcov = np.log(np.linalg.det(self.cov))</span>

    <span class="s3">def </span><span class="s1">whiten(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">&quot;&quot;&quot; 
        whiten the data by linear transformation 
 
        Parameters 
        ---------- 
        X : array_like, 1d or 2d 
            Data to be whitened, if 2d then each row contains an independent 
            sample of the multivariate random vector 
 
        Returns 
        ------- 
        np.dot(x, self.cholcovinv.T) 
 
        Notes 
        ----- 
        This only does rescaling, it does not subtract the mean, use standardize 
        for this instead 
 
        See Also 
        -------- 
        standardize : subtract mean and rescale to standardized random variable. 
        &quot;&quot;&quot;</span>
        <span class="s1">x = np.asarray(x)</span>
        <span class="s3">if </span><span class="s1">np.any(self.cov):</span>
            <span class="s0">#return np.dot(self.cholcovinv, x)</span>
            <span class="s3">return </span><span class="s1">np.dot(x</span><span class="s3">, </span><span class="s1">self.cholcovinv.T)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">x</span>

    <span class="s3">def </span><span class="s1">rvs(self</span><span class="s3">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s2">'''random variable 
 
        Parameters 
        ---------- 
        size : int or tuple 
            the number and shape of random variables to draw. 
 
        Returns 
        ------- 
        rvs : ndarray 
            the returned random variables with shape given by size and the 
            dimension of the multivariate random vector as additional last 
            dimension 
 
        Notes 
        ----- 
        uses numpy.random.multivariate_normal directly 
 
        '''</span>
        <span class="s3">return </span><span class="s1">np.random.multivariate_normal(self.mean</span><span class="s3">, </span><span class="s1">self.cov</span><span class="s3">, </span><span class="s1">size=size)</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">'''probability density function 
 
        Parameters 
        ---------- 
        x : array_like 
            can be 1d or 2d, if 2d, then each row is taken as independent 
            multivariate random vector 
 
        Returns 
        ------- 
        pdf : float or array 
            probability density value of each random vector 
 
        '''</span>

        <span class="s3">return </span><span class="s1">np.exp(self.logpdf(x))</span>

    <span class="s3">def </span><span class="s1">logpdf(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">'''logarithm of probability density function 
 
        Parameters 
        ---------- 
        x : array_like 
            can be 1d or 2d, if 2d, then each row is taken as independent 
            multivariate random vector 
 
        Returns 
        ------- 
        logpdf : float or array 
            probability density value of each random vector 
 
 
        this should be made to work with 2d x, 
        with multivariate normal vector in each row and iid across rows 
        does not work now because of dot in whiten 
 
        '''</span>
        <span class="s1">x = np.asarray(x)</span>
        <span class="s1">x_whitened = self.whiten(x - self.mean)</span>
        <span class="s1">SSR = np.sum(x_whitened**</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">llf = -SSR</span>
        <span class="s1">llf -= self.nvars * np.log(</span><span class="s4">2. </span><span class="s1">* np.pi)</span>
        <span class="s1">llf -= self.logdetcov</span>
        <span class="s1">llf *= </span><span class="s4">0.5</span>
        <span class="s3">return </span><span class="s1">llf</span>

    <span class="s1">expect_mc = expect_mc</span>


<span class="s3">class </span><span class="s1">MVNormal(MVElliptical):</span>
    <span class="s2">'''Class for Multivariate Normal Distribution 
 
    uses Cholesky decomposition of covariance matrix for the transformation 
    of the data 
 
    '''</span>
    <span class="s1">__name__ == </span><span class="s5">'Multivariate Normal Distribution'</span>


    <span class="s3">def </span><span class="s1">rvs(self</span><span class="s3">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s2">'''random variable 
 
        Parameters 
        ---------- 
        size : int or tuple 
            the number and shape of random variables to draw. 
 
        Returns 
        ------- 
        rvs : ndarray 
            the returned random variables with shape given by size and the 
            dimension of the multivariate random vector as additional last 
            dimension 
 
        Notes 
        ----- 
        uses numpy.random.multivariate_normal directly 
 
        '''</span>
        <span class="s3">return </span><span class="s1">np.random.multivariate_normal(self.mean</span><span class="s3">, </span><span class="s1">self.sigma</span><span class="s3">, </span><span class="s1">size=size)</span>

    <span class="s3">def </span><span class="s1">logpdf(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">'''logarithm of probability density function 
 
        Parameters 
        ---------- 
        x : array_like 
            can be 1d or 2d, if 2d, then each row is taken as independent 
            multivariate random vector 
 
        Returns 
        ------- 
        logpdf : float or array 
            probability density value of each random vector 
 
 
        this should be made to work with 2d x, 
        with multivariate normal vector in each row and iid across rows 
        does not work now because of dot in whiten 
 
        '''</span>
        <span class="s1">x = np.asarray(x)</span>
        <span class="s1">x_whitened = self.whiten(x - self.mean)</span>
        <span class="s1">SSR = np.sum(x_whitened**</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">llf = -SSR</span>
        <span class="s1">llf -= self.nvars * np.log(</span><span class="s4">2. </span><span class="s1">* np.pi)</span>
        <span class="s1">llf -= self.logdetsigma</span>
        <span class="s1">llf *= </span><span class="s4">0.5</span>
        <span class="s3">return </span><span class="s1">llf</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">**kwds):</span>
        <span class="s2">'''cumulative distribution function 
 
        Parameters 
        ---------- 
        x : array_like 
            can be 1d or 2d, if 2d, then each row is taken as independent 
            multivariate random vector 
        kwds : dict 
            contains options for the numerical calculation of the cdf 
 
        Returns 
        ------- 
        cdf : float or array 
            probability density value of each random vector 
 
        '''</span>
        <span class="s0">#lower = -np.inf * np.ones_like(x)</span>
        <span class="s0">#return mvstdnormcdf(lower, self.standardize(x), self.corr, **kwds)</span>
        <span class="s3">return </span><span class="s1">mvnormcdf(x</span><span class="s3">, </span><span class="s1">self.mean</span><span class="s3">, </span><span class="s1">self.cov</span><span class="s3">, </span><span class="s1">**kwds)</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">cov(self):</span>
        <span class="s2">'''covariance matrix'''</span>
        <span class="s3">return </span><span class="s1">self.sigma</span>

    <span class="s3">def </span><span class="s1">affine_transformed(self</span><span class="s3">, </span><span class="s1">shift</span><span class="s3">, </span><span class="s1">scale_matrix):</span>
        <span class="s2">'''return distribution of an affine transform 
 
        for full rank scale_matrix only 
 
        Parameters 
        ---------- 
        shift : array_like 
            shift of mean 
        scale_matrix : array_like 
            linear transformation matrix 
 
        Returns 
        ------- 
        mvt : instance of MVNormal 
            instance of multivariate normal distribution given by affine 
            transformation 
 
        Notes 
        ----- 
        the affine transformation is defined by 
        y = a + B x 
 
        where a is shift, 
        B is a scale matrix for the linear transformation 
 
        Notes 
        ----- 
        This should also work to select marginal distributions, but not 
        tested for this case yet. 
 
        currently only tested because it's called by standardized 
 
        '''</span>
        <span class="s1">B = scale_matrix  </span><span class="s0">#tmp variable</span>
        <span class="s1">mean_new = np.dot(B</span><span class="s3">, </span><span class="s1">self.mean) + shift</span>
        <span class="s1">sigma_new = np.dot(np.dot(B</span><span class="s3">, </span><span class="s1">self.sigma)</span><span class="s3">, </span><span class="s1">B.T)</span>
        <span class="s3">return </span><span class="s1">MVNormal(mean_new</span><span class="s3">, </span><span class="s1">sigma_new)</span>

    <span class="s3">def </span><span class="s1">conditional(self</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">values):</span>
        <span class="s2">r'''return conditional distribution 
 
        indices are the variables to keep, the complement is the conditioning 
        set 
        values are the values of the conditioning variables 
 
        \bar{\mu} = \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} \left( a - \mu_2 \right) 
 
        and covariance matrix 
 
        \overline{\Sigma} = \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}.T 
 
        Parameters 
        ---------- 
        indices : array_like, int 
            list of indices of variables in the marginal distribution 
        given : array_like 
            values of the conditioning variables 
 
        Returns 
        ------- 
        mvn : instance of MVNormal 
            new instance of the MVNormal class that contains the conditional 
            distribution of the variables given in indices for given 
             values of the excluded variables. 
 
 
        '''</span>
        <span class="s0">#indices need to be nd arrays for broadcasting</span>
        <span class="s1">keep = np.asarray(indices)</span>
        <span class="s1">given = np.asarray([i </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.nvars) </span><span class="s3">if </span><span class="s1">i </span><span class="s3">not in </span><span class="s1">keep])</span>
        <span class="s1">sigmakk = self.sigma[keep[:</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">keep]</span>
        <span class="s1">sigmagg = self.sigma[given[:</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">given]</span>
        <span class="s1">sigmakg = self.sigma[keep[:</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">given]</span>
        <span class="s1">sigmagk = self.sigma[given[:</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">keep]</span>


        <span class="s1">sigma_new = sigmakk - np.dot(sigmakg</span><span class="s3">, </span><span class="s1">np.linalg.solve(sigmagg</span><span class="s3">, </span><span class="s1">sigmagk))</span>
        <span class="s1">mean_new = self.mean[keep] +  \</span>
            <span class="s1">np.dot(sigmakg</span><span class="s3">, </span><span class="s1">np.linalg.solve(sigmagg</span><span class="s3">, </span><span class="s1">values-self.mean[given]))</span>

<span class="s0">#        #or</span>
<span class="s0">#        sig = np.linalg.solve(sigmagg, sigmagk).T</span>
<span class="s0">#        mean_new = self.mean[keep] + np.dot(sigmakg, values-self.mean[given])</span>
<span class="s0">#        sigma_new = sigmakk - np.dot(sigmakg, sig)</span>
        <span class="s3">return </span><span class="s1">MVNormal(mean_new</span><span class="s3">, </span><span class="s1">sigma_new)</span>


<span class="s0">#redefine some shortcuts</span>
<span class="s1">np_log = np.log</span>
<span class="s1">np_pi = np.pi</span>
<span class="s1">sps_gamln = special.gammaln</span>

<span class="s3">class </span><span class="s1">MVT(MVElliptical):</span>

    <span class="s1">__name__ == </span><span class="s5">'Multivariate Student T Distribution'</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">mean</span><span class="s3">, </span><span class="s1">sigma</span><span class="s3">, </span><span class="s1">df):</span>
        <span class="s2">'''initialize instance 
 
        Parameters 
        ---------- 
        mean : array_like 
            parameter mu (might be renamed), for symmetric distributions this 
            is the mean 
        sigma : array_like, 2d 
            dispersion matrix, covariance matrix in normal distribution, but 
            only proportional to covariance matrix in t distribution 
        args : list 
            distribution specific arguments, e.g. df for t distribution 
        kwds : dict 
            currently not used 
 
        '''</span>
        <span class="s1">super(MVT</span><span class="s3">, </span><span class="s1">self).__init__(mean</span><span class="s3">, </span><span class="s1">sigma)</span>
        <span class="s1">self.extra_args = [</span><span class="s5">'df'</span><span class="s1">]  </span><span class="s0">#overwrites extra_args of super</span>
        <span class="s1">self.df = df</span>

    <span class="s3">def </span><span class="s1">rvs(self</span><span class="s3">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s2">'''random variables with Student T distribution 
 
        Parameters 
        ---------- 
        size : int or tuple 
            the number and shape of random variables to draw. 
 
        Returns 
        ------- 
        rvs : ndarray 
            the returned random variables with shape given by size and the 
            dimension of the multivariate random vector as additional last 
            dimension 
            - TODO: Not sure if this works for size tuples with len&gt;1. 
 
        Notes 
        ----- 
        generated as a chi-square mixture of multivariate normal random 
        variables. 
        does this require df&gt;2 ? 
 
 
        '''</span>
        <span class="s3">from </span><span class="s1">.multivariate </span><span class="s3">import </span><span class="s1">multivariate_t_rvs</span>
        <span class="s3">return </span><span class="s1">multivariate_t_rvs(self.mean</span><span class="s3">, </span><span class="s1">self.sigma</span><span class="s3">, </span><span class="s1">df=self.df</span><span class="s3">, </span><span class="s1">n=size)</span>


    <span class="s3">def </span><span class="s1">logpdf(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">'''logarithm of probability density function 
 
        Parameters 
        ---------- 
        x : array_like 
            can be 1d or 2d, if 2d, then each row is taken as independent 
            multivariate random vector 
 
        Returns 
        ------- 
        logpdf : float or array 
            probability density value of each random vector 
 
        '''</span>

        <span class="s1">x = np.asarray(x)</span>

        <span class="s1">df = self.df</span>
        <span class="s1">nvars = self.nvars</span>

        <span class="s1">x_whitened = self.whiten(x - self.mean) </span><span class="s0">#should be float</span>

        <span class="s1">llf = - nvars * np_log(df * np_pi)</span>
        <span class="s1">llf -= self.logdetsigma</span>
        <span class="s1">llf -= (df + nvars) * np_log(</span><span class="s4">1 </span><span class="s1">+ np.sum(x_whitened**</span><span class="s4">2</span><span class="s3">,</span><span class="s1">-</span><span class="s4">1</span><span class="s1">) / df)</span>
        <span class="s1">llf *= </span><span class="s4">0.5</span>
        <span class="s1">llf += sps_gamln((df + nvars) / </span><span class="s4">2.</span><span class="s1">) - sps_gamln(df / </span><span class="s4">2.</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">llf</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">**kwds):</span>
        <span class="s2">'''cumulative distribution function 
 
        Parameters 
        ---------- 
        x : array_like 
            can be 1d or 2d, if 2d, then each row is taken as independent 
            multivariate random vector 
        kwds : dict 
            contains options for the numerical calculation of the cdf 
 
        Returns 
        ------- 
        cdf : float or array 
            probability density value of each random vector 
 
        '''</span>
        <span class="s1">lower = -np.inf * np.ones_like(x)</span>
        <span class="s0">#std_sigma = np.sqrt(np.diag(self.sigma))</span>
        <span class="s1">upper = (x - self.mean)/self.std_sigma</span>
        <span class="s3">return </span><span class="s1">mvstdtprob(lower</span><span class="s3">, </span><span class="s1">upper</span><span class="s3">, </span><span class="s1">self.corr</span><span class="s3">, </span><span class="s1">self.df</span><span class="s3">, </span><span class="s1">**kwds)</span>
        <span class="s0">#mvstdtcdf does not exist yet</span>
        <span class="s0">#return mvstdtcdf(lower, x, self.corr, df, **kwds)</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">cov(self):</span>
        <span class="s2">'''covariance matrix 
 
        The covariance matrix for the t distribution does not exist for df&lt;=2, 
        and is equal to sigma * df/(df-2) for df&gt;2 
 
        '''</span>
        <span class="s3">if </span><span class="s1">self.df &lt;= </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">np.nan * np.ones_like(self.sigma)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">self.df / (self.df - </span><span class="s4">2.</span><span class="s1">) * self.sigma</span>

    <span class="s3">def </span><span class="s1">affine_transformed(self</span><span class="s3">, </span><span class="s1">shift</span><span class="s3">, </span><span class="s1">scale_matrix):</span>
        <span class="s2">'''return distribution of a full rank affine transform 
 
        for full rank scale_matrix only 
 
        Parameters 
        ---------- 
        shift : array_like 
            shift of mean 
        scale_matrix : array_like 
            linear transformation matrix 
 
        Returns 
        ------- 
        mvt : instance of MVT 
            instance of multivariate t distribution given by affine 
            transformation 
 
 
        Notes 
        ----- 
 
        This checks for eigvals&lt;=0, so there are possible problems for cases 
        with positive eigenvalues close to zero. 
 
        see: http://www.statlect.com/mcdstu1.htm 
 
        I'm not sure about general case, non-full rank transformation are not 
        multivariate t distributed. 
 
        y = a + B x 
 
        where a is shift, 
        B is full rank scale matrix with same dimension as sigma 
 
        '''</span>
        <span class="s0">#full rank method could also be in elliptical and called with super</span>
        <span class="s0">#after the rank check</span>
        <span class="s1">B = scale_matrix  </span><span class="s0">#tmp variable as shorthand</span>
        <span class="s3">if not </span><span class="s1">B.shape == (self.nvars</span><span class="s3">, </span><span class="s1">self.nvars):</span>
            <span class="s3">if </span><span class="s1">(np.linalg.eigvals(B) &lt;= </span><span class="s4">0</span><span class="s1">).any():</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'affine transform has to be full rank'</span><span class="s1">)</span>

        <span class="s1">mean_new = np.dot(B</span><span class="s3">, </span><span class="s1">self.mean) + shift</span>
        <span class="s1">sigma_new = np.dot(np.dot(B</span><span class="s3">, </span><span class="s1">self.sigma)</span><span class="s3">, </span><span class="s1">B.T)</span>
        <span class="s3">return </span><span class="s1">MVT(mean_new</span><span class="s3">, </span><span class="s1">sigma_new</span><span class="s3">, </span><span class="s1">self.df)</span>


<span class="s3">def </span><span class="s1">quad2d(func=</span><span class="s3">lambda </span><span class="s1">x: </span><span class="s4">1</span><span class="s3">, </span><span class="s1">lower=(-</span><span class="s4">10</span><span class="s3">,</span><span class="s1">-</span><span class="s4">10</span><span class="s1">)</span><span class="s3">, </span><span class="s1">upper=(</span><span class="s4">10</span><span class="s3">,</span><span class="s4">10</span><span class="s1">)):</span>
    <span class="s3">def </span><span class="s1">fun(x</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s1">x = np.column_stack((x</span><span class="s3">,</span><span class="s1">y))</span>
        <span class="s3">return </span><span class="s1">func(x)</span>
    <span class="s3">from </span><span class="s1">scipy.integrate </span><span class="s3">import </span><span class="s1">dblquad</span>
    <span class="s3">return </span><span class="s1">dblquad(fun</span><span class="s3">, </span><span class="s1">lower[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">upper[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, lambda </span><span class="s1">y: lower[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
                   <span class="s3">lambda </span><span class="s1">y: upper[</span><span class="s4">1</span><span class="s1">])</span>

<span class="s3">if </span><span class="s1">__name__ == </span><span class="s5">'__main__'</span><span class="s1">:</span>

    <span class="s3">from </span><span class="s1">numpy.testing </span><span class="s3">import </span><span class="s1">assert_almost_equal</span><span class="s3">, </span><span class="s1">assert_array_almost_equal</span>

    <span class="s1">examples = [</span><span class="s5">'mvn'</span><span class="s1">]</span>

    <span class="s1">mu = (</span><span class="s4">0</span><span class="s3">,</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">covx = np.array([[</span><span class="s4">1.0</span><span class="s3">, </span><span class="s4">0.5</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">0.5</span><span class="s3">, </span><span class="s4">1.0</span><span class="s1">]])</span>
    <span class="s1">mu3 = [-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0.</span><span class="s3">, </span><span class="s4">2.</span><span class="s1">]</span>
    <span class="s1">cov3 = np.array([[ </span><span class="s4">1.  </span><span class="s3">,  </span><span class="s4">0.5 </span><span class="s3">,  </span><span class="s4">0.75</span><span class="s1">]</span><span class="s3">,</span>
                     <span class="s1">[ </span><span class="s4">0.5 </span><span class="s3">,  </span><span class="s4">1.5 </span><span class="s3">,  </span><span class="s4">0.6 </span><span class="s1">]</span><span class="s3">,</span>
                     <span class="s1">[ </span><span class="s4">0.75</span><span class="s3">,  </span><span class="s4">0.6 </span><span class="s3">,  </span><span class="s4">2.  </span><span class="s1">]])</span>


    <span class="s3">if </span><span class="s5">'mvn' </span><span class="s3">in </span><span class="s1">examples:</span>
        <span class="s1">bvn = BivariateNormal(mu</span><span class="s3">, </span><span class="s1">covx)</span>
        <span class="s1">rvs = bvn.rvs(size=</span><span class="s4">1000</span><span class="s1">)</span>
        <span class="s1">print(rvs.mean(</span><span class="s4">0</span><span class="s1">))</span>
        <span class="s1">print(np.cov(rvs</span><span class="s3">, </span><span class="s1">rowvar=</span><span class="s4">0</span><span class="s1">))</span>
        <span class="s1">print(bvn.expect())</span>
        <span class="s1">print(bvn.cdf([</span><span class="s4">0</span><span class="s3">,</span><span class="s4">0</span><span class="s1">]))</span>
        <span class="s1">bvn1 = BivariateNormal(mu</span><span class="s3">, </span><span class="s1">np.eye(</span><span class="s4">2</span><span class="s1">))</span>
        <span class="s1">bvn2 = BivariateNormal(mu</span><span class="s3">, </span><span class="s4">4</span><span class="s1">*np.eye(</span><span class="s4">2</span><span class="s1">))</span>
        <span class="s1">fun = </span><span class="s3">lambda </span><span class="s1">x : np.log(bvn1.pdf(x)) - np.log(bvn.pdf(x))</span>
        <span class="s1">print(bvn1.expect(fun))</span>
        <span class="s1">print(bvn1.kl(bvn2)</span><span class="s3">, </span><span class="s1">bvn1.kl_mc(bvn2))</span>
        <span class="s1">print(bvn2.kl(bvn1)</span><span class="s3">, </span><span class="s1">bvn2.kl_mc(bvn1))</span>
        <span class="s1">print(bvn1.kl(bvn)</span><span class="s3">, </span><span class="s1">bvn1.kl_mc(bvn))</span>
        <span class="s1">mvn = MVNormal(mu</span><span class="s3">, </span><span class="s1">covx)</span>
        <span class="s1">mvn.pdf([</span><span class="s4">0</span><span class="s3">,</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">mvn.pdf(np.zeros((</span><span class="s4">2</span><span class="s3">,</span><span class="s4">2</span><span class="s1">)))</span>
        <span class="s0">#np.dot(mvn.cholcovinv.T, mvn.cholcovinv) - mvn.covinv</span>

        <span class="s1">cov3 = np.array([[ </span><span class="s4">1.  </span><span class="s3">,  </span><span class="s4">0.5 </span><span class="s3">,  </span><span class="s4">0.75</span><span class="s1">]</span><span class="s3">,</span>
                         <span class="s1">[ </span><span class="s4">0.5 </span><span class="s3">,  </span><span class="s4">1.5 </span><span class="s3">,  </span><span class="s4">0.6 </span><span class="s1">]</span><span class="s3">,</span>
                         <span class="s1">[ </span><span class="s4">0.75</span><span class="s3">,  </span><span class="s4">0.6 </span><span class="s3">,  </span><span class="s4">2.  </span><span class="s1">]])</span>
        <span class="s1">mu3 = [-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0.</span><span class="s3">, </span><span class="s4">2.</span><span class="s1">]</span>
        <span class="s1">mvn3 = MVNormal(mu3</span><span class="s3">, </span><span class="s1">cov3)</span>
        <span class="s1">mvn3.pdf((</span><span class="s4">0.</span><span class="s3">, </span><span class="s4">2.</span><span class="s3">, </span><span class="s4">3.</span><span class="s1">))</span>
        <span class="s1">mvn3.logpdf((</span><span class="s4">0.</span><span class="s3">, </span><span class="s4">2.</span><span class="s3">, </span><span class="s4">3.</span><span class="s1">))</span>
        <span class="s0">#comparisons with R mvtnorm::dmvnorm</span>
        <span class="s0">#decimal=14</span>
<span class="s0">#        mvn3.logpdf(cov3) - [-7.667977543898155, -6.917977543898155, -5.167977543898155]</span>
<span class="s0">#        #decimal 18</span>
<span class="s0">#        mvn3.pdf(cov3) - [0.000467562492721686, 0.000989829804859273, 0.005696077243833402]</span>
<span class="s0">#        #cheating new mean, same cov</span>
<span class="s0">#        mvn3.mean = np.array([0,0,0])</span>
<span class="s0">#        #decimal= 16</span>
<span class="s0">#        mvn3.pdf(cov3) - [0.02914269740502042, 0.02269635555984291, 0.01767593948287269]</span>

        <span class="s0">#as asserts</span>
        <span class="s1">r_val = [-</span><span class="s4">7.667977543898155</span><span class="s3">, </span><span class="s1">-</span><span class="s4">6.917977543898155</span><span class="s3">, </span><span class="s1">-</span><span class="s4">5.167977543898155</span><span class="s1">]</span>
        <span class="s1">assert_array_almost_equal( mvn3.logpdf(cov3)</span><span class="s3">, </span><span class="s1">r_val</span><span class="s3">, </span><span class="s1">decimal = </span><span class="s4">14</span><span class="s1">)</span>
        <span class="s0">#decimal 18</span>
        <span class="s1">r_val = [</span><span class="s4">0.000467562492721686</span><span class="s3">, </span><span class="s4">0.000989829804859273</span><span class="s3">, </span><span class="s4">0.005696077243833402</span><span class="s1">]</span>
        <span class="s1">assert_array_almost_equal( mvn3.pdf(cov3)</span><span class="s3">, </span><span class="s1">r_val</span><span class="s3">, </span><span class="s1">decimal = </span><span class="s4">17</span><span class="s1">)</span>
        <span class="s0">#cheating new mean, same cov, too dangerous, got wrong instance in tests</span>
        <span class="s0">#mvn3.mean = np.array([0,0,0])</span>
        <span class="s1">mvn3c = MVNormal(np.array([</span><span class="s4">0</span><span class="s3">,</span><span class="s4">0</span><span class="s3">,</span><span class="s4">0</span><span class="s1">])</span><span class="s3">, </span><span class="s1">cov3)</span>
        <span class="s1">r_val = [</span><span class="s4">0.02914269740502042</span><span class="s3">, </span><span class="s4">0.02269635555984291</span><span class="s3">, </span><span class="s4">0.01767593948287269</span><span class="s1">]</span>
        <span class="s1">assert_array_almost_equal( mvn3c.pdf(cov3)</span><span class="s3">, </span><span class="s1">r_val</span><span class="s3">, </span><span class="s1">decimal = </span><span class="s4">16</span><span class="s1">)</span>

        <span class="s1">mvn3b = MVNormal((</span><span class="s4">0</span><span class="s3">,</span><span class="s4">0</span><span class="s3">,</span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">fun = </span><span class="s3">lambda </span><span class="s1">x : np.log(mvn3.pdf(x)) - np.log(mvn3b.pdf(x))</span>
        <span class="s1">print(mvn3.expect_mc(fun))</span>
        <span class="s1">print(mvn3.expect_mc(fun</span><span class="s3">, </span><span class="s1">size=</span><span class="s4">200000</span><span class="s1">))</span>


    <span class="s1">mvt = MVT((</span><span class="s4">0</span><span class="s3">,</span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(mvt.logpdf(np.array([</span><span class="s4">0.</span><span class="s3">,</span><span class="s4">0.</span><span class="s1">]))</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1.837877066409345</span><span class="s3">,</span>
                        <span class="s1">decimal=</span><span class="s4">15</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(mvt.pdf(np.array([</span><span class="s4">0.</span><span class="s3">,</span><span class="s4">0.</span><span class="s1">]))</span><span class="s3">, </span><span class="s4">0.1591549430918953</span><span class="s3">,</span>
                        <span class="s1">decimal=</span><span class="s4">15</span><span class="s1">)</span>

    <span class="s1">mvt.logpdf(np.array([</span><span class="s4">1.</span><span class="s3">,</span><span class="s4">1.</span><span class="s1">]))-(-</span><span class="s4">3.01552989458359</span><span class="s1">)</span>

    <span class="s1">mvt1 = MVT((</span><span class="s4">0</span><span class="s3">,</span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">mvt1.logpdf(np.array([</span><span class="s4">1.</span><span class="s3">,</span><span class="s4">1.</span><span class="s1">]))-(-</span><span class="s4">3.48579549941151</span><span class="s1">) </span><span class="s0">#decimal=16</span>

    <span class="s1">rvs = mvt.rvs(</span><span class="s4">100000</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(np.cov(rvs</span><span class="s3">, </span><span class="s1">rowvar=</span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">mvt.cov</span><span class="s3">, </span><span class="s1">decimal=</span><span class="s4">1</span><span class="s1">)</span>

    <span class="s1">mvt31 = MVT(mu3</span><span class="s3">, </span><span class="s1">cov3</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(mvt31.pdf(cov3)</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">0.0007276818698165781</span><span class="s3">, </span><span class="s4">0.0009980625182293658</span><span class="s3">, </span><span class="s4">0.0027661422056214652</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">decimal=</span><span class="s4">18</span><span class="s1">)</span>

    <span class="s1">mvt = MVT(mu3</span><span class="s3">, </span><span class="s1">cov3</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(mvt.pdf(cov3)</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">0.000863777424247410</span><span class="s3">, </span><span class="s4">0.001277510788307594</span><span class="s3">, </span><span class="s4">0.004156314279452241</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">decimal=</span><span class="s4">17</span><span class="s1">)</span>
</pre>
</body>
</html>