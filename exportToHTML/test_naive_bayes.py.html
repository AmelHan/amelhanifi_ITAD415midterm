<html>
<head>
<title>test_naive_bayes.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_naive_bayes.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">re</span>
<span class="s0">import </span><span class="s1">warnings</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">import </span><span class="s1">scipy.sparse</span>
<span class="s0">from </span><span class="s1">scipy.special </span><span class="s0">import </span><span class="s1">logsumexp</span>

<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">load_digits</span><span class="s0">, </span><span class="s1">load_iris</span>
<span class="s0">from </span><span class="s1">sklearn.model_selection </span><span class="s0">import </span><span class="s1">cross_val_score</span><span class="s0">, </span><span class="s1">train_test_split</span>
<span class="s0">from </span><span class="s1">sklearn.naive_bayes </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">BernoulliNB</span><span class="s0">,</span>
    <span class="s1">CategoricalNB</span><span class="s0">,</span>
    <span class="s1">ComplementNB</span><span class="s0">,</span>
    <span class="s1">GaussianNB</span><span class="s0">,</span>
    <span class="s1">MultinomialNB</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s0">,</span>
    <span class="s1">assert_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_equal</span><span class="s0">,</span>
<span class="s1">)</span>

<span class="s1">DISCRETE_NAIVE_BAYES_CLASSES = [BernoulliNB</span><span class="s0">, </span><span class="s1">CategoricalNB</span><span class="s0">, </span><span class="s1">ComplementNB</span><span class="s0">, </span><span class="s1">MultinomialNB]</span>
<span class="s1">ALL_NAIVE_BAYES_CLASSES = DISCRETE_NAIVE_BAYES_CLASSES + [GaussianNB]</span>

<span class="s1">msg = </span><span class="s2">&quot;The default value for `force_alpha` will change&quot;</span>
<span class="s1">pytestmark = pytest.mark.filterwarnings(</span><span class="s2">f&quot;ignore:</span><span class="s0">{</span><span class="s1">msg</span><span class="s0">}</span><span class="s2">:FutureWarning&quot;</span><span class="s1">)</span>

<span class="s3"># Data is just 6 separable points in the plane</span>
<span class="s1">X = np.array([[-</span><span class="s4">2</span><span class="s0">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s0">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s0">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span>
<span class="s1">y = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>


<span class="s0">def </span><span class="s1">get_random_normal_x_binary_y(global_random_seed):</span>
    <span class="s3"># A bit more random tests</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">X1 = rng.normal(size=(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">y1 = (rng.normal(size=</span><span class="s4">10</span><span class="s1">) &gt; </span><span class="s4">0</span><span class="s1">).astype(int)</span>
    <span class="s0">return </span><span class="s1">X1</span><span class="s0">, </span><span class="s1">y1</span>


<span class="s0">def </span><span class="s1">get_random_integer_x_three_classes_y(global_random_seed):</span>
    <span class="s3"># Data is 6 random integer points in a 100 dimensional space classified to</span>
    <span class="s3"># three classes.</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">X2 = rng.randint(</span><span class="s4">5</span><span class="s0">, </span><span class="s1">size=(</span><span class="s4">6</span><span class="s0">, </span><span class="s4">100</span><span class="s1">))</span>
    <span class="s1">y2 = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">])</span>
    <span class="s0">return </span><span class="s1">X2</span><span class="s0">, </span><span class="s1">y2</span>


<span class="s0">def </span><span class="s1">test_gnb():</span>
    <span class="s3"># Gaussian Naive Bayes classification.</span>
    <span class="s3"># This checks that GaussianNB implements fit and predict and returns</span>
    <span class="s3"># correct values for a simple toy dataset.</span>

    <span class="s1">clf = GaussianNB()</span>
    <span class="s1">y_pred = clf.fit(X</span><span class="s0">, </span><span class="s1">y).predict(X)</span>
    <span class="s1">assert_array_equal(y_pred</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">y_pred_proba = clf.predict_proba(X)</span>
    <span class="s1">y_pred_log_proba = clf.predict_log_proba(X)</span>
    <span class="s1">assert_array_almost_equal(np.log(y_pred_proba)</span><span class="s0">, </span><span class="s1">y_pred_log_proba</span><span class="s0">, </span><span class="s4">8</span><span class="s1">)</span>

    <span class="s3"># Test whether label mismatch between target y and classes raises</span>
    <span class="s3"># an Error</span>
    <span class="s3"># FIXME Remove this test once the more general partial_fit tests are merged</span>
    <span class="s0">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;The target label.* in y do not exist in the initial classes&quot;</span>
    <span class="s1">):</span>
        <span class="s1">GaussianNB().partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>


<span class="s0">def </span><span class="s1">test_gnb_prior(global_random_seed):</span>
    <span class="s3"># Test whether class priors are properly set.</span>
    <span class="s1">clf = GaussianNB().fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(np.array([</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]) / </span><span class="s4">6.0</span><span class="s0">, </span><span class="s1">clf.class_prior_</span><span class="s0">, </span><span class="s4">8</span><span class="s1">)</span>
    <span class="s1">X1</span><span class="s0">, </span><span class="s1">y1 = get_random_normal_x_binary_y(global_random_seed)</span>
    <span class="s1">clf = GaussianNB().fit(X1</span><span class="s0">, </span><span class="s1">y1)</span>
    <span class="s3"># Check that the class priors sum to 1</span>
    <span class="s1">assert_array_almost_equal(clf.class_prior_.sum()</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_gnb_sample_weight(global_random_seed):</span>
    <span class="s5">&quot;&quot;&quot;Test whether sample weights are properly used in GNB.&quot;&quot;&quot;</span>
    <span class="s3"># Sample weights all being 1 should not change results</span>
    <span class="s1">sw = np.ones(</span><span class="s4">6</span><span class="s1">)</span>
    <span class="s1">clf = GaussianNB().fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">clf_sw = GaussianNB().fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sw)</span>

    <span class="s1">assert_array_almost_equal(clf.theta_</span><span class="s0">, </span><span class="s1">clf_sw.theta_)</span>
    <span class="s1">assert_array_almost_equal(clf.var_</span><span class="s0">, </span><span class="s1">clf_sw.var_)</span>

    <span class="s3"># Fitting twice with half sample-weights should result</span>
    <span class="s3"># in same result as fitting once with full weights</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>

    <span class="s1">sw = rng.rand(y.shape[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">clf1 = GaussianNB().fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sw)</span>
    <span class="s1">clf2 = GaussianNB().partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">sample_weight=sw / </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">clf2.partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sw / </span><span class="s4">2</span><span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(clf1.theta_</span><span class="s0">, </span><span class="s1">clf2.theta_)</span>
    <span class="s1">assert_array_almost_equal(clf1.var_</span><span class="s0">, </span><span class="s1">clf2.var_)</span>

    <span class="s3"># Check that duplicate entries and correspondingly increased sample</span>
    <span class="s3"># weights yield the same result</span>
    <span class="s1">ind = rng.randint(</span><span class="s4">0</span><span class="s0">, </span><span class="s1">X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">sample_weight = np.bincount(ind</span><span class="s0">, </span><span class="s1">minlength=X.shape[</span><span class="s4">0</span><span class="s1">])</span>

    <span class="s1">clf_dupl = GaussianNB().fit(X[ind]</span><span class="s0">, </span><span class="s1">y[ind])</span>
    <span class="s1">clf_sw = GaussianNB().fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight)</span>

    <span class="s1">assert_array_almost_equal(clf_dupl.theta_</span><span class="s0">, </span><span class="s1">clf_sw.theta_)</span>
    <span class="s1">assert_array_almost_equal(clf_dupl.var_</span><span class="s0">, </span><span class="s1">clf_sw.var_)</span>

    <span class="s3"># non-regression test for gh-24140 where a division by zero was</span>
    <span class="s3"># occurring when a single class was present</span>
    <span class="s1">sample_weight = (y == </span><span class="s4">1</span><span class="s1">).astype(np.float64)</span>
    <span class="s1">clf = GaussianNB().fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>


<span class="s0">def </span><span class="s1">test_gnb_neg_priors():</span>
    <span class="s5">&quot;&quot;&quot;Test whether an error is raised in case of negative priors&quot;&quot;&quot;</span>
    <span class="s1">clf = GaussianNB(priors=np.array([-</span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">2.0</span><span class="s1">]))</span>

    <span class="s1">msg = </span><span class="s2">&quot;Priors must be non-negative&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_gnb_priors():</span>
    <span class="s5">&quot;&quot;&quot;Test whether the class prior override is properly used&quot;&quot;&quot;</span>
    <span class="s1">clf = GaussianNB(priors=np.array([</span><span class="s4">0.3</span><span class="s0">, </span><span class="s4">0.7</span><span class="s1">])).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">clf.predict_proba([[-</span><span class="s4">0.1</span><span class="s0">, </span><span class="s1">-</span><span class="s4">0.1</span><span class="s1">]])</span><span class="s0">,</span>
        <span class="s1">np.array([[</span><span class="s4">0.825303662161683</span><span class="s0">, </span><span class="s4">0.174696337838317</span><span class="s1">]])</span><span class="s0">,</span>
        <span class="s4">8</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(clf.class_prior_</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s4">0.3</span><span class="s0">, </span><span class="s4">0.7</span><span class="s1">]))</span>


<span class="s0">def </span><span class="s1">test_gnb_priors_sum_isclose():</span>
    <span class="s3"># test whether the class prior sum is properly tested&quot;&quot;&quot;</span>
    <span class="s1">X = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[-</span><span class="s4">1</span><span class="s0">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s4">2</span><span class="s0">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s4">3</span><span class="s0">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s4">4</span><span class="s0">, </span><span class="s1">-</span><span class="s4">5</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s4">5</span><span class="s0">, </span><span class="s1">-</span><span class="s4">4</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">4</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">5</span><span class="s0">, </span><span class="s4">5</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">priors = np.array([</span><span class="s4">0.08</span><span class="s0">, </span><span class="s4">0.14</span><span class="s0">, </span><span class="s4">0.03</span><span class="s0">, </span><span class="s4">0.16</span><span class="s0">, </span><span class="s4">0.11</span><span class="s0">, </span><span class="s4">0.16</span><span class="s0">, </span><span class="s4">0.07</span><span class="s0">, </span><span class="s4">0.14</span><span class="s0">, </span><span class="s4">0.11</span><span class="s0">, </span><span class="s4">0.0</span><span class="s1">])</span>
    <span class="s1">Y = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">3</span><span class="s0">, </span><span class="s4">4</span><span class="s0">, </span><span class="s4">5</span><span class="s0">, </span><span class="s4">6</span><span class="s0">, </span><span class="s4">7</span><span class="s0">, </span><span class="s4">8</span><span class="s0">, </span><span class="s4">9</span><span class="s0">, </span><span class="s4">10</span><span class="s1">])</span>
    <span class="s1">clf = GaussianNB(priors=priors)</span>
    <span class="s3"># smoke test for issue #9633</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>


<span class="s0">def </span><span class="s1">test_gnb_wrong_nb_priors():</span>
    <span class="s5">&quot;&quot;&quot;Test whether an error is raised if the number of prior is different 
    from the number of class&quot;&quot;&quot;</span>
    <span class="s1">clf = GaussianNB(priors=np.array([</span><span class="s4">0.25</span><span class="s0">, </span><span class="s4">0.25</span><span class="s0">, </span><span class="s4">0.25</span><span class="s0">, </span><span class="s4">0.25</span><span class="s1">]))</span>

    <span class="s1">msg = </span><span class="s2">&quot;Number of priors must match number of classes&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_gnb_prior_greater_one():</span>
    <span class="s5">&quot;&quot;&quot;Test if an error is raised if the sum of prior greater than one&quot;&quot;&quot;</span>
    <span class="s1">clf = GaussianNB(priors=np.array([</span><span class="s4">2.0</span><span class="s0">, </span><span class="s4">1.0</span><span class="s1">]))</span>

    <span class="s1">msg = </span><span class="s2">&quot;The sum of the priors should be 1&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_gnb_prior_large_bias():</span>
    <span class="s5">&quot;&quot;&quot;Test if good prediction when class prior favor largely one class&quot;&quot;&quot;</span>
    <span class="s1">clf = GaussianNB(priors=np.array([</span><span class="s4">0.01</span><span class="s0">, </span><span class="s4">0.99</span><span class="s1">]))</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">clf.predict([[-</span><span class="s4">0.1</span><span class="s0">, </span><span class="s1">-</span><span class="s4">0.1</span><span class="s1">]]) == np.array([</span><span class="s4">2</span><span class="s1">])</span>


<span class="s0">def </span><span class="s1">test_gnb_check_update_with_no_data():</span>
    <span class="s5">&quot;&quot;&quot;Test when the partial fit is called without any data&quot;&quot;&quot;</span>
    <span class="s3"># Create an empty array</span>
    <span class="s1">prev_points = </span><span class="s4">100</span>
    <span class="s1">mean = </span><span class="s4">0.0</span>
    <span class="s1">var = </span><span class="s4">1.0</span>
    <span class="s1">x_empty = np.empty((</span><span class="s4">0</span><span class="s0">, </span><span class="s1">X.shape[</span><span class="s4">1</span><span class="s1">]))</span>
    <span class="s1">tmean</span><span class="s0">, </span><span class="s1">tvar = GaussianNB._update_mean_variance(prev_points</span><span class="s0">, </span><span class="s1">mean</span><span class="s0">, </span><span class="s1">var</span><span class="s0">, </span><span class="s1">x_empty)</span>
    <span class="s0">assert </span><span class="s1">tmean == mean</span>
    <span class="s0">assert </span><span class="s1">tvar == var</span>


<span class="s0">def </span><span class="s1">test_gnb_partial_fit():</span>
    <span class="s1">clf = GaussianNB().fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">clf_pf = GaussianNB().partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">np.unique(y))</span>
    <span class="s1">assert_array_almost_equal(clf.theta_</span><span class="s0">, </span><span class="s1">clf_pf.theta_)</span>
    <span class="s1">assert_array_almost_equal(clf.var_</span><span class="s0">, </span><span class="s1">clf_pf.var_)</span>
    <span class="s1">assert_array_almost_equal(clf.class_prior_</span><span class="s0">, </span><span class="s1">clf_pf.class_prior_)</span>

    <span class="s1">clf_pf2 = GaussianNB().partial_fit(X[</span><span class="s4">0</span><span class="s1">::</span><span class="s4">2</span><span class="s0">, </span><span class="s1">:]</span><span class="s0">, </span><span class="s1">y[</span><span class="s4">0</span><span class="s1">::</span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.unique(y))</span>
    <span class="s1">clf_pf2.partial_fit(X[</span><span class="s4">1</span><span class="s1">::</span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y[</span><span class="s4">1</span><span class="s1">::</span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(clf.theta_</span><span class="s0">, </span><span class="s1">clf_pf2.theta_)</span>
    <span class="s1">assert_array_almost_equal(clf.var_</span><span class="s0">, </span><span class="s1">clf_pf2.var_)</span>
    <span class="s1">assert_array_almost_equal(clf.class_prior_</span><span class="s0">, </span><span class="s1">clf_pf2.class_prior_)</span>


<span class="s0">def </span><span class="s1">test_gnb_naive_bayes_scale_invariance():</span>
    <span class="s3"># Scaling the data should not change the prediction results</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = iris.data</span><span class="s0">, </span><span class="s1">iris.target</span>
    <span class="s1">labels = [GaussianNB().fit(f * X</span><span class="s0">, </span><span class="s1">y).predict(f * X) </span><span class="s0">for </span><span class="s1">f </span><span class="s0">in </span><span class="s1">[</span><span class="s4">1e-10</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1e10</span><span class="s1">]]</span>
    <span class="s1">assert_array_equal(labels[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">labels[</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(labels[</span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">labels[</span><span class="s4">2</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;DiscreteNaiveBayes&quot;</span><span class="s0">, </span><span class="s1">DISCRETE_NAIVE_BAYES_CLASSES)</span>
<span class="s0">def </span><span class="s1">test_discretenb_prior(DiscreteNaiveBayes</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Test whether class priors are properly set.</span>
    <span class="s1">X2</span><span class="s0">, </span><span class="s1">y2 = get_random_integer_x_three_classes_y(global_random_seed)</span>
    <span class="s1">clf = DiscreteNaiveBayes().fit(X2</span><span class="s0">, </span><span class="s1">y2)</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">np.log(np.array([</span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]) / </span><span class="s4">6.0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">clf.class_log_prior_</span><span class="s0">, </span><span class="s4">8</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;DiscreteNaiveBayes&quot;</span><span class="s0">, </span><span class="s1">DISCRETE_NAIVE_BAYES_CLASSES)</span>
<span class="s0">def </span><span class="s1">test_discretenb_partial_fit(DiscreteNaiveBayes):</span>
    <span class="s1">clf1 = DiscreteNaiveBayes()</span>
    <span class="s1">clf1.fit([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>

    <span class="s1">clf2 = DiscreteNaiveBayes()</span>
    <span class="s1">clf2.partial_fit([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">classes=[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(clf1.class_count_</span><span class="s0">, </span><span class="s1">clf2.class_count_)</span>
    <span class="s0">if </span><span class="s1">DiscreteNaiveBayes </span><span class="s0">is </span><span class="s1">CategoricalNB:</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(len(clf1.category_count_)):</span>
            <span class="s1">assert_array_equal(clf1.category_count_[i]</span><span class="s0">, </span><span class="s1">clf2.category_count_[i])</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">assert_array_equal(clf1.feature_count_</span><span class="s0">, </span><span class="s1">clf2.feature_count_)</span>

    <span class="s1">clf3 = DiscreteNaiveBayes()</span>
    <span class="s3"># all categories have to appear in the first partial fit</span>
    <span class="s1">clf3.partial_fit([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">classes=[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">clf3.partial_fit([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">clf3.partial_fit([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(clf1.class_count_</span><span class="s0">, </span><span class="s1">clf3.class_count_)</span>
    <span class="s0">if </span><span class="s1">DiscreteNaiveBayes </span><span class="s0">is </span><span class="s1">CategoricalNB:</span>
        <span class="s3"># the categories for each feature of CategoricalNB are mapped to an</span>
        <span class="s3"># index chronologically with each call of partial fit and therefore</span>
        <span class="s3"># the category_count matrices cannot be compared for equality</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(len(clf1.category_count_)):</span>
            <span class="s1">assert_array_equal(</span>
                <span class="s1">clf1.category_count_[i].shape</span><span class="s0">, </span><span class="s1">clf3.category_count_[i].shape</span>
            <span class="s1">)</span>
            <span class="s1">assert_array_equal(</span>
                <span class="s1">np.sum(clf1.category_count_[i]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">np.sum(clf3.category_count_[i]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">)</span>

        <span class="s3"># assert category 0 occurs 1x in the first class and 0x in the 2nd</span>
        <span class="s3"># class</span>
        <span class="s1">assert_array_equal(clf1.category_count_[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]))</span>
        <span class="s3"># assert category 1 occurs 0x in the first class and 2x in the 2nd</span>
        <span class="s3"># class</span>
        <span class="s1">assert_array_equal(clf1.category_count_[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s4">0</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]))</span>

        <span class="s3"># assert category 0 occurs 0x in the first class and 1x in the 2nd</span>
        <span class="s3"># class</span>
        <span class="s1">assert_array_equal(clf1.category_count_[</span><span class="s4">1</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]))</span>
        <span class="s3"># assert category 1 occurs 1x in the first class and 1x in the 2nd</span>
        <span class="s3"># class</span>
        <span class="s1">assert_array_equal(clf1.category_count_[</span><span class="s4">1</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]))</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">assert_array_equal(clf1.feature_count_</span><span class="s0">, </span><span class="s1">clf3.feature_count_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;NaiveBayes&quot;</span><span class="s0">, </span><span class="s1">ALL_NAIVE_BAYES_CLASSES)</span>
<span class="s0">def </span><span class="s1">test_NB_partial_fit_no_first_classes(NaiveBayes</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># classes is required for first call to partial fit</span>
    <span class="s1">X2</span><span class="s0">, </span><span class="s1">y2 = get_random_integer_x_three_classes_y(global_random_seed)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;classes must be passed on the first call to partial_fit.&quot;</span>
    <span class="s1">):</span>
        <span class="s1">NaiveBayes().partial_fit(X2</span><span class="s0">, </span><span class="s1">y2)</span>

    <span class="s3"># check consistency of consecutive classes values</span>
    <span class="s1">clf = NaiveBayes()</span>
    <span class="s1">clf.partial_fit(X2</span><span class="s0">, </span><span class="s1">y2</span><span class="s0">, </span><span class="s1">classes=np.unique(y2))</span>
    <span class="s0">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;is not the same as on last call to partial_fit&quot;</span>
    <span class="s1">):</span>
        <span class="s1">clf.partial_fit(X2</span><span class="s0">, </span><span class="s1">y2</span><span class="s0">, </span><span class="s1">classes=np.arange(</span><span class="s4">42</span><span class="s1">))</span>


<span class="s0">def </span><span class="s1">test_discretenb_predict_proba():</span>
    <span class="s3"># Test discrete NB classes' probability scores</span>

    <span class="s3"># The 100s below distinguish Bernoulli from multinomial.</span>
    <span class="s3"># FIXME: write a test to show this.</span>
    <span class="s1">X_bernoulli = [[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">100</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">100</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span>
    <span class="s1">X_multinomial = [[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">4</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]]</span>

    <span class="s3"># test binary case (1-d output)</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]  </span><span class="s3"># 2 is regression test for binary case, 02e673</span>
    <span class="s0">for </span><span class="s1">DiscreteNaiveBayes</span><span class="s0">, </span><span class="s1">X </span><span class="s0">in </span><span class="s1">zip(</span>
        <span class="s1">[BernoulliNB</span><span class="s0">, </span><span class="s1">MultinomialNB]</span><span class="s0">, </span><span class="s1">[X_bernoulli</span><span class="s0">, </span><span class="s1">X_multinomial]</span>
    <span class="s1">):</span>
        <span class="s1">clf = DiscreteNaiveBayes().fit(X</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s0">assert </span><span class="s1">clf.predict(X[-</span><span class="s4">1</span><span class="s1">:]) == </span><span class="s4">2</span>
        <span class="s0">assert </span><span class="s1">clf.predict_proba([X[</span><span class="s4">0</span><span class="s1">]]).shape == (</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">clf.predict_proba(X[:</span><span class="s4">2</span><span class="s1">]).sum(axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">1.0</span><span class="s1">])</span><span class="s0">, </span><span class="s4">6</span>
        <span class="s1">)</span>

    <span class="s3"># test multiclass case (2-d output, must sum to one)</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span>
    <span class="s0">for </span><span class="s1">DiscreteNaiveBayes</span><span class="s0">, </span><span class="s1">X </span><span class="s0">in </span><span class="s1">zip(</span>
        <span class="s1">[BernoulliNB</span><span class="s0">, </span><span class="s1">MultinomialNB]</span><span class="s0">, </span><span class="s1">[X_bernoulli</span><span class="s0">, </span><span class="s1">X_multinomial]</span>
    <span class="s1">):</span>
        <span class="s1">clf = DiscreteNaiveBayes().fit(X</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s0">assert </span><span class="s1">clf.predict_proba(X[</span><span class="s4">0</span><span class="s1">:</span><span class="s4">1</span><span class="s1">]).shape == (</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">clf.predict_proba(X[:</span><span class="s4">2</span><span class="s1">]).shape == (</span><span class="s4">2</span><span class="s0">, </span><span class="s4">3</span><span class="s1">)</span>
        <span class="s1">assert_almost_equal(np.sum(clf.predict_proba([X[</span><span class="s4">1</span><span class="s1">]]))</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">assert_almost_equal(np.sum(clf.predict_proba([X[-</span><span class="s4">1</span><span class="s1">]]))</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">assert_almost_equal(np.sum(np.exp(clf.class_log_prior_))</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;DiscreteNaiveBayes&quot;</span><span class="s0">, </span><span class="s1">DISCRETE_NAIVE_BAYES_CLASSES)</span>
<span class="s0">def </span><span class="s1">test_discretenb_uniform_prior(DiscreteNaiveBayes):</span>
    <span class="s3"># Test whether discrete NB classes fit a uniform prior</span>
    <span class="s3"># when fit_prior=False and class_prior=None</span>

    <span class="s1">clf = DiscreteNaiveBayes()</span>
    <span class="s1">clf.set_params(fit_prior=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">clf.fit([[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">prior = np.exp(clf.class_log_prior_)</span>
    <span class="s1">assert_array_almost_equal(prior</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s4">0.5</span><span class="s0">, </span><span class="s4">0.5</span><span class="s1">]))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;DiscreteNaiveBayes&quot;</span><span class="s0">, </span><span class="s1">DISCRETE_NAIVE_BAYES_CLASSES)</span>
<span class="s0">def </span><span class="s1">test_discretenb_provide_prior(DiscreteNaiveBayes):</span>
    <span class="s3"># Test whether discrete NB classes use provided prior</span>

    <span class="s1">clf = DiscreteNaiveBayes(class_prior=[</span><span class="s4">0.5</span><span class="s0">, </span><span class="s4">0.5</span><span class="s1">])</span>
    <span class="s1">clf.fit([[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">prior = np.exp(clf.class_log_prior_)</span>
    <span class="s1">assert_array_almost_equal(prior</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s4">0.5</span><span class="s0">, </span><span class="s4">0.5</span><span class="s1">]))</span>

    <span class="s3"># Inconsistent number of classes with prior</span>
    <span class="s1">msg = </span><span class="s2">&quot;Number of priors must match number of classes&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.fit([[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">2</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>

    <span class="s1">msg = </span><span class="s2">&quot;is not the same as on last call to partial_fit&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.partial_fit([[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">classes=[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;DiscreteNaiveBayes&quot;</span><span class="s0">, </span><span class="s1">DISCRETE_NAIVE_BAYES_CLASSES)</span>
<span class="s0">def </span><span class="s1">test_discretenb_provide_prior_with_partial_fit(DiscreteNaiveBayes):</span>
    <span class="s3"># Test whether discrete NB classes use provided prior</span>
    <span class="s3"># when using partial_fit</span>

    <span class="s1">iris = load_iris()</span>
    <span class="s1">iris_data1</span><span class="s0">, </span><span class="s1">iris_data2</span><span class="s0">, </span><span class="s1">iris_target1</span><span class="s0">, </span><span class="s1">iris_target2 = train_test_split(</span>
        <span class="s1">iris.data</span><span class="s0">, </span><span class="s1">iris.target</span><span class="s0">, </span><span class="s1">test_size=</span><span class="s4">0.4</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">415</span>
    <span class="s1">)</span>

    <span class="s0">for </span><span class="s1">prior </span><span class="s0">in </span><span class="s1">[</span><span class="s0">None, </span><span class="s1">[</span><span class="s4">0.3</span><span class="s0">, </span><span class="s4">0.3</span><span class="s0">, </span><span class="s4">0.4</span><span class="s1">]]:</span>
        <span class="s1">clf_full = DiscreteNaiveBayes(class_prior=prior)</span>
        <span class="s1">clf_full.fit(iris.data</span><span class="s0">, </span><span class="s1">iris.target)</span>
        <span class="s1">clf_partial = DiscreteNaiveBayes(class_prior=prior)</span>
        <span class="s1">clf_partial.partial_fit(iris_data1</span><span class="s0">, </span><span class="s1">iris_target1</span><span class="s0">, </span><span class="s1">classes=[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>
        <span class="s1">clf_partial.partial_fit(iris_data2</span><span class="s0">, </span><span class="s1">iris_target2)</span>
        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">clf_full.class_log_prior_</span><span class="s0">, </span><span class="s1">clf_partial.class_log_prior_</span>
        <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;DiscreteNaiveBayes&quot;</span><span class="s0">, </span><span class="s1">DISCRETE_NAIVE_BAYES_CLASSES)</span>
<span class="s0">def </span><span class="s1">test_discretenb_sample_weight_multiclass(DiscreteNaiveBayes):</span>
    <span class="s3"># check shape consistency for number of samples at fit time</span>
    <span class="s1">X = [</span>
        <span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span>
    <span class="s1">sample_weight = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s1">sample_weight /= sample_weight.sum()</span>
    <span class="s1">clf = DiscreteNaiveBayes().fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_array_equal(clf.predict(X)</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>

    <span class="s3"># Check sample weight using the partial_fit method</span>
    <span class="s1">clf = DiscreteNaiveBayes()</span>
    <span class="s1">clf.partial_fit(X[:</span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y[:</span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">classes=[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight[:</span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">clf.partial_fit(X[</span><span class="s4">2</span><span class="s1">:</span><span class="s4">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y[</span><span class="s4">2</span><span class="s1">:</span><span class="s4">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight[</span><span class="s4">2</span><span class="s1">:</span><span class="s4">3</span><span class="s1">])</span>
    <span class="s1">clf.partial_fit(X[</span><span class="s4">3</span><span class="s1">:]</span><span class="s0">, </span><span class="s1">y[</span><span class="s4">3</span><span class="s1">:]</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight[</span><span class="s4">3</span><span class="s1">:])</span>
    <span class="s1">assert_array_equal(clf.predict(X)</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;DiscreteNaiveBayes&quot;</span><span class="s0">, </span><span class="s1">DISCRETE_NAIVE_BAYES_CLASSES)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;use_partial_fit&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">False, True</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;train_on_single_class_y&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">False, True</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_discretenb_degenerate_one_class_case(</span>
    <span class="s1">DiscreteNaiveBayes</span><span class="s0">,</span>
    <span class="s1">use_partial_fit</span><span class="s0">,</span>
    <span class="s1">train_on_single_class_y</span><span class="s0">,</span>
<span class="s1">):</span>
    <span class="s3"># Most array attributes of a discrete naive Bayes classifier should have a</span>
    <span class="s3"># first-axis length equal to the number of classes. Exceptions include:</span>
    <span class="s3"># ComplementNB.feature_all_, CategoricalNB.n_categories_.</span>
    <span class="s3"># Confirm that this is the case for binary problems and the degenerate</span>
    <span class="s3"># case of a single class in the training set, when fitting with `fit` or</span>
    <span class="s3"># `partial_fit`.</span>
    <span class="s3"># Non-regression test for handling degenerate one-class case:</span>
    <span class="s3"># https://github.com/scikit-learn/scikit-learn/issues/18974</span>

    <span class="s1">X = [[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span>
    <span class="s0">if </span><span class="s1">train_on_single_class_y:</span>
        <span class="s1">X = X[:-</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">y = y[:-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">classes = sorted(list(set(y)))</span>
    <span class="s1">num_classes = len(classes)</span>

    <span class="s1">clf = DiscreteNaiveBayes()</span>
    <span class="s0">if </span><span class="s1">use_partial_fit:</span>
        <span class="s1">clf.partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=classes)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">clf.predict(X[:</span><span class="s4">1</span><span class="s1">]) == y[</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s3"># Check that attributes have expected first-axis lengths</span>
    <span class="s1">attribute_names = [</span>
        <span class="s2">&quot;classes_&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;class_count_&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;class_log_prior_&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;feature_count_&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;feature_log_prob_&quot;</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s0">for </span><span class="s1">attribute_name </span><span class="s0">in </span><span class="s1">attribute_names:</span>
        <span class="s1">attribute = getattr(clf</span><span class="s0">, </span><span class="s1">attribute_name</span><span class="s0">, None</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">attribute </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s3"># CategoricalNB has no feature_count_ attribute</span>
            <span class="s0">continue</span>
        <span class="s0">if </span><span class="s1">isinstance(attribute</span><span class="s0">, </span><span class="s1">np.ndarray):</span>
            <span class="s0">assert </span><span class="s1">attribute.shape[</span><span class="s4">0</span><span class="s1">] == num_classes</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s3"># CategoricalNB.feature_log_prob_ is a list of arrays</span>
            <span class="s0">for </span><span class="s1">element </span><span class="s0">in </span><span class="s1">attribute:</span>
                <span class="s0">assert </span><span class="s1">element.shape[</span><span class="s4">0</span><span class="s1">] == num_classes</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;kind&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s2">&quot;dense&quot;</span><span class="s0">, </span><span class="s2">&quot;sparse&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_mnnb(kind</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Test Multinomial Naive Bayes classification.</span>
    <span class="s3"># This checks that MultinomialNB implements fit and predict and returns</span>
    <span class="s3"># correct values for a simple toy dataset.</span>
    <span class="s1">X2</span><span class="s0">, </span><span class="s1">y2 = get_random_integer_x_three_classes_y(global_random_seed)</span>

    <span class="s0">if </span><span class="s1">kind == </span><span class="s2">&quot;dense&quot;</span><span class="s1">:</span>
        <span class="s1">X = X2</span>
    <span class="s0">elif </span><span class="s1">kind == </span><span class="s2">&quot;sparse&quot;</span><span class="s1">:</span>
        <span class="s1">X = scipy.sparse.csr_matrix(X2)</span>

    <span class="s3"># Check the ability to predict the learning set.</span>
    <span class="s1">clf = MultinomialNB()</span>

    <span class="s1">msg = </span><span class="s2">&quot;Negative values in data passed to&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.fit(-X</span><span class="s0">, </span><span class="s1">y2)</span>
    <span class="s1">y_pred = clf.fit(X</span><span class="s0">, </span><span class="s1">y2).predict(X)</span>

    <span class="s1">assert_array_equal(y_pred</span><span class="s0">, </span><span class="s1">y2)</span>

    <span class="s3"># Verify that np.log(clf.predict_proba(X)) gives the same results as</span>
    <span class="s3"># clf.predict_log_proba(X)</span>
    <span class="s1">y_pred_proba = clf.predict_proba(X)</span>
    <span class="s1">y_pred_log_proba = clf.predict_log_proba(X)</span>
    <span class="s1">assert_array_almost_equal(np.log(y_pred_proba)</span><span class="s0">, </span><span class="s1">y_pred_log_proba</span><span class="s0">, </span><span class="s4">8</span><span class="s1">)</span>

    <span class="s3"># Check that incremental fitting yields the same results</span>
    <span class="s1">clf2 = MultinomialNB()</span>
    <span class="s1">clf2.partial_fit(X[:</span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y2[:</span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">classes=np.unique(y2))</span>
    <span class="s1">clf2.partial_fit(X[</span><span class="s4">2</span><span class="s1">:</span><span class="s4">5</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y2[</span><span class="s4">2</span><span class="s1">:</span><span class="s4">5</span><span class="s1">])</span>
    <span class="s1">clf2.partial_fit(X[</span><span class="s4">5</span><span class="s1">:]</span><span class="s0">, </span><span class="s1">y2[</span><span class="s4">5</span><span class="s1">:])</span>

    <span class="s1">y_pred2 = clf2.predict(X)</span>
    <span class="s1">assert_array_equal(y_pred2</span><span class="s0">, </span><span class="s1">y2)</span>

    <span class="s1">y_pred_proba2 = clf2.predict_proba(X)</span>
    <span class="s1">y_pred_log_proba2 = clf2.predict_log_proba(X)</span>
    <span class="s1">assert_array_almost_equal(np.log(y_pred_proba2)</span><span class="s0">, </span><span class="s1">y_pred_log_proba2</span><span class="s0">, </span><span class="s4">8</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(y_pred_proba2</span><span class="s0">, </span><span class="s1">y_pred_proba)</span>
    <span class="s1">assert_array_almost_equal(y_pred_log_proba2</span><span class="s0">, </span><span class="s1">y_pred_log_proba)</span>

    <span class="s3"># Partial fit on the whole data at once should be the same as fit too</span>
    <span class="s1">clf3 = MultinomialNB()</span>
    <span class="s1">clf3.partial_fit(X</span><span class="s0">, </span><span class="s1">y2</span><span class="s0">, </span><span class="s1">classes=np.unique(y2))</span>

    <span class="s1">y_pred3 = clf3.predict(X)</span>
    <span class="s1">assert_array_equal(y_pred3</span><span class="s0">, </span><span class="s1">y2)</span>
    <span class="s1">y_pred_proba3 = clf3.predict_proba(X)</span>
    <span class="s1">y_pred_log_proba3 = clf3.predict_log_proba(X)</span>
    <span class="s1">assert_array_almost_equal(np.log(y_pred_proba3)</span><span class="s0">, </span><span class="s1">y_pred_log_proba3</span><span class="s0">, </span><span class="s4">8</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(y_pred_proba3</span><span class="s0">, </span><span class="s1">y_pred_proba)</span>
    <span class="s1">assert_array_almost_equal(y_pred_log_proba3</span><span class="s0">, </span><span class="s1">y_pred_log_proba)</span>


<span class="s0">def </span><span class="s1">test_mnb_prior_unobserved_targets():</span>
    <span class="s3"># test smoothing of prior for yet unobserved targets</span>

    <span class="s3"># Create toy training data</span>
    <span class="s1">X = np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>

    <span class="s1">clf = MultinomialNB()</span>

    <span class="s0">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;error&quot;</span><span class="s0">, </span><span class="s1">RuntimeWarning)</span>

        <span class="s1">clf.partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>

    <span class="s0">assert </span><span class="s1">clf.predict([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]) == </span><span class="s4">0</span>
    <span class="s0">assert </span><span class="s1">clf.predict([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]]) == </span><span class="s4">1</span>
    <span class="s0">assert </span><span class="s1">clf.predict([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]) == </span><span class="s4">0</span>

    <span class="s3"># add a training example with previously unobserved class</span>
    <span class="s0">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;error&quot;</span><span class="s0">, </span><span class="s1">RuntimeWarning)</span>

        <span class="s1">clf.partial_fit([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">2</span><span class="s1">])</span>

    <span class="s0">assert </span><span class="s1">clf.predict([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]) == </span><span class="s4">0</span>
    <span class="s0">assert </span><span class="s1">clf.predict([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]]) == </span><span class="s4">1</span>
    <span class="s0">assert </span><span class="s1">clf.predict([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]) == </span><span class="s4">2</span>


<span class="s0">def </span><span class="s1">test_bnb():</span>
    <span class="s3"># Tests that BernoulliNB when alpha=1.0 gives the same values as</span>
    <span class="s3"># those given for the toy example in Manning, Raghavan, and</span>
    <span class="s3"># Schuetze's &quot;Introduction to Information Retrieval&quot; book:</span>
    <span class="s3"># https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html</span>

    <span class="s3"># Training data points are:</span>
    <span class="s3"># Chinese Beijing Chinese (class: China)</span>
    <span class="s3"># Chinese Chinese Shanghai (class: China)</span>
    <span class="s3"># Chinese Macao (class: China)</span>
    <span class="s3"># Tokyo Japan Chinese (class: Japan)</span>

    <span class="s3"># Features are Beijing, Chinese, Japan, Macao, Shanghai, and Tokyo</span>
    <span class="s1">X = np.array(</span>
        <span class="s1">[[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span>
    <span class="s1">)</span>

    <span class="s3"># Classes are China (0), Japan (1)</span>
    <span class="s1">Y = np.array([</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>

    <span class="s3"># Fit BernoulliBN w/ alpha = 1.0</span>
    <span class="s1">clf = BernoulliNB(alpha=</span><span class="s4">1.0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s3"># Check the class prior is correct</span>
    <span class="s1">class_prior = np.array([</span><span class="s4">0.75</span><span class="s0">, </span><span class="s4">0.25</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(np.exp(clf.class_log_prior_)</span><span class="s0">, </span><span class="s1">class_prior)</span>

    <span class="s3"># Check the feature probabilities are correct</span>
    <span class="s1">feature_prob = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">0.4</span><span class="s0">, </span><span class="s4">0.8</span><span class="s0">, </span><span class="s4">0.2</span><span class="s0">, </span><span class="s4">0.4</span><span class="s0">, </span><span class="s4">0.4</span><span class="s0">, </span><span class="s4">0.2</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">1 </span><span class="s1">/ </span><span class="s4">3.0</span><span class="s0">, </span><span class="s4">2 </span><span class="s1">/ </span><span class="s4">3.0</span><span class="s0">, </span><span class="s4">2 </span><span class="s1">/ </span><span class="s4">3.0</span><span class="s0">, </span><span class="s4">1 </span><span class="s1">/ </span><span class="s4">3.0</span><span class="s0">, </span><span class="s4">1 </span><span class="s1">/ </span><span class="s4">3.0</span><span class="s0">, </span><span class="s4">2 </span><span class="s1">/ </span><span class="s4">3.0</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(np.exp(clf.feature_log_prob_)</span><span class="s0">, </span><span class="s1">feature_prob)</span>

    <span class="s3"># Testing data point is:</span>
    <span class="s3"># Chinese Chinese Chinese Tokyo Japan</span>
    <span class="s1">X_test = np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span>

    <span class="s3"># Check the predictive probabilities are correct</span>
    <span class="s1">unnorm_predict_proba = np.array([[</span><span class="s4">0.005183999999999999</span><span class="s0">, </span><span class="s4">0.02194787379972565</span><span class="s1">]])</span>
    <span class="s1">predict_proba = unnorm_predict_proba / np.sum(unnorm_predict_proba)</span>
    <span class="s1">assert_array_almost_equal(clf.predict_proba(X_test)</span><span class="s0">, </span><span class="s1">predict_proba)</span>


<span class="s0">def </span><span class="s1">test_bnb_feature_log_prob():</span>
    <span class="s3"># Test for issue #4268.</span>
    <span class="s3"># Tests that the feature log prob value computed by BernoulliNB when</span>
    <span class="s3"># alpha=1.0 is equal to the expression given in Manning, Raghavan,</span>
    <span class="s3"># and Schuetze's &quot;Introduction to Information Retrieval&quot; book:</span>
    <span class="s3"># http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html</span>

    <span class="s1">X = np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]])</span>
    <span class="s1">Y = np.array([</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>

    <span class="s3"># Fit Bernoulli NB w/ alpha = 1.0</span>
    <span class="s1">clf = BernoulliNB(alpha=</span><span class="s4">1.0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s3"># Manually form the (log) numerator and denominator that</span>
    <span class="s3"># constitute P(feature presence | class)</span>
    <span class="s1">num = np.log(clf.feature_count_ + </span><span class="s4">1.0</span><span class="s1">)</span>
    <span class="s1">denom = np.tile(np.log(clf.class_count_ + </span><span class="s4">2.0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(X.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)).T</span>

    <span class="s3"># Check manual estimate matches</span>
    <span class="s1">assert_array_almost_equal(clf.feature_log_prob_</span><span class="s0">, </span><span class="s1">(num - denom))</span>


<span class="s0">def </span><span class="s1">test_cnb():</span>
    <span class="s3"># Tests ComplementNB when alpha=1.0 for the toy example in Manning,</span>
    <span class="s3"># Raghavan, and Schuetze's &quot;Introduction to Information Retrieval&quot; book:</span>
    <span class="s3"># https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html</span>

    <span class="s3"># Training data points are:</span>
    <span class="s3"># Chinese Beijing Chinese (class: China)</span>
    <span class="s3"># Chinese Chinese Shanghai (class: China)</span>
    <span class="s3"># Chinese Macao (class: China)</span>
    <span class="s3"># Tokyo Japan Chinese (class: Japan)</span>

    <span class="s3"># Features are Beijing, Chinese, Japan, Macao, Shanghai, and Tokyo.</span>
    <span class="s1">X = np.array(</span>
        <span class="s1">[[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span>
    <span class="s1">)</span>

    <span class="s3"># Classes are China (0), Japan (1).</span>
    <span class="s1">Y = np.array([</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>

    <span class="s3"># Check that weights are correct. See steps 4-6 in Table 4 of</span>
    <span class="s3"># Rennie et al. (2003).</span>
    <span class="s1">theta = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span>
                <span class="s1">(</span><span class="s4">0 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">) / (</span><span class="s4">3 </span><span class="s1">+ </span><span class="s4">6</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">(</span><span class="s4">1 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">) / (</span><span class="s4">3 </span><span class="s1">+ </span><span class="s4">6</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">(</span><span class="s4">1 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">) / (</span><span class="s4">3 </span><span class="s1">+ </span><span class="s4">6</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">(</span><span class="s4">0 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">) / (</span><span class="s4">3 </span><span class="s1">+ </span><span class="s4">6</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">(</span><span class="s4">0 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">) / (</span><span class="s4">3 </span><span class="s1">+ </span><span class="s4">6</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">(</span><span class="s4">1 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">) / (</span><span class="s4">3 </span><span class="s1">+ </span><span class="s4">6</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span>
                <span class="s1">(</span><span class="s4">1 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">) / (</span><span class="s4">6 </span><span class="s1">+ </span><span class="s4">6</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">(</span><span class="s4">3 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">) / (</span><span class="s4">6 </span><span class="s1">+ </span><span class="s4">6</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">(</span><span class="s4">0 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">) / (</span><span class="s4">6 </span><span class="s1">+ </span><span class="s4">6</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">(</span><span class="s4">1 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">) / (</span><span class="s4">6 </span><span class="s1">+ </span><span class="s4">6</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">(</span><span class="s4">1 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">) / (</span><span class="s4">6 </span><span class="s1">+ </span><span class="s4">6</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">(</span><span class="s4">0 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">) / (</span><span class="s4">6 </span><span class="s1">+ </span><span class="s4">6</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">weights = np.zeros(theta.shape)</span>
    <span class="s1">normed_weights = np.zeros(theta.shape)</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">2</span><span class="s1">):</span>
        <span class="s1">weights[i] = -np.log(theta[i])</span>
        <span class="s1">normed_weights[i] = weights[i] / weights[i].sum()</span>

    <span class="s3"># Verify inputs are nonnegative.</span>
    <span class="s1">clf = ComplementNB(alpha=</span><span class="s4">1.0</span><span class="s1">)</span>

    <span class="s1">msg = re.escape(</span><span class="s2">&quot;Negative values in data passed to ComplementNB (input X)&quot;</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.fit(-X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s3"># Check that counts/weights are correct.</span>
    <span class="s1">feature_count = np.array([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">3</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">assert_array_equal(clf.feature_count_</span><span class="s0">, </span><span class="s1">feature_count)</span>
    <span class="s1">class_count = np.array([</span><span class="s4">3</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(clf.class_count_</span><span class="s0">, </span><span class="s1">class_count)</span>
    <span class="s1">feature_all = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">4</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(clf.feature_all_</span><span class="s0">, </span><span class="s1">feature_all)</span>
    <span class="s1">assert_array_almost_equal(clf.feature_log_prob_</span><span class="s0">, </span><span class="s1">weights)</span>

    <span class="s1">clf = ComplementNB(alpha=</span><span class="s4">1.0</span><span class="s0">, </span><span class="s1">norm=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">assert_array_almost_equal(clf.feature_log_prob_</span><span class="s0">, </span><span class="s1">normed_weights)</span>


<span class="s0">def </span><span class="s1">test_categoricalnb(global_random_seed):</span>
    <span class="s3"># Check the ability to predict the training set.</span>
    <span class="s1">clf = CategoricalNB()</span>
    <span class="s1">X2</span><span class="s0">, </span><span class="s1">y2 = get_random_integer_x_three_classes_y(global_random_seed)</span>

    <span class="s1">y_pred = clf.fit(X2</span><span class="s0">, </span><span class="s1">y2).predict(X2)</span>
    <span class="s1">assert_array_equal(y_pred</span><span class="s0">, </span><span class="s1">y2)</span>

    <span class="s1">X3 = np.array([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">4</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">5</span><span class="s1">]])</span>
    <span class="s1">y3 = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">clf = CategoricalNB(alpha=</span><span class="s4">1</span><span class="s0">, </span><span class="s1">fit_prior=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s1">clf.fit(X3</span><span class="s0">, </span><span class="s1">y3)</span>
    <span class="s1">assert_array_equal(clf.n_categories_</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s4">3</span><span class="s0">, </span><span class="s4">6</span><span class="s1">]))</span>

    <span class="s3"># Check error is raised for X with negative entries</span>
    <span class="s1">X = np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">error_msg = re.escape(</span><span class="s2">&quot;Negative values in data passed to CategoricalNB (input X)&quot;</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=error_msg):</span>
        <span class="s1">clf.predict(X)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=error_msg):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s3"># Test alpha</span>
    <span class="s1">X3_test = np.array([[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">5</span><span class="s1">]])</span>
    <span class="s3"># alpha=1 increases the count of all categories by one so the final</span>
    <span class="s3"># probability for each category is not 50/50 but 1/3 to 2/3</span>
    <span class="s1">bayes_numerator = np.array([[</span><span class="s4">1 </span><span class="s1">/ </span><span class="s4">3 </span><span class="s1">* </span><span class="s4">1 </span><span class="s1">/ </span><span class="s4">3</span><span class="s0">, </span><span class="s4">2 </span><span class="s1">/ </span><span class="s4">3 </span><span class="s1">* </span><span class="s4">2 </span><span class="s1">/ </span><span class="s4">3</span><span class="s1">]])</span>
    <span class="s1">bayes_denominator = bayes_numerator.sum()</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">clf.predict_proba(X3_test)</span><span class="s0">, </span><span class="s1">bayes_numerator / bayes_denominator</span>
    <span class="s1">)</span>

    <span class="s3"># Assert category_count has counted all features</span>
    <span class="s0">assert </span><span class="s1">len(clf.category_count_) == X3.shape[</span><span class="s4">1</span><span class="s1">]</span>

    <span class="s3"># Check sample_weight</span>
    <span class="s1">X = np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">clf = CategoricalNB(alpha=</span><span class="s4">1</span><span class="s0">, </span><span class="s1">fit_prior=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(clf.predict(np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]]))</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s4">1</span><span class="s1">]))</span>
    <span class="s1">assert_array_equal(clf.n_categories_</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]))</span>

    <span class="s0">for </span><span class="s1">factor </span><span class="s0">in </span><span class="s1">[</span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">0.3</span><span class="s0">, </span><span class="s4">5</span><span class="s0">, </span><span class="s4">0.0001</span><span class="s1">]:</span>
        <span class="s1">X = np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span>
        <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>
        <span class="s1">sample_weight = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">10</span><span class="s0">, </span><span class="s4">0.1</span><span class="s1">]) * factor</span>
        <span class="s1">clf = CategoricalNB(alpha=</span><span class="s4">1</span><span class="s0">, </span><span class="s1">fit_prior=</span><span class="s0">False</span><span class="s1">)</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
        <span class="s1">assert_array_equal(clf.predict(np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]]))</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s4">2</span><span class="s1">]))</span>
        <span class="s1">assert_array_equal(clf.n_categories_</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]))</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;min_categories, exp_X1_count, exp_X2_count, new_X, exp_n_categories_&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s3"># check min_categories with int &gt; observed categories</span>
        <span class="s1">(</span>
            <span class="s4">3</span><span class="s0">,</span>
            <span class="s1">np.array([[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]])</span><span class="s0">,</span>
            <span class="s1">np.array([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]])</span><span class="s0">,</span>
            <span class="s1">np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]])</span><span class="s0">,</span>
            <span class="s1">np.array([</span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s3"># check with list input</span>
        <span class="s1">(</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">4</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">np.array([[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]])</span><span class="s0">,</span>
            <span class="s1">np.array([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]])</span><span class="s0">,</span>
            <span class="s1">np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]])</span><span class="s0">,</span>
            <span class="s1">np.array([</span><span class="s4">3</span><span class="s0">, </span><span class="s4">4</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s3"># check min_categories with min less than actual</span>
        <span class="s1">(</span>
            <span class="s1">[</span>
                <span class="s4">1</span><span class="s0">,</span>
                <span class="s1">np.array([[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span><span class="s0">,</span>
                <span class="s1">np.array([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span><span class="s0">,</span>
                <span class="s1">np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span><span class="s0">,</span>
                <span class="s1">np.array([</span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span><span class="s0">,</span>
            <span class="s1">]</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_categoricalnb_with_min_categories(</span>
    <span class="s1">min_categories</span><span class="s0">, </span><span class="s1">exp_X1_count</span><span class="s0">, </span><span class="s1">exp_X2_count</span><span class="s0">, </span><span class="s1">new_X</span><span class="s0">, </span><span class="s1">exp_n_categories_</span>
<span class="s1">):</span>
    <span class="s1">X_n_categories = np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">y_n_categories = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">expected_prediction = np.array([</span><span class="s4">1</span><span class="s1">])</span>

    <span class="s1">clf = CategoricalNB(alpha=</span><span class="s4">1</span><span class="s0">, </span><span class="s1">fit_prior=</span><span class="s0">False, </span><span class="s1">min_categories=min_categories)</span>
    <span class="s1">clf.fit(X_n_categories</span><span class="s0">, </span><span class="s1">y_n_categories)</span>
    <span class="s1">X1_count</span><span class="s0">, </span><span class="s1">X2_count = clf.category_count_</span>
    <span class="s1">assert_array_equal(X1_count</span><span class="s0">, </span><span class="s1">exp_X1_count)</span>
    <span class="s1">assert_array_equal(X2_count</span><span class="s0">, </span><span class="s1">exp_X2_count)</span>
    <span class="s1">predictions = clf.predict(new_X)</span>
    <span class="s1">assert_array_equal(predictions</span><span class="s0">, </span><span class="s1">expected_prediction)</span>
    <span class="s1">assert_array_equal(clf.n_categories_</span><span class="s0">, </span><span class="s1">exp_n_categories_)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;min_categories, error_msg&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">([[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">4</span><span class="s1">]]</span><span class="s0">, </span><span class="s2">&quot;'min_categories' should have shape&quot;</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_categoricalnb_min_categories_errors(min_categories</span><span class="s0">, </span><span class="s1">error_msg):</span>
    <span class="s1">X = np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>

    <span class="s1">clf = CategoricalNB(alpha=</span><span class="s4">1</span><span class="s0">, </span><span class="s1">fit_prior=</span><span class="s0">False, </span><span class="s1">min_categories=min_categories)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=error_msg):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_alpha():</span>
    <span class="s3"># Setting alpha=0 should not output nan results when p(x_i|y_j)=0 is a case</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">nb = BernoulliNB(alpha=</span><span class="s4">0.0</span><span class="s1">)</span>
    <span class="s1">msg = </span><span class="s2">&quot;alpha too small will result in numeric errors, setting alpha = 1.0e-10&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">nb.partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">nb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">prob = np.array([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">assert_array_almost_equal(nb.predict_proba(X)</span><span class="s0">, </span><span class="s1">prob)</span>

    <span class="s1">nb = MultinomialNB(alpha=</span><span class="s4">0.0</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">nb.partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">nb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">prob = np.array([[</span><span class="s4">2.0 </span><span class="s1">/ </span><span class="s4">3</span><span class="s0">, </span><span class="s4">1.0 </span><span class="s1">/ </span><span class="s4">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">assert_array_almost_equal(nb.predict_proba(X)</span><span class="s0">, </span><span class="s1">prob)</span>

    <span class="s1">nb = CategoricalNB(alpha=</span><span class="s4">0.0</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">nb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">prob = np.array([[</span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0.0</span><span class="s0">, </span><span class="s4">1.0</span><span class="s1">]])</span>
    <span class="s1">assert_array_almost_equal(nb.predict_proba(X)</span><span class="s0">, </span><span class="s1">prob)</span>

    <span class="s3"># Test sparse X</span>
    <span class="s1">X = scipy.sparse.csr_matrix(X)</span>
    <span class="s1">nb = BernoulliNB(alpha=</span><span class="s4">0.0</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">nb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">prob = np.array([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">assert_array_almost_equal(nb.predict_proba(X)</span><span class="s0">, </span><span class="s1">prob)</span>

    <span class="s1">nb = MultinomialNB(alpha=</span><span class="s4">0.0</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">nb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">prob = np.array([[</span><span class="s4">2.0 </span><span class="s1">/ </span><span class="s4">3</span><span class="s0">, </span><span class="s4">1.0 </span><span class="s1">/ </span><span class="s4">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">assert_array_almost_equal(nb.predict_proba(X)</span><span class="s0">, </span><span class="s1">prob)</span>


<span class="s0">def </span><span class="s1">test_alpha_vector():</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>

    <span class="s3"># Setting alpha=np.array with same length</span>
    <span class="s3"># as number of features should be fine</span>
    <span class="s1">alpha = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">nb = MultinomialNB(alpha=alpha)</span>
    <span class="s1">nb.partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>

    <span class="s3"># Test feature probabilities uses pseudo-counts (alpha)</span>
    <span class="s1">feature_prob = np.array([[</span><span class="s4">1 </span><span class="s1">/ </span><span class="s4">2</span><span class="s0">, </span><span class="s4">1 </span><span class="s1">/ </span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">2 </span><span class="s1">/ </span><span class="s4">5</span><span class="s0">, </span><span class="s4">3 </span><span class="s1">/ </span><span class="s4">5</span><span class="s1">]])</span>
    <span class="s1">assert_array_almost_equal(nb.feature_log_prob_</span><span class="s0">, </span><span class="s1">np.log(feature_prob))</span>

    <span class="s3"># Test predictions</span>
    <span class="s1">prob = np.array([[</span><span class="s4">5 </span><span class="s1">/ </span><span class="s4">9</span><span class="s0">, </span><span class="s4">4 </span><span class="s1">/ </span><span class="s4">9</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">25 </span><span class="s1">/ </span><span class="s4">49</span><span class="s0">, </span><span class="s4">24 </span><span class="s1">/ </span><span class="s4">49</span><span class="s1">]])</span>
    <span class="s1">assert_array_almost_equal(nb.predict_proba(X)</span><span class="s0">, </span><span class="s1">prob)</span>

    <span class="s3"># Test alpha non-negative</span>
    <span class="s1">alpha = np.array([</span><span class="s4">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s4">0.1</span><span class="s1">])</span>
    <span class="s1">m_nb = MultinomialNB(alpha=alpha)</span>
    <span class="s1">expected_msg = </span><span class="s2">&quot;All values in alpha must be greater than 0.&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=expected_msg):</span>
        <span class="s1">m_nb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s3"># Test that too small pseudo-counts are replaced</span>
    <span class="s1">ALPHA_MIN = </span><span class="s4">1e-10</span>
    <span class="s1">alpha = np.array([ALPHA_MIN / </span><span class="s4">2</span><span class="s0">, </span><span class="s4">0.5</span><span class="s1">])</span>
    <span class="s1">m_nb = MultinomialNB(alpha=alpha)</span>
    <span class="s1">m_nb.partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(m_nb._check_alpha()</span><span class="s0">, </span><span class="s1">[ALPHA_MIN</span><span class="s0">, </span><span class="s4">0.5</span><span class="s1">]</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s4">12</span><span class="s1">)</span>

    <span class="s3"># Test correct dimensions</span>
    <span class="s1">alpha = np.array([</span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">2.0</span><span class="s0">, </span><span class="s4">3.0</span><span class="s1">])</span>
    <span class="s1">m_nb = MultinomialNB(alpha=alpha)</span>
    <span class="s1">expected_msg = </span><span class="s2">&quot;When alpha is an array, it should contains `n_features`&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=expected_msg):</span>
        <span class="s1">m_nb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_check_accuracy_on_digits():</span>
    <span class="s3"># Non regression test to make sure that any further refactoring / optim</span>
    <span class="s3"># of the NB models do not harm the performance on a slightly non-linearly</span>
    <span class="s3"># separable dataset</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = load_digits(return_X_y=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">binary_3v8 = np.logical_or(y == </span><span class="s4">3</span><span class="s0">, </span><span class="s1">y == </span><span class="s4">8</span><span class="s1">)</span>
    <span class="s1">X_3v8</span><span class="s0">, </span><span class="s1">y_3v8 = X[binary_3v8]</span><span class="s0">, </span><span class="s1">y[binary_3v8]</span>

    <span class="s3"># Multinomial NB</span>
    <span class="s1">scores = cross_val_score(MultinomialNB(alpha=</span><span class="s4">10</span><span class="s1">)</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">cv=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">scores.mean() &gt; </span><span class="s4">0.86</span>

    <span class="s1">scores = cross_val_score(MultinomialNB(alpha=</span><span class="s4">10</span><span class="s1">)</span><span class="s0">, </span><span class="s1">X_3v8</span><span class="s0">, </span><span class="s1">y_3v8</span><span class="s0">, </span><span class="s1">cv=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">scores.mean() &gt; </span><span class="s4">0.94</span>

    <span class="s3"># Bernoulli NB</span>
    <span class="s1">scores = cross_val_score(BernoulliNB(alpha=</span><span class="s4">10</span><span class="s1">)</span><span class="s0">, </span><span class="s1">X &gt; </span><span class="s4">4</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">cv=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">scores.mean() &gt; </span><span class="s4">0.83</span>

    <span class="s1">scores = cross_val_score(BernoulliNB(alpha=</span><span class="s4">10</span><span class="s1">)</span><span class="s0">, </span><span class="s1">X_3v8 &gt; </span><span class="s4">4</span><span class="s0">, </span><span class="s1">y_3v8</span><span class="s0">, </span><span class="s1">cv=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">scores.mean() &gt; </span><span class="s4">0.92</span>

    <span class="s3"># Gaussian NB</span>
    <span class="s1">scores = cross_val_score(GaussianNB()</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">cv=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">scores.mean() &gt; </span><span class="s4">0.77</span>

    <span class="s1">scores = cross_val_score(GaussianNB(var_smoothing=</span><span class="s4">0.1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">cv=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">scores.mean() &gt; </span><span class="s4">0.89</span>

    <span class="s1">scores = cross_val_score(GaussianNB()</span><span class="s0">, </span><span class="s1">X_3v8</span><span class="s0">, </span><span class="s1">y_3v8</span><span class="s0">, </span><span class="s1">cv=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">scores.mean() &gt; </span><span class="s4">0.86</span>


<span class="s3"># TODO(1.4): Remove</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s1">DISCRETE_NAIVE_BAYES_CLASSES)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;alpha&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0.1</span><span class="s0">, </span><span class="s4">1e-11</span><span class="s1">]</span><span class="s0">, </span><span class="s4">1e-12</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_force_alpha_deprecation(Estimator</span><span class="s0">, </span><span class="s1">alpha):</span>
    <span class="s0">if </span><span class="s1">Estimator </span><span class="s0">is </span><span class="s1">CategoricalNB </span><span class="s0">and </span><span class="s1">isinstance(alpha</span><span class="s0">, </span><span class="s1">list):</span>
        <span class="s1">pytest.skip(</span><span class="s2">&quot;CategoricalNB does not support array-like alpha values.&quot;</span><span class="s1">)</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">4</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">alpha_min = </span><span class="s4">1e-10</span>
    <span class="s1">msg = </span><span class="s2">&quot;The default value for `force_alpha` will change to `True`&quot;</span>
    <span class="s1">est = Estimator(alpha=alpha)</span>
    <span class="s1">est_force = Estimator(alpha=alpha</span><span class="s0">, </span><span class="s1">force_alpha=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">np.min(alpha) &lt; alpha_min:</span>
        <span class="s0">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s0">, </span><span class="s1">match=msg):</span>
            <span class="s1">est.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">est.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">est_force.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_check_alpha():</span>
    <span class="s5">&quot;&quot;&quot;The provided value for alpha must only be 
    used if alpha &lt; _ALPHA_MIN and force_alpha is True. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/10772 
    &quot;&quot;&quot;</span>
    <span class="s1">_ALPHA_MIN = </span><span class="s4">1e-10</span>
    <span class="s1">b = BernoulliNB(alpha=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">force_alpha=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">b._check_alpha() == </span><span class="s4">0</span>

    <span class="s1">alphas = np.array([</span><span class="s4">0.0</span><span class="s0">, </span><span class="s4">1.0</span><span class="s1">])</span>

    <span class="s1">b = BernoulliNB(alpha=alphas</span><span class="s0">, </span><span class="s1">force_alpha=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s3"># We manually set `n_features_in_` not to have `_check_alpha` err</span>
    <span class="s1">b.n_features_in_ = alphas.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">assert_array_equal(b._check_alpha()</span><span class="s0">, </span><span class="s1">alphas)</span>

    <span class="s1">msg = (</span>
        <span class="s2">&quot;alpha too small will result in numeric errors, setting alpha = %.1e&quot;</span>
        <span class="s1">% _ALPHA_MIN</span>
    <span class="s1">)</span>
    <span class="s1">b = BernoulliNB(alpha=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">force_alpha=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s0">assert </span><span class="s1">b._check_alpha() == _ALPHA_MIN</span>

    <span class="s1">b = BernoulliNB(alpha=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s0">assert </span><span class="s1">b._check_alpha() == _ALPHA_MIN</span>

    <span class="s1">b = BernoulliNB(alpha=alphas</span><span class="s0">, </span><span class="s1">force_alpha=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s3"># We manually set `n_features_in_` not to have `_check_alpha` err</span>
    <span class="s1">b.n_features_in_ = alphas.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">assert_array_equal(b._check_alpha()</span><span class="s0">, </span><span class="s1">np.array([_ALPHA_MIN</span><span class="s0">, </span><span class="s4">1.0</span><span class="s1">]))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s1">ALL_NAIVE_BAYES_CLASSES)</span>
<span class="s0">def </span><span class="s1">test_predict_joint_proba(Estimator</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s1">X2</span><span class="s0">, </span><span class="s1">y2 = get_random_integer_x_three_classes_y(global_random_seed)</span>
    <span class="s1">est = Estimator().fit(X2</span><span class="s0">, </span><span class="s1">y2)</span>
    <span class="s1">jll = est.predict_joint_log_proba(X2)</span>
    <span class="s1">log_prob_x = logsumexp(jll</span><span class="s0">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">log_prob_x_y = jll - np.atleast_2d(log_prob_x).T</span>
    <span class="s1">assert_allclose(est.predict_log_proba(X2)</span><span class="s0">, </span><span class="s1">log_prob_x_y)</span>
</pre>
</body>
</html>