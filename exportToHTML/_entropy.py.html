<html>
<head>
<title>_entropy.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_entropy.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Created on Fri Apr  2 09:06:05 2021 
 
@author: matth 
&quot;&quot;&quot;</span>

<span class="s2">from </span><span class="s1">__future__ </span><span class="s2">import </span><span class="s1">annotations</span>
<span class="s2">import </span><span class="s1">math</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">special</span>

<span class="s1">__all__ = [</span><span class="s3">'entropy'</span><span class="s2">, </span><span class="s3">'differential_entropy'</span><span class="s1">]</span>


<span class="s2">def </span><span class="s1">entropy(pk: np.typing.ArrayLike</span><span class="s2">,</span>
            <span class="s1">qk: np.typing.ArrayLike | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
            <span class="s1">base: float | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
            <span class="s1">axis: int = </span><span class="s4">0</span>
            <span class="s1">) -&gt; np.number | np.ndarray:</span>
    <span class="s0">&quot;&quot;&quot; 
    Calculate the Shannon entropy/relative entropy of given distribution(s). 
 
    If only probabilities `pk` are given, the Shannon entropy is calculated as 
    ``H = -sum(pk * log(pk))``. 
 
    If `qk` is not None, then compute the relative entropy 
    ``D = sum(pk * log(pk / qk))``. This quantity is also known 
    as the Kullback-Leibler divergence. 
 
    This routine will normalize `pk` and `qk` if they don't sum to 1. 
 
    Parameters 
    ---------- 
    pk : array_like 
        Defines the (discrete) distribution. Along each axis-slice of ``pk``, 
        element ``i`` is the  (possibly unnormalized) probability of event 
        ``i``. 
    qk : array_like, optional 
        Sequence against which the relative entropy is computed. Should be in 
        the same format as `pk`. 
    base : float, optional 
        The logarithmic base to use, defaults to ``e`` (natural logarithm). 
    axis : int, optional 
        The axis along which the entropy is calculated. Default is 0. 
 
    Returns 
    ------- 
    S : {float, array_like} 
        The calculated entropy. 
 
    Notes 
    ----- 
    Informally, the Shannon entropy quantifies the expected uncertainty 
    inherent in the possible outcomes of a discrete random variable. 
    For example, 
    if messages consisting of sequences of symbols from a set are to be 
    encoded and transmitted over a noiseless channel, then the Shannon entropy 
    ``H(pk)`` gives a tight lower bound for the average number of units of 
    information needed per symbol if the symbols occur with frequencies 
    governed by the discrete distribution `pk` [1]_. The choice of base 
    determines the choice of units; e.g., ``e`` for nats, ``2`` for bits, etc. 
 
    The relative entropy, ``D(pk|qk)``, quantifies the increase in the average 
    number of units of information needed per symbol if the encoding is 
    optimized for the probability distribution `qk` instead of the true 
    distribution `pk`. Informally, the relative entropy quantifies the expected 
    excess in surprise experienced if one believes the true distribution is 
    `qk` when it is actually `pk`. 
 
    A related quantity, the cross entropy ``CE(pk, qk)``, satisfies the 
    equation ``CE(pk, qk) = H(pk) + D(pk|qk)`` and can also be calculated with 
    the formula ``CE = -sum(pk * log(qk))``. It gives the average 
    number of units of information needed per symbol if an encoding is 
    optimized for the probability distribution `qk` when the true distribution 
    is `pk`. It is not computed directly by `entropy`, but it can be computed 
    using two calls to the function (see Examples). 
 
    See [2]_ for more information. 
 
    References 
    ---------- 
    .. [1] Shannon, C.E. (1948), A Mathematical Theory of Communication. 
           Bell System Technical Journal, 27: 379-423. 
           https://doi.org/10.1002/j.1538-7305.1948.tb01338.x 
    .. [2] Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information 
           Theory (Wiley Series in Telecommunications and Signal Processing). 
           Wiley-Interscience, USA. 
 
 
    Examples 
    -------- 
    The outcome of a fair coin is the most uncertain: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import entropy 
    &gt;&gt;&gt; base = 2  # work in units of bits 
    &gt;&gt;&gt; pk = np.array([1/2, 1/2])  # fair coin 
    &gt;&gt;&gt; H = entropy(pk, base=base) 
    &gt;&gt;&gt; H 
    1.0 
    &gt;&gt;&gt; H == -np.sum(pk * np.log(pk)) / np.log(base) 
    True 
 
    The outcome of a biased coin is less uncertain: 
 
    &gt;&gt;&gt; qk = np.array([9/10, 1/10])  # biased coin 
    &gt;&gt;&gt; entropy(qk, base=base) 
    0.46899559358928117 
 
    The relative entropy between the fair coin and biased coin is calculated 
    as: 
 
    &gt;&gt;&gt; D = entropy(pk, qk, base=base) 
    &gt;&gt;&gt; D 
    0.7369655941662062 
    &gt;&gt;&gt; D == np.sum(pk * np.log(pk/qk)) / np.log(base) 
    True 
 
    The cross entropy can be calculated as the sum of the entropy and 
    relative entropy`: 
 
    &gt;&gt;&gt; CE = entropy(pk, base=base) + entropy(pk, qk, base=base) 
    &gt;&gt;&gt; CE 
    1.736965594166206 
    &gt;&gt;&gt; CE == -np.sum(pk * np.log(qk)) / np.log(base) 
    True 
 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">base </span><span class="s2">is not None and </span><span class="s1">base &lt;= </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`base` must be a positive number or `None`.&quot;</span><span class="s1">)</span>

    <span class="s1">pk = np.asarray(pk)</span>
    <span class="s1">pk = </span><span class="s4">1.0</span><span class="s1">*pk / np.sum(pk</span><span class="s2">, </span><span class="s1">axis=axis</span><span class="s2">, </span><span class="s1">keepdims=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">qk </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">vec = special.entr(pk)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">qk = np.asarray(qk)</span>
        <span class="s1">pk</span><span class="s2">, </span><span class="s1">qk = np.broadcast_arrays(pk</span><span class="s2">, </span><span class="s1">qk)</span>
        <span class="s1">qk = </span><span class="s4">1.0</span><span class="s1">*qk / np.sum(qk</span><span class="s2">, </span><span class="s1">axis=axis</span><span class="s2">, </span><span class="s1">keepdims=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">vec = special.rel_entr(pk</span><span class="s2">, </span><span class="s1">qk)</span>
    <span class="s1">S = np.sum(vec</span><span class="s2">, </span><span class="s1">axis=axis)</span>
    <span class="s2">if </span><span class="s1">base </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">S /= np.log(base)</span>
    <span class="s2">return </span><span class="s1">S</span>


<span class="s2">def </span><span class="s1">differential_entropy(</span>
    <span class="s1">values: np.typing.ArrayLike</span><span class="s2">,</span>
    <span class="s1">*</span><span class="s2">,</span>
    <span class="s1">window_length: int | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
    <span class="s1">base: float | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
    <span class="s1">axis: int = </span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">method: str = </span><span class="s3">&quot;auto&quot;</span><span class="s2">,</span>
<span class="s1">) -&gt; np.number | np.ndarray:</span>
    <span class="s0">r&quot;&quot;&quot;Given a sample of a distribution, estimate the differential entropy. 
 
    Several estimation methods are available using the `method` parameter. By 
    default, a method is selected based the size of the sample. 
 
    Parameters 
    ---------- 
    values : sequence 
        Sample from a continuous distribution. 
    window_length : int, optional 
        Window length for computing Vasicek estimate. Must be an integer 
        between 1 and half of the sample size. If ``None`` (the default), it 
        uses the heuristic value 
 
        .. math:: 
            \left \lfloor \sqrt{n} + 0.5 \right \rfloor 
 
        where :math:`n` is the sample size. This heuristic was originally 
        proposed in [2]_ and has become common in the literature. 
    base : float, optional 
        The logarithmic base to use, defaults to ``e`` (natural logarithm). 
    axis : int, optional 
        The axis along which the differential entropy is calculated. 
        Default is 0. 
    method : {'vasicek', 'van es', 'ebrahimi', 'correa', 'auto'}, optional 
        The method used to estimate the differential entropy from the sample. 
        Default is ``'auto'``.  See Notes for more information. 
 
    Returns 
    ------- 
    entropy : float 
        The calculated differential entropy. 
 
    Notes 
    ----- 
    This function will converge to the true differential entropy in the limit 
 
    .. math:: 
        n \to \infty, \quad m \to \infty, \quad \frac{m}{n} \to 0 
 
    The optimal choice of ``window_length`` for a given sample size depends on 
    the (unknown) distribution. Typically, the smoother the density of the 
    distribution, the larger the optimal value of ``window_length`` [1]_. 
 
    The following options are available for the `method` parameter. 
 
    * ``'vasicek'`` uses the estimator presented in [1]_. This is 
      one of the first and most influential estimators of differential entropy. 
    * ``'van es'`` uses the bias-corrected estimator presented in [3]_, which 
      is not only consistent but, under some conditions, asymptotically normal. 
    * ``'ebrahimi'`` uses an estimator presented in [4]_, which was shown 
      in simulation to have smaller bias and mean squared error than 
      the Vasicek estimator. 
    * ``'correa'`` uses the estimator presented in [5]_ based on local linear 
      regression. In a simulation study, it had consistently smaller mean 
      square error than the Vasiceck estimator, but it is more expensive to 
      compute. 
    * ``'auto'`` selects the method automatically (default). Currently, 
      this selects ``'van es'`` for very small samples (&lt;10), ``'ebrahimi'`` 
      for moderate sample sizes (11-1000), and ``'vasicek'`` for larger 
      samples, but this behavior is subject to change in future versions. 
 
    All estimators are implemented as described in [6]_. 
 
    References 
    ---------- 
    .. [1] Vasicek, O. (1976). A test for normality based on sample entropy. 
           Journal of the Royal Statistical Society: 
           Series B (Methodological), 38(1), 54-59. 
    .. [2] Crzcgorzewski, P., &amp; Wirczorkowski, R. (1999). Entropy-based 
           goodness-of-fit test for exponentiality. Communications in 
           Statistics-Theory and Methods, 28(5), 1183-1202. 
    .. [3] Van Es, B. (1992). Estimating functionals related to a density by a 
           class of statistics based on spacings. Scandinavian Journal of 
           Statistics, 61-72. 
    .. [4] Ebrahimi, N., Pflughoeft, K., &amp; Soofi, E. S. (1994). Two measures 
           of sample entropy. Statistics &amp; Probability Letters, 20(3), 225-234. 
    .. [5] Correa, J. C. (1995). A new estimator of entropy. Communications 
           in Statistics-Theory and Methods, 24(10), 2439-2449. 
    .. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods. 
           Annals of Data Science, 2(2), 231-241. 
           https://link.springer.com/article/10.1007/s40745-015-0045-9 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import differential_entropy, norm 
 
    Entropy of a standard normal distribution: 
 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; values = rng.standard_normal(100) 
    &gt;&gt;&gt; differential_entropy(values) 
    1.3407817436640392 
 
    Compare with the true entropy: 
 
    &gt;&gt;&gt; float(norm.entropy()) 
    1.4189385332046727 
 
    For several sample sizes between 5 and 1000, compare the accuracy of 
    the ``'vasicek'``, ``'van es'``, and ``'ebrahimi'`` methods. Specifically, 
    compare the root mean squared error (over 1000 trials) between the estimate 
    and the true differential entropy of the distribution. 
 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; def rmse(res, expected): 
    ...     '''Root mean squared error''' 
    ...     return np.sqrt(np.mean((res - expected)**2)) 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; a, b = np.log10(5), np.log10(1000) 
    &gt;&gt;&gt; ns = np.round(np.logspace(a, b, 10)).astype(int) 
    &gt;&gt;&gt; reps = 1000  # number of repetitions for each sample size 
    &gt;&gt;&gt; expected = stats.expon.entropy() 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; method_errors = {'vasicek': [], 'van es': [], 'ebrahimi': []} 
    &gt;&gt;&gt; for method in method_errors: 
    ...     for n in ns: 
    ...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng) 
    ...        res = stats.differential_entropy(rvs, method=method, axis=-1) 
    ...        error = rmse(res, expected) 
    ...        method_errors[method].append(error) 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; for method, errors in method_errors.items(): 
    ...     plt.loglog(ns, errors, label=method) 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; plt.legend() 
    &gt;&gt;&gt; plt.xlabel('sample size') 
    &gt;&gt;&gt; plt.ylabel('RMSE (1000 trials)') 
    &gt;&gt;&gt; plt.title('Entropy Estimator Error (Exponential Distribution)') 
 
    &quot;&quot;&quot;</span>
    <span class="s1">values = np.asarray(values)</span>
    <span class="s1">values = np.moveaxis(values</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">n = values.shape[-</span><span class="s4">1</span><span class="s1">]  </span><span class="s5"># number of observations</span>

    <span class="s2">if </span><span class="s1">window_length </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">window_length = math.floor(math.sqrt(n) + </span><span class="s4">0.5</span><span class="s1">)</span>

    <span class="s2">if not </span><span class="s4">2 </span><span class="s1">&lt;= </span><span class="s4">2 </span><span class="s1">* window_length &lt; n:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span>
            <span class="s3">f&quot;Window length (</span><span class="s2">{</span><span class="s1">window_length</span><span class="s2">}</span><span class="s3">) must be positive and less &quot;</span>
            <span class="s3">f&quot;than half the sample size (</span><span class="s2">{</span><span class="s1">n</span><span class="s2">}</span><span class="s3">).&quot;</span><span class="s2">,</span>
        <span class="s1">)</span>

    <span class="s2">if </span><span class="s1">base </span><span class="s2">is not None and </span><span class="s1">base &lt;= </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`base` must be a positive number or `None`.&quot;</span><span class="s1">)</span>

    <span class="s1">sorted_data = np.sort(values</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>

    <span class="s1">methods = {</span><span class="s3">&quot;vasicek&quot;</span><span class="s1">: _vasicek_entropy</span><span class="s2">,</span>
               <span class="s3">&quot;van es&quot;</span><span class="s1">: _van_es_entropy</span><span class="s2">,</span>
               <span class="s3">&quot;correa&quot;</span><span class="s1">: _correa_entropy</span><span class="s2">,</span>
               <span class="s3">&quot;ebrahimi&quot;</span><span class="s1">: _ebrahimi_entropy</span><span class="s2">,</span>
               <span class="s3">&quot;auto&quot;</span><span class="s1">: _vasicek_entropy}</span>
    <span class="s1">method = method.lower()</span>
    <span class="s2">if </span><span class="s1">method </span><span class="s2">not in </span><span class="s1">methods:</span>
        <span class="s1">message = </span><span class="s3">f&quot;`method` must be one of </span><span class="s2">{</span><span class="s1">set(methods)</span><span class="s2">}</span><span class="s3">&quot;</span>
        <span class="s2">raise </span><span class="s1">ValueError(message)</span>

    <span class="s2">if </span><span class="s1">method == </span><span class="s3">&quot;auto&quot;</span><span class="s1">:</span>
        <span class="s2">if </span><span class="s1">n &lt;= </span><span class="s4">10</span><span class="s1">:</span>
            <span class="s1">method = </span><span class="s3">'van es'</span>
        <span class="s2">elif </span><span class="s1">n &lt;= </span><span class="s4">1000</span><span class="s1">:</span>
            <span class="s1">method = </span><span class="s3">'ebrahimi'</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">method = </span><span class="s3">'vasicek'</span>

    <span class="s1">res = methods[method](sorted_data</span><span class="s2">, </span><span class="s1">window_length)</span>

    <span class="s2">if </span><span class="s1">base </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">res /= np.log(base)</span>

    <span class="s2">return </span><span class="s1">res</span>


<span class="s2">def </span><span class="s1">_pad_along_last_axis(X</span><span class="s2">, </span><span class="s1">m):</span>
    <span class="s0">&quot;&quot;&quot;Pad the data for computing the rolling window difference.&quot;&quot;&quot;</span>
    <span class="s5"># scales a  bit better than method in _vasicek_like_entropy</span>
    <span class="s1">shape = np.array(X.shape)</span>
    <span class="s1">shape[-</span><span class="s4">1</span><span class="s1">] = m</span>
    <span class="s1">Xl = np.broadcast_to(X[...</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">shape)  </span><span class="s5"># [0] vs 0 to maintain shape</span>
    <span class="s1">Xr = np.broadcast_to(X[...</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">shape)</span>
    <span class="s2">return </span><span class="s1">np.concatenate((Xl</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">Xr)</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">_vasicek_entropy(X</span><span class="s2">, </span><span class="s1">m):</span>
    <span class="s0">&quot;&quot;&quot;Compute the Vasicek estimator as described in [6] Eq. 1.3.&quot;&quot;&quot;</span>
    <span class="s1">n = X.shape[-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">X = _pad_along_last_axis(X</span><span class="s2">, </span><span class="s1">m)</span>
    <span class="s1">differences = X[...</span><span class="s2">, </span><span class="s4">2 </span><span class="s1">* m:] - X[...</span><span class="s2">, </span><span class="s1">: -</span><span class="s4">2 </span><span class="s1">* m:]</span>
    <span class="s1">logs = np.log(n/(</span><span class="s4">2</span><span class="s1">*m) * differences)</span>
    <span class="s2">return </span><span class="s1">np.mean(logs</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">_van_es_entropy(X</span><span class="s2">, </span><span class="s1">m):</span>
    <span class="s0">&quot;&quot;&quot;Compute the van Es estimator as described in [6].&quot;&quot;&quot;</span>
    <span class="s5"># No equation number, but referred to as HVE_mn.</span>
    <span class="s5"># Typo: there should be a log within the summation.</span>
    <span class="s1">n = X.shape[-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">difference = X[...</span><span class="s2">, </span><span class="s1">m:] - X[...</span><span class="s2">, </span><span class="s1">:-m]</span>
    <span class="s1">term1 = </span><span class="s4">1</span><span class="s1">/(n-m) * np.sum(np.log((n+</span><span class="s4">1</span><span class="s1">)/m * difference)</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">k = np.arange(m</span><span class="s2">, </span><span class="s1">n+</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">term1 + np.sum(</span><span class="s4">1</span><span class="s1">/k) + np.log(m) - np.log(n+</span><span class="s4">1</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">_ebrahimi_entropy(X</span><span class="s2">, </span><span class="s1">m):</span>
    <span class="s0">&quot;&quot;&quot;Compute the Ebrahimi estimator as described in [6].&quot;&quot;&quot;</span>
    <span class="s5"># No equation number, but referred to as HE_mn</span>
    <span class="s1">n = X.shape[-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">X = _pad_along_last_axis(X</span><span class="s2">, </span><span class="s1">m)</span>

    <span class="s1">differences = X[...</span><span class="s2">, </span><span class="s4">2 </span><span class="s1">* m:] - X[...</span><span class="s2">, </span><span class="s1">: -</span><span class="s4">2 </span><span class="s1">* m:]</span>

    <span class="s1">i = np.arange(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">n+</span><span class="s4">1</span><span class="s1">).astype(float)</span>
    <span class="s1">ci = np.ones_like(i)*</span><span class="s4">2</span>
    <span class="s1">ci[i &lt;= m] = </span><span class="s4">1 </span><span class="s1">+ (i[i &lt;= m] - </span><span class="s4">1</span><span class="s1">)/m</span>
    <span class="s1">ci[i &gt;= n - m + </span><span class="s4">1</span><span class="s1">] = </span><span class="s4">1 </span><span class="s1">+ (n - i[i &gt;= n-m+</span><span class="s4">1</span><span class="s1">])/m</span>

    <span class="s1">logs = np.log(n * differences / (ci * m))</span>
    <span class="s2">return </span><span class="s1">np.mean(logs</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">_correa_entropy(X</span><span class="s2">, </span><span class="s1">m):</span>
    <span class="s0">&quot;&quot;&quot;Compute the Correa estimator as described in [6].&quot;&quot;&quot;</span>
    <span class="s5"># No equation number, but referred to as HC_mn</span>
    <span class="s1">n = X.shape[-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">X = _pad_along_last_axis(X</span><span class="s2">, </span><span class="s1">m)</span>

    <span class="s1">i = np.arange(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">n+</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">dj = np.arange(-m</span><span class="s2">, </span><span class="s1">m+</span><span class="s4">1</span><span class="s1">)[:</span><span class="s2">, None</span><span class="s1">]</span>
    <span class="s1">j = i + dj</span>
    <span class="s1">j0 = j + m - </span><span class="s4">1  </span><span class="s5"># 0-indexed version of j</span>

    <span class="s1">Xibar = np.mean(X[...</span><span class="s2">, </span><span class="s1">j0]</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">2</span><span class="s2">, </span><span class="s1">keepdims=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">difference = X[...</span><span class="s2">, </span><span class="s1">j0] - Xibar</span>
    <span class="s1">num = np.sum(difference*dj</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">2</span><span class="s1">)  </span><span class="s5"># dj is d-i</span>
    <span class="s1">den = n*np.sum(difference**</span><span class="s4">2</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">-np.mean(np.log(num/den)</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
</pre>
</body>
</html>