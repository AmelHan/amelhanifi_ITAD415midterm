<html>
<head>
<title>test_search.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_search.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Test the search module&quot;&quot;&quot;</span>

<span class="s2">import </span><span class="s1">pickle</span>
<span class="s2">import </span><span class="s1">re</span>
<span class="s2">import </span><span class="s1">sys</span>
<span class="s2">from </span><span class="s1">collections.abc </span><span class="s2">import </span><span class="s1">Iterable</span><span class="s2">, </span><span class="s1">Sized</span>
<span class="s2">from </span><span class="s1">functools </span><span class="s2">import </span><span class="s1">partial</span>
<span class="s2">from </span><span class="s1">io </span><span class="s2">import </span><span class="s1">StringIO</span>
<span class="s2">from </span><span class="s1">itertools </span><span class="s2">import </span><span class="s1">chain</span><span class="s2">, </span><span class="s1">product</span>
<span class="s2">from </span><span class="s1">types </span><span class="s2">import </span><span class="s1">GeneratorType</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">import </span><span class="s1">scipy.sparse </span><span class="s2">as </span><span class="s1">sp</span>
<span class="s2">from </span><span class="s1">scipy.stats </span><span class="s2">import </span><span class="s1">bernoulli</span><span class="s2">, </span><span class="s1">expon</span><span class="s2">, </span><span class="s1">uniform</span>

<span class="s2">from </span><span class="s1">sklearn.base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">ClassifierMixin</span><span class="s2">, </span><span class="s1">is_classifier</span>
<span class="s2">from </span><span class="s1">sklearn.cluster </span><span class="s2">import </span><span class="s1">KMeans</span>
<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">make_blobs</span><span class="s2">,</span>
    <span class="s1">make_classification</span><span class="s2">,</span>
    <span class="s1">make_multilabel_classification</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.ensemble </span><span class="s2">import </span><span class="s1">HistGradientBoostingClassifier</span>
<span class="s2">from </span><span class="s1">sklearn.experimental </span><span class="s2">import </span><span class="s1">enable_halving_search_cv  </span><span class="s3"># noqa</span>
<span class="s2">from </span><span class="s1">sklearn.impute </span><span class="s2">import </span><span class="s1">SimpleImputer</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">LinearRegression</span><span class="s2">, </span><span class="s1">Ridge</span><span class="s2">, </span><span class="s1">SGDClassifier</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">accuracy_score</span><span class="s2">,</span>
    <span class="s1">confusion_matrix</span><span class="s2">,</span>
    <span class="s1">f1_score</span><span class="s2">,</span>
    <span class="s1">make_scorer</span><span class="s2">,</span>
    <span class="s1">r2_score</span><span class="s2">,</span>
    <span class="s1">recall_score</span><span class="s2">,</span>
    <span class="s1">roc_auc_score</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.metrics.pairwise </span><span class="s2">import </span><span class="s1">euclidean_distances</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">GridSearchCV</span><span class="s2">,</span>
    <span class="s1">GroupKFold</span><span class="s2">,</span>
    <span class="s1">GroupShuffleSplit</span><span class="s2">,</span>
    <span class="s1">HalvingGridSearchCV</span><span class="s2">,</span>
    <span class="s1">KFold</span><span class="s2">,</span>
    <span class="s1">LeaveOneGroupOut</span><span class="s2">,</span>
    <span class="s1">LeavePGroupsOut</span><span class="s2">,</span>
    <span class="s1">ParameterGrid</span><span class="s2">,</span>
    <span class="s1">ParameterSampler</span><span class="s2">,</span>
    <span class="s1">RandomizedSearchCV</span><span class="s2">,</span>
    <span class="s1">StratifiedKFold</span><span class="s2">,</span>
    <span class="s1">StratifiedShuffleSplit</span><span class="s2">,</span>
    <span class="s1">train_test_split</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection._search </span><span class="s2">import </span><span class="s1">BaseSearchCV</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection._validation </span><span class="s2">import </span><span class="s1">FitFailedWarning</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection.tests.common </span><span class="s2">import </span><span class="s1">OneTimeSplitter</span>
<span class="s2">from </span><span class="s1">sklearn.neighbors </span><span class="s2">import </span><span class="s1">KernelDensity</span><span class="s2">, </span><span class="s1">KNeighborsClassifier</span><span class="s2">, </span><span class="s1">LocalOutlierFactor</span>
<span class="s2">from </span><span class="s1">sklearn.pipeline </span><span class="s2">import </span><span class="s1">Pipeline</span>
<span class="s2">from </span><span class="s1">sklearn.svm </span><span class="s2">import </span><span class="s1">SVC</span><span class="s2">, </span><span class="s1">LinearSVC</span>
<span class="s2">from </span><span class="s1">sklearn.tree </span><span class="s2">import </span><span class="s1">DecisionTreeClassifier</span><span class="s2">, </span><span class="s1">DecisionTreeRegressor</span>
<span class="s2">from </span><span class="s1">sklearn.utils._mocking </span><span class="s2">import </span><span class="s1">CheckingClassifier</span><span class="s2">, </span><span class="s1">MockDataFrame</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">MinimalClassifier</span><span class="s2">,</span>
    <span class="s1">MinimalRegressor</span><span class="s2">,</span>
    <span class="s1">MinimalTransformer</span><span class="s2">,</span>
    <span class="s1">assert_allclose</span><span class="s2">,</span>
    <span class="s1">assert_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_equal</span><span class="s2">,</span>
    <span class="s1">ignore_warnings</span><span class="s2">,</span>
<span class="s1">)</span>


<span class="s3"># Neither of the following two estimators inherit from BaseEstimator,</span>
<span class="s3"># to test hyperparameter search on user-defined classifiers.</span>
<span class="s2">class </span><span class="s1">MockClassifier:</span>
    <span class="s0">&quot;&quot;&quot;Dummy classifier to test the parameter search algorithms&quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">foo_param=</span><span class="s4">0</span><span class="s1">):</span>
        <span class="s1">self.foo_param = foo_param</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">Y):</span>
        <span class="s2">assert </span><span class="s1">len(X) == len(Y)</span>
        <span class="s1">self.classes_ = np.unique(Y)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">T):</span>
        <span class="s2">return </span><span class="s1">T.shape[</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s2">def </span><span class="s1">transform(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s2">return </span><span class="s1">X + self.foo_param</span>

    <span class="s2">def </span><span class="s1">inverse_transform(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s2">return </span><span class="s1">X - self.foo_param</span>

    <span class="s1">predict_proba = predict</span>
    <span class="s1">predict_log_proba = predict</span>
    <span class="s1">decision_function = predict</span>

    <span class="s2">def </span><span class="s1">score(self</span><span class="s2">, </span><span class="s1">X=</span><span class="s2">None, </span><span class="s1">Y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">if </span><span class="s1">self.foo_param &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">score = </span><span class="s4">1.0</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">score = </span><span class="s4">0.0</span>
        <span class="s2">return </span><span class="s1">score</span>

    <span class="s2">def </span><span class="s1">get_params(self</span><span class="s2">, </span><span class="s1">deep=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">: self.foo_param}</span>

    <span class="s2">def </span><span class="s1">set_params(self</span><span class="s2">, </span><span class="s1">**params):</span>
        <span class="s1">self.foo_param = params[</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">]</span>
        <span class="s2">return </span><span class="s1">self</span>


<span class="s2">class </span><span class="s1">LinearSVCNoScore(LinearSVC):</span>
    <span class="s0">&quot;&quot;&quot;A LinearSVC classifier that has no score method.&quot;&quot;&quot;</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">score(self):</span>
        <span class="s2">raise </span><span class="s1">AttributeError</span>


<span class="s1">X = np.array([[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]])</span>
<span class="s1">y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">assert_grid_iter_equals_getitem(grid):</span>
    <span class="s2">assert </span><span class="s1">list(grid) == [grid[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(len(grid))]</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;klass&quot;</span><span class="s2">, </span><span class="s1">[ParameterGrid</span><span class="s2">, </span><span class="s1">partial(ParameterSampler</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">10</span><span class="s1">)])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;input, error_type, error_message&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">TypeError</span><span class="s2">, </span><span class="s5">r&quot;Parameter .* a dict or a list, got: 0 of type int&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">([{</span><span class="s5">&quot;foo&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s1">]}</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">TypeError</span><span class="s2">, </span><span class="s5">r&quot;Parameter .* is not a dict \(0\)&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s5">&quot;foo&quot;</span><span class="s1">: </span><span class="s4">0</span><span class="s1">}</span><span class="s2">,</span>
            <span class="s1">TypeError</span><span class="s2">,</span>
            <span class="s5">r&quot;Parameter (grid|distribution) for parameter 'foo' (is not|needs to be) &quot;</span>
            <span class="s5">r&quot;(a list or a numpy array|iterable or a distribution).*&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_validate_parameter_input(klass</span><span class="s2">, </span><span class="s1">input</span><span class="s2">, </span><span class="s1">error_type</span><span class="s2">, </span><span class="s1">error_message):</span>
    <span class="s2">with </span><span class="s1">pytest.raises(error_type</span><span class="s2">, </span><span class="s1">match=error_message):</span>
        <span class="s1">klass(input)</span>


<span class="s2">def </span><span class="s1">test_parameter_grid():</span>
    <span class="s3"># Test basic properties of ParameterGrid.</span>
    <span class="s1">params1 = {</span><span class="s5">&quot;foo&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]}</span>
    <span class="s1">grid1 = ParameterGrid(params1)</span>
    <span class="s2">assert </span><span class="s1">isinstance(grid1</span><span class="s2">, </span><span class="s1">Iterable)</span>
    <span class="s2">assert </span><span class="s1">isinstance(grid1</span><span class="s2">, </span><span class="s1">Sized)</span>
    <span class="s2">assert </span><span class="s1">len(grid1) == </span><span class="s4">3</span>
    <span class="s1">assert_grid_iter_equals_getitem(grid1)</span>

    <span class="s1">params2 = {</span><span class="s5">&quot;foo&quot;</span><span class="s1">: [</span><span class="s4">4</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;bar&quot;</span><span class="s1">: [</span><span class="s5">&quot;ham&quot;</span><span class="s2">, </span><span class="s5">&quot;spam&quot;</span><span class="s2">, </span><span class="s5">&quot;eggs&quot;</span><span class="s1">]}</span>
    <span class="s1">grid2 = ParameterGrid(params2)</span>
    <span class="s2">assert </span><span class="s1">len(grid2) == </span><span class="s4">6</span>

    <span class="s3"># loop to assert we can iterate over the grid multiple times</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">2</span><span class="s1">):</span>
        <span class="s3"># tuple + chain transforms {&quot;a&quot;: 1, &quot;b&quot;: 2} to (&quot;a&quot;, 1, &quot;b&quot;, 2)</span>
        <span class="s1">points = set(tuple(chain(*(sorted(p.items())))) </span><span class="s2">for </span><span class="s1">p </span><span class="s2">in </span><span class="s1">grid2)</span>
        <span class="s2">assert </span><span class="s1">points == set(</span>
            <span class="s1">(</span><span class="s5">&quot;bar&quot;</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s5">&quot;foo&quot;</span><span class="s2">, </span><span class="s1">y) </span><span class="s2">for </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y </span><span class="s2">in </span><span class="s1">product(params2[</span><span class="s5">&quot;bar&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">params2[</span><span class="s5">&quot;foo&quot;</span><span class="s1">])</span>
        <span class="s1">)</span>
    <span class="s1">assert_grid_iter_equals_getitem(grid2)</span>

    <span class="s3"># Special case: empty grid (useful to get default estimator settings)</span>
    <span class="s1">empty = ParameterGrid({})</span>
    <span class="s2">assert </span><span class="s1">len(empty) == </span><span class="s4">1</span>
    <span class="s2">assert </span><span class="s1">list(empty) == [{}]</span>
    <span class="s1">assert_grid_iter_equals_getitem(empty)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(IndexError):</span>
        <span class="s1">empty[</span><span class="s4">1</span><span class="s1">]</span>

    <span class="s1">has_empty = ParameterGrid([{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">{}</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.5</span><span class="s1">]}])</span>
    <span class="s2">assert </span><span class="s1">len(has_empty) == </span><span class="s4">4</span>
    <span class="s2">assert </span><span class="s1">list(has_empty) == [{</span><span class="s5">&quot;C&quot;</span><span class="s1">: </span><span class="s4">1</span><span class="s1">}</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: </span><span class="s4">10</span><span class="s1">}</span><span class="s2">, </span><span class="s1">{}</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: </span><span class="s4">0.5</span><span class="s1">}]</span>
    <span class="s1">assert_grid_iter_equals_getitem(has_empty)</span>


<span class="s2">def </span><span class="s1">test_grid_search():</span>
    <span class="s3"># Test that the best estimator contains the right value for foo_param</span>
    <span class="s1">clf = MockClassifier()</span>
    <span class="s1">grid_search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">verbose=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s3"># make sure it selects the smallest parameter in case of ties</span>
    <span class="s1">old_stdout = sys.stdout</span>
    <span class="s1">sys.stdout = StringIO()</span>
    <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">sys.stdout = old_stdout</span>
    <span class="s2">assert </span><span class="s1">grid_search.best_estimator_.foo_param == </span><span class="s4">2</span>

    <span class="s1">assert_array_equal(grid_search.cv_results_[</span><span class="s5">&quot;param_foo_param&quot;</span><span class="s1">].data</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span>

    <span class="s3"># Smoke test the score etc:</span>
    <span class="s1">grid_search.score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">grid_search.predict_proba(X)</span>
    <span class="s1">grid_search.decision_function(X)</span>
    <span class="s1">grid_search.transform(X)</span>

    <span class="s3"># Test exception handling on scoring</span>
    <span class="s1">grid_search.scoring = </span><span class="s5">&quot;sklearn&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_grid_search_pipeline_steps():</span>
    <span class="s3"># check that parameters that are estimators are cloned before fitting</span>
    <span class="s1">pipe = Pipeline([(</span><span class="s5">&quot;regressor&quot;</span><span class="s2">, </span><span class="s1">LinearRegression())])</span>
    <span class="s1">param_grid = {</span><span class="s5">&quot;regressor&quot;</span><span class="s1">: [LinearRegression()</span><span class="s2">, </span><span class="s1">Ridge()]}</span>
    <span class="s1">grid_search = GridSearchCV(pipe</span><span class="s2">, </span><span class="s1">param_grid</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">regressor_results = grid_search.cv_results_[</span><span class="s5">&quot;param_regressor&quot;</span><span class="s1">]</span>
    <span class="s2">assert </span><span class="s1">isinstance(regressor_results[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">LinearRegression)</span>
    <span class="s2">assert </span><span class="s1">isinstance(regressor_results[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Ridge)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(regressor_results[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;coef_&quot;</span><span class="s1">)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(regressor_results[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;coef_&quot;</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">regressor_results[</span><span class="s4">0</span><span class="s1">] </span><span class="s2">is not </span><span class="s1">grid_search.best_estimator_</span>
    <span class="s2">assert </span><span class="s1">regressor_results[</span><span class="s4">1</span><span class="s1">] </span><span class="s2">is not </span><span class="s1">grid_search.best_estimator_</span>
    <span class="s3"># check that we didn't modify the parameter grid that was passed</span>
    <span class="s2">assert not </span><span class="s1">hasattr(param_grid[</span><span class="s5">&quot;regressor&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;coef_&quot;</span><span class="s1">)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(param_grid[</span><span class="s5">&quot;regressor&quot;</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;coef_&quot;</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;SearchCV&quot;</span><span class="s2">, </span><span class="s1">[GridSearchCV</span><span class="s2">, </span><span class="s1">RandomizedSearchCV])</span>
<span class="s2">def </span><span class="s1">test_SearchCV_with_fit_params(SearchCV):</span>
    <span class="s1">X = np.arange(</span><span class="s4">100</span><span class="s1">).reshape(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">5 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">clf = CheckingClassifier(expected_fit_params=[</span><span class="s5">&quot;spam&quot;</span><span class="s2">, </span><span class="s5">&quot;eggs&quot;</span><span class="s1">])</span>
    <span class="s1">searcher = SearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">error_score=</span><span class="s5">&quot;raise&quot;</span><span class="s1">)</span>

    <span class="s3"># The CheckingClassifier generates an assertion error if</span>
    <span class="s3"># a parameter is missing or has length != len(X).</span>
    <span class="s1">err_msg = </span><span class="s5">r&quot;Expected fit parameter\(s\) \['eggs'\] not seen.&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">searcher.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">spam=np.ones(</span><span class="s4">10</span><span class="s1">))</span>

    <span class="s1">err_msg = </span><span class="s5">&quot;Fit parameter spam has length 1; expected&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(AssertionError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">searcher.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">spam=np.ones(</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">eggs=np.zeros(</span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">searcher.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">spam=np.ones(</span><span class="s4">10</span><span class="s1">)</span><span class="s2">, </span><span class="s1">eggs=np.zeros(</span><span class="s4">10</span><span class="s1">))</span>


<span class="s1">@ignore_warnings</span>
<span class="s2">def </span><span class="s1">test_grid_search_no_score():</span>
    <span class="s3"># Test grid-search on classifier that has no score function.</span>
    <span class="s1">clf = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">centers=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">Cs = [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span>
    <span class="s1">clf_no_score = LinearSVCNoScore(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">grid_search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: Cs}</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s5">&quot;accuracy&quot;</span><span class="s1">)</span>
    <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">grid_search_no_score = GridSearchCV(clf_no_score</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: Cs}</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s5">&quot;accuracy&quot;</span><span class="s1">)</span>
    <span class="s3"># smoketest grid search</span>
    <span class="s1">grid_search_no_score.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s3"># check that best params are equal</span>
    <span class="s2">assert </span><span class="s1">grid_search_no_score.best_params_ == grid_search.best_params_</span>
    <span class="s3"># check that we can call score and that it gives the correct result</span>
    <span class="s2">assert </span><span class="s1">grid_search.score(X</span><span class="s2">, </span><span class="s1">y) == grid_search_no_score.score(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s3"># giving no scoring function raises an error</span>
    <span class="s1">grid_search_no_score = GridSearchCV(clf_no_score</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: Cs})</span>
    <span class="s2">with </span><span class="s1">pytest.raises(TypeError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;no scoring&quot;</span><span class="s1">):</span>
        <span class="s1">grid_search_no_score.fit([[</span><span class="s4">1</span><span class="s1">]])</span>


<span class="s2">def </span><span class="s1">test_grid_search_score_method():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">n_classes=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">flip_y=</span><span class="s4">0.2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">grid = {</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s1">]}</span>

    <span class="s1">search_no_scoring = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">grid</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s2">None</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">search_accuracy = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">grid</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s5">&quot;accuracy&quot;</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">search_no_score_method_auc = GridSearchCV(</span>
        <span class="s1">LinearSVCNoScore(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">grid</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s5">&quot;roc_auc&quot;</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">search_auc = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">grid</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s5">&quot;roc_auc&quot;</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s3"># Check warning only occurs in situation where behavior changed:</span>
    <span class="s3"># estimator requires score method to compete with scoring parameter</span>
    <span class="s1">score_no_scoring = search_no_scoring.score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">score_accuracy = search_accuracy.score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">score_no_score_auc = search_no_score_method_auc.score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">score_auc = search_auc.score(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s3"># ensure the test is sane</span>
    <span class="s2">assert </span><span class="s1">score_auc &lt; </span><span class="s4">1.0</span>
    <span class="s2">assert </span><span class="s1">score_accuracy &lt; </span><span class="s4">1.0</span>
    <span class="s2">assert </span><span class="s1">score_auc != score_accuracy</span>

    <span class="s1">assert_almost_equal(score_accuracy</span><span class="s2">, </span><span class="s1">score_no_scoring)</span>
    <span class="s1">assert_almost_equal(score_auc</span><span class="s2">, </span><span class="s1">score_no_score_auc)</span>


<span class="s2">def </span><span class="s1">test_grid_search_groups():</span>
    <span class="s3"># Check if ValueError (when groups is None) propagates to GridSearchCV</span>
    <span class="s3"># And also check if groups is correctly passed to the cv object</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">15</span><span class="s2">, </span><span class="s1">n_classes=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">groups = rng.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">15</span><span class="s1">)</span>

    <span class="s1">clf = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">grid = {</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s1">]}</span>

    <span class="s1">group_cvs = [</span>
        <span class="s1">LeaveOneGroupOut()</span><span class="s2">,</span>
        <span class="s1">LeavePGroupsOut(</span><span class="s4">2</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">GroupKFold(n_splits=</span><span class="s4">3</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">GroupShuffleSplit()</span><span class="s2">,</span>
    <span class="s1">]</span>
    <span class="s1">error_msg = </span><span class="s5">&quot;The 'groups' parameter should not be None.&quot;</span>
    <span class="s2">for </span><span class="s1">cv </span><span class="s2">in </span><span class="s1">group_cvs:</span>
        <span class="s1">gs = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">grid</span><span class="s2">, </span><span class="s1">cv=cv)</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=error_msg):</span>
            <span class="s1">gs.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">gs.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">groups=groups)</span>

    <span class="s1">non_group_cvs = [StratifiedKFold()</span><span class="s2">, </span><span class="s1">StratifiedShuffleSplit()]</span>
    <span class="s2">for </span><span class="s1">cv </span><span class="s2">in </span><span class="s1">non_group_cvs:</span>
        <span class="s1">gs = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">grid</span><span class="s2">, </span><span class="s1">cv=cv)</span>
        <span class="s3"># Should not raise an error</span>
        <span class="s1">gs.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_classes__property():</span>
    <span class="s3"># Test that classes_ property matches best_estimator_.classes_</span>
    <span class="s1">X = np.arange(</span><span class="s4">100</span><span class="s1">).reshape(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">5 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">Cs = [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span>

    <span class="s1">grid_search = GridSearchCV(LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: Cs})</span>
    <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(grid_search.best_estimator_.classes_</span><span class="s2">, </span><span class="s1">grid_search.classes_)</span>

    <span class="s3"># Test that regressors do not have a classes_ attribute</span>
    <span class="s1">grid_search = GridSearchCV(Ridge()</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: [</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">2.0</span><span class="s1">]})</span>
    <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(grid_search</span><span class="s2">, </span><span class="s5">&quot;classes_&quot;</span><span class="s1">)</span>

    <span class="s3"># Test that the grid searcher has no classes_ attribute before it's fit</span>
    <span class="s1">grid_search = GridSearchCV(LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: Cs})</span>
    <span class="s2">assert not </span><span class="s1">hasattr(grid_search</span><span class="s2">, </span><span class="s5">&quot;classes_&quot;</span><span class="s1">)</span>

    <span class="s3"># Test that the grid searcher has no classes_ attribute without a refit</span>
    <span class="s1">grid_search = GridSearchCV(</span>
        <span class="s1">LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: Cs}</span><span class="s2">, </span><span class="s1">refit=</span><span class="s2">False</span>
    <span class="s1">)</span>
    <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(grid_search</span><span class="s2">, </span><span class="s5">&quot;classes_&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_trivial_cv_results_attr():</span>
    <span class="s3"># Test search over a &quot;grid&quot; with only one point.</span>
    <span class="s1">clf = MockClassifier()</span>
    <span class="s1">grid_search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">hasattr(grid_search</span><span class="s2">, </span><span class="s5">&quot;cv_results_&quot;</span><span class="s1">)</span>

    <span class="s1">random_search = RandomizedSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">random_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">hasattr(grid_search</span><span class="s2">, </span><span class="s5">&quot;cv_results_&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_no_refit():</span>
    <span class="s3"># Test that GSCV can be used for model selection alone without refitting</span>
    <span class="s1">clf = MockClassifier()</span>
    <span class="s2">for </span><span class="s1">scoring </span><span class="s2">in </span><span class="s1">[</span><span class="s2">None, </span><span class="s1">[</span><span class="s5">&quot;accuracy&quot;</span><span class="s2">, </span><span class="s5">&quot;precision&quot;</span><span class="s1">]]:</span>
        <span class="s1">grid_search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">refit=</span><span class="s2">False, </span><span class="s1">cv=</span><span class="s4">3</span><span class="s1">)</span>
        <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">(</span>
            <span class="s2">not </span><span class="s1">hasattr(grid_search</span><span class="s2">, </span><span class="s5">&quot;best_estimator_&quot;</span><span class="s1">)</span>
            <span class="s2">and </span><span class="s1">hasattr(grid_search</span><span class="s2">, </span><span class="s5">&quot;best_index_&quot;</span><span class="s1">)</span>
            <span class="s2">and </span><span class="s1">hasattr(grid_search</span><span class="s2">, </span><span class="s5">&quot;best_params_&quot;</span><span class="s1">)</span>
        <span class="s1">)</span>

        <span class="s3"># Make sure the functions predict/transform etc. raise meaningful</span>
        <span class="s3"># error messages</span>
        <span class="s2">for </span><span class="s1">fn_name </span><span class="s2">in </span><span class="s1">(</span>
            <span class="s5">&quot;predict&quot;</span><span class="s2">,</span>
            <span class="s5">&quot;predict_proba&quot;</span><span class="s2">,</span>
            <span class="s5">&quot;predict_log_proba&quot;</span><span class="s2">,</span>
            <span class="s5">&quot;transform&quot;</span><span class="s2">,</span>
            <span class="s5">&quot;inverse_transform&quot;</span><span class="s2">,</span>
        <span class="s1">):</span>
            <span class="s1">error_msg = (</span>
                <span class="s5">f&quot;`refit=False`. </span><span class="s2">{</span><span class="s1">fn_name</span><span class="s2">} </span><span class="s5">is available only after &quot;</span>
                <span class="s5">&quot;refitting on the best parameters&quot;</span>
            <span class="s1">)</span>
            <span class="s2">with </span><span class="s1">pytest.raises(AttributeError</span><span class="s2">, </span><span class="s1">match=error_msg):</span>
                <span class="s1">getattr(grid_search</span><span class="s2">, </span><span class="s1">fn_name)(X)</span>

    <span class="s3"># Test that an invalid refit param raises appropriate error messages</span>
    <span class="s1">error_msg = (</span>
        <span class="s5">&quot;For multi-metric scoring, the parameter refit must be set to a scorer key&quot;</span>
    <span class="s1">)</span>
    <span class="s2">for </span><span class="s1">refit </span><span class="s2">in </span><span class="s1">[</span><span class="s2">True, </span><span class="s5">&quot;recall&quot;</span><span class="s2">, </span><span class="s5">&quot;accuracy&quot;</span><span class="s1">]:</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=error_msg):</span>
            <span class="s1">GridSearchCV(</span>
                <span class="s1">clf</span><span class="s2">, </span><span class="s1">{}</span><span class="s2">, </span><span class="s1">refit=refit</span><span class="s2">, </span><span class="s1">scoring={</span><span class="s5">&quot;acc&quot;</span><span class="s1">: </span><span class="s5">&quot;accuracy&quot;</span><span class="s2">, </span><span class="s5">&quot;prec&quot;</span><span class="s1">: </span><span class="s5">&quot;precision&quot;</span><span class="s1">}</span>
            <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_grid_search_error():</span>
    <span class="s3"># Test that grid search will capture errors on data with different length</span>
    <span class="s1">X_</span><span class="s2">, </span><span class="s1">y_ = make_classification(n_samples=</span><span class="s4">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">clf = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s1">)</span>
    <span class="s1">cv = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]})</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">cv.fit(X_[:</span><span class="s4">180</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y_)</span>


<span class="s2">def </span><span class="s1">test_grid_search_one_grid_point():</span>
    <span class="s1">X_</span><span class="s2">, </span><span class="s1">y_ = make_classification(n_samples=</span><span class="s4">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">param_dict = {</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">1.0</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;kernel&quot;</span><span class="s1">: [</span><span class="s5">&quot;rbf&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;gamma&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s1">]}</span>

    <span class="s1">clf = SVC(gamma=</span><span class="s5">&quot;auto&quot;</span><span class="s1">)</span>
    <span class="s1">cv = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">param_dict)</span>
    <span class="s1">cv.fit(X_</span><span class="s2">, </span><span class="s1">y_)</span>

    <span class="s1">clf = SVC(C=</span><span class="s4">1.0</span><span class="s2">, </span><span class="s1">kernel=</span><span class="s5">&quot;rbf&quot;</span><span class="s2">, </span><span class="s1">gamma=</span><span class="s4">0.1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_</span><span class="s2">, </span><span class="s1">y_)</span>

    <span class="s1">assert_array_equal(clf.dual_coef_</span><span class="s2">, </span><span class="s1">cv.best_estimator_.dual_coef_)</span>


<span class="s2">def </span><span class="s1">test_grid_search_when_param_grid_includes_range():</span>
    <span class="s3"># Test that the best estimator contains the right value for foo_param</span>
    <span class="s1">clf = MockClassifier()</span>
    <span class="s1">grid_search = </span><span class="s2">None</span>
    <span class="s1">grid_search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">: range(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">4</span><span class="s1">)}</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">grid_search.best_estimator_.foo_param == </span><span class="s4">2</span>


<span class="s2">def </span><span class="s1">test_grid_search_bad_param_grid():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">param_dict = {</span><span class="s5">&quot;C&quot;</span><span class="s1">: </span><span class="s4">1</span><span class="s1">}</span>
    <span class="s1">clf = SVC(gamma=</span><span class="s5">&quot;auto&quot;</span><span class="s1">)</span>
    <span class="s1">error_msg = re.escape(</span>
        <span class="s5">&quot;Parameter grid for parameter 'C' needs to be a list or &quot;</span>
        <span class="s5">&quot;a numpy array, but got 1 (of type int) instead. Single &quot;</span>
        <span class="s5">&quot;values need to be wrapped in a list with one element.&quot;</span>
    <span class="s1">)</span>
    <span class="s1">search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">param_dict)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(TypeError</span><span class="s2">, </span><span class="s1">match=error_msg):</span>
        <span class="s1">search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">param_dict = {</span><span class="s5">&quot;C&quot;</span><span class="s1">: []}</span>
    <span class="s1">clf = SVC()</span>
    <span class="s1">error_msg = re.escape(</span>
        <span class="s5">&quot;Parameter grid for parameter 'C' need to be a non-empty sequence, got: []&quot;</span>
    <span class="s1">)</span>
    <span class="s1">search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">param_dict)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=error_msg):</span>
        <span class="s1">search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">param_dict = {</span><span class="s5">&quot;C&quot;</span><span class="s1">: </span><span class="s5">&quot;1,2,3&quot;</span><span class="s1">}</span>
    <span class="s1">clf = SVC(gamma=</span><span class="s5">&quot;auto&quot;</span><span class="s1">)</span>
    <span class="s1">error_msg = re.escape(</span>
        <span class="s5">&quot;Parameter grid for parameter 'C' needs to be a list or a numpy array, &quot;</span>
        <span class="s5">&quot;but got '1,2,3' (of type str) instead. Single values need to be &quot;</span>
        <span class="s5">&quot;wrapped in a list with one element.&quot;</span>
    <span class="s1">)</span>
    <span class="s1">search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">param_dict)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(TypeError</span><span class="s2">, </span><span class="s1">match=error_msg):</span>
        <span class="s1">search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">param_dict = {</span><span class="s5">&quot;C&quot;</span><span class="s1">: np.ones((</span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))}</span>
    <span class="s1">clf = SVC()</span>
    <span class="s1">search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">param_dict)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_grid_search_sparse():</span>
    <span class="s3"># Test that grid search works with both dense and sparse matrices</span>
    <span class="s1">X_</span><span class="s2">, </span><span class="s1">y_ = make_classification(n_samples=</span><span class="s4">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">clf = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s1">)</span>
    <span class="s1">cv = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]})</span>
    <span class="s1">cv.fit(X_[:</span><span class="s4">180</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y_[:</span><span class="s4">180</span><span class="s1">])</span>
    <span class="s1">y_pred = cv.predict(X_[</span><span class="s4">180</span><span class="s1">:])</span>
    <span class="s1">C = cv.best_estimator_.C</span>

    <span class="s1">X_ = sp.csr_matrix(X_)</span>
    <span class="s1">clf = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s1">)</span>
    <span class="s1">cv = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]})</span>
    <span class="s1">cv.fit(X_[:</span><span class="s4">180</span><span class="s1">].tocoo()</span><span class="s2">, </span><span class="s1">y_[:</span><span class="s4">180</span><span class="s1">])</span>
    <span class="s1">y_pred2 = cv.predict(X_[</span><span class="s4">180</span><span class="s1">:])</span>
    <span class="s1">C2 = cv.best_estimator_.C</span>

    <span class="s2">assert </span><span class="s1">np.mean(y_pred == y_pred2) &gt;= </span><span class="s4">0.9</span>
    <span class="s2">assert </span><span class="s1">C == C2</span>


<span class="s2">def </span><span class="s1">test_grid_search_sparse_scoring():</span>
    <span class="s1">X_</span><span class="s2">, </span><span class="s1">y_ = make_classification(n_samples=</span><span class="s4">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">clf = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s1">)</span>
    <span class="s1">cv = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s5">&quot;f1&quot;</span><span class="s1">)</span>
    <span class="s1">cv.fit(X_[:</span><span class="s4">180</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y_[:</span><span class="s4">180</span><span class="s1">])</span>
    <span class="s1">y_pred = cv.predict(X_[</span><span class="s4">180</span><span class="s1">:])</span>
    <span class="s1">C = cv.best_estimator_.C</span>

    <span class="s1">X_ = sp.csr_matrix(X_)</span>
    <span class="s1">clf = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s1">)</span>
    <span class="s1">cv = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s5">&quot;f1&quot;</span><span class="s1">)</span>
    <span class="s1">cv.fit(X_[:</span><span class="s4">180</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y_[:</span><span class="s4">180</span><span class="s1">])</span>
    <span class="s1">y_pred2 = cv.predict(X_[</span><span class="s4">180</span><span class="s1">:])</span>
    <span class="s1">C2 = cv.best_estimator_.C</span>

    <span class="s1">assert_array_equal(y_pred</span><span class="s2">, </span><span class="s1">y_pred2)</span>
    <span class="s2">assert </span><span class="s1">C == C2</span>
    <span class="s3"># Smoke test the score</span>
    <span class="s3"># np.testing.assert_allclose(f1_score(cv.predict(X_[:180]), y[:180]),</span>
    <span class="s3">#                            cv.score(X_[:180], y[:180]))</span>

    <span class="s3"># test loss where greater is worse</span>
    <span class="s2">def </span><span class="s1">f1_loss(y_true_</span><span class="s2">, </span><span class="s1">y_pred_):</span>
        <span class="s2">return </span><span class="s1">-f1_score(y_true_</span><span class="s2">, </span><span class="s1">y_pred_)</span>

    <span class="s1">F1Loss = make_scorer(f1_loss</span><span class="s2">, </span><span class="s1">greater_is_better=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">cv = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">scoring=F1Loss)</span>
    <span class="s1">cv.fit(X_[:</span><span class="s4">180</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y_[:</span><span class="s4">180</span><span class="s1">])</span>
    <span class="s1">y_pred3 = cv.predict(X_[</span><span class="s4">180</span><span class="s1">:])</span>
    <span class="s1">C3 = cv.best_estimator_.C</span>

    <span class="s2">assert </span><span class="s1">C == C3</span>
    <span class="s1">assert_array_equal(y_pred</span><span class="s2">, </span><span class="s1">y_pred3)</span>


<span class="s2">def </span><span class="s1">test_grid_search_precomputed_kernel():</span>
    <span class="s3"># Test that grid search works when the input features are given in the</span>
    <span class="s3"># form of a precomputed kernel matrix</span>
    <span class="s1">X_</span><span class="s2">, </span><span class="s1">y_ = make_classification(n_samples=</span><span class="s4">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s3"># compute the training kernel matrix corresponding to the linear kernel</span>
    <span class="s1">K_train = np.dot(X_[:</span><span class="s4">180</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X_[:</span><span class="s4">180</span><span class="s1">].T)</span>
    <span class="s1">y_train = y_[:</span><span class="s4">180</span><span class="s1">]</span>

    <span class="s1">clf = SVC(kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s1">)</span>
    <span class="s1">cv = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]})</span>
    <span class="s1">cv.fit(K_train</span><span class="s2">, </span><span class="s1">y_train)</span>

    <span class="s2">assert </span><span class="s1">cv.best_score_ &gt;= </span><span class="s4">0</span>

    <span class="s3"># compute the test kernel matrix</span>
    <span class="s1">K_test = np.dot(X_[</span><span class="s4">180</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">X_[:</span><span class="s4">180</span><span class="s1">].T)</span>
    <span class="s1">y_test = y_[</span><span class="s4">180</span><span class="s1">:]</span>

    <span class="s1">y_pred = cv.predict(K_test)</span>

    <span class="s2">assert </span><span class="s1">np.mean(y_pred == y_test) &gt;= </span><span class="s4">0</span>

    <span class="s3"># test error is raised when the precomputed kernel is not array-like</span>
    <span class="s3"># or sparse</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">cv.fit(K_train.tolist()</span><span class="s2">, </span><span class="s1">y_train)</span>


<span class="s2">def </span><span class="s1">test_grid_search_precomputed_kernel_error_nonsquare():</span>
    <span class="s3"># Test that grid search returns an error with a non-square precomputed</span>
    <span class="s3"># training kernel matrix</span>
    <span class="s1">K_train = np.zeros((</span><span class="s4">10</span><span class="s2">, </span><span class="s4">20</span><span class="s1">))</span>
    <span class="s1">y_train = np.ones((</span><span class="s4">10</span><span class="s2">,</span><span class="s1">))</span>
    <span class="s1">clf = SVC(kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s1">)</span>
    <span class="s1">cv = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]})</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">cv.fit(K_train</span><span class="s2">, </span><span class="s1">y_train)</span>


<span class="s2">class </span><span class="s1">BrokenClassifier(BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Broken classifier that cannot be fit twice&quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">parameter=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self.parameter = parameter</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">assert not </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s5">&quot;has_been_fit_&quot;</span><span class="s1">)</span>
        <span class="s1">self.has_been_fit_ = </span><span class="s2">True</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s2">return </span><span class="s1">np.zeros(X.shape[</span><span class="s4">0</span><span class="s1">])</span>


<span class="s1">@ignore_warnings</span>
<span class="s2">def </span><span class="s1">test_refit():</span>
    <span class="s3"># Regression test for bug in refitting</span>
    <span class="s3"># Simulates re-fitting a broken estimator; this used to break with</span>
    <span class="s3"># sparse SVMs.</span>
    <span class="s1">X = np.arange(</span><span class="s4">100</span><span class="s1">).reshape(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">5 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">5</span><span class="s1">)</span>

    <span class="s1">clf = GridSearchCV(</span>
        <span class="s1">BrokenClassifier()</span><span class="s2">, </span><span class="s1">[{</span><span class="s5">&quot;parameter&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}]</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s5">&quot;precision&quot;</span><span class="s2">, </span><span class="s1">refit=</span><span class="s2">True</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_refit_callable():</span>
    <span class="s0">&quot;&quot;&quot; 
    Test refit=callable, which adds flexibility in identifying the 
    &quot;best&quot; estimator. 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">refit_callable(cv_results):</span>
        <span class="s0">&quot;&quot;&quot; 
        A dummy function tests `refit=callable` interface. 
        Return the index of a model that has the least 
        `mean_test_score`. 
        &quot;&quot;&quot;</span>
        <span class="s3"># Fit a dummy clf with `refit=True` to get a list of keys in</span>
        <span class="s3"># clf.cv_results_.</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
        <span class="s1">clf = GridSearchCV(</span>
            <span class="s1">LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.01</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span><span class="s2">,</span>
            <span class="s1">scoring=</span><span class="s5">&quot;precision&quot;</span><span class="s2">,</span>
            <span class="s1">refit=</span><span class="s2">True,</span>
        <span class="s1">)</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s3"># Ensure that `best_index_ != 0` for this dummy clf</span>
        <span class="s2">assert </span><span class="s1">clf.best_index_ != </span><span class="s4">0</span>

        <span class="s3"># Assert every key matches those in `cv_results`</span>
        <span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">clf.cv_results_.keys():</span>
            <span class="s2">assert </span><span class="s1">key </span><span class="s2">in </span><span class="s1">cv_results</span>

        <span class="s2">return </span><span class="s1">cv_results[</span><span class="s5">&quot;mean_test_score&quot;</span><span class="s1">].argmin()</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">clf = GridSearchCV(</span>
        <span class="s1">LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.01</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span><span class="s2">,</span>
        <span class="s1">scoring=</span><span class="s5">&quot;precision&quot;</span><span class="s2">,</span>
        <span class="s1">refit=refit_callable</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">assert </span><span class="s1">clf.best_index_ == </span><span class="s4">0</span>
    <span class="s3"># Ensure `best_score_` is disabled when using `refit=callable`</span>
    <span class="s2">assert not </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s5">&quot;best_score_&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_refit_callable_invalid_type():</span>
    <span class="s0">&quot;&quot;&quot; 
    Test implementation catches the errors when 'best_index_' returns an 
    invalid result. 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">refit_callable_invalid_type(cv_results):</span>
        <span class="s0">&quot;&quot;&quot; 
        A dummy function tests when returned 'best_index_' is not integer. 
        &quot;&quot;&quot;</span>
        <span class="s2">return None</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>

    <span class="s1">clf = GridSearchCV(</span>
        <span class="s1">LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span><span class="s2">,</span>
        <span class="s1">scoring=</span><span class="s5">&quot;precision&quot;</span><span class="s2">,</span>
        <span class="s1">refit=refit_callable_invalid_type</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(TypeError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;best_index_ returned is not an integer&quot;</span><span class="s1">):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;out_bound_value&quot;</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;search_cv&quot;</span><span class="s2">, </span><span class="s1">[RandomizedSearchCV</span><span class="s2">, </span><span class="s1">GridSearchCV])</span>
<span class="s2">def </span><span class="s1">test_refit_callable_out_bound(out_bound_value</span><span class="s2">, </span><span class="s1">search_cv):</span>
    <span class="s0">&quot;&quot;&quot; 
    Test implementation catches the errors when 'best_index_' returns an 
    out of bound result. 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">refit_callable_out_bound(cv_results):</span>
        <span class="s0">&quot;&quot;&quot; 
        A dummy function tests when returned 'best_index_' is out of bounds. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">out_bound_value</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>

    <span class="s1">clf = search_cv(</span>
        <span class="s1">LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span><span class="s2">,</span>
        <span class="s1">scoring=</span><span class="s5">&quot;precision&quot;</span><span class="s2">,</span>
        <span class="s1">refit=refit_callable_out_bound</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(IndexError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;best_index_ index out of range&quot;</span><span class="s1">):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_refit_callable_multi_metric():</span>
    <span class="s0">&quot;&quot;&quot; 
    Test refit=callable in multiple metric evaluation setting 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">refit_callable(cv_results):</span>
        <span class="s0">&quot;&quot;&quot; 
        A dummy function tests `refit=callable` interface. 
        Return the index of a model that has the least 
        `mean_test_prec`. 
        &quot;&quot;&quot;</span>
        <span class="s2">assert </span><span class="s5">&quot;mean_test_prec&quot; </span><span class="s2">in </span><span class="s1">cv_results</span>
        <span class="s2">return </span><span class="s1">cv_results[</span><span class="s5">&quot;mean_test_prec&quot;</span><span class="s1">].argmin()</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">scoring = {</span><span class="s5">&quot;Accuracy&quot;</span><span class="s1">: make_scorer(accuracy_score)</span><span class="s2">, </span><span class="s5">&quot;prec&quot;</span><span class="s1">: </span><span class="s5">&quot;precision&quot;</span><span class="s1">}</span>
    <span class="s1">clf = GridSearchCV(</span>
        <span class="s1">LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.01</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span><span class="s2">,</span>
        <span class="s1">scoring=scoring</span><span class="s2">,</span>
        <span class="s1">refit=refit_callable</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">assert </span><span class="s1">clf.best_index_ == </span><span class="s4">0</span>
    <span class="s3"># Ensure `best_score_` is disabled when using `refit=callable`</span>
    <span class="s2">assert not </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s5">&quot;best_score_&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_gridsearch_nd():</span>
    <span class="s3"># Pass X as list in GridSearchCV</span>
    <span class="s1">X_4d = np.arange(</span><span class="s4">10 </span><span class="s1">* </span><span class="s4">5 </span><span class="s1">* </span><span class="s4">3 </span><span class="s1">* </span><span class="s4">2</span><span class="s1">).reshape(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">y_3d = np.arange(</span><span class="s4">10 </span><span class="s1">* </span><span class="s4">7 </span><span class="s1">* </span><span class="s4">11</span><span class="s1">).reshape(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">7</span><span class="s2">, </span><span class="s4">11</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">check_X(x):</span>
        <span class="s2">return </span><span class="s1">x.shape[</span><span class="s4">1</span><span class="s1">:] == (</span><span class="s4">5</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">check_y(x):</span>
        <span class="s2">return </span><span class="s1">x.shape[</span><span class="s4">1</span><span class="s1">:] == (</span><span class="s4">7</span><span class="s2">, </span><span class="s4">11</span><span class="s1">)</span>

    <span class="s1">clf = CheckingClassifier(</span>
        <span class="s1">check_X=check_X</span><span class="s2">,</span>
        <span class="s1">check_y=check_y</span><span class="s2">,</span>
        <span class="s1">methods_to_check=[</span><span class="s5">&quot;fit&quot;</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">grid_search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]})</span>
    <span class="s1">grid_search.fit(X_4d</span><span class="s2">, </span><span class="s1">y_3d).score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">hasattr(grid_search</span><span class="s2">, </span><span class="s5">&quot;cv_results_&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_X_as_list():</span>
    <span class="s3"># Pass X as list in GridSearchCV</span>
    <span class="s1">X = np.arange(</span><span class="s4">100</span><span class="s1">).reshape(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">5 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">5</span><span class="s1">)</span>

    <span class="s1">clf = CheckingClassifier(</span>
        <span class="s1">check_X=</span><span class="s2">lambda </span><span class="s1">x: isinstance(x</span><span class="s2">, </span><span class="s1">list)</span><span class="s2">,</span>
        <span class="s1">methods_to_check=[</span><span class="s5">&quot;fit&quot;</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">cv = KFold(n_splits=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">grid_search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">cv=cv)</span>
    <span class="s1">grid_search.fit(X.tolist()</span><span class="s2">, </span><span class="s1">y).score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">hasattr(grid_search</span><span class="s2">, </span><span class="s5">&quot;cv_results_&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_y_as_list():</span>
    <span class="s3"># Pass y as list in GridSearchCV</span>
    <span class="s1">X = np.arange(</span><span class="s4">100</span><span class="s1">).reshape(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">5 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">5</span><span class="s1">)</span>

    <span class="s1">clf = CheckingClassifier(</span>
        <span class="s1">check_y=</span><span class="s2">lambda </span><span class="s1">x: isinstance(x</span><span class="s2">, </span><span class="s1">list)</span><span class="s2">,</span>
        <span class="s1">methods_to_check=[</span><span class="s5">&quot;fit&quot;</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">cv = KFold(n_splits=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">grid_search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">cv=cv)</span>
    <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y.tolist()).score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">hasattr(grid_search</span><span class="s2">, </span><span class="s5">&quot;cv_results_&quot;</span><span class="s1">)</span>


<span class="s1">@ignore_warnings</span>
<span class="s2">def </span><span class="s1">test_pandas_input():</span>
    <span class="s3"># check cross_val_score doesn't destroy pandas dataframe</span>
    <span class="s1">types = [(MockDataFrame</span><span class="s2">, </span><span class="s1">MockDataFrame)]</span>
    <span class="s2">try</span><span class="s1">:</span>
        <span class="s2">from </span><span class="s1">pandas </span><span class="s2">import </span><span class="s1">DataFrame</span><span class="s2">, </span><span class="s1">Series</span>

        <span class="s1">types.append((DataFrame</span><span class="s2">, </span><span class="s1">Series))</span>
    <span class="s2">except </span><span class="s1">ImportError:</span>
        <span class="s2">pass</span>

    <span class="s1">X = np.arange(</span><span class="s4">100</span><span class="s1">).reshape(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">5 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">5</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">InputFeatureType</span><span class="s2">, </span><span class="s1">TargetType </span><span class="s2">in </span><span class="s1">types:</span>
        <span class="s3"># X dataframe, y series</span>
        <span class="s1">X_df</span><span class="s2">, </span><span class="s1">y_ser = InputFeatureType(X)</span><span class="s2">, </span><span class="s1">TargetType(y)</span>

        <span class="s2">def </span><span class="s1">check_df(x):</span>
            <span class="s2">return </span><span class="s1">isinstance(x</span><span class="s2">, </span><span class="s1">InputFeatureType)</span>

        <span class="s2">def </span><span class="s1">check_series(x):</span>
            <span class="s2">return </span><span class="s1">isinstance(x</span><span class="s2">, </span><span class="s1">TargetType)</span>

        <span class="s1">clf = CheckingClassifier(check_X=check_df</span><span class="s2">, </span><span class="s1">check_y=check_series)</span>

        <span class="s1">grid_search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]})</span>
        <span class="s1">grid_search.fit(X_df</span><span class="s2">, </span><span class="s1">y_ser).score(X_df</span><span class="s2">, </span><span class="s1">y_ser)</span>
        <span class="s1">grid_search.predict(X_df)</span>
        <span class="s2">assert </span><span class="s1">hasattr(grid_search</span><span class="s2">, </span><span class="s5">&quot;cv_results_&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_unsupervised_grid_search():</span>
    <span class="s3"># test grid-search with unsupervised estimator</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(n_samples=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">km = KMeans(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">init=</span><span class="s5">&quot;random&quot;</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s4">1</span><span class="s1">)</span>

    <span class="s3"># Multi-metric evaluation unsupervised</span>
    <span class="s1">scoring = [</span><span class="s5">&quot;adjusted_rand_score&quot;</span><span class="s2">, </span><span class="s5">&quot;fowlkes_mallows_score&quot;</span><span class="s1">]</span>
    <span class="s2">for </span><span class="s1">refit </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;adjusted_rand_score&quot;</span><span class="s2">, </span><span class="s5">&quot;fowlkes_mallows_score&quot;</span><span class="s1">]:</span>
        <span class="s1">grid_search = GridSearchCV(</span>
            <span class="s1">km</span><span class="s2">, </span><span class="s1">param_grid=dict(n_clusters=[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">])</span><span class="s2">, </span><span class="s1">scoring=scoring</span><span class="s2">, </span><span class="s1">refit=refit</span>
        <span class="s1">)</span>
        <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s3"># Both ARI and FMS can find the right number :)</span>
        <span class="s2">assert </span><span class="s1">grid_search.best_params_[</span><span class="s5">&quot;n_clusters&quot;</span><span class="s1">] == </span><span class="s4">3</span>

    <span class="s3"># Single metric evaluation unsupervised</span>
    <span class="s1">grid_search = GridSearchCV(</span>
        <span class="s1">km</span><span class="s2">, </span><span class="s1">param_grid=dict(n_clusters=[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">])</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s5">&quot;fowlkes_mallows_score&quot;</span>
    <span class="s1">)</span>
    <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">grid_search.best_params_[</span><span class="s5">&quot;n_clusters&quot;</span><span class="s1">] == </span><span class="s4">3</span>

    <span class="s3"># Now without a score, and without y</span>
    <span class="s1">grid_search = GridSearchCV(km</span><span class="s2">, </span><span class="s1">param_grid=dict(n_clusters=[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]))</span>
    <span class="s1">grid_search.fit(X)</span>
    <span class="s2">assert </span><span class="s1">grid_search.best_params_[</span><span class="s5">&quot;n_clusters&quot;</span><span class="s1">] == </span><span class="s4">4</span>


<span class="s2">def </span><span class="s1">test_gridsearch_no_predict():</span>
    <span class="s3"># test grid-search with an estimator without predict.</span>
    <span class="s3"># slight duplication of a test from KDE</span>
    <span class="s2">def </span><span class="s1">custom_scoring(estimator</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s2">return </span><span class="s4">42 </span><span class="s2">if </span><span class="s1">estimator.bandwidth == </span><span class="s4">0.1 </span><span class="s2">else </span><span class="s4">0</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">_ = make_blobs(cluster_std=</span><span class="s4">0.1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">centers=[[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]])</span>
    <span class="s1">search = GridSearchCV(</span>
        <span class="s1">KernelDensity()</span><span class="s2">,</span>
        <span class="s1">param_grid=dict(bandwidth=[</span><span class="s4">0.01</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span><span class="s2">,</span>
        <span class="s1">scoring=custom_scoring</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">search.fit(X)</span>
    <span class="s2">assert </span><span class="s1">search.best_params_[</span><span class="s5">&quot;bandwidth&quot;</span><span class="s1">] == </span><span class="s4">0.1</span>
    <span class="s2">assert </span><span class="s1">search.best_score_ == </span><span class="s4">42</span>


<span class="s2">def </span><span class="s1">test_param_sampler():</span>
    <span class="s3"># test basic properties of param sampler</span>
    <span class="s1">param_distributions = {</span><span class="s5">&quot;kernel&quot;</span><span class="s1">: [</span><span class="s5">&quot;rbf&quot;</span><span class="s2">, </span><span class="s5">&quot;linear&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;C&quot;</span><span class="s1">: uniform(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)}</span>
    <span class="s1">sampler = ParameterSampler(</span>
        <span class="s1">param_distributions=param_distributions</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">samples = [x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">sampler]</span>
    <span class="s2">assert </span><span class="s1">len(samples) == </span><span class="s4">10</span>
    <span class="s2">for </span><span class="s1">sample </span><span class="s2">in </span><span class="s1">samples:</span>
        <span class="s2">assert </span><span class="s1">sample[</span><span class="s5">&quot;kernel&quot;</span><span class="s1">] </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;rbf&quot;</span><span class="s2">, </span><span class="s5">&quot;linear&quot;</span><span class="s1">]</span>
        <span class="s2">assert </span><span class="s4">0 </span><span class="s1">&lt;= sample[</span><span class="s5">&quot;C&quot;</span><span class="s1">] &lt;= </span><span class="s4">1</span>

    <span class="s3"># test that repeated calls yield identical parameters</span>
    <span class="s1">param_distributions = {</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">7</span><span class="s2">, </span><span class="s4">8</span><span class="s2">, </span><span class="s4">9</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]}</span>
    <span class="s1">sampler = ParameterSampler(</span>
        <span class="s1">param_distributions=param_distributions</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">[x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">sampler] == [x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">sampler]</span>

    <span class="s1">param_distributions = {</span><span class="s5">&quot;C&quot;</span><span class="s1">: uniform(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)}</span>
    <span class="s1">sampler = ParameterSampler(</span>
        <span class="s1">param_distributions=param_distributions</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">[x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">sampler] == [x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">sampler]</span>


<span class="s2">def </span><span class="s1">check_cv_results_array_types(search</span><span class="s2">, </span><span class="s1">param_keys</span><span class="s2">, </span><span class="s1">score_keys):</span>
    <span class="s3"># Check if the search `cv_results`'s array are of correct types</span>
    <span class="s1">cv_results = search.cv_results_</span>
    <span class="s2">assert </span><span class="s1">all(isinstance(cv_results[param]</span><span class="s2">, </span><span class="s1">np.ma.MaskedArray) </span><span class="s2">for </span><span class="s1">param </span><span class="s2">in </span><span class="s1">param_keys)</span>
    <span class="s2">assert </span><span class="s1">all(cv_results[key].dtype == object </span><span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">param_keys)</span>
    <span class="s2">assert not </span><span class="s1">any(isinstance(cv_results[key]</span><span class="s2">, </span><span class="s1">np.ma.MaskedArray) </span><span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">score_keys)</span>
    <span class="s2">assert </span><span class="s1">all(</span>
        <span class="s1">cv_results[key].dtype == np.float64</span>
        <span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">score_keys</span>
        <span class="s2">if not </span><span class="s1">key.startswith(</span><span class="s5">&quot;rank&quot;</span><span class="s1">)</span>
    <span class="s1">)</span>

    <span class="s1">scorer_keys = search.scorer_.keys() </span><span class="s2">if </span><span class="s1">search.multimetric_ </span><span class="s2">else </span><span class="s1">[</span><span class="s5">&quot;score&quot;</span><span class="s1">]</span>

    <span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">scorer_keys:</span>
        <span class="s2">assert </span><span class="s1">cv_results[</span><span class="s5">&quot;rank_test_%s&quot; </span><span class="s1">% key].dtype == np.int32</span>


<span class="s2">def </span><span class="s1">check_cv_results_keys(cv_results</span><span class="s2">, </span><span class="s1">param_keys</span><span class="s2">, </span><span class="s1">score_keys</span><span class="s2">, </span><span class="s1">n_cand</span><span class="s2">, </span><span class="s1">extra_keys=()):</span>
    <span class="s3"># Test the search.cv_results_ contains all the required results</span>
    <span class="s1">all_keys = param_keys + score_keys + extra_keys</span>
    <span class="s1">assert_array_equal(sorted(cv_results.keys())</span><span class="s2">, </span><span class="s1">sorted(all_keys + (</span><span class="s5">&quot;params&quot;</span><span class="s2">,</span><span class="s1">)))</span>
    <span class="s2">assert </span><span class="s1">all(cv_results[key].shape == (n_cand</span><span class="s2">,</span><span class="s1">) </span><span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">param_keys + score_keys)</span>


<span class="s2">def </span><span class="s1">test_grid_search_cv_results():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>

    <span class="s1">n_grid_points = </span><span class="s4">6</span>
    <span class="s1">params = [</span>
        <span class="s1">dict(</span>
            <span class="s1">kernel=[</span>
                <span class="s5">&quot;rbf&quot;</span><span class="s2">,</span>
            <span class="s1">]</span><span class="s2">,</span>
            <span class="s1">C=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">gamma=[</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">dict(</span>
            <span class="s1">kernel=[</span>
                <span class="s5">&quot;poly&quot;</span><span class="s2">,</span>
            <span class="s1">]</span><span class="s2">,</span>
            <span class="s1">degree=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span>

    <span class="s1">param_keys = (</span><span class="s5">&quot;param_C&quot;</span><span class="s2">, </span><span class="s5">&quot;param_degree&quot;</span><span class="s2">, </span><span class="s5">&quot;param_gamma&quot;</span><span class="s2">, </span><span class="s5">&quot;param_kernel&quot;</span><span class="s1">)</span>
    <span class="s1">score_keys = (</span>
        <span class="s5">&quot;mean_test_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;mean_train_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;rank_test_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;split0_test_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;split1_test_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;split2_test_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;split0_train_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;split1_train_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;split2_train_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;std_test_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;std_train_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;mean_fit_time&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;std_fit_time&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;mean_score_time&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;std_score_time&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">n_candidates = n_grid_points</span>

    <span class="s1">search = GridSearchCV(SVC()</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">param_grid=params</span><span class="s2">, </span><span class="s1">return_train_score=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">cv_results = search.cv_results_</span>
    <span class="s3"># Check if score and timing are reasonable</span>
    <span class="s2">assert </span><span class="s1">all(cv_results[</span><span class="s5">&quot;rank_test_score&quot;</span><span class="s1">] &gt;= </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">(all(cv_results[k] &gt;= </span><span class="s4">0</span><span class="s1">) </span><span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">score_keys </span><span class="s2">if </span><span class="s1">k != </span><span class="s5">&quot;rank_test_score&quot;</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">(</span>
        <span class="s1">all(cv_results[k] &lt;= </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">score_keys</span>
        <span class="s2">if </span><span class="s5">&quot;time&quot; </span><span class="s2">not in </span><span class="s1">k </span><span class="s2">and </span><span class="s1">k != </span><span class="s5">&quot;rank_test_score&quot;</span>
    <span class="s1">)</span>
    <span class="s3"># Check cv_results structure</span>
    <span class="s1">check_cv_results_array_types(search</span><span class="s2">, </span><span class="s1">param_keys</span><span class="s2">, </span><span class="s1">score_keys)</span>
    <span class="s1">check_cv_results_keys(cv_results</span><span class="s2">, </span><span class="s1">param_keys</span><span class="s2">, </span><span class="s1">score_keys</span><span class="s2">, </span><span class="s1">n_candidates)</span>
    <span class="s3"># Check masking</span>
    <span class="s1">cv_results = search.cv_results_</span>

    <span class="s1">poly_results = [</span>
        <span class="s1">(</span>
            <span class="s1">cv_results[</span><span class="s5">&quot;param_C&quot;</span><span class="s1">].mask[i]</span>
            <span class="s2">and </span><span class="s1">cv_results[</span><span class="s5">&quot;param_gamma&quot;</span><span class="s1">].mask[i]</span>
            <span class="s2">and not </span><span class="s1">cv_results[</span><span class="s5">&quot;param_degree&quot;</span><span class="s1">].mask[i]</span>
        <span class="s1">)</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_candidates)</span>
        <span class="s2">if </span><span class="s1">cv_results[</span><span class="s5">&quot;param_kernel&quot;</span><span class="s1">][i] == </span><span class="s5">&quot;poly&quot;</span>
    <span class="s1">]</span>
    <span class="s2">assert </span><span class="s1">all(poly_results)</span>
    <span class="s2">assert </span><span class="s1">len(poly_results) == </span><span class="s4">2</span>

    <span class="s1">rbf_results = [</span>
        <span class="s1">(</span>
            <span class="s2">not </span><span class="s1">cv_results[</span><span class="s5">&quot;param_C&quot;</span><span class="s1">].mask[i]</span>
            <span class="s2">and not </span><span class="s1">cv_results[</span><span class="s5">&quot;param_gamma&quot;</span><span class="s1">].mask[i]</span>
            <span class="s2">and </span><span class="s1">cv_results[</span><span class="s5">&quot;param_degree&quot;</span><span class="s1">].mask[i]</span>
        <span class="s1">)</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_candidates)</span>
        <span class="s2">if </span><span class="s1">cv_results[</span><span class="s5">&quot;param_kernel&quot;</span><span class="s1">][i] == </span><span class="s5">&quot;rbf&quot;</span>
    <span class="s1">]</span>
    <span class="s2">assert </span><span class="s1">all(rbf_results)</span>
    <span class="s2">assert </span><span class="s1">len(rbf_results) == </span><span class="s4">4</span>


<span class="s2">def </span><span class="s1">test_random_search_cv_results():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>

    <span class="s1">n_search_iter = </span><span class="s4">30</span>

    <span class="s1">params = [</span>
        <span class="s1">{</span><span class="s5">&quot;kernel&quot;</span><span class="s1">: [</span><span class="s5">&quot;rbf&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;C&quot;</span><span class="s1">: expon(scale=</span><span class="s4">10</span><span class="s1">)</span><span class="s2">, </span><span class="s5">&quot;gamma&quot;</span><span class="s1">: expon(scale=</span><span class="s4">0.1</span><span class="s1">)}</span><span class="s2">,</span>
        <span class="s1">{</span><span class="s5">&quot;kernel&quot;</span><span class="s1">: [</span><span class="s5">&quot;poly&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;degree&quot;</span><span class="s1">: [</span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]}</span><span class="s2">,</span>
    <span class="s1">]</span>
    <span class="s1">param_keys = (</span><span class="s5">&quot;param_C&quot;</span><span class="s2">, </span><span class="s5">&quot;param_degree&quot;</span><span class="s2">, </span><span class="s5">&quot;param_gamma&quot;</span><span class="s2">, </span><span class="s5">&quot;param_kernel&quot;</span><span class="s1">)</span>
    <span class="s1">score_keys = (</span>
        <span class="s5">&quot;mean_test_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;mean_train_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;rank_test_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;split0_test_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;split1_test_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;split2_test_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;split0_train_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;split1_train_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;split2_train_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;std_test_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;std_train_score&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;mean_fit_time&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;std_fit_time&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;mean_score_time&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;std_score_time&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">n_candidates = n_search_iter</span>

    <span class="s1">search = RandomizedSearchCV(</span>
        <span class="s1">SVC()</span><span class="s2">,</span>
        <span class="s1">n_iter=n_search_iter</span><span class="s2">,</span>
        <span class="s1">cv=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">param_distributions=params</span><span class="s2">,</span>
        <span class="s1">return_train_score=</span><span class="s2">True,</span>
    <span class="s1">)</span>
    <span class="s1">search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">cv_results = search.cv_results_</span>
    <span class="s3"># Check results structure</span>
    <span class="s1">check_cv_results_array_types(search</span><span class="s2">, </span><span class="s1">param_keys</span><span class="s2">, </span><span class="s1">score_keys)</span>
    <span class="s1">check_cv_results_keys(cv_results</span><span class="s2">, </span><span class="s1">param_keys</span><span class="s2">, </span><span class="s1">score_keys</span><span class="s2">, </span><span class="s1">n_candidates)</span>
    <span class="s2">assert </span><span class="s1">all(</span>
        <span class="s1">(</span>
            <span class="s1">cv_results[</span><span class="s5">&quot;param_C&quot;</span><span class="s1">].mask[i]</span>
            <span class="s2">and </span><span class="s1">cv_results[</span><span class="s5">&quot;param_gamma&quot;</span><span class="s1">].mask[i]</span>
            <span class="s2">and not </span><span class="s1">cv_results[</span><span class="s5">&quot;param_degree&quot;</span><span class="s1">].mask[i]</span>
        <span class="s1">)</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_candidates)</span>
        <span class="s2">if </span><span class="s1">cv_results[</span><span class="s5">&quot;param_kernel&quot;</span><span class="s1">][i] == </span><span class="s5">&quot;poly&quot;</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">all(</span>
        <span class="s1">(</span>
            <span class="s2">not </span><span class="s1">cv_results[</span><span class="s5">&quot;param_C&quot;</span><span class="s1">].mask[i]</span>
            <span class="s2">and not </span><span class="s1">cv_results[</span><span class="s5">&quot;param_gamma&quot;</span><span class="s1">].mask[i]</span>
            <span class="s2">and </span><span class="s1">cv_results[</span><span class="s5">&quot;param_degree&quot;</span><span class="s1">].mask[i]</span>
        <span class="s1">)</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_candidates)</span>
        <span class="s2">if </span><span class="s1">cv_results[</span><span class="s5">&quot;param_kernel&quot;</span><span class="s1">][i] == </span><span class="s5">&quot;rbf&quot;</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;SearchCV, specialized_params&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(GridSearchCV</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;param_grid&quot;</span><span class="s1">: {</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]}})</span><span class="s2">,</span>
        <span class="s1">(RandomizedSearchCV</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;param_distributions&quot;</span><span class="s1">: {</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]}</span><span class="s2">, </span><span class="s5">&quot;n_iter&quot;</span><span class="s1">: </span><span class="s4">2</span><span class="s1">})</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_search_default_iid(SearchCV</span><span class="s2">, </span><span class="s1">specialized_params):</span>
    <span class="s3"># Test the IID parameter  TODO: Clearly this test does something else???</span>
    <span class="s3"># noise-free simple 2d-data</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(</span>
        <span class="s1">centers=[[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">cluster_std=</span><span class="s4">0.1</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
        <span class="s1">n_samples=</span><span class="s4">80</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s3"># split dataset into two folds that are not iid</span>
    <span class="s3"># first one contains data of all 4 blobs, second only from two.</span>
    <span class="s1">mask = np.ones(X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=bool)</span>
    <span class="s1">mask[np.where(y == </span><span class="s4">1</span><span class="s1">)[</span><span class="s4">0</span><span class="s1">][::</span><span class="s4">2</span><span class="s1">]] = </span><span class="s4">0</span>
    <span class="s1">mask[np.where(y == </span><span class="s4">2</span><span class="s1">)[</span><span class="s4">0</span><span class="s1">][::</span><span class="s4">2</span><span class="s1">]] = </span><span class="s4">0</span>
    <span class="s3"># this leads to perfect classification on one fold and a score of 1/3 on</span>
    <span class="s3"># the other</span>
    <span class="s3"># create &quot;cv&quot; for splits</span>
    <span class="s1">cv = [[mask</span><span class="s2">, </span><span class="s1">~mask]</span><span class="s2">, </span><span class="s1">[~mask</span><span class="s2">, </span><span class="s1">mask]]</span>

    <span class="s1">common_params = {</span><span class="s5">&quot;estimator&quot;</span><span class="s1">: SVC()</span><span class="s2">, </span><span class="s5">&quot;cv&quot;</span><span class="s1">: cv</span><span class="s2">, </span><span class="s5">&quot;return_train_score&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>
    <span class="s1">search = SearchCV(**common_params</span><span class="s2">, </span><span class="s1">**specialized_params)</span>
    <span class="s1">search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">test_cv_scores = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">search.cv_results_[</span><span class="s5">&quot;split%d_test_score&quot; </span><span class="s1">% s][</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">range(search.n_splits_)</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">test_mean = search.cv_results_[</span><span class="s5">&quot;mean_test_score&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">test_std = search.cv_results_[</span><span class="s5">&quot;std_test_score&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s1">train_cv_scores = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">search.cv_results_[</span><span class="s5">&quot;split%d_train_score&quot; </span><span class="s1">% s][</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">range(search.n_splits_)</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">train_mean = search.cv_results_[</span><span class="s5">&quot;mean_train_score&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">train_std = search.cv_results_[</span><span class="s5">&quot;std_train_score&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s2">assert </span><span class="s1">search.cv_results_[</span><span class="s5">&quot;param_C&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1</span>
    <span class="s3"># scores are the same as above</span>
    <span class="s1">assert_allclose(test_cv_scores</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1.0 </span><span class="s1">/ </span><span class="s4">3.0</span><span class="s1">])</span>
    <span class="s1">assert_allclose(train_cv_scores</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s3"># Unweighted mean/std is used</span>
    <span class="s2">assert </span><span class="s1">test_mean == pytest.approx(np.mean(test_cv_scores))</span>
    <span class="s2">assert </span><span class="s1">test_std == pytest.approx(np.std(test_cv_scores))</span>

    <span class="s3"># For the train scores, we do not take a weighted mean irrespective of</span>
    <span class="s3"># i.i.d. or not</span>
    <span class="s2">assert </span><span class="s1">train_mean == pytest.approx(</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">train_std == pytest.approx(</span><span class="s4">0</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_grid_search_cv_results_multimetric():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>

    <span class="s1">n_splits = </span><span class="s4">3</span>
    <span class="s1">params = [</span>
        <span class="s1">dict(</span>
            <span class="s1">kernel=[</span>
                <span class="s5">&quot;rbf&quot;</span><span class="s2">,</span>
            <span class="s1">]</span><span class="s2">,</span>
            <span class="s1">C=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">gamma=[</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">dict(</span>
            <span class="s1">kernel=[</span>
                <span class="s5">&quot;poly&quot;</span><span class="s2">,</span>
            <span class="s1">]</span><span class="s2">,</span>
            <span class="s1">degree=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span>

    <span class="s1">grid_searches = []</span>
    <span class="s2">for </span><span class="s1">scoring </span><span class="s2">in </span><span class="s1">(</span>
        <span class="s1">{</span><span class="s5">&quot;accuracy&quot;</span><span class="s1">: make_scorer(accuracy_score)</span><span class="s2">, </span><span class="s5">&quot;recall&quot;</span><span class="s1">: make_scorer(recall_score)}</span><span class="s2">,</span>
        <span class="s5">&quot;accuracy&quot;</span><span class="s2">,</span>
        <span class="s5">&quot;recall&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">grid_search = GridSearchCV(</span>
            <span class="s1">SVC()</span><span class="s2">, </span><span class="s1">cv=n_splits</span><span class="s2">, </span><span class="s1">param_grid=params</span><span class="s2">, </span><span class="s1">scoring=scoring</span><span class="s2">, </span><span class="s1">refit=</span><span class="s2">False</span>
        <span class="s1">)</span>
        <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">grid_searches.append(grid_search)</span>

    <span class="s1">compare_cv_results_multimetric_with_single(*grid_searches)</span>


<span class="s2">def </span><span class="s1">test_random_search_cv_results_multimetric():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>

    <span class="s1">n_splits = </span><span class="s4">3</span>
    <span class="s1">n_search_iter = </span><span class="s4">30</span>

    <span class="s3"># Scipy 0.12's stats dists do not accept seed, hence we use param grid</span>
    <span class="s1">params = dict(C=np.logspace(-</span><span class="s4">4</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">)</span><span class="s2">, </span><span class="s1">gamma=np.logspace(-</span><span class="s4">5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s1">base=</span><span class="s4">0.1</span><span class="s1">))</span>
    <span class="s2">for </span><span class="s1">refit </span><span class="s2">in </span><span class="s1">(</span><span class="s2">True, False</span><span class="s1">):</span>
        <span class="s1">random_searches = []</span>
        <span class="s2">for </span><span class="s1">scoring </span><span class="s2">in </span><span class="s1">((</span><span class="s5">&quot;accuracy&quot;</span><span class="s2">, </span><span class="s5">&quot;recall&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s5">&quot;accuracy&quot;</span><span class="s2">, </span><span class="s5">&quot;recall&quot;</span><span class="s1">):</span>
            <span class="s3"># If True, for multi-metric pass refit='accuracy'</span>
            <span class="s2">if </span><span class="s1">refit:</span>
                <span class="s1">probability = </span><span class="s2">True</span>
                <span class="s1">refit = </span><span class="s5">&quot;accuracy&quot; </span><span class="s2">if </span><span class="s1">isinstance(scoring</span><span class="s2">, </span><span class="s1">tuple) </span><span class="s2">else </span><span class="s1">refit</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">probability = </span><span class="s2">False</span>
            <span class="s1">clf = SVC(probability=probability</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
            <span class="s1">random_search = RandomizedSearchCV(</span>
                <span class="s1">clf</span><span class="s2">,</span>
                <span class="s1">n_iter=n_search_iter</span><span class="s2">,</span>
                <span class="s1">cv=n_splits</span><span class="s2">,</span>
                <span class="s1">param_distributions=params</span><span class="s2">,</span>
                <span class="s1">scoring=scoring</span><span class="s2">,</span>
                <span class="s1">refit=refit</span><span class="s2">,</span>
                <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">random_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
            <span class="s1">random_searches.append(random_search)</span>

        <span class="s1">compare_cv_results_multimetric_with_single(*random_searches)</span>
        <span class="s1">compare_refit_methods_when_refit_with_acc(</span>
            <span class="s1">random_searches[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">random_searches[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">refit</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">compare_cv_results_multimetric_with_single(search_multi</span><span class="s2">, </span><span class="s1">search_acc</span><span class="s2">, </span><span class="s1">search_rec):</span>
    <span class="s0">&quot;&quot;&quot;Compare multi-metric cv_results with the ensemble of multiple 
    single metric cv_results from single metric grid/random search&quot;&quot;&quot;</span>

    <span class="s2">assert </span><span class="s1">search_multi.multimetric_</span>
    <span class="s1">assert_array_equal(sorted(search_multi.scorer_)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;accuracy&quot;</span><span class="s2">, </span><span class="s5">&quot;recall&quot;</span><span class="s1">))</span>

    <span class="s1">cv_results_multi = search_multi.cv_results_</span>
    <span class="s1">cv_results_acc_rec = {</span>
        <span class="s1">re.sub(</span><span class="s5">&quot;_score$&quot;</span><span class="s2">, </span><span class="s5">&quot;_accuracy&quot;</span><span class="s2">, </span><span class="s1">k): v </span><span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">v </span><span class="s2">in </span><span class="s1">search_acc.cv_results_.items()</span>
    <span class="s1">}</span>
    <span class="s1">cv_results_acc_rec.update(</span>
        <span class="s1">{re.sub(</span><span class="s5">&quot;_score$&quot;</span><span class="s2">, </span><span class="s5">&quot;_recall&quot;</span><span class="s2">, </span><span class="s1">k): v </span><span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">v </span><span class="s2">in </span><span class="s1">search_rec.cv_results_.items()}</span>
    <span class="s1">)</span>

    <span class="s3"># Check if score and timing are reasonable, also checks if the keys</span>
    <span class="s3"># are present</span>
    <span class="s2">assert </span><span class="s1">all(</span>
        <span class="s1">(</span>
            <span class="s1">np.all(cv_results_multi[k] &lt;= </span><span class="s4">1</span><span class="s1">)</span>
            <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">(</span>
                <span class="s5">&quot;mean_score_time&quot;</span><span class="s2">,</span>
                <span class="s5">&quot;std_score_time&quot;</span><span class="s2">,</span>
                <span class="s5">&quot;mean_fit_time&quot;</span><span class="s2">,</span>
                <span class="s5">&quot;std_fit_time&quot;</span><span class="s2">,</span>
            <span class="s1">)</span>
        <span class="s1">)</span>
    <span class="s1">)</span>

    <span class="s3"># Compare the keys, other than time keys, among multi-metric and</span>
    <span class="s3"># single metric grid search results. np.testing.assert_equal performs a</span>
    <span class="s3"># deep nested comparison of the two cv_results dicts</span>
    <span class="s1">np.testing.assert_equal(</span>
        <span class="s1">{k: v </span><span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">v </span><span class="s2">in </span><span class="s1">cv_results_multi.items() </span><span class="s2">if not </span><span class="s1">k.endswith(</span><span class="s5">&quot;_time&quot;</span><span class="s1">)}</span><span class="s2">,</span>
        <span class="s1">{k: v </span><span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">v </span><span class="s2">in </span><span class="s1">cv_results_acc_rec.items() </span><span class="s2">if not </span><span class="s1">k.endswith(</span><span class="s5">&quot;_time&quot;</span><span class="s1">)}</span><span class="s2">,</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">compare_refit_methods_when_refit_with_acc(search_multi</span><span class="s2">, </span><span class="s1">search_acc</span><span class="s2">, </span><span class="s1">refit):</span>
    <span class="s0">&quot;&quot;&quot;Compare refit multi-metric search methods with single metric methods&quot;&quot;&quot;</span>
    <span class="s2">assert </span><span class="s1">search_acc.refit == refit</span>
    <span class="s2">if </span><span class="s1">refit:</span>
        <span class="s2">assert </span><span class="s1">search_multi.refit == </span><span class="s5">&quot;accuracy&quot;</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">assert not </span><span class="s1">search_multi.refit</span>
        <span class="s2">return  </span><span class="s3"># search cannot predict/score without refit</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(n_samples=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s2">for </span><span class="s1">method </span><span class="s2">in </span><span class="s1">(</span><span class="s5">&quot;predict&quot;</span><span class="s2">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s2">, </span><span class="s5">&quot;predict_log_proba&quot;</span><span class="s1">):</span>
        <span class="s1">assert_almost_equal(</span>
            <span class="s1">getattr(search_multi</span><span class="s2">, </span><span class="s1">method)(X)</span><span class="s2">, </span><span class="s1">getattr(search_acc</span><span class="s2">, </span><span class="s1">method)(X)</span>
        <span class="s1">)</span>
    <span class="s1">assert_almost_equal(search_multi.score(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">, </span><span class="s1">search_acc.score(X</span><span class="s2">, </span><span class="s1">y))</span>
    <span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">(</span><span class="s5">&quot;best_index_&quot;</span><span class="s2">, </span><span class="s5">&quot;best_score_&quot;</span><span class="s2">, </span><span class="s5">&quot;best_params_&quot;</span><span class="s1">):</span>
        <span class="s2">assert </span><span class="s1">getattr(search_multi</span><span class="s2">, </span><span class="s1">key) == getattr(search_acc</span><span class="s2">, </span><span class="s1">key)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;search_cv&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">RandomizedSearchCV(</span>
            <span class="s1">estimator=DecisionTreeClassifier()</span><span class="s2">,</span>
            <span class="s1">param_distributions={</span><span class="s5">&quot;max_depth&quot;</span><span class="s1">: [</span><span class="s4">5</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]}</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">GridSearchCV(</span>
            <span class="s1">estimator=DecisionTreeClassifier()</span><span class="s2">, </span><span class="s1">param_grid={</span><span class="s5">&quot;max_depth&quot;</span><span class="s1">: [</span><span class="s4">5</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]}</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_search_cv_score_samples_error(search_cv):</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(n_samples=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">search_cv.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s3"># Make sure to error out when underlying estimator does not implement</span>
    <span class="s3"># the method `score_samples`</span>
    <span class="s1">err_msg = </span><span class="s5">&quot;'DecisionTreeClassifier' object has no attribute 'score_samples'&quot;</span>

    <span class="s2">with </span><span class="s1">pytest.raises(AttributeError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">search_cv.score_samples(X)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;search_cv&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">RandomizedSearchCV(</span>
            <span class="s1">estimator=LocalOutlierFactor(novelty=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">param_distributions={</span><span class="s5">&quot;n_neighbors&quot;</span><span class="s1">: [</span><span class="s4">5</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]}</span><span class="s2">,</span>
            <span class="s1">scoring=</span><span class="s5">&quot;precision&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">GridSearchCV(</span>
            <span class="s1">estimator=LocalOutlierFactor(novelty=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">param_grid={</span><span class="s5">&quot;n_neighbors&quot;</span><span class="s1">: [</span><span class="s4">5</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]}</span><span class="s2">,</span>
            <span class="s1">scoring=</span><span class="s5">&quot;precision&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_search_cv_score_samples_method(search_cv):</span>
    <span class="s3"># Set parameters</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s4">300</span>
    <span class="s1">outliers_fraction = </span><span class="s4">0.15</span>
    <span class="s1">n_outliers = int(outliers_fraction * n_samples)</span>
    <span class="s1">n_inliers = n_samples - n_outliers</span>

    <span class="s3"># Create dataset</span>
    <span class="s1">X = make_blobs(</span>
        <span class="s1">n_samples=n_inliers</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">2</span><span class="s2">,</span>
        <span class="s1">centers=[[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]]</span><span class="s2">,</span>
        <span class="s1">cluster_std=</span><span class="s4">0.5</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s3"># Add some noisy points</span>
    <span class="s1">X = np.concatenate([X</span><span class="s2">, </span><span class="s1">rng.uniform(low=-</span><span class="s4">6</span><span class="s2">, </span><span class="s1">high=</span><span class="s4">6</span><span class="s2">, </span><span class="s1">size=(n_outliers</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s3"># Define labels to be able to score the estimator with `search_cv`</span>
    <span class="s1">y_true = np.array([</span><span class="s4">1</span><span class="s1">] * n_samples)</span>
    <span class="s1">y_true[-n_outliers:] = -</span><span class="s4">1</span>

    <span class="s3"># Fit on data</span>
    <span class="s1">search_cv.fit(X</span><span class="s2">, </span><span class="s1">y_true)</span>

    <span class="s3"># Verify that the stand alone estimator yields the same results</span>
    <span class="s3"># as the ones obtained with *SearchCV</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">search_cv.score_samples(X)</span><span class="s2">, </span><span class="s1">search_cv.best_estimator_.score_samples(X)</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_search_cv_results_rank_tie_breaking():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(n_samples=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>

    <span class="s3"># The two C values are close enough to give similar models</span>
    <span class="s3"># which would result in a tie of their mean cv-scores</span>
    <span class="s1">param_grid = {</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1.001</span><span class="s2">, </span><span class="s4">0.001</span><span class="s1">]}</span>

    <span class="s1">grid_search = GridSearchCV(SVC()</span><span class="s2">, </span><span class="s1">param_grid=param_grid</span><span class="s2">, </span><span class="s1">return_train_score=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">random_search = RandomizedSearchCV(</span>
        <span class="s1">SVC()</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">param_distributions=param_grid</span><span class="s2">, </span><span class="s1">return_train_score=</span><span class="s2">True</span>
    <span class="s1">)</span>

    <span class="s2">for </span><span class="s1">search </span><span class="s2">in </span><span class="s1">(grid_search</span><span class="s2">, </span><span class="s1">random_search):</span>
        <span class="s1">search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">cv_results = search.cv_results_</span>
        <span class="s3"># Check tie breaking strategy -</span>
        <span class="s3"># Check that there is a tie in the mean scores between</span>
        <span class="s3"># candidates 1 and 2 alone</span>
        <span class="s1">assert_almost_equal(</span>
            <span class="s1">cv_results[</span><span class="s5">&quot;mean_test_score&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">cv_results[</span><span class="s5">&quot;mean_test_score&quot;</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">)</span>
        <span class="s1">assert_almost_equal(</span>
            <span class="s1">cv_results[</span><span class="s5">&quot;mean_train_score&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">cv_results[</span><span class="s5">&quot;mean_train_score&quot;</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">)</span>
        <span class="s2">assert not </span><span class="s1">np.allclose(</span>
            <span class="s1">cv_results[</span><span class="s5">&quot;mean_test_score&quot;</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">cv_results[</span><span class="s5">&quot;mean_test_score&quot;</span><span class="s1">][</span><span class="s4">2</span><span class="s1">]</span>
        <span class="s1">)</span>
        <span class="s2">assert not </span><span class="s1">np.allclose(</span>
            <span class="s1">cv_results[</span><span class="s5">&quot;mean_train_score&quot;</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">cv_results[</span><span class="s5">&quot;mean_train_score&quot;</span><span class="s1">][</span><span class="s4">2</span><span class="s1">]</span>
        <span class="s1">)</span>
        <span class="s3"># 'min' rank should be assigned to the tied candidates</span>
        <span class="s1">assert_almost_equal(search.cv_results_[</span><span class="s5">&quot;rank_test_score&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_search_cv_results_none_param():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = [[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">estimators = (DecisionTreeRegressor()</span><span class="s2">, </span><span class="s1">DecisionTreeClassifier())</span>
    <span class="s1">est_parameters = {</span><span class="s5">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s2">, None</span><span class="s1">]}</span>
    <span class="s1">cv = KFold()</span>

    <span class="s2">for </span><span class="s1">est </span><span class="s2">in </span><span class="s1">estimators:</span>
        <span class="s1">grid_search = GridSearchCV(</span>
            <span class="s1">est</span><span class="s2">,</span>
            <span class="s1">est_parameters</span><span class="s2">,</span>
            <span class="s1">cv=cv</span><span class="s2">,</span>
        <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">assert_array_equal(grid_search.cv_results_[</span><span class="s5">&quot;param_random_state&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, None</span><span class="s1">])</span>


<span class="s1">@ignore_warnings()</span>
<span class="s2">def </span><span class="s1">test_search_cv_timing():</span>
    <span class="s1">svc = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">X = [</span>
        <span class="s1">[</span>
            <span class="s4">1</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span>
            <span class="s4">2</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span>
            <span class="s4">3</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span>
            <span class="s4">4</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
    <span class="s1">]</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>

    <span class="s1">gs = GridSearchCV(svc</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">error_score=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rs = RandomizedSearchCV(svc</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">error_score=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">search </span><span class="s2">in </span><span class="s1">(gs</span><span class="s2">, </span><span class="s1">rs):</span>
        <span class="s1">search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;mean_fit_time&quot;</span><span class="s2">, </span><span class="s5">&quot;std_fit_time&quot;</span><span class="s1">]:</span>
            <span class="s3"># NOTE The precision of time.time in windows is not high</span>
            <span class="s3"># enough for the fit/score times to be non-zero for trivial X and y</span>
            <span class="s2">assert </span><span class="s1">np.all(search.cv_results_[key] &gt;= </span><span class="s4">0</span><span class="s1">)</span>
            <span class="s2">assert </span><span class="s1">np.all(search.cv_results_[key] &lt; </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;mean_score_time&quot;</span><span class="s2">, </span><span class="s5">&quot;std_score_time&quot;</span><span class="s1">]:</span>
            <span class="s2">assert </span><span class="s1">search.cv_results_[key][</span><span class="s4">1</span><span class="s1">] &gt;= </span><span class="s4">0</span>
            <span class="s2">assert </span><span class="s1">search.cv_results_[key][</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">0.0</span>
            <span class="s2">assert </span><span class="s1">np.all(search.cv_results_[key] &lt; </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s2">assert </span><span class="s1">hasattr(search</span><span class="s2">, </span><span class="s5">&quot;refit_time_&quot;</span><span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">isinstance(search.refit_time_</span><span class="s2">, </span><span class="s1">float)</span>
        <span class="s2">assert </span><span class="s1">search.refit_time_ &gt;= </span><span class="s4">0</span>


<span class="s2">def </span><span class="s1">test_grid_search_correct_score_results():</span>
    <span class="s3"># test that correct scores are used</span>
    <span class="s1">n_splits = </span><span class="s4">3</span>
    <span class="s1">clf = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">centers=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">Cs = [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span>
    <span class="s2">for </span><span class="s1">score </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;f1&quot;</span><span class="s2">, </span><span class="s5">&quot;roc_auc&quot;</span><span class="s1">]:</span>
        <span class="s1">grid_search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: Cs}</span><span class="s2">, </span><span class="s1">scoring=score</span><span class="s2">, </span><span class="s1">cv=n_splits)</span>
        <span class="s1">cv_results = grid_search.fit(X</span><span class="s2">, </span><span class="s1">y).cv_results_</span>

        <span class="s3"># Test scorer names</span>
        <span class="s1">result_keys = list(cv_results.keys())</span>
        <span class="s1">expected_keys = (</span><span class="s5">&quot;mean_test_score&quot;</span><span class="s2">, </span><span class="s5">&quot;rank_test_score&quot;</span><span class="s1">) + tuple(</span>
            <span class="s5">&quot;split%d_test_score&quot; </span><span class="s1">% cv_i </span><span class="s2">for </span><span class="s1">cv_i </span><span class="s2">in </span><span class="s1">range(n_splits)</span>
        <span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">all(np.isin(expected_keys</span><span class="s2">, </span><span class="s1">result_keys))</span>

        <span class="s1">cv = StratifiedKFold(n_splits=n_splits)</span>
        <span class="s1">n_splits = grid_search.n_splits_</span>
        <span class="s2">for </span><span class="s1">candidate_i</span><span class="s2">, </span><span class="s1">C </span><span class="s2">in </span><span class="s1">enumerate(Cs):</span>
            <span class="s1">clf.set_params(C=C)</span>
            <span class="s1">cv_scores = np.array(</span>
                <span class="s1">[</span>
                    <span class="s1">grid_search.cv_results_[</span><span class="s5">&quot;split%d_test_score&quot; </span><span class="s1">% s][candidate_i]</span>
                    <span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">range(n_splits)</span>
                <span class="s1">]</span>
            <span class="s1">)</span>
            <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">(train</span><span class="s2">, </span><span class="s1">test) </span><span class="s2">in </span><span class="s1">enumerate(cv.split(X</span><span class="s2">, </span><span class="s1">y)):</span>
                <span class="s1">clf.fit(X[train]</span><span class="s2">, </span><span class="s1">y[train])</span>
                <span class="s2">if </span><span class="s1">score == </span><span class="s5">&quot;f1&quot;</span><span class="s1">:</span>
                    <span class="s1">correct_score = f1_score(y[test]</span><span class="s2">, </span><span class="s1">clf.predict(X[test]))</span>
                <span class="s2">elif </span><span class="s1">score == </span><span class="s5">&quot;roc_auc&quot;</span><span class="s1">:</span>
                    <span class="s1">dec = clf.decision_function(X[test])</span>
                    <span class="s1">correct_score = roc_auc_score(y[test]</span><span class="s2">, </span><span class="s1">dec)</span>
                <span class="s1">assert_almost_equal(correct_score</span><span class="s2">, </span><span class="s1">cv_scores[i])</span>


<span class="s2">def </span><span class="s1">test_pickle():</span>
    <span class="s3"># Test that a fit search can be pickled</span>
    <span class="s1">clf = MockClassifier()</span>
    <span class="s1">grid_search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">refit=</span><span class="s2">True, </span><span class="s1">cv=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">grid_search_pickled = pickle.loads(pickle.dumps(grid_search))</span>
    <span class="s1">assert_array_almost_equal(grid_search.predict(X)</span><span class="s2">, </span><span class="s1">grid_search_pickled.predict(X))</span>

    <span class="s1">random_search = RandomizedSearchCV(</span>
        <span class="s1">clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">refit=</span><span class="s2">True, </span><span class="s1">n_iter=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">3</span>
    <span class="s1">)</span>
    <span class="s1">random_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">random_search_pickled = pickle.loads(pickle.dumps(random_search))</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">random_search.predict(X)</span><span class="s2">, </span><span class="s1">random_search_pickled.predict(X)</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_grid_search_with_multioutput_data():</span>
    <span class="s3"># Test search with multi-output estimator</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_multilabel_classification(return_indicator=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">est_parameters = {</span><span class="s5">&quot;max_depth&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]}</span>
    <span class="s1">cv = KFold()</span>

    <span class="s1">estimators = [</span>
        <span class="s1">DecisionTreeRegressor(random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span>

    <span class="s3"># Test with grid search cv</span>
    <span class="s2">for </span><span class="s1">est </span><span class="s2">in </span><span class="s1">estimators:</span>
        <span class="s1">grid_search = GridSearchCV(est</span><span class="s2">, </span><span class="s1">est_parameters</span><span class="s2">, </span><span class="s1">cv=cv)</span>
        <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">res_params = grid_search.cv_results_[</span><span class="s5">&quot;params&quot;</span><span class="s1">]</span>
        <span class="s2">for </span><span class="s1">cand_i </span><span class="s2">in </span><span class="s1">range(len(res_params)):</span>
            <span class="s1">est.set_params(**res_params[cand_i])</span>

            <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">(train</span><span class="s2">, </span><span class="s1">test) </span><span class="s2">in </span><span class="s1">enumerate(cv.split(X</span><span class="s2">, </span><span class="s1">y)):</span>
                <span class="s1">est.fit(X[train]</span><span class="s2">, </span><span class="s1">y[train])</span>
                <span class="s1">correct_score = est.score(X[test]</span><span class="s2">, </span><span class="s1">y[test])</span>
                <span class="s1">assert_almost_equal(</span>
                    <span class="s1">correct_score</span><span class="s2">,</span>
                    <span class="s1">grid_search.cv_results_[</span><span class="s5">&quot;split%d_test_score&quot; </span><span class="s1">% i][cand_i]</span><span class="s2">,</span>
                <span class="s1">)</span>

    <span class="s3"># Test with a randomized search</span>
    <span class="s2">for </span><span class="s1">est </span><span class="s2">in </span><span class="s1">estimators:</span>
        <span class="s1">random_search = RandomizedSearchCV(est</span><span class="s2">, </span><span class="s1">est_parameters</span><span class="s2">, </span><span class="s1">cv=cv</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">3</span><span class="s1">)</span>
        <span class="s1">random_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">res_params = random_search.cv_results_[</span><span class="s5">&quot;params&quot;</span><span class="s1">]</span>
        <span class="s2">for </span><span class="s1">cand_i </span><span class="s2">in </span><span class="s1">range(len(res_params)):</span>
            <span class="s1">est.set_params(**res_params[cand_i])</span>

            <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">(train</span><span class="s2">, </span><span class="s1">test) </span><span class="s2">in </span><span class="s1">enumerate(cv.split(X</span><span class="s2">, </span><span class="s1">y)):</span>
                <span class="s1">est.fit(X[train]</span><span class="s2">, </span><span class="s1">y[train])</span>
                <span class="s1">correct_score = est.score(X[test]</span><span class="s2">, </span><span class="s1">y[test])</span>
                <span class="s1">assert_almost_equal(</span>
                    <span class="s1">correct_score</span><span class="s2">,</span>
                    <span class="s1">random_search.cv_results_[</span><span class="s5">&quot;split%d_test_score&quot; </span><span class="s1">% i][cand_i]</span><span class="s2">,</span>
                <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_predict_proba_disabled():</span>
    <span class="s3"># Test predict_proba when disabled on estimator.</span>
    <span class="s1">X = np.arange(</span><span class="s4">20</span><span class="s1">).reshape(</span><span class="s4">5</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">clf = SVC(probability=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">gs = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{}</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">2</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(gs</span><span class="s2">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_grid_search_allows_nans():</span>
    <span class="s3"># Test GridSearchCV with SimpleImputer</span>
    <span class="s1">X = np.arange(</span><span class="s4">20</span><span class="s2">, </span><span class="s1">dtype=np.float64).reshape(</span><span class="s4">5</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">X[</span><span class="s4">2</span><span class="s2">, </span><span class="s1">:] = np.nan</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">p = Pipeline(</span>
        <span class="s1">[</span>
            <span class="s1">(</span><span class="s5">&quot;imputer&quot;</span><span class="s2">, </span><span class="s1">SimpleImputer(strategy=</span><span class="s5">&quot;mean&quot;</span><span class="s2">, </span><span class="s1">missing_values=np.nan))</span><span class="s2">,</span>
            <span class="s1">(</span><span class="s5">&quot;classifier&quot;</span><span class="s2">, </span><span class="s1">MockClassifier())</span><span class="s2">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">GridSearchCV(p</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;classifier__foo_param&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">2</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">class </span><span class="s1">FailingClassifier(BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Classifier that raises a ValueError on fit()&quot;&quot;&quot;</span>

    <span class="s1">FAILING_PARAMETER = </span><span class="s4">2</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">parameter=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self.parameter = parameter</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">if </span><span class="s1">self.parameter == FailingClassifier.FAILING_PARAMETER:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Failing classifier failed as required&quot;</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s2">return </span><span class="s1">np.zeros(X.shape[</span><span class="s4">0</span><span class="s1">])</span>

    <span class="s2">def </span><span class="s1">score(self</span><span class="s2">, </span><span class="s1">X=</span><span class="s2">None, </span><span class="s1">Y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s4">0.0</span>


<span class="s2">def </span><span class="s1">test_grid_search_failing_classifier():</span>
    <span class="s3"># GridSearchCV with on_error != 'raise'</span>
    <span class="s3"># Ensures that a warning is raised and score reset where appropriate.</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">clf = FailingClassifier()</span>

    <span class="s3"># refit=False because we only want to check that errors caused by fits</span>
    <span class="s3"># to individual folds will be caught and warnings raised instead. If</span>
    <span class="s3"># refit was done, then an exception would be raised on refit and not</span>
    <span class="s3"># caught by grid_search (expected behavior), and this would cause an</span>
    <span class="s3"># error in this test.</span>
    <span class="s1">gs = GridSearchCV(</span>
        <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">[{</span><span class="s5">&quot;parameter&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]}]</span><span class="s2">,</span>
        <span class="s1">scoring=</span><span class="s5">&quot;accuracy&quot;</span><span class="s2">,</span>
        <span class="s1">refit=</span><span class="s2">False,</span>
        <span class="s1">error_score=</span><span class="s4">0.0</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">warning_message = re.compile(</span>
        <span class="s5">&quot;5 fits failed.+total of 15.+The score on these&quot;</span>
        <span class="s5">r&quot; train-test partitions for these parameters will be set to 0\.0.+&quot;</span>
        <span class="s5">&quot;5 fits failed with the following error.+ValueError.+Failing classifier failed&quot;</span>
        <span class="s5">&quot; as required&quot;</span><span class="s2">,</span>
        <span class="s1">flags=re.DOTALL</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(FitFailedWarning</span><span class="s2">, </span><span class="s1">match=warning_message):</span>
        <span class="s1">gs.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">n_candidates = len(gs.cv_results_[</span><span class="s5">&quot;params&quot;</span><span class="s1">])</span>

    <span class="s3"># Ensure that grid scores were set to zero as required for those fits</span>
    <span class="s3"># that are expected to fail.</span>
    <span class="s2">def </span><span class="s1">get_cand_scores(i):</span>
        <span class="s2">return </span><span class="s1">np.array(</span>
            <span class="s1">[gs.cv_results_[</span><span class="s5">&quot;split%d_test_score&quot; </span><span class="s1">% s][i] </span><span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">range(gs.n_splits_)]</span>
        <span class="s1">)</span>

    <span class="s2">assert </span><span class="s1">all(</span>
        <span class="s1">(</span>
            <span class="s1">np.all(get_cand_scores(cand_i) == </span><span class="s4">0.0</span><span class="s1">)</span>
            <span class="s2">for </span><span class="s1">cand_i </span><span class="s2">in </span><span class="s1">range(n_candidates)</span>
            <span class="s2">if </span><span class="s1">gs.cv_results_[</span><span class="s5">&quot;param_parameter&quot;</span><span class="s1">][cand_i]</span>
            <span class="s1">== FailingClassifier.FAILING_PARAMETER</span>
        <span class="s1">)</span>
    <span class="s1">)</span>

    <span class="s1">gs = GridSearchCV(</span>
        <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">[{</span><span class="s5">&quot;parameter&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]}]</span><span class="s2">,</span>
        <span class="s1">scoring=</span><span class="s5">&quot;accuracy&quot;</span><span class="s2">,</span>
        <span class="s1">refit=</span><span class="s2">False,</span>
        <span class="s1">error_score=float(</span><span class="s5">&quot;nan&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">warning_message = re.compile(</span>
        <span class="s5">&quot;5 fits failed.+total of 15.+The score on these&quot;</span>
        <span class="s5">r&quot; train-test partitions for these parameters will be set to nan.+&quot;</span>
        <span class="s5">&quot;5 fits failed with the following error.+ValueError.+Failing classifier failed&quot;</span>
        <span class="s5">&quot; as required&quot;</span><span class="s2">,</span>
        <span class="s1">flags=re.DOTALL</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(FitFailedWarning</span><span class="s2">, </span><span class="s1">match=warning_message):</span>
        <span class="s1">gs.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">n_candidates = len(gs.cv_results_[</span><span class="s5">&quot;params&quot;</span><span class="s1">])</span>
    <span class="s2">assert </span><span class="s1">all(</span>
        <span class="s1">np.all(np.isnan(get_cand_scores(cand_i)))</span>
        <span class="s2">for </span><span class="s1">cand_i </span><span class="s2">in </span><span class="s1">range(n_candidates)</span>
        <span class="s2">if </span><span class="s1">gs.cv_results_[</span><span class="s5">&quot;param_parameter&quot;</span><span class="s1">][cand_i]</span>
        <span class="s1">== FailingClassifier.FAILING_PARAMETER</span>
    <span class="s1">)</span>

    <span class="s1">ranks = gs.cv_results_[</span><span class="s5">&quot;rank_test_score&quot;</span><span class="s1">]</span>

    <span class="s3"># Check that succeeded estimators have lower ranks</span>
    <span class="s2">assert </span><span class="s1">ranks[</span><span class="s4">0</span><span class="s1">] &lt;= </span><span class="s4">2 </span><span class="s2">and </span><span class="s1">ranks[</span><span class="s4">1</span><span class="s1">] &lt;= </span><span class="s4">2</span>
    <span class="s3"># Check that failed estimator has the highest rank</span>
    <span class="s2">assert </span><span class="s1">ranks[clf.FAILING_PARAMETER] == </span><span class="s4">3</span>
    <span class="s2">assert </span><span class="s1">gs.best_index_ != clf.FAILING_PARAMETER</span>


<span class="s2">def </span><span class="s1">test_grid_search_classifier_all_fits_fail():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">clf = FailingClassifier()</span>

    <span class="s1">gs = GridSearchCV(</span>
        <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">[{</span><span class="s5">&quot;parameter&quot;</span><span class="s1">: [FailingClassifier.FAILING_PARAMETER] * </span><span class="s4">3</span><span class="s1">}]</span><span class="s2">,</span>
        <span class="s1">error_score=</span><span class="s4">0.0</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">warning_message = re.compile(</span>
        <span class="s1">(</span>
            <span class="s5">&quot;All the 15 fits failed.+15 fits failed with the following&quot;</span>
            <span class="s5">&quot; error.+ValueError.+Failing classifier failed as required&quot;</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">flags=re.DOTALL</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=warning_message):</span>
        <span class="s1">gs.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_grid_search_failing_classifier_raise():</span>
    <span class="s3"># GridSearchCV with on_error == 'raise' raises the error</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">clf = FailingClassifier()</span>

    <span class="s3"># refit=False because we want to test the behaviour of the grid search part</span>
    <span class="s1">gs = GridSearchCV(</span>
        <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">[{</span><span class="s5">&quot;parameter&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]}]</span><span class="s2">,</span>
        <span class="s1">scoring=</span><span class="s5">&quot;accuracy&quot;</span><span class="s2">,</span>
        <span class="s1">refit=</span><span class="s2">False,</span>
        <span class="s1">error_score=</span><span class="s5">&quot;raise&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s3"># FailingClassifier issues a ValueError so this is what we look for.</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">gs.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_parameters_sampler_replacement():</span>
    <span class="s3"># raise warning if n_iter is bigger than total parameter space</span>
    <span class="s1">params = [</span>
        <span class="s1">{</span><span class="s5">&quot;first&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;second&quot;</span><span class="s1">: [</span><span class="s5">&quot;a&quot;</span><span class="s2">, </span><span class="s5">&quot;b&quot;</span><span class="s2">, </span><span class="s5">&quot;c&quot;</span><span class="s1">]}</span><span class="s2">,</span>
        <span class="s1">{</span><span class="s5">&quot;third&quot;</span><span class="s1">: [</span><span class="s5">&quot;two&quot;</span><span class="s2">, </span><span class="s5">&quot;values&quot;</span><span class="s1">]}</span><span class="s2">,</span>
    <span class="s1">]</span>
    <span class="s1">sampler = ParameterSampler(params</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">9</span><span class="s1">)</span>
    <span class="s1">n_iter = </span><span class="s4">9</span>
    <span class="s1">grid_size = </span><span class="s4">8</span>
    <span class="s1">expected_warning = (</span>
        <span class="s5">&quot;The total space of parameters %d is smaller &quot;</span>
        <span class="s5">&quot;than n_iter=%d. Running %d iterations. For &quot;</span>
        <span class="s5">&quot;exhaustive searches, use GridSearchCV.&quot; </span><span class="s1">% (grid_size</span><span class="s2">, </span><span class="s1">n_iter</span><span class="s2">, </span><span class="s1">grid_size)</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s2">, </span><span class="s1">match=expected_warning):</span>
        <span class="s1">list(sampler)</span>

    <span class="s3"># degenerates to GridSearchCV if n_iter the same as grid_size</span>
    <span class="s1">sampler = ParameterSampler(params</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">8</span><span class="s1">)</span>
    <span class="s1">samples = list(sampler)</span>
    <span class="s2">assert </span><span class="s1">len(samples) == </span><span class="s4">8</span>
    <span class="s2">for </span><span class="s1">values </span><span class="s2">in </span><span class="s1">ParameterGrid(params):</span>
        <span class="s2">assert </span><span class="s1">values </span><span class="s2">in </span><span class="s1">samples</span>
    <span class="s2">assert </span><span class="s1">len(ParameterSampler(params</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">1000</span><span class="s1">)) == </span><span class="s4">8</span>

    <span class="s3"># test sampling without replacement in a large grid</span>
    <span class="s1">params = {</span><span class="s5">&quot;a&quot;</span><span class="s1">: range(</span><span class="s4">10</span><span class="s1">)</span><span class="s2">, </span><span class="s5">&quot;b&quot;</span><span class="s1">: range(</span><span class="s4">10</span><span class="s1">)</span><span class="s2">, </span><span class="s5">&quot;c&quot;</span><span class="s1">: range(</span><span class="s4">10</span><span class="s1">)}</span>
    <span class="s1">sampler = ParameterSampler(params</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">99</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">samples = list(sampler)</span>
    <span class="s2">assert </span><span class="s1">len(samples) == </span><span class="s4">99</span>
    <span class="s1">hashable_samples = [</span><span class="s5">&quot;a%db%dc%d&quot; </span><span class="s1">% (p[</span><span class="s5">&quot;a&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">p[</span><span class="s5">&quot;b&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">p[</span><span class="s5">&quot;c&quot;</span><span class="s1">]) </span><span class="s2">for </span><span class="s1">p </span><span class="s2">in </span><span class="s1">samples]</span>
    <span class="s2">assert </span><span class="s1">len(set(hashable_samples)) == </span><span class="s4">99</span>

    <span class="s3"># doesn't go into infinite loops</span>
    <span class="s1">params_distribution = {</span><span class="s5">&quot;first&quot;</span><span class="s1">: bernoulli(</span><span class="s4">0.5</span><span class="s1">)</span><span class="s2">, </span><span class="s5">&quot;second&quot;</span><span class="s1">: [</span><span class="s5">&quot;a&quot;</span><span class="s2">, </span><span class="s5">&quot;b&quot;</span><span class="s2">, </span><span class="s5">&quot;c&quot;</span><span class="s1">]}</span>
    <span class="s1">sampler = ParameterSampler(params_distribution</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">7</span><span class="s1">)</span>
    <span class="s1">samples = list(sampler)</span>
    <span class="s2">assert </span><span class="s1">len(samples) == </span><span class="s4">7</span>


<span class="s2">def </span><span class="s1">test_stochastic_gradient_loss_param():</span>
    <span class="s3"># Make sure the predict_proba works when loss is specified</span>
    <span class="s3"># as one of the parameters in the param_grid.</span>
    <span class="s1">param_grid = {</span>
        <span class="s5">&quot;loss&quot;</span><span class="s1">: [</span><span class="s5">&quot;log_loss&quot;</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">}</span>
    <span class="s1">X = np.arange(</span><span class="s4">24</span><span class="s1">).reshape(</span><span class="s4">6</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">clf = GridSearchCV(</span>
        <span class="s1">estimator=SGDClassifier(loss=</span><span class="s5">&quot;hinge&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">param_grid=param_grid</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">3</span>
    <span class="s1">)</span>

    <span class="s3"># When the estimator is not fitted, `predict_proba` is not available as the</span>
    <span class="s3"># loss is 'hinge'.</span>
    <span class="s2">assert not </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf.predict_proba(X)</span>
    <span class="s1">clf.predict_log_proba(X)</span>

    <span class="s3"># Make sure `predict_proba` is not available when setting loss=['hinge']</span>
    <span class="s3"># in param_grid</span>
    <span class="s1">param_grid = {</span>
        <span class="s5">&quot;loss&quot;</span><span class="s1">: [</span><span class="s5">&quot;hinge&quot;</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">}</span>
    <span class="s1">clf = GridSearchCV(</span>
        <span class="s1">estimator=SGDClassifier(loss=</span><span class="s5">&quot;hinge&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">param_grid=param_grid</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">3</span>
    <span class="s1">)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_search_train_scores_set_to_false():</span>
    <span class="s1">X = np.arange(</span><span class="s4">6</span><span class="s1">).reshape(</span><span class="s4">6</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">clf = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">gs = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">param_grid={</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.2</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">gs.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_grid_search_cv_splits_consistency():</span>
    <span class="s3"># Check if a one time iterable is accepted as a cv parameter.</span>
    <span class="s1">n_samples = </span><span class="s4">100</span>
    <span class="s1">n_splits = </span><span class="s4">5</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=n_samples</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">gs = GridSearchCV(</span>
        <span class="s1">LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">param_grid={</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.2</span><span class="s2">, </span><span class="s4">0.3</span><span class="s1">]}</span><span class="s2">,</span>
        <span class="s1">cv=OneTimeSplitter(n_splits=n_splits</span><span class="s2">, </span><span class="s1">n_samples=n_samples)</span><span class="s2">,</span>
        <span class="s1">return_train_score=</span><span class="s2">True,</span>
    <span class="s1">)</span>
    <span class="s1">gs.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">gs2 = GridSearchCV(</span>
        <span class="s1">LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">param_grid={</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.2</span><span class="s2">, </span><span class="s4">0.3</span><span class="s1">]}</span><span class="s2">,</span>
        <span class="s1">cv=KFold(n_splits=n_splits)</span><span class="s2">,</span>
        <span class="s1">return_train_score=</span><span class="s2">True,</span>
    <span class="s1">)</span>
    <span class="s1">gs2.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s3"># Give generator as a cv parameter</span>
    <span class="s2">assert </span><span class="s1">isinstance(</span>
        <span class="s1">KFold(n_splits=n_splits</span><span class="s2">, </span><span class="s1">shuffle=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).split(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">,</span>
        <span class="s1">GeneratorType</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">gs3 = GridSearchCV(</span>
        <span class="s1">LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">param_grid={</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.2</span><span class="s2">, </span><span class="s4">0.3</span><span class="s1">]}</span><span class="s2">,</span>
        <span class="s1">cv=KFold(n_splits=n_splits</span><span class="s2">, </span><span class="s1">shuffle=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).split(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">,</span>
        <span class="s1">return_train_score=</span><span class="s2">True,</span>
    <span class="s1">)</span>
    <span class="s1">gs3.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">gs4 = GridSearchCV(</span>
        <span class="s1">LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">param_grid={</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.2</span><span class="s2">, </span><span class="s4">0.3</span><span class="s1">]}</span><span class="s2">,</span>
        <span class="s1">cv=KFold(n_splits=n_splits</span><span class="s2">, </span><span class="s1">shuffle=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">return_train_score=</span><span class="s2">True,</span>
    <span class="s1">)</span>
    <span class="s1">gs4.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">def </span><span class="s1">_pop_time_keys(cv_results):</span>
        <span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">(</span>
            <span class="s5">&quot;mean_fit_time&quot;</span><span class="s2">,</span>
            <span class="s5">&quot;std_fit_time&quot;</span><span class="s2">,</span>
            <span class="s5">&quot;mean_score_time&quot;</span><span class="s2">,</span>
            <span class="s5">&quot;std_score_time&quot;</span><span class="s2">,</span>
        <span class="s1">):</span>
            <span class="s1">cv_results.pop(key)</span>
        <span class="s2">return </span><span class="s1">cv_results</span>

    <span class="s3"># Check if generators are supported as cv and</span>
    <span class="s3"># that the splits are consistent</span>
    <span class="s1">np.testing.assert_equal(</span>
        <span class="s1">_pop_time_keys(gs3.cv_results_)</span><span class="s2">, </span><span class="s1">_pop_time_keys(gs4.cv_results_)</span>
    <span class="s1">)</span>

    <span class="s3"># OneTimeSplitter is a non-re-entrant cv where split can be called only</span>
    <span class="s3"># once if ``cv.split`` is called once per param setting in GridSearchCV.fit</span>
    <span class="s3"># the 2nd and 3rd parameter will not be evaluated as no train/test indices</span>
    <span class="s3"># will be generated for the 2nd and subsequent cv.split calls.</span>
    <span class="s3"># This is a check to make sure cv.split is not called once per param</span>
    <span class="s3"># setting.</span>
    <span class="s1">np.testing.assert_equal(</span>
        <span class="s1">{k: v </span><span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">v </span><span class="s2">in </span><span class="s1">gs.cv_results_.items() </span><span class="s2">if not </span><span class="s1">k.endswith(</span><span class="s5">&quot;_time&quot;</span><span class="s1">)}</span><span class="s2">,</span>
        <span class="s1">{k: v </span><span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">v </span><span class="s2">in </span><span class="s1">gs2.cv_results_.items() </span><span class="s2">if not </span><span class="s1">k.endswith(</span><span class="s5">&quot;_time&quot;</span><span class="s1">)}</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s3"># Check consistency of folds across the parameters</span>
    <span class="s1">gs = GridSearchCV(</span>
        <span class="s1">LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">param_grid={</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.2</span><span class="s2">, </span><span class="s4">0.2</span><span class="s1">]}</span><span class="s2">,</span>
        <span class="s1">cv=KFold(n_splits=n_splits</span><span class="s2">, </span><span class="s1">shuffle=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">return_train_score=</span><span class="s2">True,</span>
    <span class="s1">)</span>
    <span class="s1">gs.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s3"># As the first two param settings (C=0.1) and the next two param</span>
    <span class="s3"># settings (C=0.2) are same, the test and train scores must also be</span>
    <span class="s3"># same as long as the same train/test indices are generated for all</span>
    <span class="s3"># the cv splits, for both param setting</span>
    <span class="s2">for </span><span class="s1">score_type </span><span class="s2">in </span><span class="s1">(</span><span class="s5">&quot;train&quot;</span><span class="s2">, </span><span class="s5">&quot;test&quot;</span><span class="s1">):</span>
        <span class="s1">per_param_scores = {}</span>
        <span class="s2">for </span><span class="s1">param_i </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">4</span><span class="s1">):</span>
            <span class="s1">per_param_scores[param_i] = [</span>
                <span class="s1">gs.cv_results_[</span><span class="s5">&quot;split%d_%s_score&quot; </span><span class="s1">% (s</span><span class="s2">, </span><span class="s1">score_type)][param_i]</span>
                <span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">5</span><span class="s1">)</span>
            <span class="s1">]</span>

        <span class="s1">assert_array_almost_equal(per_param_scores[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">per_param_scores[</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s1">assert_array_almost_equal(per_param_scores[</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">per_param_scores[</span><span class="s4">3</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_transform_inverse_transform_round_trip():</span>
    <span class="s1">clf = MockClassifier()</span>
    <span class="s1">grid_search = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;foo_param&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">verbose=</span><span class="s4">3</span><span class="s1">)</span>

    <span class="s1">grid_search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">X_round_trip = grid_search.inverse_transform(grid_search.transform(X))</span>
    <span class="s1">assert_array_equal(X</span><span class="s2">, </span><span class="s1">X_round_trip)</span>


<span class="s2">def </span><span class="s1">test_custom_run_search():</span>
    <span class="s2">def </span><span class="s1">check_results(results</span><span class="s2">, </span><span class="s1">gscv):</span>
        <span class="s1">exp_results = gscv.cv_results_</span>
        <span class="s2">assert </span><span class="s1">sorted(results.keys()) == sorted(exp_results)</span>
        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">results:</span>
            <span class="s2">if not </span><span class="s1">k.endswith(</span><span class="s5">&quot;_time&quot;</span><span class="s1">):</span>
                <span class="s3"># XXX: results['params'] is a list :|</span>
                <span class="s1">results[k] = np.asanyarray(results[k])</span>
                <span class="s2">if </span><span class="s1">results[k].dtype.kind == </span><span class="s5">&quot;O&quot;</span><span class="s1">:</span>
                    <span class="s1">assert_array_equal(</span>
                        <span class="s1">exp_results[k]</span><span class="s2">, </span><span class="s1">results[k]</span><span class="s2">, </span><span class="s1">err_msg=</span><span class="s5">&quot;Checking &quot; </span><span class="s1">+ k</span>
                    <span class="s1">)</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">assert_allclose(exp_results[k]</span><span class="s2">, </span><span class="s1">results[k]</span><span class="s2">, </span><span class="s1">err_msg=</span><span class="s5">&quot;Checking &quot; </span><span class="s1">+ k)</span>

    <span class="s2">def </span><span class="s1">fit_grid(param_grid):</span>
        <span class="s2">return </span><span class="s1">GridSearchCV(clf</span><span class="s2">, </span><span class="s1">param_grid</span><span class="s2">, </span><span class="s1">return_train_score=</span><span class="s2">True</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">class </span><span class="s1">CustomSearchCV(BaseSearchCV):</span>
        <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">estimator</span><span class="s2">, </span><span class="s1">**kwargs):</span>
            <span class="s1">super().__init__(estimator</span><span class="s2">, </span><span class="s1">**kwargs)</span>

        <span class="s2">def </span><span class="s1">_run_search(self</span><span class="s2">, </span><span class="s1">evaluate):</span>
            <span class="s1">results = evaluate([{</span><span class="s5">&quot;max_depth&quot;</span><span class="s1">: </span><span class="s4">1</span><span class="s1">}</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;max_depth&quot;</span><span class="s1">: </span><span class="s4">2</span><span class="s1">}])</span>
            <span class="s1">check_results(results</span><span class="s2">, </span><span class="s1">fit_grid({</span><span class="s5">&quot;max_depth&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]}))</span>
            <span class="s1">results = evaluate([{</span><span class="s5">&quot;min_samples_split&quot;</span><span class="s1">: </span><span class="s4">5</span><span class="s1">}</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;min_samples_split&quot;</span><span class="s1">: </span><span class="s4">10</span><span class="s1">}])</span>
            <span class="s1">check_results(</span>
                <span class="s1">results</span><span class="s2">,</span>
                <span class="s1">fit_grid([{</span><span class="s5">&quot;max_depth&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;min_samples_split&quot;</span><span class="s1">: [</span><span class="s4">5</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]}])</span><span class="s2">,</span>
            <span class="s1">)</span>

    <span class="s3"># Using regressor to make sure each score differs</span>
    <span class="s1">clf = DecisionTreeRegressor(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">n_informative=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">mycv = CustomSearchCV(clf</span><span class="s2">, </span><span class="s1">return_train_score=</span><span class="s2">True</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">gscv = fit_grid([{</span><span class="s5">&quot;max_depth&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;min_samples_split&quot;</span><span class="s1">: [</span><span class="s4">5</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]}])</span>

    <span class="s1">results = mycv.cv_results_</span>
    <span class="s1">check_results(results</span><span class="s2">, </span><span class="s1">gscv)</span>
    <span class="s2">for </span><span class="s1">attr </span><span class="s2">in </span><span class="s1">dir(gscv):</span>
        <span class="s2">if </span><span class="s1">(</span>
            <span class="s1">attr[</span><span class="s4">0</span><span class="s1">].islower()</span>
            <span class="s2">and </span><span class="s1">attr[-</span><span class="s4">1</span><span class="s1">:] == </span><span class="s5">&quot;_&quot;</span>
            <span class="s2">and </span><span class="s1">attr</span>
            <span class="s2">not in </span><span class="s1">{</span>
                <span class="s5">&quot;cv_results_&quot;</span><span class="s2">,</span>
                <span class="s5">&quot;best_estimator_&quot;</span><span class="s2">,</span>
                <span class="s5">&quot;refit_time_&quot;</span><span class="s2">,</span>
                <span class="s5">&quot;classes_&quot;</span><span class="s2">,</span>
                <span class="s5">&quot;scorer_&quot;</span><span class="s2">,</span>
            <span class="s1">}</span>
        <span class="s1">):</span>
            <span class="s2">assert </span><span class="s1">getattr(gscv</span><span class="s2">, </span><span class="s1">attr) == getattr(mycv</span><span class="s2">, </span><span class="s1">attr)</span><span class="s2">, </span><span class="s1">(</span>
                <span class="s5">&quot;Attribute %s not equal&quot; </span><span class="s1">% attr</span>
            <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test__custom_fit_no_run_search():</span>
    <span class="s2">class </span><span class="s1">NoRunSearchSearchCV(BaseSearchCV):</span>
        <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">estimator</span><span class="s2">, </span><span class="s1">**kwargs):</span>
            <span class="s1">super().__init__(estimator</span><span class="s2">, </span><span class="s1">**kwargs)</span>

        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None, </span><span class="s1">groups=</span><span class="s2">None, </span><span class="s1">**fit_params):</span>
            <span class="s2">return </span><span class="s1">self</span>

    <span class="s3"># this should not raise any exceptions</span>
    <span class="s1">NoRunSearchSearchCV(SVC()).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">class </span><span class="s1">BadSearchCV(BaseSearchCV):</span>
        <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">estimator</span><span class="s2">, </span><span class="s1">**kwargs):</span>
            <span class="s1">super().__init__(estimator</span><span class="s2">, </span><span class="s1">**kwargs)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(NotImplementedError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;_run_search not implemented.&quot;</span><span class="s1">):</span>
        <span class="s3"># this should raise a NotImplementedError</span>
        <span class="s1">BadSearchCV(SVC()).fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_empty_cv_iterator_error():</span>
    <span class="s3"># Use global X, y</span>

    <span class="s3"># create cv</span>
    <span class="s1">cv = KFold(n_splits=</span><span class="s4">3</span><span class="s1">).split(X)</span>

    <span class="s3"># pop all of it, this should cause the expected ValueError</span>
    <span class="s1">[u </span><span class="s2">for </span><span class="s1">u </span><span class="s2">in </span><span class="s1">cv]</span>
    <span class="s3"># cv is empty now</span>

    <span class="s1">train_size = </span><span class="s4">100</span>
    <span class="s1">ridge = RandomizedSearchCV(Ridge()</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: [</span><span class="s4">1e-3</span><span class="s2">, </span><span class="s4">1e-2</span><span class="s2">, </span><span class="s4">1e-1</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">cv=cv</span><span class="s2">, </span><span class="s1">n_jobs=</span><span class="s4">4</span><span class="s1">)</span>

    <span class="s3"># assert that this raises an error</span>
    <span class="s2">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s2">,</span>
        <span class="s1">match=(</span>
            <span class="s5">&quot;No fits were performed. &quot;</span>
            <span class="s5">&quot;Was the CV iterator empty</span><span class="s2">\\</span><span class="s5">? &quot;</span>
            <span class="s5">&quot;Were there no candidates</span><span class="s2">\\</span><span class="s5">?&quot;</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">ridge.fit(X[:train_size]</span><span class="s2">, </span><span class="s1">y[:train_size])</span>


<span class="s2">def </span><span class="s1">test_random_search_bad_cv():</span>
    <span class="s3"># Use global X, y</span>

    <span class="s2">class </span><span class="s1">BrokenKFold(KFold):</span>
        <span class="s2">def </span><span class="s1">get_n_splits(self</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">**kw):</span>
            <span class="s2">return </span><span class="s4">1</span>

    <span class="s3"># create bad cv</span>
    <span class="s1">cv = BrokenKFold(n_splits=</span><span class="s4">3</span><span class="s1">)</span>

    <span class="s1">train_size = </span><span class="s4">100</span>
    <span class="s1">ridge = RandomizedSearchCV(Ridge()</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: [</span><span class="s4">1e-3</span><span class="s2">, </span><span class="s4">1e-2</span><span class="s2">, </span><span class="s4">1e-1</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">cv=cv</span><span class="s2">, </span><span class="s1">n_jobs=</span><span class="s4">4</span><span class="s1">)</span>

    <span class="s3"># assert that this raises an error</span>
    <span class="s2">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s2">,</span>
        <span class="s1">match=(</span>
            <span class="s5">&quot;cv.split and cv.get_n_splits returned &quot;</span>
            <span class="s5">&quot;inconsistent results. Expected </span><span class="s2">\\</span><span class="s5">d+ &quot;</span>
            <span class="s5">&quot;splits, got </span><span class="s2">\\</span><span class="s5">d+&quot;</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">ridge.fit(X[:train_size]</span><span class="s2">, </span><span class="s1">y[:train_size])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;return_train_score&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">False, True</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;SearchCV, specialized_params&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(GridSearchCV</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;param_grid&quot;</span><span class="s1">: {</span><span class="s5">&quot;max_depth&quot;</span><span class="s1">: [</span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">8</span><span class="s1">]}})</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">RandomizedSearchCV</span><span class="s2">,</span>
            <span class="s1">{</span><span class="s5">&quot;param_distributions&quot;</span><span class="s1">: {</span><span class="s5">&quot;max_depth&quot;</span><span class="s1">: [</span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">8</span><span class="s1">]}</span><span class="s2">, </span><span class="s5">&quot;n_iter&quot;</span><span class="s1">: </span><span class="s4">4</span><span class="s1">}</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_searchcv_raise_warning_with_non_finite_score(</span>
    <span class="s1">SearchCV</span><span class="s2">, </span><span class="s1">specialized_params</span><span class="s2">, </span><span class="s1">return_train_score</span>
<span class="s1">):</span>
    <span class="s3"># Non-regression test for:</span>
    <span class="s3"># https://github.com/scikit-learn/scikit-learn/issues/10529</span>
    <span class="s3"># Check that we raise a UserWarning when a non-finite score is</span>
    <span class="s3"># computed in the SearchCV</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_classes=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s2">class </span><span class="s1">FailingScorer:</span>
        <span class="s0">&quot;&quot;&quot;Scorer that will fail for some split but not all.&quot;&quot;&quot;</span>

        <span class="s2">def </span><span class="s1">__init__(self):</span>
            <span class="s1">self.n_counts = </span><span class="s4">0</span>

        <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">estimator</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
            <span class="s1">self.n_counts += </span><span class="s4">1</span>
            <span class="s2">if </span><span class="s1">self.n_counts % </span><span class="s4">5 </span><span class="s1">== </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">np.nan</span>
            <span class="s2">return </span><span class="s4">1</span>

    <span class="s1">grid = SearchCV(</span>
        <span class="s1">DecisionTreeClassifier()</span><span class="s2">,</span>
        <span class="s1">scoring=FailingScorer()</span><span class="s2">,</span>
        <span class="s1">cv=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">return_train_score=return_train_score</span><span class="s2">,</span>
        <span class="s1">**specialized_params</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s2">with </span><span class="s1">pytest.warns(UserWarning) </span><span class="s2">as </span><span class="s1">warn_msg:</span>
        <span class="s1">grid.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">set_with_warning = [</span><span class="s5">&quot;test&quot;</span><span class="s2">, </span><span class="s5">&quot;train&quot;</span><span class="s1">] </span><span class="s2">if </span><span class="s1">return_train_score </span><span class="s2">else </span><span class="s1">[</span><span class="s5">&quot;test&quot;</span><span class="s1">]</span>
    <span class="s2">assert </span><span class="s1">len(warn_msg) == len(set_with_warning)</span>
    <span class="s2">for </span><span class="s1">msg</span><span class="s2">, </span><span class="s1">dataset </span><span class="s2">in </span><span class="s1">zip(warn_msg</span><span class="s2">, </span><span class="s1">set_with_warning):</span>
        <span class="s2">assert </span><span class="s5">f&quot;One or more of the </span><span class="s2">{</span><span class="s1">dataset</span><span class="s2">} </span><span class="s5">scores are non-finite&quot; </span><span class="s2">in </span><span class="s1">str(msg.message)</span>

    <span class="s3"># all non-finite scores should be equally ranked last</span>
    <span class="s1">last_rank = grid.cv_results_[</span><span class="s5">&quot;rank_test_score&quot;</span><span class="s1">].max()</span>
    <span class="s1">non_finite_mask = np.isnan(grid.cv_results_[</span><span class="s5">&quot;mean_test_score&quot;</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(grid.cv_results_[</span><span class="s5">&quot;rank_test_score&quot;</span><span class="s1">][non_finite_mask]</span><span class="s2">, </span><span class="s1">last_rank)</span>
    <span class="s3"># all finite scores should be better ranked than the non-finite scores</span>
    <span class="s2">assert </span><span class="s1">np.all(grid.cv_results_[</span><span class="s5">&quot;rank_test_score&quot;</span><span class="s1">][~non_finite_mask] &lt; last_rank)</span>


<span class="s2">def </span><span class="s1">test_callable_multimetric_confusion_matrix():</span>
    <span class="s3"># Test callable with many metrics inserts the correct names and metrics</span>
    <span class="s3"># into the search cv object</span>
    <span class="s2">def </span><span class="s1">custom_scorer(clf</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s1">y_pred = clf.predict(X)</span>
        <span class="s1">cm = confusion_matrix(y</span><span class="s2">, </span><span class="s1">y_pred)</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s5">&quot;tn&quot;</span><span class="s1">: cm[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;fp&quot;</span><span class="s1">: cm[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;fn&quot;</span><span class="s1">: cm[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;tp&quot;</span><span class="s1">: cm[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">40</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">est = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">search = GridSearchCV(est</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">scoring=custom_scorer</span><span class="s2">, </span><span class="s1">refit=</span><span class="s5">&quot;fp&quot;</span><span class="s1">)</span>

    <span class="s1">search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">score_names = [</span><span class="s5">&quot;tn&quot;</span><span class="s2">, </span><span class="s5">&quot;fp&quot;</span><span class="s2">, </span><span class="s5">&quot;fn&quot;</span><span class="s2">, </span><span class="s5">&quot;tp&quot;</span><span class="s1">]</span>
    <span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">score_names:</span>
        <span class="s2">assert </span><span class="s5">&quot;mean_test_{}&quot;</span><span class="s1">.format(name) </span><span class="s2">in </span><span class="s1">search.cv_results_</span>

    <span class="s1">y_pred = search.predict(X)</span>
    <span class="s1">cm = confusion_matrix(y</span><span class="s2">, </span><span class="s1">y_pred)</span>
    <span class="s2">assert </span><span class="s1">search.score(X</span><span class="s2">, </span><span class="s1">y) == pytest.approx(cm[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_callable_multimetric_same_as_list_of_strings():</span>
    <span class="s3"># Test callable multimetric is the same as a list of strings</span>
    <span class="s2">def </span><span class="s1">custom_scorer(est</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s1">y_pred = est.predict(X)</span>
        <span class="s2">return </span><span class="s1">{</span>
            <span class="s5">&quot;recall&quot;</span><span class="s1">: recall_score(y</span><span class="s2">, </span><span class="s1">y_pred)</span><span class="s2">,</span>
            <span class="s5">&quot;accuracy&quot;</span><span class="s1">: accuracy_score(y</span><span class="s2">, </span><span class="s1">y_pred)</span><span class="s2">,</span>
        <span class="s1">}</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">40</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">est = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">search_callable = GridSearchCV(</span>
        <span class="s1">est</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">scoring=custom_scorer</span><span class="s2">, </span><span class="s1">refit=</span><span class="s5">&quot;recall&quot;</span>
    <span class="s1">)</span>
    <span class="s1">search_str = GridSearchCV(</span>
        <span class="s1">est</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">scoring=[</span><span class="s5">&quot;recall&quot;</span><span class="s2">, </span><span class="s5">&quot;accuracy&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">refit=</span><span class="s5">&quot;recall&quot;</span>
    <span class="s1">)</span>

    <span class="s1">search_callable.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">search_str.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">assert </span><span class="s1">search_callable.best_score_ == pytest.approx(search_str.best_score_)</span>
    <span class="s2">assert </span><span class="s1">search_callable.best_index_ == search_str.best_index_</span>
    <span class="s2">assert </span><span class="s1">search_callable.score(X</span><span class="s2">, </span><span class="s1">y) == pytest.approx(search_str.score(X</span><span class="s2">, </span><span class="s1">y))</span>


<span class="s2">def </span><span class="s1">test_callable_single_metric_same_as_single_string():</span>
    <span class="s3"># Tests callable scorer is the same as scoring with a single string</span>
    <span class="s2">def </span><span class="s1">custom_scorer(est</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s1">y_pred = est.predict(X)</span>
        <span class="s2">return </span><span class="s1">recall_score(y</span><span class="s2">, </span><span class="s1">y_pred)</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">40</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">est = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">search_callable = GridSearchCV(</span>
        <span class="s1">est</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">scoring=custom_scorer</span><span class="s2">, </span><span class="s1">refit=</span><span class="s2">True</span>
    <span class="s1">)</span>
    <span class="s1">search_str = GridSearchCV(est</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s5">&quot;recall&quot;</span><span class="s2">, </span><span class="s1">refit=</span><span class="s5">&quot;recall&quot;</span><span class="s1">)</span>
    <span class="s1">search_list_str = GridSearchCV(</span>
        <span class="s1">est</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span><span class="s2">, </span><span class="s1">scoring=[</span><span class="s5">&quot;recall&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">refit=</span><span class="s5">&quot;recall&quot;</span>
    <span class="s1">)</span>
    <span class="s1">search_callable.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">search_str.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">search_list_str.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">assert </span><span class="s1">search_callable.best_score_ == pytest.approx(search_str.best_score_)</span>
    <span class="s2">assert </span><span class="s1">search_callable.best_index_ == search_str.best_index_</span>
    <span class="s2">assert </span><span class="s1">search_callable.score(X</span><span class="s2">, </span><span class="s1">y) == pytest.approx(search_str.score(X</span><span class="s2">, </span><span class="s1">y))</span>

    <span class="s2">assert </span><span class="s1">search_list_str.best_score_ == pytest.approx(search_str.best_score_)</span>
    <span class="s2">assert </span><span class="s1">search_list_str.best_index_ == search_str.best_index_</span>
    <span class="s2">assert </span><span class="s1">search_list_str.score(X</span><span class="s2">, </span><span class="s1">y) == pytest.approx(search_str.score(X</span><span class="s2">, </span><span class="s1">y))</span>


<span class="s2">def </span><span class="s1">test_callable_multimetric_error_on_invalid_key():</span>
    <span class="s3"># Raises when the callable scorer does not return a dict with `refit` key.</span>
    <span class="s2">def </span><span class="s1">bad_scorer(est</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s5">&quot;bad_name&quot;</span><span class="s1">: </span><span class="s4">1</span><span class="s1">}</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">40</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">clf = GridSearchCV(</span>
        <span class="s1">LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">{</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]}</span><span class="s2">,</span>
        <span class="s1">scoring=bad_scorer</span><span class="s2">,</span>
        <span class="s1">refit=</span><span class="s5">&quot;good_name&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">msg = (</span>
        <span class="s5">&quot;For multi-metric scoring, the parameter refit must be set to a &quot;</span>
        <span class="s5">&quot;scorer key or a callable to refit&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_callable_multimetric_error_failing_clf():</span>
    <span class="s3"># Warns when there is an estimator the fails to fit with a float</span>
    <span class="s3"># error_score</span>
    <span class="s2">def </span><span class="s1">custom_scorer(est</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s5">&quot;acc&quot;</span><span class="s1">: </span><span class="s4">1</span><span class="s1">}</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">clf = FailingClassifier()</span>
    <span class="s1">gs = GridSearchCV(</span>
        <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">[{</span><span class="s5">&quot;parameter&quot;</span><span class="s1">: [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]}]</span><span class="s2">,</span>
        <span class="s1">scoring=custom_scorer</span><span class="s2">,</span>
        <span class="s1">refit=</span><span class="s2">False,</span>
        <span class="s1">error_score=</span><span class="s4">0.1</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">warning_message = re.compile(</span>
        <span class="s5">&quot;5 fits failed.+total of 15.+The score on these&quot;</span>
        <span class="s5">r&quot; train-test partitions for these parameters will be set to 0\.1&quot;</span><span class="s2">,</span>
        <span class="s1">flags=re.DOTALL</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(FitFailedWarning</span><span class="s2">, </span><span class="s1">match=warning_message):</span>
        <span class="s1">gs.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">assert_allclose(gs.cv_results_[</span><span class="s5">&quot;mean_test_acc&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0.1</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_callable_multimetric_clf_all_fits_fail():</span>
    <span class="s3"># Warns and raises when all estimator fails to fit.</span>
    <span class="s2">def </span><span class="s1">custom_scorer(est</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s5">&quot;acc&quot;</span><span class="s1">: </span><span class="s4">1</span><span class="s1">}</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">clf = FailingClassifier()</span>

    <span class="s1">gs = GridSearchCV(</span>
        <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">[{</span><span class="s5">&quot;parameter&quot;</span><span class="s1">: [FailingClassifier.FAILING_PARAMETER] * </span><span class="s4">3</span><span class="s1">}]</span><span class="s2">,</span>
        <span class="s1">scoring=custom_scorer</span><span class="s2">,</span>
        <span class="s1">refit=</span><span class="s2">False,</span>
        <span class="s1">error_score=</span><span class="s4">0.1</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">individual_fit_error_message = </span><span class="s5">&quot;ValueError: Failing classifier failed as required&quot;</span>
    <span class="s1">error_message = re.compile(</span>
        <span class="s1">(</span>
            <span class="s5">&quot;All the 15 fits failed.+your model is misconfigured.+&quot;</span>
            <span class="s5">f&quot;</span><span class="s2">{</span><span class="s1">individual_fit_error_message</span><span class="s2">}</span><span class="s5">&quot;</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">flags=re.DOTALL</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=error_message):</span>
        <span class="s1">gs.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_n_features_in():</span>
    <span class="s3"># make sure grid search and random search delegate n_features_in to the</span>
    <span class="s3"># best estimator</span>
    <span class="s1">n_features = </span><span class="s4">4</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_features=n_features)</span>
    <span class="s1">gbdt = HistGradientBoostingClassifier()</span>
    <span class="s1">param_grid = {</span><span class="s5">&quot;max_iter&quot;</span><span class="s1">: [</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]}</span>
    <span class="s1">gs = GridSearchCV(gbdt</span><span class="s2">, </span><span class="s1">param_grid)</span>
    <span class="s1">rs = RandomizedSearchCV(gbdt</span><span class="s2">, </span><span class="s1">param_grid</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(gs</span><span class="s2">, </span><span class="s5">&quot;n_features_in_&quot;</span><span class="s1">)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(rs</span><span class="s2">, </span><span class="s5">&quot;n_features_in_&quot;</span><span class="s1">)</span>
    <span class="s1">gs.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">rs.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">gs.n_features_in_ == n_features</span>
    <span class="s2">assert </span><span class="s1">rs.n_features_in_ == n_features</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;pairwise&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_search_cv_pairwise_property_delegated_to_base_estimator(pairwise):</span>
    <span class="s0">&quot;&quot;&quot; 
    Test implementation of BaseSearchCV has the pairwise tag 
    which matches the pairwise tag of its estimator. 
    This test make sure pairwise tag is delegated to the base estimator. 
 
    Non-regression test for issue #13920. 
    &quot;&quot;&quot;</span>

    <span class="s2">class </span><span class="s1">TestEstimator(BaseEstimator):</span>
        <span class="s2">def </span><span class="s1">_more_tags(self):</span>
            <span class="s2">return </span><span class="s1">{</span><span class="s5">&quot;pairwise&quot;</span><span class="s1">: pairwise}</span>

    <span class="s1">est = TestEstimator()</span>
    <span class="s1">attr_message = </span><span class="s5">&quot;BaseSearchCV pairwise tag must match estimator&quot;</span>
    <span class="s1">cv = GridSearchCV(est</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;n_neighbors&quot;</span><span class="s1">: [</span><span class="s4">10</span><span class="s1">]})</span>
    <span class="s2">assert </span><span class="s1">pairwise == cv._get_tags()[</span><span class="s5">&quot;pairwise&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">attr_message</span>


<span class="s2">def </span><span class="s1">test_search_cv__pairwise_property_delegated_to_base_estimator():</span>
    <span class="s0">&quot;&quot;&quot; 
    Test implementation of BaseSearchCV has the pairwise property 
    which matches the pairwise tag of its estimator. 
    This test make sure pairwise tag is delegated to the base estimator. 
 
    Non-regression test for issue #13920. 
    &quot;&quot;&quot;</span>

    <span class="s2">class </span><span class="s1">EstimatorPairwise(BaseEstimator):</span>
        <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">pairwise=</span><span class="s2">True</span><span class="s1">):</span>
            <span class="s1">self.pairwise = pairwise</span>

        <span class="s2">def </span><span class="s1">_more_tags(self):</span>
            <span class="s2">return </span><span class="s1">{</span><span class="s5">&quot;pairwise&quot;</span><span class="s1">: self.pairwise}</span>

    <span class="s1">est = EstimatorPairwise()</span>
    <span class="s1">attr_message = </span><span class="s5">&quot;BaseSearchCV _pairwise property must match estimator&quot;</span>

    <span class="s2">for </span><span class="s1">_pairwise_setting </span><span class="s2">in </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">]:</span>
        <span class="s1">est.set_params(pairwise=_pairwise_setting)</span>
        <span class="s1">cv = GridSearchCV(est</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;n_neighbors&quot;</span><span class="s1">: [</span><span class="s4">10</span><span class="s1">]})</span>
        <span class="s2">assert </span><span class="s1">_pairwise_setting == cv._get_tags()[</span><span class="s5">&quot;pairwise&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">attr_message</span>


<span class="s2">def </span><span class="s1">test_search_cv_pairwise_property_equivalence_of_precomputed():</span>
    <span class="s0">&quot;&quot;&quot; 
    Test implementation of BaseSearchCV has the pairwise tag 
    which matches the pairwise tag of its estimator. 
    This test ensures the equivalence of 'precomputed'. 
 
    Non-regression test for issue #13920. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s4">50</span>
    <span class="s1">n_splits = </span><span class="s4">2</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=n_samples</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">grid_params = {</span><span class="s5">&quot;n_neighbors&quot;</span><span class="s1">: [</span><span class="s4">10</span><span class="s1">]}</span>

    <span class="s3"># defaults to euclidean metric (minkowski p = 2)</span>
    <span class="s1">clf = KNeighborsClassifier()</span>
    <span class="s1">cv = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">grid_params</span><span class="s2">, </span><span class="s1">cv=n_splits)</span>
    <span class="s1">cv.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">preds_original = cv.predict(X)</span>

    <span class="s3"># precompute euclidean metric to validate pairwise is working</span>
    <span class="s1">X_precomputed = euclidean_distances(X)</span>
    <span class="s1">clf = KNeighborsClassifier(metric=</span><span class="s5">&quot;precomputed&quot;</span><span class="s1">)</span>
    <span class="s1">cv = GridSearchCV(clf</span><span class="s2">, </span><span class="s1">grid_params</span><span class="s2">, </span><span class="s1">cv=n_splits)</span>
    <span class="s1">cv.fit(X_precomputed</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">preds_precomputed = cv.predict(X_precomputed)</span>

    <span class="s1">attr_message = </span><span class="s5">&quot;GridSearchCV not identical with precomputed metric&quot;</span>
    <span class="s2">assert </span><span class="s1">(preds_original == preds_precomputed).all()</span><span class="s2">, </span><span class="s1">attr_message</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;SearchCV, param_search&quot;</span><span class="s2">,</span>
    <span class="s1">[(GridSearchCV</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;a&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.01</span><span class="s1">]})</span><span class="s2">, </span><span class="s1">(RandomizedSearchCV</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;a&quot;</span><span class="s1">: uniform(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">)})]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_scalar_fit_param(SearchCV</span><span class="s2">, </span><span class="s1">param_search):</span>
    <span class="s3"># unofficially sanctioned tolerance for scalar values in fit_params</span>
    <span class="s3"># non-regression test for:</span>
    <span class="s3"># https://github.com/scikit-learn/scikit-learn/issues/15805</span>
    <span class="s2">class </span><span class="s1">TestEstimator(ClassifierMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
        <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">a=</span><span class="s2">None</span><span class="s1">):</span>
            <span class="s1">self.a = a</span>

        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">r=</span><span class="s2">None</span><span class="s1">):</span>
            <span class="s1">self.r_ = r</span>

        <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
            <span class="s2">return </span><span class="s1">np.zeros(shape=(len(X)))</span>

    <span class="s1">model = SearchCV(TestEstimator()</span><span class="s2">, </span><span class="s1">param_search)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">model.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">r=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">model.best_estimator_.r_ == </span><span class="s4">42</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;SearchCV, param_search&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(GridSearchCV</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.01</span><span class="s1">]})</span><span class="s2">,</span>
        <span class="s1">(RandomizedSearchCV</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: uniform(</span><span class="s4">0.01</span><span class="s2">, </span><span class="s4">0.1</span><span class="s1">)})</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_scalar_fit_param_compat(SearchCV</span><span class="s2">, </span><span class="s1">param_search):</span>
    <span class="s3"># check support for scalar values in fit_params, for instance in LightGBM</span>
    <span class="s3"># that do not exactly respect the scikit-learn API contract but that we do</span>
    <span class="s3"># not want to break without an explicit deprecation cycle and API</span>
    <span class="s3"># recommendations for implementing early stopping with a user provided</span>
    <span class="s3"># validation set. non-regression test for:</span>
    <span class="s3"># https://github.com/scikit-learn/scikit-learn/issues/15805</span>
    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_valid</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_valid = train_test_split(</span>
        <span class="s1">*make_classification(random_state=</span><span class="s4">42</span><span class="s1">)</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span>
    <span class="s1">)</span>

    <span class="s2">class </span><span class="s1">_FitParamClassifier(SGDClassifier):</span>
        <span class="s2">def </span><span class="s1">fit(</span>
            <span class="s1">self</span><span class="s2">,</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">y</span><span class="s2">,</span>
            <span class="s1">sample_weight=</span><span class="s2">None,</span>
            <span class="s1">tuple_of_arrays=</span><span class="s2">None,</span>
            <span class="s1">scalar_param=</span><span class="s2">None,</span>
            <span class="s1">callable_param=</span><span class="s2">None,</span>
        <span class="s1">):</span>
            <span class="s1">super().fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
            <span class="s2">assert </span><span class="s1">scalar_param &gt; </span><span class="s4">0</span>
            <span class="s2">assert </span><span class="s1">callable(callable_param)</span>

            <span class="s3"># The tuple of arrays should be preserved as tuple.</span>
            <span class="s2">assert </span><span class="s1">isinstance(tuple_of_arrays</span><span class="s2">, </span><span class="s1">tuple)</span>
            <span class="s2">assert </span><span class="s1">tuple_of_arrays[</span><span class="s4">0</span><span class="s1">].ndim == </span><span class="s4">2</span>
            <span class="s2">assert </span><span class="s1">tuple_of_arrays[</span><span class="s4">1</span><span class="s1">].ndim == </span><span class="s4">1</span>
            <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">_fit_param_callable():</span>
        <span class="s2">pass</span>

    <span class="s1">model = SearchCV(_FitParamClassifier()</span><span class="s2">, </span><span class="s1">param_search)</span>

    <span class="s3"># NOTE: `fit_params` should be data dependent (e.g. `sample_weight`) which</span>
    <span class="s3"># is not the case for the following parameters. But this abuse is common in</span>
    <span class="s3"># popular third-party libraries and we should tolerate this behavior for</span>
    <span class="s3"># now and be careful not to break support for those without following</span>
    <span class="s3"># proper deprecation cycle.</span>
    <span class="s1">fit_params = {</span>
        <span class="s5">&quot;tuple_of_arrays&quot;</span><span class="s1">: (X_valid</span><span class="s2">, </span><span class="s1">y_valid)</span><span class="s2">,</span>
        <span class="s5">&quot;callable_param&quot;</span><span class="s1">: _fit_param_callable</span><span class="s2">,</span>
        <span class="s5">&quot;scalar_param&quot;</span><span class="s1">: </span><span class="s4">42</span><span class="s2">,</span>
    <span class="s1">}</span>
    <span class="s1">model.fit(X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">**fit_params)</span>


<span class="s3"># FIXME: Replace this test with a full `check_estimator` once we have API only</span>
<span class="s3"># checks.</span>
<span class="s1">@pytest.mark.filterwarnings(</span><span class="s5">&quot;ignore:The total space of parameters 4 is&quot;</span><span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;SearchCV&quot;</span><span class="s2">, </span><span class="s1">[GridSearchCV</span><span class="s2">, </span><span class="s1">RandomizedSearchCV])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;Predictor&quot;</span><span class="s2">, </span><span class="s1">[MinimalRegressor</span><span class="s2">, </span><span class="s1">MinimalClassifier])</span>
<span class="s2">def </span><span class="s1">test_search_cv_using_minimal_compatible_estimator(SearchCV</span><span class="s2">, </span><span class="s1">Predictor):</span>
    <span class="s3"># Check that third-party library can run tests without inheriting from</span>
    <span class="s3"># BaseEstimator.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = rng.randn(</span><span class="s4">25</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">5 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">20</span><span class="s1">)</span>

    <span class="s1">model = Pipeline(</span>
        <span class="s1">[(</span><span class="s5">&quot;transformer&quot;</span><span class="s2">, </span><span class="s1">MinimalTransformer())</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;predictor&quot;</span><span class="s2">, </span><span class="s1">Predictor())]</span>
    <span class="s1">)</span>

    <span class="s1">params = {</span>
        <span class="s5">&quot;transformer__param&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;predictor__parama&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">}</span>
    <span class="s1">search = SearchCV(model</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">error_score=</span><span class="s5">&quot;raise&quot;</span><span class="s1">)</span>
    <span class="s1">search.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">assert </span><span class="s1">search.best_params_.keys() == params.keys()</span>

    <span class="s1">y_pred = search.predict(X)</span>
    <span class="s2">if </span><span class="s1">is_classifier(search):</span>
        <span class="s1">assert_array_equal(y_pred</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">search.score(X</span><span class="s2">, </span><span class="s1">y) == pytest.approx(accuracy_score(y</span><span class="s2">, </span><span class="s1">y_pred))</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">assert_allclose(y_pred</span><span class="s2">, </span><span class="s1">y.mean())</span>
        <span class="s2">assert </span><span class="s1">search.score(X</span><span class="s2">, </span><span class="s1">y) == pytest.approx(r2_score(y</span><span class="s2">, </span><span class="s1">y_pred))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;return_train_score&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_search_cv_verbose_3(capsys</span><span class="s2">, </span><span class="s1">return_train_score):</span>
    <span class="s0">&quot;&quot;&quot;Check that search cv with verbose&gt;2 shows the score for single 
    metrics. non-regression test for #19658.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">n_classes=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">flip_y=</span><span class="s4">0.2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf = LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">grid = {</span><span class="s5">&quot;C&quot;</span><span class="s1">: [</span><span class="s4">0.1</span><span class="s1">]}</span>

    <span class="s1">GridSearchCV(</span>
        <span class="s1">clf</span><span class="s2">,</span>
        <span class="s1">grid</span><span class="s2">,</span>
        <span class="s1">scoring=</span><span class="s5">&quot;accuracy&quot;</span><span class="s2">,</span>
        <span class="s1">verbose=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">cv=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">return_train_score=return_train_score</span><span class="s2">,</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">captured = capsys.readouterr().out</span>
    <span class="s2">if </span><span class="s1">return_train_score:</span>
        <span class="s1">match = re.findall(</span><span class="s5">r&quot;score=\(train=[\d\.]+, test=[\d.]+\)&quot;</span><span class="s2">, </span><span class="s1">captured)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">match = re.findall(</span><span class="s5">r&quot;score=[\d\.]+&quot;</span><span class="s2">, </span><span class="s1">captured)</span>
    <span class="s2">assert </span><span class="s1">len(match) == </span><span class="s4">3</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;SearchCV, param_search&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(GridSearchCV</span><span class="s2">, </span><span class="s5">&quot;param_grid&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(RandomizedSearchCV</span><span class="s2">, </span><span class="s5">&quot;param_distributions&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(HalvingGridSearchCV</span><span class="s2">, </span><span class="s5">&quot;param_grid&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_search_estimator_param(SearchCV</span><span class="s2">, </span><span class="s1">param_search):</span>
    <span class="s3"># test that SearchCV object doesn't change the object given in the parameter grid</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(random_state=</span><span class="s4">42</span><span class="s1">)</span>

    <span class="s1">params = {</span><span class="s5">&quot;clf&quot;</span><span class="s1">: [LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s1">)]</span><span class="s2">, </span><span class="s5">&quot;clf__C&quot;</span><span class="s1">: [</span><span class="s4">0.01</span><span class="s1">]}</span>
    <span class="s1">orig_C = params[</span><span class="s5">&quot;clf&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">].C</span>

    <span class="s1">pipe = Pipeline([(</span><span class="s5">&quot;trs&quot;</span><span class="s2">, </span><span class="s1">MinimalTransformer())</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;clf&quot;</span><span class="s2">, None</span><span class="s1">)])</span>

    <span class="s1">param_grid_search = {param_search: params}</span>
    <span class="s1">gs = SearchCV(pipe</span><span class="s2">, </span><span class="s1">refit=</span><span class="s2">True, </span><span class="s1">cv=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s5">&quot;accuracy&quot;</span><span class="s2">, </span><span class="s1">**param_grid_search).fit(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span>
    <span class="s1">)</span>

    <span class="s3"># testing that the original object in params is not changed</span>
    <span class="s2">assert </span><span class="s1">params[</span><span class="s5">&quot;clf&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">].C == orig_C</span>
    <span class="s3"># testing that the GS is setting the parameter of the step correctly</span>
    <span class="s2">assert </span><span class="s1">gs.best_estimator_.named_steps[</span><span class="s5">&quot;clf&quot;</span><span class="s1">].C == </span><span class="s4">0.01</span>
</pre>
</body>
</html>