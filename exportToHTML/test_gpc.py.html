<html>
<head>
<title>test_gpc.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_gpc.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Testing for Gaussian process classification &quot;&quot;&quot;</span>

<span class="s2"># Author: Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de&gt;</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">pytest</span>
<span class="s3">from </span><span class="s1">scipy.optimize </span><span class="s3">import </span><span class="s1">approx_fprime</span>

<span class="s3">from </span><span class="s1">sklearn.exceptions </span><span class="s3">import </span><span class="s1">ConvergenceWarning</span>
<span class="s3">from </span><span class="s1">sklearn.gaussian_process </span><span class="s3">import </span><span class="s1">GaussianProcessClassifier</span>
<span class="s3">from </span><span class="s1">sklearn.gaussian_process.kernels </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">RBF</span><span class="s3">,</span>
    <span class="s1">CompoundKernel</span><span class="s3">,</span>
    <span class="s1">WhiteKernel</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">sklearn.gaussian_process.kernels </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">ConstantKernel </span><span class="s3">as </span><span class="s1">C</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">sklearn.gaussian_process.tests._mini_sequence_kernel </span><span class="s3">import </span><span class="s1">MiniSeqKernel</span>
<span class="s3">from </span><span class="s1">sklearn.utils._testing </span><span class="s3">import </span><span class="s1">assert_almost_equal</span><span class="s3">, </span><span class="s1">assert_array_equal</span>


<span class="s3">def </span><span class="s1">f(x):</span>
    <span class="s3">return </span><span class="s1">np.sin(x)</span>


<span class="s1">X = np.atleast_2d(np.linspace(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">10</span><span class="s3">, </span><span class="s4">30</span><span class="s1">)).T</span>
<span class="s1">X2 = np.atleast_2d([</span><span class="s4">2.0</span><span class="s3">, </span><span class="s4">4.0</span><span class="s3">, </span><span class="s4">5.5</span><span class="s3">, </span><span class="s4">6.5</span><span class="s3">, </span><span class="s4">7.5</span><span class="s1">]).T</span>
<span class="s1">y = np.array(f(X).ravel() &gt; </span><span class="s4">0</span><span class="s3">, </span><span class="s1">dtype=int)</span>
<span class="s1">fX = f(X).ravel()</span>
<span class="s1">y_mc = np.empty(y.shape</span><span class="s3">, </span><span class="s1">dtype=int)  </span><span class="s2"># multi-class</span>
<span class="s1">y_mc[fX &lt; -</span><span class="s4">0.35</span><span class="s1">] = </span><span class="s4">0</span>
<span class="s1">y_mc[(fX &gt;= -</span><span class="s4">0.35</span><span class="s1">) &amp; (fX &lt; </span><span class="s4">0.35</span><span class="s1">)] = </span><span class="s4">1</span>
<span class="s1">y_mc[fX &gt; </span><span class="s4">0.35</span><span class="s1">] = </span><span class="s4">2</span>


<span class="s1">fixed_kernel = RBF(length_scale=</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">length_scale_bounds=</span><span class="s5">&quot;fixed&quot;</span><span class="s1">)</span>
<span class="s1">kernels = [</span>
    <span class="s1">RBF(length_scale=</span><span class="s4">0.1</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">fixed_kernel</span><span class="s3">,</span>
    <span class="s1">RBF(length_scale=</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">length_scale_bounds=(</span><span class="s4">1e-3</span><span class="s3">, </span><span class="s4">1e3</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">C(</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1e-2</span><span class="s3">, </span><span class="s4">1e2</span><span class="s1">)) * RBF(length_scale=</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">length_scale_bounds=(</span><span class="s4">1e-3</span><span class="s3">, </span><span class="s4">1e3</span><span class="s1">))</span><span class="s3">,</span>
<span class="s1">]</span>
<span class="s1">non_fixed_kernels = [kernel </span><span class="s3">for </span><span class="s1">kernel </span><span class="s3">in </span><span class="s1">kernels </span><span class="s3">if </span><span class="s1">kernel != fixed_kernel]</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_predict_consistent(kernel):</span>
    <span class="s2"># Check binary predict decision has also predicted probability above 0.5.</span>
    <span class="s1">gpc = GaussianProcessClassifier(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(gpc.predict(X)</span><span class="s3">, </span><span class="s1">gpc.predict_proba(X)[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">] &gt;= </span><span class="s4">0.5</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_predict_consistent_structured():</span>
    <span class="s2"># Check binary predict decision has also predicted probability above 0.5.</span>
    <span class="s1">X = [</span><span class="s5">&quot;A&quot;</span><span class="s3">, </span><span class="s5">&quot;AB&quot;</span><span class="s3">, </span><span class="s5">&quot;B&quot;</span><span class="s1">]</span>
    <span class="s1">y = np.array([</span><span class="s3">True, False, True</span><span class="s1">])</span>
    <span class="s1">kernel = MiniSeqKernel(baseline_similarity_bounds=</span><span class="s5">&quot;fixed&quot;</span><span class="s1">)</span>
    <span class="s1">gpc = GaussianProcessClassifier(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(gpc.predict(X)</span><span class="s3">, </span><span class="s1">gpc.predict_proba(X)[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">] &gt;= </span><span class="s4">0.5</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">non_fixed_kernels)</span>
<span class="s3">def </span><span class="s1">test_lml_improving(kernel):</span>
    <span class="s2"># Test that hyperparameter-tuning improves log-marginal likelihood.</span>
    <span class="s1">gpc = GaussianProcessClassifier(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">gpc.log_marginal_likelihood(gpc.kernel_.theta) &gt; gpc.log_marginal_likelihood(</span>
        <span class="s1">kernel.theta</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_lml_precomputed(kernel):</span>
    <span class="s2"># Test that lml of optimized kernel is stored correctly.</span>
    <span class="s1">gpc = GaussianProcessClassifier(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(</span>
        <span class="s1">gpc.log_marginal_likelihood(gpc.kernel_.theta)</span><span class="s3">, </span><span class="s1">gpc.log_marginal_likelihood()</span><span class="s3">, </span><span class="s4">7</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_lml_without_cloning_kernel(kernel):</span>
    <span class="s2"># Test that clone_kernel=False has side-effects of kernel.theta.</span>
    <span class="s1">gpc = GaussianProcessClassifier(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">input_theta = np.ones(gpc.kernel_.theta.shape</span><span class="s3">, </span><span class="s1">dtype=np.float64)</span>

    <span class="s1">gpc.log_marginal_likelihood(input_theta</span><span class="s3">, </span><span class="s1">clone_kernel=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(gpc.kernel_.theta</span><span class="s3">, </span><span class="s1">input_theta</span><span class="s3">, </span><span class="s4">7</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">non_fixed_kernels)</span>
<span class="s3">def </span><span class="s1">test_converged_to_local_maximum(kernel):</span>
    <span class="s2"># Test that we are in local maximum after hyperparameter-optimization.</span>
    <span class="s1">gpc = GaussianProcessClassifier(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">lml</span><span class="s3">, </span><span class="s1">lml_gradient = gpc.log_marginal_likelihood(gpc.kernel_.theta</span><span class="s3">, True</span><span class="s1">)</span>

    <span class="s3">assert </span><span class="s1">np.all(</span>
        <span class="s1">(np.abs(lml_gradient) &lt; </span><span class="s4">1e-4</span><span class="s1">)</span>
        <span class="s1">| (gpc.kernel_.theta == gpc.kernel_.bounds[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">| (gpc.kernel_.theta == gpc.kernel_.bounds[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_lml_gradient(kernel):</span>
    <span class="s2"># Compare analytic and numeric gradient of log marginal likelihood.</span>
    <span class="s1">gpc = GaussianProcessClassifier(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">lml</span><span class="s3">, </span><span class="s1">lml_gradient = gpc.log_marginal_likelihood(kernel.theta</span><span class="s3">, True</span><span class="s1">)</span>
    <span class="s1">lml_gradient_approx = approx_fprime(</span>
        <span class="s1">kernel.theta</span><span class="s3">, lambda </span><span class="s1">theta: gpc.log_marginal_likelihood(theta</span><span class="s3">, False</span><span class="s1">)</span><span class="s3">, </span><span class="s4">1e-10</span>
    <span class="s1">)</span>

    <span class="s1">assert_almost_equal(lml_gradient</span><span class="s3">, </span><span class="s1">lml_gradient_approx</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_random_starts(global_random_seed):</span>
    <span class="s2"># Test that an increasing number of random-starts of GP fitting only</span>
    <span class="s2"># increases the log marginal likelihood of the chosen theta.</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = </span><span class="s4">25</span><span class="s3">, </span><span class="s4">2</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s3">, </span><span class="s1">n_features) * </span><span class="s4">2 </span><span class="s1">- </span><span class="s4">1</span>
    <span class="s1">y = (np.sin(X).sum(axis=</span><span class="s4">1</span><span class="s1">) + np.sin(</span><span class="s4">3 </span><span class="s1">* X).sum(axis=</span><span class="s4">1</span><span class="s1">)) &gt; </span><span class="s4">0</span>

    <span class="s1">kernel = C(</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1e-2</span><span class="s3">, </span><span class="s4">1e2</span><span class="s1">)) * RBF(</span>
        <span class="s1">length_scale=[</span><span class="s4">1e-3</span><span class="s1">] * n_features</span><span class="s3">, </span><span class="s1">length_scale_bounds=[(</span><span class="s4">1e-4</span><span class="s3">, </span><span class="s4">1e2</span><span class="s1">)] * n_features</span>
    <span class="s1">)</span>
    <span class="s1">last_lml = -np.inf</span>
    <span class="s3">for </span><span class="s1">n_restarts_optimizer </span><span class="s3">in </span><span class="s1">range(</span><span class="s4">5</span><span class="s1">):</span>
        <span class="s1">gp = GaussianProcessClassifier(</span>
            <span class="s1">kernel=kernel</span><span class="s3">,</span>
            <span class="s1">n_restarts_optimizer=n_restarts_optimizer</span><span class="s3">,</span>
            <span class="s1">random_state=global_random_seed</span><span class="s3">,</span>
        <span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s1">lml = gp.log_marginal_likelihood(gp.kernel_.theta)</span>
        <span class="s3">assert </span><span class="s1">lml &gt; last_lml - np.finfo(np.float32).eps</span>
        <span class="s1">last_lml = lml</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">non_fixed_kernels)</span>
<span class="s3">def </span><span class="s1">test_custom_optimizer(kernel</span><span class="s3">, </span><span class="s1">global_random_seed):</span>
    <span class="s2"># Test that GPC can use externally defined optimizers.</span>
    <span class="s2"># Define a dummy optimizer that simply tests 10 random hyperparameters</span>
    <span class="s3">def </span><span class="s1">optimizer(obj_func</span><span class="s3">, </span><span class="s1">initial_theta</span><span class="s3">, </span><span class="s1">bounds):</span>
        <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
        <span class="s1">theta_opt</span><span class="s3">, </span><span class="s1">func_min = initial_theta</span><span class="s3">, </span><span class="s1">obj_func(</span>
            <span class="s1">initial_theta</span><span class="s3">, </span><span class="s1">eval_gradient=</span><span class="s3">False</span>
        <span class="s1">)</span>
        <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">):</span>
            <span class="s1">theta = np.atleast_1d(</span>
                <span class="s1">rng.uniform(np.maximum(-</span><span class="s4">2</span><span class="s3">, </span><span class="s1">bounds[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])</span><span class="s3">, </span><span class="s1">np.minimum(</span><span class="s4">1</span><span class="s3">, </span><span class="s1">bounds[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]))</span>
            <span class="s1">)</span>
            <span class="s1">f = obj_func(theta</span><span class="s3">, </span><span class="s1">eval_gradient=</span><span class="s3">False</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">f &lt; func_min:</span>
                <span class="s1">theta_opt</span><span class="s3">, </span><span class="s1">func_min = theta</span><span class="s3">, </span><span class="s1">f</span>
        <span class="s3">return </span><span class="s1">theta_opt</span><span class="s3">, </span><span class="s1">func_min</span>

    <span class="s1">gpc = GaussianProcessClassifier(kernel=kernel</span><span class="s3">, </span><span class="s1">optimizer=optimizer)</span>
    <span class="s1">gpc.fit(X</span><span class="s3">, </span><span class="s1">y_mc)</span>
    <span class="s2"># Checks that optimizer improved marginal likelihood</span>
    <span class="s3">assert </span><span class="s1">gpc.log_marginal_likelihood(</span>
        <span class="s1">gpc.kernel_.theta</span>
    <span class="s1">) &gt;= gpc.log_marginal_likelihood(kernel.theta)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_multi_class(kernel):</span>
    <span class="s2"># Test GPC for multi-class classification problems.</span>
    <span class="s1">gpc = GaussianProcessClassifier(kernel=kernel)</span>
    <span class="s1">gpc.fit(X</span><span class="s3">, </span><span class="s1">y_mc)</span>

    <span class="s1">y_prob = gpc.predict_proba(X2)</span>
    <span class="s1">assert_almost_equal(y_prob.sum(</span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s1">y_pred = gpc.predict(X2)</span>
    <span class="s1">assert_array_equal(np.argmax(y_prob</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">y_pred)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_multi_class_n_jobs(kernel):</span>
    <span class="s2"># Test that multi-class GPC produces identical results with n_jobs&gt;1.</span>
    <span class="s1">gpc = GaussianProcessClassifier(kernel=kernel)</span>
    <span class="s1">gpc.fit(X</span><span class="s3">, </span><span class="s1">y_mc)</span>

    <span class="s1">gpc_2 = GaussianProcessClassifier(kernel=kernel</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">gpc_2.fit(X</span><span class="s3">, </span><span class="s1">y_mc)</span>

    <span class="s1">y_prob = gpc.predict_proba(X2)</span>
    <span class="s1">y_prob_2 = gpc_2.predict_proba(X2)</span>
    <span class="s1">assert_almost_equal(y_prob</span><span class="s3">, </span><span class="s1">y_prob_2)</span>


<span class="s3">def </span><span class="s1">test_warning_bounds():</span>
    <span class="s1">kernel = RBF(length_scale_bounds=[</span><span class="s4">1e-5</span><span class="s3">, </span><span class="s4">1e-3</span><span class="s1">])</span>
    <span class="s1">gpc = GaussianProcessClassifier(kernel=kernel)</span>
    <span class="s1">warning_message = (</span>
        <span class="s5">&quot;The optimal value found for dimension 0 of parameter &quot;</span>
        <span class="s5">&quot;length_scale is close to the specified upper bound &quot;</span>
        <span class="s5">&quot;0.001. Increasing the bound and calling fit again may &quot;</span>
        <span class="s5">&quot;find a better value.&quot;</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s3">, </span><span class="s1">match=warning_message):</span>
        <span class="s1">gpc.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">kernel_sum = WhiteKernel(noise_level_bounds=[</span><span class="s4">1e-5</span><span class="s3">, </span><span class="s4">1e-3</span><span class="s1">]) + RBF(</span>
        <span class="s1">length_scale_bounds=[</span><span class="s4">1e3</span><span class="s3">, </span><span class="s4">1e5</span><span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">gpc_sum = GaussianProcessClassifier(kernel=kernel_sum)</span>
    <span class="s3">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s3">True</span><span class="s1">) </span><span class="s3">as </span><span class="s1">record:</span>
        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;always&quot;</span><span class="s1">)</span>
        <span class="s1">gpc_sum.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s3">assert </span><span class="s1">len(record) == </span><span class="s4">2</span>

        <span class="s3">assert </span><span class="s1">issubclass(record[</span><span class="s4">0</span><span class="s1">].category</span><span class="s3">, </span><span class="s1">ConvergenceWarning)</span>
        <span class="s3">assert </span><span class="s1">(</span>
            <span class="s1">record[</span><span class="s4">0</span><span class="s1">].message.args[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">== </span><span class="s5">&quot;The optimal value found for &quot;</span>
            <span class="s5">&quot;dimension 0 of parameter &quot;</span>
            <span class="s5">&quot;k1__noise_level is close to the &quot;</span>
            <span class="s5">&quot;specified upper bound 0.001. &quot;</span>
            <span class="s5">&quot;Increasing the bound and calling &quot;</span>
            <span class="s5">&quot;fit again may find a better value.&quot;</span>
        <span class="s1">)</span>

        <span class="s3">assert </span><span class="s1">issubclass(record[</span><span class="s4">1</span><span class="s1">].category</span><span class="s3">, </span><span class="s1">ConvergenceWarning)</span>
        <span class="s3">assert </span><span class="s1">(</span>
            <span class="s1">record[</span><span class="s4">1</span><span class="s1">].message.args[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">== </span><span class="s5">&quot;The optimal value found for &quot;</span>
            <span class="s5">&quot;dimension 0 of parameter &quot;</span>
            <span class="s5">&quot;k2__length_scale is close to the &quot;</span>
            <span class="s5">&quot;specified lower bound 1000.0. &quot;</span>
            <span class="s5">&quot;Decreasing the bound and calling &quot;</span>
            <span class="s5">&quot;fit again may find a better value.&quot;</span>
        <span class="s1">)</span>

    <span class="s1">X_tile = np.tile(X</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">kernel_dims = RBF(length_scale=[</span><span class="s4">1.0</span><span class="s3">, </span><span class="s4">2.0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">length_scale_bounds=[</span><span class="s4">1e1</span><span class="s3">, </span><span class="s4">1e2</span><span class="s1">])</span>
    <span class="s1">gpc_dims = GaussianProcessClassifier(kernel=kernel_dims)</span>

    <span class="s3">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s3">True</span><span class="s1">) </span><span class="s3">as </span><span class="s1">record:</span>
        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;always&quot;</span><span class="s1">)</span>
        <span class="s1">gpc_dims.fit(X_tile</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s3">assert </span><span class="s1">len(record) == </span><span class="s4">2</span>

        <span class="s3">assert </span><span class="s1">issubclass(record[</span><span class="s4">0</span><span class="s1">].category</span><span class="s3">, </span><span class="s1">ConvergenceWarning)</span>
        <span class="s3">assert </span><span class="s1">(</span>
            <span class="s1">record[</span><span class="s4">0</span><span class="s1">].message.args[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">== </span><span class="s5">&quot;The optimal value found for &quot;</span>
            <span class="s5">&quot;dimension 0 of parameter &quot;</span>
            <span class="s5">&quot;length_scale is close to the &quot;</span>
            <span class="s5">&quot;specified upper bound 100.0. &quot;</span>
            <span class="s5">&quot;Increasing the bound and calling &quot;</span>
            <span class="s5">&quot;fit again may find a better value.&quot;</span>
        <span class="s1">)</span>

        <span class="s3">assert </span><span class="s1">issubclass(record[</span><span class="s4">1</span><span class="s1">].category</span><span class="s3">, </span><span class="s1">ConvergenceWarning)</span>
        <span class="s3">assert </span><span class="s1">(</span>
            <span class="s1">record[</span><span class="s4">1</span><span class="s1">].message.args[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">== </span><span class="s5">&quot;The optimal value found for &quot;</span>
            <span class="s5">&quot;dimension 1 of parameter &quot;</span>
            <span class="s5">&quot;length_scale is close to the &quot;</span>
            <span class="s5">&quot;specified upper bound 100.0. &quot;</span>
            <span class="s5">&quot;Increasing the bound and calling &quot;</span>
            <span class="s5">&quot;fit again may find a better value.&quot;</span>
        <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;params, error_type, err_msg&quot;</span><span class="s3">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s5">&quot;kernel&quot;</span><span class="s1">: CompoundKernel(</span><span class="s4">0</span><span class="s1">)}</span><span class="s3">,</span>
            <span class="s1">ValueError</span><span class="s3">,</span>
            <span class="s5">&quot;kernel cannot be a CompoundKernel&quot;</span><span class="s3">,</span>
        <span class="s1">)</span>
    <span class="s1">]</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">test_gpc_fit_error(params</span><span class="s3">, </span><span class="s1">error_type</span><span class="s3">, </span><span class="s1">err_msg):</span>
    <span class="s0">&quot;&quot;&quot;Check that expected error are raised during fit.&quot;&quot;&quot;</span>
    <span class="s1">gpc = GaussianProcessClassifier(**params)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(error_type</span><span class="s3">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">gpc.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
</pre>
</body>
</html>