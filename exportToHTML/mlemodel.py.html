<html>
<head>
<title>mlemodel.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
mlemodel.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot; 
State Space Model 
 
Author: Chad Fulton 
License: Simplified-BSD 
&quot;&quot;&quot;</span>
<span class="s3">from </span><span class="s1">statsmodels.compat.pandas </span><span class="s3">import </span><span class="s1">is_int_index</span>

<span class="s3">import </span><span class="s1">contextlib</span>
<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">import </span><span class="s1">datetime </span><span class="s3">as </span><span class="s1">dt</span>
<span class="s3">from </span><span class="s1">types </span><span class="s3">import </span><span class="s1">SimpleNamespace</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">pandas </span><span class="s3">as </span><span class="s1">pd</span>
<span class="s3">from </span><span class="s1">scipy.stats </span><span class="s3">import </span><span class="s1">norm</span>

<span class="s3">from </span><span class="s1">statsmodels.tools.tools </span><span class="s3">import </span><span class="s1">pinv_extended</span><span class="s3">, </span><span class="s1">Bunch</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s3">import </span><span class="s1">PrecisionWarning</span><span class="s3">, </span><span class="s1">ValueWarning</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.numdiff </span><span class="s3">import </span><span class="s1">(_get_epsilon</span><span class="s3">, </span><span class="s1">approx_hess_cs</span><span class="s3">,</span>
                                       <span class="s1">approx_fprime_cs</span><span class="s3">, </span><span class="s1">approx_fprime)</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.decorators </span><span class="s3">import </span><span class="s1">cache_readonly</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.eval_measures </span><span class="s3">import </span><span class="s1">aic</span><span class="s3">, </span><span class="s1">aicc</span><span class="s3">, </span><span class="s1">bic</span><span class="s3">, </span><span class="s1">hqic</span>

<span class="s3">import </span><span class="s1">statsmodels.base.wrapper </span><span class="s3">as </span><span class="s1">wrap</span>

<span class="s3">import </span><span class="s1">statsmodels.tsa.base.prediction </span><span class="s3">as </span><span class="s1">pred</span>

<span class="s3">from </span><span class="s1">statsmodels.base.data </span><span class="s3">import </span><span class="s1">PandasData</span>
<span class="s3">import </span><span class="s1">statsmodels.tsa.base.tsa_model </span><span class="s3">as </span><span class="s1">tsbase</span>

<span class="s3">from </span><span class="s1">.news </span><span class="s3">import </span><span class="s1">NewsResults</span>
<span class="s3">from </span><span class="s1">.simulation_smoother </span><span class="s3">import </span><span class="s1">SimulationSmoother</span>
<span class="s3">from </span><span class="s1">.kalman_smoother </span><span class="s3">import </span><span class="s1">SmootherResults</span>
<span class="s3">from </span><span class="s1">.kalman_filter </span><span class="s3">import </span><span class="s1">INVERT_UNIVARIATE</span><span class="s3">, </span><span class="s1">SOLVE_LU</span><span class="s3">, </span><span class="s1">MEMORY_CONSERVE</span>
<span class="s3">from </span><span class="s1">.initialization </span><span class="s3">import </span><span class="s1">Initialization</span>
<span class="s3">from </span><span class="s1">.tools </span><span class="s3">import </span><span class="s1">prepare_exog</span><span class="s3">, </span><span class="s1">concat</span><span class="s3">, </span><span class="s1">_safe_cond</span><span class="s3">, </span><span class="s1">get_impact_dates</span>


<span class="s3">def </span><span class="s1">_handle_args(names</span><span class="s3">, </span><span class="s1">defaults</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s1">output_args = []</span>
    <span class="s0"># We need to handle positional arguments in two ways, in case this was</span>
    <span class="s0"># called by a Scipy optimization routine</span>
    <span class="s3">if </span><span class="s1">len(args) &gt; </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s0"># the fit() method will pass a dictionary</span>
        <span class="s3">if </span><span class="s1">isinstance(args[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dict):</span>
            <span class="s1">flags = args[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s0"># otherwise, a user may have just used positional arguments...</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">flags = dict(zip(names</span><span class="s3">, </span><span class="s1">args))</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(names)):</span>
            <span class="s1">output_args.append(flags.get(names[i]</span><span class="s3">, </span><span class="s1">defaults[i]))</span>

        <span class="s3">for </span><span class="s1">name</span><span class="s3">, </span><span class="s1">value </span><span class="s3">in </span><span class="s1">flags.items():</span>
            <span class="s3">if </span><span class="s1">name </span><span class="s3">in </span><span class="s1">kwargs:</span>
                <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;loglike() got multiple values for keyword&quot;</span>
                                <span class="s5">&quot; argument '%s'&quot; </span><span class="s1">% name)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(names)):</span>
            <span class="s1">output_args.append(kwargs.pop(names[i]</span><span class="s3">, </span><span class="s1">defaults[i]))</span>

    <span class="s3">return </span><span class="s1">tuple(output_args) + (kwargs</span><span class="s3">,</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_check_index(desired_index</span><span class="s3">, </span><span class="s1">dta</span><span class="s3">, </span><span class="s1">title=</span><span class="s5">'data'</span><span class="s1">):</span>
    <span class="s1">given_index = </span><span class="s3">None</span>
    <span class="s3">if </span><span class="s1">isinstance(dta</span><span class="s3">, </span><span class="s1">(pd.Series</span><span class="s3">, </span><span class="s1">pd.DataFrame)):</span>
        <span class="s1">given_index = dta.index</span>
    <span class="s3">if </span><span class="s1">given_index </span><span class="s3">is not None and not </span><span class="s1">desired_index.equals(given_index):</span>
        <span class="s1">desired_freq = getattr(desired_index</span><span class="s3">, </span><span class="s5">'freq'</span><span class="s3">, None</span><span class="s1">)</span>
        <span class="s1">given_freq = getattr(given_index</span><span class="s3">, </span><span class="s5">'freq'</span><span class="s3">, None</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">((desired_freq </span><span class="s3">is not None or </span><span class="s1">given_freq </span><span class="s3">is not None</span><span class="s1">) </span><span class="s3">and</span>
                <span class="s1">desired_freq != given_freq):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Given %s does not have an index'</span>
                             <span class="s5">' that extends the index of the'</span>
                             <span class="s5">' model. Expected index frequency is'</span>
                             <span class="s5">' &quot;%s&quot;, but got &quot;%s&quot;.'</span>
                             <span class="s1">% (title</span><span class="s3">, </span><span class="s1">desired_freq</span><span class="s3">, </span><span class="s1">given_freq))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Given %s does not have an index'</span>
                             <span class="s5">' that extends the index of the'</span>
                             <span class="s5">' model.' </span><span class="s1">% title)</span>


<span class="s3">class </span><span class="s1">MLEModel(tsbase.TimeSeriesModel):</span>
    <span class="s2">r&quot;&quot;&quot; 
    State space model for maximum likelihood estimation 
 
    Parameters 
    ---------- 
    endog : array_like 
        The observed time-series process :math:`y` 
    k_states : int 
        The dimension of the unobserved state process. 
    exog : array_like, optional 
        Array of exogenous regressors, shaped nobs x k. Default is no 
        exogenous regressors. 
    dates : array_like of datetime, optional 
        An array-like object of datetime objects. If a Pandas object is given 
        for endog, it is assumed to have a DateIndex. 
    freq : str, optional 
        The frequency of the time-series. A Pandas offset or 'B', 'D', 'W', 
        'M', 'A', or 'Q'. This is optional if dates are given. 
    **kwargs 
        Keyword arguments may be used to provide default values for state space 
        matrices or for Kalman filtering options. See `Representation`, and 
        `KalmanFilter` for more details. 
 
    Attributes 
    ---------- 
    ssm : statsmodels.tsa.statespace.kalman_filter.KalmanFilter 
        Underlying state space representation. 
 
    See Also 
    -------- 
    statsmodels.tsa.statespace.mlemodel.MLEResults 
    statsmodels.tsa.statespace.kalman_filter.KalmanFilter 
    statsmodels.tsa.statespace.representation.Representation 
 
    Notes 
    ----- 
    This class wraps the state space model with Kalman filtering to add in 
    functionality for maximum likelihood estimation. In particular, it adds 
    the concept of updating the state space representation based on a defined 
    set of parameters, through the `update` method or `updater` attribute (see 
    below for more details on which to use when), and it adds a `fit` method 
    which uses a numerical optimizer to select the parameters that maximize 
    the likelihood of the model. 
 
    The `start_params` `update` method must be overridden in the 
    child class (and the `transform` and `untransform` methods, if needed). 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">k_states</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">dates=</span><span class="s3">None, </span><span class="s1">freq=</span><span class="s3">None,</span>
                 <span class="s1">**kwargs):</span>
        <span class="s0"># Initialize the model base</span>
        <span class="s1">super(MLEModel</span><span class="s3">, </span><span class="s1">self).__init__(endog=endog</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">,</span>
                                       <span class="s1">dates=dates</span><span class="s3">, </span><span class="s1">freq=freq</span><span class="s3">,</span>
                                       <span class="s1">missing=</span><span class="s5">'none'</span><span class="s1">)</span>

        <span class="s0"># Store kwargs to recreate model</span>
        <span class="s1">self._init_kwargs = kwargs</span>

        <span class="s0"># Prepared the endog array: C-ordered, shape=(nobs x k_endog)</span>
        <span class="s1">self.endog</span><span class="s3">, </span><span class="s1">self.exog = self.prepare_data()</span>

        <span class="s0"># Dimensions</span>
        <span class="s1">self.nobs = self.endog.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">self.k_states = k_states</span>

        <span class="s0"># Initialize the state-space representation</span>
        <span class="s1">self.initialize_statespace(**kwargs)</span>

        <span class="s0"># Setup holder for fixed parameters</span>
        <span class="s1">self._has_fixed_params = </span><span class="s3">False</span>
        <span class="s1">self._fixed_params = </span><span class="s3">None</span>
        <span class="s1">self._params_index = </span><span class="s3">None</span>
        <span class="s1">self._fixed_params_index = </span><span class="s3">None</span>
        <span class="s1">self._free_params_index = </span><span class="s3">None</span>

    <span class="s3">def </span><span class="s1">prepare_data(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Prepare data for use in the state space representation 
        &quot;&quot;&quot;</span>
        <span class="s1">endog = np.array(self.data.orig_endog</span><span class="s3">, </span><span class="s1">order=</span><span class="s5">'C'</span><span class="s1">)</span>
        <span class="s1">exog = self.data.orig_exog</span>
        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">exog = np.array(exog)</span>

        <span class="s0"># Base class may allow 1-dim data, whereas we need 2-dim</span>
        <span class="s3">if </span><span class="s1">endog.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">endog.shape = (endog.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)  </span><span class="s0"># this will be C-contiguous</span>

        <span class="s3">return </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span>

    <span class="s3">def </span><span class="s1">initialize_statespace(self</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Initialize the state space representation 
 
        Parameters 
        ---------- 
        **kwargs 
            Additional keyword arguments to pass to the state space class 
            constructor. 
        &quot;&quot;&quot;</span>
        <span class="s0"># (Now self.endog is C-ordered and in long format (nobs x k_endog). To</span>
        <span class="s0"># get F-ordered and in wide format just need to transpose)</span>
        <span class="s1">endog = self.endog.T</span>

        <span class="s0"># Instantiate the state space object</span>
        <span class="s1">self.ssm = SimulationSmoother(endog.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">self.k_states</span><span class="s3">,</span>
                                      <span class="s1">nobs=endog.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s0"># Bind the data to the model</span>
        <span class="s1">self.ssm.bind(endog)</span>

        <span class="s0"># Other dimensions, now that `ssm` is available</span>
        <span class="s1">self.k_endog = self.ssm.k_endog</span>

    <span class="s3">def </span><span class="s1">_get_index_with_final_state(self):</span>
        <span class="s0"># The index we inherit from `TimeSeriesModel` will only cover the</span>
        <span class="s0"># data sample itself, but we will also need an index value for the</span>
        <span class="s0"># final state which is the next time step to the last datapoint.</span>
        <span class="s0"># This method figures out an appropriate value for the three types of</span>
        <span class="s0"># supported indexes: date-based, Int64Index, or RangeIndex</span>
        <span class="s3">if </span><span class="s1">self._index_dates:</span>
            <span class="s3">if </span><span class="s1">isinstance(self._index</span><span class="s3">, </span><span class="s1">pd.DatetimeIndex):</span>
                <span class="s1">index = pd.date_range(</span>
                    <span class="s1">start=self._index[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">periods=len(self._index) + </span><span class="s4">1</span><span class="s3">,</span>
                    <span class="s1">freq=self._index.freq)</span>
            <span class="s3">elif </span><span class="s1">isinstance(self._index</span><span class="s3">, </span><span class="s1">pd.PeriodIndex):</span>
                <span class="s1">index = pd.period_range(</span>
                    <span class="s1">start=self._index[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">periods=len(self._index) + </span><span class="s4">1</span><span class="s3">,</span>
                    <span class="s1">freq=self._index.freq)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s3">raise </span><span class="s1">NotImplementedError</span>
        <span class="s3">elif </span><span class="s1">isinstance(self._index</span><span class="s3">, </span><span class="s1">pd.RangeIndex):</span>
            <span class="s0"># COMPAT: pd.RangeIndex does not have start, stop, step prior to</span>
            <span class="s0">#         pandas 0.25</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s1">start = self._index.start</span>
                <span class="s1">stop = self._index.stop</span>
                <span class="s1">step = self._index.step</span>
            <span class="s3">except </span><span class="s1">AttributeError:</span>
                <span class="s1">start = self._index._start</span>
                <span class="s1">stop = self._index._stop</span>
                <span class="s1">step = self._index._step</span>
            <span class="s1">index = pd.RangeIndex(start</span><span class="s3">, </span><span class="s1">stop + step</span><span class="s3">, </span><span class="s1">step)</span>
        <span class="s3">elif </span><span class="s1">is_int_index(self._index):</span>
            <span class="s0"># The only valid Int64Index is a full, incrementing index, so this</span>
            <span class="s0"># is general</span>
            <span class="s1">value = self._index[-</span><span class="s4">1</span><span class="s1">] + </span><span class="s4">1</span>
            <span class="s1">index = pd.Index(self._index.tolist() + [value])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError</span>
        <span class="s3">return </span><span class="s1">index</span>

    <span class="s3">def </span><span class="s1">__setitem__(self</span><span class="s3">, </span><span class="s1">key</span><span class="s3">, </span><span class="s1">value):</span>
        <span class="s3">return </span><span class="s1">self.ssm.__setitem__(key</span><span class="s3">, </span><span class="s1">value)</span>

    <span class="s3">def </span><span class="s1">__getitem__(self</span><span class="s3">, </span><span class="s1">key):</span>
        <span class="s3">return </span><span class="s1">self.ssm.__getitem__(key)</span>

    <span class="s3">def </span><span class="s1">_get_init_kwds(self):</span>
        <span class="s0"># Get keywords based on model attributes</span>
        <span class="s1">kwds = super(MLEModel</span><span class="s3">, </span><span class="s1">self)._get_init_kwds()</span>

        <span class="s3">for </span><span class="s1">key</span><span class="s3">, </span><span class="s1">value </span><span class="s3">in </span><span class="s1">kwds.items():</span>
            <span class="s3">if </span><span class="s1">value </span><span class="s3">is None and </span><span class="s1">hasattr(self.ssm</span><span class="s3">, </span><span class="s1">key):</span>
                <span class="s1">kwds[key] = getattr(self.ssm</span><span class="s3">, </span><span class="s1">key)</span>

        <span class="s3">return </span><span class="s1">kwds</span>

    <span class="s3">def </span><span class="s1">clone(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Clone state space model with new data and optionally new specification 
 
        Parameters 
        ---------- 
        endog : array_like 
            The observed time-series process :math:`y` 
        k_states : int 
            The dimension of the unobserved state process. 
        exog : array_like, optional 
            Array of exogenous regressors, shaped nobs x k. Default is no 
            exogenous regressors. 
        kwargs 
            Keyword arguments to pass to the new model class to change the 
            model specification. 
 
        Returns 
        ------- 
        model : MLEModel subclass 
 
        Notes 
        ----- 
        This method must be implemented 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">'This method is not implemented in the base'</span>
                                  <span class="s5">' class and must be set up by each specific'</span>
                                  <span class="s5">' model.'</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_clone_from_init_kwds(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s0"># Cannot make this the default, because there is extra work required</span>
        <span class="s0"># for subclasses to make _get_init_kwds useful.</span>
        <span class="s1">use_kwargs = self._get_init_kwds()</span>
        <span class="s1">use_kwargs.update(kwargs)</span>

        <span class="s0"># Check for `exog`</span>
        <span class="s3">if </span><span class="s1">getattr(self</span><span class="s3">, </span><span class="s5">'k_exog'</span><span class="s3">, </span><span class="s4">0</span><span class="s1">) &gt; </span><span class="s4">0 </span><span class="s3">and </span><span class="s1">kwargs.get(</span><span class="s5">'exog'</span><span class="s3">, None</span><span class="s1">) </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cloning a model with an exogenous component'</span>
                             <span class="s5">' requires specifying a new exogenous array using'</span>
                             <span class="s5">' the `exog` argument.'</span><span class="s1">)</span>

        <span class="s1">mod = self.__class__(endog</span><span class="s3">, </span><span class="s1">**use_kwargs)</span>
        <span class="s3">return </span><span class="s1">mod</span>

    <span class="s3">def </span><span class="s1">set_filter_method(self</span><span class="s3">, </span><span class="s1">filter_method=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Set the filtering method 
 
        The filtering method controls aspects of which Kalman filtering 
        approach will be used. 
 
        Parameters 
        ---------- 
        filter_method : int, optional 
            Bitmask value to set the filter method to. See notes for details. 
        **kwargs 
            Keyword arguments may be used to influence the filter method by 
            setting individual boolean flags. See notes for details. 
 
        Notes 
        ----- 
        This method is rarely used. See the corresponding function in the 
        `KalmanFilter` class for details. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.ssm.set_filter_method(filter_method</span><span class="s3">, </span><span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">set_inversion_method(self</span><span class="s3">, </span><span class="s1">inversion_method=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Set the inversion method 
 
        The Kalman filter may contain one matrix inversion: that of the 
        forecast error covariance matrix. The inversion method controls how and 
        if that inverse is performed. 
 
        Parameters 
        ---------- 
        inversion_method : int, optional 
            Bitmask value to set the inversion method to. See notes for 
            details. 
        **kwargs 
            Keyword arguments may be used to influence the inversion method by 
            setting individual boolean flags. See notes for details. 
 
        Notes 
        ----- 
        This method is rarely used. See the corresponding function in the 
        `KalmanFilter` class for details. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.ssm.set_inversion_method(inversion_method</span><span class="s3">, </span><span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">set_stability_method(self</span><span class="s3">, </span><span class="s1">stability_method=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Set the numerical stability method 
 
        The Kalman filter is a recursive algorithm that may in some cases 
        suffer issues with numerical stability. The stability method controls 
        what, if any, measures are taken to promote stability. 
 
        Parameters 
        ---------- 
        stability_method : int, optional 
            Bitmask value to set the stability method to. See notes for 
            details. 
        **kwargs 
            Keyword arguments may be used to influence the stability method by 
            setting individual boolean flags. See notes for details. 
 
        Notes 
        ----- 
        This method is rarely used. See the corresponding function in the 
        `KalmanFilter` class for details. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.ssm.set_stability_method(stability_method</span><span class="s3">, </span><span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">set_conserve_memory(self</span><span class="s3">, </span><span class="s1">conserve_memory=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Set the memory conservation method 
 
        By default, the Kalman filter computes a number of intermediate 
        matrices at each iteration. The memory conservation options control 
        which of those matrices are stored. 
 
        Parameters 
        ---------- 
        conserve_memory : int, optional 
            Bitmask value to set the memory conservation method to. See notes 
            for details. 
        **kwargs 
            Keyword arguments may be used to influence the memory conservation 
            method by setting individual boolean flags. 
 
        Notes 
        ----- 
        This method is rarely used. See the corresponding function in the 
        `KalmanFilter` class for details. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.ssm.set_conserve_memory(conserve_memory</span><span class="s3">, </span><span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">set_smoother_output(self</span><span class="s3">, </span><span class="s1">smoother_output=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Set the smoother output 
 
        The smoother can produce several types of results. The smoother output 
        variable controls which are calculated and returned. 
 
        Parameters 
        ---------- 
        smoother_output : int, optional 
            Bitmask value to set the smoother output to. See notes for details. 
        **kwargs 
            Keyword arguments may be used to influence the smoother output by 
            setting individual boolean flags. 
 
        Notes 
        ----- 
        This method is rarely used. See the corresponding function in the 
        `KalmanSmoother` class for details. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.ssm.set_smoother_output(smoother_output</span><span class="s3">, </span><span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">initialize_known(self</span><span class="s3">, </span><span class="s1">initial_state</span><span class="s3">, </span><span class="s1">initial_state_cov):</span>
        <span class="s2">&quot;&quot;&quot;Initialize known&quot;&quot;&quot;</span>
        <span class="s1">self.ssm.initialize_known(initial_state</span><span class="s3">, </span><span class="s1">initial_state_cov)</span>

    <span class="s3">def </span><span class="s1">initialize_approximate_diffuse(self</span><span class="s3">, </span><span class="s1">variance=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;Initialize approximate diffuse&quot;&quot;&quot;</span>
        <span class="s1">self.ssm.initialize_approximate_diffuse(variance)</span>

    <span class="s3">def </span><span class="s1">initialize_stationary(self):</span>
        <span class="s2">&quot;&quot;&quot;Initialize stationary&quot;&quot;&quot;</span>
        <span class="s1">self.ssm.initialize_stationary()</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">initialization(self):</span>
        <span class="s3">return </span><span class="s1">self.ssm.initialization</span>

    <span class="s1">@initialization.setter</span>
    <span class="s3">def </span><span class="s1">initialization(self</span><span class="s3">, </span><span class="s1">value):</span>
        <span class="s1">self.ssm.initialization = value</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">initial_variance(self):</span>
        <span class="s3">return </span><span class="s1">self.ssm.initial_variance</span>

    <span class="s1">@initial_variance.setter</span>
    <span class="s3">def </span><span class="s1">initial_variance(self</span><span class="s3">, </span><span class="s1">value):</span>
        <span class="s1">self.ssm.initial_variance = value</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">loglikelihood_burn(self):</span>
        <span class="s3">return </span><span class="s1">self.ssm.loglikelihood_burn</span>

    <span class="s1">@loglikelihood_burn.setter</span>
    <span class="s3">def </span><span class="s1">loglikelihood_burn(self</span><span class="s3">, </span><span class="s1">value):</span>
        <span class="s1">self.ssm.loglikelihood_burn = value</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">tolerance(self):</span>
        <span class="s3">return </span><span class="s1">self.ssm.tolerance</span>

    <span class="s1">@tolerance.setter</span>
    <span class="s3">def </span><span class="s1">tolerance(self</span><span class="s3">, </span><span class="s1">value):</span>
        <span class="s1">self.ssm.tolerance = value</span>

    <span class="s3">def </span><span class="s1">_validate_can_fix_params(self</span><span class="s3">, </span><span class="s1">param_names):</span>
        <span class="s3">for </span><span class="s1">param_name </span><span class="s3">in </span><span class="s1">param_names:</span>
            <span class="s3">if </span><span class="s1">param_name </span><span class="s3">not in </span><span class="s1">self.param_names:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Invalid parameter name passed: &quot;%s&quot;.'</span>
                                 <span class="s1">% param_name)</span>

    <span class="s1">@contextlib.contextmanager</span>
    <span class="s3">def </span><span class="s1">fix_params(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot; 
        Fix parameters to specific values (context manager) 
 
        Parameters 
        ---------- 
        params : dict 
            Dictionary describing the fixed parameter values, of the form 
            `param_name: fixed_value`. See the `param_names` property for valid 
            parameter names. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; mod = sm.tsa.SARIMAX(endog, order=(1, 0, 1)) 
        &gt;&gt;&gt; with mod.fix_params({'ar.L1': 0.5}): 
                res = mod.fit() 
        &quot;&quot;&quot;</span>
        <span class="s1">k_params = len(self.param_names)</span>
        <span class="s0"># Initialization (this is done here rather than in the constructor</span>
        <span class="s0"># because param_names may not be available at that point)</span>
        <span class="s3">if </span><span class="s1">self._fixed_params </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self._fixed_params = {}</span>
            <span class="s1">self._params_index = dict(</span>
                <span class="s1">zip(self.param_names</span><span class="s3">, </span><span class="s1">np.arange(k_params)))</span>

        <span class="s0"># Cache the current fixed parameters</span>
        <span class="s1">cache_fixed_params = self._fixed_params.copy()</span>
        <span class="s1">cache_has_fixed_params = self._has_fixed_params</span>
        <span class="s1">cache_fixed_params_index = self._fixed_params_index</span>
        <span class="s1">cache_free_params_index = self._free_params_index</span>

        <span class="s0"># Validate parameter names and values</span>
        <span class="s1">all_fixed_param_names = (</span>
            <span class="s1">set(params.keys()) | set(self._fixed_params.keys())</span>
        <span class="s1">)</span>
        <span class="s1">self._validate_can_fix_params(all_fixed_param_names)</span>

        <span class="s0"># Set the new fixed parameters, keeping the order as given by</span>
        <span class="s0"># param_names</span>
        <span class="s1">self._fixed_params.update(params)</span>
        <span class="s1">self._fixed_params = dict([</span>
            <span class="s1">(name</span><span class="s3">, </span><span class="s1">self._fixed_params[name]) </span><span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">self.param_names</span>
            <span class="s3">if </span><span class="s1">name </span><span class="s3">in </span><span class="s1">self._fixed_params])</span>

        <span class="s0"># Update associated values</span>
        <span class="s1">self._has_fixed_params = </span><span class="s3">True</span>
        <span class="s1">self._fixed_params_index = [self._params_index[key]</span>
                                    <span class="s3">for </span><span class="s1">key </span><span class="s3">in </span><span class="s1">self._fixed_params.keys()]</span>
        <span class="s1">self._free_params_index = list(</span>
            <span class="s1">set(np.arange(k_params)).difference(self._fixed_params_index))</span>

        <span class="s3">try</span><span class="s1">:</span>
            <span class="s3">yield</span>
        <span class="s3">finally</span><span class="s1">:</span>
            <span class="s0"># Reset the fixed parameters</span>
            <span class="s1">self._has_fixed_params = cache_has_fixed_params</span>
            <span class="s1">self._fixed_params = cache_fixed_params</span>
            <span class="s1">self._fixed_params_index = cache_fixed_params_index</span>
            <span class="s1">self._free_params_index = cache_free_params_index</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">False,</span>
            <span class="s1">cov_type=</span><span class="s3">None, </span><span class="s1">cov_kwds=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s5">'lbfgs'</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s4">50</span><span class="s3">,</span>
            <span class="s1">full_output=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None, </span><span class="s1">return_params=</span><span class="s3">False,</span>
            <span class="s1">optim_score=</span><span class="s3">None, </span><span class="s1">optim_complex_step=</span><span class="s3">None, </span><span class="s1">optim_hessian=</span><span class="s3">None,</span>
            <span class="s1">flags=</span><span class="s3">None, </span><span class="s1">low_memory=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Fits the model by maximum likelihood via Kalman filter. 
 
        Parameters 
        ---------- 
        start_params : array_like, optional 
            Initial guess of the solution for the loglikelihood maximization. 
            If None, the default is given by Model.start_params. 
        transformed : bool, optional 
            Whether or not `start_params` is already transformed. Default is 
            True. 
        includes_fixed : bool, optional 
            If parameters were previously fixed with the `fix_params` method, 
            this argument describes whether or not `start_params` also includes 
            the fixed parameters, in addition to the free parameters. Default 
            is False. 
        cov_type : str, optional 
            The `cov_type` keyword governs the method for calculating the 
            covariance matrix of parameter estimates. Can be one of: 
 
            - 'opg' for the outer product of gradient estimator 
            - 'oim' for the observed information matrix estimator, calculated 
              using the method of Harvey (1989) 
            - 'approx' for the observed information matrix estimator, 
              calculated using a numerical approximation of the Hessian matrix. 
            - 'robust' for an approximate (quasi-maximum likelihood) covariance 
              matrix that may be valid even in the presence of some 
              misspecifications. Intermediate calculations use the 'oim' 
              method. 
            - 'robust_approx' is the same as 'robust' except that the 
              intermediate calculations use the 'approx' method. 
            - 'none' for no covariance matrix calculation. 
 
            Default is 'opg' unless memory conservation is used to avoid 
            computing the loglikelihood values for each observation, in which 
            case the default is 'approx'. 
        cov_kwds : dict or None, optional 
            A dictionary of arguments affecting covariance matrix computation. 
 
            **opg, oim, approx, robust, robust_approx** 
 
            - 'approx_complex_step' : bool, optional - If True, numerical 
              approximations are computed using complex-step methods. If False, 
              numerical approximations are computed using finite difference 
              methods. Default is True. 
            - 'approx_centered' : bool, optional - If True, numerical 
              approximations computed using finite difference methods use a 
              centered approximation. Default is False. 
        method : str, optional 
            The `method` determines which solver from `scipy.optimize` 
            is used, and it can be chosen from among the following strings: 
 
            - 'newton' for Newton-Raphson 
            - 'nm' for Nelder-Mead 
            - 'bfgs' for Broyden-Fletcher-Goldfarb-Shanno (BFGS) 
            - 'lbfgs' for limited-memory BFGS with optional box constraints 
            - 'powell' for modified Powell's method 
            - 'cg' for conjugate gradient 
            - 'ncg' for Newton-conjugate gradient 
            - 'basinhopping' for global basin-hopping solver 
 
            The explicit arguments in `fit` are passed to the solver, 
            with the exception of the basin-hopping solver. Each 
            solver has several optional arguments that are not the same across 
            solvers. See the notes section below (or scipy.optimize) for the 
            available arguments and for the list of explicit arguments that the 
            basin-hopping solver supports. 
        maxiter : int, optional 
            The maximum number of iterations to perform. 
        full_output : bool, optional 
            Set to True to have all available output in the Results object's 
            mle_retvals attribute. The output is dependent on the solver. 
            See LikelihoodModelResults notes section for more information. 
        disp : bool, optional 
            Set to True to print convergence messages. 
        callback : callable callback(xk), optional 
            Called after each iteration, as callback(xk), where xk is the 
            current parameter vector. 
        return_params : bool, optional 
            Whether or not to return only the array of maximizing parameters. 
            Default is False. 
        optim_score : {'harvey', 'approx'} or None, optional 
            The method by which the score vector is calculated. 'harvey' uses 
            the method from Harvey (1989), 'approx' uses either finite 
            difference or complex step differentiation depending upon the 
            value of `optim_complex_step`, and None uses the built-in gradient 
            approximation of the optimizer. Default is None. This keyword is 
            only relevant if the optimization method uses the score. 
        optim_complex_step : bool, optional 
            Whether or not to use complex step differentiation when 
            approximating the score; if False, finite difference approximation 
            is used. Default is True. This keyword is only relevant if 
            `optim_score` is set to 'harvey' or 'approx'. 
        optim_hessian : {'opg','oim','approx'}, optional 
            The method by which the Hessian is numerically approximated. 'opg' 
            uses outer product of gradients, 'oim' uses the information 
            matrix formula from Harvey (1989), and 'approx' uses numerical 
            approximation. This keyword is only relevant if the 
            optimization method uses the Hessian matrix. 
        low_memory : bool, optional 
            If set to True, techniques are applied to substantially reduce 
            memory usage. If used, some features of the results object will 
            not be available (including smoothed results and in-sample 
            prediction), although out-of-sample forecasting is possible. 
            Default is False. 
        **kwargs 
            Additional keyword arguments to pass to the optimizer. 
 
        Returns 
        ------- 
        results 
            Results object holding results from fitting a state space model. 
 
        See Also 
        -------- 
        statsmodels.base.model.LikelihoodModel.fit 
        statsmodels.tsa.statespace.mlemodel.MLEResults 
        statsmodels.tsa.statespace.structural.UnobservedComponentsResults 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">start_params </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">start_params = self.start_params</span>
            <span class="s1">transformed = </span><span class="s3">True</span>
            <span class="s1">includes_fixed = </span><span class="s3">True</span>

        <span class="s0"># Update the score method</span>
        <span class="s3">if </span><span class="s1">optim_score </span><span class="s3">is None and </span><span class="s1">method == </span><span class="s5">'lbfgs'</span><span class="s1">:</span>
            <span class="s1">kwargs.setdefault(</span><span class="s5">'approx_grad'</span><span class="s3">, True</span><span class="s1">)</span>
            <span class="s1">kwargs.setdefault(</span><span class="s5">'epsilon'</span><span class="s3">, </span><span class="s4">1e-5</span><span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">optim_score </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">optim_score = </span><span class="s5">'approx'</span>

        <span class="s0"># Check for complex step differentiation</span>
        <span class="s3">if </span><span class="s1">optim_complex_step </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">optim_complex_step = </span><span class="s3">not </span><span class="s1">self.ssm._complex_endog</span>
        <span class="s3">elif </span><span class="s1">optim_complex_step </span><span class="s3">and </span><span class="s1">self.ssm._complex_endog:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot use complex step derivatives when data'</span>
                             <span class="s5">' or parameters are complex.'</span><span class="s1">)</span>

        <span class="s0"># Standardize starting parameters</span>
        <span class="s1">start_params = self.handle_params(start_params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True,</span>
                                          <span class="s1">includes_fixed=includes_fixed)</span>

        <span class="s0"># Unconstrain the starting parameters</span>
        <span class="s3">if </span><span class="s1">transformed:</span>
            <span class="s1">start_params = self.untransform_params(start_params)</span>

        <span class="s0"># Remove any fixed parameters</span>
        <span class="s3">if </span><span class="s1">self._has_fixed_params:</span>
            <span class="s1">start_params = start_params[self._free_params_index]</span>

        <span class="s0"># If all parameters are fixed, we are done</span>
        <span class="s3">if </span><span class="s1">self._has_fixed_params </span><span class="s3">and </span><span class="s1">len(start_params) == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">mlefit = Bunch(params=[]</span><span class="s3">, </span><span class="s1">mle_retvals=</span><span class="s3">None,</span>
                           <span class="s1">mle_settings=</span><span class="s3">None</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s0"># Remove disallowed kwargs</span>
            <span class="s1">disallow = (</span>
                <span class="s5">&quot;concentrate_scale&quot;</span><span class="s3">,</span>
                <span class="s5">&quot;enforce_stationarity&quot;</span><span class="s3">,</span>
                <span class="s5">&quot;enforce_invertibility&quot;</span>
            <span class="s1">)</span>
            <span class="s1">kwargs = {k: v </span><span class="s3">for </span><span class="s1">k</span><span class="s3">, </span><span class="s1">v </span><span class="s3">in </span><span class="s1">kwargs.items() </span><span class="s3">if </span><span class="s1">k </span><span class="s3">not in </span><span class="s1">disallow}</span>
            <span class="s0"># Maximum likelihood estimation</span>
            <span class="s3">if </span><span class="s1">flags </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">flags = {}</span>
            <span class="s1">flags.update({</span>
                <span class="s5">'transformed'</span><span class="s1">: </span><span class="s3">False,</span>
                <span class="s5">'includes_fixed'</span><span class="s1">: </span><span class="s3">False,</span>
                <span class="s5">'score_method'</span><span class="s1">: optim_score</span><span class="s3">,</span>
                <span class="s5">'approx_complex_step'</span><span class="s1">: optim_complex_step</span>
            <span class="s1">})</span>
            <span class="s3">if </span><span class="s1">optim_hessian </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">flags[</span><span class="s5">'hessian_method'</span><span class="s1">] = optim_hessian</span>
            <span class="s1">fargs = (flags</span><span class="s3">,</span><span class="s1">)</span>
            <span class="s1">mlefit = super(MLEModel</span><span class="s3">, </span><span class="s1">self).fit(start_params</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">,</span>
                                               <span class="s1">fargs=fargs</span><span class="s3">,</span>
                                               <span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                                               <span class="s1">full_output=full_output</span><span class="s3">,</span>
                                               <span class="s1">disp=disp</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">,</span>
                                               <span class="s1">skip_hessian=</span><span class="s3">True, </span><span class="s1">**kwargs)</span>

        <span class="s0"># Just return the fitted parameters if requested</span>
        <span class="s3">if </span><span class="s1">return_params:</span>
            <span class="s3">return </span><span class="s1">self.handle_params(mlefit.params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">False,</span>
                                      <span class="s1">includes_fixed=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s0"># Otherwise construct the results class if desired</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s0"># Handle memory conservation option</span>
            <span class="s3">if </span><span class="s1">low_memory:</span>
                <span class="s1">conserve_memory = self.ssm.conserve_memory</span>
                <span class="s1">self.ssm.set_conserve_memory(MEMORY_CONSERVE)</span>

            <span class="s0"># Perform filtering / smoothing</span>
            <span class="s3">if </span><span class="s1">(self.ssm.memory_no_predicted </span><span class="s3">or </span><span class="s1">self.ssm.memory_no_gain</span>
                    <span class="s3">or </span><span class="s1">self.ssm.memory_no_smoothing):</span>
                <span class="s1">func = self.filter</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">func = self.smooth</span>
            <span class="s1">res = func(mlefit.params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">False, </span><span class="s1">includes_fixed=</span><span class="s3">False,</span>
                       <span class="s1">cov_type=cov_type</span><span class="s3">, </span><span class="s1">cov_kwds=cov_kwds)</span>

            <span class="s1">res.mlefit = mlefit</span>
            <span class="s1">res.mle_retvals = mlefit.mle_retvals</span>
            <span class="s1">res.mle_settings = mlefit.mle_settings</span>

            <span class="s0"># Reset memory conservation</span>
            <span class="s3">if </span><span class="s1">low_memory:</span>
                <span class="s1">self.ssm.set_conserve_memory(conserve_memory)</span>

            <span class="s3">return </span><span class="s1">res</span>

    <span class="s3">def </span><span class="s1">fit_constrained(self</span><span class="s3">, </span><span class="s1">constraints</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">**fit_kwds):</span>
        <span class="s2">&quot;&quot;&quot; 
        Fit the model with some parameters subject to equality constraints. 
 
        Parameters 
        ---------- 
        constraints : dict 
            Dictionary of constraints, of the form `param_name: fixed_value`. 
            See the `param_names` property for valid parameter names. 
        start_params : array_like, optional 
            Initial guess of the solution for the loglikelihood maximization. 
            If None, the default is given by Model.start_params. 
        **fit_kwds : keyword arguments 
            fit_kwds are used in the optimization of the remaining parameters. 
 
        Returns 
        ------- 
        results : Results instance 
 
        Examples 
        -------- 
        &gt;&gt;&gt; mod = sm.tsa.SARIMAX(endog, order=(1, 0, 1)) 
        &gt;&gt;&gt; res = mod.fit_constrained({'ar.L1': 0.5}) 
        &quot;&quot;&quot;</span>
        <span class="s3">with </span><span class="s1">self.fix_params(constraints):</span>
            <span class="s1">res = self.fit(start_params</span><span class="s3">, </span><span class="s1">**fit_kwds)</span>
        <span class="s3">return </span><span class="s1">res</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">_res_classes(self):</span>
        <span class="s3">return </span><span class="s1">{</span><span class="s5">'fit'</span><span class="s1">: (MLEResults</span><span class="s3">, </span><span class="s1">MLEResultsWrapper)}</span>

    <span class="s3">def </span><span class="s1">_wrap_results(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">result</span><span class="s3">, </span><span class="s1">return_raw</span><span class="s3">, </span><span class="s1">cov_type=</span><span class="s3">None,</span>
                      <span class="s1">cov_kwds=</span><span class="s3">None, </span><span class="s1">results_class=</span><span class="s3">None, </span><span class="s1">wrapper_class=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s3">if not </span><span class="s1">return_raw:</span>
            <span class="s0"># Wrap in a results object</span>
            <span class="s1">result_kwargs = {}</span>
            <span class="s3">if </span><span class="s1">cov_type </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">result_kwargs[</span><span class="s5">'cov_type'</span><span class="s1">] = cov_type</span>
            <span class="s3">if </span><span class="s1">cov_kwds </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">result_kwargs[</span><span class="s5">'cov_kwds'</span><span class="s1">] = cov_kwds</span>

            <span class="s3">if </span><span class="s1">results_class </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">results_class = self._res_classes[</span><span class="s5">'fit'</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s3">if </span><span class="s1">wrapper_class </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">wrapper_class = self._res_classes[</span><span class="s5">'fit'</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]</span>

            <span class="s1">res = results_class(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">result</span><span class="s3">, </span><span class="s1">**result_kwargs)</span>
            <span class="s1">result = wrapper_class(res)</span>
        <span class="s3">return </span><span class="s1">result</span>

    <span class="s3">def </span><span class="s1">filter(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">False,</span>
               <span class="s1">complex_step=</span><span class="s3">False, </span><span class="s1">cov_type=</span><span class="s3">None, </span><span class="s1">cov_kwds=</span><span class="s3">None,</span>
               <span class="s1">return_ssm=</span><span class="s3">False, </span><span class="s1">results_class=</span><span class="s3">None,</span>
               <span class="s1">results_wrapper_class=</span><span class="s3">None, </span><span class="s1">low_memory=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Kalman filtering 
 
        Parameters 
        ---------- 
        params : array_like 
            Array of parameters at which to evaluate the loglikelihood 
            function. 
        transformed : bool, optional 
            Whether or not `params` is already transformed. Default is True. 
        return_ssm : bool,optional 
            Whether or not to return only the state space output or a full 
            results object. Default is to return a full results object. 
        cov_type : str, optional 
            See `MLEResults.fit` for a description of covariance matrix types 
            for results object. 
        cov_kwds : dict or None, optional 
            See `MLEResults.get_robustcov_results` for a description required 
            keywords for alternative covariance estimators 
        low_memory : bool, optional 
            If set to True, techniques are applied to substantially reduce 
            memory usage. If used, some features of the results object will 
            not be available (including in-sample prediction), although 
            out-of-sample forecasting is possible. Default is False. 
        **kwargs 
            Additional keyword arguments to pass to the Kalman filter. See 
            `KalmanFilter.filter` for more details. 
        &quot;&quot;&quot;</span>
        <span class="s1">params = self.handle_params(params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                                    <span class="s1">includes_fixed=includes_fixed)</span>
        <span class="s1">self.update(params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True,</span>
                    <span class="s1">complex_step=complex_step)</span>

        <span class="s0"># Save the parameter names</span>
        <span class="s1">self.data.param_names = self.param_names</span>

        <span class="s3">if </span><span class="s1">complex_step:</span>
            <span class="s1">kwargs[</span><span class="s5">'inversion_method'</span><span class="s1">] = INVERT_UNIVARIATE | SOLVE_LU</span>

        <span class="s0"># Handle memory conservation</span>
        <span class="s3">if </span><span class="s1">low_memory:</span>
            <span class="s1">kwargs[</span><span class="s5">'conserve_memory'</span><span class="s1">] = MEMORY_CONSERVE</span>

        <span class="s0"># Get the state space output</span>
        <span class="s1">result = self.ssm.filter(complex_step=complex_step</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s0"># Wrap in a results object</span>
        <span class="s3">return </span><span class="s1">self._wrap_results(params</span><span class="s3">, </span><span class="s1">result</span><span class="s3">, </span><span class="s1">return_ssm</span><span class="s3">, </span><span class="s1">cov_type</span><span class="s3">,</span>
                                  <span class="s1">cov_kwds</span><span class="s3">, </span><span class="s1">results_class</span><span class="s3">,</span>
                                  <span class="s1">results_wrapper_class)</span>

    <span class="s3">def </span><span class="s1">smooth(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">False,</span>
               <span class="s1">complex_step=</span><span class="s3">False, </span><span class="s1">cov_type=</span><span class="s3">None, </span><span class="s1">cov_kwds=</span><span class="s3">None,</span>
               <span class="s1">return_ssm=</span><span class="s3">False, </span><span class="s1">results_class=</span><span class="s3">None,</span>
               <span class="s1">results_wrapper_class=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Kalman smoothing 
 
        Parameters 
        ---------- 
        params : array_like 
            Array of parameters at which to evaluate the loglikelihood 
            function. 
        transformed : bool, optional 
            Whether or not `params` is already transformed. Default is True. 
        return_ssm : bool,optional 
            Whether or not to return only the state space output or a full 
            results object. Default is to return a full results object. 
        cov_type : str, optional 
            See `MLEResults.fit` for a description of covariance matrix types 
            for results object. 
        cov_kwds : dict or None, optional 
            See `MLEResults.get_robustcov_results` for a description required 
            keywords for alternative covariance estimators 
        **kwargs 
            Additional keyword arguments to pass to the Kalman filter. See 
            `KalmanFilter.filter` for more details. 
        &quot;&quot;&quot;</span>
        <span class="s1">params = self.handle_params(params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                                    <span class="s1">includes_fixed=includes_fixed)</span>
        <span class="s1">self.update(params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True,</span>
                    <span class="s1">complex_step=complex_step)</span>

        <span class="s0"># Save the parameter names</span>
        <span class="s1">self.data.param_names = self.param_names</span>

        <span class="s3">if </span><span class="s1">complex_step:</span>
            <span class="s1">kwargs[</span><span class="s5">'inversion_method'</span><span class="s1">] = INVERT_UNIVARIATE | SOLVE_LU</span>

        <span class="s0"># Get the state space output</span>
        <span class="s1">result = self.ssm.smooth(complex_step=complex_step</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s0"># Wrap in a results object</span>
        <span class="s3">return </span><span class="s1">self._wrap_results(params</span><span class="s3">, </span><span class="s1">result</span><span class="s3">, </span><span class="s1">return_ssm</span><span class="s3">, </span><span class="s1">cov_type</span><span class="s3">,</span>
                                  <span class="s1">cov_kwds</span><span class="s3">, </span><span class="s1">results_class</span><span class="s3">,</span>
                                  <span class="s1">results_wrapper_class)</span>

    <span class="s1">_loglike_param_names = [</span><span class="s5">'transformed'</span><span class="s3">, </span><span class="s5">'includes_fixed'</span><span class="s3">, </span><span class="s5">'complex_step'</span><span class="s1">]</span>
    <span class="s1">_loglike_param_defaults = [</span><span class="s3">True, False, False</span><span class="s1">]</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Loglikelihood evaluation 
 
        Parameters 
        ---------- 
        params : array_like 
            Array of parameters at which to evaluate the loglikelihood 
            function. 
        transformed : bool, optional 
            Whether or not `params` is already transformed. Default is True. 
        **kwargs 
            Additional keyword arguments to pass to the Kalman filter. See 
            `KalmanFilter.filter` for more details. 
 
        See Also 
        -------- 
        update : modifies the internal state of the state space model to 
                 reflect new params 
 
        Notes 
        ----- 
        [1]_ recommend maximizing the average likelihood to avoid scale issues; 
        this is done automatically by the base Model fit method. 
 
        References 
        ---------- 
        .. [1] Koopman, Siem Jan, Neil Shephard, and Jurgen A. Doornik. 1999. 
           Statistical Algorithms for Models in State Space Using SsfPack 2.2. 
           Econometrics Journal 2 (1): 107-60. doi:10.1111/1368-423X.00023. 
        &quot;&quot;&quot;</span>
        <span class="s1">transformed</span><span class="s3">, </span><span class="s1">includes_fixed</span><span class="s3">, </span><span class="s1">complex_step</span><span class="s3">, </span><span class="s1">kwargs = _handle_args(</span>
            <span class="s1">MLEModel._loglike_param_names</span><span class="s3">, </span><span class="s1">MLEModel._loglike_param_defaults</span><span class="s3">,</span>
            <span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s1">params = self.handle_params(params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                                    <span class="s1">includes_fixed=includes_fixed)</span>
        <span class="s1">self.update(params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True,</span>
                    <span class="s1">complex_step=complex_step)</span>

        <span class="s3">if </span><span class="s1">complex_step:</span>
            <span class="s1">kwargs[</span><span class="s5">'inversion_method'</span><span class="s1">] = INVERT_UNIVARIATE | SOLVE_LU</span>

        <span class="s1">loglike = self.ssm.loglike(complex_step=complex_step</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s0"># Koopman, Shephard, and Doornik recommend maximizing the average</span>
        <span class="s0"># likelihood to avoid scale issues, but the averaging is done</span>
        <span class="s0"># automatically in the base model `fit` method</span>
        <span class="s3">return </span><span class="s1">loglike</span>

    <span class="s3">def </span><span class="s1">loglikeobs(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">False,</span>
                   <span class="s1">complex_step=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Loglikelihood evaluation 
 
        Parameters 
        ---------- 
        params : array_like 
            Array of parameters at which to evaluate the loglikelihood 
            function. 
        transformed : bool, optional 
            Whether or not `params` is already transformed. Default is True. 
        **kwargs 
            Additional keyword arguments to pass to the Kalman filter. See 
            `KalmanFilter.filter` for more details. 
 
        See Also 
        -------- 
        update : modifies the internal state of the Model to reflect new params 
 
        Notes 
        ----- 
        [1]_ recommend maximizing the average likelihood to avoid scale issues; 
        this is done automatically by the base Model fit method. 
 
        References 
        ---------- 
        .. [1] Koopman, Siem Jan, Neil Shephard, and Jurgen A. Doornik. 1999. 
           Statistical Algorithms for Models in State Space Using SsfPack 2.2. 
           Econometrics Journal 2 (1): 107-60. doi:10.1111/1368-423X.00023. 
        &quot;&quot;&quot;</span>
        <span class="s1">params = self.handle_params(params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                                    <span class="s1">includes_fixed=includes_fixed)</span>

        <span class="s0"># If we're using complex-step differentiation, then we cannot use</span>
        <span class="s0"># Cholesky factorization</span>
        <span class="s3">if </span><span class="s1">complex_step:</span>
            <span class="s1">kwargs[</span><span class="s5">'inversion_method'</span><span class="s1">] = INVERT_UNIVARIATE | SOLVE_LU</span>

        <span class="s1">self.update(params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True,</span>
                    <span class="s1">complex_step=complex_step)</span>

        <span class="s3">return </span><span class="s1">self.ssm.loglikeobs(complex_step=complex_step</span><span class="s3">, </span><span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">simulation_smoother(self</span><span class="s3">, </span><span class="s1">simulation_output=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s2">r&quot;&quot;&quot; 
        Retrieve a simulation smoother for the state space model. 
 
        Parameters 
        ---------- 
        simulation_output : int, optional 
            Determines which simulation smoother output is calculated. 
            Default is all (including state and disturbances). 
        **kwargs 
            Additional keyword arguments, used to set the simulation output. 
            See `set_simulation_output` for more details. 
 
        Returns 
        ------- 
        SimulationSmoothResults 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.ssm.simulation_smoother(</span>
            <span class="s1">simulation_output=simulation_output</span><span class="s3">, </span><span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">_forecasts_error_partial_derivatives(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True,</span>
                                             <span class="s1">includes_fixed=</span><span class="s3">False,</span>
                                             <span class="s1">approx_complex_step=</span><span class="s3">None,</span>
                                             <span class="s1">approx_centered=</span><span class="s3">False,</span>
                                             <span class="s1">res=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s1">params = np.array(params</span><span class="s3">, </span><span class="s1">ndmin=</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s0"># We cannot use complex-step differentiation with non-transformed</span>
        <span class="s0"># parameters</span>
        <span class="s3">if </span><span class="s1">approx_complex_step </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">approx_complex_step = transformed</span>
        <span class="s3">if not </span><span class="s1">transformed </span><span class="s3">and </span><span class="s1">approx_complex_step:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Cannot use complex-step approximations to&quot;</span>
                             <span class="s5">&quot; calculate the observed_information_matrix&quot;</span>
                             <span class="s5">&quot; with untransformed parameters.&quot;</span><span class="s1">)</span>

        <span class="s0"># If we're using complex-step differentiation, then we cannot use</span>
        <span class="s0"># Cholesky factorization</span>
        <span class="s3">if </span><span class="s1">approx_complex_step:</span>
            <span class="s1">kwargs[</span><span class="s5">'inversion_method'</span><span class="s1">] = INVERT_UNIVARIATE | SOLVE_LU</span>

        <span class="s0"># Get values at the params themselves</span>
        <span class="s3">if </span><span class="s1">res </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self.update(params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                        <span class="s1">includes_fixed=includes_fixed</span><span class="s3">,</span>
                        <span class="s1">complex_step=approx_complex_step)</span>
            <span class="s1">res = self.ssm.filter(complex_step=approx_complex_step</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s0"># Setup</span>
        <span class="s1">n = len(params)</span>

        <span class="s0"># Compute partial derivatives w.r.t. forecast error and forecast</span>
        <span class="s0"># error covariance</span>
        <span class="s1">partials_forecasts_error = (</span>
            <span class="s1">np.zeros((self.k_endog</span><span class="s3">, </span><span class="s1">self.nobs</span><span class="s3">, </span><span class="s1">n))</span>
        <span class="s1">)</span>
        <span class="s1">partials_forecasts_error_cov = (</span>
            <span class="s1">np.zeros((self.k_endog</span><span class="s3">, </span><span class="s1">self.k_endog</span><span class="s3">, </span><span class="s1">self.nobs</span><span class="s3">, </span><span class="s1">n))</span>
        <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">approx_complex_step:</span>
            <span class="s1">epsilon = _get_epsilon(params</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, None, </span><span class="s1">n)</span>
            <span class="s1">increments = np.identity(n) * </span><span class="s4">1j </span><span class="s1">* epsilon</span>

            <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">ih </span><span class="s3">in </span><span class="s1">enumerate(increments):</span>
                <span class="s1">self.update(params + ih</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                            <span class="s1">includes_fixed=includes_fixed</span><span class="s3">,</span>
                            <span class="s1">complex_step=</span><span class="s3">True</span><span class="s1">)</span>
                <span class="s1">_res = self.ssm.filter(complex_step=</span><span class="s3">True, </span><span class="s1">**kwargs)</span>

                <span class="s1">partials_forecasts_error[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">i] = (</span>
                    <span class="s1">_res.forecasts_error.imag / epsilon[i]</span>
                <span class="s1">)</span>

                <span class="s1">partials_forecasts_error_cov[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">i] = (</span>
                    <span class="s1">_res.forecasts_error_cov.imag / epsilon[i]</span>
                <span class="s1">)</span>
        <span class="s3">elif not </span><span class="s1">approx_centered:</span>
            <span class="s1">epsilon = _get_epsilon(params</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, None, </span><span class="s1">n)</span>
            <span class="s1">ei = np.zeros((n</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">float)</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n):</span>
                <span class="s1">ei[i] = epsilon[i]</span>
                <span class="s1">self.update(params + ei</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                            <span class="s1">includes_fixed=includes_fixed</span><span class="s3">, </span><span class="s1">complex_step=</span><span class="s3">False</span><span class="s1">)</span>
                <span class="s1">_res = self.ssm.filter(complex_step=</span><span class="s3">False, </span><span class="s1">**kwargs)</span>

                <span class="s1">partials_forecasts_error[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">i] = (</span>
                    <span class="s1">_res.forecasts_error - res.forecasts_error) / epsilon[i]</span>

                <span class="s1">partials_forecasts_error_cov[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">i] = (</span>
                    <span class="s1">_res.forecasts_error_cov -</span>
                    <span class="s1">res.forecasts_error_cov) / epsilon[i]</span>
                <span class="s1">ei[i] = </span><span class="s4">0.0</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">epsilon = _get_epsilon(params</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, None, </span><span class="s1">n) / </span><span class="s4">2.</span>
            <span class="s1">ei = np.zeros((n</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">float)</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n):</span>
                <span class="s1">ei[i] = epsilon[i]</span>

                <span class="s1">self.update(params + ei</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                            <span class="s1">includes_fixed=includes_fixed</span><span class="s3">, </span><span class="s1">complex_step=</span><span class="s3">False</span><span class="s1">)</span>
                <span class="s1">_res1 = self.ssm.filter(complex_step=</span><span class="s3">False, </span><span class="s1">**kwargs)</span>

                <span class="s1">self.update(params - ei</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                            <span class="s1">includes_fixed=includes_fixed</span><span class="s3">, </span><span class="s1">complex_step=</span><span class="s3">False</span><span class="s1">)</span>
                <span class="s1">_res2 = self.ssm.filter(complex_step=</span><span class="s3">False, </span><span class="s1">**kwargs)</span>

                <span class="s1">partials_forecasts_error[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">i] = (</span>
                    <span class="s1">(_res1.forecasts_error - _res2.forecasts_error) /</span>
                    <span class="s1">(</span><span class="s4">2 </span><span class="s1">* epsilon[i]))</span>

                <span class="s1">partials_forecasts_error_cov[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">i] = (</span>
                    <span class="s1">(_res1.forecasts_error_cov - _res2.forecasts_error_cov) /</span>
                    <span class="s1">(</span><span class="s4">2 </span><span class="s1">* epsilon[i]))</span>

                <span class="s1">ei[i] = </span><span class="s4">0.0</span>

        <span class="s3">return </span><span class="s1">partials_forecasts_error</span><span class="s3">, </span><span class="s1">partials_forecasts_error_cov</span>

    <span class="s3">def </span><span class="s1">observed_information_matrix(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True,</span>
                                    <span class="s1">includes_fixed=</span><span class="s3">False,</span>
                                    <span class="s1">approx_complex_step=</span><span class="s3">None,</span>
                                    <span class="s1">approx_centered=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Observed information matrix 
 
        Parameters 
        ---------- 
        params : array_like, optional 
            Array of parameters at which to evaluate the loglikelihood 
            function. 
        **kwargs 
            Additional keyword arguments to pass to the Kalman filter. See 
            `KalmanFilter.filter` for more details. 
 
        Notes 
        ----- 
        This method is from Harvey (1989), which shows that the information 
        matrix only depends on terms from the gradient. This implementation is 
        partially analytic and partially numeric approximation, therefore, 
        because it uses the analytic formula for the information matrix, with 
        numerically computed elements of the gradient. 
 
        References 
        ---------- 
        Harvey, Andrew C. 1990. 
        Forecasting, Structural Time Series Models and the Kalman Filter. 
        Cambridge University Press. 
        &quot;&quot;&quot;</span>
        <span class="s1">params = np.array(params</span><span class="s3">, </span><span class="s1">ndmin=</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s0"># Setup</span>
        <span class="s1">n = len(params)</span>

        <span class="s0"># We cannot use complex-step differentiation with non-transformed</span>
        <span class="s0"># parameters</span>
        <span class="s3">if </span><span class="s1">approx_complex_step </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">approx_complex_step = transformed</span>
        <span class="s3">if not </span><span class="s1">transformed </span><span class="s3">and </span><span class="s1">approx_complex_step:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Cannot use complex-step approximations to&quot;</span>
                             <span class="s5">&quot; calculate the observed_information_matrix&quot;</span>
                             <span class="s5">&quot; with untransformed parameters.&quot;</span><span class="s1">)</span>

        <span class="s0"># Get values at the params themselves</span>
        <span class="s1">params = self.handle_params(params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                                    <span class="s1">includes_fixed=includes_fixed)</span>
        <span class="s1">self.update(params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True,</span>
                    <span class="s1">complex_step=approx_complex_step)</span>
        <span class="s0"># If we're using complex-step differentiation, then we cannot use</span>
        <span class="s0"># Cholesky factorization</span>
        <span class="s3">if </span><span class="s1">approx_complex_step:</span>
            <span class="s1">kwargs[</span><span class="s5">'inversion_method'</span><span class="s1">] = INVERT_UNIVARIATE | SOLVE_LU</span>
        <span class="s1">res = self.ssm.filter(complex_step=approx_complex_step</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s1">dtype = self.ssm.dtype</span>

        <span class="s0"># Save this for inversion later</span>
        <span class="s1">inv_forecasts_error_cov = res.forecasts_error_cov.copy()</span>

        <span class="s1">partials_forecasts_error</span><span class="s3">, </span><span class="s1">partials_forecasts_error_cov = (</span>
            <span class="s1">self._forecasts_error_partial_derivatives(</span>
                <span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">, </span><span class="s1">includes_fixed=includes_fixed</span><span class="s3">,</span>
                <span class="s1">approx_complex_step=approx_complex_step</span><span class="s3">,</span>
                <span class="s1">approx_centered=approx_centered</span><span class="s3">, </span><span class="s1">res=res</span><span class="s3">, </span><span class="s1">**kwargs))</span>

        <span class="s0"># Compute the information matrix</span>
        <span class="s1">tmp = np.zeros((self.k_endog</span><span class="s3">, </span><span class="s1">self.k_endog</span><span class="s3">, </span><span class="s1">self.nobs</span><span class="s3">, </span><span class="s1">n)</span><span class="s3">, </span><span class="s1">dtype=dtype)</span>

        <span class="s1">information_matrix = np.zeros((n</span><span class="s3">, </span><span class="s1">n)</span><span class="s3">, </span><span class="s1">dtype=dtype)</span>
        <span class="s1">d = np.maximum(self.ssm.loglikelihood_burn</span><span class="s3">, </span><span class="s1">res.nobs_diffuse)</span>
        <span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">range(d</span><span class="s3">, </span><span class="s1">self.nobs):</span>
            <span class="s1">inv_forecasts_error_cov[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">t] = (</span>
                <span class="s1">np.linalg.inv(res.forecasts_error_cov[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">t])</span>
            <span class="s1">)</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n):</span>
                <span class="s1">tmp[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">t</span><span class="s3">, </span><span class="s1">i] = np.dot(</span>
                    <span class="s1">inv_forecasts_error_cov[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">t]</span><span class="s3">,</span>
                    <span class="s1">partials_forecasts_error_cov[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">t</span><span class="s3">, </span><span class="s1">i]</span>
                <span class="s1">)</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n):</span>
                <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(n):</span>
                    <span class="s1">information_matrix[i</span><span class="s3">, </span><span class="s1">j] += (</span>
                        <span class="s4">0.5 </span><span class="s1">* np.trace(np.dot(tmp[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">t</span><span class="s3">, </span><span class="s1">i]</span><span class="s3">,</span>
                                              <span class="s1">tmp[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">t</span><span class="s3">, </span><span class="s1">j]))</span>
                    <span class="s1">)</span>
                    <span class="s1">information_matrix[i</span><span class="s3">, </span><span class="s1">j] += np.inner(</span>
                        <span class="s1">partials_forecasts_error[:</span><span class="s3">, </span><span class="s1">t</span><span class="s3">, </span><span class="s1">i]</span><span class="s3">,</span>
                        <span class="s1">np.dot(inv_forecasts_error_cov[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">t]</span><span class="s3">,</span>
                               <span class="s1">partials_forecasts_error[:</span><span class="s3">, </span><span class="s1">t</span><span class="s3">, </span><span class="s1">j])</span>
                    <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">information_matrix / (self.nobs - self.ssm.loglikelihood_burn)</span>

    <span class="s3">def </span><span class="s1">opg_information_matrix(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True,</span>
                               <span class="s1">includes_fixed=</span><span class="s3">False, </span><span class="s1">approx_complex_step=</span><span class="s3">None,</span>
                               <span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Outer product of gradients information matrix 
 
        Parameters 
        ---------- 
        params : array_like, optional 
            Array of parameters at which to evaluate the loglikelihood 
            function. 
        **kwargs 
            Additional arguments to the `loglikeobs` method. 
 
        References 
        ---------- 
        Berndt, Ernst R., Bronwyn Hall, Robert Hall, and Jerry Hausman. 1974. 
        Estimation and Inference in Nonlinear Structural Models. 
        NBER Chapters. National Bureau of Economic Research, Inc. 
        &quot;&quot;&quot;</span>
        <span class="s0"># We cannot use complex-step differentiation with non-transformed</span>
        <span class="s0"># parameters</span>
        <span class="s3">if </span><span class="s1">approx_complex_step </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">approx_complex_step = transformed</span>
        <span class="s3">if not </span><span class="s1">transformed </span><span class="s3">and </span><span class="s1">approx_complex_step:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Cannot use complex-step approximations to&quot;</span>
                             <span class="s5">&quot; calculate the observed_information_matrix&quot;</span>
                             <span class="s5">&quot; with untransformed parameters.&quot;</span><span class="s1">)</span>

        <span class="s1">score_obs = self.score_obs(params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                                   <span class="s1">includes_fixed=includes_fixed</span><span class="s3">,</span>
                                   <span class="s1">approx_complex_step=approx_complex_step</span><span class="s3">,</span>
                                   <span class="s1">**kwargs).transpose()</span>
        <span class="s3">return </span><span class="s1">(</span>
            <span class="s1">np.inner(score_obs</span><span class="s3">, </span><span class="s1">score_obs) /</span>
            <span class="s1">(self.nobs - self.ssm.loglikelihood_burn)</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_score_complex_step(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s0"># the default epsilon can be too small</span>
        <span class="s0"># inversion_method = INVERT_UNIVARIATE | SOLVE_LU</span>
        <span class="s1">epsilon = _get_epsilon(params</span><span class="s3">, </span><span class="s4">2.</span><span class="s3">, None, </span><span class="s1">len(params))</span>
        <span class="s1">kwargs[</span><span class="s5">'transformed'</span><span class="s1">] = </span><span class="s3">True</span>
        <span class="s1">kwargs[</span><span class="s5">'complex_step'</span><span class="s1">] = </span><span class="s3">True</span>
        <span class="s3">return </span><span class="s1">approx_fprime_cs(params</span><span class="s3">, </span><span class="s1">self.loglike</span><span class="s3">, </span><span class="s1">epsilon=epsilon</span><span class="s3">,</span>
                                <span class="s1">kwargs=kwargs)</span>

    <span class="s3">def </span><span class="s1">_score_finite_difference(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">approx_centered=</span><span class="s3">False,</span>
                                 <span class="s1">**kwargs):</span>
        <span class="s1">kwargs[</span><span class="s5">'transformed'</span><span class="s1">] = </span><span class="s3">True</span>
        <span class="s3">return </span><span class="s1">approx_fprime(params</span><span class="s3">, </span><span class="s1">self.loglike</span><span class="s3">, </span><span class="s1">kwargs=kwargs</span><span class="s3">,</span>
                             <span class="s1">centered=approx_centered)</span>

    <span class="s3">def </span><span class="s1">_score_harvey(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">approx_complex_step=</span><span class="s3">True, </span><span class="s1">**kwargs):</span>
        <span class="s1">score_obs = self._score_obs_harvey(</span>
            <span class="s1">params</span><span class="s3">, </span><span class="s1">approx_complex_step=approx_complex_step</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s3">return </span><span class="s1">np.sum(score_obs</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_score_obs_harvey(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">approx_complex_step=</span><span class="s3">True,</span>
                          <span class="s1">approx_centered=</span><span class="s3">False, </span><span class="s1">includes_fixed=</span><span class="s3">False,</span>
                          <span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Score 
 
        Parameters 
        ---------- 
        params : array_like, optional 
            Array of parameters at which to evaluate the loglikelihood 
            function. 
        **kwargs 
            Additional keyword arguments to pass to the Kalman filter. See 
            `KalmanFilter.filter` for more details. 
 
        Notes 
        ----- 
        This method is from Harvey (1989), section 3.4.5 
 
        References 
        ---------- 
        Harvey, Andrew C. 1990. 
        Forecasting, Structural Time Series Models and the Kalman Filter. 
        Cambridge University Press. 
        &quot;&quot;&quot;</span>
        <span class="s1">params = np.array(params</span><span class="s3">, </span><span class="s1">ndmin=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">n = len(params)</span>

        <span class="s0"># Get values at the params themselves</span>
        <span class="s1">self.update(params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=includes_fixed</span><span class="s3">,</span>
                    <span class="s1">complex_step=approx_complex_step)</span>
        <span class="s3">if </span><span class="s1">approx_complex_step:</span>
            <span class="s1">kwargs[</span><span class="s5">'inversion_method'</span><span class="s1">] = INVERT_UNIVARIATE | SOLVE_LU</span>
        <span class="s3">if </span><span class="s5">'transformed' </span><span class="s3">in </span><span class="s1">kwargs:</span>
            <span class="s3">del </span><span class="s1">kwargs[</span><span class="s5">'transformed'</span><span class="s1">]</span>
        <span class="s1">res = self.ssm.filter(complex_step=approx_complex_step</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s0"># Get forecasts error partials</span>
        <span class="s1">partials_forecasts_error</span><span class="s3">, </span><span class="s1">partials_forecasts_error_cov = (</span>
            <span class="s1">self._forecasts_error_partial_derivatives(</span>
                <span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=includes_fixed</span><span class="s3">,</span>
                <span class="s1">approx_complex_step=approx_complex_step</span><span class="s3">,</span>
                <span class="s1">approx_centered=approx_centered</span><span class="s3">, </span><span class="s1">res=res</span><span class="s3">, </span><span class="s1">**kwargs))</span>

        <span class="s0"># Compute partial derivatives w.r.t. likelihood function</span>
        <span class="s1">partials = np.zeros((self.nobs</span><span class="s3">, </span><span class="s1">n))</span>
        <span class="s1">k_endog = self.k_endog</span>
        <span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">range(self.nobs):</span>
            <span class="s1">inv_forecasts_error_cov = np.linalg.inv(</span>
                    <span class="s1">res.forecasts_error_cov[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">t])</span>

            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n):</span>
                <span class="s1">partials[t</span><span class="s3">, </span><span class="s1">i] += np.trace(np.dot(</span>
                    <span class="s1">np.dot(inv_forecasts_error_cov</span><span class="s3">,</span>
                           <span class="s1">partials_forecasts_error_cov[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">t</span><span class="s3">, </span><span class="s1">i])</span><span class="s3">,</span>
                    <span class="s1">(np.eye(k_endog) -</span>
                     <span class="s1">np.dot(inv_forecasts_error_cov</span><span class="s3">,</span>
                            <span class="s1">np.outer(res.forecasts_error[:</span><span class="s3">, </span><span class="s1">t]</span><span class="s3">,</span>
                                     <span class="s1">res.forecasts_error[:</span><span class="s3">, </span><span class="s1">t])))))</span>
                <span class="s0"># 2 * dv / di * F^{-1} v_t</span>
                <span class="s0"># where x = F^{-1} v_t or F x = v</span>
                <span class="s1">partials[t</span><span class="s3">, </span><span class="s1">i] += </span><span class="s4">2 </span><span class="s1">* np.dot(</span>
                    <span class="s1">partials_forecasts_error[:</span><span class="s3">, </span><span class="s1">t</span><span class="s3">, </span><span class="s1">i]</span><span class="s3">,</span>
                    <span class="s1">np.dot(inv_forecasts_error_cov</span><span class="s3">, </span><span class="s1">res.forecasts_error[:</span><span class="s3">, </span><span class="s1">t]))</span>

        <span class="s3">return </span><span class="s1">-partials / </span><span class="s4">2.</span>

    <span class="s1">_score_param_names = [</span><span class="s5">'transformed'</span><span class="s3">, </span><span class="s5">'includes_fixed'</span><span class="s3">, </span><span class="s5">'score_method'</span><span class="s3">,</span>
                          <span class="s5">'approx_complex_step'</span><span class="s3">, </span><span class="s5">'approx_centered'</span><span class="s1">]</span>
    <span class="s1">_score_param_defaults = [</span><span class="s3">True, False, </span><span class="s5">'approx'</span><span class="s3">, None, False</span><span class="s1">]</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Compute the score function at params. 
 
        Parameters 
        ---------- 
        params : array_like 
            Array of parameters at which to evaluate the score. 
        *args 
            Additional positional arguments to the `loglike` method. 
        **kwargs 
            Additional keyword arguments to the `loglike` method. 
 
        Returns 
        ------- 
        score : ndarray 
            Score, evaluated at `params`. 
 
        Notes 
        ----- 
        This is a numerical approximation, calculated using first-order complex 
        step differentiation on the `loglike` method. 
 
        Both args and kwargs are necessary because the optimizer from 
        `fit` must call this function and only supports passing arguments via 
        args (for example `scipy.optimize.fmin_l_bfgs`). 
        &quot;&quot;&quot;</span>
        <span class="s1">(transformed</span><span class="s3">, </span><span class="s1">includes_fixed</span><span class="s3">, </span><span class="s1">method</span><span class="s3">, </span><span class="s1">approx_complex_step</span><span class="s3">,</span>
         <span class="s1">approx_centered</span><span class="s3">, </span><span class="s1">kwargs) = (</span>
            <span class="s1">_handle_args(MLEModel._score_param_names</span><span class="s3">,</span>
                         <span class="s1">MLEModel._score_param_defaults</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs))</span>
        <span class="s0"># For fit() calls, the method is called 'score_method' (to distinguish</span>
        <span class="s0"># it from the method used for fit) but generally in kwargs the method</span>
        <span class="s0"># will just be called 'method'</span>
        <span class="s3">if </span><span class="s5">'method' </span><span class="s3">in </span><span class="s1">kwargs:</span>
            <span class="s1">method = kwargs.pop(</span><span class="s5">'method'</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">approx_complex_step </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">approx_complex_step = </span><span class="s3">not </span><span class="s1">self.ssm._complex_endog</span>
        <span class="s3">if </span><span class="s1">approx_complex_step </span><span class="s3">and </span><span class="s1">self.ssm._complex_endog:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot use complex step derivatives when data'</span>
                             <span class="s5">' or parameters are complex.'</span><span class="s1">)</span>

        <span class="s1">out = self.handle_params(</span>
            <span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">, </span><span class="s1">includes_fixed=includes_fixed</span><span class="s3">,</span>
            <span class="s1">return_jacobian=</span><span class="s3">not </span><span class="s1">transformed)</span>
        <span class="s3">if </span><span class="s1">transformed:</span>
            <span class="s1">params = out</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">params</span><span class="s3">, </span><span class="s1">transform_score = out</span>

        <span class="s3">if </span><span class="s1">method == </span><span class="s5">'harvey'</span><span class="s1">:</span>
            <span class="s1">kwargs[</span><span class="s5">'includes_fixed'</span><span class="s1">] = </span><span class="s3">True</span>
            <span class="s1">score = self._score_harvey(</span>
                <span class="s1">params</span><span class="s3">, </span><span class="s1">approx_complex_step=approx_complex_step</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s3">elif </span><span class="s1">method == </span><span class="s5">'approx' </span><span class="s3">and </span><span class="s1">approx_complex_step:</span>
            <span class="s1">kwargs[</span><span class="s5">'includes_fixed'</span><span class="s1">] = </span><span class="s3">True</span>
            <span class="s1">score = self._score_complex_step(params</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s3">elif </span><span class="s1">method == </span><span class="s5">'approx'</span><span class="s1">:</span>
            <span class="s1">kwargs[</span><span class="s5">'includes_fixed'</span><span class="s1">] = </span><span class="s3">True</span>
            <span class="s1">score = self._score_finite_difference(</span>
                <span class="s1">params</span><span class="s3">, </span><span class="s1">approx_centered=approx_centered</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">'Invalid score method.'</span><span class="s1">)</span>

        <span class="s3">if not </span><span class="s1">transformed:</span>
            <span class="s1">score = np.dot(transform_score</span><span class="s3">, </span><span class="s1">score)</span>

        <span class="s3">if </span><span class="s1">self._has_fixed_params </span><span class="s3">and not </span><span class="s1">includes_fixed:</span>
            <span class="s1">score = score[self._free_params_index]</span>

        <span class="s3">return </span><span class="s1">score</span>

    <span class="s3">def </span><span class="s1">score_obs(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">method=</span><span class="s5">'approx'</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True,</span>
                  <span class="s1">includes_fixed=</span><span class="s3">False, </span><span class="s1">approx_complex_step=</span><span class="s3">None,</span>
                  <span class="s1">approx_centered=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Compute the score per observation, evaluated at params 
 
        Parameters 
        ---------- 
        params : array_like 
            Array of parameters at which to evaluate the score. 
        **kwargs 
            Additional arguments to the `loglike` method. 
 
        Returns 
        ------- 
        score : ndarray 
            Score per observation, evaluated at `params`. 
 
        Notes 
        ----- 
        This is a numerical approximation, calculated using first-order complex 
        step differentiation on the `loglikeobs` method. 
        &quot;&quot;&quot;</span>
        <span class="s3">if not </span><span class="s1">transformed </span><span class="s3">and </span><span class="s1">approx_complex_step:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Cannot use complex-step approximations to&quot;</span>
                             <span class="s5">&quot; calculate the score at each observation&quot;</span>
                             <span class="s5">&quot; with untransformed parameters.&quot;</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">approx_complex_step </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">approx_complex_step = </span><span class="s3">not </span><span class="s1">self.ssm._complex_endog</span>
        <span class="s3">if </span><span class="s1">approx_complex_step </span><span class="s3">and </span><span class="s1">self.ssm._complex_endog:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot use complex step derivatives when data'</span>
                             <span class="s5">' or parameters are complex.'</span><span class="s1">)</span>

        <span class="s1">params = self.handle_params(params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True,</span>
                                    <span class="s1">includes_fixed=includes_fixed)</span>
        <span class="s1">kwargs[</span><span class="s5">'transformed'</span><span class="s1">] = transformed</span>
        <span class="s1">kwargs[</span><span class="s5">'includes_fixed'</span><span class="s1">] = </span><span class="s3">True</span>

        <span class="s3">if </span><span class="s1">method == </span><span class="s5">'harvey'</span><span class="s1">:</span>
            <span class="s1">score = self._score_obs_harvey(</span>
                <span class="s1">params</span><span class="s3">, </span><span class="s1">approx_complex_step=approx_complex_step</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s3">elif </span><span class="s1">method == </span><span class="s5">'approx' </span><span class="s3">and </span><span class="s1">approx_complex_step:</span>
            <span class="s0"># the default epsilon can be too small</span>
            <span class="s1">epsilon = _get_epsilon(params</span><span class="s3">, </span><span class="s4">2.</span><span class="s3">, None, </span><span class="s1">len(params))</span>
            <span class="s1">kwargs[</span><span class="s5">'complex_step'</span><span class="s1">] = </span><span class="s3">True</span>
            <span class="s1">score = approx_fprime_cs(params</span><span class="s3">, </span><span class="s1">self.loglikeobs</span><span class="s3">, </span><span class="s1">epsilon=epsilon</span><span class="s3">,</span>
                                     <span class="s1">kwargs=kwargs)</span>
        <span class="s3">elif </span><span class="s1">method == </span><span class="s5">'approx'</span><span class="s1">:</span>
            <span class="s1">score = approx_fprime(params</span><span class="s3">, </span><span class="s1">self.loglikeobs</span><span class="s3">, </span><span class="s1">kwargs=kwargs</span><span class="s3">,</span>
                                  <span class="s1">centered=approx_centered)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">'Invalid scoreobs method.'</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">score</span>

    <span class="s1">_hessian_param_names = [</span><span class="s5">'transformed'</span><span class="s3">, </span><span class="s5">'hessian_method'</span><span class="s3">,</span>
                            <span class="s5">'approx_complex_step'</span><span class="s3">, </span><span class="s5">'approx_centered'</span><span class="s1">]</span>
    <span class="s1">_hessian_param_defaults = [</span><span class="s3">True, </span><span class="s5">'approx'</span><span class="s3">, None, False</span><span class="s1">]</span>

    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s2">r&quot;&quot;&quot; 
        Hessian matrix of the likelihood function, evaluated at the given 
        parameters 
 
        Parameters 
        ---------- 
        params : array_like 
            Array of parameters at which to evaluate the hessian. 
        *args 
            Additional positional arguments to the `loglike` method. 
        **kwargs 
            Additional keyword arguments to the `loglike` method. 
 
        Returns 
        ------- 
        hessian : ndarray 
            Hessian matrix evaluated at `params` 
 
        Notes 
        ----- 
        This is a numerical approximation. 
 
        Both args and kwargs are necessary because the optimizer from 
        `fit` must call this function and only supports passing arguments via 
        args (for example `scipy.optimize.fmin_l_bfgs`). 
        &quot;&quot;&quot;</span>
        <span class="s1">transformed</span><span class="s3">, </span><span class="s1">method</span><span class="s3">, </span><span class="s1">approx_complex_step</span><span class="s3">, </span><span class="s1">approx_centered</span><span class="s3">, </span><span class="s1">kwargs = (</span>
            <span class="s1">_handle_args(MLEModel._hessian_param_names</span><span class="s3">,</span>
                         <span class="s1">MLEModel._hessian_param_defaults</span><span class="s3">,</span>
                         <span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs))</span>
        <span class="s0"># For fit() calls, the method is called 'hessian_method' (to</span>
        <span class="s0"># distinguish it from the method used for fit) but generally in kwargs</span>
        <span class="s0"># the method will just be called 'method'</span>
        <span class="s3">if </span><span class="s5">'method' </span><span class="s3">in </span><span class="s1">kwargs:</span>
            <span class="s1">method = kwargs.pop(</span><span class="s5">'method'</span><span class="s1">)</span>

        <span class="s3">if not </span><span class="s1">transformed </span><span class="s3">and </span><span class="s1">approx_complex_step:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Cannot use complex-step approximations to&quot;</span>
                             <span class="s5">&quot; calculate the hessian with untransformed&quot;</span>
                             <span class="s5">&quot; parameters.&quot;</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">approx_complex_step </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">approx_complex_step = </span><span class="s3">not </span><span class="s1">self.ssm._complex_endog</span>
        <span class="s3">if </span><span class="s1">approx_complex_step </span><span class="s3">and </span><span class="s1">self.ssm._complex_endog:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot use complex step derivatives when data'</span>
                             <span class="s5">' or parameters are complex.'</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">method == </span><span class="s5">'oim'</span><span class="s1">:</span>
            <span class="s1">hessian = self._hessian_oim(</span>
                <span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                <span class="s1">approx_complex_step=approx_complex_step</span><span class="s3">,</span>
                <span class="s1">approx_centered=approx_centered</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s3">elif </span><span class="s1">method == </span><span class="s5">'opg'</span><span class="s1">:</span>
            <span class="s1">hessian = self._hessian_opg(</span>
                <span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                <span class="s1">approx_complex_step=approx_complex_step</span><span class="s3">,</span>
                <span class="s1">approx_centered=approx_centered</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s3">elif </span><span class="s1">method == </span><span class="s5">'approx' </span><span class="s3">and </span><span class="s1">approx_complex_step:</span>
            <span class="s1">hessian = self._hessian_complex_step(</span>
                <span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s3">elif </span><span class="s1">method == </span><span class="s5">'approx'</span><span class="s1">:</span>
            <span class="s1">hessian = self._hessian_finite_difference(</span>
                <span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                <span class="s1">approx_centered=approx_centered</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">'Invalid Hessian calculation method.'</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">hessian</span>

    <span class="s3">def </span><span class="s1">_hessian_oim(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Hessian matrix computed using the Harvey (1989) information matrix 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">-self.observed_information_matrix(params</span><span class="s3">, </span><span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">_hessian_opg(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Hessian matrix computed using the outer product of gradients 
        information matrix 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">-self.opg_information_matrix(params</span><span class="s3">, </span><span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">_hessian_finite_difference(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">approx_centered=</span><span class="s3">False,</span>
                                   <span class="s1">**kwargs):</span>
        <span class="s1">params = np.array(params</span><span class="s3">, </span><span class="s1">ndmin=</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s1">warnings.warn(</span><span class="s5">'Calculation of the Hessian using finite differences'</span>
                      <span class="s5">' is usually subject to substantial approximation'</span>
                      <span class="s5">' errors.'</span><span class="s3">, </span><span class="s1">PrecisionWarning)</span>

        <span class="s3">if not </span><span class="s1">approx_centered:</span>
            <span class="s1">epsilon = _get_epsilon(params</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, None, </span><span class="s1">len(params))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">epsilon = _get_epsilon(params</span><span class="s3">, </span><span class="s4">4</span><span class="s3">, None, </span><span class="s1">len(params)) / </span><span class="s4">2</span>
        <span class="s1">hessian = approx_fprime(params</span><span class="s3">, </span><span class="s1">self._score_finite_difference</span><span class="s3">,</span>
                                <span class="s1">epsilon=epsilon</span><span class="s3">, </span><span class="s1">kwargs=kwargs</span><span class="s3">,</span>
                                <span class="s1">centered=approx_centered)</span>

        <span class="s3">return </span><span class="s1">hessian / (self.nobs - self.ssm.loglikelihood_burn)</span>

    <span class="s3">def </span><span class="s1">_hessian_complex_step(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Hessian matrix computed by second-order complex-step differentiation 
        on the `loglike` function. 
        &quot;&quot;&quot;</span>
        <span class="s0"># the default epsilon can be too small</span>
        <span class="s1">epsilon = _get_epsilon(params</span><span class="s3">, </span><span class="s4">3.</span><span class="s3">, None, </span><span class="s1">len(params))</span>
        <span class="s1">kwargs[</span><span class="s5">'transformed'</span><span class="s1">] = </span><span class="s3">True</span>
        <span class="s1">kwargs[</span><span class="s5">'complex_step'</span><span class="s1">] = </span><span class="s3">True</span>
        <span class="s1">hessian = approx_hess_cs(</span>
            <span class="s1">params</span><span class="s3">, </span><span class="s1">self.loglike</span><span class="s3">, </span><span class="s1">epsilon=epsilon</span><span class="s3">, </span><span class="s1">kwargs=kwargs)</span>

        <span class="s3">return </span><span class="s1">hessian / (self.nobs - self.ssm.loglikelihood_burn)</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">start_params(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (array) Starting parameters for maximum likelihood estimation. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s5">'_start_params'</span><span class="s1">):</span>
            <span class="s3">return </span><span class="s1">self._start_params</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">param_names(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (list of str) List of human readable parameter names (for parameters 
        actually included in the model). 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s5">'_param_names'</span><span class="s1">):</span>
            <span class="s3">return </span><span class="s1">self._param_names</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s1">names = [</span><span class="s5">'param.%d' </span><span class="s1">% i </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(self.start_params))]</span>
            <span class="s3">except </span><span class="s1">NotImplementedError:</span>
                <span class="s1">names = []</span>
            <span class="s3">return </span><span class="s1">names</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">state_names(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (list of str) List of human readable names for unobserved states. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s5">'_state_names'</span><span class="s1">):</span>
            <span class="s3">return </span><span class="s1">self._state_names</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">names = [</span><span class="s5">'state.%d' </span><span class="s1">% i </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.k_states)]</span>
        <span class="s3">return </span><span class="s1">names</span>

    <span class="s3">def </span><span class="s1">transform_jacobian(self</span><span class="s3">, </span><span class="s1">unconstrained</span><span class="s3">, </span><span class="s1">approx_centered=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Jacobian matrix for the parameter transformation function 
 
        Parameters 
        ---------- 
        unconstrained : array_like 
            Array of unconstrained parameters used by the optimizer. 
 
        Returns 
        ------- 
        jacobian : ndarray 
            Jacobian matrix of the transformation, evaluated at `unconstrained` 
 
        See Also 
        -------- 
        transform_params 
 
        Notes 
        ----- 
        This is a numerical approximation using finite differences. Note that 
        in general complex step methods cannot be used because it is not 
        guaranteed that the `transform_params` method is a real function (e.g. 
        if Cholesky decomposition is used). 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">approx_fprime(unconstrained</span><span class="s3">, </span><span class="s1">self.transform_params</span><span class="s3">,</span>
                             <span class="s1">centered=approx_centered)</span>

    <span class="s3">def </span><span class="s1">transform_params(self</span><span class="s3">, </span><span class="s1">unconstrained):</span>
        <span class="s2">&quot;&quot;&quot; 
        Transform unconstrained parameters used by the optimizer to constrained 
        parameters used in likelihood evaluation 
 
        Parameters 
        ---------- 
        unconstrained : array_like 
            Array of unconstrained parameters used by the optimizer, to be 
            transformed. 
 
        Returns 
        ------- 
        constrained : array_like 
            Array of constrained parameters which may be used in likelihood 
            evaluation. 
 
        Notes 
        ----- 
        This is a noop in the base class, subclasses should override where 
        appropriate. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.array(unconstrained</span><span class="s3">, </span><span class="s1">ndmin=</span><span class="s4">1</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">untransform_params(self</span><span class="s3">, </span><span class="s1">constrained):</span>
        <span class="s2">&quot;&quot;&quot; 
        Transform constrained parameters used in likelihood evaluation 
        to unconstrained parameters used by the optimizer 
 
        Parameters 
        ---------- 
        constrained : array_like 
            Array of constrained parameters used in likelihood evaluation, to 
            be transformed. 
 
        Returns 
        ------- 
        unconstrained : array_like 
            Array of unconstrained parameters used by the optimizer. 
 
        Notes 
        ----- 
        This is a noop in the base class, subclasses should override where 
        appropriate. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.array(constrained</span><span class="s3">, </span><span class="s1">ndmin=</span><span class="s4">1</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">handle_params(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">False,</span>
                      <span class="s1">return_jacobian=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Ensure model parameters satisfy shape and other requirements 
        &quot;&quot;&quot;</span>
        <span class="s1">params = np.array(params</span><span class="s3">, </span><span class="s1">ndmin=</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s0"># Never want integer dtype, so convert to floats</span>
        <span class="s3">if </span><span class="s1">np.issubdtype(params.dtype</span><span class="s3">, </span><span class="s1">np.integer):</span>
            <span class="s1">params = params.astype(np.float64)</span>

        <span class="s3">if not </span><span class="s1">includes_fixed </span><span class="s3">and </span><span class="s1">self._has_fixed_params:</span>
            <span class="s1">k_params = len(self.param_names)</span>
            <span class="s1">new_params = np.zeros(k_params</span><span class="s3">, </span><span class="s1">dtype=params.dtype) * np.nan</span>
            <span class="s1">new_params[self._free_params_index] = params</span>
            <span class="s1">params = new_params</span>

        <span class="s3">if not </span><span class="s1">transformed:</span>
            <span class="s0"># It may be the case that the transformation relies on having</span>
            <span class="s0"># &quot;some&quot; (non-NaN) values for the fixed parameters, even if we will</span>
            <span class="s0"># not actually be transforming the fixed parameters (as they will)</span>
            <span class="s0"># be set below regardless</span>
            <span class="s3">if not </span><span class="s1">includes_fixed </span><span class="s3">and </span><span class="s1">self._has_fixed_params:</span>
                <span class="s1">params[self._fixed_params_index] = (</span>
                    <span class="s1">list(self._fixed_params.values()))</span>

            <span class="s3">if </span><span class="s1">return_jacobian:</span>
                <span class="s1">transform_score = self.transform_jacobian(params)</span>
            <span class="s1">params = self.transform_params(params)</span>

        <span class="s3">if not </span><span class="s1">includes_fixed </span><span class="s3">and </span><span class="s1">self._has_fixed_params:</span>
            <span class="s1">params[self._fixed_params_index] = (</span>
                <span class="s1">list(self._fixed_params.values()))</span>

        <span class="s3">return </span><span class="s1">(params</span><span class="s3">, </span><span class="s1">transform_score) </span><span class="s3">if </span><span class="s1">return_jacobian </span><span class="s3">else </span><span class="s1">params</span>

    <span class="s3">def </span><span class="s1">update(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">False,</span>
               <span class="s1">complex_step=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Update the parameters of the model 
 
        Parameters 
        ---------- 
        params : array_like 
            Array of new parameters. 
        transformed : bool, optional 
            Whether or not `params` is already transformed. If set to False, 
            `transform_params` is called. Default is True. 
 
        Returns 
        ------- 
        params : array_like 
            Array of parameters. 
 
        Notes 
        ----- 
        Since Model is a base class, this method should be overridden by 
        subclasses to perform actual updating steps. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.handle_params(params=params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                                  <span class="s1">includes_fixed=includes_fixed)</span>

    <span class="s3">def </span><span class="s1">_validate_out_of_sample_exog(self</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">out_of_sample):</span>
        <span class="s2">&quot;&quot;&quot; 
        Validate given `exog` as satisfactory for out-of-sample operations 
 
        Parameters 
        ---------- 
        exog : array_like or None 
            New observations of exogenous regressors, if applicable. 
        out_of_sample : int 
            Number of new observations required. 
 
        Returns 
        ------- 
        exog : array or None 
            A numpy array of shape (out_of_sample, k_exog) if the model 
            contains an `exog` component, or None if it does not. 
        &quot;&quot;&quot;</span>
        <span class="s1">k_exog = getattr(self</span><span class="s3">, </span><span class="s5">'k_exog'</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">out_of_sample </span><span class="s3">and </span><span class="s1">k_exog &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Out-of-sample operations in a model'</span>
                                 <span class="s5">' with a regression component require'</span>
                                 <span class="s5">' additional exogenous values via the'</span>
                                 <span class="s5">' `exog` argument.'</span><span class="s1">)</span>
            <span class="s1">exog = np.array(exog)</span>
            <span class="s1">required_exog_shape = (out_of_sample</span><span class="s3">, </span><span class="s1">self.k_exog)</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s1">exog = exog.reshape(required_exog_shape)</span>
            <span class="s3">except </span><span class="s1">ValueError:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Provided exogenous values are not of the'</span>
                                 <span class="s5">' appropriate shape. Required %s, got %s.'</span>
                                 <span class="s1">% (str(required_exog_shape)</span><span class="s3">,</span>
                                    <span class="s1">str(exog.shape)))</span>
        <span class="s3">elif </span><span class="s1">k_exog &gt; </span><span class="s4">0 </span><span class="s3">and </span><span class="s1">exog </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">exog = </span><span class="s3">None</span>
            <span class="s1">warnings.warn(</span><span class="s5">'Exogenous array provided, but additional data'</span>
                          <span class="s5">' is not required. `exog` argument ignored.'</span><span class="s3">,</span>
                          <span class="s1">ValueWarning)</span>

        <span class="s3">return </span><span class="s1">exog</span>

    <span class="s3">def </span><span class="s1">_get_extension_time_varying_matrices(</span>
            <span class="s1">self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">out_of_sample</span><span class="s3">, </span><span class="s1">extend_kwargs=</span><span class="s3">None,</span>
            <span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Get updated time-varying state space system matrices 
 
        Parameters 
        ---------- 
        params : array_like 
            Array of parameters used to construct the time-varying system 
            matrices. 
        exog : array_like or None 
            New observations of exogenous regressors, if applicable. 
        out_of_sample : int 
            Number of new observations required. 
        extend_kwargs : dict, optional 
            Dictionary of keyword arguments to pass to the state space model 
            constructor. For example, for an SARIMAX state space model, this 
            could be used to pass the `concentrate_scale=True` keyword 
            argument. Any arguments that are not explicitly set in this 
            dictionary will be copied from the current model instance. 
        transformed : bool, optional 
            Whether or not `start_params` is already transformed. Default is 
            True. 
        includes_fixed : bool, optional 
            If parameters were previously fixed with the `fix_params` method, 
            this argument describes whether or not `start_params` also includes 
            the fixed parameters, in addition to the free parameters. Default 
            is False. 
        &quot;&quot;&quot;</span>
        <span class="s0"># Get the appropriate exog for the extended sample</span>
        <span class="s1">exog = self._validate_out_of_sample_exog(exog</span><span class="s3">, </span><span class="s1">out_of_sample)</span>

        <span class="s0"># Create extended model</span>
        <span class="s3">if </span><span class="s1">extend_kwargs </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">extend_kwargs = {}</span>

        <span class="s0"># Handle trend offset for extended model</span>
        <span class="s3">if </span><span class="s1">getattr(self</span><span class="s3">, </span><span class="s5">'k_trend'</span><span class="s3">, </span><span class="s4">0</span><span class="s1">) &gt; </span><span class="s4">0 </span><span class="s3">and </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s5">'trend_offset'</span><span class="s1">):</span>
            <span class="s1">extend_kwargs.setdefault(</span>
                <span class="s5">'trend_offset'</span><span class="s3">, </span><span class="s1">self.trend_offset + self.nobs)</span>

        <span class="s1">mod_extend = self.clone(</span>
            <span class="s1">endog=np.zeros((out_of_sample</span><span class="s3">, </span><span class="s1">self.k_endog))</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">,</span>
            <span class="s1">**extend_kwargs)</span>
        <span class="s1">mod_extend.update(params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                          <span class="s1">includes_fixed=includes_fixed)</span>

        <span class="s0"># Retrieve the extensions to the time-varying system matrices and</span>
        <span class="s0"># put them in kwargs</span>
        <span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">self.ssm.shapes.keys():</span>
            <span class="s3">if </span><span class="s1">name == </span><span class="s5">'obs' </span><span class="s3">or </span><span class="s1">name </span><span class="s3">in </span><span class="s1">kwargs:</span>
                <span class="s3">continue</span>
            <span class="s1">original = getattr(self.ssm</span><span class="s3">, </span><span class="s1">name)</span>
            <span class="s1">extended = getattr(mod_extend.ssm</span><span class="s3">, </span><span class="s1">name)</span>
            <span class="s1">so = original.shape[-</span><span class="s4">1</span><span class="s1">]</span>
            <span class="s1">se = extended.shape[-</span><span class="s4">1</span><span class="s1">]</span>
            <span class="s3">if </span><span class="s1">((so &gt; </span><span class="s4">1 </span><span class="s3">or </span><span class="s1">se &gt; </span><span class="s4">1</span><span class="s1">) </span><span class="s3">or </span><span class="s1">(</span>
                    <span class="s1">so == </span><span class="s4">1 </span><span class="s3">and </span><span class="s1">self.nobs == </span><span class="s4">1 </span><span class="s3">and</span>
                    <span class="s1">np.any(original[...</span><span class="s3">, </span><span class="s4">0</span><span class="s1">] != extended[...</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]))):</span>
                <span class="s1">kwargs[name] = extended[...</span><span class="s3">, </span><span class="s1">-out_of_sample:]</span>

        <span class="s3">return </span><span class="s1">kwargs</span>

    <span class="s3">def </span><span class="s1">simulate(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">nsimulations</span><span class="s3">, </span><span class="s1">measurement_shocks=</span><span class="s3">None,</span>
                 <span class="s1">state_shocks=</span><span class="s3">None, </span><span class="s1">initial_state=</span><span class="s3">None, </span><span class="s1">anchor=</span><span class="s3">None,</span>
                 <span class="s1">repetitions=</span><span class="s3">None, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">extend_model=</span><span class="s3">None,</span>
                 <span class="s1">extend_kwargs=</span><span class="s3">None, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">False,</span>
                 <span class="s1">pretransformed_measurement_shocks=</span><span class="s3">True,</span>
                 <span class="s1">pretransformed_state_shocks=</span><span class="s3">True,</span>
                 <span class="s1">pretransformed_initial_state=</span><span class="s3">True, </span><span class="s1">random_state=</span><span class="s3">None,</span>
                 <span class="s1">**kwargs):</span>
        <span class="s2">r&quot;&quot;&quot; 
        Simulate a new time series following the state space model 
 
        Parameters 
        ---------- 
        params : array_like 
            Array of parameters to use in constructing the state space 
            representation to use when simulating. 
        nsimulations : int 
            The number of observations to simulate. If the model is 
            time-invariant this can be any number. If the model is 
            time-varying, then this number must be less than or equal to the 
            number of observations. 
        measurement_shocks : array_like, optional 
            If specified, these are the shocks to the measurement equation, 
            :math:`\varepsilon_t`. If unspecified, these are automatically 
            generated using a pseudo-random number generator. If specified, 
            must be shaped `nsimulations` x `k_endog`, where `k_endog` is the 
            same as in the state space model. 
        state_shocks : array_like, optional 
            If specified, these are the shocks to the state equation, 
            :math:`\eta_t`. If unspecified, these are automatically 
            generated using a pseudo-random number generator. If specified, 
            must be shaped `nsimulations` x `k_posdef` where `k_posdef` is the 
            same as in the state space model. 
        initial_state : array_like, optional 
            If specified, this is the initial state vector to use in 
            simulation, which should be shaped (`k_states` x 1), where 
            `k_states` is the same as in the state space model. If unspecified, 
            but the model has been initialized, then that initialization is 
            used. This must be specified if `anchor` is anything other than 
            &quot;start&quot; or 0 (or else you can use the `simulate` method on a 
            results object rather than on the model object). 
        anchor : int, str, or datetime, optional 
            First period for simulation. The simulation will be conditional on 
            all existing datapoints prior to the `anchor`.  Type depends on the 
            index of the given `endog` in the model. Two special cases are the 
            strings 'start' and 'end'. `start` refers to beginning the 
            simulation at the first period of the sample, and `end` refers to 
            beginning the simulation at the first period after the sample. 
            Integer values can run from 0 to `nobs`, or can be negative to 
            apply negative indexing. Finally, if a date/time index was provided 
            to the model, then this argument can be a date string to parse or a 
            datetime type. Default is 'start'. 
        repetitions : int, optional 
            Number of simulated paths to generate. Default is 1 simulated path. 
        exog : array_like, optional 
            New observations of exogenous regressors, if applicable. 
        transformed : bool, optional 
            Whether or not `params` is already transformed. Default is 
            True. 
        includes_fixed : bool, optional 
            If parameters were previously fixed with the `fix_params` method, 
            this argument describes whether or not `params` also includes 
            the fixed parameters, in addition to the free parameters. Default 
            is False. 
        pretransformed_measurement_shocks : bool, optional 
            If `measurement_shocks` is provided, this flag indicates whether it 
            should be directly used as the shocks. If False, then it is assumed 
            to contain draws from the standard Normal distribution that must be 
            transformed using the `obs_cov` covariance matrix. Default is True. 
        pretransformed_state_shocks : bool, optional 
            If `state_shocks` is provided, this flag indicates whether it 
            should be directly used as the shocks. If False, then it is assumed 
            to contain draws from the standard Normal distribution that must be 
            transformed using the `state_cov` covariance matrix. Default is 
            True. 
        pretransformed_initial_state : bool, optional 
            If `initial_state` is provided, this flag indicates whether it 
            should be directly used as the initial_state. If False, then it is 
            assumed to contain draws from the standard Normal distribution that 
            must be transformed using the `initial_state_cov` covariance 
            matrix. Default is True. 
        random_state : {None, int, Generator, RandomState}, optional 
            If `seed` is None (or `np.random`), the 
            class:``~numpy.random.RandomState`` singleton is used. 
            If `seed` is an int, a new class:``~numpy.random.RandomState`` 
            instance is used, seeded with `seed`. 
            If `seed` is already a class:``~numpy.random.Generator`` or 
            class:``~numpy.random.RandomState`` instance then that instance is 
            used. 
 
        Returns 
        ------- 
        simulated_obs : ndarray 
            An array of simulated observations. If `repetitions=None`, then it 
            will be shaped (nsimulations x k_endog) or (nsimulations,) if 
            `k_endog=1`. Otherwise it will be shaped 
            (nsimulations x k_endog x repetitions). If the model was given 
            Pandas input then the output will be a Pandas object. If 
            `k_endog &gt; 1` and `repetitions` is not None, then the output will 
            be a Pandas DataFrame that has a MultiIndex for the columns, with 
            the first level containing the names of the `endog` variables and 
            the second level containing the repetition number. 
 
        See Also 
        -------- 
        impulse_responses 
            Impulse response functions 
        &quot;&quot;&quot;</span>
        <span class="s0"># Make sure the model class has the current parameters</span>
        <span class="s1">self.update(params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                    <span class="s1">includes_fixed=includes_fixed)</span>

        <span class="s0"># Get the starting location</span>
        <span class="s3">if </span><span class="s1">anchor </span><span class="s3">is None or </span><span class="s1">anchor == </span><span class="s5">'start'</span><span class="s1">:</span>
            <span class="s1">iloc = </span><span class="s4">0</span>
        <span class="s3">elif </span><span class="s1">anchor == </span><span class="s5">'end'</span><span class="s1">:</span>
            <span class="s1">iloc = self.nobs</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">iloc</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ = self._get_index_loc(anchor)</span>
            <span class="s3">if </span><span class="s1">isinstance(iloc</span><span class="s3">, </span><span class="s1">slice):</span>
                <span class="s1">iloc = iloc.start</span>

        <span class="s3">if </span><span class="s1">iloc &lt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">iloc = self.nobs + iloc</span>
        <span class="s3">if </span><span class="s1">iloc &gt; self.nobs:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot anchor simulation outside of the sample.'</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">iloc &gt; </span><span class="s4">0 </span><span class="s3">and </span><span class="s1">initial_state </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'If `anchor` is after the start of the sample,'</span>
                             <span class="s5">' must provide a value for `initial_state`.'</span><span class="s1">)</span>

        <span class="s0"># Get updated time-varying system matrices in **kwargs, if necessary</span>
        <span class="s1">out_of_sample = max(iloc + nsimulations - self.nobs</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">extend_model </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">extend_model = self.exog </span><span class="s3">is not None or not </span><span class="s1">self.ssm.time_invariant</span>
        <span class="s3">if </span><span class="s1">out_of_sample </span><span class="s3">and </span><span class="s1">extend_model:</span>
            <span class="s1">kwargs = self._get_extension_time_varying_matrices(</span>
                <span class="s1">params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">out_of_sample</span><span class="s3">, </span><span class="s1">extend_kwargs</span><span class="s3">,</span>
                <span class="s1">transformed=transformed</span><span class="s3">, </span><span class="s1">includes_fixed=includes_fixed</span><span class="s3">,</span>
                <span class="s1">**kwargs)</span>

        <span class="s0"># Standardize the dimensions of the initial state</span>
        <span class="s3">if </span><span class="s1">initial_state </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">initial_state = np.array(initial_state)</span>
            <span class="s3">if </span><span class="s1">initial_state.ndim &lt; </span><span class="s4">2</span><span class="s1">:</span>
                <span class="s1">initial_state = np.atleast_2d(initial_state).T</span>

        <span class="s0"># Construct a model that represents the simulation period</span>
        <span class="s1">end = min(self.nobs</span><span class="s3">, </span><span class="s1">iloc + nsimulations)</span>
        <span class="s1">nextend = iloc + nsimulations - end</span>
        <span class="s1">sim_model = self.ssm.extend(np.zeros((nextend</span><span class="s3">, </span><span class="s1">self.k_endog))</span><span class="s3">,</span>
                                    <span class="s1">start=iloc</span><span class="s3">, </span><span class="s1">end=end</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s0"># Simulate the data</span>
        <span class="s1">_repetitions = </span><span class="s4">1 </span><span class="s3">if </span><span class="s1">repetitions </span><span class="s3">is None else </span><span class="s1">repetitions</span>
        <span class="s1">sim = np.zeros((nsimulations</span><span class="s3">, </span><span class="s1">self.k_endog</span><span class="s3">, </span><span class="s1">_repetitions))</span>
        <span class="s1">simulator = </span><span class="s3">None</span>

        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(_repetitions):</span>
            <span class="s1">initial_state_variates = </span><span class="s3">None</span>
            <span class="s3">if </span><span class="s1">initial_state </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s3">if </span><span class="s1">initial_state.shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
                    <span class="s1">initial_state_variates = initial_state[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">initial_state_variates = initial_state[:</span><span class="s3">, </span><span class="s1">i]</span>

            <span class="s0"># TODO: allow specifying measurement / state shocks for each</span>
            <span class="s0"># repetition?</span>

            <span class="s1">out</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">simulator = sim_model.simulate(</span>
                <span class="s1">nsimulations</span><span class="s3">, </span><span class="s1">measurement_shocks</span><span class="s3">, </span><span class="s1">state_shocks</span><span class="s3">,</span>
                <span class="s1">initial_state_variates</span><span class="s3">,</span>
                <span class="s1">pretransformed_measurement_shocks=(</span>
                    <span class="s1">pretransformed_measurement_shocks)</span><span class="s3">,</span>
                <span class="s1">pretransformed_state_shocks=pretransformed_state_shocks</span><span class="s3">,</span>
                <span class="s1">pretransformed_initial_state=pretransformed_initial_state</span><span class="s3">,</span>
                <span class="s1">simulator=simulator</span><span class="s3">, </span><span class="s1">return_simulator=</span><span class="s3">True,</span>
                <span class="s1">random_state=random_state)</span>

            <span class="s1">sim[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">i] = out</span>

        <span class="s0"># Wrap data / squeeze where appropriate</span>
        <span class="s1">use_pandas = isinstance(self.data</span><span class="s3">, </span><span class="s1">PandasData)</span>
        <span class="s1">index = </span><span class="s3">None</span>
        <span class="s3">if </span><span class="s1">use_pandas:</span>
            <span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">index = self._get_prediction_index(</span>
                <span class="s1">iloc</span><span class="s3">, </span><span class="s1">iloc + nsimulations - </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s0"># If `repetitions` isn't set, we squeeze the last dimension(s)</span>
        <span class="s3">if </span><span class="s1">repetitions </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self.k_endog == </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">sim = sim[:</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span>
                <span class="s3">if </span><span class="s1">use_pandas:</span>
                    <span class="s1">sim = pd.Series(sim</span><span class="s3">, </span><span class="s1">index=index</span><span class="s3">, </span><span class="s1">name=self.endog_names)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">sim = sim[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span>
                <span class="s3">if </span><span class="s1">use_pandas:</span>
                    <span class="s1">sim = pd.DataFrame(sim</span><span class="s3">, </span><span class="s1">index=index</span><span class="s3">,</span>
                                       <span class="s1">columns=self.endog_names)</span>
        <span class="s3">elif </span><span class="s1">use_pandas:</span>
            <span class="s1">shape = sim.shape</span>
            <span class="s1">endog_names = self.endog_names</span>
            <span class="s3">if not </span><span class="s1">isinstance(endog_names</span><span class="s3">, </span><span class="s1">list):</span>
                <span class="s1">endog_names = [endog_names]</span>
            <span class="s1">columns = pd.MultiIndex.from_product([endog_names</span><span class="s3">,</span>
                                                  <span class="s1">np.arange(shape[</span><span class="s4">2</span><span class="s1">])])</span>
            <span class="s1">sim = pd.DataFrame(sim.reshape(shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">shape[</span><span class="s4">1</span><span class="s1">] * shape[</span><span class="s4">2</span><span class="s1">])</span><span class="s3">,</span>
                               <span class="s1">index=index</span><span class="s3">, </span><span class="s1">columns=columns)</span>

        <span class="s3">return </span><span class="s1">sim</span>

    <span class="s3">def </span><span class="s1">impulse_responses(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">steps=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">impulse=</span><span class="s4">0</span><span class="s3">,</span>
                          <span class="s1">orthogonalized=</span><span class="s3">False, </span><span class="s1">cumulative=</span><span class="s3">False, </span><span class="s1">anchor=</span><span class="s3">None,</span>
                          <span class="s1">exog=</span><span class="s3">None, </span><span class="s1">extend_model=</span><span class="s3">None, </span><span class="s1">extend_kwargs=</span><span class="s3">None,</span>
                          <span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Impulse response function 
 
        Parameters 
        ---------- 
        params : array_like 
            Array of model parameters. 
        steps : int, optional 
            The number of steps for which impulse responses are calculated. 
            Default is 1. Note that for time-invariant models, the initial 
            impulse is not counted as a step, so if `steps=1`, the output will 
            have 2 entries. 
        impulse : int, str or array_like 
            If an integer, the state innovation to pulse; must be between 0 
            and `k_posdef-1`. If a str, it indicates which column of df 
            the unit (1) impulse is given. 
            Alternatively, a custom impulse vector may be provided; must be 
            shaped `k_posdef x 1`. 
        orthogonalized : bool, optional 
            Whether or not to perform impulse using orthogonalized innovations. 
            Note that this will also affect custum `impulse` vectors. Default 
            is False. 
        cumulative : bool, optional 
            Whether or not to return cumulative impulse responses. Default is 
            False. 
        anchor : int, str, or datetime, optional 
            Time point within the sample for the state innovation impulse. Type 
            depends on the index of the given `endog` in the model. Two special 
            cases are the strings 'start' and 'end', which refer to setting the 
            impulse at the first and last points of the sample, respectively. 
            Integer values can run from 0 to `nobs - 1`, or can be negative to 
            apply negative indexing. Finally, if a date/time index was provided 
            to the model, then this argument can be a date string to parse or a 
            datetime type. Default is 'start'. 
        exog : array_like, optional 
            New observations of exogenous regressors for our-of-sample periods, 
            if applicable. 
        transformed : bool, optional 
            Whether or not `params` is already transformed. Default is 
            True. 
        includes_fixed : bool, optional 
            If parameters were previously fixed with the `fix_params` method, 
            this argument describes whether or not `params` also includes 
            the fixed parameters, in addition to the free parameters. Default 
            is False. 
        **kwargs 
            If the model has time-varying design or transition matrices and the 
            combination of `anchor` and `steps` implies creating impulse 
            responses for the out-of-sample period, then these matrices must 
            have updated values provided for the out-of-sample steps. For 
            example, if `design` is a time-varying component, `nobs` is 10, 
            `anchor=1`, and `steps` is 15, a (`k_endog` x `k_states` x 7) 
            matrix must be provided with the new design matrix values. 
 
        Returns 
        ------- 
        impulse_responses : ndarray 
            Responses for each endogenous variable due to the impulse 
            given by the `impulse` argument. For a time-invariant model, the 
            impulse responses are given for `steps + 1` elements (this gives 
            the &quot;initial impulse&quot; followed by `steps` responses for the 
            important cases of VAR and SARIMAX models), while for time-varying 
            models the impulse responses are only given for `steps` elements 
            (to avoid having to unexpectedly provide updated time-varying 
            matrices). 
 
        See Also 
        -------- 
        simulate 
            Simulate a time series according to the given state space model, 
            optionally with specified series for the innovations. 
 
        Notes 
        ----- 
        Intercepts in the measurement and state equation are ignored when 
        calculating impulse responses. 
 
        TODO: add an option to allow changing the ordering for the 
              orthogonalized option. Will require permuting matrices when 
              constructing the extended model. 
        &quot;&quot;&quot;</span>
        <span class="s0"># Make sure the model class has the current parameters</span>
        <span class="s1">self.update(params</span><span class="s3">, </span><span class="s1">transformed=transformed</span><span class="s3">,</span>
                    <span class="s1">includes_fixed=includes_fixed)</span>

        <span class="s0"># For time-invariant models, add an additional `step`. This is the</span>
        <span class="s0"># default for time-invariant models based on the expected behavior for</span>
        <span class="s0"># ARIMA and VAR models: we want to record the initial impulse and also</span>
        <span class="s0"># `steps` values of the responses afterwards.</span>
        <span class="s0"># Note: we don't modify `steps` itself, because</span>
        <span class="s0"># `KalmanFilter.impulse_responses` also adds an additional step in this</span>
        <span class="s0"># case (this is so that there isn't different behavior when calling</span>
        <span class="s0"># this method versus that method). We just need to also keep track of</span>
        <span class="s0"># this here because we need to generate the correct extended model.</span>
        <span class="s1">additional_steps = </span><span class="s4">0</span>
        <span class="s3">if </span><span class="s1">(self.ssm._design.shape[</span><span class="s4">2</span><span class="s1">] == </span><span class="s4">1 </span><span class="s3">and</span>
                <span class="s1">self.ssm._transition.shape[</span><span class="s4">2</span><span class="s1">] == </span><span class="s4">1 </span><span class="s3">and</span>
                <span class="s1">self.ssm._selection.shape[</span><span class="s4">2</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">):</span>
            <span class="s1">additional_steps = </span><span class="s4">1</span>

        <span class="s0"># Get the starting location</span>
        <span class="s3">if </span><span class="s1">anchor </span><span class="s3">is None or </span><span class="s1">anchor == </span><span class="s5">'start'</span><span class="s1">:</span>
            <span class="s1">iloc = </span><span class="s4">0</span>
        <span class="s3">elif </span><span class="s1">anchor == </span><span class="s5">'end'</span><span class="s1">:</span>
            <span class="s1">iloc = self.nobs - </span><span class="s4">1</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">iloc</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ = self._get_index_loc(anchor)</span>
            <span class="s3">if </span><span class="s1">isinstance(iloc</span><span class="s3">, </span><span class="s1">slice):</span>
                <span class="s1">iloc = iloc.start</span>

        <span class="s3">if </span><span class="s1">iloc &lt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">iloc = self.nobs + iloc</span>
        <span class="s3">if </span><span class="s1">iloc &gt;= self.nobs:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot anchor impulse responses outside of the'</span>
                             <span class="s5">' sample.'</span><span class="s1">)</span>

        <span class="s1">time_invariant = (</span>
            <span class="s1">self.ssm._design.shape[</span><span class="s4">2</span><span class="s1">] == self.ssm._obs_cov.shape[</span><span class="s4">2</span><span class="s1">] ==</span>
            <span class="s1">self.ssm._transition.shape[</span><span class="s4">2</span><span class="s1">] == self.ssm._selection.shape[</span><span class="s4">2</span><span class="s1">] ==</span>
            <span class="s1">self.ssm._state_cov.shape[</span><span class="s4">2</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s0"># Get updated time-varying system matrices in **kwargs, if necessary</span>
        <span class="s0"># (Note: KalmanFilter adds 1 to steps to account for the first impulse)</span>
        <span class="s1">out_of_sample = max(</span>
            <span class="s1">iloc + (steps + additional_steps + </span><span class="s4">1</span><span class="s1">) - self.nobs</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">extend_model </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">extend_model = self.exog </span><span class="s3">is not None and not </span><span class="s1">time_invariant</span>
        <span class="s3">if </span><span class="s1">out_of_sample </span><span class="s3">and </span><span class="s1">extend_model:</span>
            <span class="s1">kwargs = self._get_extension_time_varying_matrices(</span>
                <span class="s1">params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">out_of_sample</span><span class="s3">, </span><span class="s1">extend_kwargs</span><span class="s3">,</span>
                <span class="s1">transformed=transformed</span><span class="s3">, </span><span class="s1">includes_fixed=includes_fixed</span><span class="s3">,</span>
                <span class="s1">**kwargs)</span>

        <span class="s0"># Special handling for matrix terms that are time-varying but</span>
        <span class="s0"># irrelevant for impulse response functions. Must be set since</span>
        <span class="s0"># ssm.extend() requires that we pass new matrices for these, but they</span>
        <span class="s0"># are ignored for IRF purposes.</span>
        <span class="s1">end = min(self.nobs</span><span class="s3">, </span><span class="s1">iloc + steps + additional_steps)</span>
        <span class="s1">nextend = iloc + (steps + additional_steps + </span><span class="s4">1</span><span class="s1">) - end</span>
        <span class="s3">if </span><span class="s1">(</span><span class="s5">'obs_intercept' </span><span class="s3">not in </span><span class="s1">kwargs </span><span class="s3">and</span>
                <span class="s1">self.ssm._obs_intercept.shape[</span><span class="s4">1</span><span class="s1">] &gt; </span><span class="s4">1</span><span class="s1">):</span>
            <span class="s1">kwargs[</span><span class="s5">'obs_intercept'</span><span class="s1">] = np.zeros((self.k_endog</span><span class="s3">, </span><span class="s1">nextend))</span>
        <span class="s3">if </span><span class="s1">(</span><span class="s5">'state_intercept' </span><span class="s3">not in </span><span class="s1">kwargs </span><span class="s3">and</span>
                <span class="s1">self.ssm._state_intercept.shape[</span><span class="s4">1</span><span class="s1">] &gt; </span><span class="s4">1</span><span class="s1">):</span>
            <span class="s1">kwargs[</span><span class="s5">'state_intercept'</span><span class="s1">] = np.zeros((self.k_states</span><span class="s3">, </span><span class="s1">nextend))</span>
        <span class="s3">if </span><span class="s5">'obs_cov' </span><span class="s3">not in </span><span class="s1">kwargs </span><span class="s3">and </span><span class="s1">self.ssm._obs_cov.shape[</span><span class="s4">2</span><span class="s1">] &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">kwargs[</span><span class="s5">'obs_cov'</span><span class="s1">] = np.zeros((self.k_endog</span><span class="s3">, </span><span class="s1">self.k_endog</span><span class="s3">, </span><span class="s1">nextend))</span>
        <span class="s0"># Special handling for matrix terms that are time-varying but</span>
        <span class="s0"># only the value at the anchor matters for IRF purposes.</span>
        <span class="s3">if </span><span class="s5">'state_cov' </span><span class="s3">not in </span><span class="s1">kwargs </span><span class="s3">and </span><span class="s1">self.ssm._state_cov.shape[</span><span class="s4">2</span><span class="s1">] &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">tmp = np.zeros((self.ssm.k_posdef</span><span class="s3">, </span><span class="s1">self.ssm.k_posdef</span><span class="s3">, </span><span class="s1">nextend))</span>
            <span class="s1">tmp[:] = self[</span><span class="s5">'state_cov'</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">iloc:iloc + </span><span class="s4">1</span><span class="s1">]</span>
            <span class="s1">kwargs[</span><span class="s5">'state_cov'</span><span class="s1">] = tmp</span>
        <span class="s3">if </span><span class="s5">'selection' </span><span class="s3">not in </span><span class="s1">kwargs </span><span class="s3">and </span><span class="s1">self.ssm._selection.shape[</span><span class="s4">2</span><span class="s1">] &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">tmp = np.zeros((self.k_states</span><span class="s3">, </span><span class="s1">self.ssm.k_posdef</span><span class="s3">, </span><span class="s1">nextend))</span>
            <span class="s1">tmp[:] = self[</span><span class="s5">'selection'</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">iloc:iloc + </span><span class="s4">1</span><span class="s1">]</span>
            <span class="s1">kwargs[</span><span class="s5">'selection'</span><span class="s1">] = tmp</span>

        <span class="s0"># Construct a model that represents the simulation period</span>
        <span class="s1">sim_model = self.ssm.extend(np.empty((nextend</span><span class="s3">, </span><span class="s1">self.k_endog))</span><span class="s3">,</span>
                                    <span class="s1">start=iloc</span><span class="s3">, </span><span class="s1">end=end</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s0"># Compute the impulse responses</span>

        <span class="s0"># Convert endog name to index</span>
        <span class="s1">use_pandas = isinstance(self.data</span><span class="s3">, </span><span class="s1">PandasData)</span>
        <span class="s3">if </span><span class="s1">type(impulse) == str:</span>
            <span class="s3">if not </span><span class="s1">use_pandas:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Endog must be pd.DataFrame.'</span><span class="s1">)</span>
            <span class="s1">impulse = self.endog_names.index(impulse)</span>

        <span class="s1">irfs = sim_model.impulse_responses(</span>
            <span class="s1">steps</span><span class="s3">, </span><span class="s1">impulse</span><span class="s3">, </span><span class="s1">orthogonalized</span><span class="s3">, </span><span class="s1">cumulative)</span>

        <span class="s0"># IRF is (nobs x k_endog); do not want to squeeze in case of steps = 1</span>
        <span class="s3">if </span><span class="s1">irfs.shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">irfs = irfs[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span>

        <span class="s0"># Wrap data / squeeze where appropriate</span>
        <span class="s3">if </span><span class="s1">use_pandas:</span>
            <span class="s3">if </span><span class="s1">self.k_endog == </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">irfs = pd.Series(irfs</span><span class="s3">, </span><span class="s1">name=self.endog_names)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">irfs = pd.DataFrame(irfs</span><span class="s3">, </span><span class="s1">columns=self.endog_names)</span>
        <span class="s3">return </span><span class="s1">irfs</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">from_formula(cls</span><span class="s3">, </span><span class="s1">formula</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">subset=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Not implemented for state space models 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>


<span class="s3">class </span><span class="s1">MLEResults(tsbase.TimeSeriesModelResults):</span>
    <span class="s2">r&quot;&quot;&quot; 
    Class to hold results from fitting a state space model. 
 
    Parameters 
    ---------- 
    model : MLEModel instance 
        The fitted model instance 
    params : ndarray 
        Fitted parameters 
    filter_results : KalmanFilter instance 
        The underlying state space model and Kalman filter output 
 
    Attributes 
    ---------- 
    model : Model instance 
        A reference to the model that was fit. 
    filter_results : KalmanFilter instance 
        The underlying state space model and Kalman filter output 
    nobs : float 
        The number of observations used to fit the model. 
    params : ndarray 
        The parameters of the model. 
    scale : float 
        This is currently set to 1.0 unless the model uses concentrated 
        filtering. 
 
    See Also 
    -------- 
    MLEModel 
    statsmodels.tsa.statespace.kalman_filter.FilterResults 
    statsmodels.tsa.statespace.representation.FrozenRepresentation 
    &quot;&quot;&quot;</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">model</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">results</span><span class="s3">, </span><span class="s1">cov_type=</span><span class="s3">None, </span><span class="s1">cov_kwds=</span><span class="s3">None,</span>
                 <span class="s1">**kwargs):</span>
        <span class="s1">self.data = model.data</span>
        <span class="s1">scale = results.scale</span>

        <span class="s1">tsbase.TimeSeriesModelResults.__init__(self</span><span class="s3">, </span><span class="s1">model</span><span class="s3">, </span><span class="s1">params</span><span class="s3">,</span>
                                               <span class="s1">normalized_cov_params=</span><span class="s3">None,</span>
                                               <span class="s1">scale=scale)</span>

        <span class="s0"># Save the fixed parameters</span>
        <span class="s1">self._has_fixed_params = self.model._has_fixed_params</span>
        <span class="s1">self._fixed_params_index = self.model._fixed_params_index</span>
        <span class="s1">self._free_params_index = self.model._free_params_index</span>
        <span class="s0"># TODO: seems like maybe self.fixed_params should be the dictionary</span>
        <span class="s0"># itself, not just the keys?</span>
        <span class="s3">if </span><span class="s1">self._has_fixed_params:</span>
            <span class="s1">self._fixed_params = self.model._fixed_params.copy()</span>
            <span class="s1">self.fixed_params = list(self._fixed_params.keys())</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._fixed_params = </span><span class="s3">None</span>
            <span class="s1">self.fixed_params = []</span>
        <span class="s1">self.param_names = [</span>
            <span class="s5">'%s (fixed)' </span><span class="s1">% name </span><span class="s3">if </span><span class="s1">name </span><span class="s3">in </span><span class="s1">self.fixed_params </span><span class="s3">else </span><span class="s1">name</span>
            <span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">(self.data.param_names </span><span class="s3">or </span><span class="s1">[])]</span>

        <span class="s0"># Save the state space representation output</span>
        <span class="s1">self.filter_results = results</span>
        <span class="s3">if </span><span class="s1">isinstance(results</span><span class="s3">, </span><span class="s1">SmootherResults):</span>
            <span class="s1">self.smoother_results = results</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.smoother_results = </span><span class="s3">None</span>

        <span class="s0"># Dimensions</span>
        <span class="s1">self.nobs = self.filter_results.nobs</span>
        <span class="s1">self.nobs_diffuse = self.filter_results.nobs_diffuse</span>
        <span class="s3">if </span><span class="s1">self.nobs_diffuse &gt; </span><span class="s4">0 </span><span class="s3">and </span><span class="s1">self.loglikelihood_burn &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span><span class="s5">'Care should be used when applying a loglikelihood'</span>
                          <span class="s5">' burn to a model with exact diffuse initialization.'</span>
                          <span class="s5">' Some results objects, e.g. degrees of freedom,'</span>
                          <span class="s5">' expect only one of the two to be set.'</span><span class="s1">)</span>
        <span class="s0"># This only excludes explicitly burned (usually approximate diffuse)</span>
        <span class="s0"># periods but does not exclude exact diffuse periods. This is</span>
        <span class="s0"># because the loglikelihood remains valid for the initial periods in</span>
        <span class="s0"># the exact diffuse case (see DK, 2012, section 7.2) and so also do</span>
        <span class="s0"># e.g. information criteria (see DK, 2012, section 7.4) and the score</span>
        <span class="s0"># vector (see DK, 2012, section 7.3.3, equation 7.15).</span>
        <span class="s0"># However, other objects should be excluded in the diffuse periods</span>
        <span class="s0"># (e.g. the diffuse forecast errors, so in some cases a different</span>
        <span class="s0"># nobs_effective will have to be computed and used)</span>
        <span class="s1">self.nobs_effective = self.nobs - self.loglikelihood_burn</span>

        <span class="s1">P = self.filter_results.initial_diffuse_state_cov</span>
        <span class="s1">self.k_diffuse_states = </span><span class="s4">0 </span><span class="s3">if </span><span class="s1">P </span><span class="s3">is None else </span><span class="s1">np.sum(np.diagonal(P) == </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s0"># Degrees of freedom (see DK 2012, section 7.4)</span>
        <span class="s1">k_free_params = self.params.size - len(self.fixed_params)</span>
        <span class="s1">self.df_model = (k_free_params + self.k_diffuse_states</span>
                         <span class="s1">+ self.filter_results.filter_concentrated)</span>
        <span class="s1">self.df_resid = self.nobs_effective - self.df_model</span>

        <span class="s0"># Setup covariance matrix notes dictionary</span>
        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s5">'cov_kwds'</span><span class="s1">):</span>
            <span class="s1">self.cov_kwds = {}</span>
        <span class="s3">if </span><span class="s1">cov_type </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">cov_type = </span><span class="s5">'approx' </span><span class="s3">if </span><span class="s1">results.memory_no_likelihood </span><span class="s3">else </span><span class="s5">'opg'</span>
        <span class="s1">self.cov_type = cov_type</span>

        <span class="s0"># Setup the cache</span>
        <span class="s1">self._cache = {}</span>

        <span class="s0"># Handle covariance matrix calculation</span>
        <span class="s3">if </span><span class="s1">cov_kwds </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">cov_kwds = {}</span>
        <span class="s1">self._cov_approx_complex_step = (</span>
            <span class="s1">cov_kwds.pop(</span><span class="s5">'approx_complex_step'</span><span class="s3">, True</span><span class="s1">))</span>
        <span class="s1">self._cov_approx_centered = cov_kwds.pop(</span><span class="s5">'approx_centered'</span><span class="s3">, False</span><span class="s1">)</span>
        <span class="s3">try</span><span class="s1">:</span>
            <span class="s1">self._rank = </span><span class="s3">None</span>
            <span class="s1">self._get_robustcov_results(cov_type=cov_type</span><span class="s3">, </span><span class="s1">use_self=</span><span class="s3">True,</span>
                                        <span class="s1">**cov_kwds)</span>
        <span class="s3">except </span><span class="s1">np.linalg.LinAlgError:</span>
            <span class="s1">self._rank = </span><span class="s4">0</span>
            <span class="s1">k_params = len(self.params)</span>
            <span class="s1">self.cov_params_default = np.zeros((k_params</span><span class="s3">, </span><span class="s1">k_params)) * np.nan</span>
            <span class="s1">self.cov_kwds[</span><span class="s5">'cov_type'</span><span class="s1">] = (</span>
                <span class="s5">'Covariance matrix could not be calculated: singular.'</span>
                <span class="s5">' information matrix.'</span><span class="s1">)</span>
        <span class="s1">self.model.update(self.params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True</span><span class="s1">)</span>

        <span class="s0"># References of filter and smoother output</span>
        <span class="s1">extra_arrays = [</span>
            <span class="s5">'filtered_state'</span><span class="s3">, </span><span class="s5">'filtered_state_cov'</span><span class="s3">, </span><span class="s5">'predicted_state'</span><span class="s3">,</span>
            <span class="s5">'predicted_state_cov'</span><span class="s3">, </span><span class="s5">'forecasts'</span><span class="s3">, </span><span class="s5">'forecasts_error'</span><span class="s3">,</span>
            <span class="s5">'forecasts_error_cov'</span><span class="s3">, </span><span class="s5">'standardized_forecasts_error'</span><span class="s3">,</span>
            <span class="s5">'forecasts_error_diffuse_cov'</span><span class="s3">, </span><span class="s5">'predicted_diffuse_state_cov'</span><span class="s3">,</span>
            <span class="s5">'scaled_smoothed_estimator'</span><span class="s3">,</span>
            <span class="s5">'scaled_smoothed_estimator_cov'</span><span class="s3">, </span><span class="s5">'smoothing_error'</span><span class="s3">,</span>
            <span class="s5">'smoothed_state'</span><span class="s3">,</span>
            <span class="s5">'smoothed_state_cov'</span><span class="s3">, </span><span class="s5">'smoothed_state_autocov'</span><span class="s3">,</span>
            <span class="s5">'smoothed_measurement_disturbance'</span><span class="s3">,</span>
            <span class="s5">'smoothed_state_disturbance'</span><span class="s3">,</span>
            <span class="s5">'smoothed_measurement_disturbance_cov'</span><span class="s3">,</span>
            <span class="s5">'smoothed_state_disturbance_cov'</span><span class="s1">]</span>
        <span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">extra_arrays:</span>
            <span class="s1">setattr(self</span><span class="s3">, </span><span class="s1">name</span><span class="s3">, </span><span class="s1">getattr(self.filter_results</span><span class="s3">, </span><span class="s1">name</span><span class="s3">, None</span><span class="s1">))</span>

        <span class="s0"># Remove too-short results when memory conservation was used</span>
        <span class="s3">if </span><span class="s1">self.filter_results.memory_no_forecast_mean:</span>
            <span class="s1">self.forecasts = </span><span class="s3">None</span>
            <span class="s1">self.forecasts_error = </span><span class="s3">None</span>
        <span class="s3">if </span><span class="s1">self.filter_results.memory_no_forecast_cov:</span>
            <span class="s1">self.forecasts_error_cov = </span><span class="s3">None</span>
        <span class="s3">if </span><span class="s1">self.filter_results.memory_no_predicted_mean:</span>
            <span class="s1">self.predicted_state = </span><span class="s3">None</span>
        <span class="s3">if </span><span class="s1">self.filter_results.memory_no_predicted_cov:</span>
            <span class="s1">self.predicted_state_cov = </span><span class="s3">None</span>
        <span class="s3">if </span><span class="s1">self.filter_results.memory_no_filtered_mean:</span>
            <span class="s1">self.filtered_state = </span><span class="s3">None</span>
        <span class="s3">if </span><span class="s1">self.filter_results.memory_no_filtered_cov:</span>
            <span class="s1">self.filtered_state_cov = </span><span class="s3">None</span>
        <span class="s3">if </span><span class="s1">self.filter_results.memory_no_gain:</span>
            <span class="s3">pass</span>
        <span class="s3">if </span><span class="s1">self.filter_results.memory_no_smoothing:</span>
            <span class="s3">pass</span>
        <span class="s3">if </span><span class="s1">self.filter_results.memory_no_std_forecast:</span>
            <span class="s1">self.standardized_forecasts_error = </span><span class="s3">None</span>

        <span class="s0"># Save more convenient access to states</span>
        <span class="s0"># (will create a private attribute _states here and provide actual</span>
        <span class="s0"># access via a getter, so that we can e.g. issue a warning in the case</span>
        <span class="s0"># that a useless Pandas index was given in the model specification)</span>
        <span class="s1">self._states = SimpleNamespace()</span>

        <span class="s1">use_pandas = isinstance(self.data</span><span class="s3">, </span><span class="s1">PandasData)</span>
        <span class="s1">index = self.model._index</span>
        <span class="s1">columns = self.model.state_names</span>

        <span class="s0"># Predicted states</span>
        <span class="s0"># Note: a complication here is that we also include the initial values</span>
        <span class="s0"># here, so that we need an extended index in the Pandas case</span>
        <span class="s3">if </span><span class="s1">(self.predicted_state </span><span class="s3">is None or</span>
                <span class="s1">self.filter_results.memory_no_predicted_mean):</span>
            <span class="s1">self._states.predicted = </span><span class="s3">None</span>
        <span class="s3">elif </span><span class="s1">use_pandas:</span>
            <span class="s1">extended_index = self.model._get_index_with_final_state()</span>
            <span class="s1">self._states.predicted = pd.DataFrame(</span>
                <span class="s1">self.predicted_state.T</span><span class="s3">, </span><span class="s1">index=extended_index</span><span class="s3">, </span><span class="s1">columns=columns)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._states.predicted = self.predicted_state.T</span>
        <span class="s3">if </span><span class="s1">(self.predicted_state_cov </span><span class="s3">is None or</span>
                <span class="s1">self.filter_results.memory_no_predicted_cov):</span>
            <span class="s1">self._states.predicted_cov = </span><span class="s3">None</span>
        <span class="s3">elif </span><span class="s1">use_pandas:</span>
            <span class="s1">extended_index = self.model._get_index_with_final_state()</span>
            <span class="s1">tmp = np.transpose(self.predicted_state_cov</span><span class="s3">, </span><span class="s1">(</span><span class="s4">2</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span>
            <span class="s1">self._states.predicted_cov = pd.DataFrame(</span>
                <span class="s1">np.reshape(tmp</span><span class="s3">, </span><span class="s1">(tmp.shape[</span><span class="s4">0</span><span class="s1">] * tmp.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">tmp.shape[</span><span class="s4">2</span><span class="s1">]))</span><span class="s3">,</span>
                <span class="s1">index=pd.MultiIndex.from_product(</span>
                    <span class="s1">[extended_index</span><span class="s3">, </span><span class="s1">columns]).swaplevel()</span><span class="s3">,</span>
                <span class="s1">columns=columns)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._states.predicted_cov = np.transpose(</span>
                <span class="s1">self.predicted_state_cov</span><span class="s3">, </span><span class="s1">(</span><span class="s4">2</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span>

        <span class="s0"># Filtered states</span>
        <span class="s3">if </span><span class="s1">(self.filtered_state </span><span class="s3">is None or</span>
                <span class="s1">self.filter_results.memory_no_filtered_mean):</span>
            <span class="s1">self._states.filtered = </span><span class="s3">None</span>
        <span class="s3">elif </span><span class="s1">use_pandas:</span>
            <span class="s1">self._states.filtered = pd.DataFrame(</span>
                <span class="s1">self.filtered_state.T</span><span class="s3">, </span><span class="s1">index=index</span><span class="s3">, </span><span class="s1">columns=columns)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._states.filtered = self.filtered_state.T</span>
        <span class="s3">if </span><span class="s1">(self.filtered_state_cov </span><span class="s3">is None or</span>
                <span class="s1">self.filter_results.memory_no_filtered_cov):</span>
            <span class="s1">self._states.filtered_cov = </span><span class="s3">None</span>
        <span class="s3">elif </span><span class="s1">use_pandas:</span>
            <span class="s1">tmp = np.transpose(self.filtered_state_cov</span><span class="s3">, </span><span class="s1">(</span><span class="s4">2</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span>
            <span class="s1">self._states.filtered_cov = pd.DataFrame(</span>
                <span class="s1">np.reshape(tmp</span><span class="s3">, </span><span class="s1">(tmp.shape[</span><span class="s4">0</span><span class="s1">] * tmp.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">tmp.shape[</span><span class="s4">2</span><span class="s1">]))</span><span class="s3">,</span>
                <span class="s1">index=pd.MultiIndex.from_product([index</span><span class="s3">, </span><span class="s1">columns]).swaplevel()</span><span class="s3">,</span>
                <span class="s1">columns=columns)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._states.filtered_cov = np.transpose(</span>
                <span class="s1">self.filtered_state_cov</span><span class="s3">, </span><span class="s1">(</span><span class="s4">2</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span>

        <span class="s0"># Smoothed states</span>
        <span class="s3">if </span><span class="s1">self.smoothed_state </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self._states.smoothed = </span><span class="s3">None</span>
        <span class="s3">elif </span><span class="s1">use_pandas:</span>
            <span class="s1">self._states.smoothed = pd.DataFrame(</span>
                <span class="s1">self.smoothed_state.T</span><span class="s3">, </span><span class="s1">index=index</span><span class="s3">, </span><span class="s1">columns=columns)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._states.smoothed = self.smoothed_state.T</span>
        <span class="s3">if </span><span class="s1">self.smoothed_state_cov </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self._states.smoothed_cov = </span><span class="s3">None</span>
        <span class="s3">elif </span><span class="s1">use_pandas:</span>
            <span class="s1">tmp = np.transpose(self.smoothed_state_cov</span><span class="s3">, </span><span class="s1">(</span><span class="s4">2</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span>
            <span class="s1">self._states.smoothed_cov = pd.DataFrame(</span>
                <span class="s1">np.reshape(tmp</span><span class="s3">, </span><span class="s1">(tmp.shape[</span><span class="s4">0</span><span class="s1">] * tmp.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">tmp.shape[</span><span class="s4">2</span><span class="s1">]))</span><span class="s3">,</span>
                <span class="s1">index=pd.MultiIndex.from_product([index</span><span class="s3">, </span><span class="s1">columns]).swaplevel()</span><span class="s3">,</span>
                <span class="s1">columns=columns)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._states.smoothed_cov = np.transpose(</span>
                <span class="s1">self.smoothed_state_cov</span><span class="s3">, </span><span class="s1">(</span><span class="s4">2</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span>

        <span class="s0"># Handle removing data</span>
        <span class="s1">self._data_attr_model = getattr(self</span><span class="s3">, </span><span class="s5">'_data_attr_model'</span><span class="s3">, </span><span class="s1">[])</span>
        <span class="s1">self._data_attr_model.extend([</span><span class="s5">'ssm'</span><span class="s1">])</span>
        <span class="s1">self._data_attr.extend(extra_arrays)</span>
        <span class="s1">self._data_attr.extend([</span><span class="s5">'filter_results'</span><span class="s3">, </span><span class="s5">'smoother_results'</span><span class="s1">])</span>

    <span class="s3">def </span><span class="s1">_get_robustcov_results(self</span><span class="s3">, </span><span class="s1">cov_type=</span><span class="s5">'opg'</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Create new results instance with specified covariance estimator as 
        default 
 
        Note: creating new results instance currently not supported. 
 
        Parameters 
        ---------- 
        cov_type : str 
            the type of covariance matrix estimator to use. See Notes below 
        kwargs : depends on cov_type 
            Required or optional arguments for covariance calculation. 
            See Notes below. 
 
        Returns 
        ------- 
        results : results instance 
            This method creates a new results instance with the requested 
            covariance as the default covariance of the parameters. 
            Inferential statistics like p-values and hypothesis tests will be 
            based on this covariance matrix. 
 
        Notes 
        ----- 
        The following covariance types and required or optional arguments are 
        currently available: 
 
        - 'opg' for the outer product of gradient estimator 
        - 'oim' for the observed information matrix estimator, calculated 
          using the method of Harvey (1989) 
        - 'approx' for the observed information matrix estimator, 
          calculated using a numerical approximation of the Hessian matrix. 
          Uses complex step approximation by default, or uses finite 
          differences if `approx_complex_step=False` in the `cov_kwds` 
          dictionary. 
        - 'robust' for an approximate (quasi-maximum likelihood) covariance 
          matrix that may be valid even in the presence of some 
          misspecifications. Intermediate calculations use the 'oim' 
          method. 
        - 'robust_approx' is the same as 'robust' except that the 
          intermediate calculations use the 'approx' method. 
        - 'none' for no covariance matrix calculation. 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.base.covtype </span><span class="s3">import </span><span class="s1">descriptions</span>

        <span class="s1">use_self = kwargs.pop(</span><span class="s5">'use_self'</span><span class="s3">, False</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">use_self:</span>
            <span class="s1">res = self</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError</span>
            <span class="s1">res = self.__class__(</span>
                <span class="s1">self.model</span><span class="s3">, </span><span class="s1">self.params</span><span class="s3">,</span>
                <span class="s1">normalized_cov_params=self.normalized_cov_params</span><span class="s3">,</span>
                <span class="s1">scale=self.scale)</span>

        <span class="s0"># Set the new covariance type</span>
        <span class="s1">res.cov_type = cov_type</span>
        <span class="s1">res.cov_kwds = {}</span>

        <span class="s0"># Calculate the new covariance matrix</span>
        <span class="s1">approx_complex_step = self._cov_approx_complex_step</span>
        <span class="s3">if </span><span class="s1">approx_complex_step:</span>
            <span class="s1">approx_type_str = </span><span class="s5">'complex-step'</span>
        <span class="s3">elif </span><span class="s1">self._cov_approx_centered:</span>
            <span class="s1">approx_type_str = </span><span class="s5">'centered finite differences'</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">approx_type_str = </span><span class="s5">'finite differences'</span>

        <span class="s1">k_params = len(self.params)</span>
        <span class="s3">if </span><span class="s1">k_params == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">res.cov_params_default = np.zeros((</span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s1">))</span>
            <span class="s1">res._rank = </span><span class="s4">0</span>
            <span class="s1">res.cov_kwds[</span><span class="s5">'description'</span><span class="s1">] = </span><span class="s5">'No parameters estimated.'</span>
        <span class="s3">elif </span><span class="s1">cov_type == </span><span class="s5">'custom'</span><span class="s1">:</span>
            <span class="s1">res.cov_type = kwargs[</span><span class="s5">'custom_cov_type'</span><span class="s1">]</span>
            <span class="s1">res.cov_params_default = kwargs[</span><span class="s5">'custom_cov_params'</span><span class="s1">]</span>
            <span class="s1">res.cov_kwds[</span><span class="s5">'description'</span><span class="s1">] = kwargs[</span><span class="s5">'custom_description'</span><span class="s1">]</span>
            <span class="s3">if </span><span class="s1">len(self.fixed_params) &gt; </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s1">mask = np.ix_(self._free_params_index</span><span class="s3">, </span><span class="s1">self._free_params_index)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">mask = np.s_[...]</span>
            <span class="s1">res._rank = np.linalg.matrix_rank(res.cov_params_default[mask])</span>
        <span class="s3">elif </span><span class="s1">cov_type == </span><span class="s5">'none'</span><span class="s1">:</span>
            <span class="s1">res.cov_params_default = np.zeros((k_params</span><span class="s3">, </span><span class="s1">k_params)) * np.nan</span>
            <span class="s1">res._rank = np.nan</span>
            <span class="s1">res.cov_kwds[</span><span class="s5">'description'</span><span class="s1">] = descriptions[</span><span class="s5">'none'</span><span class="s1">]</span>
        <span class="s3">elif </span><span class="s1">self.cov_type == </span><span class="s5">'approx'</span><span class="s1">:</span>
            <span class="s1">res.cov_params_default = res.cov_params_approx</span>
            <span class="s1">res.cov_kwds[</span><span class="s5">'description'</span><span class="s1">] = descriptions[</span><span class="s5">'approx'</span><span class="s1">].format(</span>
                                                <span class="s1">approx_type=approx_type_str)</span>
        <span class="s3">elif </span><span class="s1">self.cov_type == </span><span class="s5">'oim'</span><span class="s1">:</span>
            <span class="s1">res.cov_params_default = res.cov_params_oim</span>
            <span class="s1">res.cov_kwds[</span><span class="s5">'description'</span><span class="s1">] = descriptions[</span><span class="s5">'OIM'</span><span class="s1">].format(</span>
                                                <span class="s1">approx_type=approx_type_str)</span>
        <span class="s3">elif </span><span class="s1">self.cov_type == </span><span class="s5">'opg'</span><span class="s1">:</span>
            <span class="s1">res.cov_params_default = res.cov_params_opg</span>
            <span class="s1">res.cov_kwds[</span><span class="s5">'description'</span><span class="s1">] = descriptions[</span><span class="s5">'OPG'</span><span class="s1">].format(</span>
                                                <span class="s1">approx_type=approx_type_str)</span>
        <span class="s3">elif </span><span class="s1">self.cov_type == </span><span class="s5">'robust' </span><span class="s3">or </span><span class="s1">self.cov_type == </span><span class="s5">'robust_oim'</span><span class="s1">:</span>
            <span class="s1">res.cov_params_default = res.cov_params_robust_oim</span>
            <span class="s1">res.cov_kwds[</span><span class="s5">'description'</span><span class="s1">] = descriptions[</span><span class="s5">'robust-OIM'</span><span class="s1">].format(</span>
                                                <span class="s1">approx_type=approx_type_str)</span>
        <span class="s3">elif </span><span class="s1">self.cov_type == </span><span class="s5">'robust_approx'</span><span class="s1">:</span>
            <span class="s1">res.cov_params_default = res.cov_params_robust_approx</span>
            <span class="s1">res.cov_kwds[</span><span class="s5">'description'</span><span class="s1">] = descriptions[</span><span class="s5">'robust-approx'</span><span class="s1">].format(</span>
                                                <span class="s1">approx_type=approx_type_str)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">'Invalid covariance matrix type.'</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">res</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">aic(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (float) Akaike Information Criterion 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">aic(self.llf</span><span class="s3">, </span><span class="s1">self.nobs_effective</span><span class="s3">, </span><span class="s1">self.df_model)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">aicc(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (float) Akaike Information Criterion with small sample correction 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">aicc(self.llf</span><span class="s3">, </span><span class="s1">self.nobs_effective</span><span class="s3">, </span><span class="s1">self.df_model)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">bic(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (float) Bayes Information Criterion 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">bic(self.llf</span><span class="s3">, </span><span class="s1">self.nobs_effective</span><span class="s3">, </span><span class="s1">self.df_model)</span>

    <span class="s3">def </span><span class="s1">_cov_params_approx(self</span><span class="s3">, </span><span class="s1">approx_complex_step=</span><span class="s3">True,</span>
                           <span class="s1">approx_centered=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s1">evaluated_hessian = self.nobs_effective * self.model.hessian(</span>
            <span class="s1">params=self.params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True,</span>
            <span class="s1">method=</span><span class="s5">'approx'</span><span class="s3">, </span><span class="s1">approx_complex_step=approx_complex_step</span><span class="s3">,</span>
            <span class="s1">approx_centered=approx_centered)</span>
        <span class="s0"># TODO: Case with &quot;not approx_complex_step&quot; is not hit in</span>
        <span class="s0"># tests as of 2017-05-19</span>

        <span class="s3">if </span><span class="s1">len(self.fixed_params) &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">mask = np.ix_(self._free_params_index</span><span class="s3">, </span><span class="s1">self._free_params_index)</span>
            <span class="s1">(tmp</span><span class="s3">, </span><span class="s1">singular_values) = pinv_extended(evaluated_hessian[mask])</span>
            <span class="s1">neg_cov = np.zeros_like(evaluated_hessian) * np.nan</span>
            <span class="s1">neg_cov[mask] = tmp</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">(neg_cov</span><span class="s3">, </span><span class="s1">singular_values) = pinv_extended(evaluated_hessian)</span>

        <span class="s1">self.model.update(self.params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">self._rank </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self._rank = np.linalg.matrix_rank(np.diag(singular_values))</span>
        <span class="s3">return </span><span class="s1">-neg_cov</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">cov_params_approx(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (array) The variance / covariance matrix. Computed using the numerical 
        Hessian approximated by complex step or finite differences methods. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self._cov_params_approx(self._cov_approx_complex_step</span><span class="s3">,</span>
                                       <span class="s1">self._cov_approx_centered)</span>

    <span class="s3">def </span><span class="s1">_cov_params_oim(self</span><span class="s3">, </span><span class="s1">approx_complex_step=</span><span class="s3">True, </span><span class="s1">approx_centered=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s1">evaluated_hessian = self.nobs_effective * self.model.hessian(</span>
            <span class="s1">self.params</span><span class="s3">, </span><span class="s1">hessian_method=</span><span class="s5">'oim'</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True,</span>
            <span class="s1">includes_fixed=</span><span class="s3">True, </span><span class="s1">approx_complex_step=approx_complex_step</span><span class="s3">,</span>
            <span class="s1">approx_centered=approx_centered)</span>

        <span class="s3">if </span><span class="s1">len(self.fixed_params) &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">mask = np.ix_(self._free_params_index</span><span class="s3">, </span><span class="s1">self._free_params_index)</span>
            <span class="s1">(tmp</span><span class="s3">, </span><span class="s1">singular_values) = pinv_extended(evaluated_hessian[mask])</span>
            <span class="s1">neg_cov = np.zeros_like(evaluated_hessian) * np.nan</span>
            <span class="s1">neg_cov[mask] = tmp</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">(neg_cov</span><span class="s3">, </span><span class="s1">singular_values) = pinv_extended(evaluated_hessian)</span>

        <span class="s1">self.model.update(self.params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">self._rank </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self._rank = np.linalg.matrix_rank(np.diag(singular_values))</span>
        <span class="s3">return </span><span class="s1">-neg_cov</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">cov_params_oim(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (array) The variance / covariance matrix. Computed using the method 
        from Harvey (1989). 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self._cov_params_oim(self._cov_approx_complex_step</span><span class="s3">,</span>
                                    <span class="s1">self._cov_approx_centered)</span>

    <span class="s3">def </span><span class="s1">_cov_params_opg(self</span><span class="s3">, </span><span class="s1">approx_complex_step=</span><span class="s3">True, </span><span class="s1">approx_centered=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s1">evaluated_hessian = self.nobs_effective * self.model._hessian_opg(</span>
            <span class="s1">self.params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True,</span>
            <span class="s1">approx_complex_step=approx_complex_step</span><span class="s3">,</span>
            <span class="s1">approx_centered=approx_centered)</span>

        <span class="s1">no_free_params = (self._free_params_index </span><span class="s3">is not None and</span>
                          <span class="s1">len(self._free_params_index) == </span><span class="s4">0</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">no_free_params:</span>
            <span class="s1">neg_cov = np.zeros_like(evaluated_hessian) * np.nan</span>
            <span class="s1">singular_values = np.empty(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">len(self.fixed_params) &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">mask = np.ix_(self._free_params_index</span><span class="s3">, </span><span class="s1">self._free_params_index)</span>
            <span class="s1">(tmp</span><span class="s3">, </span><span class="s1">singular_values) = pinv_extended(evaluated_hessian[mask])</span>
            <span class="s1">neg_cov = np.zeros_like(evaluated_hessian) * np.nan</span>
            <span class="s1">neg_cov[mask] = tmp</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">(neg_cov</span><span class="s3">, </span><span class="s1">singular_values) = pinv_extended(evaluated_hessian)</span>

        <span class="s1">self.model.update(self.params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">self._rank </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">no_free_params:</span>
                <span class="s1">self._rank = </span><span class="s4">0</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">self._rank = np.linalg.matrix_rank(np.diag(singular_values))</span>
        <span class="s3">return </span><span class="s1">-neg_cov</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">cov_params_opg(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (array) The variance / covariance matrix. Computed using the outer 
        product of gradients method. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self._cov_params_opg(self._cov_approx_complex_step</span><span class="s3">,</span>
                                    <span class="s1">self._cov_approx_centered)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">cov_params_robust(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (array) The QMLE variance / covariance matrix. Alias for 
        `cov_params_robust_oim` 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.cov_params_robust_oim</span>

    <span class="s3">def </span><span class="s1">_cov_params_robust_oim(self</span><span class="s3">, </span><span class="s1">approx_complex_step=</span><span class="s3">True,</span>
                               <span class="s1">approx_centered=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s1">cov_opg = self._cov_params_opg(approx_complex_step=approx_complex_step</span><span class="s3">,</span>
                                       <span class="s1">approx_centered=approx_centered)</span>

        <span class="s1">evaluated_hessian = self.nobs_effective * self.model.hessian(</span>
            <span class="s1">self.params</span><span class="s3">, </span><span class="s1">hessian_method=</span><span class="s5">'oim'</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True,</span>
            <span class="s1">includes_fixed=</span><span class="s3">True, </span><span class="s1">approx_complex_step=approx_complex_step</span><span class="s3">,</span>
            <span class="s1">approx_centered=approx_centered)</span>

        <span class="s3">if </span><span class="s1">len(self.fixed_params) &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">mask = np.ix_(self._free_params_index</span><span class="s3">, </span><span class="s1">self._free_params_index)</span>
            <span class="s1">cov_params = np.zeros_like(evaluated_hessian) * np.nan</span>

            <span class="s1">cov_opg = cov_opg[mask]</span>
            <span class="s1">evaluated_hessian = evaluated_hessian[mask]</span>

            <span class="s1">tmp</span><span class="s3">, </span><span class="s1">singular_values = pinv_extended(</span>
                <span class="s1">np.dot(np.dot(evaluated_hessian</span><span class="s3">, </span><span class="s1">cov_opg)</span><span class="s3">, </span><span class="s1">evaluated_hessian))</span>

            <span class="s1">cov_params[mask] = tmp</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">(cov_params</span><span class="s3">, </span><span class="s1">singular_values) = pinv_extended(</span>
                <span class="s1">np.dot(np.dot(evaluated_hessian</span><span class="s3">, </span><span class="s1">cov_opg)</span><span class="s3">, </span><span class="s1">evaluated_hessian))</span>

        <span class="s1">self.model.update(self.params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">self._rank </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self._rank = np.linalg.matrix_rank(np.diag(singular_values))</span>
        <span class="s3">return </span><span class="s1">cov_params</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">cov_params_robust_oim(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (array) The QMLE variance / covariance matrix. Computed using the 
        method from Harvey (1989) as the evaluated hessian. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self._cov_params_robust_oim(self._cov_approx_complex_step</span><span class="s3">,</span>
                                           <span class="s1">self._cov_approx_centered)</span>

    <span class="s3">def </span><span class="s1">_cov_params_robust_approx(self</span><span class="s3">, </span><span class="s1">approx_complex_step=</span><span class="s3">True,</span>
                                  <span class="s1">approx_centered=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s1">cov_opg = self._cov_params_opg(approx_complex_step=approx_complex_step</span><span class="s3">,</span>
                                       <span class="s1">approx_centered=approx_centered)</span>

        <span class="s1">evaluated_hessian = self.nobs_effective * self.model.hessian(</span>
            <span class="s1">self.params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True,</span>
            <span class="s1">method=</span><span class="s5">'approx'</span><span class="s3">, </span><span class="s1">approx_complex_step=approx_complex_step)</span>
        <span class="s0"># TODO: Case with &quot;not approx_complex_step&quot; is not</span>
        <span class="s0"># hit in tests as of 2017-05-19</span>

        <span class="s3">if </span><span class="s1">len(self.fixed_params) &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">mask = np.ix_(self._free_params_index</span><span class="s3">, </span><span class="s1">self._free_params_index)</span>
            <span class="s1">cov_params = np.zeros_like(evaluated_hessian) * np.nan</span>

            <span class="s1">cov_opg = cov_opg[mask]</span>
            <span class="s1">evaluated_hessian = evaluated_hessian[mask]</span>

            <span class="s1">tmp</span><span class="s3">, </span><span class="s1">singular_values = pinv_extended(</span>
                <span class="s1">np.dot(np.dot(evaluated_hessian</span><span class="s3">, </span><span class="s1">cov_opg)</span><span class="s3">, </span><span class="s1">evaluated_hessian))</span>

            <span class="s1">cov_params[mask] = tmp</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">(cov_params</span><span class="s3">, </span><span class="s1">singular_values) = pinv_extended(</span>
                <span class="s1">np.dot(np.dot(evaluated_hessian</span><span class="s3">, </span><span class="s1">cov_opg)</span><span class="s3">, </span><span class="s1">evaluated_hessian))</span>

        <span class="s1">self.model.update(self.params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">self._rank </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self._rank = np.linalg.matrix_rank(np.diag(singular_values))</span>
        <span class="s3">return </span><span class="s1">cov_params</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">cov_params_robust_approx(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (array) The QMLE variance / covariance matrix. Computed using the 
        numerical Hessian as the evaluated hessian. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self._cov_params_robust_approx(self._cov_approx_complex_step</span><span class="s3">,</span>
                                              <span class="s1">self._cov_approx_centered)</span>

    <span class="s3">def </span><span class="s1">info_criteria(self</span><span class="s3">, </span><span class="s1">criteria</span><span class="s3">, </span><span class="s1">method=</span><span class="s5">'standard'</span><span class="s1">):</span>
        <span class="s2">r&quot;&quot;&quot; 
        Information criteria 
 
        Parameters 
        ---------- 
        criteria : {'aic', 'bic', 'hqic'} 
            The information criteria to compute. 
        method : {'standard', 'lutkepohl'} 
            The method for information criteria computation. Default is 
            'standard' method; 'lutkepohl' computes the information criteria 
            as in Lütkepohl (2007). See Notes for formulas. 
 
        Notes 
        ----- 
        The `'standard'` formulas are: 
 
        .. math:: 
 
            AIC &amp; = -2 \log L(Y_n | \hat \psi) + 2 k \\ 
            BIC &amp; = -2 \log L(Y_n | \hat \psi) + k \log n \\ 
            HQIC &amp; = -2 \log L(Y_n | \hat \psi) + 2 k \log \log n \\ 
 
        where :math:`\hat \psi` are the maximum likelihood estimates of the 
        parameters, :math:`n` is the number of observations, and `k` is the 
        number of estimated parameters. 
 
        Note that the `'standard'` formulas are returned from the `aic`, `bic`, 
        and `hqic` results attributes. 
 
        The `'lutkepohl'` formulas are (Lütkepohl, 2010): 
 
        .. math:: 
 
            AIC_L &amp; = \log | Q | + \frac{2 k}{n} \\ 
            BIC_L &amp; = \log | Q | + \frac{k \log n}{n} \\ 
            HQIC_L &amp; = \log | Q | + \frac{2 k \log \log n}{n} \\ 
 
        where :math:`Q` is the state covariance matrix. Note that the Lütkepohl 
        definitions do not apply to all state space models, and should be used 
        with care outside of SARIMAX and VARMAX models. 
 
        References 
        ---------- 
        .. [*] Lütkepohl, Helmut. 2007. *New Introduction to Multiple Time* 
           *Series Analysis.* Berlin: Springer. 
        &quot;&quot;&quot;</span>
        <span class="s1">criteria = criteria.lower()</span>
        <span class="s1">method = method.lower()</span>

        <span class="s3">if </span><span class="s1">method == </span><span class="s5">'standard'</span><span class="s1">:</span>
            <span class="s1">out = getattr(self</span><span class="s3">, </span><span class="s1">criteria)</span>
        <span class="s3">elif </span><span class="s1">method == </span><span class="s5">'lutkepohl'</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self.filter_results.state_cov.shape[-</span><span class="s4">1</span><span class="s1">] &gt; </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot compute Lütkepohl statistics for'</span>
                                 <span class="s5">' models with time-varying state covariance'</span>
                                 <span class="s5">' matrix.'</span><span class="s1">)</span>

            <span class="s1">cov = self.filter_results.state_cov[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span>
            <span class="s3">if </span><span class="s1">criteria == </span><span class="s5">'aic'</span><span class="s1">:</span>
                <span class="s1">out = np.squeeze(np.linalg.slogdet(cov)[</span><span class="s4">1</span><span class="s1">] +</span>
                                 <span class="s4">2 </span><span class="s1">* self.df_model / self.nobs_effective)</span>
            <span class="s3">elif </span><span class="s1">criteria == </span><span class="s5">'bic'</span><span class="s1">:</span>
                <span class="s1">out = np.squeeze(np.linalg.slogdet(cov)[</span><span class="s4">1</span><span class="s1">] +</span>
                                 <span class="s1">self.df_model * np.log(self.nobs_effective) /</span>
                                 <span class="s1">self.nobs_effective)</span>
            <span class="s3">elif </span><span class="s1">criteria == </span><span class="s5">'hqic'</span><span class="s1">:</span>
                <span class="s1">out = np.squeeze(np.linalg.slogdet(cov)[</span><span class="s4">1</span><span class="s1">] +</span>
                                 <span class="s4">2 </span><span class="s1">* self.df_model *</span>
                                 <span class="s1">np.log(np.log(self.nobs_effective)) /</span>
                                 <span class="s1">self.nobs_effective)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Invalid information criteria'</span><span class="s1">)</span>

        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Invalid information criteria computation method'</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">out</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">fittedvalues(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (array) The predicted values of the model. An (nobs x k_endog) array. 
        &quot;&quot;&quot;</span>
        <span class="s0"># This is a (k_endog x nobs array; do not want to squeeze in case of</span>
        <span class="s0"># the corner case where nobs = 1 (mostly a concern in the predict or</span>
        <span class="s0"># forecast functions, but here also to maintain consistency)</span>
        <span class="s1">fittedvalues = self.forecasts</span>
        <span class="s3">if </span><span class="s1">fittedvalues </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">pass</span>
        <span class="s3">elif </span><span class="s1">fittedvalues.shape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">fittedvalues = fittedvalues[</span><span class="s4">0</span><span class="s3">, </span><span class="s1">:]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">fittedvalues = fittedvalues.T</span>
        <span class="s3">return </span><span class="s1">fittedvalues</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">hqic(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (float) Hannan-Quinn Information Criterion 
        &quot;&quot;&quot;</span>
        <span class="s0"># return (-2 * self.llf +</span>
        <span class="s0">#         2 * np.log(np.log(self.nobs_effective)) * self.df_model)</span>
        <span class="s3">return </span><span class="s1">hqic(self.llf</span><span class="s3">, </span><span class="s1">self.nobs_effective</span><span class="s3">, </span><span class="s1">self.df_model)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">llf_obs(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (float) The value of the log-likelihood function evaluated at `params`. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.filter_results.llf_obs</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">llf(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (float) The value of the log-likelihood function evaluated at `params`. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.filter_results.llf</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">loglikelihood_burn(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (float) The number of observations during which the likelihood is not 
        evaluated. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.filter_results.loglikelihood_burn</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">mae(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (float) Mean absolute error 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.mean(np.abs(self.resid))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">mse(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (float) Mean squared error 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.sse / self.nobs</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">pvalues(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (array) The p-values associated with the z-statistics of the 
        coefficients. Note that the coefficients are assumed to have a Normal 
        distribution. 
        &quot;&quot;&quot;</span>
        <span class="s1">pvalues = np.zeros_like(self.zvalues) * np.nan</span>
        <span class="s1">mask = np.ones_like(pvalues</span><span class="s3">, </span><span class="s1">dtype=bool)</span>
        <span class="s1">mask[self._free_params_index] = </span><span class="s3">True</span>
        <span class="s1">mask &amp;= ~np.isnan(self.zvalues)</span>
        <span class="s1">pvalues[mask] = norm.sf(np.abs(self.zvalues[mask])) * </span><span class="s4">2</span>
        <span class="s3">return </span><span class="s1">pvalues</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (array) The model residuals. An (nobs x k_endog) array. 
        &quot;&quot;&quot;</span>
        <span class="s0"># This is a (k_endog x nobs array; do not want to squeeze in case of</span>
        <span class="s0"># the corner case where nobs = 1 (mostly a concern in the predict or</span>
        <span class="s0"># forecast functions, but here also to maintain consistency)</span>
        <span class="s1">resid = self.forecasts_error</span>
        <span class="s3">if </span><span class="s1">resid </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">pass</span>
        <span class="s3">elif </span><span class="s1">resid.shape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">resid = resid[</span><span class="s4">0</span><span class="s3">, </span><span class="s1">:]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">resid = resid.T</span>
        <span class="s3">return </span><span class="s1">resid</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">states(self):</span>
        <span class="s3">if </span><span class="s1">self.model._index_generated </span><span class="s3">and not </span><span class="s1">self.model._index_none:</span>
            <span class="s1">warnings.warn(</span><span class="s5">'No supported index is available. The `states`'</span>
                          <span class="s5">' DataFrame uses a generated integer index'</span><span class="s3">,</span>
                          <span class="s1">ValueWarning)</span>
        <span class="s3">return </span><span class="s1">self._states</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">sse(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (float) Sum of squared errors 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.sum(self.resid**</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">zvalues(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        (array) The z-statistics for the coefficients. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.params / self.bse</span>

    <span class="s3">def </span><span class="s1">test_normality(self</span><span class="s3">, </span><span class="s1">method):</span>
        <span class="s2">&quot;&quot;&quot; 
        Test for normality of standardized residuals. 
 
        Null hypothesis is normality. 
 
        Parameters 
        ---------- 
        method : {'jarquebera', None} 
            The statistical test for normality. Must be 'jarquebera' for 
            Jarque-Bera normality test. If None, an attempt is made to select 
            an appropriate test. 
 
        See Also 
        -------- 
        statsmodels.stats.stattools.jarque_bera 
            The Jarque-Bera test of normality. 
 
        Notes 
        ----- 
        Let `d` = max(loglikelihood_burn, nobs_diffuse); this test is 
        calculated ignoring the first `d` residuals. 
 
        In the case of missing data, the maintained hypothesis is that the 
        data are missing completely at random. This test is then run on the 
        standardized residuals excluding those corresponding to missing 
        observations. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">method </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">method = </span><span class="s5">'jarquebera'</span>

        <span class="s3">if </span><span class="s1">self.standardized_forecasts_error </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot compute test statistic when standardized'</span>
                             <span class="s5">' forecast errors have not been computed.'</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">method == </span><span class="s5">'jarquebera'</span><span class="s1">:</span>
            <span class="s3">from </span><span class="s1">statsmodels.stats.stattools </span><span class="s3">import </span><span class="s1">jarque_bera</span>
            <span class="s1">d = np.maximum(self.loglikelihood_burn</span><span class="s3">, </span><span class="s1">self.nobs_diffuse)</span>
            <span class="s1">output = []</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.model.k_endog):</span>
                <span class="s1">resid = self.filter_results.standardized_forecasts_error[i</span><span class="s3">, </span><span class="s1">d:]</span>
                <span class="s1">mask = ~np.isnan(resid)</span>
                <span class="s1">output.append(jarque_bera(resid[mask]))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">'Invalid normality test method.'</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">np.array(output)</span>

    <span class="s3">def </span><span class="s1">test_heteroskedasticity(self</span><span class="s3">, </span><span class="s1">method</span><span class="s3">, </span><span class="s1">alternative=</span><span class="s5">'two-sided'</span><span class="s3">,</span>
                                <span class="s1">use_f=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s2">r&quot;&quot;&quot; 
        Test for heteroskedasticity of standardized residuals 
 
        Tests whether the sum-of-squares in the first third of the sample is 
        significantly different than the sum-of-squares in the last third 
        of the sample. Analogous to a Goldfeld-Quandt test. The null hypothesis 
        is of no heteroskedasticity. 
 
        Parameters 
        ---------- 
        method : {'breakvar', None} 
            The statistical test for heteroskedasticity. Must be 'breakvar' 
            for test of a break in the variance. If None, an attempt is 
            made to select an appropriate test. 
        alternative : str, 'increasing', 'decreasing' or 'two-sided' 
            This specifies the alternative for the p-value calculation. Default 
            is two-sided. 
        use_f : bool, optional 
            Whether or not to compare against the asymptotic distribution 
            (chi-squared) or the approximate small-sample distribution (F). 
            Default is True (i.e. default is to compare against an F 
            distribution). 
 
        Returns 
        ------- 
        output : ndarray 
            An array with `(test_statistic, pvalue)` for each endogenous 
            variable. The array is then sized `(k_endog, 2)`. If the method is 
            called as `het = res.test_heteroskedasticity()`, then `het[0]` is 
            an array of size 2 corresponding to the first endogenous variable, 
            where `het[0][0]` is the test statistic, and `het[0][1]` is the 
            p-value. 
 
        See Also 
        -------- 
        statsmodels.tsa.stattools.breakvar_heteroskedasticity_test 
 
        Notes 
        ----- 
        The null hypothesis is of no heteroskedasticity. 
 
        For :math:`h = [T/3]`, the test statistic is: 
 
        .. math:: 
 
            H(h) = \sum_{t=T-h+1}^T  \tilde v_t^2 
            \Bigg / \sum_{t=d+1}^{d+1+h} \tilde v_t^2 
 
        where :math:`d` = max(loglikelihood_burn, nobs_diffuse)` (usually 
        corresponding to diffuse initialization under either the approximate 
        or exact approach). 
 
        This statistic can be tested against an :math:`F(h,h)` distribution. 
        Alternatively, :math:`h H(h)` is asymptotically distributed according 
        to :math:`\chi_h^2`; this second test can be applied by passing 
        `use_f=True` as an argument. 
 
        See section 5.4 of [1]_ for the above formula and discussion, as well 
        as additional details. 
 
        TODO 
 
        - Allow specification of :math:`h` 
 
        References 
        ---------- 
        .. [1] Harvey, Andrew C. 1990. *Forecasting, Structural Time Series* 
               *Models and the Kalman Filter.* Cambridge University Press. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">method </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">method = </span><span class="s5">'breakvar'</span>

        <span class="s3">if </span><span class="s1">self.standardized_forecasts_error </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot compute test statistic when standardized'</span>
                             <span class="s5">' forecast errors have not been computed.'</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">method == </span><span class="s5">'breakvar'</span><span class="s1">:</span>
            <span class="s3">from </span><span class="s1">statsmodels.tsa.stattools </span><span class="s3">import </span><span class="s1">(</span>
                <span class="s1">breakvar_heteroskedasticity_test</span>
                <span class="s1">)</span>
            <span class="s0"># Store some values</span>
            <span class="s1">resid = self.filter_results.standardized_forecasts_error</span>
            <span class="s1">d = np.maximum(self.loglikelihood_burn</span><span class="s3">, </span><span class="s1">self.nobs_diffuse)</span>
            <span class="s0"># This differs from self.nobs_effective because here we want to</span>
            <span class="s0"># exclude exact diffuse periods, whereas self.nobs_effective only</span>
            <span class="s0"># excludes explicitly burned (usually approximate diffuse) periods.</span>
            <span class="s1">nobs_effective = self.nobs - d</span>
            <span class="s1">h = int(np.round(nobs_effective / </span><span class="s4">3</span><span class="s1">))</span>

            <span class="s1">test_statistics = []</span>
            <span class="s1">p_values = []</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.model.k_endog):</span>
                <span class="s1">test_statistic</span><span class="s3">, </span><span class="s1">p_value = breakvar_heteroskedasticity_test(</span>
                    <span class="s1">resid[i</span><span class="s3">, </span><span class="s1">d:]</span><span class="s3">,</span>
                    <span class="s1">subset_length=h</span><span class="s3">,</span>
                    <span class="s1">alternative=alternative</span><span class="s3">,</span>
                    <span class="s1">use_f=use_f</span>
                    <span class="s1">)</span>
                <span class="s1">test_statistics.append(test_statistic)</span>
                <span class="s1">p_values.append(p_value)</span>

            <span class="s1">output = np.c_[test_statistics</span><span class="s3">, </span><span class="s1">p_values]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">'Invalid heteroskedasticity test'</span>
                                      <span class="s5">' method.'</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">output</span>

    <span class="s3">def </span><span class="s1">test_serial_correlation(self</span><span class="s3">, </span><span class="s1">method</span><span class="s3">, </span><span class="s1">df_adjust=</span><span class="s3">False, </span><span class="s1">lags=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Ljung-Box test for no serial correlation of standardized residuals 
 
        Null hypothesis is no serial correlation. 
 
        Parameters 
        ---------- 
        method : {'ljungbox','boxpierece', None} 
            The statistical test for serial correlation. If None, an attempt is 
            made to select an appropriate test. 
        lags : None, int or array_like 
            If lags is an integer then this is taken to be the largest lag 
            that is included, the test result is reported for all smaller lag 
            length. 
            If lags is a list or array, then all lags are included up to the 
            largest lag in the list, however only the tests for the lags in the 
            list are reported. 
            If lags is None, then the default maxlag is min(10, nobs // 5) for 
            non-seasonal models and min(2*m, nobs // 5) for seasonal time 
            series where m is the seasonal period. 
        df_adjust : bool, optional 
            If True, the degrees of freedom consumed by the model is subtracted 
            from the degrees-of-freedom used in the test so that the adjusted 
            dof for the statistics are lags - model_df. In an ARMA model, this 
            value is usually p+q where p is the AR order and q is the MA order. 
            When using df_adjust, it is not possible to use tests based on 
            fewer than model_df lags. 
        Returns 
        ------- 
        output : ndarray 
            An array with `(test_statistic, pvalue)` for each endogenous 
            variable and each lag. The array is then sized 
            `(k_endog, 2, lags)`. If the method is called as 
            `ljungbox = res.test_serial_correlation()`, then `ljungbox[i]` 
            holds the results of the Ljung-Box test (as would be returned by 
            `statsmodels.stats.diagnostic.acorr_ljungbox`) for the `i` th 
            endogenous variable. 
 
        See Also 
        -------- 
        statsmodels.stats.diagnostic.acorr_ljungbox 
            Ljung-Box test for serial correlation. 
 
        Notes 
        ----- 
        Let `d` = max(loglikelihood_burn, nobs_diffuse); this test is 
        calculated ignoring the first `d` residuals. 
 
        Output is nan for any endogenous variable which has missing values. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">method </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">method = </span><span class="s5">'ljungbox'</span>

        <span class="s3">if </span><span class="s1">self.standardized_forecasts_error </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot compute test statistic when standardized'</span>
                             <span class="s5">' forecast errors have not been computed.'</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">method == </span><span class="s5">'ljungbox' </span><span class="s3">or </span><span class="s1">method == </span><span class="s5">'boxpierce'</span><span class="s1">:</span>
            <span class="s3">from </span><span class="s1">statsmodels.stats.diagnostic </span><span class="s3">import </span><span class="s1">acorr_ljungbox</span>
            <span class="s1">d = np.maximum(self.loglikelihood_burn</span><span class="s3">, </span><span class="s1">self.nobs_diffuse)</span>
            <span class="s0"># This differs from self.nobs_effective because here we want to</span>
            <span class="s0"># exclude exact diffuse periods, whereas self.nobs_effective only</span>
            <span class="s0"># excludes explicitly burned (usually approximate diffuse) periods.</span>
            <span class="s1">nobs_effective = self.nobs - d</span>
            <span class="s1">output = []</span>

            <span class="s0"># Default lags for acorr_ljungbox is 40, but may not always have</span>
            <span class="s0"># that many observations</span>
            <span class="s3">if </span><span class="s1">lags </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">seasonal_periods = getattr(self.model</span><span class="s3">, </span><span class="s5">&quot;seasonal_periods&quot;</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
                <span class="s3">if </span><span class="s1">seasonal_periods:</span>
                    <span class="s1">lags = min(</span><span class="s4">2 </span><span class="s1">* seasonal_periods</span><span class="s3">, </span><span class="s1">nobs_effective // </span><span class="s4">5</span><span class="s1">)</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">lags = min(</span><span class="s4">10</span><span class="s3">, </span><span class="s1">nobs_effective // </span><span class="s4">5</span><span class="s1">)</span>

            <span class="s1">model_df = </span><span class="s4">0</span>
            <span class="s3">if </span><span class="s1">df_adjust:</span>
                <span class="s1">model_df = max(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">self.df_model - self.k_diffuse_states - </span><span class="s4">1</span><span class="s1">)</span>

            <span class="s1">cols = [</span><span class="s4">2</span><span class="s3">, </span><span class="s4">3</span><span class="s1">] </span><span class="s3">if </span><span class="s1">method == </span><span class="s5">'boxpierce' </span><span class="s3">else </span><span class="s1">[</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.model.k_endog):</span>
                <span class="s1">results = acorr_ljungbox(</span>
                    <span class="s1">self.filter_results.standardized_forecasts_error[i][d:]</span><span class="s3">,</span>
                    <span class="s1">lags=lags</span><span class="s3">, </span><span class="s1">boxpierce=(method == </span><span class="s5">'boxpierce'</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">model_df=model_df)</span>
                <span class="s1">output.append(np.asarray(results)[:</span><span class="s3">, </span><span class="s1">cols].T)</span>

            <span class="s1">output = np.c_[output]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">'Invalid serial correlation test'</span>
                                      <span class="s5">' method.'</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">output</span>

    <span class="s3">def </span><span class="s1">get_prediction(self</span><span class="s3">, </span><span class="s1">start=</span><span class="s3">None, </span><span class="s1">end=</span><span class="s3">None, </span><span class="s1">dynamic=</span><span class="s3">False,</span>
                       <span class="s1">information_set=</span><span class="s5">'predicted'</span><span class="s3">, </span><span class="s1">signal_only=</span><span class="s3">False,</span>
                       <span class="s1">index=</span><span class="s3">None, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">extend_model=</span><span class="s3">None,</span>
                       <span class="s1">extend_kwargs=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s2">r&quot;&quot;&quot; 
        In-sample prediction and out-of-sample forecasting 
 
        Parameters 
        ---------- 
        start : int, str, or datetime, optional 
            Zero-indexed observation number at which to start forecasting, 
            i.e., the first forecast is start. Can also be a date string to 
            parse or a datetime type. Default is the the zeroth observation. 
        end : int, str, or datetime, optional 
            Zero-indexed observation number at which to end forecasting, i.e., 
            the last forecast is end. Can also be a date string to 
            parse or a datetime type. However, if the dates index does not 
            have a fixed frequency, end must be an integer index if you 
            want out of sample prediction. Default is the last observation in 
            the sample. 
        dynamic : bool, int, str, or datetime, optional 
            Integer offset relative to `start` at which to begin dynamic 
            prediction. Can also be an absolute date string to parse or a 
            datetime type (these are not interpreted as offsets). 
            Prior to this observation, true endogenous values will be used for 
            prediction; starting with this observation and continuing through 
            the end of prediction, forecasted endogenous values will be used 
            instead. 
        information_set : str, optional 
            The information set to condition each prediction on. Default is 
            &quot;predicted&quot;, which computes predictions of period t values 
            conditional on observed data through period t-1; these are 
            one-step-ahead predictions, and correspond with the typical 
            `fittedvalues` results attribute. Alternatives are &quot;filtered&quot;, 
            which computes predictions of period t values conditional on 
            observed data through period t, and &quot;smoothed&quot;, which computes 
            predictions of period t values conditional on the entire dataset 
            (including also future observations t+1, t+2, ...). 
        signal_only : bool, optional 
            Whether to compute predictions of only the &quot;signal&quot; component of 
            the observation equation. Default is False. For example, the 
            observation equation of a time-invariant model is 
            :math:`y_t = d + Z \alpha_t + \varepsilon_t`, and the &quot;signal&quot; 
            component is then :math:`Z \alpha_t`. If this argument is set to 
            True, then predictions of the &quot;signal&quot; :math:`Z \alpha_t` will be 
            returned. Otherwise, the default is for predictions of :math:`y_t` 
            to be returned. 
        **kwargs 
            Additional arguments may required for forecasting beyond the end 
            of the sample. See `FilterResults.predict` for more details. 
 
        Returns 
        ------- 
        predictions : PredictionResults 
            PredictionResults instance containing in-sample predictions / 
            out-of-sample forecasts and results including confidence intervals. 
 
        See Also 
        -------- 
        forecast 
            Out-of-sample forecasts. 
        predict 
            In-sample predictions and out-of-sample forecasts. 
        get_forecast 
            Out-of-sample forecasts and results including confidence intervals. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">start </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">start = </span><span class="s4">0</span>

        <span class="s0"># Handle start, end, dynamic</span>
        <span class="s1">start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">out_of_sample</span><span class="s3">, </span><span class="s1">prediction_index = (</span>
            <span class="s1">self.model._get_prediction_index(start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">index))</span>

        <span class="s0"># Handle `dynamic`</span>
        <span class="s3">if </span><span class="s1">isinstance(dynamic</span><span class="s3">, </span><span class="s1">(str</span><span class="s3">, </span><span class="s1">dt.datetime</span><span class="s3">, </span><span class="s1">pd.Timestamp)):</span>
            <span class="s1">dynamic</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ = self.model._get_index_loc(dynamic)</span>
            <span class="s0"># Convert to offset relative to start</span>
            <span class="s1">dynamic = dynamic - start</span>

        <span class="s0"># If we have out-of-sample forecasting and `exog` or in general any</span>
        <span class="s0"># kind of time-varying state space model, then we need to create an</span>
        <span class="s0"># extended model to get updated state space system matrices</span>
        <span class="s3">if </span><span class="s1">extend_model </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">extend_model = (self.model.exog </span><span class="s3">is not None or</span>
                            <span class="s3">not </span><span class="s1">self.filter_results.time_invariant)</span>
        <span class="s3">if </span><span class="s1">out_of_sample </span><span class="s3">and </span><span class="s1">extend_model:</span>
            <span class="s1">kwargs = self.model._get_extension_time_varying_matrices(</span>
                <span class="s1">self.params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">out_of_sample</span><span class="s3">, </span><span class="s1">extend_kwargs</span><span class="s3">,</span>
                <span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True, </span><span class="s1">**kwargs)</span>

        <span class="s0"># Make sure the model class has the current parameters</span>
        <span class="s1">self.model.update(self.params</span><span class="s3">, </span><span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True</span><span class="s1">)</span>

        <span class="s0"># Perform the prediction</span>
        <span class="s0"># This is a (k_endog x npredictions) array; do not want to squeeze in</span>
        <span class="s0"># case of npredictions = 1</span>
        <span class="s1">prediction_results = self.filter_results.predict(</span>
            <span class="s1">start</span><span class="s3">, </span><span class="s1">end + out_of_sample + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">dynamic</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s0"># Return a new mlemodel.PredictionResults object</span>
        <span class="s3">return </span><span class="s1">PredictionResultsWrapper(PredictionResults(</span>
            <span class="s1">self</span><span class="s3">, </span><span class="s1">prediction_results</span><span class="s3">, </span><span class="s1">information_set=information_set</span><span class="s3">,</span>
            <span class="s1">signal_only=signal_only</span><span class="s3">, </span><span class="s1">row_labels=prediction_index))</span>

    <span class="s3">def </span><span class="s1">get_forecast(self</span><span class="s3">, </span><span class="s1">steps=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">signal_only=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s2">r&quot;&quot;&quot; 
        Out-of-sample forecasts and prediction intervals 
 
        Parameters 
        ---------- 
        steps : int, str, or datetime, optional 
            If an integer, the number of steps to forecast from the end of the 
            sample. Can also be a date string to parse or a datetime type. 
            However, if the dates index does not have a fixed frequency, steps 
            must be an integer. Default is 1. 
        signal_only : bool, optional 
            Whether to compute forecasts of only the &quot;signal&quot; component of 
            the observation equation. Default is False. For example, the 
            observation equation of a time-invariant model is 
            :math:`y_t = d + Z \alpha_t + \varepsilon_t`, and the &quot;signal&quot; 
            component is then :math:`Z \alpha_t`. If this argument is set to 
            True, then forecasts of the &quot;signal&quot; :math:`Z \alpha_t` will be 
            returned. Otherwise, the default is for forecasts of :math:`y_t` 
            to be returned. 
        **kwargs 
            Additional arguments may required for forecasting beyond the end 
            of the sample. See `FilterResults.predict` for more details. 
 
        Returns 
        ------- 
        forecasts : PredictionResults 
            PredictionResults instance containing out-of-sample forecasts and 
            results including confidence intervals. 
 
        See also 
        -------- 
        forecast 
            Out-of-sample forecasts. 
        predict 
            In-sample predictions and out-of-sample forecasts. 
        get_prediction 
            In-sample predictions / out-of-sample forecasts and results 
            including confidence intervals. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">isinstance(steps</span><span class="s3">, </span><span class="s1">int):</span>
            <span class="s1">end = self.nobs + steps - </span><span class="s4">1</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">end = steps</span>
        <span class="s3">return </span><span class="s1">self.get_prediction(start=self.nobs</span><span class="s3">, </span><span class="s1">end=end</span><span class="s3">,</span>
                                   <span class="s1">signal_only=signal_only</span><span class="s3">, </span><span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">start=</span><span class="s3">None, </span><span class="s1">end=</span><span class="s3">None, </span><span class="s1">dynamic=</span><span class="s3">False,</span>
                <span class="s1">information_set=</span><span class="s5">'predicted'</span><span class="s3">, </span><span class="s1">signal_only=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s2">r&quot;&quot;&quot; 
        In-sample prediction and out-of-sample forecasting 
 
        Parameters 
        ---------- 
        start : {int, str,datetime}, optional 
            Zero-indexed observation number at which to start forecasting, 
            i.e., the first forecast is start. Can also be a date string to 
            parse or a datetime type. Default is the zeroth observation. 
        end : {int, str,datetime}, optional 
            Zero-indexed observation number at which to end forecasting, i.e., 
            the last forecast is end. Can also be a date string to 
            parse or a datetime type. However, if the dates index does not 
            have a fixed frequency, end must be an integer index if you 
            want out of sample prediction. Default is the last observation in 
            the sample. 
        dynamic : {bool, int, str,datetime}, optional 
            Integer offset relative to `start` at which to begin dynamic 
            prediction. Can also be an absolute date string to parse or a 
            datetime type (these are not interpreted as offsets). 
            Prior to this observation, true endogenous values will be used for 
            prediction; starting with this observation and continuing through 
            the end of prediction, forecasted endogenous values will be used 
            instead. 
        information_set : str, optional 
            The information set to condition each prediction on. Default is 
            &quot;predicted&quot;, which computes predictions of period t values 
            conditional on observed data through period t-1; these are 
            one-step-ahead predictions, and correspond with the typical 
            `fittedvalues` results attribute. Alternatives are &quot;filtered&quot;, 
            which computes predictions of period t values conditional on 
            observed data through period t, and &quot;smoothed&quot;, which computes 
            predictions of period t values conditional on the entire dataset 
            (including also future observations t+1, t+2, ...). 
        signal_only : bool, optional 
            Whether to compute predictions of only the &quot;signal&quot; component of 
            the observation equation. Default is False. For example, the 
            observation equation of a time-invariant model is 
            :math:`y_t = d + Z \alpha_t + \varepsilon_t`, and the &quot;signal&quot; 
            component is then :math:`Z \alpha_t`. If this argument is set to 
            True, then predictions of the &quot;signal&quot; :math:`Z \alpha_t` will be 
            returned. Otherwise, the default is for predictions of :math:`y_t` 
            to be returned. 
        **kwargs 
            Additional arguments may be required for forecasting beyond the end 
            of the sample. See ``FilterResults.predict`` for more details. 
 
        Returns 
        ------- 
        predictions : array_like 
            In-sample predictions / Out-of-sample forecasts. (Numpy array or 
            Pandas Series or DataFrame, depending on input and dimensions). 
            Dimensions are `(npredict x k_endog)`. 
 
        See Also 
        -------- 
        forecast 
            Out-of-sample forecasts. 
        get_forecast 
            Out-of-sample forecasts and results including confidence intervals. 
        get_prediction 
            In-sample predictions / out-of-sample forecasts and results 
            including confidence intervals. 
        &quot;&quot;&quot;</span>
        <span class="s0"># Perform the prediction</span>
        <span class="s1">prediction_results = self.get_prediction(</span>
            <span class="s1">start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">dynamic</span><span class="s3">, </span><span class="s1">information_set=information_set</span><span class="s3">,</span>
            <span class="s1">signal_only=signal_only</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s3">return </span><span class="s1">prediction_results.predicted_mean</span>

    <span class="s3">def </span><span class="s1">forecast(self</span><span class="s3">, </span><span class="s1">steps=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">signal_only=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s2">r&quot;&quot;&quot; 
        Out-of-sample forecasts 
 
        Parameters 
        ---------- 
        steps : int, str, or datetime, optional 
            If an integer, the number of steps to forecast from the end of the 
            sample. Can also be a date string to parse or a datetime type. 
            However, if the dates index does not have a fixed frequency, steps 
            must be an integer. Default is 1. 
        signal_only : bool, optional 
            Whether to compute forecasts of only the &quot;signal&quot; component of 
            the observation equation. Default is False. For example, the 
            observation equation of a time-invariant model is 
            :math:`y_t = d + Z \alpha_t + \varepsilon_t`, and the &quot;signal&quot; 
            component is then :math:`Z \alpha_t`. If this argument is set to 
            True, then forecasts of the &quot;signal&quot; :math:`Z \alpha_t` will be 
            returned. Otherwise, the default is for forecasts of :math:`y_t` 
            to be returned. 
        **kwargs 
            Additional arguments may required for forecasting beyond the end 
            of the sample. See `FilterResults.predict` for more details. 
 
        Returns 
        ------- 
        forecast : array_like 
            Out-of-sample forecasts (Numpy array or Pandas Series or DataFrame, 
            depending on input and dimensions). 
            Dimensions are `(steps x k_endog)`. 
 
        See Also 
        -------- 
        predict 
            In-sample predictions and out-of-sample forecasts. 
        get_forecast 
            Out-of-sample forecasts and results including confidence intervals. 
        get_prediction 
            In-sample predictions / out-of-sample forecasts and results 
            including confidence intervals. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">isinstance(steps</span><span class="s3">, </span><span class="s1">int):</span>
            <span class="s1">end = self.nobs + steps - </span><span class="s4">1</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">end = steps</span>
        <span class="s3">return </span><span class="s1">self.predict(start=self.nobs</span><span class="s3">, </span><span class="s1">end=end</span><span class="s3">, </span><span class="s1">signal_only=signal_only</span><span class="s3">,</span>
                            <span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">simulate(self</span><span class="s3">, </span><span class="s1">nsimulations</span><span class="s3">, </span><span class="s1">measurement_shocks=</span><span class="s3">None,</span>
                 <span class="s1">state_shocks=</span><span class="s3">None, </span><span class="s1">initial_state=</span><span class="s3">None, </span><span class="s1">anchor=</span><span class="s3">None,</span>
                 <span class="s1">repetitions=</span><span class="s3">None, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">extend_model=</span><span class="s3">None,</span>
                 <span class="s1">extend_kwargs=</span><span class="s3">None,</span>
                 <span class="s1">pretransformed_measurement_shocks=</span><span class="s3">True,</span>
                 <span class="s1">pretransformed_state_shocks=</span><span class="s3">True,</span>
                 <span class="s1">pretransformed_initial_state=</span><span class="s3">True,</span>
                 <span class="s1">random_state=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s2">r&quot;&quot;&quot; 
        Simulate a new time series following the state space model 
 
        Parameters 
        ---------- 
        nsimulations : int 
            The number of observations to simulate. If the model is 
            time-invariant this can be any number. If the model is 
            time-varying, then this number must be less than or equal to the 
            number 
        measurement_shocks : array_like, optional 
            If specified, these are the shocks to the measurement equation, 
            :math:`\varepsilon_t`. If unspecified, these are automatically 
            generated using a pseudo-random number generator. If specified, 
            must be shaped `nsimulations` x `k_endog`, where `k_endog` is the 
            same as in the state space model. 
        state_shocks : array_like, optional 
            If specified, these are the shocks to the state equation, 
            :math:`\eta_t`. If unspecified, these are automatically 
            generated using a pseudo-random number generator. If specified, 
            must be shaped `nsimulations` x `k_posdef` where `k_posdef` is the 
            same as in the state space model. 
        initial_state : array_like, optional 
            If specified, this is the initial state vector to use in 
            simulation, which should be shaped (`k_states` x 1), where 
            `k_states` is the same as in the state space model. If unspecified, 
            but the model has been initialized, then that initialization is 
            used. This must be specified if `anchor` is anything other than 
            &quot;start&quot; or 0. 
        anchor : int, str, or datetime, optional 
            Starting point from which to begin the simulations; type depends on 
            the index of the given `endog` model. Two special cases are the 
            strings 'start' and 'end', which refer to starting at the beginning 
            and end of the sample, respectively. If a date/time index was 
            provided to the model, then this argument can be a date string to 
            parse or a datetime type. Otherwise, an integer index should be 
            given. Default is 'start'. 
        repetitions : int, optional 
            Number of simulated paths to generate. Default is 1 simulated path. 
        exog : array_like, optional 
            New observations of exogenous regressors, if applicable. 
        pretransformed_measurement_shocks : bool, optional 
            If `measurement_shocks` is provided, this flag indicates whether it 
            should be directly used as the shocks. If False, then it is assumed 
            to contain draws from the standard Normal distribution that must be 
            transformed using the `obs_cov` covariance matrix. Default is True. 
        pretransformed_state_shocks : bool, optional 
            If `state_shocks` is provided, this flag indicates whether it 
            should be directly used as the shocks. If False, then it is assumed 
            to contain draws from the standard Normal distribution that must be 
            transformed using the `state_cov` covariance matrix. Default is 
            True. 
        pretransformed_initial_state : bool, optional 
            If `initial_state` is provided, this flag indicates whether it 
            should be directly used as the initial_state. If False, then it is 
            assumed to contain draws from the standard Normal distribution that 
            must be transformed using the `initial_state_cov` covariance 
            matrix. Default is True. 
        random_state : {None, int, Generator, RandomState}, optional 
            If `seed` is None (or `np.random`), the 
            class:``~numpy.random.RandomState`` singleton is used. 
            If `seed` is an int, a new class:``~numpy.random.RandomState`` 
            instance is used, seeded with `seed`. 
            If `seed` is already a class:``~numpy.random.Generator`` or 
            class:``~numpy.random.RandomState`` instance then that instance is 
            used. 
 
        Returns 
        ------- 
        simulated_obs : ndarray 
            An array of simulated observations. If `repetitions=None`, then it 
            will be shaped (nsimulations x k_endog) or (nsimulations,) if 
            `k_endog=1`. Otherwise it will be shaped 
            (nsimulations x k_endog x repetitions). If the model was given 
            Pandas input then the output will be a Pandas object. If 
            `k_endog &gt; 1` and `repetitions` is not None, then the output will 
            be a Pandas DataFrame that has a MultiIndex for the columns, with 
            the first level containing the names of the `endog` variables and 
            the second level containing the repetition number. 
 
        See Also 
        -------- 
        impulse_responses 
            Impulse response functions 
        &quot;&quot;&quot;</span>
        <span class="s0"># Get the starting location</span>
        <span class="s3">if </span><span class="s1">anchor </span><span class="s3">is None or </span><span class="s1">anchor == </span><span class="s5">'start'</span><span class="s1">:</span>
            <span class="s1">iloc = </span><span class="s4">0</span>
        <span class="s3">elif </span><span class="s1">anchor == </span><span class="s5">'end'</span><span class="s1">:</span>
            <span class="s1">iloc = self.nobs</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">iloc</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ = self.model._get_index_loc(anchor)</span>
            <span class="s3">if </span><span class="s1">isinstance(iloc</span><span class="s3">, </span><span class="s1">slice):</span>
                <span class="s1">iloc = iloc.start</span>

        <span class="s3">if </span><span class="s1">iloc &lt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">iloc = self.nobs + iloc</span>
        <span class="s3">if </span><span class="s1">iloc &gt; self.nobs:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot anchor simulation outside of the sample.'</span><span class="s1">)</span>

        <span class="s0"># Setup the initial state</span>
        <span class="s3">if </span><span class="s1">initial_state </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">initial_state_moments = (</span>
                <span class="s1">self.predicted_state[:</span><span class="s3">, </span><span class="s1">iloc]</span><span class="s3">,</span>
                <span class="s1">self.predicted_state_cov[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">iloc])</span>

            <span class="s1">_repetitions = </span><span class="s4">1 </span><span class="s3">if </span><span class="s1">repetitions </span><span class="s3">is None else </span><span class="s1">repetitions</span>

            <span class="s1">initial_state = np.random.multivariate_normal(</span>
                <span class="s1">*initial_state_moments</span><span class="s3">, </span><span class="s1">size=_repetitions).T</span>

        <span class="s1">scale = self.scale </span><span class="s3">if </span><span class="s1">self.filter_results.filter_concentrated </span><span class="s3">else None</span>
        <span class="s3">with </span><span class="s1">self.model.ssm.fixed_scale(scale):</span>
            <span class="s1">sim = self.model.simulate(</span>
                <span class="s1">self.params</span><span class="s3">, </span><span class="s1">nsimulations</span><span class="s3">,</span>
                <span class="s1">measurement_shocks=measurement_shocks</span><span class="s3">,</span>
                <span class="s1">state_shocks=state_shocks</span><span class="s3">, </span><span class="s1">initial_state=initial_state</span><span class="s3">,</span>
                <span class="s1">anchor=anchor</span><span class="s3">, </span><span class="s1">repetitions=repetitions</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">,</span>
                <span class="s1">transformed=</span><span class="s3">True, </span><span class="s1">includes_fixed=</span><span class="s3">True,</span>
                <span class="s1">extend_model=extend_model</span><span class="s3">, </span><span class="s1">extend_kwargs=extend_kwargs</span><span class="s3">,</span>
                <span class="s1">pretransformed_measurement_shocks=(</span>
                    <span class="s1">pretransformed_measurement_shocks)</span><span class="s3">,</span>
                <span class="s1">pretransformed_state_shocks=pretransformed_state_shocks</span><span class="s3">,</span>
                <span class="s1">pretransformed_initial_state=pretransformed_initial_state</span><span class="s3">,</span>
                <span class="s1">random_state=random_state</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s3">return </span><span class="s1">sim</span>

    <span class="s3">def </span><span class="s1">impulse_responses(self</span><span class="s3">, </span><span class="s1">steps=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">impulse=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">orthogonalized=</span><span class="s3">False,</span>
                          <span class="s1">cumulative=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Impulse response function 
 
        Parameters 
        ---------- 
        steps : int, optional 
            The number of steps for which impulse responses are calculated. 
            Default is 1. Note that for time-invariant models, the initial 
            impulse is not counted as a step, so if `steps=1`, the output will 
            have 2 entries. 
        impulse : int, str or array_like 
            If an integer, the state innovation to pulse; must be between 0 
            and `k_posdef-1`. If a str, it indicates which column of df 
            the unit (1) impulse is given. 
            Alternatively, a custom impulse vector may be provided; must be 
            shaped `k_posdef x 1`. 
        orthogonalized : bool, optional 
            Whether or not to perform impulse using orthogonalized innovations. 
            Note that this will also affect custum `impulse` vectors. Default 
            is False. 
        cumulative : bool, optional 
            Whether or not to return cumulative impulse responses. Default is 
            False. 
        anchor : int, str, or datetime, optional 
            Time point within the sample for the state innovation impulse. Type 
            depends on the index of the given `endog` in the model. Two special 
            cases are the strings 'start' and 'end', which refer to setting the 
            impulse at the first and last points of the sample, respectively. 
            Integer values can run from 0 to `nobs - 1`, or can be negative to 
            apply negative indexing. Finally, if a date/time index was provided 
            to the model, then this argument can be a date string to parse or a 
            datetime type. Default is 'start'. 
        exog : array_like, optional 
            New observations of exogenous regressors, if applicable. 
        **kwargs 
            If the model has time-varying design or transition matrices and the 
            combination of `anchor` and `steps` implies creating impulse 
            responses for the out-of-sample period, then these matrices must 
            have updated values provided for the out-of-sample steps. For 
            example, if `design` is a time-varying component, `nobs` is 10, 
            `anchor=1`, and `steps` is 15, a (`k_endog` x `k_states` x 7) 
            matrix must be provided with the new design matrix values. 
 
        Returns 
        ------- 
        impulse_responses : ndarray 
            Responses for each endogenous variable due to the impulse 
            given by the `impulse` argument. For a time-invariant model, the 
            impulse responses are given for `steps + 1` elements (this gives 
            the &quot;initial impulse&quot; followed by `steps` responses for the 
            important cases of VAR and SARIMAX models), while for time-varying 
            models the impulse responses are only given for `steps` elements 
            (to avoid having to unexpectedly provide updated time-varying 
            matrices). 
 
        See Also 
        -------- 
        simulate 
            Simulate a time series according to the given state space model, 
            optionally with specified series for the innovations. 
 
        Notes 
        ----- 
        Intercepts in the measurement and state equation are ignored when 
        calculating impulse responses. 
        &quot;&quot;&quot;</span>
        <span class="s1">scale = self.scale </span><span class="s3">if </span><span class="s1">self.filter_results.filter_concentrated </span><span class="s3">else None</span>
        <span class="s3">with </span><span class="s1">self.model.ssm.fixed_scale(scale):</span>
            <span class="s1">irfs = self.model.impulse_responses(self.params</span><span class="s3">, </span><span class="s1">steps</span><span class="s3">, </span><span class="s1">impulse</span><span class="s3">,</span>
                                                <span class="s1">orthogonalized</span><span class="s3">, </span><span class="s1">cumulative</span><span class="s3">,</span>
                                                <span class="s1">**kwargs)</span>
            <span class="s0"># These are wrapped automatically, so just return the array</span>
            <span class="s3">if </span><span class="s1">isinstance(irfs</span><span class="s3">, </span><span class="s1">(pd.Series</span><span class="s3">, </span><span class="s1">pd.DataFrame)):</span>
                <span class="s1">irfs = irfs.values</span>
        <span class="s3">return </span><span class="s1">irfs</span>

    <span class="s3">def </span><span class="s1">_apply(self</span><span class="s3">, </span><span class="s1">mod</span><span class="s3">, </span><span class="s1">refit=</span><span class="s3">False, </span><span class="s1">fit_kwargs=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s3">if </span><span class="s1">fit_kwargs </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">fit_kwargs = {}</span>

        <span class="s3">if </span><span class="s1">refit:</span>
            <span class="s1">fit_kwargs.setdefault(</span><span class="s5">'start_params'</span><span class="s3">, </span><span class="s1">self.params)</span>
            <span class="s3">if </span><span class="s1">self._has_fixed_params:</span>
                <span class="s1">fit_kwargs.setdefault(</span><span class="s5">'includes_fixed'</span><span class="s3">, True</span><span class="s1">)</span>
                <span class="s1">res = mod.fit_constrained(self._fixed_params</span><span class="s3">, </span><span class="s1">**fit_kwargs)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">res = mod.fit(**fit_kwargs)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s5">'cov_type' </span><span class="s3">in </span><span class="s1">fit_kwargs:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot specify covariance type in'</span>
                                 <span class="s5">' `fit_kwargs` unless refitting'</span>
                                 <span class="s5">' parameters (not available in extend).'</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s5">'cov_kwds' </span><span class="s3">in </span><span class="s1">fit_kwargs:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot specify covariance keyword arguments'</span>
                                 <span class="s5">' in `fit_kwargs` unless refitting'</span>
                                 <span class="s5">' parameters (not available in extend).'</span><span class="s1">)</span>

            <span class="s3">if </span><span class="s1">self.cov_type == </span><span class="s5">'none'</span><span class="s1">:</span>
                <span class="s1">fit_kwargs[</span><span class="s5">'cov_type'</span><span class="s1">] = </span><span class="s5">'none'</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">fit_kwargs[</span><span class="s5">'cov_type'</span><span class="s1">] = </span><span class="s5">'custom'</span>
                <span class="s1">fit_kwargs[</span><span class="s5">'cov_kwds'</span><span class="s1">] = {</span>
                    <span class="s5">'custom_cov_type'</span><span class="s1">: self.cov_type</span><span class="s3">,</span>
                    <span class="s5">'custom_cov_params'</span><span class="s1">: self.cov_params_default</span><span class="s3">,</span>
                    <span class="s5">'custom_description'</span><span class="s1">: (</span><span class="s5">'Parameters and standard errors'</span>
                                           <span class="s5">' were estimated using a different'</span>
                                           <span class="s5">' dataset and were then applied to'</span>
                                           <span class="s5">' this dataset. %s'</span>
                                           <span class="s1">% self.cov_kwds[</span><span class="s5">'description'</span><span class="s1">])}</span>

            <span class="s3">if </span><span class="s1">self.smoother_results </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">func = mod.smooth</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">func = mod.filter</span>

            <span class="s3">if </span><span class="s1">self._has_fixed_params:</span>
                <span class="s3">with </span><span class="s1">mod.fix_params(self._fixed_params):</span>
                    <span class="s1">fit_kwargs.setdefault(</span><span class="s5">'includes_fixed'</span><span class="s3">, True</span><span class="s1">)</span>
                    <span class="s1">res = func(self.params</span><span class="s3">, </span><span class="s1">**fit_kwargs)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">res = func(self.params</span><span class="s3">, </span><span class="s1">**fit_kwargs)</span>

        <span class="s3">return </span><span class="s1">res</span>

    <span class="s3">def </span><span class="s1">_get_previous_updated(self</span><span class="s3">, </span><span class="s1">comparison</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None,</span>
                              <span class="s1">comparison_type=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s0"># If we were given data, create a new results object</span>
        <span class="s1">comparison_dataset = </span><span class="s3">not </span><span class="s1">isinstance(</span>
            <span class="s1">comparison</span><span class="s3">, </span><span class="s1">(MLEResults</span><span class="s3">, </span><span class="s1">MLEResultsWrapper))</span>
        <span class="s3">if </span><span class="s1">comparison_dataset:</span>
            <span class="s0"># If `exog` is longer than `comparison`, then we extend it to match</span>
            <span class="s1">nobs_endog = len(comparison)</span>
            <span class="s1">nobs_exog = len(exog) </span><span class="s3">if </span><span class="s1">exog </span><span class="s3">is not None else </span><span class="s1">nobs_endog</span>

            <span class="s3">if </span><span class="s1">nobs_exog &gt; nobs_endog:</span>
                <span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">ix = self.model._get_prediction_index(</span>
                    <span class="s1">start=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">end=nobs_exog - </span><span class="s4">1</span><span class="s1">)</span>
                <span class="s0"># TODO: check that the index of `comparison` matches the model</span>
                <span class="s1">comparison = np.asarray(comparison)</span>
                <span class="s3">if </span><span class="s1">comparison.ndim &lt; </span><span class="s4">2</span><span class="s1">:</span>
                    <span class="s1">comparison = np.atleast_2d(comparison).T</span>
                <span class="s3">if </span><span class="s1">(comparison.ndim != </span><span class="s4">2 </span><span class="s3">or</span>
                        <span class="s1">comparison.shape[</span><span class="s4">1</span><span class="s1">] != self.model.k_endog):</span>
                    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Invalid shape for `comparison`. Must'</span>
                                     <span class="s5">f' contain </span><span class="s3">{</span><span class="s1">self.model.k_endog</span><span class="s3">} </span><span class="s5">columns.'</span><span class="s1">)</span>
                <span class="s1">extra = np.zeros((nobs_exog - nobs_endog</span><span class="s3">,</span>
                                  <span class="s1">self.model.k_endog)) * np.nan</span>
                <span class="s1">comparison = pd.DataFrame(</span>
                    <span class="s1">np.concatenate([comparison</span><span class="s3">, </span><span class="s1">extra]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">index=ix</span><span class="s3">,</span>
                    <span class="s1">columns=self.model.endog_names)</span>

            <span class="s0"># Get the results object</span>
            <span class="s1">comparison = self.apply(comparison</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">,</span>
                                    <span class="s1">copy_initialization=</span><span class="s3">True, </span><span class="s1">**kwargs)</span>

        <span class="s0"># Now, figure out the `updated` versus `previous` results objects</span>
        <span class="s1">nmissing = self.filter_results.missing.sum()</span>
        <span class="s1">nmissing_comparison = comparison.filter_results.missing.sum()</span>
        <span class="s3">if </span><span class="s1">(comparison_type == </span><span class="s5">'updated' </span><span class="s3">or </span><span class="s1">(comparison_type </span><span class="s3">is None and </span><span class="s1">(</span>
                <span class="s1">comparison.nobs &gt; self.nobs </span><span class="s3">or</span>
                <span class="s1">(comparison.nobs == self.nobs </span><span class="s3">and</span>
                 <span class="s1">nmissing &gt; nmissing_comparison)))):</span>
            <span class="s1">updated = comparison</span>
            <span class="s1">previous = self</span>
        <span class="s3">elif </span><span class="s1">(comparison_type == </span><span class="s5">'previous' </span><span class="s3">or </span><span class="s1">(comparison_type </span><span class="s3">is None and </span><span class="s1">(</span>
                <span class="s1">comparison.nobs &lt; self.nobs </span><span class="s3">or</span>
                <span class="s1">(comparison.nobs == self.nobs </span><span class="s3">and</span>
                 <span class="s1">nmissing &lt; nmissing_comparison)))):</span>
            <span class="s1">updated = self</span>
            <span class="s1">previous = comparison</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Could not automatically determine the type'</span>
                             <span class="s5">' of comparison requested to compute the'</span>
                             <span class="s5">' News, so it must be specified as &quot;updated&quot;'</span>
                             <span class="s5">' or &quot;previous&quot;, using the `comparison_type`'</span>
                             <span class="s5">' keyword argument'</span><span class="s1">)</span>

        <span class="s0"># Check that the index of `updated` is a superset of the</span>
        <span class="s0"># index of `previous`</span>
        <span class="s0"># Note: the try/except block is for Pandas &lt; 0.25, in which</span>
        <span class="s0"># `PeriodIndex.difference` raises a ValueError if the argument is not</span>
        <span class="s0"># also a `PeriodIndex`.</span>
        <span class="s1">diff = previous.model._index.difference(updated.model._index)</span>
        <span class="s3">if </span><span class="s1">len(diff) &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'The index associated with the updated results is'</span>
                             <span class="s5">' not a superset of the index associated with the'</span>
                             <span class="s5">' previous results, and so these datasets do not'</span>
                             <span class="s5">' appear to be related. Can only compute the'</span>
                             <span class="s5">' news by comparing this results set to previous'</span>
                             <span class="s5">' results objects.'</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">previous</span><span class="s3">, </span><span class="s1">updated</span><span class="s3">, </span><span class="s1">comparison_dataset</span>

    <span class="s3">def </span><span class="s1">_news_previous_results(self</span><span class="s3">, </span><span class="s1">previous</span><span class="s3">, </span><span class="s1">start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">periods</span><span class="s3">,</span>
                               <span class="s1">state_index=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0"># Compute the news</span>
        <span class="s1">out = self.smoother_results.news(previous.smoother_results</span><span class="s3">,</span>
                                         <span class="s1">start=start</span><span class="s3">, </span><span class="s1">end=end</span><span class="s3">,</span>
                                         <span class="s1">state_index=state_index)</span>
        <span class="s3">return </span><span class="s1">out</span>

    <span class="s3">def </span><span class="s1">_news_updated_results(self</span><span class="s3">, </span><span class="s1">updated</span><span class="s3">, </span><span class="s1">start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">periods</span><span class="s3">,</span>
                              <span class="s1">state_index=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s3">return </span><span class="s1">updated._news_previous_results(self</span><span class="s3">, </span><span class="s1">start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">periods</span><span class="s3">,</span>
                                              <span class="s1">state_index=state_index)</span>

    <span class="s3">def </span><span class="s1">_news_previous_data(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">periods</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">,</span>
                            <span class="s1">state_index=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">previous = self.apply(endog</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">copy_initialization=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">self._news_previous_results(previous</span><span class="s3">, </span><span class="s1">start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">periods</span><span class="s3">,</span>
                                           <span class="s1">state_index=state_index)</span>

    <span class="s3">def </span><span class="s1">_news_updated_data(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">periods</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">,</span>
                           <span class="s1">state_index=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">updated = self.apply(endog</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">copy_initialization=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">self._news_updated_results(updated</span><span class="s3">, </span><span class="s1">start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">periods</span><span class="s3">,</span>
                                          <span class="s1">state_index=state_index)</span>

    <span class="s3">def </span><span class="s1">news(self</span><span class="s3">, </span><span class="s1">comparison</span><span class="s3">, </span><span class="s1">impact_date=</span><span class="s3">None, </span><span class="s1">impacted_variable=</span><span class="s3">None,</span>
             <span class="s1">start=</span><span class="s3">None, </span><span class="s1">end=</span><span class="s3">None, </span><span class="s1">periods=</span><span class="s3">None, </span><span class="s1">exog=</span><span class="s3">None,</span>
             <span class="s1">comparison_type=</span><span class="s3">None, </span><span class="s1">state_index=</span><span class="s3">None, </span><span class="s1">return_raw=</span><span class="s3">False,</span>
             <span class="s1">tolerance=</span><span class="s4">1e-10</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Compute impacts from updated data (news and revisions) 
 
        Parameters 
        ---------- 
        comparison : array_like or MLEResults 
            An updated dataset with updated and/or revised data from which the 
            news can be computed, or an updated or previous results object 
            to use in computing the news. 
        impact_date : int, str, or datetime, optional 
            A single specific period of impacts from news and revisions to 
            compute. Can also be a date string to parse or a datetime type. 
            This argument cannot be used in combination with `start`, `end`, or 
            `periods`. Default is the first out-of-sample observation. 
        impacted_variable : str, list, array, or slice, optional 
            Observation variable label or slice of labels specifying that only 
            specific impacted variables should be shown in the News output. The 
            impacted variable(s) describe the variables that were *affected* by 
            the news. If you do not know the labels for the variables, check 
            the `endog_names` attribute of the model instance. 
        start : int, str, or datetime, optional 
            The first period of impacts from news and revisions to compute. 
            Can also be a date string to parse or a datetime type. Default is 
            the first out-of-sample observation. 
        end : int, str, or datetime, optional 
            The last period of impacts from news and revisions to compute. 
            Can also be a date string to parse or a datetime type. Default is 
            the first out-of-sample observation. 
        periods : int, optional 
            The number of periods of impacts from news and revisions to 
            compute. 
        exog : array_like, optional 
            Array of exogenous regressors for the out-of-sample period, if 
            applicable. 
        comparison_type : {None, 'previous', 'updated'} 
            This denotes whether the `comparison` argument represents a 
            *previous* results object or dataset or an *updated* results object 
            or dataset. If not specified, then an attempt is made to determine 
            the comparison type. 
        state_index : array_like, optional 
            An optional index specifying a subset of states to use when 
            constructing the impacts of revisions and news. For example, if 
            `state_index=[0, 1]` is passed, then only the impacts to the 
            observed variables arising from the impacts to the first two 
            states will be returned. Default is to use all states. 
        return_raw : bool, optional 
            Whether or not to return only the specific output or a full 
            results object. Default is to return a full results object. 
        tolerance : float, optional 
            The numerical threshold for determining zero impact. Default is 
            that any impact less than 1e-10 is assumed to be zero. 
 
        Returns 
        ------- 
        NewsResults 
            Impacts of data revisions and news on estimates 
 
        References 
        ---------- 
        .. [1] Bańbura, Marta, and Michele Modugno. 
               &quot;Maximum likelihood estimation of factor models on datasets with 
               arbitrary pattern of missing data.&quot; 
               Journal of Applied Econometrics 29, no. 1 (2014): 133-160. 
        .. [2] Bańbura, Marta, Domenico Giannone, and Lucrezia Reichlin. 
               &quot;Nowcasting.&quot; 
               The Oxford Handbook of Economic Forecasting. July 8, 2011. 
        .. [3] Bańbura, Marta, Domenico Giannone, Michele Modugno, and Lucrezia 
               Reichlin. 
               &quot;Now-casting and the real-time data flow.&quot; 
               In Handbook of economic forecasting, vol. 2, pp. 195-237. 
               Elsevier, 2013. 
        &quot;&quot;&quot;</span>
        <span class="s0"># Validate input</span>
        <span class="s3">if </span><span class="s1">self.smoother_results </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot compute news without Kalman smoother'</span>
                             <span class="s5">' results.'</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">state_index </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">state_index = np.sort(np.array(state_index</span><span class="s3">, </span><span class="s1">dtype=int))</span>
            <span class="s3">if </span><span class="s1">state_index[</span><span class="s4">0</span><span class="s1">] &lt; </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'Cannot include negative indexes in'</span>
                                 <span class="s5">' `state_index`.'</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">state_index[-</span><span class="s4">1</span><span class="s1">] &gt;= self.model.k_states:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">f'Given state index </span><span class="s3">{</span><span class="s1">state_index[-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">} </span><span class="s5">is too'</span>
                                 <span class="s5">' large for the number of states in the model'</span>
                                 <span class="s5">f' (</span><span class="s3">{</span><span class="s1">self.model.k_states</span><span class="s3">}</span><span class="s5">).'</span><span class="s1">)</span>

        <span class="s0"># Get the previous and updated results objects from `self` and</span>
        <span class="s0"># `comparison`:</span>
        <span class="s1">previous</span><span class="s3">, </span><span class="s1">updated</span><span class="s3">, </span><span class="s1">comparison_dataset = self._get_previous_updated(</span>
            <span class="s1">comparison</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">comparison_type=comparison_type</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s0"># Handle start, end, periods</span>
        <span class="s1">start</span><span class="s3">, </span><span class="s1">end</span><span class="s3">, </span><span class="s1">prediction_index = get_impact_dates(</span>
            <span class="s1">previous_model=previous.model</span><span class="s3">, </span><span class="s1">updated_model=updated.model</span><span class="s3">,</span>
            <span class="s1">impact_date=impact_date</span><span class="s3">, </span><span class="s1">start=start</span><span class="s3">, </span><span class="s1">end=end</span><span class="s3">, </span><span class="s1">periods=periods)</span>

        <span class="s0"># News results will always use Pandas, so if the model's data was not</span>
        <span class="s0"># from Pandas, we'll create an index, as if the model's data had been</span>
        <span class="s0"># given a default Pandas index.</span>
        <span class="s3">if </span><span class="s1">prediction_index </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">prediction_index = pd.RangeIndex(start=start</span><span class="s3">, </span><span class="s1">stop=end + </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s0"># For time-varying models try to create an appended `updated` model</span>
        <span class="s0"># with NaN values. Do not extend the model if this was already done</span>
        <span class="s0"># above (i.e. the case that `comparison` was a new dataset), because</span>
        <span class="s0"># in that case `exog` and `kwargs` should have</span>
        <span class="s0"># been set with the input `comparison` dataset in mind, and so would be</span>
        <span class="s0"># useless here. Ultimately, we've already extended `updated` as far</span>
        <span class="s0"># as we can. So raise an  exception in that case with a useful message.</span>
        <span class="s0"># However, we still want to try to accommodate extending the model here</span>
        <span class="s0"># if it is possible.</span>
        <span class="s0"># Note that we do not need to extend time-invariant models, because</span>
        <span class="s0"># `KalmanSmoother.news` can itself handle any impact dates for</span>
        <span class="s0"># time-invariant models.</span>
        <span class="s1">time_varying = </span><span class="s3">not </span><span class="s1">(previous.filter_results.time_invariant </span><span class="s3">or</span>
                            <span class="s1">updated.filter_results.time_invariant)</span>
        <span class="s3">if </span><span class="s1">time_varying </span><span class="s3">and </span><span class="s1">end &gt;= updated.nobs:</span>
            <span class="s0"># If we the given `comparison` was a dataset and either `exog` or</span>
            <span class="s0"># `kwargs` was set, then we assume that we cannot create an updated</span>
            <span class="s0"># time-varying model (because then we can't tell if `kwargs` and</span>
            <span class="s0"># `exog` arguments are meant to apply to the `comparison` dataset</span>
            <span class="s0"># or to this extension)</span>
            <span class="s3">if </span><span class="s1">comparison_dataset </span><span class="s3">and </span><span class="s1">(exog </span><span class="s3">is not None or </span><span class="s1">len(kwargs) &gt; </span><span class="s4">0</span><span class="s1">):</span>
                <span class="s3">if </span><span class="s1">comparison </span><span class="s3">is </span><span class="s1">updated:</span>
                    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'If providing an updated dataset as the'</span>
                                     <span class="s5">' `comparison` with a time-varying model,'</span>
                                     <span class="s5">' then the `end` period cannot be beyond'</span>
                                     <span class="s5">' the end of that updated dataset.'</span><span class="s1">)</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'If providing an previous dataset as the'</span>
                                     <span class="s5">' `comparison` with a time-varying model,'</span>
                                     <span class="s5">' then the `end` period cannot be beyond'</span>
                                     <span class="s5">' the end of the (updated) results'</span>
                                     <span class="s5">' object.'</span><span class="s1">)</span>

            <span class="s0"># Try to extend `updated`</span>
            <span class="s1">updated_orig = updated</span>
            <span class="s0"># TODO: `append` should fix this k_endog=1 issue for us</span>
            <span class="s0"># TODO: is the + 1 necessary?</span>
            <span class="s3">if </span><span class="s1">self.model.k_endog &gt; </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">extra = np.zeros((end - updated.nobs + </span><span class="s4">1</span><span class="s3">,</span>
                                  <span class="s1">self.model.k_endog)) * np.nan</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">extra = np.zeros((end - updated.nobs + </span><span class="s4">1</span><span class="s3">,</span><span class="s1">)) * np.nan</span>
            <span class="s1">updated = updated_orig.append(extra</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s0"># Compute the news</span>
        <span class="s1">news_results = (</span>
            <span class="s1">updated._news_previous_results(previous</span><span class="s3">, </span><span class="s1">start</span><span class="s3">, </span><span class="s1">end + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">periods</span><span class="s3">,</span>
                                           <span class="s1">state_index=state_index))</span>

        <span class="s3">if not </span><span class="s1">return_raw:</span>
            <span class="s1">news_results = NewsResults(</span>
                <span class="s1">news_results</span><span class="s3">, </span><span class="s1">self</span><span class="s3">, </span><span class="s1">updated</span><span class="s3">, </span><span class="s1">previous</span><span class="s3">, </span><span class="s1">impacted_variable</span><span class="s3">,</span>
                <span class="s1">tolerance</span><span class="s3">, </span><span class="s1">row_labels=prediction_index)</span>
        <span class="s3">return </span><span class="s1">news_results</span>

    <span class="s3">def </span><span class="s1">get_smoothed_decomposition(self</span><span class="s3">, </span><span class="s1">decomposition_of=</span><span class="s5">'smoothed_state'</span><span class="s3">,</span>
                                   <span class="s1">state_index=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">r&quot;&quot;&quot; 
        Decompose smoothed output into contributions from observations 
 
        Parameters 
        ---------- 
        decomposition_of : {&quot;smoothed_state&quot;, &quot;smoothed_signal&quot;} 
            The object to perform a decomposition of. If it is set to 
            &quot;smoothed_state&quot;, then the elements of the smoothed state vector 
            are decomposed into the contributions of each observation. If it 
            is set to &quot;smoothed_signal&quot;, then the predictions of the 
            observation vector based on the smoothed state vector are 
            decomposed. Default is &quot;smoothed_state&quot;. 
        state_index : array_like, optional 
            An optional index specifying a subset of states to use when 
            constructing the decomposition of the &quot;smoothed_signal&quot;. For 
            example, if `state_index=[0, 1]` is passed, then only the 
            contributions of observed variables to the smoothed signal arising 
            from the first two states will be returned. Note that if not all 
            states are used, the contributions will not sum to the smoothed 
            signal. Default is to use all states. 
 
        Returns 
        ------- 
        data_contributions : pd.DataFrame 
            Contributions of observations to the decomposed object. If the 
            smoothed state is being decomposed, then `data_contributions` is 
            shaped `(k_states x nobs, k_endog x nobs)` with a `pd.MultiIndex` 
            index corresponding to `state_to x date_to` and `pd.MultiIndex` 
            columns corresponding to `variable_from x date_from`. If the 
            smoothed signal is being decomposed, then `data_contributions` is 
            shaped `(k_endog x nobs, k_endog x nobs)` with `pd.MultiIndex`-es 
            corresponding to `variable_to x date_to` and 
            `variable_from x date_from`. 
        obs_intercept_contributions : pd.DataFrame 
            Contributions of the observation intercept to the decomposed 
            object. If the smoothed state is being decomposed, then 
            `obs_intercept_contributions` is 
            shaped `(k_states x nobs, k_endog x nobs)` with a `pd.MultiIndex` 
            index corresponding to `state_to x date_to` and `pd.MultiIndex` 
            columns corresponding to `obs_intercept_from x date_from`. If the 
            smoothed signal is being decomposed, then 
            `obs_intercept_contributions` is shaped 
            `(k_endog x nobs, k_endog x nobs)` with `pd.MultiIndex`-es 
            corresponding to `variable_to x date_to` and 
            `obs_intercept_from x date_from`. 
        state_intercept_contributions : pd.DataFrame 
            Contributions of the state intercept to the decomposed 
            object. If the smoothed state is being decomposed, then 
            `state_intercept_contributions` is 
            shaped `(k_states x nobs, k_states x nobs)` with a `pd.MultiIndex` 
            index corresponding to `state_to x date_to` and `pd.MultiIndex` 
            columns corresponding to `state_intercept_from x date_from`. If the 
            smoothed signal is being decomposed, then 
            `state_intercept_contributions` is shaped 
            `(k_endog x nobs, k_states x nobs)` with `pd.MultiIndex`-es 
            corresponding to `variable_to x date_to` and 
            `state_intercept_from x date_from`. 
        prior_contributions : pd.DataFrame 
            Contributions of the prior to the decomposed object. If the 
            smoothed state is being decomposed, then `prior_contributions` is 
            shaped `(nobs x k_states, k_states)`, with a `pd.MultiIndex` 
            index corresponding to `state_to x date_to` and columns 
            corresponding to elements of the prior mean (aka &quot;initial state&quot;). 
            If the smoothed signal is being decomposed, then 
            `prior_contributions` is shaped `(nobs x k_endog, k_states)`, 
            with a `pd.MultiIndex` index corresponding to 
            `variable_to x date_to` and columns corresponding to elements of 
            the prior mean. 
 
        Notes 
        ----- 
        Denote the smoothed state at time :math:`t` by :math:`\alpha_t`. Then 
        the smoothed signal is :math:`Z_t \alpha_t`, where :math:`Z_t` is the 
        design matrix operative at time :math:`t`. 
        &quot;&quot;&quot;</span>
        <span class="s1">(data_contributions</span><span class="s3">, </span><span class="s1">obs_intercept_contributions</span><span class="s3">,</span>
         <span class="s1">state_intercept_contributions</span><span class="s3">, </span><span class="s1">prior_contributions) = (</span>
            <span class="s1">self.smoother_results.get_smoothed_decomposition(</span>
                <span class="s1">decomposition_of=decomposition_of</span><span class="s3">, </span><span class="s1">state_index=state_index))</span>

        <span class="s0"># Construct indexes</span>
        <span class="s1">endog_names = self.model.endog_names</span>
        <span class="s3">if </span><span class="s1">self.model.k_endog == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">endog_names = [endog_names]</span>

        <span class="s3">if </span><span class="s1">decomposition_of == </span><span class="s5">'smoothed_state'</span><span class="s1">:</span>
            <span class="s1">contributions_to = pd.MultiIndex.from_product(</span>
                <span class="s1">[self.model.state_names</span><span class="s3">, </span><span class="s1">self.model._index]</span><span class="s3">,</span>
                <span class="s1">names=[</span><span class="s5">'state_to'</span><span class="s3">, </span><span class="s5">'date_to'</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">contributions_to = pd.MultiIndex.from_product(</span>
                <span class="s1">[endog_names</span><span class="s3">, </span><span class="s1">self.model._index]</span><span class="s3">,</span>
                <span class="s1">names=[</span><span class="s5">'variable_to'</span><span class="s3">, </span><span class="s5">'date_to'</span><span class="s1">])</span>
        <span class="s1">contributions_from = pd.MultiIndex.from_product(</span>
            <span class="s1">[endog_names</span><span class="s3">, </span><span class="s1">self.model._index]</span><span class="s3">,</span>
            <span class="s1">names=[</span><span class="s5">'variable_from'</span><span class="s3">, </span><span class="s5">'date_from'</span><span class="s1">])</span>
        <span class="s1">obs_intercept_contributions_from = pd.MultiIndex.from_product(</span>
            <span class="s1">[endog_names</span><span class="s3">, </span><span class="s1">self.model._index]</span><span class="s3">,</span>
            <span class="s1">names=[</span><span class="s5">'obs_intercept_from'</span><span class="s3">, </span><span class="s5">'date_from'</span><span class="s1">])</span>
        <span class="s1">state_intercept_contributions_from = pd.MultiIndex.from_product(</span>
            <span class="s1">[self.model.state_names</span><span class="s3">, </span><span class="s1">self.model._index]</span><span class="s3">,</span>
            <span class="s1">names=[</span><span class="s5">'state_intercept_from'</span><span class="s3">, </span><span class="s5">'date_from'</span><span class="s1">])</span>
        <span class="s1">prior_contributions_from = pd.Index(self.model.state_names</span><span class="s3">,</span>
                                            <span class="s1">name=</span><span class="s5">'initial_state_from'</span><span class="s1">)</span>

        <span class="s0"># Construct DataFrames</span>
        <span class="s1">shape = data_contributions.shape</span>
        <span class="s1">data_contributions = pd.DataFrame(</span>
            <span class="s1">data_contributions.reshape(</span>
                <span class="s1">shape[</span><span class="s4">0</span><span class="s1">] * shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">shape[</span><span class="s4">2</span><span class="s1">] * shape[</span><span class="s4">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">order=</span><span class="s5">'F'</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">index=contributions_to</span><span class="s3">, </span><span class="s1">columns=contributions_from)</span>

        <span class="s1">shape = obs_intercept_contributions.shape</span>
        <span class="s1">obs_intercept_contributions = pd.DataFrame(</span>
            <span class="s1">obs_intercept_contributions.reshape(</span>
                <span class="s1">shape[</span><span class="s4">0</span><span class="s1">] * shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">shape[</span><span class="s4">2</span><span class="s1">] * shape[</span><span class="s4">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">order=</span><span class="s5">'F'</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">index=contributions_to</span><span class="s3">, </span><span class="s1">columns=obs_intercept_contributions_from)</span>

        <span class="s1">shape = state_intercept_contributions.shape</span>
        <span class="s1">state_intercept_contributions = pd.DataFrame(</span>
            <span class="s1">state_intercept_contributions.reshape(</span>
                <span class="s1">shape[</span><span class="s4">0</span><span class="s1">] * shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">shape[</span><span class="s4">2</span><span class="s1">] * shape[</span><span class="s4">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">order=</span><span class="s5">'F'</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">index=contributions_to</span><span class="s3">, </span><span class="s1">columns=state_intercept_contributions_from)</span>

        <span class="s1">shape = prior_contributions.shape</span>
        <span class="s1">prior_contributions = pd.DataFrame(</span>
            <span class="s1">prior_contributions.reshape(shape[</span><span class="s4">0</span><span class="s1">] * shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">shape[</span><span class="s4">2</span><span class="s1">]</span><span class="s3">,</span>
                                        <span class="s1">order=</span><span class="s5">'F'</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">index=contributions_to</span><span class="s3">, </span><span class="s1">columns=prior_contributions_from)</span>

        <span class="s3">return </span><span class="s1">(data_contributions</span><span class="s3">, </span><span class="s1">obs_intercept_contributions</span><span class="s3">,</span>
                <span class="s1">state_intercept_contributions</span><span class="s3">, </span><span class="s1">prior_contributions)</span>

    <span class="s3">def </span><span class="s1">append(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">refit=</span><span class="s3">False, </span><span class="s1">fit_kwargs=</span><span class="s3">None,</span>
               <span class="s1">copy_initialization=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Recreate the results object with new data appended to the original data 
 
        Creates a new result object applied to a dataset that is created by 
        appending new data to the end of the model's original data. The new 
        results can then be used for analysis or forecasting. 
 
        Parameters 
        ---------- 
        endog : array_like 
            New observations from the modeled time-series process. 
        exog : array_like, optional 
            New observations of exogenous regressors, if applicable. 
        refit : bool, optional 
            Whether to re-fit the parameters, based on the combined dataset. 
            Default is False (so parameters from the current results object 
            are used to create the new results object). 
        copy_initialization : bool, optional 
            Whether or not to copy the initialization from the current results 
            set to the new model. Default is False 
        fit_kwargs : dict, optional 
            Keyword arguments to pass to `fit` (if `refit=True`) or `filter` / 
            `smooth`. 
        copy_initialization : bool, optional 
        **kwargs 
            Keyword arguments may be used to modify model specification 
            arguments when created the new model object. 
 
        Returns 
        ------- 
        results 
            Updated Results object, that includes results from both the 
            original dataset and the new dataset. 
 
        Notes 
        ----- 
        The `endog` and `exog` arguments to this method must be formatted in 
        the same way (e.g. Pandas Series versus Numpy array) as were the 
        `endog` and `exog` arrays passed to the original model. 
 
        The `endog` argument to this method should consist of new observations 
        that occurred directly after the last element of `endog`. For any other 
        kind of dataset, see the `apply` method. 
 
        This method will apply filtering to all of the original data as well 
        as to the new data. To apply filtering only to the new data (which 
        can be much faster if the original dataset is large), see the `extend` 
        method. 
 
        See Also 
        -------- 
        statsmodels.tsa.statespace.mlemodel.MLEResults.extend 
        statsmodels.tsa.statespace.mlemodel.MLEResults.apply 
 
        Examples 
        -------- 
        &gt;&gt;&gt; index = pd.period_range(start='2000', periods=2, freq='A') 
        &gt;&gt;&gt; original_observations = pd.Series([1.2, 1.5], index=index) 
        &gt;&gt;&gt; mod = sm.tsa.SARIMAX(original_observations) 
        &gt;&gt;&gt; res = mod.fit() 
        &gt;&gt;&gt; print(res.params) 
        ar.L1     0.9756 
        sigma2    0.0889 
        dtype: float64 
        &gt;&gt;&gt; print(res.fittedvalues) 
        2000    0.0000 
        2001    1.1707 
        Freq: A-DEC, dtype: float64 
        &gt;&gt;&gt; print(res.forecast(1)) 
        2002    1.4634 
        Freq: A-DEC, dtype: float64 
 
        &gt;&gt;&gt; new_index = pd.period_range(start='2002', periods=1, freq='A') 
        &gt;&gt;&gt; new_observations = pd.Series([0.9], index=new_index) 
        &gt;&gt;&gt; updated_res = res.append(new_observations) 
        &gt;&gt;&gt; print(updated_res.params) 
        ar.L1     0.9756 
        sigma2    0.0889 
        dtype: float64 
        &gt;&gt;&gt; print(updated_res.fittedvalues) 
        2000    0.0000 
        2001    1.1707 
        2002    1.4634 
        Freq: A-DEC, dtype: float64 
        &gt;&gt;&gt; print(updated_res.forecast(1)) 
        2003    0.878 
        Freq: A-DEC, dtype: float64 
        &quot;&quot;&quot;</span>
        <span class="s1">start = self.nobs</span>
        <span class="s1">end = self.nobs + len(endog) - </span><span class="s4">1</span>
        <span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">append_ix = self.model._get_prediction_index(start</span><span class="s3">, </span><span class="s1">end)</span>

        <span class="s0"># Check the index of the new data</span>
        <span class="s3">if </span><span class="s1">isinstance(self.model.data</span><span class="s3">, </span><span class="s1">PandasData):</span>
            <span class="s1">_check_index(append_ix</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s5">'`endog`'</span><span class="s1">)</span>

        <span class="s0"># Concatenate the new data to original data</span>
        <span class="s1">new_endog = concat([self.model.data.orig_endog</span><span class="s3">, </span><span class="s1">endog]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s3">,</span>
                           <span class="s1">allow_mix=</span><span class="s3">True</span><span class="s1">)</span>

        <span class="s0"># Handle `exog`</span>
        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">_</span><span class="s3">, </span><span class="s1">exog = prepare_exog(exog)</span>
            <span class="s1">_check_index(append_ix</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s5">'`exog`'</span><span class="s1">)</span>

            <span class="s1">new_exog = concat([self.model.data.orig_exog</span><span class="s3">, </span><span class="s1">exog]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s3">,</span>
                              <span class="s1">allow_mix=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">new_exog = </span><span class="s3">None</span>

        <span class="s0"># Create a continuous index for the combined data</span>
        <span class="s3">if </span><span class="s1">isinstance(self.model.data</span><span class="s3">, </span><span class="s1">PandasData):</span>
            <span class="s1">start = </span><span class="s4">0</span>
            <span class="s1">end = len(new_endog) - </span><span class="s4">1</span>
            <span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">new_index = self.model._get_prediction_index(start</span><span class="s3">, </span><span class="s1">end)</span>

            <span class="s0"># Standardize `endog` to have the right index and columns</span>
            <span class="s1">columns = self.model.endog_names</span>
            <span class="s3">if not </span><span class="s1">isinstance(columns</span><span class="s3">, </span><span class="s1">list):</span>
                <span class="s1">columns = [columns]</span>
            <span class="s1">new_endog = pd.DataFrame(new_endog</span><span class="s3">, </span><span class="s1">index=new_index</span><span class="s3">,</span>
                                     <span class="s1">columns=columns)</span>

            <span class="s0"># Standardize `exog` to have the right index</span>
            <span class="s3">if </span><span class="s1">new_exog </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">new_exog = pd.DataFrame(new_exog</span><span class="s3">, </span><span class="s1">index=new_index</span><span class="s3">,</span>
                                        <span class="s1">columns=self.model.exog_names)</span>

        <span class="s3">if </span><span class="s1">copy_initialization:</span>
            <span class="s1">init = Initialization.from_results(self.filter_results)</span>
            <span class="s1">kwargs.setdefault(</span><span class="s5">'initialization'</span><span class="s3">, </span><span class="s1">init)</span>

        <span class="s1">mod = self.model.clone(new_endog</span><span class="s3">, </span><span class="s1">exog=new_exog</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s1">res = self._apply(mod</span><span class="s3">, </span><span class="s1">refit=refit</span><span class="s3">, </span><span class="s1">fit_kwargs=fit_kwargs)</span>

        <span class="s3">return </span><span class="s1">res</span>

    <span class="s3">def </span><span class="s1">extend(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">fit_kwargs=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Recreate the results object for new data that extends the original data 
 
        Creates a new result object applied to a new dataset that is assumed to 
        follow directly from the end of the model's original data. The new 
        results can then be used for analysis or forecasting. 
 
        Parameters 
        ---------- 
        endog : array_like 
            New observations from the modeled time-series process. 
        exog : array_like, optional 
            New observations of exogenous regressors, if applicable. 
        fit_kwargs : dict, optional 
            Keyword arguments to pass to `filter` or `smooth`. 
        **kwargs 
            Keyword arguments may be used to modify model specification 
            arguments when created the new model object. 
 
        Returns 
        ------- 
        results 
            Updated Results object, that includes results only for the new 
            dataset. 
 
        See Also 
        -------- 
        statsmodels.tsa.statespace.mlemodel.MLEResults.append 
        statsmodels.tsa.statespace.mlemodel.MLEResults.apply 
 
        Notes 
        ----- 
        The `endog` argument to this method should consist of new observations 
        that occurred directly after the last element of the model's original 
        `endog` array. For any other kind of dataset, see the `apply` method. 
 
        This method will apply filtering only to the new data provided by the 
        `endog` argument, which can be much faster than re-filtering the entire 
        dataset. However, the returned results object will only have results 
        for the new data. To retrieve results for both the new data and the 
        original data, see the `append` method. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; index = pd.period_range(start='2000', periods=2, freq='A') 
        &gt;&gt;&gt; original_observations = pd.Series([1.2, 1.5], index=index) 
        &gt;&gt;&gt; mod = sm.tsa.SARIMAX(original_observations) 
        &gt;&gt;&gt; res = mod.fit() 
        &gt;&gt;&gt; print(res.params) 
        ar.L1     0.9756 
        sigma2    0.0889 
        dtype: float64 
        &gt;&gt;&gt; print(res.fittedvalues) 
        2000    0.0000 
        2001    1.1707 
        Freq: A-DEC, dtype: float64 
        &gt;&gt;&gt; print(res.forecast(1)) 
        2002    1.4634 
        Freq: A-DEC, dtype: float64 
 
        &gt;&gt;&gt; new_index = pd.period_range(start='2002', periods=1, freq='A') 
        &gt;&gt;&gt; new_observations = pd.Series([0.9], index=new_index) 
        &gt;&gt;&gt; updated_res = res.extend(new_observations) 
        &gt;&gt;&gt; print(updated_res.params) 
        ar.L1     0.9756 
        sigma2    0.0889 
        dtype: float64 
        &gt;&gt;&gt; print(updated_res.fittedvalues) 
        2002    1.4634 
        Freq: A-DEC, dtype: float64 
        &gt;&gt;&gt; print(updated_res.forecast(1)) 
        2003    0.878 
        Freq: A-DEC, dtype: float64 
        &quot;&quot;&quot;</span>
        <span class="s1">start = self.nobs</span>
        <span class="s1">end = self.nobs + len(endog) - </span><span class="s4">1</span>
        <span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">extend_ix = self.model._get_prediction_index(start</span><span class="s3">, </span><span class="s1">end)</span>

        <span class="s3">if </span><span class="s1">isinstance(self.model.data</span><span class="s3">, </span><span class="s1">PandasData):</span>
            <span class="s1">_check_index(extend_ix</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s5">'`endog`'</span><span class="s1">)</span>

            <span class="s0"># Standardize `endog` to have the right index and columns</span>
            <span class="s1">columns = self.model.endog_names</span>
            <span class="s3">if not </span><span class="s1">isinstance(columns</span><span class="s3">, </span><span class="s1">list):</span>
                <span class="s1">columns = [columns]</span>
            <span class="s1">endog = pd.DataFrame(endog</span><span class="s3">, </span><span class="s1">index=extend_ix</span><span class="s3">, </span><span class="s1">columns=columns)</span>
        <span class="s0"># Extend the current fit result to additional data</span>
        <span class="s1">mod = self.model.clone(endog</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s1">mod.ssm.initialization = Initialization(</span>
            <span class="s1">mod.k_states</span><span class="s3">, </span><span class="s5">'known'</span><span class="s3">, </span><span class="s1">constant=self.predicted_state[...</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">stationary_cov=self.predicted_state_cov[...</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s1">res = self._apply(mod</span><span class="s3">, </span><span class="s1">refit=</span><span class="s3">False, </span><span class="s1">fit_kwargs=fit_kwargs)</span>

        <span class="s3">return </span><span class="s1">res</span>

    <span class="s3">def </span><span class="s1">apply(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">refit=</span><span class="s3">False, </span><span class="s1">fit_kwargs=</span><span class="s3">None,</span>
              <span class="s1">copy_initialization=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Apply the fitted parameters to new data unrelated to the original data 
 
        Creates a new result object using the current fitted parameters, 
        applied to a completely new dataset that is assumed to be unrelated to 
        the model's original data. The new results can then be used for 
        analysis or forecasting. 
 
        Parameters 
        ---------- 
        endog : array_like 
            New observations from the modeled time-series process. 
        exog : array_like, optional 
            New observations of exogenous regressors, if applicable. 
        refit : bool, optional 
            Whether to re-fit the parameters, using the new dataset. 
            Default is False (so parameters from the current results object 
            are used to create the new results object). 
        copy_initialization : bool, optional 
            Whether or not to copy the initialization from the current results 
            set to the new model. Default is False 
        fit_kwargs : dict, optional 
            Keyword arguments to pass to `fit` (if `refit=True`) or `filter` / 
            `smooth`. 
        **kwargs 
            Keyword arguments may be used to modify model specification 
            arguments when created the new model object. 
 
        Returns 
        ------- 
        results 
            Updated Results object, that includes results only for the new 
            dataset. 
 
        See Also 
        -------- 
        statsmodels.tsa.statespace.mlemodel.MLEResults.append 
        statsmodels.tsa.statespace.mlemodel.MLEResults.apply 
 
        Notes 
        ----- 
        The `endog` argument to this method should consist of new observations 
        that are not necessarily related to the original model's `endog` 
        dataset. For observations that continue that original dataset by follow 
        directly after its last element, see the `append` and `extend` methods. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; index = pd.period_range(start='2000', periods=2, freq='A') 
        &gt;&gt;&gt; original_observations = pd.Series([1.2, 1.5], index=index) 
        &gt;&gt;&gt; mod = sm.tsa.SARIMAX(original_observations) 
        &gt;&gt;&gt; res = mod.fit() 
        &gt;&gt;&gt; print(res.params) 
        ar.L1     0.9756 
        sigma2    0.0889 
        dtype: float64 
        &gt;&gt;&gt; print(res.fittedvalues) 
        2000    0.0000 
        2001    1.1707 
        Freq: A-DEC, dtype: float64 
        &gt;&gt;&gt; print(res.forecast(1)) 
        2002    1.4634 
        Freq: A-DEC, dtype: float64 
 
        &gt;&gt;&gt; new_index = pd.period_range(start='1980', periods=3, freq='A') 
        &gt;&gt;&gt; new_observations = pd.Series([1.4, 0.3, 1.2], index=new_index) 
        &gt;&gt;&gt; new_res = res.apply(new_observations) 
        &gt;&gt;&gt; print(new_res.params) 
        ar.L1     0.9756 
        sigma2    0.0889 
        dtype: float64 
        &gt;&gt;&gt; print(new_res.fittedvalues) 
        1980    1.1707 
        1981    1.3659 
        1982    0.2927 
        Freq: A-DEC, dtype: float64 
        Freq: A-DEC, dtype: float64 
        &gt;&gt;&gt; print(new_res.forecast(1)) 
        1983    1.1707 
        Freq: A-DEC, dtype: float64 
        &quot;&quot;&quot;</span>
        <span class="s1">mod = self.model.clone(endog</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s3">if </span><span class="s1">copy_initialization:</span>
            <span class="s1">init = Initialization.from_results(self.filter_results)</span>
            <span class="s1">mod.ssm.initialization = init</span>

        <span class="s1">res = self._apply(mod</span><span class="s3">, </span><span class="s1">refit=refit</span><span class="s3">, </span><span class="s1">fit_kwargs=fit_kwargs)</span>

        <span class="s3">return </span><span class="s1">res</span>

    <span class="s3">def </span><span class="s1">plot_diagnostics(self</span><span class="s3">, </span><span class="s1">variable=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">lags=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">fig=</span><span class="s3">None, </span><span class="s1">figsize=</span><span class="s3">None,</span>
                         <span class="s1">truncate_endog_names=</span><span class="s4">24</span><span class="s3">, </span><span class="s1">auto_ylims=</span><span class="s3">False,</span>
                         <span class="s1">bartlett_confint=</span><span class="s3">False, </span><span class="s1">acf_kwargs=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Diagnostic plots for standardized residuals of one endogenous variable 
 
        Parameters 
        ---------- 
        variable : int, optional 
            Index of the endogenous variable for which the diagnostic plots 
            should be created. Default is 0. 
        lags : int, optional 
            Number of lags to include in the correlogram. Default is 10. 
        fig : Figure, optional 
            If given, subplots are created in this figure instead of in a new 
            figure. Note that the 2x2 grid will be created in the provided 
            figure using `fig.add_subplot()`. 
        figsize : tuple, optional 
            If a figure is created, this argument allows specifying a size. 
            The tuple is (width, height). 
        auto_ylims : bool, optional 
            If True, adjusts automatically the y-axis limits to ACF values. 
        bartlett_confint : bool, default True 
            Confidence intervals for ACF values are generally placed at 2 
            standard errors around r_k. The formula used for standard error 
            depends upon the situation. If the autocorrelations are being used 
            to test for randomness of residuals as part of the ARIMA routine, 
            the standard errors are determined assuming the residuals are white 
            noise. The approximate formula for any lag is that standard error 
            of each r_k = 1/sqrt(N). See section 9.4 of [1] for more details on 
            the 1/sqrt(N) result. For more elementary discussion, see section 
            5.3.2 in [2]. 
            For the ACF of raw data, the standard error at a lag k is 
            found as if the right model was an MA(k-1). This allows the 
            possible interpretation that if all autocorrelations past a 
            certain lag are within the limits, the model might be an MA of 
            order defined by the last significant autocorrelation. In this 
            case, a moving average model is assumed for the data and the 
            standard errors for the confidence intervals should be 
            generated using Bartlett's formula. For more details on 
            Bartlett formula result, see section 7.2 in [1].+ 
        acf_kwargs : dict, optional 
            Optional dictionary of keyword arguments that are directly passed 
            on to the correlogram Matplotlib plot produced by plot_acf(). 
 
        Returns 
        ------- 
        Figure 
            Figure instance with diagnostic plots 
 
        See Also 
        -------- 
        statsmodels.graphics.gofplots.qqplot 
        statsmodels.graphics.tsaplots.plot_acf 
 
        Notes 
        ----- 
        Produces a 2x2 plot grid with the following plots (ordered clockwise 
        from top left): 
 
        1. Standardized residuals over time 
        2. Histogram plus estimated density of standardized residuals, along 
           with a Normal(0,1) density plotted for reference. 
        3. Normal Q-Q plot, with Normal reference line. 
        4. Correlogram 
 
        References 
        ---------- 
        [1] Brockwell and Davis, 1987. Time Series Theory and Methods 
        [2] Brockwell and Davis, 2010. Introduction to Time Series and 
        Forecasting, 2nd edition. 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.graphics.utils </span><span class="s3">import </span><span class="s1">_import_mpl</span><span class="s3">, </span><span class="s1">create_mpl_fig</span>
        <span class="s1">_import_mpl()</span>
        <span class="s1">fig = create_mpl_fig(fig</span><span class="s3">, </span><span class="s1">figsize)</span>
        <span class="s0"># Eliminate residuals associated with burned or diffuse likelihoods</span>
        <span class="s1">d = np.maximum(self.loglikelihood_burn</span><span class="s3">, </span><span class="s1">self.nobs_diffuse)</span>

        <span class="s0"># If given a variable name, find the index</span>
        <span class="s3">if </span><span class="s1">isinstance(variable</span><span class="s3">, </span><span class="s1">str):</span>
            <span class="s1">variable = self.model.endog_names.index(variable)</span>

        <span class="s0"># Get residuals</span>
        <span class="s3">if </span><span class="s1">hasattr(self.data</span><span class="s3">, </span><span class="s5">'dates'</span><span class="s1">) </span><span class="s3">and </span><span class="s1">self.data.dates </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">ix = self.data.dates[d:]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">ix = np.arange(self.nobs - d)</span>
        <span class="s1">resid = pd.Series(</span>
            <span class="s1">self.filter_results.standardized_forecasts_error[variable</span><span class="s3">, </span><span class="s1">d:]</span><span class="s3">,</span>
            <span class="s1">index=ix)</span>

        <span class="s3">if </span><span class="s1">resid.shape[</span><span class="s4">0</span><span class="s1">] &lt; max(d</span><span class="s3">, </span><span class="s1">lags):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s5">&quot;Length of endogenous variable must be larger the the number &quot;</span>
                <span class="s5">&quot;of lags used in the model and the number of observations &quot;</span>
                <span class="s5">&quot;burned in the log-likelihood calculation.&quot;</span>
            <span class="s1">)</span>

        <span class="s0"># Top-left: residuals vs time</span>
        <span class="s1">ax = fig.add_subplot(</span><span class="s4">221</span><span class="s1">)</span>
        <span class="s1">resid.dropna().plot(ax=ax)</span>
        <span class="s1">ax.hlines(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">ix[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">ix[-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s4">0.5</span><span class="s1">)</span>
        <span class="s1">ax.set_xlim(ix[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">ix[-</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s1">name = self.model.endog_names[variable]</span>
        <span class="s3">if </span><span class="s1">len(name) &gt; truncate_endog_names:</span>
            <span class="s1">name = name[:truncate_endog_names - </span><span class="s4">3</span><span class="s1">] + </span><span class="s5">'...'</span>
        <span class="s1">ax.set_title(</span><span class="s5">f'Standardized residual for &quot;</span><span class="s3">{</span><span class="s1">name</span><span class="s3">}</span><span class="s5">&quot;'</span><span class="s1">)</span>

        <span class="s0"># Top-right: histogram, Gaussian kernel density, Normal density</span>
        <span class="s0"># Can only do histogram and Gaussian kernel density on the non-null</span>
        <span class="s0"># elements</span>
        <span class="s1">resid_nonmissing = resid.dropna()</span>
        <span class="s1">ax = fig.add_subplot(</span><span class="s4">222</span><span class="s1">)</span>

        <span class="s1">ax.hist(resid_nonmissing</span><span class="s3">, </span><span class="s1">density=</span><span class="s3">True, </span><span class="s1">label=</span><span class="s5">'Hist'</span><span class="s3">,</span>
                <span class="s1">edgecolor=</span><span class="s5">'#FFFFFF'</span><span class="s1">)</span>

        <span class="s3">from </span><span class="s1">scipy.stats </span><span class="s3">import </span><span class="s1">gaussian_kde</span><span class="s3">, </span><span class="s1">norm</span>
        <span class="s1">kde = gaussian_kde(resid_nonmissing)</span>
        <span class="s1">xlim = (-</span><span class="s4">1.96</span><span class="s1">*</span><span class="s4">2</span><span class="s3">, </span><span class="s4">1.96</span><span class="s1">*</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">x = np.linspace(xlim[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">xlim[</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s1">ax.plot(x</span><span class="s3">, </span><span class="s1">kde(x)</span><span class="s3">, </span><span class="s1">label=</span><span class="s5">'KDE'</span><span class="s1">)</span>
        <span class="s1">ax.plot(x</span><span class="s3">, </span><span class="s1">norm.pdf(x)</span><span class="s3">, </span><span class="s1">label=</span><span class="s5">'N(0,1)'</span><span class="s1">)</span>
        <span class="s1">ax.set_xlim(xlim)</span>
        <span class="s1">ax.legend()</span>
        <span class="s1">ax.set_title(</span><span class="s5">'Histogram plus estimated density'</span><span class="s1">)</span>

        <span class="s0"># Bottom-left: QQ plot</span>
        <span class="s1">ax = fig.add_subplot(</span><span class="s4">223</span><span class="s1">)</span>
        <span class="s3">from </span><span class="s1">statsmodels.graphics.gofplots </span><span class="s3">import </span><span class="s1">qqplot</span>
        <span class="s1">qqplot(resid_nonmissing</span><span class="s3">, </span><span class="s1">line=</span><span class="s5">'s'</span><span class="s3">, </span><span class="s1">ax=ax)</span>
        <span class="s1">ax.set_title(</span><span class="s5">'Normal Q-Q'</span><span class="s1">)</span>

        <span class="s0"># Bottom-right: Correlogram</span>
        <span class="s1">ax = fig.add_subplot(</span><span class="s4">224</span><span class="s1">)</span>
        <span class="s3">from </span><span class="s1">statsmodels.graphics.tsaplots </span><span class="s3">import </span><span class="s1">plot_acf</span>

        <span class="s3">if </span><span class="s1">acf_kwargs </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">acf_kwargs = {}</span>
        <span class="s1">plot_acf(resid</span><span class="s3">, </span><span class="s1">ax=ax</span><span class="s3">, </span><span class="s1">lags=lags</span><span class="s3">, </span><span class="s1">auto_ylims=auto_ylims</span><span class="s3">,</span>
                 <span class="s1">bartlett_confint=bartlett_confint</span><span class="s3">, </span><span class="s1">**acf_kwargs)</span>
        <span class="s1">ax.set_title(</span><span class="s5">'Correlogram'</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">fig</span>

    <span class="s3">def </span><span class="s1">summary(self</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s4">.05</span><span class="s3">, </span><span class="s1">start=</span><span class="s3">None, </span><span class="s1">title=</span><span class="s3">None, </span><span class="s1">model_name=</span><span class="s3">None,</span>
                <span class="s1">display_params=</span><span class="s3">True, </span><span class="s1">display_diagnostics=</span><span class="s3">True,</span>
                <span class="s1">truncate_endog_names=</span><span class="s3">None, </span><span class="s1">display_max_endog=</span><span class="s3">None,</span>
                <span class="s1">extra_top_left=</span><span class="s3">None, </span><span class="s1">extra_top_right=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Summarize the Model 
 
        Parameters 
        ---------- 
        alpha : float, optional 
            Significance level for the confidence intervals. Default is 0.05. 
        start : int, optional 
            Integer of the start observation. Default is 0. 
        model_name : str 
            The name of the model used. Default is to use model class name. 
 
        Returns 
        ------- 
        summary : Summary instance 
            This holds the summary table and text, which can be printed or 
            converted to various output formats. 
 
        See Also 
        -------- 
        statsmodels.iolib.summary.Summary 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.iolib.summary </span><span class="s3">import </span><span class="s1">Summary</span>
        <span class="s3">from </span><span class="s1">statsmodels.iolib.table </span><span class="s3">import </span><span class="s1">SimpleTable</span>
        <span class="s3">from </span><span class="s1">statsmodels.iolib.tableformatting </span><span class="s3">import </span><span class="s1">fmt_params</span>

        <span class="s0"># Model specification results</span>
        <span class="s1">model = self.model</span>
        <span class="s3">if </span><span class="s1">title </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">title = </span><span class="s5">'Statespace Model Results'</span>

        <span class="s3">if </span><span class="s1">start </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">start = </span><span class="s4">0</span>
        <span class="s3">if </span><span class="s1">self.model._index_dates:</span>
            <span class="s1">ix = self.model._index</span>
            <span class="s1">d = ix[start]</span>
            <span class="s1">sample = [</span><span class="s5">'%02d-%02d-%02d' </span><span class="s1">% (d.month</span><span class="s3">, </span><span class="s1">d.day</span><span class="s3">, </span><span class="s1">d.year)]</span>
            <span class="s1">d = ix[-</span><span class="s4">1</span><span class="s1">]</span>
            <span class="s1">sample += [</span><span class="s5">'- ' </span><span class="s1">+ </span><span class="s5">'%02d-%02d-%02d' </span><span class="s1">% (d.month</span><span class="s3">, </span><span class="s1">d.day</span><span class="s3">, </span><span class="s1">d.year)]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">sample = [str(start)</span><span class="s3">, </span><span class="s5">' - ' </span><span class="s1">+ str(self.nobs)]</span>

        <span class="s0"># Standardize the model name as a list of str</span>
        <span class="s3">if </span><span class="s1">model_name </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">model_name = model.__class__.__name__</span>

        <span class="s0"># Truncate endog names</span>
        <span class="s3">if </span><span class="s1">truncate_endog_names </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">truncate_endog_names = </span><span class="s3">False if </span><span class="s1">self.model.k_endog == </span><span class="s4">1 </span><span class="s3">else </span><span class="s4">24</span>
        <span class="s1">endog_names = self.model.endog_names</span>
        <span class="s3">if not </span><span class="s1">isinstance(endog_names</span><span class="s3">, </span><span class="s1">list):</span>
            <span class="s1">endog_names = [endog_names]</span>
        <span class="s1">endog_names = [str(name) </span><span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">endog_names]</span>
        <span class="s3">if </span><span class="s1">truncate_endog_names </span><span class="s3">is not False</span><span class="s1">:</span>
            <span class="s1">n = truncate_endog_names</span>
            <span class="s1">endog_names = [name </span><span class="s3">if </span><span class="s1">len(name) &lt;= n </span><span class="s3">else </span><span class="s1">name[:n] + </span><span class="s5">'...'</span>
                           <span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">endog_names]</span>

        <span class="s0"># Shorten the endog name list if applicable</span>
        <span class="s3">if </span><span class="s1">display_max_endog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">display_max_endog = np.inf</span>
        <span class="s1">yname = </span><span class="s3">None</span>
        <span class="s3">if </span><span class="s1">self.model.k_endog &gt; display_max_endog:</span>
            <span class="s1">k = self.model.k_endog - </span><span class="s4">1</span>
            <span class="s1">yname = </span><span class="s5">'&quot;' </span><span class="s1">+ endog_names[</span><span class="s4">0</span><span class="s1">] + </span><span class="s5">f'&quot;, and </span><span class="s3">{</span><span class="s1">k</span><span class="s3">} </span><span class="s5">more'</span>

        <span class="s0"># Create the tables</span>
        <span class="s3">if not </span><span class="s1">isinstance(model_name</span><span class="s3">, </span><span class="s1">list):</span>
            <span class="s1">model_name = [model_name]</span>

        <span class="s1">top_left = [(</span><span class="s5">'Dep. Variable:'</span><span class="s3">, None</span><span class="s1">)]</span>
        <span class="s1">top_left.append((</span><span class="s5">'Model:'</span><span class="s3">, </span><span class="s1">[model_name[</span><span class="s4">0</span><span class="s1">]]))</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s3">, </span><span class="s1">len(model_name)):</span>
            <span class="s1">top_left.append((</span><span class="s5">''</span><span class="s3">, </span><span class="s1">[</span><span class="s5">'+ ' </span><span class="s1">+ model_name[i]]))</span>
        <span class="s1">top_left += [</span>
            <span class="s1">(</span><span class="s5">'Date:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s5">'Time:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s5">'Sample:'</span><span class="s3">, </span><span class="s1">[sample[</span><span class="s4">0</span><span class="s1">]])</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s5">''</span><span class="s3">, </span><span class="s1">[sample[</span><span class="s4">1</span><span class="s1">]])</span>
        <span class="s1">]</span>

        <span class="s1">top_right = [</span>
            <span class="s1">(</span><span class="s5">'No. Observations:'</span><span class="s3">, </span><span class="s1">[self.nobs])</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s5">'Log Likelihood'</span><span class="s3">, </span><span class="s1">[</span><span class="s5">&quot;%#5.3f&quot; </span><span class="s1">% self.llf])</span><span class="s3">,</span>
        <span class="s1">]</span>
        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s5">'rsquared'</span><span class="s1">):</span>
            <span class="s1">top_right.append((</span><span class="s5">'R-squared:'</span><span class="s3">, </span><span class="s1">[</span><span class="s5">&quot;%#8.3f&quot; </span><span class="s1">% self.rsquared]))</span>
        <span class="s1">top_right += [</span>
            <span class="s1">(</span><span class="s5">'AIC'</span><span class="s3">, </span><span class="s1">[</span><span class="s5">&quot;%#5.3f&quot; </span><span class="s1">% self.aic])</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s5">'BIC'</span><span class="s3">, </span><span class="s1">[</span><span class="s5">&quot;%#5.3f&quot; </span><span class="s1">% self.bic])</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s5">'HQIC'</span><span class="s3">, </span><span class="s1">[</span><span class="s5">&quot;%#5.3f&quot; </span><span class="s1">% self.hqic])]</span>
        <span class="s3">if </span><span class="s1">(self.filter_results </span><span class="s3">is not None and</span>
                <span class="s1">self.filter_results.filter_concentrated):</span>
            <span class="s1">top_right.append((</span><span class="s5">'Scale'</span><span class="s3">, </span><span class="s1">[</span><span class="s5">&quot;%#5.3f&quot; </span><span class="s1">% self.scale]))</span>

        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s5">'cov_type'</span><span class="s1">):</span>
            <span class="s1">cov_type = self.cov_type</span>
            <span class="s3">if </span><span class="s1">cov_type == </span><span class="s5">'none'</span><span class="s1">:</span>
                <span class="s1">cov_type = </span><span class="s5">'Not computed'</span>
            <span class="s1">top_left.append((</span><span class="s5">'Covariance Type:'</span><span class="s3">, </span><span class="s1">[cov_type]))</span>

        <span class="s3">if </span><span class="s1">extra_top_left </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">top_left += extra_top_left</span>
        <span class="s3">if </span><span class="s1">extra_top_right </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">top_right += extra_top_right</span>

        <span class="s1">summary = Summary()</span>
        <span class="s1">summary.add_table_2cols(self</span><span class="s3">, </span><span class="s1">gleft=top_left</span><span class="s3">, </span><span class="s1">gright=top_right</span><span class="s3">,</span>
                                <span class="s1">title=title</span><span class="s3">, </span><span class="s1">yname=yname)</span>
        <span class="s1">table_ix = </span><span class="s4">1</span>
        <span class="s3">if </span><span class="s1">len(self.params) &gt; </span><span class="s4">0 </span><span class="s3">and </span><span class="s1">display_params:</span>
            <span class="s1">summary.add_table_params(self</span><span class="s3">, </span><span class="s1">alpha=alpha</span><span class="s3">,</span>
                                     <span class="s1">xname=self.param_names</span><span class="s3">, </span><span class="s1">use_t=</span><span class="s3">False</span><span class="s1">)</span>
            <span class="s1">table_ix += </span><span class="s4">1</span>

        <span class="s0"># Diagnostic tests results</span>
        <span class="s3">if </span><span class="s1">display_diagnostics:</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s1">het = self.test_heteroskedasticity(method=</span><span class="s5">'breakvar'</span><span class="s1">)</span>
            <span class="s3">except </span><span class="s1">Exception:  </span><span class="s0"># FIXME: catch something specific</span>
                <span class="s1">het = np.zeros((self.model.k_endog</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)) * np.nan</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s1">lb = self.test_serial_correlation(method=</span><span class="s5">'ljungbox'</span><span class="s3">, </span><span class="s1">lags=[</span><span class="s4">1</span><span class="s1">])</span>
            <span class="s3">except </span><span class="s1">Exception:  </span><span class="s0"># FIXME: catch something specific</span>
                <span class="s1">lb = np.zeros((self.model.k_endog</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)) * np.nan</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s1">jb = self.test_normality(method=</span><span class="s5">'jarquebera'</span><span class="s1">)</span>
            <span class="s3">except </span><span class="s1">Exception:  </span><span class="s0"># FIXME: catch something specific</span>
                <span class="s1">jb = np.zeros((self.model.k_endog</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)) * np.nan</span>

            <span class="s3">if </span><span class="s1">self.model.k_endog &lt;= display_max_endog:</span>
                <span class="s1">format_str = </span><span class="s3">lambda </span><span class="s1">array: [  </span><span class="s0"># noqa:E731</span>
                    <span class="s5">', '</span><span class="s1">.join([</span><span class="s5">'{0:.2f}'</span><span class="s1">.format(i) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">array])</span>
                <span class="s1">]</span>
                <span class="s1">diagn_left = [</span>
                    <span class="s1">(</span><span class="s5">'Ljung-Box (L1) (Q):'</span><span class="s3">, </span><span class="s1">format_str(lb[:</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]))</span><span class="s3">,</span>
                    <span class="s1">(</span><span class="s5">'Prob(Q):'</span><span class="s3">, </span><span class="s1">format_str(lb[:</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]))</span><span class="s3">,</span>
                    <span class="s1">(</span><span class="s5">'Heteroskedasticity (H):'</span><span class="s3">, </span><span class="s1">format_str(het[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]))</span><span class="s3">,</span>
                    <span class="s1">(</span><span class="s5">'Prob(H) (two-sided):'</span><span class="s3">, </span><span class="s1">format_str(het[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]))]</span>

                <span class="s1">diagn_right = [(</span><span class="s5">'Jarque-Bera (JB):'</span><span class="s3">, </span><span class="s1">format_str(jb[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]))</span><span class="s3">,</span>
                               <span class="s1">(</span><span class="s5">'Prob(JB):'</span><span class="s3">, </span><span class="s1">format_str(jb[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]))</span><span class="s3">,</span>
                               <span class="s1">(</span><span class="s5">'Skew:'</span><span class="s3">, </span><span class="s1">format_str(jb[:</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]))</span><span class="s3">,</span>
                               <span class="s1">(</span><span class="s5">'Kurtosis:'</span><span class="s3">, </span><span class="s1">format_str(jb[:</span><span class="s3">, </span><span class="s4">3</span><span class="s1">]))</span>
                               <span class="s1">]</span>

                <span class="s1">summary.add_table_2cols(self</span><span class="s3">, </span><span class="s1">gleft=diagn_left</span><span class="s3">,</span>
                                        <span class="s1">gright=diagn_right</span><span class="s3">, </span><span class="s1">title=</span><span class="s5">&quot;&quot;</span><span class="s1">)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">columns = [</span><span class="s5">'LjungBox</span><span class="s3">\n</span><span class="s5">(L1) (Q)'</span><span class="s3">, </span><span class="s5">'Prob(Q)'</span><span class="s3">,</span>
                           <span class="s5">'Het.(H)'</span><span class="s3">, </span><span class="s5">'Prob(H)'</span><span class="s3">,</span>
                           <span class="s5">'Jarque</span><span class="s3">\n</span><span class="s5">Bera(JB)'</span><span class="s3">, </span><span class="s5">'Prob(JB)'</span><span class="s3">, </span><span class="s5">'Skew'</span><span class="s3">, </span><span class="s5">'Kurtosis'</span><span class="s1">]</span>
                <span class="s1">data = pd.DataFrame(</span>
                    <span class="s1">np.c_[lb[:</span><span class="s3">, </span><span class="s1">:</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">het[:</span><span class="s3">, </span><span class="s1">:</span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">jb[:</span><span class="s3">, </span><span class="s1">:</span><span class="s4">4</span><span class="s1">]]</span><span class="s3">,</span>
                    <span class="s1">index=endog_names</span><span class="s3">, </span><span class="s1">columns=columns).applymap(</span>
                        <span class="s3">lambda </span><span class="s1">num: </span><span class="s5">'' </span><span class="s3">if </span><span class="s1">pd.isnull(num) </span><span class="s3">else </span><span class="s5">'%.2f' </span><span class="s1">% num)</span>
                <span class="s1">data.index.name = </span><span class="s5">'Residual of</span><span class="s3">\n</span><span class="s5">Dep. variable'</span>
                <span class="s1">data = data.reset_index()</span>

                <span class="s1">params_data = data.values</span>
                <span class="s1">params_header = data.columns.tolist()</span>
                <span class="s1">params_stubs = </span><span class="s3">None</span>

                <span class="s1">title = </span><span class="s5">'Residual diagnostics:'</span>
                <span class="s1">table = SimpleTable(</span>
                    <span class="s1">params_data</span><span class="s3">, </span><span class="s1">params_header</span><span class="s3">, </span><span class="s1">params_stubs</span><span class="s3">,</span>
                    <span class="s1">txt_fmt=fmt_params</span><span class="s3">, </span><span class="s1">title=title)</span>
                <span class="s1">summary.tables.insert(table_ix</span><span class="s3">, </span><span class="s1">table)</span>

        <span class="s0"># Add warnings/notes, added to text format only</span>
        <span class="s1">etext = []</span>
        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s5">'cov_type'</span><span class="s1">) </span><span class="s3">and </span><span class="s5">'description' </span><span class="s3">in </span><span class="s1">self.cov_kwds:</span>
            <span class="s1">etext.append(self.cov_kwds[</span><span class="s5">'description'</span><span class="s1">])</span>
        <span class="s3">if </span><span class="s1">self._rank &lt; (len(self.params) - len(self.fixed_params)):</span>
            <span class="s1">cov_params = self.cov_params()</span>
            <span class="s3">if </span><span class="s1">len(self.fixed_params) &gt; </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s1">mask = np.ix_(self._free_params_index</span><span class="s3">, </span><span class="s1">self._free_params_index)</span>
                <span class="s1">cov_params = cov_params[mask]</span>
            <span class="s1">etext.append(</span><span class="s5">&quot;Covariance matrix is singular or near-singular,&quot;</span>
                         <span class="s5">&quot; with condition number %6.3g. Standard errors may be&quot;</span>
                         <span class="s5">&quot; unstable.&quot; </span><span class="s1">% _safe_cond(cov_params))</span>

        <span class="s3">if </span><span class="s1">etext:</span>
            <span class="s1">etext = [</span><span class="s5">&quot;[{0}] {1}&quot;</span><span class="s1">.format(i + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">text)</span>
                     <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">text </span><span class="s3">in </span><span class="s1">enumerate(etext)]</span>
            <span class="s1">etext.insert(</span><span class="s4">0</span><span class="s3">, </span><span class="s5">&quot;Warnings:&quot;</span><span class="s1">)</span>
            <span class="s1">summary.add_extra_txt(etext)</span>

        <span class="s3">return </span><span class="s1">summary</span>


<span class="s3">class </span><span class="s1">MLEResultsWrapper(wrap.ResultsWrapper):</span>
    <span class="s1">_attrs = {</span>
        <span class="s5">'zvalues'</span><span class="s1">: </span><span class="s5">'columns'</span><span class="s3">,</span>
        <span class="s5">'cov_params_approx'</span><span class="s1">: </span><span class="s5">'cov'</span><span class="s3">,</span>
        <span class="s5">'cov_params_default'</span><span class="s1">: </span><span class="s5">'cov'</span><span class="s3">,</span>
        <span class="s5">'cov_params_oim'</span><span class="s1">: </span><span class="s5">'cov'</span><span class="s3">,</span>
        <span class="s5">'cov_params_opg'</span><span class="s1">: </span><span class="s5">'cov'</span><span class="s3">,</span>
        <span class="s5">'cov_params_robust'</span><span class="s1">: </span><span class="s5">'cov'</span><span class="s3">,</span>
        <span class="s5">'cov_params_robust_approx'</span><span class="s1">: </span><span class="s5">'cov'</span><span class="s3">,</span>
        <span class="s5">'cov_params_robust_oim'</span><span class="s1">: </span><span class="s5">'cov'</span><span class="s3">,</span>
    <span class="s1">}</span>
    <span class="s1">_wrap_attrs = wrap.union_dicts(tsbase.TimeSeriesResultsWrapper._wrap_attrs</span><span class="s3">,</span>
                                   <span class="s1">_attrs)</span>
    <span class="s1">_methods = {</span>
        <span class="s5">'forecast'</span><span class="s1">: </span><span class="s5">'dates'</span><span class="s3">,</span>
        <span class="s5">'impulse_responses'</span><span class="s1">: </span><span class="s5">'ynames'</span>
    <span class="s1">}</span>
    <span class="s1">_wrap_methods = wrap.union_dicts(</span>
        <span class="s1">tsbase.TimeSeriesResultsWrapper._wrap_methods</span><span class="s3">, </span><span class="s1">_methods)</span>
<span class="s1">wrap.populate_wrapper(MLEResultsWrapper</span><span class="s3">, </span><span class="s1">MLEResults)  </span><span class="s0"># noqa:E305</span>


<span class="s3">class </span><span class="s1">PredictionResults(pred.PredictionResults):</span>
    <span class="s2">&quot;&quot;&quot; 
    Prediction result from MLE models 
 
    Parameters 
    ---------- 
    model : MLEModel 
        The models used to make the prediction 
    prediction_results : kalman_filter.PredictionResults instance 
        Results object from prediction after fitting or filtering a state space 
        model. 
    row_labels : iterable 
        Row labels for the predicted data. 
    information_set : str 
        Name of information set 
    signal_only : bool 
        Whether the prediction is for the signal only 
 
    Attributes 
    ---------- 
    model : MLEModel 
        The models used to make the prediction 
    prediction_results : kalman_filter.PredictionResults instance 
        Results object from prediction after fitting or filtering a state space 
        model. 
    information_set : str 
        Name of information set 
    signal_only : bool 
        Whether the prediction is for the signal only 
    &quot;&quot;&quot;</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">model</span><span class="s3">, </span><span class="s1">prediction_results</span><span class="s3">, </span><span class="s1">row_labels=</span><span class="s3">None,</span>
                 <span class="s1">information_set=</span><span class="s5">'predicted'</span><span class="s3">, </span><span class="s1">signal_only=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s3">if </span><span class="s1">model.model.k_endog == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">endog = pd.Series(prediction_results.endog[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">,</span>
                              <span class="s1">name=model.model.endog_names)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">endog = pd.DataFrame(prediction_results.endog.T</span><span class="s3">,</span>
                                 <span class="s1">columns=model.model.endog_names)</span>
        <span class="s1">self.model = Bunch(data=model.data.__class__(</span>
            <span class="s1">endog=endog</span><span class="s3">, </span><span class="s1">predict_dates=row_labels))</span>
        <span class="s1">self.prediction_results = prediction_results</span>

        <span class="s1">self.information_set = information_set</span>
        <span class="s1">self.signal_only = signal_only</span>

        <span class="s0"># Get required values</span>
        <span class="s1">k_endog</span><span class="s3">, </span><span class="s1">nobs = prediction_results.endog.shape</span>
        <span class="s1">res = self.prediction_results.results</span>
        <span class="s3">if </span><span class="s1">information_set == </span><span class="s5">'predicted' </span><span class="s3">and not </span><span class="s1">res.memory_no_forecast_mean:</span>
            <span class="s3">if not </span><span class="s1">signal_only:</span>
                <span class="s1">predicted_mean = self.prediction_results.forecasts</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">predicted_mean = self.prediction_results.predicted_signal</span>
        <span class="s3">elif </span><span class="s1">information_set == </span><span class="s5">'filtered' </span><span class="s3">and not </span><span class="s1">res.memory_no_filtered_mean:</span>
            <span class="s3">if not </span><span class="s1">signal_only:</span>
                <span class="s1">predicted_mean = self.prediction_results.filtered_forecasts</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">predicted_mean = self.prediction_results.filtered_signal</span>
        <span class="s3">elif </span><span class="s1">information_set == </span><span class="s5">'smoothed'</span><span class="s1">:</span>
            <span class="s3">if not </span><span class="s1">signal_only:</span>
                <span class="s1">predicted_mean = self.prediction_results.smoothed_forecasts</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">predicted_mean = self.prediction_results.smoothed_signal</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">predicted_mean = np.zeros((k_endog</span><span class="s3">, </span><span class="s1">nobs)) * np.nan</span>

        <span class="s3">if </span><span class="s1">predicted_mean.shape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">predicted_mean = predicted_mean[</span><span class="s4">0</span><span class="s3">, </span><span class="s1">:]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">predicted_mean = predicted_mean.transpose()</span>

        <span class="s3">if </span><span class="s1">information_set == </span><span class="s5">'predicted' </span><span class="s3">and not </span><span class="s1">res.memory_no_forecast_cov:</span>
            <span class="s3">if not </span><span class="s1">signal_only:</span>
                <span class="s1">var_pred_mean = self.prediction_results.forecasts_error_cov</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">var_pred_mean = self.prediction_results.predicted_signal_cov</span>
        <span class="s3">elif </span><span class="s1">information_set == </span><span class="s5">'filtered' </span><span class="s3">and not </span><span class="s1">res.memory_no_filtered_mean:</span>
            <span class="s3">if not </span><span class="s1">signal_only:</span>
                <span class="s1">var_pred_mean = (</span>
                    <span class="s1">self.prediction_results.filtered_forecasts_error_cov)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">var_pred_mean = self.prediction_results.filtered_signal_cov</span>
        <span class="s3">elif </span><span class="s1">information_set == </span><span class="s5">'smoothed'</span><span class="s1">:</span>
            <span class="s3">if not </span><span class="s1">signal_only:</span>
                <span class="s1">var_pred_mean = (</span>
                    <span class="s1">self.prediction_results.smoothed_forecasts_error_cov)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">var_pred_mean = self.prediction_results.smoothed_signal_cov</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">var_pred_mean = np.zeros((k_endog</span><span class="s3">, </span><span class="s1">k_endog</span><span class="s3">, </span><span class="s1">nobs)) * np.nan</span>

        <span class="s3">if </span><span class="s1">var_pred_mean.shape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">var_pred_mean = var_pred_mean[</span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">:]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">var_pred_mean = var_pred_mean.transpose()</span>

        <span class="s0"># Initialize</span>
        <span class="s1">super(PredictionResults</span><span class="s3">, </span><span class="s1">self).__init__(predicted_mean</span><span class="s3">, </span><span class="s1">var_pred_mean</span><span class="s3">,</span>
                                                <span class="s1">dist=</span><span class="s5">'norm'</span><span class="s3">,</span>
                                                <span class="s1">row_labels=row_labels)</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">se_mean(self):</span>
        <span class="s0"># Replace negative values with np.nan to avoid a RuntimeWarning</span>
        <span class="s1">var_pred_mean = self.var_pred_mean.copy()</span>
        <span class="s1">var_pred_mean[var_pred_mean &lt; </span><span class="s4">0</span><span class="s1">] = np.nan</span>
        <span class="s3">if </span><span class="s1">var_pred_mean.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">se_mean = np.sqrt(var_pred_mean)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">se_mean = np.sqrt(var_pred_mean.T.diagonal())</span>
        <span class="s3">return </span><span class="s1">se_mean</span>

    <span class="s3">def </span><span class="s1">conf_int(self</span><span class="s3">, </span><span class="s1">method=</span><span class="s5">'endpoint'</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s4">0.05</span><span class="s3">, </span><span class="s1">**kwds):</span>
        <span class="s0"># TODO: this performs metadata wrapping, and that should be handled</span>
        <span class="s0">#       by attach_* methods. However, they do not currently support</span>
        <span class="s0">#       this use case.</span>
        <span class="s1">_use_pandas = self._use_pandas</span>
        <span class="s1">self._use_pandas = </span><span class="s3">False</span>
        <span class="s1">conf_int = super(PredictionResults</span><span class="s3">, </span><span class="s1">self).conf_int(alpha</span><span class="s3">, </span><span class="s1">**kwds)</span>
        <span class="s1">self._use_pandas = _use_pandas</span>

        <span class="s0"># Create a dataframe</span>
        <span class="s3">if </span><span class="s1">self._row_labels </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">conf_int = pd.DataFrame(conf_int</span><span class="s3">, </span><span class="s1">index=self.row_labels)</span>

            <span class="s0"># Attach the endog names</span>
            <span class="s1">ynames = self.model.data.ynames</span>
            <span class="s3">if not </span><span class="s1">type(ynames) == list:</span>
                <span class="s1">ynames = [ynames]</span>
            <span class="s1">names = ([</span><span class="s5">'lower {0}'</span><span class="s1">.format(name) </span><span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">ynames] +</span>
                     <span class="s1">[</span><span class="s5">'upper {0}'</span><span class="s1">.format(name) </span><span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">ynames])</span>
            <span class="s1">conf_int.columns = names</span>

        <span class="s3">return </span><span class="s1">conf_int</span>

    <span class="s3">def </span><span class="s1">summary_frame(self</span><span class="s3">, </span><span class="s1">endog=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s4">0.05</span><span class="s1">):</span>
        <span class="s0"># TODO: finish and cleanup</span>
        <span class="s0"># import pandas as pd</span>
        <span class="s0"># ci_obs = self.conf_int(alpha=alpha, obs=True) # need to split</span>
        <span class="s1">ci_mean = np.asarray(self.conf_int(alpha=alpha))</span>
        <span class="s1">_use_pandas = self._use_pandas</span>
        <span class="s1">self._use_pandas = </span><span class="s3">False</span>
        <span class="s1">to_include = {}</span>
        <span class="s3">if </span><span class="s1">self.predicted_mean.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">yname = self.model.data.ynames</span>
            <span class="s1">to_include[</span><span class="s5">'mean'</span><span class="s1">] = self.predicted_mean</span>
            <span class="s1">to_include[</span><span class="s5">'mean_se'</span><span class="s1">] = self.se_mean</span>
            <span class="s1">k_endog = </span><span class="s4">1</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">yname = self.model.data.ynames[endog]</span>
            <span class="s1">to_include[</span><span class="s5">'mean'</span><span class="s1">] = self.predicted_mean[:</span><span class="s3">, </span><span class="s1">endog]</span>
            <span class="s1">to_include[</span><span class="s5">'mean_se'</span><span class="s1">] = self.se_mean[:</span><span class="s3">, </span><span class="s1">endog]</span>
            <span class="s1">k_endog = self.predicted_mean.shape[</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">self._use_pandas = _use_pandas</span>
        <span class="s1">to_include[</span><span class="s5">'mean_ci_lower'</span><span class="s1">] = ci_mean[:</span><span class="s3">, </span><span class="s1">endog]</span>
        <span class="s1">to_include[</span><span class="s5">'mean_ci_upper'</span><span class="s1">] = ci_mean[:</span><span class="s3">, </span><span class="s1">k_endog + endog]</span>

        <span class="s0"># pandas dict does not handle 2d_array</span>
        <span class="s0"># data = np.column_stack(list(to_include.values()))</span>
        <span class="s0"># names = ....</span>
        <span class="s1">res = pd.DataFrame(to_include</span><span class="s3">, </span><span class="s1">index=self._row_labels</span><span class="s3">,</span>
                           <span class="s1">columns=list(to_include.keys()))</span>
        <span class="s1">res.columns.name = yname</span>
        <span class="s3">return </span><span class="s1">res</span>


<span class="s3">class </span><span class="s1">PredictionResultsWrapper(wrap.ResultsWrapper):</span>
    <span class="s1">_attrs = {</span>
        <span class="s5">'predicted_mean'</span><span class="s1">: </span><span class="s5">'dates'</span><span class="s3">,</span>
        <span class="s5">'se_mean'</span><span class="s1">: </span><span class="s5">'dates'</span><span class="s3">,</span>
        <span class="s5">'t_values'</span><span class="s1">: </span><span class="s5">'dates'</span><span class="s3">,</span>
    <span class="s1">}</span>
    <span class="s1">_wrap_attrs = wrap.union_dicts(_attrs)</span>

    <span class="s1">_methods = {}</span>
    <span class="s1">_wrap_methods = wrap.union_dicts(_methods)</span>
<span class="s1">wrap.populate_wrapper(PredictionResultsWrapper</span><span class="s3">, </span><span class="s1">PredictionResults)  </span><span class="s0"># noqa:E305</span>
</pre>
</body>
</html>