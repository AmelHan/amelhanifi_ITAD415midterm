<html>
<head>
<title>test_warm_start.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6897bb;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #808080;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_warm_start.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">assert_allclose</span><span class="s0">, </span><span class="s1">assert_array_equal</span>

<span class="s0">from </span><span class="s1">sklearn.base </span><span class="s0">import </span><span class="s1">clone</span>
<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">make_classification</span><span class="s0">, </span><span class="s1">make_regression</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">HistGradientBoostingClassifier</span><span class="s0">,</span>
    <span class="s1">HistGradientBoostingRegressor</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.metrics </span><span class="s0">import </span><span class="s1">check_scoring</span>

<span class="s1">X_classification</span><span class="s0">, </span><span class="s1">y_classification = make_classification(random_state=</span><span class="s2">0</span><span class="s1">)</span>
<span class="s1">X_regression</span><span class="s0">, </span><span class="s1">y_regression = make_regression(random_state=</span><span class="s2">0</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">_assert_predictor_equal(gb_1</span><span class="s0">, </span><span class="s1">gb_2</span><span class="s0">, </span><span class="s1">X):</span>
    <span class="s3">&quot;&quot;&quot;Assert that two HistGBM instances are identical.&quot;&quot;&quot;</span>
    <span class="s4"># Check identical nodes for each tree</span>
    <span class="s0">for </span><span class="s1">pred_ith_1</span><span class="s0">, </span><span class="s1">pred_ith_2 </span><span class="s0">in </span><span class="s1">zip(gb_1._predictors</span><span class="s0">, </span><span class="s1">gb_2._predictors):</span>
        <span class="s0">for </span><span class="s1">predictor_1</span><span class="s0">, </span><span class="s1">predictor_2 </span><span class="s0">in </span><span class="s1">zip(pred_ith_1</span><span class="s0">, </span><span class="s1">pred_ith_2):</span>
            <span class="s1">assert_array_equal(predictor_1.nodes</span><span class="s0">, </span><span class="s1">predictor_2.nodes)</span>

    <span class="s4"># Check identical predictions</span>
    <span class="s1">assert_allclose(gb_1.predict(X)</span><span class="s0">, </span><span class="s1">gb_2.predict(X))</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;GradientBoosting, X, y&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">X_classification</span><span class="s0">, </span><span class="s1">y_classification)</span><span class="s0">,</span>
        <span class="s1">(HistGradientBoostingRegressor</span><span class="s0">, </span><span class="s1">X_regression</span><span class="s0">, </span><span class="s1">y_regression)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_max_iter_with_warm_start_validation(GradientBoosting</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s4"># Check that a ValueError is raised when the maximum number of iterations</span>
    <span class="s4"># is smaller than the number of iterations from the previous fit when warm</span>
    <span class="s4"># start is True.</span>

    <span class="s1">estimator = GradientBoosting(max_iter=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">early_stopping=</span><span class="s0">False, </span><span class="s1">warm_start=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">estimator.set_params(max_iter=</span><span class="s2">5</span><span class="s1">)</span>
    <span class="s1">err_msg = (</span>
        <span class="s5">&quot;max_iter=5 must be larger than or equal to n_iter_=10 when warm_start==True&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">estimator.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;GradientBoosting, X, y&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">X_classification</span><span class="s0">, </span><span class="s1">y_classification)</span><span class="s0">,</span>
        <span class="s1">(HistGradientBoostingRegressor</span><span class="s0">, </span><span class="s1">X_regression</span><span class="s0">, </span><span class="s1">y_regression)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_warm_start_yields_identical_results(GradientBoosting</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s4"># Make sure that fitting 50 iterations and then 25 with warm start is</span>
    <span class="s4"># equivalent to fitting 75 iterations.</span>

    <span class="s1">rng = </span><span class="s2">42</span>
    <span class="s1">gb_warm_start = GradientBoosting(</span>
        <span class="s1">n_iter_no_change=</span><span class="s2">100</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">50</span><span class="s0">, </span><span class="s1">random_state=rng</span><span class="s0">, </span><span class="s1">warm_start=</span><span class="s0">True</span>
    <span class="s1">)</span>
    <span class="s1">gb_warm_start.fit(X</span><span class="s0">, </span><span class="s1">y).set_params(max_iter=</span><span class="s2">75</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">gb_no_warm_start = GradientBoosting(</span>
        <span class="s1">n_iter_no_change=</span><span class="s2">100</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s2">75</span><span class="s0">, </span><span class="s1">random_state=rng</span><span class="s0">, </span><span class="s1">warm_start=</span><span class="s0">False</span>
    <span class="s1">)</span>
    <span class="s1">gb_no_warm_start.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s4"># Check that both predictors are equal</span>
    <span class="s1">_assert_predictor_equal(gb_warm_start</span><span class="s0">, </span><span class="s1">gb_no_warm_start</span><span class="s0">, </span><span class="s1">X)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;GradientBoosting, X, y&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">X_classification</span><span class="s0">, </span><span class="s1">y_classification)</span><span class="s0">,</span>
        <span class="s1">(HistGradientBoostingRegressor</span><span class="s0">, </span><span class="s1">X_regression</span><span class="s0">, </span><span class="s1">y_regression)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_warm_start_max_depth(GradientBoosting</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s4"># Test if possible to fit trees of different depth in ensemble.</span>
    <span class="s1">gb = GradientBoosting(</span>
        <span class="s1">max_iter=</span><span class="s2">20</span><span class="s0">,</span>
        <span class="s1">min_samples_leaf=</span><span class="s2">1</span><span class="s0">,</span>
        <span class="s1">warm_start=</span><span class="s0">True,</span>
        <span class="s1">max_depth=</span><span class="s2">2</span><span class="s0">,</span>
        <span class="s1">early_stopping=</span><span class="s0">False,</span>
    <span class="s1">)</span>
    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">gb.set_params(max_iter=</span><span class="s2">30</span><span class="s0">, </span><span class="s1">max_depth=</span><span class="s2">3</span><span class="s0">, </span><span class="s1">n_iter_no_change=</span><span class="s2">110</span><span class="s1">)</span>
    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s4"># First 20 trees have max_depth == 2</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">20</span><span class="s1">):</span>
        <span class="s0">assert </span><span class="s1">gb._predictors[i][</span><span class="s2">0</span><span class="s1">].get_max_depth() == </span><span class="s2">2</span>
    <span class="s4"># Last 10 trees have max_depth == 3</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">1</span><span class="s0">, </span><span class="s2">11</span><span class="s1">):</span>
        <span class="s0">assert </span><span class="s1">gb._predictors[-i][</span><span class="s2">0</span><span class="s1">].get_max_depth() == </span><span class="s2">3</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;GradientBoosting, X, y&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">X_classification</span><span class="s0">, </span><span class="s1">y_classification)</span><span class="s0">,</span>
        <span class="s1">(HistGradientBoostingRegressor</span><span class="s0">, </span><span class="s1">X_regression</span><span class="s0">, </span><span class="s1">y_regression)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;scoring&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s0">None, </span><span class="s5">&quot;loss&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_warm_start_early_stopping(GradientBoosting</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">scoring):</span>
    <span class="s4"># Make sure that early stopping occurs after a small number of iterations</span>
    <span class="s4"># when fitting a second time with warm starting.</span>

    <span class="s1">n_iter_no_change = </span><span class="s2">5</span>
    <span class="s1">gb = GradientBoosting(</span>
        <span class="s1">n_iter_no_change=n_iter_no_change</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s2">10000</span><span class="s0">,</span>
        <span class="s1">early_stopping=</span><span class="s0">True,</span>
        <span class="s1">random_state=</span><span class="s2">42</span><span class="s0">,</span>
        <span class="s1">warm_start=</span><span class="s0">True,</span>
        <span class="s1">tol=</span><span class="s2">1e-3</span><span class="s0">,</span>
        <span class="s1">scoring=scoring</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">n_iter_first_fit = gb.n_iter_</span>
    <span class="s1">gb.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">n_iter_second_fit = gb.n_iter_</span>
    <span class="s0">assert </span><span class="s2">0 </span><span class="s1">&lt; n_iter_second_fit - n_iter_first_fit &lt; n_iter_no_change</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;GradientBoosting, X, y&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">X_classification</span><span class="s0">, </span><span class="s1">y_classification)</span><span class="s0">,</span>
        <span class="s1">(HistGradientBoostingRegressor</span><span class="s0">, </span><span class="s1">X_regression</span><span class="s0">, </span><span class="s1">y_regression)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_warm_start_equal_n_estimators(GradientBoosting</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s4"># Test if warm start with equal n_estimators does nothing</span>
    <span class="s1">gb_1 = GradientBoosting(max_depth=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">early_stopping=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">gb_1.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">gb_2 = clone(gb_1)</span>
    <span class="s1">gb_2.set_params(max_iter=gb_1.max_iter</span><span class="s0">, </span><span class="s1">warm_start=</span><span class="s0">True, </span><span class="s1">n_iter_no_change=</span><span class="s2">5</span><span class="s1">)</span>
    <span class="s1">gb_2.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s4"># Check that both predictors are equal</span>
    <span class="s1">_assert_predictor_equal(gb_1</span><span class="s0">, </span><span class="s1">gb_2</span><span class="s0">, </span><span class="s1">X)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;GradientBoosting, X, y&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">X_classification</span><span class="s0">, </span><span class="s1">y_classification)</span><span class="s0">,</span>
        <span class="s1">(HistGradientBoostingRegressor</span><span class="s0">, </span><span class="s1">X_regression</span><span class="s0">, </span><span class="s1">y_regression)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_warm_start_clear(GradientBoosting</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s4"># Test if fit clears state.</span>
    <span class="s1">gb_1 = GradientBoosting(n_iter_no_change=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">gb_1.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">gb_2 = GradientBoosting(n_iter_no_change=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">42</span><span class="s0">, </span><span class="s1">warm_start=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">gb_2.fit(X</span><span class="s0">, </span><span class="s1">y)  </span><span class="s4"># inits state</span>
    <span class="s1">gb_2.set_params(warm_start=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">gb_2.fit(X</span><span class="s0">, </span><span class="s1">y)  </span><span class="s4"># clears old state and equals est</span>

    <span class="s4"># Check that both predictors have the same train_score_ and</span>
    <span class="s4"># validation_score_ attributes</span>
    <span class="s1">assert_allclose(gb_1.train_score_</span><span class="s0">, </span><span class="s1">gb_2.train_score_)</span>
    <span class="s1">assert_allclose(gb_1.validation_score_</span><span class="s0">, </span><span class="s1">gb_2.validation_score_)</span>

    <span class="s4"># Check that both predictors are equal</span>
    <span class="s1">_assert_predictor_equal(gb_1</span><span class="s0">, </span><span class="s1">gb_2</span><span class="s0">, </span><span class="s1">X)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;GradientBoosting, X, y&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">X_classification</span><span class="s0">, </span><span class="s1">y_classification)</span><span class="s0">,</span>
        <span class="s1">(HistGradientBoostingRegressor</span><span class="s0">, </span><span class="s1">X_regression</span><span class="s0">, </span><span class="s1">y_regression)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;rng_type&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s5">&quot;none&quot;</span><span class="s0">, </span><span class="s5">&quot;int&quot;</span><span class="s0">, </span><span class="s5">&quot;instance&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_random_seeds_warm_start(GradientBoosting</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">rng_type):</span>
    <span class="s4"># Make sure the seeds for train/val split and small trainset subsampling</span>
    <span class="s4"># are correctly set in a warm start context.</span>
    <span class="s0">def </span><span class="s1">_get_rng(rng_type):</span>
        <span class="s4"># Helper to avoid consuming rngs</span>
        <span class="s0">if </span><span class="s1">rng_type == </span><span class="s5">&quot;none&quot;</span><span class="s1">:</span>
            <span class="s0">return None</span>
        <span class="s0">elif </span><span class="s1">rng_type == </span><span class="s5">&quot;int&quot;</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s2">42</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s1">random_state = _get_rng(rng_type)</span>
    <span class="s1">gb_1 = GradientBoosting(early_stopping=</span><span class="s0">True, </span><span class="s1">max_iter=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">random_state=random_state)</span>
    <span class="s1">gb_1.set_params(scoring=check_scoring(gb_1))</span>
    <span class="s1">gb_1.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">random_seed_1_1 = gb_1._random_seed</span>

    <span class="s1">gb_1.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">random_seed_1_2 = gb_1._random_seed  </span><span class="s4"># clear the old state, different seed</span>

    <span class="s1">random_state = _get_rng(rng_type)</span>
    <span class="s1">gb_2 = GradientBoosting(</span>
        <span class="s1">early_stopping=</span><span class="s0">True, </span><span class="s1">max_iter=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">random_state=random_state</span><span class="s0">, </span><span class="s1">warm_start=</span><span class="s0">True</span>
    <span class="s1">)</span>
    <span class="s1">gb_2.set_params(scoring=check_scoring(gb_2))</span>
    <span class="s1">gb_2.fit(X</span><span class="s0">, </span><span class="s1">y)  </span><span class="s4"># inits state</span>
    <span class="s1">random_seed_2_1 = gb_2._random_seed</span>
    <span class="s1">gb_2.fit(X</span><span class="s0">, </span><span class="s1">y)  </span><span class="s4"># clears old state and equals est</span>
    <span class="s1">random_seed_2_2 = gb_2._random_seed</span>

    <span class="s4"># Without warm starting, the seeds should be</span>
    <span class="s4"># * all different if random state is None</span>
    <span class="s4"># * all equal if random state is an integer</span>
    <span class="s4"># * different when refitting and equal with a new estimator (because</span>
    <span class="s4">#   the random state is mutated)</span>
    <span class="s0">if </span><span class="s1">rng_type == </span><span class="s5">&quot;none&quot;</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">random_seed_1_1 != random_seed_1_2 != random_seed_2_1</span>
    <span class="s0">elif </span><span class="s1">rng_type == </span><span class="s5">&quot;int&quot;</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">random_seed_1_1 == random_seed_1_2 == random_seed_2_1</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">random_seed_1_1 == random_seed_2_1 != random_seed_1_2</span>

    <span class="s4"># With warm starting, the seeds must be equal</span>
    <span class="s0">assert </span><span class="s1">random_seed_2_1 == random_seed_2_2</span>
</pre>
</body>
</html>