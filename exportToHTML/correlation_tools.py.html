<html>
<head>
<title>correlation_tools.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
correlation_tools.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot; 
 
Created on Fri Aug 17 13:10:52 2012 
 
Author: Josef Perktold 
License: BSD-3 
&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">scipy.sparse </span><span class="s3">as </span><span class="s1">sparse</span>
<span class="s3">from </span><span class="s1">scipy.sparse.linalg </span><span class="s3">import </span><span class="s1">svds</span>
<span class="s3">from </span><span class="s1">scipy.optimize </span><span class="s3">import </span><span class="s1">fminbound</span>
<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">from </span><span class="s1">statsmodels.tools.tools </span><span class="s3">import </span><span class="s1">Bunch</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">IterationLimitWarning</span><span class="s3">, </span><span class="s1">iteration_limit_doc)</span>


<span class="s3">def </span><span class="s1">clip_evals(x</span><span class="s3">, </span><span class="s1">value=</span><span class="s4">0</span><span class="s1">):  </span><span class="s0"># threshold=0, value=0):</span>
    <span class="s1">evals</span><span class="s3">, </span><span class="s1">evecs = np.linalg.eigh(x)</span>
    <span class="s1">clipped = np.any(evals &lt; value)</span>
    <span class="s1">x_new = np.dot(evecs * np.maximum(evals</span><span class="s3">, </span><span class="s1">value)</span><span class="s3">, </span><span class="s1">evecs.T)</span>
    <span class="s3">return </span><span class="s1">x_new</span><span class="s3">, </span><span class="s1">clipped</span>


<span class="s3">def </span><span class="s1">corr_nearest(corr</span><span class="s3">, </span><span class="s1">threshold=</span><span class="s4">1e-15</span><span class="s3">, </span><span class="s1">n_fact=</span><span class="s4">100</span><span class="s1">):</span>
    <span class="s2">''' 
    Find the nearest correlation matrix that is positive semi-definite. 
 
    The function iteratively adjust the correlation matrix by clipping the 
    eigenvalues of a difference matrix. The diagonal elements are set to one. 
 
    Parameters 
    ---------- 
    corr : ndarray, (k, k) 
        initial correlation matrix 
    threshold : float 
        clipping threshold for smallest eigenvalue, see Notes 
    n_fact : int or float 
        factor to determine the maximum number of iterations. The maximum 
        number of iterations is the integer part of the number of columns in 
        the correlation matrix times n_fact. 
 
    Returns 
    ------- 
    corr_new : ndarray, (optional) 
        corrected correlation matrix 
 
    Notes 
    ----- 
    The smallest eigenvalue of the corrected correlation matrix is 
    approximately equal to the ``threshold``. 
    If the threshold=0, then the smallest eigenvalue of the correlation matrix 
    might be negative, but zero within a numerical error, for example in the 
    range of -1e-16. 
 
    Assumes input correlation matrix is symmetric. 
 
    Stops after the first step if correlation matrix is already positive 
    semi-definite or positive definite, so that smallest eigenvalue is above 
    threshold. In this case, the returned array is not the original, but 
    is equal to it within numerical precision. 
 
    See Also 
    -------- 
    corr_clipped 
    cov_nearest 
 
    '''</span>
    <span class="s1">k_vars = corr.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s3">if </span><span class="s1">k_vars != corr.shape[</span><span class="s4">1</span><span class="s1">]:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;matrix is not square&quot;</span><span class="s1">)</span>

    <span class="s1">diff = np.zeros(corr.shape)</span>
    <span class="s1">x_new = corr.copy()</span>
    <span class="s1">diag_idx = np.arange(k_vars)</span>

    <span class="s3">for </span><span class="s1">ii </span><span class="s3">in </span><span class="s1">range(int(len(corr) * n_fact)):</span>
        <span class="s1">x_adj = x_new - diff</span>
        <span class="s1">x_psd</span><span class="s3">, </span><span class="s1">clipped = clip_evals(x_adj</span><span class="s3">, </span><span class="s1">value=threshold)</span>
        <span class="s3">if not </span><span class="s1">clipped:</span>
            <span class="s1">x_new = x_psd</span>
            <span class="s3">break</span>
        <span class="s1">diff = x_psd - x_adj</span>
        <span class="s1">x_new = x_psd.copy()</span>
        <span class="s1">x_new[diag_idx</span><span class="s3">, </span><span class="s1">diag_idx] = </span><span class="s4">1</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">warnings.warn(iteration_limit_doc</span><span class="s3">, </span><span class="s1">IterationLimitWarning)</span>

    <span class="s3">return </span><span class="s1">x_new</span>


<span class="s3">def </span><span class="s1">corr_clipped(corr</span><span class="s3">, </span><span class="s1">threshold=</span><span class="s4">1e-15</span><span class="s1">):</span>
    <span class="s2">''' 
    Find a near correlation matrix that is positive semi-definite 
 
    This function clips the eigenvalues, replacing eigenvalues smaller than 
    the threshold by the threshold. The new matrix is normalized, so that the 
    diagonal elements are one. 
    Compared to corr_nearest, the distance between the original correlation 
    matrix and the positive definite correlation matrix is larger, however, 
    it is much faster since it only computes eigenvalues once. 
 
    Parameters 
    ---------- 
    corr : ndarray, (k, k) 
        initial correlation matrix 
    threshold : float 
        clipping threshold for smallest eigenvalue, see Notes 
 
    Returns 
    ------- 
    corr_new : ndarray, (optional) 
        corrected correlation matrix 
 
 
    Notes 
    ----- 
    The smallest eigenvalue of the corrected correlation matrix is 
    approximately equal to the ``threshold``. In examples, the 
    smallest eigenvalue can be by a factor of 10 smaller than the threshold, 
    e.g. threshold 1e-8 can result in smallest eigenvalue in the range 
    between 1e-9 and 1e-8. 
    If the threshold=0, then the smallest eigenvalue of the correlation matrix 
    might be negative, but zero within a numerical error, for example in the 
    range of -1e-16. 
 
    Assumes input correlation matrix is symmetric. The diagonal elements of 
    returned correlation matrix is set to ones. 
 
    If the correlation matrix is already positive semi-definite given the 
    threshold, then the original correlation matrix is returned. 
 
    ``cov_clipped`` is 40 or more times faster than ``cov_nearest`` in simple 
    example, but has a slightly larger approximation error. 
 
    See Also 
    -------- 
    corr_nearest 
    cov_nearest 
 
    '''</span>
    <span class="s1">x_new</span><span class="s3">, </span><span class="s1">clipped = clip_evals(corr</span><span class="s3">, </span><span class="s1">value=threshold)</span>
    <span class="s3">if not </span><span class="s1">clipped:</span>
        <span class="s3">return </span><span class="s1">corr</span>

    <span class="s0"># cov2corr</span>
    <span class="s1">x_std = np.sqrt(np.diag(x_new))</span>
    <span class="s1">x_new = x_new / x_std / x_std[:</span><span class="s3">, None</span><span class="s1">]</span>
    <span class="s3">return </span><span class="s1">x_new</span>


<span class="s3">def </span><span class="s1">cov_nearest(cov</span><span class="s3">, </span><span class="s1">method=</span><span class="s5">'clipped'</span><span class="s3">, </span><span class="s1">threshold=</span><span class="s4">1e-15</span><span class="s3">, </span><span class="s1">n_fact=</span><span class="s4">100</span><span class="s3">,</span>
                <span class="s1">return_all=</span><span class="s3">False</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot; 
    Find the nearest covariance matrix that is positive (semi-) definite 
 
    This leaves the diagonal, i.e. the variance, unchanged 
 
    Parameters 
    ---------- 
    cov : ndarray, (k,k) 
        initial covariance matrix 
    method : str 
        if &quot;clipped&quot;, then the faster but less accurate ``corr_clipped`` is 
        used.if &quot;nearest&quot;, then ``corr_nearest`` is used 
    threshold : float 
        clipping threshold for smallest eigen value, see Notes 
    n_fact : int or float 
        factor to determine the maximum number of iterations in 
        ``corr_nearest``. See its doc string 
    return_all : bool 
        if False (default), then only the covariance matrix is returned. 
        If True, then correlation matrix and standard deviation are 
        additionally returned. 
 
    Returns 
    ------- 
    cov_ : ndarray 
        corrected covariance matrix 
    corr_ : ndarray, (optional) 
        corrected correlation matrix 
    std_ : ndarray, (optional) 
        standard deviation 
 
 
    Notes 
    ----- 
    This converts the covariance matrix to a correlation matrix. Then, finds 
    the nearest correlation matrix that is positive semidefinite and converts 
    it back to a covariance matrix using the initial standard deviation. 
 
    The smallest eigenvalue of the intermediate correlation matrix is 
    approximately equal to the ``threshold``. 
    If the threshold=0, then the smallest eigenvalue of the correlation matrix 
    might be negative, but zero within a numerical error, for example in the 
    range of -1e-16. 
 
    Assumes input covariance matrix is symmetric. 
 
    See Also 
    -------- 
    corr_nearest 
    corr_clipped 
    &quot;&quot;&quot;</span>

    <span class="s3">from </span><span class="s1">statsmodels.stats.moment_helpers </span><span class="s3">import </span><span class="s1">cov2corr</span><span class="s3">, </span><span class="s1">corr2cov</span>
    <span class="s1">cov_</span><span class="s3">, </span><span class="s1">std_ = cov2corr(cov</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">method == </span><span class="s5">'clipped'</span><span class="s1">:</span>
        <span class="s1">corr_ = corr_clipped(cov_</span><span class="s3">, </span><span class="s1">threshold=threshold)</span>
    <span class="s3">else</span><span class="s1">:  </span><span class="s0"># method == 'nearest'</span>
        <span class="s1">corr_ = corr_nearest(cov_</span><span class="s3">, </span><span class="s1">threshold=threshold</span><span class="s3">, </span><span class="s1">n_fact=n_fact)</span>

    <span class="s1">cov_ = corr2cov(corr_</span><span class="s3">, </span><span class="s1">std_)</span>

    <span class="s3">if </span><span class="s1">return_all:</span>
        <span class="s3">return </span><span class="s1">cov_</span><span class="s3">, </span><span class="s1">corr_</span><span class="s3">, </span><span class="s1">std_</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">cov_</span>


<span class="s3">def </span><span class="s1">_nmono_linesearch(obj</span><span class="s3">, </span><span class="s1">grad</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">d</span><span class="s3">, </span><span class="s1">obj_hist</span><span class="s3">, </span><span class="s1">M=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">sig1=</span><span class="s4">0.1</span><span class="s3">,</span>
                      <span class="s1">sig2=</span><span class="s4">0.9</span><span class="s3">, </span><span class="s1">gam=</span><span class="s4">1e-4</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s4">100</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot; 
    Implements the non-monotone line search of Grippo et al. (1986), 
    as described in Birgin, Martinez and Raydan (2013). 
 
    Parameters 
    ---------- 
    obj : real-valued function 
        The objective function, to be minimized 
    grad : vector-valued function 
        The gradient of the objective function 
    x : array_like 
        The starting point for the line search 
    d : array_like 
        The search direction 
    obj_hist : array_like 
        Objective function history (must contain at least one value) 
    M : positive int 
        Number of previous function points to consider (see references 
        for details). 
    sig1 : real 
        Tuning parameter, see references for details. 
    sig2 : real 
        Tuning parameter, see references for details. 
    gam : real 
        Tuning parameter, see references for details. 
    maxiter : int 
        The maximum number of iterations; returns Nones if convergence 
        does not occur by this point 
 
    Returns 
    ------- 
    alpha : real 
        The step value 
    x : Array_like 
        The function argument at the final step 
    obval : Real 
        The function value at the final step 
    g : Array_like 
        The gradient at the final step 
 
    Notes 
    ----- 
    The basic idea is to take a big step in the direction of the 
    gradient, even if the function value is not decreased (but there 
    is a maximum allowed increase in terms of the recent history of 
    the iterates). 
 
    References 
    ---------- 
    Grippo L, Lampariello F, Lucidi S (1986). A Nonmonotone Line 
    Search Technique for Newton's Method. SIAM Journal on Numerical 
    Analysis, 23, 707-716. 
 
    E. Birgin, J.M. Martinez, and M. Raydan. Spectral projected 
    gradient methods: Review and perspectives. Journal of Statistical 
    Software (preprint). 
    &quot;&quot;&quot;</span>

    <span class="s1">alpha = </span><span class="s4">1.</span>
    <span class="s1">last_obval = obj(x)</span>
    <span class="s1">obj_max = max(obj_hist[-M:])</span>

    <span class="s3">for </span><span class="s1">iter </span><span class="s3">in </span><span class="s1">range(maxiter):</span>

        <span class="s1">obval = obj(x + alpha*d)</span>
        <span class="s1">g = grad(x)</span>
        <span class="s1">gtd = (g * d).sum()</span>

        <span class="s3">if </span><span class="s1">obval &lt;= obj_max + gam*alpha*gtd:</span>
            <span class="s3">return </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">x + alpha*d</span><span class="s3">, </span><span class="s1">obval</span><span class="s3">, </span><span class="s1">g</span>

        <span class="s1">a1 = -</span><span class="s4">0.5</span><span class="s1">*alpha**</span><span class="s4">2</span><span class="s1">*gtd / (obval - last_obval - alpha*gtd)</span>

        <span class="s3">if </span><span class="s1">(sig1 &lt;= a1) </span><span class="s3">and </span><span class="s1">(a1 &lt;= sig2*alpha):</span>
            <span class="s1">alpha = a1</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha /= </span><span class="s4">2.</span>

        <span class="s1">last_obval = obval</span>

    <span class="s3">return None, None, None, None</span>


<span class="s3">def </span><span class="s1">_spg_optim(func</span><span class="s3">, </span><span class="s1">grad</span><span class="s3">, </span><span class="s1">start</span><span class="s3">, </span><span class="s1">project</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s4">1e4</span><span class="s3">, </span><span class="s1">M=</span><span class="s4">10</span><span class="s3">,</span>
               <span class="s1">ctol=</span><span class="s4">1e-3</span><span class="s3">, </span><span class="s1">maxiter_nmls=</span><span class="s4">200</span><span class="s3">, </span><span class="s1">lam_min=</span><span class="s4">1e-30</span><span class="s3">,</span>
               <span class="s1">lam_max=</span><span class="s4">1e30</span><span class="s3">, </span><span class="s1">sig1=</span><span class="s4">0.1</span><span class="s3">, </span><span class="s1">sig2=</span><span class="s4">0.9</span><span class="s3">, </span><span class="s1">gam=</span><span class="s4">1e-4</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot; 
    Implements the spectral projected gradient method for minimizing a 
    differentiable function on a convex domain. 
 
    Parameters 
    ---------- 
    func : real valued function 
        The objective function to be minimized. 
    grad : real array-valued function 
        The gradient of the objective function 
    start : array_like 
        The starting point 
    project : function 
        In-place projection of the argument to the domain 
        of func. 
    ... See notes regarding additional arguments 
 
    Returns 
    ------- 
    rslt : Bunch 
        rslt.params is the final iterate, other fields describe 
        convergence status. 
 
    Notes 
    ----- 
    This can be an effective heuristic algorithm for problems where no 
    guaranteed algorithm for computing a global minimizer is known. 
 
    There are a number of tuning parameters, but these generally 
    should not be changed except for `maxiter` (positive integer) and 
    `ctol` (small positive real).  See the Birgin et al reference for 
    more information about the tuning parameters. 
 
    Reference 
    --------- 
    E. Birgin, J.M. Martinez, and M. Raydan. Spectral projected 
    gradient methods: Review and perspectives. Journal of Statistical 
    Software (preprint).  Available at: 
    http://www.ime.usp.br/~egbirgin/publications/bmr5.pdf 
    &quot;&quot;&quot;</span>

    <span class="s1">lam = min(</span><span class="s4">10</span><span class="s1">*lam_min</span><span class="s3">, </span><span class="s1">lam_max)</span>

    <span class="s1">params = start.copy()</span>
    <span class="s1">gval = grad(params)</span>

    <span class="s1">obj_hist = [func(params)</span><span class="s3">, </span><span class="s1">]</span>

    <span class="s3">for </span><span class="s1">itr </span><span class="s3">in </span><span class="s1">range(int(maxiter)):</span>

        <span class="s0"># Check convergence</span>
        <span class="s1">df = params - gval</span>
        <span class="s1">project(df)</span>
        <span class="s1">df -= params</span>
        <span class="s3">if </span><span class="s1">np.max(np.abs(df)) &lt; ctol:</span>
            <span class="s3">return </span><span class="s1">Bunch(**{</span><span class="s5">&quot;Converged&quot;</span><span class="s1">: </span><span class="s3">True, </span><span class="s5">&quot;params&quot;</span><span class="s1">: params</span><span class="s3">,</span>
                            <span class="s5">&quot;objective_values&quot;</span><span class="s1">: obj_hist</span><span class="s3">,</span>
                            <span class="s5">&quot;Message&quot;</span><span class="s1">: </span><span class="s5">&quot;Converged successfully&quot;</span><span class="s1">})</span>

        <span class="s0"># The line search direction</span>
        <span class="s1">d = params - lam*gval</span>
        <span class="s1">project(d)</span>
        <span class="s1">d -= params</span>

        <span class="s0"># Carry out the nonmonotone line search</span>
        <span class="s1">alpha</span><span class="s3">, </span><span class="s1">params1</span><span class="s3">, </span><span class="s1">fval</span><span class="s3">, </span><span class="s1">gval1 = _nmono_linesearch(</span>
            <span class="s1">func</span><span class="s3">,</span>
            <span class="s1">grad</span><span class="s3">,</span>
            <span class="s1">params</span><span class="s3">,</span>
            <span class="s1">d</span><span class="s3">,</span>
            <span class="s1">obj_hist</span><span class="s3">,</span>
            <span class="s1">M=M</span><span class="s3">,</span>
            <span class="s1">sig1=sig1</span><span class="s3">,</span>
            <span class="s1">sig2=sig2</span><span class="s3">,</span>
            <span class="s1">gam=gam</span><span class="s3">,</span>
            <span class="s1">maxiter=maxiter_nmls)</span>

        <span class="s3">if </span><span class="s1">alpha </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">Bunch(**{</span><span class="s5">&quot;Converged&quot;</span><span class="s1">: </span><span class="s3">False, </span><span class="s5">&quot;params&quot;</span><span class="s1">: params</span><span class="s3">,</span>
                            <span class="s5">&quot;objective_values&quot;</span><span class="s1">: obj_hist</span><span class="s3">,</span>
                            <span class="s5">&quot;Message&quot;</span><span class="s1">: </span><span class="s5">&quot;Failed in nmono_linesearch&quot;</span><span class="s1">})</span>

        <span class="s1">obj_hist.append(fval)</span>
        <span class="s1">s = params1 - params</span>
        <span class="s1">y = gval1 - gval</span>

        <span class="s1">sy = (s*y).sum()</span>
        <span class="s3">if </span><span class="s1">sy &lt;= </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">lam = lam_max</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">ss = (s*s).sum()</span>
            <span class="s1">lam = max(lam_min</span><span class="s3">, </span><span class="s1">min(ss/sy</span><span class="s3">, </span><span class="s1">lam_max))</span>

        <span class="s1">params = params1</span>
        <span class="s1">gval = gval1</span>

    <span class="s3">return </span><span class="s1">Bunch(**{</span><span class="s5">&quot;Converged&quot;</span><span class="s1">: </span><span class="s3">False, </span><span class="s5">&quot;params&quot;</span><span class="s1">: params</span><span class="s3">,</span>
                    <span class="s5">&quot;objective_values&quot;</span><span class="s1">: obj_hist</span><span class="s3">,</span>
                    <span class="s5">&quot;Message&quot;</span><span class="s1">: </span><span class="s5">&quot;spg_optim did not converge&quot;</span><span class="s1">})</span>


<span class="s3">def </span><span class="s1">_project_correlation_factors(X):</span>
    <span class="s2">&quot;&quot;&quot; 
    Project a matrix into the domain of matrices whose row-wise sums 
    of squares are less than or equal to 1. 
 
    The input matrix is modified in-place. 
    &quot;&quot;&quot;</span>
    <span class="s1">nm = np.sqrt((X*X).sum(</span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">ii = np.flatnonzero(nm &gt; </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">len(ii) &gt; </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s1">X[ii</span><span class="s3">, </span><span class="s1">:] /= nm[ii][:</span><span class="s3">, None</span><span class="s1">]</span>


<span class="s3">class </span><span class="s1">FactoredPSDMatrix:</span>
    <span class="s2">&quot;&quot;&quot; 
    Representation of a positive semidefinite matrix in factored form. 
 
    The representation is constructed based on a vector `diag` and 
    rectangular matrix `root`, such that the PSD matrix represented by 
    the class instance is Diag + root * root', where Diag is the 
    square diagonal matrix with `diag` on its main diagonal. 
 
    Parameters 
    ---------- 
    diag : 1d array_like 
        See above 
    root : 2d array_like 
        See above 
 
    Notes 
    ----- 
    The matrix is represented internally in the form Diag^{1/2}(I + 
    factor * scales * factor')Diag^{1/2}, where `Diag` and `scales` 
    are diagonal matrices, and `factor` is an orthogonal matrix. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">diag</span><span class="s3">, </span><span class="s1">root):</span>
        <span class="s1">self.diag = diag</span>
        <span class="s1">self.root = root</span>
        <span class="s1">root = root / np.sqrt(diag)[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s1">u</span><span class="s3">, </span><span class="s1">s</span><span class="s3">, </span><span class="s1">vt = np.linalg.svd(root</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">self.factor = u</span>
        <span class="s1">self.scales = s**</span><span class="s4">2</span>

    <span class="s3">def </span><span class="s1">to_matrix(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Returns the PSD matrix represented by this instance as a full 
        (square) matrix. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.diag(self.diag) + np.dot(self.root</span><span class="s3">, </span><span class="s1">self.root.T)</span>

    <span class="s3">def </span><span class="s1">decorrelate(self</span><span class="s3">, </span><span class="s1">rhs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Decorrelate the columns of `rhs`. 
 
        Parameters 
        ---------- 
        rhs : array_like 
            A 2 dimensional array with the same number of rows as the 
            PSD matrix represented by the class instance. 
 
        Returns 
        ------- 
        C^{-1/2} * rhs, where C is the covariance matrix represented 
        by this class instance. 
 
        Notes 
        ----- 
        The returned matrix has the identity matrix as its row-wise 
        population covariance matrix. 
 
        This function exploits the factor structure for efficiency. 
        &quot;&quot;&quot;</span>

        <span class="s0"># I + factor * qval * factor' is the inverse square root of</span>
        <span class="s0"># the covariance matrix in the homogeneous case where diag =</span>
        <span class="s0"># 1.</span>
        <span class="s1">qval = -</span><span class="s4">1 </span><span class="s1">+ </span><span class="s4">1 </span><span class="s1">/ np.sqrt(</span><span class="s4">1 </span><span class="s1">+ self.scales)</span>

        <span class="s0"># Decorrelate in the general case.</span>
        <span class="s1">rhs = rhs / np.sqrt(self.diag)[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s1">rhs1 = np.dot(self.factor.T</span><span class="s3">, </span><span class="s1">rhs)</span>
        <span class="s1">rhs1 *= qval[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s1">rhs1 = np.dot(self.factor</span><span class="s3">, </span><span class="s1">rhs1)</span>
        <span class="s1">rhs += rhs1</span>

        <span class="s3">return </span><span class="s1">rhs</span>

    <span class="s3">def </span><span class="s1">solve(self</span><span class="s3">, </span><span class="s1">rhs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Solve a linear system of equations with factor-structured 
        coefficients. 
 
        Parameters 
        ---------- 
        rhs : array_like 
            A 2 dimensional array with the same number of rows as the 
            PSD matrix represented by the class instance. 
 
        Returns 
        ------- 
        C^{-1} * rhs, where C is the covariance matrix represented 
        by this class instance. 
 
        Notes 
        ----- 
        This function exploits the factor structure for efficiency. 
        &quot;&quot;&quot;</span>

        <span class="s1">qval = -self.scales / (</span><span class="s4">1 </span><span class="s1">+ self.scales)</span>
        <span class="s1">dr = np.sqrt(self.diag)</span>
        <span class="s1">rhs = rhs / dr[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s1">mat = qval[:</span><span class="s3">, None</span><span class="s1">] * np.dot(self.factor.T</span><span class="s3">, </span><span class="s1">rhs)</span>
        <span class="s1">rhs = rhs + np.dot(self.factor</span><span class="s3">, </span><span class="s1">mat)</span>
        <span class="s3">return </span><span class="s1">rhs / dr[:</span><span class="s3">, None</span><span class="s1">]</span>

    <span class="s3">def </span><span class="s1">logdet(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Returns the logarithm of the determinant of a 
        factor-structured matrix. 
        &quot;&quot;&quot;</span>

        <span class="s1">logdet = np.sum(np.log(self.diag))</span>
        <span class="s1">logdet += np.sum(np.log(self.scales))</span>
        <span class="s1">logdet += np.sum(np.log(</span><span class="s4">1 </span><span class="s1">+ </span><span class="s4">1 </span><span class="s1">/ self.scales))</span>

        <span class="s3">return </span><span class="s1">logdet</span>


<span class="s3">def </span><span class="s1">corr_nearest_factor(corr</span><span class="s3">, </span><span class="s1">rank</span><span class="s3">, </span><span class="s1">ctol=</span><span class="s4">1e-6</span><span class="s3">, </span><span class="s1">lam_min=</span><span class="s4">1e-30</span><span class="s3">,</span>
                        <span class="s1">lam_max=</span><span class="s4">1e30</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s4">1000</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot; 
    Find the nearest correlation matrix with factor structure to a 
    given square matrix. 
 
    Parameters 
    ---------- 
    corr : square array 
        The target matrix (to which the nearest correlation matrix is 
        sought).  Must be square, but need not be positive 
        semidefinite. 
    rank : int 
        The rank of the factor structure of the solution, i.e., the 
        number of linearly independent columns of X. 
    ctol : positive real 
        Convergence criterion. 
    lam_min : float 
        Tuning parameter for spectral projected gradient optimization 
        (smallest allowed step in the search direction). 
    lam_max : float 
        Tuning parameter for spectral projected gradient optimization 
        (largest allowed step in the search direction). 
    maxiter : int 
        Maximum number of iterations in spectral projected gradient 
        optimization. 
 
    Returns 
    ------- 
    rslt : Bunch 
        rslt.corr is a FactoredPSDMatrix defining the estimated 
        correlation structure.  Other fields of `rslt` contain 
        returned values from spg_optim. 
 
    Notes 
    ----- 
    A correlation matrix has factor structure if it can be written in 
    the form I + XX' - diag(XX'), where X is n x k with linearly 
    independent columns, and with each row having sum of squares at 
    most equal to 1.  The approximation is made in terms of the 
    Frobenius norm. 
 
    This routine is useful when one has an approximate correlation 
    matrix that is not positive semidefinite, and there is need to 
    estimate the inverse, square root, or inverse square root of the 
    population correlation matrix.  The factor structure allows these 
    tasks to be done without constructing any n x n matrices. 
 
    This is a non-convex problem with no known guaranteed globally 
    convergent algorithm for computing the solution.  Borsdof, Higham 
    and Raydan (2010) compared several methods for this problem and 
    found the spectral projected gradient (SPG) method (used here) to 
    perform best. 
 
    The input matrix `corr` can be a dense numpy array or any scipy 
    sparse matrix.  The latter is useful if the input matrix is 
    obtained by thresholding a very large sample correlation matrix. 
    If `corr` is sparse, the calculations are optimized to save 
    memory, so no working matrix with more than 10^6 elements is 
    constructed. 
 
    References 
    ---------- 
    .. [*] R Borsdof, N Higham, M Raydan (2010).  Computing a nearest 
       correlation matrix with factor structure. SIAM J Matrix Anal Appl, 
       31:5, 2603-2622. 
       http://eprints.ma.man.ac.uk/1523/01/covered/MIMS_ep2009_87.pdf 
 
    Examples 
    -------- 
    Hard thresholding a correlation matrix may result in a matrix that 
    is not positive semidefinite.  We can approximate a hard 
    thresholded correlation matrix with a PSD matrix as follows, where 
    `corr` is the input correlation matrix. 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from statsmodels.stats.correlation_tools import corr_nearest_factor 
    &gt;&gt;&gt; np.random.seed(1234) 
    &gt;&gt;&gt; b = 1.5 - np.random.rand(10, 1) 
    &gt;&gt;&gt; x = np.random.randn(100,1).dot(b.T) + np.random.randn(100,10) 
    &gt;&gt;&gt; corr = np.corrcoef(x.T) 
    &gt;&gt;&gt; corr = corr * (np.abs(corr) &gt;= 0.3) 
    &gt;&gt;&gt; rslt = corr_nearest_factor(corr, 3) 
    &quot;&quot;&quot;</span>

    <span class="s1">p</span><span class="s3">, </span><span class="s1">_ = corr.shape</span>

    <span class="s0"># Starting values (following the PCA method in BHR).</span>
    <span class="s1">u</span><span class="s3">, </span><span class="s1">s</span><span class="s3">, </span><span class="s1">vt = svds(corr</span><span class="s3">, </span><span class="s1">rank)</span>
    <span class="s1">X = u * np.sqrt(s)</span>
    <span class="s1">nm = np.sqrt((X**</span><span class="s4">2</span><span class="s1">).sum(</span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">ii = np.flatnonzero(nm &gt; </span><span class="s4">1e-5</span><span class="s1">)</span>
    <span class="s1">X[ii</span><span class="s3">, </span><span class="s1">:] /= nm[ii][:</span><span class="s3">, None</span><span class="s1">]</span>

    <span class="s0"># Zero the diagonal</span>
    <span class="s1">corr1 = corr.copy()</span>
    <span class="s3">if </span><span class="s1">type(corr1) == np.ndarray:</span>
        <span class="s1">np.fill_diagonal(corr1</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s3">elif </span><span class="s1">sparse.issparse(corr1):</span>
        <span class="s1">corr1.setdiag(np.zeros(corr1.shape[</span><span class="s4">0</span><span class="s1">]))</span>
        <span class="s1">corr1.eliminate_zeros()</span>
        <span class="s1">corr1.sort_indices()</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Matrix type not supported&quot;</span><span class="s1">)</span>

    <span class="s0"># The gradient, from lemma 4.1 of BHR.</span>
    <span class="s3">def </span><span class="s1">grad(X):</span>
        <span class="s1">gr = np.dot(X</span><span class="s3">, </span><span class="s1">np.dot(X.T</span><span class="s3">, </span><span class="s1">X))</span>
        <span class="s3">if </span><span class="s1">type(corr1) == np.ndarray:</span>
            <span class="s1">gr -= np.dot(corr1</span><span class="s3">, </span><span class="s1">X)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">gr -= corr1.dot(X)</span>
        <span class="s1">gr -= (X*X).sum(</span><span class="s4">1</span><span class="s1">)[:</span><span class="s3">, None</span><span class="s1">] * X</span>
        <span class="s3">return </span><span class="s4">4</span><span class="s1">*gr</span>

    <span class="s0"># The objective function (sum of squared deviations between fitted</span>
    <span class="s0"># and observed arrays).</span>
    <span class="s3">def </span><span class="s1">func(X):</span>
        <span class="s3">if </span><span class="s1">type(corr1) == np.ndarray:</span>
            <span class="s1">M = np.dot(X</span><span class="s3">, </span><span class="s1">X.T)</span>
            <span class="s1">np.fill_diagonal(M</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">M -= corr1</span>
            <span class="s1">fval = (M*M).sum()</span>
            <span class="s3">return </span><span class="s1">fval</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">fval = </span><span class="s4">0.</span>
            <span class="s0"># Control the size of intermediates</span>
            <span class="s1">max_ws = </span><span class="s4">1e6</span>
            <span class="s1">bs = int(max_ws / X.shape[</span><span class="s4">0</span><span class="s1">])</span>
            <span class="s1">ir = </span><span class="s4">0</span>
            <span class="s3">while </span><span class="s1">ir &lt; X.shape[</span><span class="s4">0</span><span class="s1">]:</span>
                <span class="s1">ir2 = min(ir+bs</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s4">0</span><span class="s1">])</span>
                <span class="s1">u = np.dot(X[ir:ir2</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">X.T)</span>
                <span class="s1">ii = np.arange(u.shape[</span><span class="s4">0</span><span class="s1">])</span>
                <span class="s1">u[ii</span><span class="s3">, </span><span class="s1">ir+ii] = </span><span class="s4">0</span>
                <span class="s1">u -= np.asarray(corr1[ir:ir2</span><span class="s3">, </span><span class="s1">:].todense())</span>
                <span class="s1">fval += (u*u).sum()</span>
                <span class="s1">ir += bs</span>
            <span class="s3">return </span><span class="s1">fval</span>

    <span class="s1">rslt = _spg_optim(func</span><span class="s3">, </span><span class="s1">grad</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">_project_correlation_factors</span><span class="s3">, </span><span class="s1">ctol=ctol</span><span class="s3">,</span>
                      <span class="s1">lam_min=lam_min</span><span class="s3">, </span><span class="s1">lam_max=lam_max</span><span class="s3">, </span><span class="s1">maxiter=maxiter)</span>
    <span class="s1">root = rslt.params</span>
    <span class="s1">diag = </span><span class="s4">1 </span><span class="s1">- (root**</span><span class="s4">2</span><span class="s1">).sum(</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">soln = FactoredPSDMatrix(diag</span><span class="s3">, </span><span class="s1">root)</span>
    <span class="s1">rslt.corr = soln</span>
    <span class="s3">del </span><span class="s1">rslt.params</span>
    <span class="s3">return </span><span class="s1">rslt</span>


<span class="s3">def </span><span class="s1">cov_nearest_factor_homog(cov</span><span class="s3">, </span><span class="s1">rank):</span>
    <span class="s2">&quot;&quot;&quot; 
    Approximate an arbitrary square matrix with a factor-structured 
    matrix of the form k*I + XX'. 
 
    Parameters 
    ---------- 
    cov : array_like 
        The input array, must be square but need not be positive 
        semidefinite 
    rank : int 
        The rank of the fitted factor structure 
 
    Returns 
    ------- 
    A FactoredPSDMatrix instance containing the fitted matrix 
 
    Notes 
    ----- 
    This routine is useful if one has an estimated covariance matrix 
    that is not SPD, and the ultimate goal is to estimate the inverse, 
    square root, or inverse square root of the true covariance 
    matrix. The factor structure allows these tasks to be performed 
    without constructing any n x n matrices. 
 
    The calculations use the fact that if k is known, then X can be 
    determined from the eigen-decomposition of cov - k*I, which can 
    in turn be easily obtained form the eigen-decomposition of `cov`. 
    Thus the problem can be reduced to a 1-dimensional search for k 
    that does not require repeated eigen-decompositions. 
 
    If the input matrix is sparse, then cov - k*I is also sparse, so 
    the eigen-decomposition can be done efficiently using sparse 
    routines. 
 
    The one-dimensional search for the optimal value of k is not 
    convex, so a local minimum could be obtained. 
 
    Examples 
    -------- 
    Hard thresholding a covariance matrix may result in a matrix that 
    is not positive semidefinite.  We can approximate a hard 
    thresholded covariance matrix with a PSD matrix as follows: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; np.random.seed(1234) 
    &gt;&gt;&gt; b = 1.5 - np.random.rand(10, 1) 
    &gt;&gt;&gt; x = np.random.randn(100,1).dot(b.T) + np.random.randn(100,10) 
    &gt;&gt;&gt; cov = np.cov(x) 
    &gt;&gt;&gt; cov = cov * (np.abs(cov) &gt;= 0.3) 
    &gt;&gt;&gt; rslt = cov_nearest_factor_homog(cov, 3) 
    &quot;&quot;&quot;</span>

    <span class="s1">m</span><span class="s3">, </span><span class="s1">n = cov.shape</span>

    <span class="s1">Q</span><span class="s3">, </span><span class="s1">Lambda</span><span class="s3">, </span><span class="s1">_ = svds(cov</span><span class="s3">, </span><span class="s1">rank)</span>

    <span class="s3">if </span><span class="s1">sparse.issparse(cov):</span>
        <span class="s1">QSQ = np.dot(Q.T</span><span class="s3">, </span><span class="s1">cov.dot(Q))</span>
        <span class="s1">ts = cov.diagonal().sum()</span>
        <span class="s1">tss = cov.dot(cov).diagonal().sum()</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">QSQ = np.dot(Q.T</span><span class="s3">, </span><span class="s1">np.dot(cov</span><span class="s3">, </span><span class="s1">Q))</span>
        <span class="s1">ts = np.trace(cov)</span>
        <span class="s1">tss = np.trace(np.dot(cov</span><span class="s3">, </span><span class="s1">cov))</span>

    <span class="s3">def </span><span class="s1">fun(k):</span>
        <span class="s1">Lambda_t = Lambda - k</span>
        <span class="s1">v = tss + m*(k**</span><span class="s4">2</span><span class="s1">) + np.sum(Lambda_t**</span><span class="s4">2</span><span class="s1">) - </span><span class="s4">2</span><span class="s1">*k*ts</span>
        <span class="s1">v += </span><span class="s4">2</span><span class="s1">*k*np.sum(Lambda_t) - </span><span class="s4">2</span><span class="s1">*np.sum(np.diag(QSQ) * Lambda_t)</span>
        <span class="s3">return </span><span class="s1">v</span>

    <span class="s0"># Get the optimal decomposition</span>
    <span class="s1">k_opt = fminbound(fun</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1e5</span><span class="s1">)</span>
    <span class="s1">Lambda_opt = Lambda - k_opt</span>
    <span class="s1">fac_opt = Q * np.sqrt(Lambda_opt)</span>

    <span class="s1">diag = k_opt * np.ones(m</span><span class="s3">, </span><span class="s1">dtype=np.float64)  </span><span class="s0"># - (fac_opt**2).sum(1)</span>
    <span class="s3">return </span><span class="s1">FactoredPSDMatrix(diag</span><span class="s3">, </span><span class="s1">fac_opt)</span>


<span class="s3">def </span><span class="s1">corr_thresholded(data</span><span class="s3">, </span><span class="s1">minabs=</span><span class="s3">None, </span><span class="s1">max_elt=</span><span class="s4">1e7</span><span class="s1">):</span>
    <span class="s2">r&quot;&quot;&quot; 
    Construct a sparse matrix containing the thresholded row-wise 
    correlation matrix from a data array. 
 
    Parameters 
    ---------- 
    data : array_like 
        The data from which the row-wise thresholded correlation 
        matrix is to be computed. 
    minabs : non-negative real 
        The threshold value; correlation coefficients smaller in 
        magnitude than minabs are set to zero.  If None, defaults 
        to 1 / sqrt(n), see Notes for more information. 
 
    Returns 
    ------- 
    cormat : sparse.coo_matrix 
        The thresholded correlation matrix, in COO format. 
 
    Notes 
    ----- 
    This is an alternative to C = np.corrcoef(data); C \*= (np.abs(C) 
    &gt;= absmin), suitable for very tall data matrices. 
 
    If the data are jointly Gaussian, the marginal sampling 
    distributions of the elements of the sample correlation matrix are 
    approximately Gaussian with standard deviation 1 / sqrt(n).  The 
    default value of ``minabs`` is thus equal to 1 standard error, which 
    will set to zero approximately 68% of the estimated correlation 
    coefficients for which the population value is zero. 
 
    No intermediate matrix with more than ``max_elt`` values will be 
    constructed.  However memory use could still be high if a large 
    number of correlation values exceed `minabs` in magnitude. 
 
    The thresholded matrix is returned in COO format, which can easily 
    be converted to other sparse formats. 
 
    Examples 
    -------- 
    Here X is a tall data matrix (e.g. with 100,000 rows and 50 
    columns).  The row-wise correlation matrix of X is calculated 
    and stored in sparse form, with all entries smaller than 0.3 
    treated as 0. 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; np.random.seed(1234) 
    &gt;&gt;&gt; b = 1.5 - np.random.rand(10, 1) 
    &gt;&gt;&gt; x = np.random.randn(100,1).dot(b.T) + np.random.randn(100,10) 
    &gt;&gt;&gt; cmat = corr_thresholded(x, 0.3) 
    &quot;&quot;&quot;</span>

    <span class="s1">nrow</span><span class="s3">, </span><span class="s1">ncol = data.shape</span>

    <span class="s3">if </span><span class="s1">minabs </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">minabs = </span><span class="s4">1. </span><span class="s1">/ float(ncol)</span>

    <span class="s0"># Row-standardize the data</span>
    <span class="s1">data = data.copy()</span>
    <span class="s1">data -= data.mean(</span><span class="s4">1</span><span class="s1">)[:</span><span class="s3">, None</span><span class="s1">]</span>
    <span class="s1">sd = data.std(</span><span class="s4">1</span><span class="s3">, </span><span class="s1">ddof=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">ii = np.flatnonzero(sd &gt; </span><span class="s4">1e-5</span><span class="s1">)</span>
    <span class="s1">data[ii</span><span class="s3">, </span><span class="s1">:] /= sd[ii][:</span><span class="s3">, None</span><span class="s1">]</span>
    <span class="s1">ii = np.flatnonzero(sd &lt;= </span><span class="s4">1e-5</span><span class="s1">)</span>
    <span class="s1">data[ii</span><span class="s3">, </span><span class="s1">:] = </span><span class="s4">0</span>

    <span class="s0"># Number of rows to process in one pass</span>
    <span class="s1">bs = int(np.floor(max_elt / nrow))</span>

    <span class="s1">ipos_all</span><span class="s3">, </span><span class="s1">jpos_all</span><span class="s3">, </span><span class="s1">cor_values = []</span><span class="s3">, </span><span class="s1">[]</span><span class="s3">, </span><span class="s1">[]</span>

    <span class="s1">ir = </span><span class="s4">0</span>
    <span class="s3">while </span><span class="s1">ir &lt; nrow:</span>
        <span class="s1">ir2 = min(data.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">ir + bs)</span>
        <span class="s1">cm = np.dot(data[ir:ir2</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">data.T) / (ncol - </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">cma = np.abs(cm)</span>
        <span class="s1">ipos</span><span class="s3">, </span><span class="s1">jpos = np.nonzero(cma &gt;= minabs)</span>
        <span class="s1">ipos_all.append(ipos + ir)</span>
        <span class="s1">jpos_all.append(jpos)</span>
        <span class="s1">cor_values.append(cm[ipos</span><span class="s3">, </span><span class="s1">jpos])</span>
        <span class="s1">ir += bs</span>

    <span class="s1">ipos = np.concatenate(ipos_all)</span>
    <span class="s1">jpos = np.concatenate(jpos_all)</span>
    <span class="s1">cor_values = np.concatenate(cor_values)</span>

    <span class="s1">cmat = sparse.coo_matrix((cor_values</span><span class="s3">, </span><span class="s1">(ipos</span><span class="s3">, </span><span class="s1">jpos))</span><span class="s3">, </span><span class="s1">(nrow</span><span class="s3">, </span><span class="s1">nrow))</span>

    <span class="s3">return </span><span class="s1">cmat</span>


<span class="s3">class </span><span class="s1">MultivariateKernel:</span>
    <span class="s2">&quot;&quot;&quot; 
    Base class for multivariate kernels. 
 
    An instance of MultivariateKernel implements a `call` method having 
    signature `call(x, loc)`, returning the kernel weights comparing `x` 
    (a 1d ndarray) to each row of `loc` (a 2d ndarray). 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">call(self</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">loc):</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">def </span><span class="s1">set_bandwidth(self</span><span class="s3">, </span><span class="s1">bw):</span>
        <span class="s2">&quot;&quot;&quot; 
        Set the bandwidth to the given vector. 
 
        Parameters 
        ---------- 
        bw : array_like 
            A vector of non-negative bandwidth values. 
        &quot;&quot;&quot;</span>

        <span class="s1">self.bw = bw</span>
        <span class="s1">self._setup()</span>

    <span class="s3">def </span><span class="s1">_setup(self):</span>

        <span class="s0"># Precompute the squared bandwidth values.</span>
        <span class="s1">self.bwk = np.prod(self.bw)</span>
        <span class="s1">self.bw2 = self.bw * self.bw</span>

    <span class="s3">def </span><span class="s1">set_default_bw(self</span><span class="s3">, </span><span class="s1">loc</span><span class="s3">, </span><span class="s1">bwm=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Set default bandwiths based on domain values. 
 
        Parameters 
        ---------- 
        loc : array_like 
            Values from the domain to which the kernel will 
            be applied. 
        bwm : scalar, optional 
            A non-negative scalar that is used to multiply 
            the default bandwidth. 
        &quot;&quot;&quot;</span>

        <span class="s1">sd = loc.std(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">q25</span><span class="s3">, </span><span class="s1">q75 = np.percentile(loc</span><span class="s3">, </span><span class="s1">[</span><span class="s4">25</span><span class="s3">, </span><span class="s4">75</span><span class="s1">]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">iqr = (q75 - q25) / </span><span class="s4">1.349</span>
        <span class="s1">bw = np.where(iqr &lt; sd</span><span class="s3">, </span><span class="s1">iqr</span><span class="s3">, </span><span class="s1">sd)</span>
        <span class="s1">bw *= </span><span class="s4">0.9 </span><span class="s1">/ loc.shape[</span><span class="s4">0</span><span class="s1">] ** </span><span class="s4">0.2</span>

        <span class="s3">if </span><span class="s1">bwm </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">bw *= bwm</span>

        <span class="s0"># The final bandwidths</span>
        <span class="s1">self.bw = np.asarray(bw</span><span class="s3">, </span><span class="s1">dtype=np.float64)</span>

        <span class="s1">self._setup()</span>


<span class="s3">class </span><span class="s1">GaussianMultivariateKernel(MultivariateKernel):</span>
    <span class="s2">&quot;&quot;&quot; 
    The Gaussian (squared exponential) multivariate kernel. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">call(self</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">loc):</span>
        <span class="s3">return </span><span class="s1">np.exp(-(x - loc)**</span><span class="s4">2 </span><span class="s1">/ (</span><span class="s4">2 </span><span class="s1">* self.bw2)).sum(</span><span class="s4">1</span><span class="s1">) / self.bwk</span>


<span class="s3">def </span><span class="s1">kernel_covariance(exog</span><span class="s3">, </span><span class="s1">loc</span><span class="s3">, </span><span class="s1">groups</span><span class="s3">, </span><span class="s1">kernel=</span><span class="s3">None, </span><span class="s1">bw=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot; 
    Use kernel averaging to estimate a multivariate covariance function. 
 
    The goal is to estimate a covariance function C(x, y) = 
    cov(Z(x), Z(y)) where x, y are vectors in R^p (e.g. representing 
    locations in time or space), and Z(.) represents a multivariate 
    process on R^p. 
 
    The data used for estimation can be observed at arbitrary values of the 
    position vector, and there can be multiple independent observations 
    from the process. 
 
    Parameters 
    ---------- 
    exog : array_like 
        The rows of exog are realizations of the process obtained at 
        specified points. 
    loc : array_like 
        The rows of loc are the locations (e.g. in space or time) at 
        which the rows of exog are observed. 
    groups : array_like 
        The values of groups are labels for distinct independent copies 
        of the process. 
    kernel : MultivariateKernel instance, optional 
        An instance of MultivariateKernel, defaults to 
        GaussianMultivariateKernel. 
    bw : array_like or scalar 
        A bandwidth vector, or bandwidth multiplier.  If a 1d array, it 
        contains kernel bandwidths for each component of the process, and 
        must have length equal to the number of columns of exog.  If a scalar, 
        bw is a bandwidth multiplier used to adjust the default bandwidth; if 
        None, a default bandwidth is used. 
 
    Returns 
    ------- 
    A real-valued function C(x, y) that returns an estimate of the covariance 
    between values of the process located at x and y. 
 
    References 
    ---------- 
    .. [1] Genton M, W Kleiber (2015).  Cross covariance functions for 
        multivariate geostatics.  Statistical Science 30(2). 
        https://arxiv.org/pdf/1507.08017.pdf 
    &quot;&quot;&quot;</span>

    <span class="s1">exog = np.asarray(exog)</span>
    <span class="s1">loc = np.asarray(loc)</span>
    <span class="s1">groups = np.asarray(groups)</span>

    <span class="s3">if </span><span class="s1">loc.ndim == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">loc = loc[:</span><span class="s3">, None</span><span class="s1">]</span>

    <span class="s1">v = [exog.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">loc.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">len(groups)]</span>
    <span class="s3">if </span><span class="s1">min(v) != max(v):</span>
        <span class="s1">msg = </span><span class="s5">&quot;exog, loc, and groups must have the same number of rows&quot;</span>
        <span class="s3">raise </span><span class="s1">ValueError(msg)</span>

    <span class="s0"># Map from group labels to the row indices in each group.</span>
    <span class="s1">ix = {}</span>
    <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">g </span><span class="s3">in </span><span class="s1">enumerate(groups):</span>
        <span class="s3">if </span><span class="s1">g </span><span class="s3">not in </span><span class="s1">ix:</span>
            <span class="s1">ix[g] = []</span>
        <span class="s1">ix[g].append(i)</span>
    <span class="s3">for </span><span class="s1">g </span><span class="s3">in </span><span class="s1">ix.keys():</span>
        <span class="s1">ix[g] = np.sort(ix[g])</span>

    <span class="s3">if </span><span class="s1">kernel </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">kernel = GaussianMultivariateKernel()</span>

    <span class="s3">if </span><span class="s1">bw </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">kernel.set_default_bw(loc)</span>
    <span class="s3">elif </span><span class="s1">np.isscalar(bw):</span>
        <span class="s1">kernel.set_default_bw(loc</span><span class="s3">, </span><span class="s1">bwm=bw)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">kernel.set_bandwidth(bw)</span>

    <span class="s3">def </span><span class="s1">cov(x</span><span class="s3">, </span><span class="s1">y):</span>

        <span class="s1">kx = kernel.call(x</span><span class="s3">, </span><span class="s1">loc)</span>
        <span class="s1">ky = kernel.call(y</span><span class="s3">, </span><span class="s1">loc)</span>

        <span class="s1">cm</span><span class="s3">, </span><span class="s1">cw = </span><span class="s4">0.</span><span class="s3">, </span><span class="s4">0.</span>

        <span class="s3">for </span><span class="s1">g</span><span class="s3">, </span><span class="s1">ii </span><span class="s3">in </span><span class="s1">ix.items():</span>

            <span class="s1">m = len(ii)</span>
            <span class="s1">j1</span><span class="s3">, </span><span class="s1">j2 = np.indices((m</span><span class="s3">, </span><span class="s1">m))</span>
            <span class="s1">j1 = ii[j1.flat]</span>
            <span class="s1">j2 = ii[j2.flat]</span>
            <span class="s1">w = kx[j1] * ky[j2]</span>

            <span class="s0"># TODO: some other form of broadcasting may be faster than</span>
            <span class="s0"># einsum here</span>
            <span class="s1">cm += np.einsum(</span><span class="s5">&quot;ij,ik,i-&gt;jk&quot;</span><span class="s3">, </span><span class="s1">exog[j1</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">exog[j2</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">w)</span>
            <span class="s1">cw += w.sum()</span>

        <span class="s3">if </span><span class="s1">cw &lt; </span><span class="s4">1e-10</span><span class="s1">:</span>
            <span class="s1">msg = (</span><span class="s5">&quot;Effective sample size is 0.  The bandwidth may be too &quot; </span><span class="s1">+</span>
                   <span class="s5">&quot;small, or you are outside the range of your data.&quot;</span><span class="s1">)</span>
            <span class="s1">warnings.warn(msg)</span>
            <span class="s3">return </span><span class="s1">np.nan * np.ones_like(cm)</span>

        <span class="s3">return </span><span class="s1">cm / cw</span>

    <span class="s3">return </span><span class="s1">cov</span>
</pre>
</body>
</html>