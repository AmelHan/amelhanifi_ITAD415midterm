<html>
<head>
<title>smoothers_lowess_old.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
smoothers_lowess_old.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Univariate lowess function, like in R. 
 
References 
---------- 
Hastie, Tibshirani, Friedman. (2009) The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition: Chapter 6. 
 
Cleveland, W.S. (1979) &quot;Robust Locally Weighted Regression and Smoothing Scatterplots&quot;. Journal of the American Statistical Association 74 (368): 829-836. 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">numpy.linalg </span><span class="s2">import </span><span class="s1">lstsq</span>


<span class="s2">def </span><span class="s1">lowess(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">frac=</span><span class="s3">2.</span><span class="s1">/</span><span class="s3">3</span><span class="s2">, </span><span class="s1">it=</span><span class="s3">3</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    LOWESS (Locally Weighted Scatterplot Smoothing) 
 
    A lowess function that outs smoothed estimates of endog 
    at the given exog values from points (exog, endog) 
 
    Parameters 
    ---------- 
    endog : 1-D numpy array 
        The y-values of the observed points 
    exog : 1-D numpy array 
        The x-values of the observed points 
    frac : float 
        Between 0 and 1. The fraction of the data used 
        when estimating each y-value. 
    it : int 
        The number of residual-based reweightings 
        to perform. 
 
    Returns 
    ------- 
    out: numpy array 
        A numpy array with two columns. The first column 
        is the sorted x values and the second column the 
        associated estimated y-values. 
 
    Notes 
    ----- 
    This lowess function implements the algorithm given in the 
    reference below using local linear estimates. 
 
    Suppose the input data has N points. The algorithm works by 
    estimating the true ``y_i`` by taking the frac*N closest points 
    to ``(x_i,y_i)`` based on their x values and estimating ``y_i`` 
    using a weighted linear regression. The weight for ``(x_j,y_j)`` 
    is `_lowess_tricube` function applied to ``|x_i-x_j|``. 
 
    If ``iter &gt; 0``, then further weighted local linear regressions 
    are performed, where the weights are the same as above 
    times the `_lowess_bisquare` function of the residuals. Each iteration 
    takes approximately the same amount of time as the original fit, 
    so these iterations are expensive. They are most useful when 
    the noise has extremely heavy tails, such as Cauchy noise. 
    Noise with less heavy-tails, such as t-distributions with ``df &gt; 2``, 
    are less problematic. The weights downgrade the influence of 
    points with large residuals. In the extreme case, points whose 
    residuals are larger than 6 times the median absolute residual 
    are given weight 0. 
 
    Some experimentation is likely required to find a good 
    choice of frac and iter for a particular dataset. 
 
    References 
    ---------- 
    Cleveland, W.S. (1979) &quot;Robust Locally Weighted Regression 
    and Smoothing Scatterplots&quot;. Journal of the American Statistical 
    Association 74 (368): 829-836. 
 
    Examples 
    -------- 
    The below allows a comparison between how different the fits from 
    `lowess` for different values of frac can be. 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; import statsmodels.api as sm 
    &gt;&gt;&gt; lowess = sm.nonparametric.lowess 
    &gt;&gt;&gt; x = np.random.uniform(low=-2*np.pi, high=2*np.pi, size=500) 
    &gt;&gt;&gt; y = np.sin(x) + np.random.normal(size=len(x)) 
    &gt;&gt;&gt; z = lowess(y, x) 
    &gt;&gt;&gt; w = lowess(y, x, frac=1./3) 
 
    This gives a similar comparison for when it is 0 vs not. 
 
    &gt;&gt;&gt; import scipy.stats as stats 
    &gt;&gt;&gt; x = np.random.uniform(low=-2*np.pi, high=2*np.pi, size=500) 
    &gt;&gt;&gt; y = np.sin(x) + stats.cauchy.rvs(size=len(x)) 
    &gt;&gt;&gt; z = lowess(y, x, frac= 1./3, it=0) 
    &gt;&gt;&gt; w = lowess(y, x, frac=1./3) 
    &quot;&quot;&quot;</span>
    <span class="s1">x = exog</span>

    <span class="s2">if </span><span class="s1">exog.ndim != </span><span class="s3">1</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">'exog must be a vector'</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">endog.ndim != </span><span class="s3">1</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">'endog must be a vector'</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">endog.shape[</span><span class="s3">0</span><span class="s1">] != x.shape[</span><span class="s3">0</span><span class="s1">] :</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">'exog and endog must have same length'</span><span class="s1">)</span>

    <span class="s1">n = exog.shape[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">fitted = np.zeros(n)</span>

    <span class="s1">k = int(frac * n)</span>

    <span class="s1">index_array = np.argsort(exog)</span>
    <span class="s1">x_copy = np.array(exog[index_array]) </span><span class="s5">#, dtype ='float32')</span>
    <span class="s1">y_copy = endog[index_array]</span>

    <span class="s1">fitted</span><span class="s2">, </span><span class="s1">weights = _lowess_initial_fit(x_copy</span><span class="s2">, </span><span class="s1">y_copy</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n)</span>

    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(it):</span>
        <span class="s1">_lowess_robustify_fit(x_copy</span><span class="s2">, </span><span class="s1">y_copy</span><span class="s2">, </span><span class="s1">fitted</span><span class="s2">,</span>
                              <span class="s1">weights</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n)</span>

    <span class="s1">out = np.array([x_copy</span><span class="s2">, </span><span class="s1">fitted]).T</span>
    <span class="s1">out.shape = (n</span><span class="s2">,</span><span class="s3">2</span><span class="s1">)</span>

    <span class="s2">return </span><span class="s1">out</span>


<span class="s2">def </span><span class="s1">_lowess_initial_fit(x_copy</span><span class="s2">, </span><span class="s1">y_copy</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n):</span>
    <span class="s0">&quot;&quot;&quot; 
    The initial weighted local linear regression for lowess. 
 
    Parameters 
    ---------- 
    x_copy : 1-d ndarray 
        The x-values/exogenous part of the data being smoothed 
    y_copy : 1-d ndarray 
        The y-values/ endogenous part of the data being smoothed 
   k : int 
        The number of data points which affect the linear fit for 
        each estimated point 
    n : int 
        The total number of points 
 
    Returns 
    ------- 
    fitted : 1-d ndarray 
        The fitted y-values 
    weights : 2-d ndarray 
        An n by k array. The contribution to the weights in the 
        local linear fit coming from the distances between the 
        x-values 
 
   &quot;&quot;&quot;</span>
    <span class="s1">weights = np.zeros((n</span><span class="s2">,</span><span class="s1">k)</span><span class="s2">, </span><span class="s1">dtype = x_copy.dtype)</span>
    <span class="s1">nn_indices = [</span><span class="s3">0</span><span class="s2">,</span><span class="s1">k]</span>

    <span class="s1">X = np.ones((k</span><span class="s2">,</span><span class="s3">2</span><span class="s1">))</span>
    <span class="s1">fitted = np.zeros(n)</span>

    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n):</span>
        <span class="s5">#note: all _lowess functions are inplace, no return</span>
        <span class="s1">left_width = x_copy[i] - x_copy[nn_indices[</span><span class="s3">0</span><span class="s1">]]</span>
        <span class="s1">right_width = x_copy[nn_indices[</span><span class="s3">1</span><span class="s1">]-</span><span class="s3">1</span><span class="s1">] - x_copy[i]</span>
        <span class="s1">width = max(left_width</span><span class="s2">, </span><span class="s1">right_width)</span>
        <span class="s1">_lowess_wt_standardize(weights[i</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">,</span>
                                <span class="s1">x_copy[nn_indices[</span><span class="s3">0</span><span class="s1">]:nn_indices[</span><span class="s3">1</span><span class="s1">]]</span><span class="s2">,</span>
                            <span class="s1">x_copy[i]</span><span class="s2">, </span><span class="s1">width)</span>
        <span class="s1">_lowess_tricube(weights[i</span><span class="s2">,</span><span class="s1">:])</span>
        <span class="s1">weights[i</span><span class="s2">,</span><span class="s1">:] = np.sqrt(weights[i</span><span class="s2">,</span><span class="s1">:])</span>

        <span class="s1">X[:</span><span class="s2">,</span><span class="s3">1</span><span class="s1">] = x_copy[nn_indices[</span><span class="s3">0</span><span class="s1">]:nn_indices[</span><span class="s3">1</span><span class="s1">]]</span>
        <span class="s1">y_i = weights[i</span><span class="s2">,</span><span class="s1">:] * y_copy[nn_indices[</span><span class="s3">0</span><span class="s1">]:nn_indices[</span><span class="s3">1</span><span class="s1">]]</span>

        <span class="s1">beta = lstsq(weights[i</span><span class="s2">,</span><span class="s1">:].reshape(k</span><span class="s2">,</span><span class="s3">1</span><span class="s1">) * X</span><span class="s2">, </span><span class="s1">y_i</span><span class="s2">, </span><span class="s1">rcond=-</span><span class="s3">1</span><span class="s1">)[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">fitted[i] = beta[</span><span class="s3">0</span><span class="s1">] + beta[</span><span class="s3">1</span><span class="s1">]*x_copy[i]</span>

        <span class="s1">_lowess_update_nn(x_copy</span><span class="s2">, </span><span class="s1">nn_indices</span><span class="s2">, </span><span class="s1">i+</span><span class="s3">1</span><span class="s1">)</span>


    <span class="s2">return </span><span class="s1">fitted</span><span class="s2">, </span><span class="s1">weights</span>


<span class="s2">def </span><span class="s1">_lowess_wt_standardize(weights</span><span class="s2">, </span><span class="s1">new_entries</span><span class="s2">, </span><span class="s1">x_copy_i</span><span class="s2">, </span><span class="s1">width):</span>
    <span class="s0">&quot;&quot;&quot; 
    The initial phase of creating the weights. 
    Subtract the current x_i and divide by the width. 
 
    Parameters 
    ---------- 
    weights : ndarray 
        The memory where (new_entries - x_copy_i)/width will be placed 
    new_entries : ndarray 
        The x-values of the k closest points to x[i] 
    x_copy_i : float 
        x[i], the i'th point in the (sorted) x values 
    width : float 
        The maximum distance between x[i] and any point in new_entries 
 
    Returns 
    ------- 
    Nothing. The modifications are made to weight in place. 
    &quot;&quot;&quot;</span>
    <span class="s1">weights[:] = new_entries</span>
    <span class="s1">weights -= x_copy_i</span>
    <span class="s1">weights /= width</span>


<span class="s2">def </span><span class="s1">_lowess_robustify_fit(x_copy</span><span class="s2">, </span><span class="s1">y_copy</span><span class="s2">, </span><span class="s1">fitted</span><span class="s2">, </span><span class="s1">weights</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n):</span>
    <span class="s0">&quot;&quot;&quot; 
    Additional weighted local linear regressions, performed if 
    iter&gt;0. They take into account the sizes of the residuals, 
    to eliminate the effect of extreme outliers. 
 
    Parameters 
    ---------- 
    x_copy : 1-d ndarray 
        The x-values/exogenous part of the data being smoothed 
    y_copy : 1-d ndarray 
        The y-values/ endogenous part of the data being smoothed 
    fitted : 1-d ndarray 
        The fitted y-values from the previous iteration 
    weights : 2-d ndarray 
        An n by k array. The contribution to the weights in the 
        local linear fit coming from the distances between the 
        x-values 
    k : int 
        The number of data points which affect the linear fit for 
        each estimated point 
    n : int 
        The total number of points 
 
   Returns 
    ------- 
    Nothing. The fitted values are modified in place. 
    &quot;&quot;&quot;</span>
    <span class="s1">nn_indices = [</span><span class="s3">0</span><span class="s2">,</span><span class="s1">k]</span>
    <span class="s1">X = np.ones((k</span><span class="s2">,</span><span class="s3">2</span><span class="s1">))</span>

    <span class="s1">residual_weights = np.copy(y_copy)</span>
    <span class="s1">residual_weights.shape = (n</span><span class="s2">,</span><span class="s1">)</span>
    <span class="s1">residual_weights -= fitted</span>
    <span class="s1">residual_weights = np.absolute(residual_weights)</span><span class="s5">#, out=residual_weights)</span>
    <span class="s1">s = np.median(residual_weights)</span>
    <span class="s1">residual_weights /= (</span><span class="s3">6</span><span class="s1">*s)</span>
    <span class="s1">too_big = residual_weights&gt;=</span><span class="s3">1</span>
    <span class="s1">_lowess_bisquare(residual_weights)</span>
    <span class="s1">residual_weights[too_big] = </span><span class="s3">0</span>


    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n):</span>
        <span class="s1">total_weights = weights[i</span><span class="s2">,</span><span class="s1">:] * np.sqrt(residual_weights[nn_indices[</span><span class="s3">0</span><span class="s1">]:</span>
                                                        <span class="s1">nn_indices[</span><span class="s3">1</span><span class="s1">]])</span>

        <span class="s1">X[:</span><span class="s2">,</span><span class="s3">1</span><span class="s1">] = x_copy[nn_indices[</span><span class="s3">0</span><span class="s1">]:nn_indices[</span><span class="s3">1</span><span class="s1">]]</span>
        <span class="s1">y_i = total_weights * y_copy[nn_indices[</span><span class="s3">0</span><span class="s1">]:nn_indices[</span><span class="s3">1</span><span class="s1">]]</span>
        <span class="s1">total_weights.shape = (k</span><span class="s2">,</span><span class="s3">1</span><span class="s1">)</span>

        <span class="s1">beta = lstsq(total_weights * X</span><span class="s2">, </span><span class="s1">y_i</span><span class="s2">, </span><span class="s1">rcond=-</span><span class="s3">1</span><span class="s1">)[</span><span class="s3">0</span><span class="s1">]</span>

        <span class="s1">fitted[i] = beta[</span><span class="s3">0</span><span class="s1">] + beta[</span><span class="s3">1</span><span class="s1">] * x_copy[i]</span>

        <span class="s1">_lowess_update_nn(x_copy</span><span class="s2">, </span><span class="s1">nn_indices</span><span class="s2">, </span><span class="s1">i+</span><span class="s3">1</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">_lowess_update_nn(x</span><span class="s2">, </span><span class="s1">cur_nn</span><span class="s2">,</span><span class="s1">i):</span>
    <span class="s0">&quot;&quot;&quot; 
    Update the endpoints of the nearest neighbors to 
    the ith point. 
 
    Parameters 
    ---------- 
    x : iterable 
        The sorted points of x-values 
    cur_nn : list of length 2 
        The two current indices between which are the 
        k closest points to x[i]. (The actual value of 
        k is irrelevant for the algorithm. 
    i : int 
        The index of the current value in x for which 
        the k closest points are desired. 
 
    Returns 
    ------- 
    Nothing. It modifies cur_nn in place. 
    &quot;&quot;&quot;</span>
    <span class="s2">while True</span><span class="s1">:</span>
        <span class="s2">if </span><span class="s1">cur_nn[</span><span class="s3">1</span><span class="s1">]&lt;x.size:</span>
            <span class="s1">left_dist = x[i] - x[cur_nn[</span><span class="s3">0</span><span class="s1">]]</span>
            <span class="s1">new_right_dist = x[cur_nn[</span><span class="s3">1</span><span class="s1">]] - x[i]</span>
            <span class="s2">if </span><span class="s1">new_right_dist &lt; left_dist:</span>
                <span class="s1">cur_nn[</span><span class="s3">0</span><span class="s1">] = cur_nn[</span><span class="s3">0</span><span class="s1">] + </span><span class="s3">1</span>
                <span class="s1">cur_nn[</span><span class="s3">1</span><span class="s1">] = cur_nn[</span><span class="s3">1</span><span class="s1">] + </span><span class="s3">1</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s2">break</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">break</span>


<span class="s2">def </span><span class="s1">_lowess_tricube(t):</span>
    <span class="s0">&quot;&quot;&quot; 
    The _tricube function applied to a numpy array. 
    The tricube function is (1-abs(t)**3)**3. 
 
    Parameters 
    ---------- 
    t : ndarray 
        Array the tricube function is applied to elementwise and 
        in-place. 
 
    Returns 
    ------- 
    Nothing 
    &quot;&quot;&quot;</span>
    <span class="s5">#t = (1-np.abs(t)**3)**3</span>
    <span class="s1">t[:] = np.absolute(t) </span><span class="s5">#, out=t) #numpy version?</span>
    <span class="s1">_lowess_mycube(t)</span>
    <span class="s1">t[:] = np.negative(t) </span><span class="s5">#, out = t)</span>
    <span class="s1">t += </span><span class="s3">1</span>
    <span class="s1">_lowess_mycube(t)</span>


<span class="s2">def </span><span class="s1">_lowess_mycube(t):</span>
    <span class="s0">&quot;&quot;&quot; 
    Fast matrix cube 
 
    Parameters 
    ---------- 
    t : ndarray 
        Array that is cubed, elementwise and in-place 
 
    Returns 
    ------- 
    Nothing 
    &quot;&quot;&quot;</span>
    <span class="s5">#t **= 3</span>
    <span class="s1">t2 = t*t</span>
    <span class="s1">t *= t2</span>


<span class="s2">def </span><span class="s1">_lowess_bisquare(t):</span>
    <span class="s0">&quot;&quot;&quot; 
    The bisquare function applied to a numpy array. 
    The bisquare function is (1-t**2)**2. 
 
    Parameters 
    ---------- 
    t : ndarray 
        array bisquare function is applied to, element-wise and in-place. 
 
    Returns 
    ------- 
    Nothing 
    &quot;&quot;&quot;</span>
    <span class="s5">#t = (1-t**2)**2</span>
    <span class="s1">t *= t</span>
    <span class="s1">t[:] = np.negative(t) </span><span class="s5">#, out=t)</span>
    <span class="s1">t += </span><span class="s3">1</span>
    <span class="s1">t *= t</span>
</pre>
</body>
</html>