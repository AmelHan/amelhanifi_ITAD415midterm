<html>
<head>
<title>_hash.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_hash.py</font>
</center></td></tr></table>
<pre><span class="s0"># Author: Lars Buitinck</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">from </span><span class="s1">itertools </span><span class="s2">import </span><span class="s1">chain</span>
<span class="s2">from </span><span class="s1">numbers </span><span class="s2">import </span><span class="s1">Integral</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">scipy.sparse </span><span class="s2">as </span><span class="s1">sp</span>

<span class="s2">from </span><span class="s1">..base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">TransformerMixin</span><span class="s2">, </span><span class="s1">_fit_context</span>
<span class="s2">from </span><span class="s1">..utils._param_validation </span><span class="s2">import </span><span class="s1">Interval</span><span class="s2">, </span><span class="s1">StrOptions</span>
<span class="s2">from </span><span class="s1">._hashing_fast </span><span class="s2">import </span><span class="s1">transform </span><span class="s2">as </span><span class="s1">_hashing_transform</span>


<span class="s2">def </span><span class="s1">_iteritems(d):</span>
    <span class="s3">&quot;&quot;&quot;Like d.iteritems, but accepts any collections.Mapping.&quot;&quot;&quot;</span>
    <span class="s2">return </span><span class="s1">d.iteritems() </span><span class="s2">if </span><span class="s1">hasattr(d</span><span class="s2">, </span><span class="s4">&quot;iteritems&quot;</span><span class="s1">) </span><span class="s2">else </span><span class="s1">d.items()</span>


<span class="s2">class </span><span class="s1">FeatureHasher(TransformerMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
    <span class="s3">&quot;&quot;&quot;Implements feature hashing, aka the hashing trick. 
 
    This class turns sequences of symbolic feature names (strings) into 
    scipy.sparse matrices, using a hash function to compute the matrix column 
    corresponding to a name. The hash function employed is the signed 32-bit 
    version of Murmurhash3. 
 
    Feature names of type byte string are used as-is. Unicode strings are 
    converted to UTF-8 first, but no Unicode normalization is done. 
    Feature values must be (finite) numbers. 
 
    This class is a low-memory alternative to DictVectorizer and 
    CountVectorizer, intended for large-scale (online) learning and situations 
    where memory is tight, e.g. when running prediction code on embedded 
    devices. 
 
    For an efficiency comparision of the different feature extractors, see 
    :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`. 
 
    Read more in the :ref:`User Guide &lt;feature_hashing&gt;`. 
 
    .. versionadded:: 0.13 
 
    Parameters 
    ---------- 
    n_features : int, default=2**20 
        The number of features (columns) in the output matrices. Small numbers 
        of features are likely to cause hash collisions, but large numbers 
        will cause larger coefficient dimensions in linear learners. 
    input_type : str, default='dict' 
        Choose a string from {'dict', 'pair', 'string'}. 
        Either &quot;dict&quot; (the default) to accept dictionaries over 
        (feature_name, value); &quot;pair&quot; to accept pairs of (feature_name, value); 
        or &quot;string&quot; to accept single strings. 
        feature_name should be a string, while value should be a number. 
        In the case of &quot;string&quot;, a value of 1 is implied. 
        The feature_name is hashed to find the appropriate column for the 
        feature. The value's sign might be flipped in the output (but see 
        non_negative, below). 
    dtype : numpy dtype, default=np.float64 
        The type of feature values. Passed to scipy.sparse matrix constructors 
        as the dtype argument. Do not set this to bool, np.boolean or any 
        unsigned integer type. 
    alternate_sign : bool, default=True 
        When True, an alternating sign is added to the features as to 
        approximately conserve the inner product in the hashed space even for 
        small n_features. This approach is similar to sparse random projection. 
 
        .. versionchanged:: 0.19 
            ``alternate_sign`` replaces the now deprecated ``non_negative`` 
            parameter. 
 
    See Also 
    -------- 
    DictVectorizer : Vectorizes string-valued features using a hash table. 
    sklearn.preprocessing.OneHotEncoder : Handles nominal/categorical features. 
 
    Notes 
    ----- 
    This estimator is :term:`stateless` and does not need to be fitted. 
    However, we recommend to call :meth:`fit_transform` instead of 
    :meth:`transform`, as parameter validation is only performed in 
    :meth:`fit`. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.feature_extraction import FeatureHasher 
    &gt;&gt;&gt; h = FeatureHasher(n_features=10) 
    &gt;&gt;&gt; D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}] 
    &gt;&gt;&gt; f = h.transform(D) 
    &gt;&gt;&gt; f.toarray() 
    array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.], 
           [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]]) 
 
    With `input_type=&quot;string&quot;`, the input must be an iterable over iterables of 
    strings: 
 
    &gt;&gt;&gt; h = FeatureHasher(n_features=8, input_type=&quot;string&quot;) 
    &gt;&gt;&gt; raw_X = [[&quot;dog&quot;, &quot;cat&quot;, &quot;snake&quot;], [&quot;snake&quot;, &quot;dog&quot;], [&quot;cat&quot;, &quot;bird&quot;]] 
    &gt;&gt;&gt; f = h.transform(raw_X) 
    &gt;&gt;&gt; f.toarray() 
    array([[ 0.,  0.,  0., -1.,  0., -1.,  0.,  1.], 
           [ 0.,  0.,  0., -1.,  0., -1.,  0.,  0.], 
           [ 0., -1.,  0.,  0.,  0.,  0.,  0.,  1.]]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;n_features&quot;</span><span class="s1">: [Interval(Integral</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s1">np.iinfo(np.int32).max</span><span class="s2">, </span><span class="s1">closed=</span><span class="s4">&quot;both&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s4">&quot;input_type&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;dict&quot;</span><span class="s2">, </span><span class="s4">&quot;pair&quot;</span><span class="s2">, </span><span class="s4">&quot;string&quot;</span><span class="s1">})]</span><span class="s2">,</span>
        <span class="s4">&quot;dtype&quot;</span><span class="s1">: </span><span class="s4">&quot;no_validation&quot;</span><span class="s2">,  </span><span class="s0"># delegate to numpy</span>
        <span class="s4">&quot;alternate_sign&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">n_features=(</span><span class="s5">2</span><span class="s1">**</span><span class="s5">20</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">input_type=</span><span class="s4">&quot;dict&quot;</span><span class="s2">,</span>
        <span class="s1">dtype=np.float64</span><span class="s2">,</span>
        <span class="s1">alternate_sign=</span><span class="s2">True,</span>
    <span class="s1">):</span>
        <span class="s1">self.dtype = dtype</span>
        <span class="s1">self.input_type = input_type</span>
        <span class="s1">self.n_features = n_features</span>
        <span class="s1">self.alternate_sign = alternate_sign</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X=</span><span class="s2">None, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s3">&quot;&quot;&quot;Only validates estimator's parameters. 
 
        This method allows to: (i) validate the estimator's parameters and 
        (ii) be consistent with the scikit-learn transformer API. 
 
        Parameters 
        ---------- 
        X : Ignored 
            Not used, present here for API consistency by convention. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        Returns 
        ------- 
        self : object 
            FeatureHasher class instance. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">transform(self</span><span class="s2">, </span><span class="s1">raw_X):</span>
        <span class="s3">&quot;&quot;&quot;Transform a sequence of instances to a scipy.sparse matrix. 
 
        Parameters 
        ---------- 
        raw_X : iterable over iterable over raw features, length = n_samples 
            Samples. Each sample must be iterable an (e.g., a list or tuple) 
            containing/generating feature names (and optionally values, see 
            the input_type constructor argument) which will be hashed. 
            raw_X need not support the len function, so it can be the result 
            of a generator; n_samples is determined on the fly. 
 
        Returns 
        ------- 
        X : sparse matrix of shape (n_samples, n_features) 
            Feature matrix, for use with estimators or further transformers. 
        &quot;&quot;&quot;</span>
        <span class="s1">raw_X = iter(raw_X)</span>
        <span class="s2">if </span><span class="s1">self.input_type == </span><span class="s4">&quot;dict&quot;</span><span class="s1">:</span>
            <span class="s1">raw_X = (_iteritems(d) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">raw_X)</span>
        <span class="s2">elif </span><span class="s1">self.input_type == </span><span class="s4">&quot;string&quot;</span><span class="s1">:</span>
            <span class="s1">first_raw_X = next(raw_X)</span>
            <span class="s2">if </span><span class="s1">isinstance(first_raw_X</span><span class="s2">, </span><span class="s1">str):</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;Samples can not be a single string. The input must be an iterable&quot;</span>
                    <span class="s4">&quot; over iterables of strings.&quot;</span>
                <span class="s1">)</span>
            <span class="s1">raw_X_ = chain([first_raw_X]</span><span class="s2">, </span><span class="s1">raw_X)</span>
            <span class="s1">raw_X = (((f</span><span class="s2">, </span><span class="s5">1</span><span class="s1">) </span><span class="s2">for </span><span class="s1">f </span><span class="s2">in </span><span class="s1">x) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">raw_X_)</span>

        <span class="s1">indices</span><span class="s2">, </span><span class="s1">indptr</span><span class="s2">, </span><span class="s1">values = _hashing_transform(</span>
            <span class="s1">raw_X</span><span class="s2">, </span><span class="s1">self.n_features</span><span class="s2">, </span><span class="s1">self.dtype</span><span class="s2">, </span><span class="s1">self.alternate_sign</span><span class="s2">, </span><span class="s1">seed=</span><span class="s5">0</span>
        <span class="s1">)</span>
        <span class="s1">n_samples = indptr.shape[</span><span class="s5">0</span><span class="s1">] - </span><span class="s5">1</span>

        <span class="s2">if </span><span class="s1">n_samples == </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Cannot vectorize empty sequence.&quot;</span><span class="s1">)</span>

        <span class="s1">X = sp.csr_matrix(</span>
            <span class="s1">(values</span><span class="s2">, </span><span class="s1">indices</span><span class="s2">, </span><span class="s1">indptr)</span><span class="s2">,</span>
            <span class="s1">dtype=self.dtype</span><span class="s2">,</span>
            <span class="s1">shape=(n_samples</span><span class="s2">, </span><span class="s1">self.n_features)</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">X.sum_duplicates()  </span><span class="s0"># also sorts the indices</span>

        <span class="s2">return </span><span class="s1">X</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s4">&quot;X_types&quot;</span><span class="s1">: [self.input_type]}</span>
</pre>
</body>
</html>