<html>
<head>
<title>test_score_objects.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #808080;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_score_objects.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">numbers</span>
<span class="s0">import </span><span class="s1">os</span>
<span class="s0">import </span><span class="s1">pickle</span>
<span class="s0">import </span><span class="s1">shutil</span>
<span class="s0">import </span><span class="s1">tempfile</span>
<span class="s0">from </span><span class="s1">copy </span><span class="s0">import </span><span class="s1">deepcopy</span>
<span class="s0">from </span><span class="s1">functools </span><span class="s0">import </span><span class="s1">partial</span>
<span class="s0">from </span><span class="s1">unittest.mock </span><span class="s0">import </span><span class="s1">Mock</span>

<span class="s0">import </span><span class="s1">joblib</span>
<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">assert_allclose</span>

<span class="s0">from </span><span class="s1">sklearn </span><span class="s0">import </span><span class="s1">config_context</span>
<span class="s0">from </span><span class="s1">sklearn.base </span><span class="s0">import </span><span class="s1">BaseEstimator</span>
<span class="s0">from </span><span class="s1">sklearn.cluster </span><span class="s0">import </span><span class="s1">KMeans</span>
<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">load_diabetes</span><span class="s0">,</span>
    <span class="s1">make_blobs</span><span class="s0">,</span>
    <span class="s1">make_classification</span><span class="s0">,</span>
    <span class="s1">make_multilabel_classification</span><span class="s0">,</span>
    <span class="s1">make_regression</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.linear_model </span><span class="s0">import </span><span class="s1">LogisticRegression</span><span class="s0">, </span><span class="s1">Perceptron</span><span class="s0">, </span><span class="s1">Ridge</span>
<span class="s0">from </span><span class="s1">sklearn.metrics </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">accuracy_score</span><span class="s0">,</span>
    <span class="s1">average_precision_score</span><span class="s0">,</span>
    <span class="s1">balanced_accuracy_score</span><span class="s0">,</span>
    <span class="s1">brier_score_loss</span><span class="s0">,</span>
    <span class="s1">check_scoring</span><span class="s0">,</span>
    <span class="s1">f1_score</span><span class="s0">,</span>
    <span class="s1">fbeta_score</span><span class="s0">,</span>
    <span class="s1">get_scorer</span><span class="s0">,</span>
    <span class="s1">get_scorer_names</span><span class="s0">,</span>
    <span class="s1">jaccard_score</span><span class="s0">,</span>
    <span class="s1">log_loss</span><span class="s0">,</span>
    <span class="s1">make_scorer</span><span class="s0">,</span>
    <span class="s1">matthews_corrcoef</span><span class="s0">,</span>
    <span class="s1">precision_score</span><span class="s0">,</span>
    <span class="s1">r2_score</span><span class="s0">,</span>
    <span class="s1">recall_score</span><span class="s0">,</span>
    <span class="s1">roc_auc_score</span><span class="s0">,</span>
    <span class="s1">top_k_accuracy_score</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.metrics </span><span class="s0">import </span><span class="s1">cluster </span><span class="s0">as </span><span class="s1">cluster_module</span>
<span class="s0">from </span><span class="s1">sklearn.metrics._scorer </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">_check_multimetric_scoring</span><span class="s0">,</span>
    <span class="s1">_MultimetricScorer</span><span class="s0">,</span>
    <span class="s1">_PassthroughScorer</span><span class="s0">,</span>
    <span class="s1">_PredictScorer</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.model_selection </span><span class="s0">import </span><span class="s1">GridSearchCV</span><span class="s0">, </span><span class="s1">cross_val_score</span><span class="s0">, </span><span class="s1">train_test_split</span>
<span class="s0">from </span><span class="s1">sklearn.multiclass </span><span class="s0">import </span><span class="s1">OneVsRestClassifier</span>
<span class="s0">from </span><span class="s1">sklearn.neighbors </span><span class="s0">import </span><span class="s1">KNeighborsClassifier</span>
<span class="s0">from </span><span class="s1">sklearn.pipeline </span><span class="s0">import </span><span class="s1">make_pipeline</span>
<span class="s0">from </span><span class="s1">sklearn.svm </span><span class="s0">import </span><span class="s1">LinearSVC</span>
<span class="s0">from </span><span class="s1">sklearn.tests.test_metadata_routing </span><span class="s0">import </span><span class="s1">assert_request_is_empty</span>
<span class="s0">from </span><span class="s1">sklearn.tree </span><span class="s0">import </span><span class="s1">DecisionTreeClassifier</span><span class="s0">, </span><span class="s1">DecisionTreeRegressor</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">assert_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_equal</span><span class="s0">,</span>
    <span class="s1">ignore_warnings</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.utils.metadata_routing </span><span class="s0">import </span><span class="s1">MetadataRouter</span>

<span class="s1">REGRESSION_SCORERS = [</span>
    <span class="s2">&quot;explained_variance&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;r2&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;neg_mean_absolute_error&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;neg_mean_squared_error&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;neg_mean_absolute_percentage_error&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;neg_mean_squared_log_error&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;neg_median_absolute_error&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;neg_root_mean_squared_error&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;mean_absolute_error&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;mean_absolute_percentage_error&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;mean_squared_error&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;median_absolute_error&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;max_error&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;neg_mean_poisson_deviance&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;neg_mean_gamma_deviance&quot;</span><span class="s0">,</span>
<span class="s1">]</span>

<span class="s1">CLF_SCORERS = [</span>
    <span class="s2">&quot;accuracy&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;balanced_accuracy&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;top_k_accuracy&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;f1&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;f1_weighted&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;f1_macro&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;f1_micro&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;roc_auc&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;average_precision&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;precision&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;precision_weighted&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;precision_macro&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;precision_micro&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;recall&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;recall_weighted&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;recall_macro&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;recall_micro&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;neg_log_loss&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;neg_brier_score&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;jaccard&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;jaccard_weighted&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;jaccard_macro&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;jaccard_micro&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;roc_auc_ovr&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;roc_auc_ovo&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;roc_auc_ovr_weighted&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;roc_auc_ovo_weighted&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;matthews_corrcoef&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;positive_likelihood_ratio&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;neg_negative_likelihood_ratio&quot;</span><span class="s0">,</span>
<span class="s1">]</span>

<span class="s3"># All supervised cluster scorers (They behave like classification metric)</span>
<span class="s1">CLUSTER_SCORERS = [</span>
    <span class="s2">&quot;adjusted_rand_score&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;rand_score&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;homogeneity_score&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;completeness_score&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;v_measure_score&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;mutual_info_score&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;adjusted_mutual_info_score&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;normalized_mutual_info_score&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;fowlkes_mallows_score&quot;</span><span class="s0">,</span>
<span class="s1">]</span>

<span class="s1">MULTILABEL_ONLY_SCORERS = [</span>
    <span class="s2">&quot;precision_samples&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;recall_samples&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;f1_samples&quot;</span><span class="s0">,</span>
    <span class="s2">&quot;jaccard_samples&quot;</span><span class="s0">,</span>
<span class="s1">]</span>

<span class="s1">REQUIRE_POSITIVE_Y_SCORERS = [</span><span class="s2">&quot;neg_mean_poisson_deviance&quot;</span><span class="s0">, </span><span class="s2">&quot;neg_mean_gamma_deviance&quot;</span><span class="s1">]</span>


<span class="s0">def </span><span class="s1">_require_positive_y(y):</span>
    <span class="s4">&quot;&quot;&quot;Make targets strictly positive&quot;&quot;&quot;</span>
    <span class="s1">offset = abs(y.min()) + </span><span class="s5">1</span>
    <span class="s1">y = y + offset</span>
    <span class="s0">return </span><span class="s1">y</span>


<span class="s0">def </span><span class="s1">_make_estimators(X_train</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_ml_train):</span>
    <span class="s3"># Make estimators that make sense to test various scoring methods</span>
    <span class="s1">sensible_regr = DecisionTreeRegressor(random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s3"># some of the regressions scorers require strictly positive input.</span>
    <span class="s1">sensible_regr.fit(X_train</span><span class="s0">, </span><span class="s1">_require_positive_y(y_train))</span>
    <span class="s1">sensible_clf = DecisionTreeClassifier(random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">sensible_clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">sensible_ml_clf = DecisionTreeClassifier(random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">sensible_ml_clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_ml_train)</span>
    <span class="s0">return </span><span class="s1">dict(</span>
        <span class="s1">[(name</span><span class="s0">, </span><span class="s1">sensible_regr) </span><span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">REGRESSION_SCORERS]</span>
        <span class="s1">+ [(name</span><span class="s0">, </span><span class="s1">sensible_clf) </span><span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CLF_SCORERS]</span>
        <span class="s1">+ [(name</span><span class="s0">, </span><span class="s1">sensible_clf) </span><span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CLUSTER_SCORERS]</span>
        <span class="s1">+ [(name</span><span class="s0">, </span><span class="s1">sensible_ml_clf) </span><span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">MULTILABEL_ONLY_SCORERS]</span>
    <span class="s1">)</span>


<span class="s1">X_mm</span><span class="s0">, </span><span class="s1">y_mm</span><span class="s0">, </span><span class="s1">y_ml_mm = </span><span class="s0">None, None, None</span>
<span class="s1">ESTIMATORS = </span><span class="s0">None</span>
<span class="s1">TEMP_FOLDER = </span><span class="s0">None</span>


<span class="s0">def </span><span class="s1">setup_module():</span>
    <span class="s3"># Create some memory mapped data</span>
    <span class="s0">global </span><span class="s1">X_mm</span><span class="s0">, </span><span class="s1">y_mm</span><span class="s0">, </span><span class="s1">y_ml_mm</span><span class="s0">, </span><span class="s1">TEMP_FOLDER</span><span class="s0">, </span><span class="s1">ESTIMATORS</span>
    <span class="s1">TEMP_FOLDER = tempfile.mkdtemp(prefix=</span><span class="s2">&quot;sklearn_test_score_objects_&quot;</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s5">30</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s5">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">_</span><span class="s0">, </span><span class="s1">y_ml = make_multilabel_classification(n_samples=X.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">filename = os.path.join(TEMP_FOLDER</span><span class="s0">, </span><span class="s2">&quot;test_data.pkl&quot;</span><span class="s1">)</span>
    <span class="s1">joblib.dump((X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">y_ml)</span><span class="s0">, </span><span class="s1">filename)</span>
    <span class="s1">X_mm</span><span class="s0">, </span><span class="s1">y_mm</span><span class="s0">, </span><span class="s1">y_ml_mm = joblib.load(filename</span><span class="s0">, </span><span class="s1">mmap_mode=</span><span class="s2">&quot;r&quot;</span><span class="s1">)</span>
    <span class="s1">ESTIMATORS = _make_estimators(X_mm</span><span class="s0">, </span><span class="s1">y_mm</span><span class="s0">, </span><span class="s1">y_ml_mm)</span>


<span class="s0">def </span><span class="s1">teardown_module():</span>
    <span class="s0">global </span><span class="s1">X_mm</span><span class="s0">, </span><span class="s1">y_mm</span><span class="s0">, </span><span class="s1">y_ml_mm</span><span class="s0">, </span><span class="s1">TEMP_FOLDER</span><span class="s0">, </span><span class="s1">ESTIMATORS</span>
    <span class="s3"># GC closes the mmap file descriptors</span>
    <span class="s1">X_mm</span><span class="s0">, </span><span class="s1">y_mm</span><span class="s0">, </span><span class="s1">y_ml_mm</span><span class="s0">, </span><span class="s1">ESTIMATORS = </span><span class="s0">None, None, None, None</span>
    <span class="s1">shutil.rmtree(TEMP_FOLDER)</span>


<span class="s0">class </span><span class="s1">EstimatorWithFit(BaseEstimator):</span>
    <span class="s4">&quot;&quot;&quot;Dummy estimator to test scoring validators&quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
        <span class="s0">return </span><span class="s1">self</span>


<span class="s0">class </span><span class="s1">EstimatorWithFitAndScore:</span>
    <span class="s4">&quot;&quot;&quot;Dummy estimator to test scoring validators&quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
        <span class="s0">return </span><span class="s1">self</span>

    <span class="s0">def </span><span class="s1">score(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
        <span class="s0">return </span><span class="s5">1.0</span>


<span class="s0">class </span><span class="s1">EstimatorWithFitAndPredict:</span>
    <span class="s4">&quot;&quot;&quot;Dummy estimator to test scoring validators&quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
        <span class="s1">self.y = y</span>
        <span class="s0">return </span><span class="s1">self</span>

    <span class="s0">def </span><span class="s1">predict(self</span><span class="s0">, </span><span class="s1">X):</span>
        <span class="s0">return </span><span class="s1">self.y</span>


<span class="s0">class </span><span class="s1">DummyScorer:</span>
    <span class="s4">&quot;&quot;&quot;Dummy scorer that always returns 1.&quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__call__(self</span><span class="s0">, </span><span class="s1">est</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
        <span class="s0">return </span><span class="s5">1</span>


<span class="s0">def </span><span class="s1">test_all_scorers_repr():</span>
    <span class="s3"># Test that all scorers have a working repr</span>
    <span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">get_scorer_names():</span>
        <span class="s1">repr(get_scorer(name))</span>


<span class="s0">def </span><span class="s1">check_scoring_validator_for_single_metric_usecases(scoring_validator):</span>
    <span class="s3"># Test all branches of single metric usecases</span>
    <span class="s1">estimator = EstimatorWithFitAndScore()</span>
    <span class="s1">estimator.fit([[</span><span class="s5">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">])</span>
    <span class="s1">scorer = scoring_validator(estimator)</span>
    <span class="s0">assert </span><span class="s1">isinstance(scorer</span><span class="s0">, </span><span class="s1">_PassthroughScorer)</span>
    <span class="s1">assert_almost_equal(scorer(estimator</span><span class="s0">, </span><span class="s1">[[</span><span class="s5">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">])</span><span class="s0">, </span><span class="s5">1.0</span><span class="s1">)</span>

    <span class="s1">estimator = EstimatorWithFitAndPredict()</span>
    <span class="s1">estimator.fit([[</span><span class="s5">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">])</span>
    <span class="s1">pattern = (</span>
        <span class="s2">r&quot;If no scoring is specified, the estimator passed should have&quot;</span>
        <span class="s2">r&quot; a 'score' method\. The estimator .* does not\.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(TypeError</span><span class="s0">, </span><span class="s1">match=pattern):</span>
        <span class="s1">scoring_validator(estimator)</span>

    <span class="s1">scorer = scoring_validator(estimator</span><span class="s0">, </span><span class="s1">scoring=</span><span class="s2">&quot;accuracy&quot;</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(scorer(estimator</span><span class="s0">, </span><span class="s1">[[</span><span class="s5">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">])</span><span class="s0">, </span><span class="s5">1.0</span><span class="s1">)</span>

    <span class="s1">estimator = EstimatorWithFit()</span>
    <span class="s1">scorer = scoring_validator(estimator</span><span class="s0">, </span><span class="s1">scoring=</span><span class="s2">&quot;accuracy&quot;</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">isinstance(scorer</span><span class="s0">, </span><span class="s1">_PredictScorer)</span>

    <span class="s3"># Test the allow_none parameter for check_scoring alone</span>
    <span class="s0">if </span><span class="s1">scoring_validator </span><span class="s0">is </span><span class="s1">check_scoring:</span>
        <span class="s1">estimator = EstimatorWithFit()</span>
        <span class="s1">scorer = scoring_validator(estimator</span><span class="s0">, </span><span class="s1">allow_none=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">scorer </span><span class="s0">is None</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;scoring&quot;</span><span class="s0">,</span>
    <span class="s1">(</span>
        <span class="s1">(</span><span class="s2">&quot;accuracy&quot;</span><span class="s0">,</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s2">&quot;precision&quot;</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">{</span><span class="s2">&quot;acc&quot;</span><span class="s1">: </span><span class="s2">&quot;accuracy&quot;</span><span class="s0">, </span><span class="s2">&quot;precision&quot;</span><span class="s1">: </span><span class="s2">&quot;precision&quot;</span><span class="s1">}</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;accuracy&quot;</span><span class="s0">, </span><span class="s2">&quot;precision&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s2">&quot;precision&quot;</span><span class="s0">, </span><span class="s2">&quot;accuracy&quot;</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">{</span>
            <span class="s2">&quot;accuracy&quot;</span><span class="s1">: make_scorer(accuracy_score)</span><span class="s0">,</span>
            <span class="s2">&quot;precision&quot;</span><span class="s1">: make_scorer(precision_score)</span><span class="s0">,</span>
        <span class="s1">}</span><span class="s0">,</span>
    <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">ids=[</span>
        <span class="s2">&quot;single_tuple&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;single_list&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;dict_str&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;multi_tuple&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;multi_list&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;dict_callable&quot;</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_check_scoring_and_check_multimetric_scoring(scoring):</span>
    <span class="s1">check_scoring_validator_for_single_metric_usecases(check_scoring)</span>
    <span class="s3"># To make sure the check_scoring is correctly applied to the constituent</span>
    <span class="s3"># scorers</span>

    <span class="s1">estimator = LinearSVC(dual=</span><span class="s2">&quot;auto&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">estimator.fit([[</span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">3</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s1">])</span>

    <span class="s1">scorers = _check_multimetric_scoring(estimator</span><span class="s0">, </span><span class="s1">scoring)</span>
    <span class="s0">assert </span><span class="s1">isinstance(scorers</span><span class="s0">, </span><span class="s1">dict)</span>
    <span class="s0">assert </span><span class="s1">sorted(scorers.keys()) == sorted(list(scoring))</span>
    <span class="s0">assert </span><span class="s1">all(</span>
        <span class="s1">[isinstance(scorer</span><span class="s0">, </span><span class="s1">_PredictScorer) </span><span class="s0">for </span><span class="s1">scorer </span><span class="s0">in </span><span class="s1">list(scorers.values())]</span>
    <span class="s1">)</span>

    <span class="s0">if </span><span class="s2">&quot;acc&quot; </span><span class="s0">in </span><span class="s1">scoring:</span>
        <span class="s1">assert_almost_equal(</span>
            <span class="s1">scorers[</span><span class="s2">&quot;acc&quot;</span><span class="s1">](estimator</span><span class="s0">, </span><span class="s1">[[</span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">3</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s1">])</span><span class="s0">, </span><span class="s5">2.0 </span><span class="s1">/ </span><span class="s5">3.0</span>
        <span class="s1">)</span>
    <span class="s0">if </span><span class="s2">&quot;accuracy&quot; </span><span class="s0">in </span><span class="s1">scoring:</span>
        <span class="s1">assert_almost_equal(</span>
            <span class="s1">scorers[</span><span class="s2">&quot;accuracy&quot;</span><span class="s1">](estimator</span><span class="s0">, </span><span class="s1">[[</span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">3</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s1">])</span><span class="s0">, </span><span class="s5">2.0 </span><span class="s1">/ </span><span class="s5">3.0</span>
        <span class="s1">)</span>
    <span class="s0">if </span><span class="s2">&quot;precision&quot; </span><span class="s0">in </span><span class="s1">scoring:</span>
        <span class="s1">assert_almost_equal(</span>
            <span class="s1">scorers[</span><span class="s2">&quot;precision&quot;</span><span class="s1">](estimator</span><span class="s0">, </span><span class="s1">[[</span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">3</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s1">])</span><span class="s0">, </span><span class="s5">0.5</span>
        <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;scoring, msg&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">(make_scorer(precision_score)</span><span class="s0">, </span><span class="s1">make_scorer(accuracy_score))</span><span class="s0">,</span>
            <span class="s2">&quot;One or more of the elements were callables&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">([</span><span class="s5">5</span><span class="s1">]</span><span class="s0">, </span><span class="s2">&quot;Non-string types were found&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">((make_scorer(precision_score)</span><span class="s0">,</span><span class="s1">)</span><span class="s0">, </span><span class="s2">&quot;One or more of the elements were callables&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(()</span><span class="s0">, </span><span class="s2">&quot;Empty list was given&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">((</span><span class="s2">&quot;f1&quot;</span><span class="s0">, </span><span class="s2">&quot;f1&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s2">&quot;Duplicate elements were found&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">({</span><span class="s5">4</span><span class="s1">: </span><span class="s2">&quot;accuracy&quot;</span><span class="s1">}</span><span class="s0">, </span><span class="s2">&quot;Non-string types were found in the keys&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">({}</span><span class="s0">, </span><span class="s2">&quot;An empty dict was passed&quot;</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
    <span class="s1">ids=[</span>
        <span class="s2">&quot;tuple of callables&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;list of int&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;tuple of one callable&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;empty tuple&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;non-unique str&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;non-string key dict&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;empty dict&quot;</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_check_scoring_and_check_multimetric_scoring_errors(scoring</span><span class="s0">, </span><span class="s1">msg):</span>
    <span class="s3"># Make sure it raises errors when scoring parameter is not valid.</span>
    <span class="s3"># More weird corner cases are tested at test_validation.py</span>
    <span class="s1">estimator = EstimatorWithFitAndPredict()</span>
    <span class="s1">estimator.fit([[</span><span class="s5">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">])</span>

    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">_check_multimetric_scoring(estimator</span><span class="s0">, </span><span class="s1">scoring=scoring)</span>


<span class="s0">def </span><span class="s1">test_check_scoring_gridsearchcv():</span>
    <span class="s3"># test that check_scoring works on GridSearchCV and pipeline.</span>
    <span class="s3"># slightly redundant non-regression test.</span>

    <span class="s1">grid = GridSearchCV(LinearSVC(dual=</span><span class="s2">&quot;auto&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s1">param_grid={</span><span class="s2">&quot;C&quot;</span><span class="s1">: [</span><span class="s5">0.1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">]}</span><span class="s0">, </span><span class="s1">cv=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">scorer = check_scoring(grid</span><span class="s0">, </span><span class="s1">scoring=</span><span class="s2">&quot;f1&quot;</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">isinstance(scorer</span><span class="s0">, </span><span class="s1">_PredictScorer)</span>

    <span class="s1">pipe = make_pipeline(LinearSVC(dual=</span><span class="s2">&quot;auto&quot;</span><span class="s1">))</span>
    <span class="s1">scorer = check_scoring(pipe</span><span class="s0">, </span><span class="s1">scoring=</span><span class="s2">&quot;f1&quot;</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">isinstance(scorer</span><span class="s0">, </span><span class="s1">_PredictScorer)</span>

    <span class="s3"># check that cross_val_score definitely calls the scorer</span>
    <span class="s3"># and doesn't make any assumptions about the estimator apart from having a</span>
    <span class="s3"># fit.</span>
    <span class="s1">scores = cross_val_score(</span>
        <span class="s1">EstimatorWithFit()</span><span class="s0">, </span><span class="s1">[[</span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">3</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">scoring=DummyScorer()</span><span class="s0">, </span><span class="s1">cv=</span><span class="s5">3</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(scores</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_make_scorer():</span>
    <span class="s3"># Sanity check on the make_scorer factory function.</span>
    <span class="s1">f = </span><span class="s0">lambda </span><span class="s1">*args: </span><span class="s5">0</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">make_scorer(f</span><span class="s0">, </span><span class="s1">needs_threshold=</span><span class="s0">True, </span><span class="s1">needs_proba=</span><span class="s0">True</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;scorer_name, metric&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s2">&quot;f1&quot;</span><span class="s0">, </span><span class="s1">f1_score)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;f1_weighted&quot;</span><span class="s0">, </span><span class="s1">partial(f1_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;f1_macro&quot;</span><span class="s0">, </span><span class="s1">partial(f1_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;macro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;f1_micro&quot;</span><span class="s0">, </span><span class="s1">partial(f1_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;micro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;precision&quot;</span><span class="s0">, </span><span class="s1">precision_score)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;precision_weighted&quot;</span><span class="s0">, </span><span class="s1">partial(precision_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;precision_macro&quot;</span><span class="s0">, </span><span class="s1">partial(precision_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;macro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;precision_micro&quot;</span><span class="s0">, </span><span class="s1">partial(precision_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;micro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;recall&quot;</span><span class="s0">, </span><span class="s1">recall_score)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;recall_weighted&quot;</span><span class="s0">, </span><span class="s1">partial(recall_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;recall_macro&quot;</span><span class="s0">, </span><span class="s1">partial(recall_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;macro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;recall_micro&quot;</span><span class="s0">, </span><span class="s1">partial(recall_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;micro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;jaccard&quot;</span><span class="s0">, </span><span class="s1">jaccard_score)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;jaccard_weighted&quot;</span><span class="s0">, </span><span class="s1">partial(jaccard_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;jaccard_macro&quot;</span><span class="s0">, </span><span class="s1">partial(jaccard_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;macro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;jaccard_micro&quot;</span><span class="s0">, </span><span class="s1">partial(jaccard_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;micro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;top_k_accuracy&quot;</span><span class="s0">, </span><span class="s1">top_k_accuracy_score)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;matthews_corrcoef&quot;</span><span class="s0">, </span><span class="s1">matthews_corrcoef)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_classification_binary_scores(scorer_name</span><span class="s0">, </span><span class="s1">metric):</span>
    <span class="s3"># check consistency between score and scorer for scores supporting</span>
    <span class="s3"># binary classification.</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(random_state=</span><span class="s5">0</span><span class="s0">, </span><span class="s1">centers=</span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">clf = LinearSVC(dual=</span><span class="s2">&quot;auto&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s1">score = get_scorer(scorer_name)(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s1">expected_score = metric(y_test</span><span class="s0">, </span><span class="s1">clf.predict(X_test))</span>
    <span class="s1">assert_almost_equal(score</span><span class="s0">, </span><span class="s1">expected_score)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;scorer_name, metric&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s2">&quot;accuracy&quot;</span><span class="s0">, </span><span class="s1">accuracy_score)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;balanced_accuracy&quot;</span><span class="s0">, </span><span class="s1">balanced_accuracy_score)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;f1_weighted&quot;</span><span class="s0">, </span><span class="s1">partial(f1_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;f1_macro&quot;</span><span class="s0">, </span><span class="s1">partial(f1_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;macro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;f1_micro&quot;</span><span class="s0">, </span><span class="s1">partial(f1_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;micro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;precision_weighted&quot;</span><span class="s0">, </span><span class="s1">partial(precision_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;precision_macro&quot;</span><span class="s0">, </span><span class="s1">partial(precision_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;macro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;precision_micro&quot;</span><span class="s0">, </span><span class="s1">partial(precision_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;micro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;recall_weighted&quot;</span><span class="s0">, </span><span class="s1">partial(recall_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;recall_macro&quot;</span><span class="s0">, </span><span class="s1">partial(recall_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;macro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;recall_micro&quot;</span><span class="s0">, </span><span class="s1">partial(recall_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;micro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;jaccard_weighted&quot;</span><span class="s0">, </span><span class="s1">partial(jaccard_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;jaccard_macro&quot;</span><span class="s0">, </span><span class="s1">partial(jaccard_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;macro&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;jaccard_micro&quot;</span><span class="s0">, </span><span class="s1">partial(jaccard_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;micro&quot;</span><span class="s1">))</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_classification_multiclass_scores(scorer_name</span><span class="s0">, </span><span class="s1">metric):</span>
    <span class="s3"># check consistency between score and scorer for scores supporting</span>
    <span class="s3"># multiclass classification.</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_classes=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">n_informative=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">n_samples=</span><span class="s5">30</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span>
    <span class="s1">)</span>

    <span class="s3"># use `stratify` = y to ensure train and test sets capture all classes</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s0">, </span><span class="s1">stratify=y</span>
    <span class="s1">)</span>

    <span class="s1">clf = DecisionTreeClassifier(random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">score = get_scorer(scorer_name)(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s1">expected_score = metric(y_test</span><span class="s0">, </span><span class="s1">clf.predict(X_test))</span>
    <span class="s0">assert </span><span class="s1">score == pytest.approx(expected_score)</span>


<span class="s0">def </span><span class="s1">test_custom_scorer_pickling():</span>
    <span class="s3"># test that custom scorer can be pickled</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(random_state=</span><span class="s5">0</span><span class="s0">, </span><span class="s1">centers=</span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">clf = LinearSVC(dual=</span><span class="s2">&quot;auto&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s1">scorer = make_scorer(fbeta_score</span><span class="s0">, </span><span class="s1">beta=</span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">score1 = scorer(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s1">unpickled_scorer = pickle.loads(pickle.dumps(scorer))</span>
    <span class="s1">score2 = unpickled_scorer(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s0">assert </span><span class="s1">score1 == pytest.approx(score2)</span>

    <span class="s3"># smoke test the repr:</span>
    <span class="s1">repr(fbeta_score)</span>


<span class="s0">def </span><span class="s1">test_regression_scorers():</span>
    <span class="s3"># Test regression scorers.</span>
    <span class="s1">diabetes = load_diabetes()</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = diabetes.data</span><span class="s0">, </span><span class="s1">diabetes.target</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">clf = Ridge()</span>
    <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">score1 = get_scorer(</span><span class="s2">&quot;r2&quot;</span><span class="s1">)(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s1">score2 = r2_score(y_test</span><span class="s0">, </span><span class="s1">clf.predict(X_test))</span>
    <span class="s1">assert_almost_equal(score1</span><span class="s0">, </span><span class="s1">score2)</span>


<span class="s0">def </span><span class="s1">test_thresholded_scorers():</span>
    <span class="s3"># Test scorers that take thresholds.</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(random_state=</span><span class="s5">0</span><span class="s0">, </span><span class="s1">centers=</span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">clf = LogisticRegression(random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">score1 = get_scorer(</span><span class="s2">&quot;roc_auc&quot;</span><span class="s1">)(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s1">score2 = roc_auc_score(y_test</span><span class="s0">, </span><span class="s1">clf.decision_function(X_test))</span>
    <span class="s1">score3 = roc_auc_score(y_test</span><span class="s0">, </span><span class="s1">clf.predict_proba(X_test)[:</span><span class="s0">, </span><span class="s5">1</span><span class="s1">])</span>
    <span class="s1">assert_almost_equal(score1</span><span class="s0">, </span><span class="s1">score2)</span>
    <span class="s1">assert_almost_equal(score1</span><span class="s0">, </span><span class="s1">score3)</span>

    <span class="s1">logscore = get_scorer(</span><span class="s2">&quot;neg_log_loss&quot;</span><span class="s1">)(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s1">logloss = log_loss(y_test</span><span class="s0">, </span><span class="s1">clf.predict_proba(X_test))</span>
    <span class="s1">assert_almost_equal(-logscore</span><span class="s0">, </span><span class="s1">logloss)</span>

    <span class="s3"># same for an estimator without decision_function</span>
    <span class="s1">clf = DecisionTreeClassifier()</span>
    <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">score1 = get_scorer(</span><span class="s2">&quot;roc_auc&quot;</span><span class="s1">)(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s1">score2 = roc_auc_score(y_test</span><span class="s0">, </span><span class="s1">clf.predict_proba(X_test)[:</span><span class="s0">, </span><span class="s5">1</span><span class="s1">])</span>
    <span class="s1">assert_almost_equal(score1</span><span class="s0">, </span><span class="s1">score2)</span>

    <span class="s3"># test with a regressor (no decision_function)</span>
    <span class="s1">reg = DecisionTreeRegressor()</span>
    <span class="s1">reg.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">score1 = get_scorer(</span><span class="s2">&quot;roc_auc&quot;</span><span class="s1">)(reg</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s1">score2 = roc_auc_score(y_test</span><span class="s0">, </span><span class="s1">reg.predict(X_test))</span>
    <span class="s1">assert_almost_equal(score1</span><span class="s0">, </span><span class="s1">score2)</span>

    <span class="s3"># Test that an exception is raised on more than two classes</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(random_state=</span><span class="s5">0</span><span class="s0">, </span><span class="s1">centers=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;multiclass format is not supported&quot;</span><span class="s1">):</span>
        <span class="s1">get_scorer(</span><span class="s2">&quot;roc_auc&quot;</span><span class="s1">)(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>

    <span class="s3"># test error is raised with a single class present in model</span>
    <span class="s3"># (predict_proba shape is not suitable for binary auc)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(random_state=</span><span class="s5">0</span><span class="s0">, </span><span class="s1">centers=</span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">clf = DecisionTreeClassifier()</span>
    <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">np.zeros_like(y_train))</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;need classifier with two classes&quot;</span><span class="s1">):</span>
        <span class="s1">get_scorer(</span><span class="s2">&quot;roc_auc&quot;</span><span class="s1">)(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>

    <span class="s3"># for proba scorers</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;need classifier with two classes&quot;</span><span class="s1">):</span>
        <span class="s1">get_scorer(</span><span class="s2">&quot;neg_log_loss&quot;</span><span class="s1">)(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>


<span class="s0">def </span><span class="s1">test_thresholded_scorers_multilabel_indicator_data():</span>
    <span class="s3"># Test that the scorer work with multilabel-indicator format</span>
    <span class="s3"># for multilabel and multi-output multi-class classifier</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_multilabel_classification(allow_unlabeled=</span><span class="s0">False, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s3"># Multi-output multi-class predict_proba</span>
    <span class="s1">clf = DecisionTreeClassifier()</span>
    <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">y_proba = clf.predict_proba(X_test)</span>
    <span class="s1">score1 = get_scorer(</span><span class="s2">&quot;roc_auc&quot;</span><span class="s1">)(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s1">score2 = roc_auc_score(y_test</span><span class="s0">, </span><span class="s1">np.vstack([p[:</span><span class="s0">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">] </span><span class="s0">for </span><span class="s1">p </span><span class="s0">in </span><span class="s1">y_proba]).T)</span>
    <span class="s1">assert_almost_equal(score1</span><span class="s0">, </span><span class="s1">score2)</span>

    <span class="s3"># Multi-output multi-class decision_function</span>
    <span class="s3"># TODO Is there any yet?</span>
    <span class="s0">class </span><span class="s1">TreeWithDecisionFunction(DecisionTreeClassifier):</span>
        <span class="s3"># disable predict_proba</span>
        <span class="s1">predict_proba = </span><span class="s0">None</span>

        <span class="s0">def </span><span class="s1">decision_function(self</span><span class="s0">, </span><span class="s1">X):</span>
            <span class="s0">return </span><span class="s1">[p[:</span><span class="s0">, </span><span class="s5">1</span><span class="s1">] </span><span class="s0">for </span><span class="s1">p </span><span class="s0">in </span><span class="s1">DecisionTreeClassifier.predict_proba(self</span><span class="s0">, </span><span class="s1">X)]</span>

    <span class="s1">clf = TreeWithDecisionFunction()</span>
    <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">y_proba = clf.decision_function(X_test)</span>
    <span class="s1">score1 = get_scorer(</span><span class="s2">&quot;roc_auc&quot;</span><span class="s1">)(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s1">score2 = roc_auc_score(y_test</span><span class="s0">, </span><span class="s1">np.vstack([p </span><span class="s0">for </span><span class="s1">p </span><span class="s0">in </span><span class="s1">y_proba]).T)</span>
    <span class="s1">assert_almost_equal(score1</span><span class="s0">, </span><span class="s1">score2)</span>

    <span class="s3"># Multilabel predict_proba</span>
    <span class="s1">clf = OneVsRestClassifier(DecisionTreeClassifier())</span>
    <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">score1 = get_scorer(</span><span class="s2">&quot;roc_auc&quot;</span><span class="s1">)(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s1">score2 = roc_auc_score(y_test</span><span class="s0">, </span><span class="s1">clf.predict_proba(X_test))</span>
    <span class="s1">assert_almost_equal(score1</span><span class="s0">, </span><span class="s1">score2)</span>

    <span class="s3"># Multilabel decision function</span>
    <span class="s1">clf = OneVsRestClassifier(LinearSVC(dual=</span><span class="s2">&quot;auto&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">))</span>
    <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">score1 = get_scorer(</span><span class="s2">&quot;roc_auc&quot;</span><span class="s1">)(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s1">score2 = roc_auc_score(y_test</span><span class="s0">, </span><span class="s1">clf.decision_function(X_test))</span>
    <span class="s1">assert_almost_equal(score1</span><span class="s0">, </span><span class="s1">score2)</span>


<span class="s0">def </span><span class="s1">test_supervised_cluster_scorers():</span>
    <span class="s3"># Test clustering scorers against gold standard labeling.</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(random_state=</span><span class="s5">0</span><span class="s0">, </span><span class="s1">centers=</span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">km = KMeans(n_clusters=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">n_init=</span><span class="s2">&quot;auto&quot;</span><span class="s1">)</span>
    <span class="s1">km.fit(X_train)</span>
    <span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">CLUSTER_SCORERS:</span>
        <span class="s1">score1 = get_scorer(name)(km</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
        <span class="s1">score2 = getattr(cluster_module</span><span class="s0">, </span><span class="s1">name)(y_test</span><span class="s0">, </span><span class="s1">km.predict(X_test))</span>
        <span class="s1">assert_almost_equal(score1</span><span class="s0">, </span><span class="s1">score2)</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">test_raises_on_score_list():</span>
    <span class="s3"># Test that when a list of scores is returned, we raise proper errors.</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">f1_scorer_no_average = make_scorer(f1_score</span><span class="s0">, </span><span class="s1">average=</span><span class="s0">None</span><span class="s1">)</span>
    <span class="s1">clf = DecisionTreeClassifier()</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">cross_val_score(clf</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">scoring=f1_scorer_no_average)</span>
    <span class="s1">grid_search = GridSearchCV(</span>
        <span class="s1">clf</span><span class="s0">, </span><span class="s1">scoring=f1_scorer_no_average</span><span class="s0">, </span><span class="s1">param_grid={</span><span class="s2">&quot;max_depth&quot;</span><span class="s1">: [</span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s1">]}</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">grid_search.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">test_classification_scorer_sample_weight():</span>
    <span class="s3"># Test that classification scorers support sample_weight or raise sensible</span>
    <span class="s3"># errors</span>

    <span class="s3"># Unlike the metrics invariance test, in the scorer case it's harder</span>
    <span class="s3"># to ensure that, on the classifier output, weighted and unweighted</span>
    <span class="s3"># scores really should be unequal.</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">_</span><span class="s0">, </span><span class="s1">y_ml = make_multilabel_classification(n_samples=X.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">split = train_test_split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">y_ml</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test</span><span class="s0">, </span><span class="s1">y_ml_train</span><span class="s0">, </span><span class="s1">y_ml_test = split</span>

    <span class="s1">sample_weight = np.ones_like(y_test)</span>
    <span class="s1">sample_weight[:</span><span class="s5">10</span><span class="s1">] = </span><span class="s5">0</span>

    <span class="s3"># get sensible estimators for each metric</span>
    <span class="s1">estimator = _make_estimators(X_train</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_ml_train)</span>

    <span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">get_scorer_names():</span>
        <span class="s1">scorer = get_scorer(name)</span>
        <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">REGRESSION_SCORERS:</span>
            <span class="s3"># skip the regression scores</span>
            <span class="s0">continue</span>
        <span class="s0">if </span><span class="s1">name == </span><span class="s2">&quot;top_k_accuracy&quot;</span><span class="s1">:</span>
            <span class="s3"># in the binary case k &gt; 1 will always lead to a perfect score</span>
            <span class="s1">scorer._kwargs = {</span><span class="s2">&quot;k&quot;</span><span class="s1">: </span><span class="s5">1</span><span class="s1">}</span>
        <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">MULTILABEL_ONLY_SCORERS:</span>
            <span class="s1">target = y_ml_test</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">target = y_test</span>
        <span class="s0">try</span><span class="s1">:</span>
            <span class="s1">weighted = scorer(</span>
                <span class="s1">estimator[name]</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">target</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight</span>
            <span class="s1">)</span>
            <span class="s1">ignored = scorer(estimator[name]</span><span class="s0">, </span><span class="s1">X_test[</span><span class="s5">10</span><span class="s1">:]</span><span class="s0">, </span><span class="s1">target[</span><span class="s5">10</span><span class="s1">:])</span>
            <span class="s1">unweighted = scorer(estimator[name]</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">target)</span>
            <span class="s3"># this should not raise. sample_weight should be ignored if None.</span>
            <span class="s1">_ = scorer(estimator[name]</span><span class="s0">, </span><span class="s1">X_test[:</span><span class="s5">10</span><span class="s1">]</span><span class="s0">, </span><span class="s1">target[:</span><span class="s5">10</span><span class="s1">]</span><span class="s0">, </span><span class="s1">sample_weight=</span><span class="s0">None</span><span class="s1">)</span>
            <span class="s0">assert </span><span class="s1">weighted != unweighted</span><span class="s0">, </span><span class="s1">(</span>
                <span class="s2">f&quot;scorer </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">behaves identically when called with &quot;</span>
                <span class="s2">f&quot;sample weights: </span><span class="s0">{</span><span class="s1">weighted</span><span class="s0">} </span><span class="s2">vs </span><span class="s0">{</span><span class="s1">unweighted</span><span class="s0">}</span><span class="s2">&quot;</span>
            <span class="s1">)</span>
            <span class="s1">assert_almost_equal(</span>
                <span class="s1">weighted</span><span class="s0">,</span>
                <span class="s1">ignored</span><span class="s0">,</span>
                <span class="s1">err_msg=(</span>
                    <span class="s2">f&quot;scorer </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">behaves differently &quot;</span>
                    <span class="s2">&quot;when ignoring samples and setting &quot;</span>
                    <span class="s2">f&quot;sample_weight to 0: </span><span class="s0">{</span><span class="s1">weighted</span><span class="s0">} </span><span class="s2">vs </span><span class="s0">{</span><span class="s1">ignored</span><span class="s0">}</span><span class="s2">&quot;</span>
                <span class="s1">)</span><span class="s0">,</span>
            <span class="s1">)</span>

        <span class="s0">except </span><span class="s1">TypeError </span><span class="s0">as </span><span class="s1">e:</span>
            <span class="s0">assert </span><span class="s2">&quot;sample_weight&quot; </span><span class="s0">in </span><span class="s1">str(e)</span><span class="s0">, </span><span class="s1">(</span>
                <span class="s2">f&quot;scorer </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">raises unhelpful exception when called &quot;</span>
                <span class="s2">f&quot;with sample weights: </span><span class="s0">{</span><span class="s1">str(e)</span><span class="s0">}</span><span class="s2">&quot;</span>
            <span class="s1">)</span>


<span class="s1">@ignore_warnings</span>
<span class="s0">def </span><span class="s1">test_regression_scorer_sample_weight():</span>
    <span class="s3"># Test that regression scorers support sample_weight or raise sensible</span>
    <span class="s3"># errors</span>

    <span class="s3"># Odd number of test samples req for neg_median_absolute_error</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(n_samples=</span><span class="s5">101</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s5">20</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">y = _require_positive_y(y)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">sample_weight = np.ones_like(y_test)</span>
    <span class="s3"># Odd number req for neg_median_absolute_error</span>
    <span class="s1">sample_weight[:</span><span class="s5">11</span><span class="s1">] = </span><span class="s5">0</span>

    <span class="s1">reg = DecisionTreeRegressor(random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">reg.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">get_scorer_names():</span>
        <span class="s1">scorer = get_scorer(name)</span>
        <span class="s0">if </span><span class="s1">name </span><span class="s0">not in </span><span class="s1">REGRESSION_SCORERS:</span>
            <span class="s3"># skip classification scorers</span>
            <span class="s0">continue</span>
        <span class="s0">try</span><span class="s1">:</span>
            <span class="s1">weighted = scorer(reg</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
            <span class="s1">ignored = scorer(reg</span><span class="s0">, </span><span class="s1">X_test[</span><span class="s5">11</span><span class="s1">:]</span><span class="s0">, </span><span class="s1">y_test[</span><span class="s5">11</span><span class="s1">:])</span>
            <span class="s1">unweighted = scorer(reg</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
            <span class="s0">assert </span><span class="s1">weighted != unweighted</span><span class="s0">, </span><span class="s1">(</span>
                <span class="s2">f&quot;scorer </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">behaves identically when called with &quot;</span>
                <span class="s2">f&quot;sample weights: </span><span class="s0">{</span><span class="s1">weighted</span><span class="s0">} </span><span class="s2">vs </span><span class="s0">{</span><span class="s1">unweighted</span><span class="s0">}</span><span class="s2">&quot;</span>
            <span class="s1">)</span>
            <span class="s1">assert_almost_equal(</span>
                <span class="s1">weighted</span><span class="s0">,</span>
                <span class="s1">ignored</span><span class="s0">,</span>
                <span class="s1">err_msg=(</span>
                    <span class="s2">f&quot;scorer </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">behaves differently &quot;</span>
                    <span class="s2">&quot;when ignoring samples and setting &quot;</span>
                    <span class="s2">f&quot;sample_weight to 0: </span><span class="s0">{</span><span class="s1">weighted</span><span class="s0">} </span><span class="s2">vs </span><span class="s0">{</span><span class="s1">ignored</span><span class="s0">}</span><span class="s2">&quot;</span>
                <span class="s1">)</span><span class="s0">,</span>
            <span class="s1">)</span>

        <span class="s0">except </span><span class="s1">TypeError </span><span class="s0">as </span><span class="s1">e:</span>
            <span class="s0">assert </span><span class="s2">&quot;sample_weight&quot; </span><span class="s0">in </span><span class="s1">str(e)</span><span class="s0">, </span><span class="s1">(</span>
                <span class="s2">f&quot;scorer </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">raises unhelpful exception when called &quot;</span>
                <span class="s2">f&quot;with sample weights: </span><span class="s0">{</span><span class="s1">str(e)</span><span class="s0">}</span><span class="s2">&quot;</span>
            <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;name&quot;</span><span class="s0">, </span><span class="s1">get_scorer_names())</span>
<span class="s0">def </span><span class="s1">test_scorer_memmap_input(name):</span>
    <span class="s3"># Non-regression test for #6147: some score functions would</span>
    <span class="s3"># return singleton memmap when computed on memmap data instead of scalar</span>
    <span class="s3"># float values.</span>

    <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">REQUIRE_POSITIVE_Y_SCORERS:</span>
        <span class="s1">y_mm_1 = _require_positive_y(y_mm)</span>
        <span class="s1">y_ml_mm_1 = _require_positive_y(y_ml_mm)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y_mm_1</span><span class="s0">, </span><span class="s1">y_ml_mm_1 = y_mm</span><span class="s0">, </span><span class="s1">y_ml_mm</span>

    <span class="s3"># UndefinedMetricWarning for P / R scores</span>
    <span class="s0">with </span><span class="s1">ignore_warnings():</span>
        <span class="s1">scorer</span><span class="s0">, </span><span class="s1">estimator = get_scorer(name)</span><span class="s0">, </span><span class="s1">ESTIMATORS[name]</span>
        <span class="s0">if </span><span class="s1">name </span><span class="s0">in </span><span class="s1">MULTILABEL_ONLY_SCORERS:</span>
            <span class="s1">score = scorer(estimator</span><span class="s0">, </span><span class="s1">X_mm</span><span class="s0">, </span><span class="s1">y_ml_mm_1)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">score = scorer(estimator</span><span class="s0">, </span><span class="s1">X_mm</span><span class="s0">, </span><span class="s1">y_mm_1)</span>
        <span class="s0">assert </span><span class="s1">isinstance(score</span><span class="s0">, </span><span class="s1">numbers.Number)</span><span class="s0">, </span><span class="s1">name</span>


<span class="s0">def </span><span class="s1">test_scoring_is_not_metric():</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;make_scorer&quot;</span><span class="s1">):</span>
        <span class="s1">check_scoring(LogisticRegression()</span><span class="s0">, </span><span class="s1">scoring=f1_score)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;make_scorer&quot;</span><span class="s1">):</span>
        <span class="s1">check_scoring(LogisticRegression()</span><span class="s0">, </span><span class="s1">scoring=roc_auc_score)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;make_scorer&quot;</span><span class="s1">):</span>
        <span class="s1">check_scoring(Ridge()</span><span class="s0">, </span><span class="s1">scoring=r2_score)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;make_scorer&quot;</span><span class="s1">):</span>
        <span class="s1">check_scoring(KMeans()</span><span class="s0">, </span><span class="s1">scoring=cluster_module.adjusted_rand_score)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;make_scorer&quot;</span><span class="s1">):</span>
        <span class="s1">check_scoring(KMeans()</span><span class="s0">, </span><span class="s1">scoring=cluster_module.rand_score)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s1">(</span>
        <span class="s2">&quot;scorers,expected_predict_count,&quot;</span>
        <span class="s2">&quot;expected_predict_proba_count,expected_decision_func_count&quot;</span>
    <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">{</span>
                <span class="s2">&quot;a1&quot;</span><span class="s1">: </span><span class="s2">&quot;accuracy&quot;</span><span class="s0">,</span>
                <span class="s2">&quot;a2&quot;</span><span class="s1">: </span><span class="s2">&quot;accuracy&quot;</span><span class="s0">,</span>
                <span class="s2">&quot;ll1&quot;</span><span class="s1">: </span><span class="s2">&quot;neg_log_loss&quot;</span><span class="s0">,</span>
                <span class="s2">&quot;ll2&quot;</span><span class="s1">: </span><span class="s2">&quot;neg_log_loss&quot;</span><span class="s0">,</span>
                <span class="s2">&quot;ra1&quot;</span><span class="s1">: </span><span class="s2">&quot;roc_auc&quot;</span><span class="s0">,</span>
                <span class="s2">&quot;ra2&quot;</span><span class="s1">: </span><span class="s2">&quot;roc_auc&quot;</span><span class="s0">,</span>
            <span class="s1">}</span><span class="s0">,</span>
            <span class="s5">1</span><span class="s0">,</span>
            <span class="s5">1</span><span class="s0">,</span>
            <span class="s5">1</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">([</span><span class="s2">&quot;roc_auc&quot;</span><span class="s0">, </span><span class="s2">&quot;accuracy&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">([</span><span class="s2">&quot;neg_log_loss&quot;</span><span class="s0">, </span><span class="s2">&quot;accuracy&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_multimetric_scorer_calls_method_once(</span>
    <span class="s1">scorers</span><span class="s0">,</span>
    <span class="s1">expected_predict_count</span><span class="s0">,</span>
    <span class="s1">expected_predict_proba_count</span><span class="s0">,</span>
    <span class="s1">expected_decision_func_count</span><span class="s0">,</span>
<span class="s1">):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = np.array([[</span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s5">0</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s1">])</span>

    <span class="s1">mock_est = Mock()</span>
    <span class="s1">mock_est._estimator_type = </span><span class="s2">&quot;classifier&quot;</span>
    <span class="s1">fit_func = Mock(return_value=mock_est</span><span class="s0">, </span><span class="s1">name=</span><span class="s2">&quot;fit&quot;</span><span class="s1">)</span>
    <span class="s1">fit_func.__name__ = </span><span class="s2">&quot;fit&quot;</span>
    <span class="s1">predict_func = Mock(return_value=y</span><span class="s0">, </span><span class="s1">name=</span><span class="s2">&quot;predict&quot;</span><span class="s1">)</span>
    <span class="s1">predict_func.__name__ = </span><span class="s2">&quot;predict&quot;</span>

    <span class="s1">pos_proba = np.random.rand(X.shape[</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">proba = np.c_[</span><span class="s5">1 </span><span class="s1">- pos_proba</span><span class="s0">, </span><span class="s1">pos_proba]</span>
    <span class="s1">predict_proba_func = Mock(return_value=proba</span><span class="s0">, </span><span class="s1">name=</span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">)</span>
    <span class="s1">predict_proba_func.__name__ = </span><span class="s2">&quot;predict_proba&quot;</span>
    <span class="s1">decision_function_func = Mock(return_value=pos_proba</span><span class="s0">, </span><span class="s1">name=</span><span class="s2">&quot;decision_function&quot;</span><span class="s1">)</span>
    <span class="s1">decision_function_func.__name__ = </span><span class="s2">&quot;decision_function&quot;</span>

    <span class="s1">mock_est.fit = fit_func</span>
    <span class="s1">mock_est.predict = predict_func</span>
    <span class="s1">mock_est.predict_proba = predict_proba_func</span>
    <span class="s1">mock_est.decision_function = decision_function_func</span>
    <span class="s3"># add the classes that would be found during fit</span>
    <span class="s1">mock_est.classes_ = np.array([</span><span class="s5">0</span><span class="s0">, </span><span class="s5">1</span><span class="s1">])</span>

    <span class="s1">scorer_dict = _check_multimetric_scoring(LogisticRegression()</span><span class="s0">, </span><span class="s1">scorers)</span>
    <span class="s1">multi_scorer = _MultimetricScorer(scorers=scorer_dict)</span>
    <span class="s1">results = multi_scorer(mock_est</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">assert </span><span class="s1">set(scorers) == set(results)  </span><span class="s3"># compare dict keys</span>

    <span class="s0">assert </span><span class="s1">predict_func.call_count == expected_predict_count</span>
    <span class="s0">assert </span><span class="s1">predict_proba_func.call_count == expected_predict_proba_count</span>
    <span class="s0">assert </span><span class="s1">decision_function_func.call_count == expected_decision_func_count</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;scorers&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">([</span><span class="s2">&quot;roc_auc&quot;</span><span class="s0">, </span><span class="s2">&quot;neg_log_loss&quot;</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span>
                <span class="s2">&quot;roc_auc&quot;</span><span class="s1">: make_scorer(roc_auc_score</span><span class="s0">, </span><span class="s1">needs_threshold=</span><span class="s0">True</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s2">&quot;neg_log_loss&quot;</span><span class="s1">: make_scorer(log_loss</span><span class="s0">, </span><span class="s1">needs_proba=</span><span class="s0">True</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">}</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_multimetric_scorer_calls_method_once_classifier_no_decision(scorers):</span>
    <span class="s1">predict_proba_call_cnt = </span><span class="s5">0</span>

    <span class="s0">class </span><span class="s1">MockKNeighborsClassifier(KNeighborsClassifier):</span>
        <span class="s0">def </span><span class="s1">predict_proba(self</span><span class="s0">, </span><span class="s1">X):</span>
            <span class="s0">nonlocal </span><span class="s1">predict_proba_call_cnt</span>
            <span class="s1">predict_proba_call_cnt += </span><span class="s5">1</span>
            <span class="s0">return </span><span class="s1">super().predict_proba(X)</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = np.array([[</span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s5">0</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s1">])</span>

    <span class="s3"># no decision function</span>
    <span class="s1">clf = MockKNeighborsClassifier(n_neighbors=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">scorer_dict = _check_multimetric_scoring(clf</span><span class="s0">, </span><span class="s1">scorers)</span>
    <span class="s1">scorer = _MultimetricScorer(scorers=scorer_dict)</span>
    <span class="s1">scorer(clf</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">assert </span><span class="s1">predict_proba_call_cnt == </span><span class="s5">1</span>


<span class="s0">def </span><span class="s1">test_multimetric_scorer_calls_method_once_regressor_threshold():</span>
    <span class="s1">predict_called_cnt = </span><span class="s5">0</span>

    <span class="s0">class </span><span class="s1">MockDecisionTreeRegressor(DecisionTreeRegressor):</span>
        <span class="s0">def </span><span class="s1">predict(self</span><span class="s0">, </span><span class="s1">X):</span>
            <span class="s0">nonlocal </span><span class="s1">predict_called_cnt</span>
            <span class="s1">predict_called_cnt += </span><span class="s5">1</span>
            <span class="s0">return </span><span class="s1">super().predict(X)</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = np.array([[</span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">0</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s5">0</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s1">])</span>

    <span class="s3"># no decision function</span>
    <span class="s1">clf = MockDecisionTreeRegressor()</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">scorers = {</span><span class="s2">&quot;neg_mse&quot;</span><span class="s1">: </span><span class="s2">&quot;neg_mean_squared_error&quot;</span><span class="s0">, </span><span class="s2">&quot;r2&quot;</span><span class="s1">: </span><span class="s2">&quot;r2&quot;</span><span class="s1">}</span>
    <span class="s1">scorer_dict = _check_multimetric_scoring(clf</span><span class="s0">, </span><span class="s1">scorers)</span>
    <span class="s1">scorer = _MultimetricScorer(scorers=scorer_dict)</span>
    <span class="s1">scorer(clf</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">assert </span><span class="s1">predict_called_cnt == </span><span class="s5">1</span>


<span class="s0">def </span><span class="s1">test_multimetric_scorer_sanity_check():</span>
    <span class="s3"># scoring dictionary returned is the same as calling each scorer separately</span>
    <span class="s1">scorers = {</span>
        <span class="s2">&quot;a1&quot;</span><span class="s1">: </span><span class="s2">&quot;accuracy&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;a2&quot;</span><span class="s1">: </span><span class="s2">&quot;accuracy&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;ll1&quot;</span><span class="s1">: </span><span class="s2">&quot;neg_log_loss&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;ll2&quot;</span><span class="s1">: </span><span class="s2">&quot;neg_log_loss&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;ra1&quot;</span><span class="s1">: </span><span class="s2">&quot;roc_auc&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;ra2&quot;</span><span class="s1">: </span><span class="s2">&quot;roc_auc&quot;</span><span class="s0">,</span>
    <span class="s1">}</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(random_state=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">clf = DecisionTreeClassifier()</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">scorer_dict = _check_multimetric_scoring(clf</span><span class="s0">, </span><span class="s1">scorers)</span>
    <span class="s1">multi_scorer = _MultimetricScorer(scorers=scorer_dict)</span>

    <span class="s1">result = multi_scorer(clf</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">separate_scores = {</span>
        <span class="s1">name: get_scorer(name)(clf</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;accuracy&quot;</span><span class="s0">, </span><span class="s2">&quot;neg_log_loss&quot;</span><span class="s0">, </span><span class="s2">&quot;roc_auc&quot;</span><span class="s1">]</span>
    <span class="s1">}</span>

    <span class="s0">for </span><span class="s1">key</span><span class="s0">, </span><span class="s1">value </span><span class="s0">in </span><span class="s1">result.items():</span>
        <span class="s1">score_name = scorers[key]</span>
        <span class="s1">assert_allclose(value</span><span class="s0">, </span><span class="s1">separate_scores[score_name])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;raise_exc&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_multimetric_scorer_exception_handling(raise_exc):</span>
    <span class="s4">&quot;&quot;&quot;Check that the calling of the `_MultimetricScorer` returns 
    exception messages in the result dict for the failing scorers 
    in case of `raise_exc` is `False` and if `raise_exc` is `True`, 
    then the proper exception is raised. 
    &quot;&quot;&quot;</span>
    <span class="s1">scorers = {</span>
        <span class="s2">&quot;failing_1&quot;</span><span class="s1">: </span><span class="s2">&quot;neg_mean_squared_log_error&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;non_failing&quot;</span><span class="s1">: </span><span class="s2">&quot;neg_median_absolute_error&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;failing_2&quot;</span><span class="s1">: </span><span class="s2">&quot;neg_mean_squared_log_error&quot;</span><span class="s0">,</span>
    <span class="s1">}</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s5">50</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s5">2</span><span class="s0">, </span><span class="s1">n_redundant=</span><span class="s5">0</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span>
    <span class="s1">)</span>
    <span class="s1">y *= -</span><span class="s5">1  </span><span class="s3"># neg_mean_squared_log_error fails if y contains negative values</span>

    <span class="s1">clf = DecisionTreeClassifier().fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">scorer_dict = _check_multimetric_scoring(clf</span><span class="s0">, </span><span class="s1">scorers)</span>
    <span class="s1">multi_scorer = _MultimetricScorer(scorers=scorer_dict</span><span class="s0">, </span><span class="s1">raise_exc=raise_exc)</span>

    <span class="s1">error_msg = (</span>
        <span class="s2">&quot;Mean Squared Logarithmic Error cannot be used when targets contain&quot;</span>
        <span class="s2">&quot; negative values.&quot;</span>
    <span class="s1">)</span>

    <span class="s0">if </span><span class="s1">raise_exc:</span>
        <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=error_msg):</span>
            <span class="s1">multi_scorer(clf</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">result = multi_scorer(clf</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y)</span>

        <span class="s1">exception_message_1 = result[</span><span class="s2">&quot;failing_1&quot;</span><span class="s1">]</span>
        <span class="s1">score = result[</span><span class="s2">&quot;non_failing&quot;</span><span class="s1">]</span>
        <span class="s1">exception_message_2 = result[</span><span class="s2">&quot;failing_2&quot;</span><span class="s1">]</span>

        <span class="s0">assert </span><span class="s1">isinstance(exception_message_1</span><span class="s0">, </span><span class="s1">str) </span><span class="s0">and </span><span class="s1">error_msg </span><span class="s0">in </span><span class="s1">exception_message_1</span>
        <span class="s0">assert </span><span class="s1">isinstance(score</span><span class="s0">, </span><span class="s1">float)</span>
        <span class="s0">assert </span><span class="s1">isinstance(exception_message_2</span><span class="s0">, </span><span class="s1">str) </span><span class="s0">and </span><span class="s1">error_msg </span><span class="s0">in </span><span class="s1">exception_message_2</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;scorer_name, metric&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s2">&quot;roc_auc_ovr&quot;</span><span class="s0">, </span><span class="s1">partial(roc_auc_score</span><span class="s0">, </span><span class="s1">multi_class=</span><span class="s2">&quot;ovr&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;roc_auc_ovo&quot;</span><span class="s0">, </span><span class="s1">partial(roc_auc_score</span><span class="s0">, </span><span class="s1">multi_class=</span><span class="s2">&quot;ovo&quot;</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s2">&quot;roc_auc_ovr_weighted&quot;</span><span class="s0">,</span>
            <span class="s1">partial(roc_auc_score</span><span class="s0">, </span><span class="s1">multi_class=</span><span class="s2">&quot;ovr&quot;</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s2">&quot;roc_auc_ovo_weighted&quot;</span><span class="s0">,</span>
            <span class="s1">partial(roc_auc_score</span><span class="s0">, </span><span class="s1">multi_class=</span><span class="s2">&quot;ovo&quot;</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_multiclass_roc_proba_scorer(scorer_name</span><span class="s0">, </span><span class="s1">metric):</span>
    <span class="s1">scorer = get_scorer(scorer_name)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_classes=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">n_informative=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">n_samples=</span><span class="s5">20</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span>
    <span class="s1">)</span>
    <span class="s1">lr = LogisticRegression(multi_class=</span><span class="s2">&quot;multinomial&quot;</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">y_proba = lr.predict_proba(X)</span>
    <span class="s1">expected_score = metric(y</span><span class="s0">, </span><span class="s1">y_proba)</span>

    <span class="s0">assert </span><span class="s1">scorer(lr</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y) == pytest.approx(expected_score)</span>


<span class="s0">def </span><span class="s1">test_multiclass_roc_proba_scorer_label():</span>
    <span class="s1">scorer = make_scorer(</span>
        <span class="s1">roc_auc_score</span><span class="s0">, </span><span class="s1">multi_class=</span><span class="s2">&quot;ovo&quot;</span><span class="s0">, </span><span class="s1">labels=[</span><span class="s5">0</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">needs_proba=</span><span class="s0">True</span>
    <span class="s1">)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_classes=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">n_informative=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">n_samples=</span><span class="s5">20</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span>
    <span class="s1">)</span>
    <span class="s1">lr = LogisticRegression(multi_class=</span><span class="s2">&quot;multinomial&quot;</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">y_proba = lr.predict_proba(X)</span>

    <span class="s1">y_binary = y == </span><span class="s5">0</span>
    <span class="s1">expected_score = roc_auc_score(</span>
        <span class="s1">y_binary</span><span class="s0">, </span><span class="s1">y_proba</span><span class="s0">, </span><span class="s1">multi_class=</span><span class="s2">&quot;ovo&quot;</span><span class="s0">, </span><span class="s1">labels=[</span><span class="s5">0</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">scorer(lr</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y_binary) == pytest.approx(expected_score)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;scorer_name&quot;</span><span class="s0">,</span>
    <span class="s1">[</span><span class="s2">&quot;roc_auc_ovr&quot;</span><span class="s0">, </span><span class="s2">&quot;roc_auc_ovo&quot;</span><span class="s0">, </span><span class="s2">&quot;roc_auc_ovr_weighted&quot;</span><span class="s0">, </span><span class="s2">&quot;roc_auc_ovo_weighted&quot;</span><span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_multiclass_roc_no_proba_scorer_errors(scorer_name):</span>
    <span class="s3"># Perceptron has no predict_proba</span>
    <span class="s1">scorer = get_scorer(scorer_name)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_classes=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">n_informative=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">n_samples=</span><span class="s5">20</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span>
    <span class="s1">)</span>
    <span class="s1">lr = Perceptron().fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">msg = </span><span class="s2">&quot;Perceptron has none of the following attributes: predict_proba.&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(AttributeError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">scorer(lr</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.fixture</span>
<span class="s0">def </span><span class="s1">string_labeled_classification_problem():</span>
    <span class="s4">&quot;&quot;&quot;Train a classifier on binary problem with string target. 
 
    The classifier is trained on a binary classification problem where the 
    minority class of interest has a string label that is intentionally not the 
    greatest class label using the lexicographic order. In this case, &quot;cancer&quot; 
    is the positive label, and `classifier.classes_` is 
    `[&quot;cancer&quot;, &quot;not cancer&quot;]`. 
 
    In addition, the dataset is imbalanced to better identify problems when 
    using non-symmetric performance metrics such as f1-score, average precision 
    and so on. 
 
    Returns 
    ------- 
    classifier : estimator object 
        Trained classifier on the binary problem. 
    X_test : ndarray of shape (n_samples, n_features) 
        Data to be used as testing set in tests. 
    y_test : ndarray of shape (n_samples,), dtype=object 
        Binary target where labels are strings. 
    y_pred : ndarray of shape (n_samples,), dtype=object 
        Prediction of `classifier` when predicting for `X_test`. 
    y_pred_proba : ndarray of shape (n_samples, 2), dtype=np.float64 
        Probabilities of `classifier` when predicting for `X_test`. 
    y_pred_decision : ndarray of shape (n_samples,), dtype=np.float64 
        Decision function values of `classifier` when predicting on `X_test`. 
    &quot;&quot;&quot;</span>
    <span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">load_breast_cancer</span>
    <span class="s0">from </span><span class="s1">sklearn.utils </span><span class="s0">import </span><span class="s1">shuffle</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = load_breast_cancer(return_X_y=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s3"># create an highly imbalanced classification task</span>
    <span class="s1">idx_positive = np.flatnonzero(y == </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">idx_negative = np.flatnonzero(y == </span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">idx_selected = np.hstack([idx_negative</span><span class="s0">, </span><span class="s1">idx_positive[:</span><span class="s5">25</span><span class="s1">]])</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = X[idx_selected]</span><span class="s0">, </span><span class="s1">y[idx_selected]</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = shuffle(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">42</span><span class="s1">)</span>
    <span class="s3"># only use 2 features to make the problem even harder</span>
    <span class="s1">X = X[:</span><span class="s0">, </span><span class="s1">:</span><span class="s5">2</span><span class="s1">]</span>
    <span class="s1">y = np.array([</span><span class="s2">&quot;cancer&quot; </span><span class="s0">if </span><span class="s1">c == </span><span class="s5">1 </span><span class="s0">else </span><span class="s2">&quot;not cancer&quot; </span><span class="s0">for </span><span class="s1">c </span><span class="s0">in </span><span class="s1">y]</span><span class="s0">, </span><span class="s1">dtype=object)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">X</span><span class="s0">,</span>
        <span class="s1">y</span><span class="s0">,</span>
        <span class="s1">stratify=y</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s5">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">classifier = LogisticRegression().fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">y_pred = classifier.predict(X_test)</span>
    <span class="s1">y_pred_proba = classifier.predict_proba(X_test)</span>
    <span class="s1">y_pred_decision = classifier.decision_function(X_test)</span>

    <span class="s0">return </span><span class="s1">classifier</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test</span><span class="s0">, </span><span class="s1">y_pred</span><span class="s0">, </span><span class="s1">y_pred_proba</span><span class="s0">, </span><span class="s1">y_pred_decision</span>


<span class="s0">def </span><span class="s1">test_average_precision_pos_label(string_labeled_classification_problem):</span>
    <span class="s3"># check that _ThresholdScorer will lead to the right score when passing</span>
    <span class="s3"># `pos_label`. Currently, only `average_precision_score` is defined to</span>
    <span class="s3"># be such a scorer.</span>
    <span class="s1">(</span>
        <span class="s1">clf</span><span class="s0">,</span>
        <span class="s1">X_test</span><span class="s0">,</span>
        <span class="s1">y_test</span><span class="s0">,</span>
        <span class="s1">_</span><span class="s0">,</span>
        <span class="s1">y_pred_proba</span><span class="s0">,</span>
        <span class="s1">y_pred_decision</span><span class="s0">,</span>
    <span class="s1">) = string_labeled_classification_problem</span>

    <span class="s1">pos_label = </span><span class="s2">&quot;cancer&quot;</span>
    <span class="s3"># we need to select the positive column or reverse the decision values</span>
    <span class="s1">y_pred_proba = y_pred_proba[:</span><span class="s0">, </span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">y_pred_decision = y_pred_decision * -</span><span class="s5">1</span>
    <span class="s0">assert </span><span class="s1">clf.classes_[</span><span class="s5">0</span><span class="s1">] == pos_label</span>

    <span class="s3"># check that when calling the scoring function, probability estimates and</span>
    <span class="s3"># decision values lead to the same results</span>
    <span class="s1">ap_proba = average_precision_score(y_test</span><span class="s0">, </span><span class="s1">y_pred_proba</span><span class="s0">, </span><span class="s1">pos_label=pos_label)</span>
    <span class="s1">ap_decision_function = average_precision_score(</span>
        <span class="s1">y_test</span><span class="s0">, </span><span class="s1">y_pred_decision</span><span class="s0">, </span><span class="s1">pos_label=pos_label</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">ap_proba == pytest.approx(ap_decision_function)</span>

    <span class="s3"># create a scorer which would require to pass a `pos_label`</span>
    <span class="s3"># check that it fails if `pos_label` is not provided</span>
    <span class="s1">average_precision_scorer = make_scorer(</span>
        <span class="s1">average_precision_score</span><span class="s0">,</span>
        <span class="s1">needs_threshold=</span><span class="s0">True,</span>
    <span class="s1">)</span>
    <span class="s1">err_msg = </span><span class="s2">&quot;pos_label=1 is not a valid label. It should be one of &quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">average_precision_scorer(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>

    <span class="s3"># otherwise, the scorer should give the same results than calling the</span>
    <span class="s3"># scoring function</span>
    <span class="s1">average_precision_scorer = make_scorer(</span>
        <span class="s1">average_precision_score</span><span class="s0">, </span><span class="s1">needs_threshold=</span><span class="s0">True, </span><span class="s1">pos_label=pos_label</span>
    <span class="s1">)</span>
    <span class="s1">ap_scorer = average_precision_scorer(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>

    <span class="s0">assert </span><span class="s1">ap_scorer == pytest.approx(ap_proba)</span>

    <span class="s3"># The above scorer call is using `clf.decision_function`. We will force</span>
    <span class="s3"># it to use `clf.predict_proba`.</span>
    <span class="s1">clf_without_predict_proba = deepcopy(clf)</span>

    <span class="s0">def </span><span class="s1">_predict_proba(self</span><span class="s0">, </span><span class="s1">X):</span>
        <span class="s0">raise </span><span class="s1">NotImplementedError</span>

    <span class="s1">clf_without_predict_proba.predict_proba = partial(</span>
        <span class="s1">_predict_proba</span><span class="s0">, </span><span class="s1">clf_without_predict_proba</span>
    <span class="s1">)</span>
    <span class="s3"># sanity check</span>
    <span class="s0">with </span><span class="s1">pytest.raises(NotImplementedError):</span>
        <span class="s1">clf_without_predict_proba.predict_proba(X_test)</span>

    <span class="s1">ap_scorer = average_precision_scorer(clf_without_predict_proba</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s0">assert </span><span class="s1">ap_scorer == pytest.approx(ap_proba)</span>


<span class="s0">def </span><span class="s1">test_brier_score_loss_pos_label(string_labeled_classification_problem):</span>
    <span class="s3"># check that _ProbaScorer leads to the right score when `pos_label` is</span>
    <span class="s3"># provided. Currently only the `brier_score_loss` is defined to be such</span>
    <span class="s3"># a scorer.</span>
    <span class="s1">clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test</span><span class="s0">, </span><span class="s1">_</span><span class="s0">, </span><span class="s1">y_pred_proba</span><span class="s0">, </span><span class="s1">_ = string_labeled_classification_problem</span>

    <span class="s1">pos_label = </span><span class="s2">&quot;cancer&quot;</span>
    <span class="s0">assert </span><span class="s1">clf.classes_[</span><span class="s5">0</span><span class="s1">] == pos_label</span>

    <span class="s3"># brier score loss is symmetric</span>
    <span class="s1">brier_pos_cancer = brier_score_loss(y_test</span><span class="s0">, </span><span class="s1">y_pred_proba[:</span><span class="s0">, </span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">pos_label=</span><span class="s2">&quot;cancer&quot;</span><span class="s1">)</span>
    <span class="s1">brier_pos_not_cancer = brier_score_loss(</span>
        <span class="s1">y_test</span><span class="s0">, </span><span class="s1">y_pred_proba[:</span><span class="s0">, </span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">pos_label=</span><span class="s2">&quot;not cancer&quot;</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">brier_pos_cancer == pytest.approx(brier_pos_not_cancer)</span>

    <span class="s1">brier_scorer = make_scorer(</span>
        <span class="s1">brier_score_loss</span><span class="s0">,</span>
        <span class="s1">needs_proba=</span><span class="s0">True,</span>
        <span class="s1">pos_label=pos_label</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">brier_scorer(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test) == pytest.approx(brier_pos_cancer)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;score_func&quot;</span><span class="s0">, </span><span class="s1">[f1_score</span><span class="s0">, </span><span class="s1">precision_score</span><span class="s0">, </span><span class="s1">recall_score</span><span class="s0">, </span><span class="s1">jaccard_score]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_non_symmetric_metric_pos_label(</span>
    <span class="s1">score_func</span><span class="s0">, </span><span class="s1">string_labeled_classification_problem</span>
<span class="s1">):</span>
    <span class="s3"># check that _PredictScorer leads to the right score when `pos_label` is</span>
    <span class="s3"># provided. We check for all possible metric supported.</span>
    <span class="s3"># Note: At some point we may end up having &quot;scorer tags&quot;.</span>
    <span class="s1">clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test</span><span class="s0">, </span><span class="s1">y_pred</span><span class="s0">, </span><span class="s1">_</span><span class="s0">, </span><span class="s1">_ = string_labeled_classification_problem</span>

    <span class="s1">pos_label = </span><span class="s2">&quot;cancer&quot;</span>
    <span class="s0">assert </span><span class="s1">clf.classes_[</span><span class="s5">0</span><span class="s1">] == pos_label</span>

    <span class="s1">score_pos_cancer = score_func(y_test</span><span class="s0">, </span><span class="s1">y_pred</span><span class="s0">, </span><span class="s1">pos_label=</span><span class="s2">&quot;cancer&quot;</span><span class="s1">)</span>
    <span class="s1">score_pos_not_cancer = score_func(y_test</span><span class="s0">, </span><span class="s1">y_pred</span><span class="s0">, </span><span class="s1">pos_label=</span><span class="s2">&quot;not cancer&quot;</span><span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">score_pos_cancer != pytest.approx(score_pos_not_cancer)</span>

    <span class="s1">scorer = make_scorer(score_func</span><span class="s0">, </span><span class="s1">pos_label=pos_label)</span>
    <span class="s0">assert </span><span class="s1">scorer(clf</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test) == pytest.approx(score_pos_cancer)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;scorer&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">make_scorer(average_precision_score</span><span class="s0">, </span><span class="s1">needs_threshold=</span><span class="s0">True, </span><span class="s1">pos_label=</span><span class="s2">&quot;xxx&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">make_scorer(brier_score_loss</span><span class="s0">, </span><span class="s1">needs_proba=</span><span class="s0">True, </span><span class="s1">pos_label=</span><span class="s2">&quot;xxx&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">make_scorer(f1_score</span><span class="s0">, </span><span class="s1">pos_label=</span><span class="s2">&quot;xxx&quot;</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
    <span class="s1">ids=[</span><span class="s2">&quot;ThresholdScorer&quot;</span><span class="s0">, </span><span class="s2">&quot;ProbaScorer&quot;</span><span class="s0">, </span><span class="s2">&quot;PredictScorer&quot;</span><span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_scorer_select_proba_error(scorer):</span>
    <span class="s3"># check that we raise the proper error when passing an unknown</span>
    <span class="s3"># pos_label</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_classes=</span><span class="s5">2</span><span class="s0">, </span><span class="s1">n_informative=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">n_samples=</span><span class="s5">20</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span>
    <span class="s1">)</span>
    <span class="s1">lr = LogisticRegression().fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">scorer._kwargs[</span><span class="s2">&quot;pos_label&quot;</span><span class="s1">] </span><span class="s0">not in </span><span class="s1">np.unique(y).tolist()</span>

    <span class="s1">err_msg = </span><span class="s2">&quot;is not a valid label&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">scorer(lr</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_get_scorer_return_copy():</span>
    <span class="s3"># test that get_scorer returns a copy</span>
    <span class="s0">assert </span><span class="s1">get_scorer(</span><span class="s2">&quot;roc_auc&quot;</span><span class="s1">) </span><span class="s0">is not </span><span class="s1">get_scorer(</span><span class="s2">&quot;roc_auc&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_scorer_no_op_multiclass_select_proba():</span>
    <span class="s3"># check that calling a ProbaScorer on a multiclass problem do not raise</span>
    <span class="s3"># even if `y_true` would be binary during the scoring.</span>
    <span class="s3"># `_select_proba_binary` should not be called in this case.</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_classes=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">n_informative=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">n_samples=</span><span class="s5">20</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span>
    <span class="s1">)</span>
    <span class="s1">lr = LogisticRegression().fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">mask_last_class = y == lr.classes_[-</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test = X[~mask_last_class]</span><span class="s0">, </span><span class="s1">y[~mask_last_class]</span>
    <span class="s1">assert_array_equal(np.unique(y_test)</span><span class="s0">, </span><span class="s1">lr.classes_[:-</span><span class="s5">1</span><span class="s1">])</span>

    <span class="s1">scorer = make_scorer(</span>
        <span class="s1">roc_auc_score</span><span class="s0">,</span>
        <span class="s1">needs_proba=</span><span class="s0">True,</span>
        <span class="s1">multi_class=</span><span class="s2">&quot;ovo&quot;</span><span class="s0">,</span>
        <span class="s1">labels=lr.classes_</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">scorer(lr</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_test)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;name&quot;</span><span class="s0">, </span><span class="s1">get_scorer_names()</span><span class="s0">, </span><span class="s1">ids=get_scorer_names())</span>
<span class="s0">def </span><span class="s1">test_scorer_metadata_request(name):</span>
    <span class="s4">&quot;&quot;&quot;Testing metadata requests for scorers. 
 
    This test checks many small things in a large test, to reduce the 
    boilerplate required for each section. 
    &quot;&quot;&quot;</span>
    <span class="s0">with </span><span class="s1">config_context(enable_metadata_routing=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s3"># Make sure they expose the routing methods.</span>
        <span class="s1">scorer = get_scorer(name)</span>
        <span class="s0">assert </span><span class="s1">hasattr(scorer</span><span class="s0">, </span><span class="s2">&quot;set_score_request&quot;</span><span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">hasattr(scorer</span><span class="s0">, </span><span class="s2">&quot;get_metadata_routing&quot;</span><span class="s1">)</span>

        <span class="s3"># Check that by default no metadata is requested.</span>
        <span class="s1">assert_request_is_empty(scorer.get_metadata_routing())</span>

        <span class="s1">weighted_scorer = scorer.set_score_request(sample_weight=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s3"># set_score_request should mutate the instance, rather than returning a</span>
        <span class="s3"># new instance</span>
        <span class="s0">assert </span><span class="s1">weighted_scorer </span><span class="s0">is </span><span class="s1">scorer</span>

        <span class="s3"># make sure the scorer doesn't request anything on methods other than</span>
        <span class="s3"># `score`, and that the requested value on `score` is correct.</span>
        <span class="s1">assert_request_is_empty(weighted_scorer.get_metadata_routing()</span><span class="s0">, </span><span class="s1">exclude=</span><span class="s2">&quot;score&quot;</span><span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">(</span>
            <span class="s1">weighted_scorer.get_metadata_routing().score.requests[</span><span class="s2">&quot;sample_weight&quot;</span><span class="s1">]</span>
            <span class="s0">is True</span>
        <span class="s1">)</span>

        <span class="s3"># make sure putting the scorer in a router doesn't request anything by</span>
        <span class="s3"># default</span>
        <span class="s1">router = MetadataRouter(owner=</span><span class="s2">&quot;test&quot;</span><span class="s1">).add(</span>
            <span class="s1">method_mapping=</span><span class="s2">&quot;score&quot;</span><span class="s0">, </span><span class="s1">scorer=get_scorer(name)</span>
        <span class="s1">)</span>
        <span class="s3"># make sure `sample_weight` is refused if passed.</span>
        <span class="s0">with </span><span class="s1">pytest.raises(TypeError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;got unexpected argument&quot;</span><span class="s1">):</span>
            <span class="s1">router.validate_metadata(params={</span><span class="s2">&quot;sample_weight&quot;</span><span class="s1">: </span><span class="s5">1</span><span class="s1">}</span><span class="s0">, </span><span class="s1">method=</span><span class="s2">&quot;score&quot;</span><span class="s1">)</span>
        <span class="s3"># make sure `sample_weight` is not routed even if passed.</span>
        <span class="s1">routed_params = router.route_params(params={</span><span class="s2">&quot;sample_weight&quot;</span><span class="s1">: </span><span class="s5">1</span><span class="s1">}</span><span class="s0">, </span><span class="s1">caller=</span><span class="s2">&quot;score&quot;</span><span class="s1">)</span>
        <span class="s0">assert not </span><span class="s1">routed_params.scorer.score</span>

        <span class="s3"># make sure putting weighted_scorer in a router requests sample_weight</span>
        <span class="s1">router = MetadataRouter(owner=</span><span class="s2">&quot;test&quot;</span><span class="s1">).add(</span>
            <span class="s1">scorer=weighted_scorer</span><span class="s0">, </span><span class="s1">method_mapping=</span><span class="s2">&quot;score&quot;</span>
        <span class="s1">)</span>
        <span class="s1">router.validate_metadata(params={</span><span class="s2">&quot;sample_weight&quot;</span><span class="s1">: </span><span class="s5">1</span><span class="s1">}</span><span class="s0">, </span><span class="s1">method=</span><span class="s2">&quot;score&quot;</span><span class="s1">)</span>
        <span class="s1">routed_params = router.route_params(params={</span><span class="s2">&quot;sample_weight&quot;</span><span class="s1">: </span><span class="s5">1</span><span class="s1">}</span><span class="s0">, </span><span class="s1">caller=</span><span class="s2">&quot;score&quot;</span><span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">list(routed_params.scorer.score.keys()) == [</span><span class="s2">&quot;sample_weight&quot;</span><span class="s1">]</span>


<span class="s0">def </span><span class="s1">test_metadata_kwarg_conflict():</span>
    <span class="s4">&quot;&quot;&quot;This test makes sure the right warning is raised if the user passes 
    some metadata both as a constructor to make_scorer, and during __call__. 
    &quot;&quot;&quot;</span>
    <span class="s0">with </span><span class="s1">config_context(enable_metadata_routing=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span>
            <span class="s1">n_classes=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">n_informative=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">n_samples=</span><span class="s5">20</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span>
        <span class="s1">)</span>
        <span class="s1">lr = LogisticRegression().fit(X</span><span class="s0">, </span><span class="s1">y)</span>

        <span class="s1">scorer = make_scorer(</span>
            <span class="s1">roc_auc_score</span><span class="s0">,</span>
            <span class="s1">needs_proba=</span><span class="s0">True,</span>
            <span class="s1">multi_class=</span><span class="s2">&quot;ovo&quot;</span><span class="s0">,</span>
            <span class="s1">labels=lr.classes_</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;already set as kwargs&quot;</span><span class="s1">):</span>
            <span class="s1">scorer.set_score_request(labels=</span><span class="s0">True</span><span class="s1">)</span>

        <span class="s0">with </span><span class="s1">config_context(enable_metadata_routing=</span><span class="s0">True</span><span class="s1">):</span>
            <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;There is an overlap&quot;</span><span class="s1">):</span>
                <span class="s1">scorer(lr</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">labels=lr.classes_)</span>


<span class="s0">def </span><span class="s1">test_PassthroughScorer_metadata_request():</span>
    <span class="s4">&quot;&quot;&quot;Test that _PassthroughScorer properly routes metadata. 
 
    _PassthroughScorer should behave like a consumer, mirroring whatever is the 
    underlying score method. 
    &quot;&quot;&quot;</span>
    <span class="s0">with </span><span class="s1">config_context(enable_metadata_routing=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s1">scorer = _PassthroughScorer(</span>
            <span class="s1">estimator=LinearSVC()</span>
            <span class="s1">.set_score_request(sample_weight=</span><span class="s2">&quot;alias&quot;</span><span class="s1">)</span>
            <span class="s1">.set_fit_request(sample_weight=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s1">)</span>
        <span class="s3"># test that _PassthroughScorer leaves everything other than `score` empty</span>
        <span class="s1">assert_request_is_empty(scorer.get_metadata_routing()</span><span class="s0">, </span><span class="s1">exclude=</span><span class="s2">&quot;score&quot;</span><span class="s1">)</span>
        <span class="s3"># test that _PassthroughScorer doesn't behave like a router and leaves</span>
        <span class="s3"># the request as is.</span>
        <span class="s0">assert </span><span class="s1">scorer.get_metadata_routing().score.requests[</span><span class="s2">&quot;sample_weight&quot;</span><span class="s1">] == </span><span class="s2">&quot;alias&quot;</span>


<span class="s0">def </span><span class="s1">test_multimetric_scoring_metadata_routing():</span>
    <span class="s3"># Test that _MultimetricScorer properly routes metadata.</span>
    <span class="s0">def </span><span class="s1">score1(y_true</span><span class="s0">, </span><span class="s1">y_pred):</span>
        <span class="s0">return </span><span class="s5">1</span>

    <span class="s0">def </span><span class="s1">score2(y_true</span><span class="s0">, </span><span class="s1">y_pred</span><span class="s0">, </span><span class="s1">sample_weight=</span><span class="s2">&quot;test&quot;</span><span class="s1">):</span>
        <span class="s3"># make sure sample_weight is not passed</span>
        <span class="s0">assert </span><span class="s1">sample_weight == </span><span class="s2">&quot;test&quot;</span>
        <span class="s0">return </span><span class="s5">1</span>

    <span class="s0">def </span><span class="s1">score3(y_true</span><span class="s0">, </span><span class="s1">y_pred</span><span class="s0">, </span><span class="s1">sample_weight=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s3"># make sure sample_weight is passed</span>
        <span class="s0">assert </span><span class="s1">sample_weight </span><span class="s0">is not None</span>
        <span class="s0">return </span><span class="s5">1</span>

    <span class="s1">scorers = {</span>
        <span class="s2">&quot;score1&quot;</span><span class="s1">: make_scorer(score1)</span><span class="s0">,</span>
        <span class="s2">&quot;score2&quot;</span><span class="s1">: make_scorer(score2).set_score_request(sample_weight=</span><span class="s0">False</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s2">&quot;score3&quot;</span><span class="s1">: make_scorer(score3).set_score_request(sample_weight=</span><span class="s0">True</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">}</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s5">50</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s5">2</span><span class="s0">, </span><span class="s1">n_redundant=</span><span class="s5">0</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span>
    <span class="s1">)</span>

    <span class="s1">clf = DecisionTreeClassifier().fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">scorer_dict = _check_multimetric_scoring(clf</span><span class="s0">, </span><span class="s1">scorers)</span>
    <span class="s1">multi_scorer = _MultimetricScorer(scorers=scorer_dict)</span>
    <span class="s3"># this should fail, because metadata routing is not enabled and w/o it we</span>
    <span class="s3"># don't support different metadata for different scorers.</span>
    <span class="s3"># TODO: remove when enable_metadata_routing is deprecated</span>
    <span class="s0">with </span><span class="s1">pytest.raises(TypeError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;got an unexpected keyword argument&quot;</span><span class="s1">):</span>
        <span class="s1">multi_scorer(clf</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3"># This passes since routing is done.</span>
    <span class="s0">with </span><span class="s1">config_context(enable_metadata_routing=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s1">multi_scorer(clf</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=</span><span class="s5">1</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_kwargs_without_metadata_routing_error():</span>
    <span class="s3"># Test that kwargs are not supported in scorers if metadata routing is not</span>
    <span class="s3"># enabled.</span>
    <span class="s3"># TODO: remove when enable_metadata_routing is deprecated</span>
    <span class="s0">def </span><span class="s1">score(y_true</span><span class="s0">, </span><span class="s1">y_pred</span><span class="s0">, </span><span class="s1">param=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s0">return </span><span class="s5">1  </span><span class="s3"># pragma: no cover</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s5">50</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s5">2</span><span class="s0">, </span><span class="s1">n_redundant=</span><span class="s5">0</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span>
    <span class="s1">)</span>

    <span class="s1">clf = DecisionTreeClassifier().fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">scorer = make_scorer(score)</span>
    <span class="s0">with </span><span class="s1">config_context(enable_metadata_routing=</span><span class="s0">False</span><span class="s1">):</span>
        <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;kwargs is only supported if&quot;</span><span class="s1">):</span>
            <span class="s1">scorer(clf</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">param=</span><span class="s2">&quot;blah&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_get_scorer_multilabel_indicator():</span>
    <span class="s4">&quot;&quot;&quot;Check that our scorer deal with multi-label indicator matrices. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/26817 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = make_multilabel_classification(n_samples=</span><span class="s5">72</span><span class="s0">, </span><span class="s1">n_classes=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">Y_train</span><span class="s0">, </span><span class="s1">Y_test = train_test_split(X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">estimator = KNeighborsClassifier().fit(X_train</span><span class="s0">, </span><span class="s1">Y_train)</span>

    <span class="s1">score = get_scorer(</span><span class="s2">&quot;average_precision&quot;</span><span class="s1">)(estimator</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">Y_test)</span>
    <span class="s0">assert </span><span class="s1">score &gt; </span><span class="s5">0.8</span>
</pre>
</body>
</html>