<html>
<head>
<title>rlm.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
rlm.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Robust linear models with support for the M-estimators  listed under  
:ref:`norms &lt;norms&gt;`. 
 
References 
---------- 
PJ Huber.  'Robust Statistics' John Wiley and Sons, Inc., New York.  1981. 
 
PJ Huber.  1973,  'The 1972 Wald Memorial Lectures: Robust Regression:  
    Asymptotics, Conjectures, and Monte Carlo.'  The Annals of Statistics,  
    1.5, 799-821. 
 
R Venables, B Ripley. 'Modern Applied Statistics in S'  Springer, New York,  
    2002. 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">tools</span>
<span class="s2">from </span><span class="s1">regression </span><span class="s2">import </span><span class="s1">WLS</span><span class="s2">, </span><span class="s1">GLS</span>
<span class="s2">from </span><span class="s1">robust </span><span class="s2">import </span><span class="s1">norms</span><span class="s2">, </span><span class="s1">scale</span>
<span class="s2">from </span><span class="s1">model </span><span class="s2">import </span><span class="s1">LikelihoodModel</span><span class="s2">, </span><span class="s1">LikelihoodModelResults</span>

<span class="s1">__all__ = [</span><span class="s3">'RLM'</span><span class="s1">]</span>

<span class="s2">class </span><span class="s1">RLM(LikelihoodModel):</span>
    <span class="s0">&quot;&quot;&quot; 
    Robust Linear Models 
 
    Estimate a robust linear model via iteratively reweighted least squares 
    given a robust criterion estimator. 
 
    Parameters 
    ---------- 
    endog : array-like 
        1d endogenous response variable 
    exog : array-like 
        n x p exogenous design matrix 
    M : scikits.statsmodels.robust.norms.RobustNorm, optional 
        The robust criterion function for downweighting outliers. 
        The current options are LeastSquares, HuberT, RamsayE, AndrewWave, 
        TrimmedMean, Hampel, and TukeyBiweight.  The default is HuberT(). 
        See scikits.statsmodels.robust.norms for more information. 
 
     
    Methods 
    ------- 
    deviance 
        Returns the (unnormalized) log-likelihood of the model 
    fit 
        Fits the model.  Returns an RLMResults class. 
    information 
        Not yet implemented. 
    newton 
        Not yet implemented. 
    results 
        A property that returns an RLMResults class.  Equivalent to calling 
        fit with the default arguments. 
    score 
        Not yet implemented. 
         
    Notes 
    ----- 
     
    **Attributes** 
     
    df_model : float 
        The degrees of freedom of the model.  The number of regressors p less 
        one for the intercept.  Note that the reported model degrees 
        of freedom does not count the intercept as a regressor, though  
        the model is assumed to have an intercept. 
    df_resid : float 
        The residual degrees of freedom.  The number of observations n 
        less the number of regressors p.  Note that here p does include 
        the intercept as using a degree of freedom. 
    endog : array 
        See above.  Note that endog is a reference to the data so that if 
        data is already an array and it is changed, then `endog` changes 
        as well. 
    exog : array 
        See above.  Note that endog is a reference to the data so that if 
        data is already an array and it is changed, then `endog` changes 
        as well. 
    history : dict 
        Contains information about the iterations. Its keys are `fittedvalues`, 
        `deviance`, and `params`. 
    M : scikits.statsmodels.robust.norms.RobustNorm 
         See above.  Robust estimator instance instantiated.  
    nobs : float 
        The number of observations n 
    pinv_wexog : array 
        The pseudoinverse of the design / exogenous data array.  Note that  
        RLM has no whiten method, so this is just the pseudo inverse of the  
        design.     
    normalized_cov_params : array 
        The p x p normalized covariance of the design / exogenous data. 
        This is approximately equal to (X.T X)^(-1)         
 
 
    Examples 
    --------- 
    &gt;&gt;&gt; import scikits.statsmodels as sm 
    &gt;&gt;&gt; data = sm.datasets.stackloss.Load() 
    &gt;&gt;&gt; data.exog = sm.add_constant(data.exog) 
    &gt;&gt;&gt; rlm_model = sm.RLM(data.endog, data.exog, \ 
                           M=sm.robust.norms.HuberT()) 
     
    &gt;&gt;&gt; rlm_results = rlm_model.fit() 
    &gt;&gt;&gt; rlm_results.params 
    array([  0.82938433,   0.92606597,  -0.12784672, -41.02649835]) 
    &gt;&gt;&gt; rlm_results.bse 
    array([ 0.11100521,  0.30293016,  0.12864961,  9.79189854]) 
    &gt;&gt;&gt; rlm_results_HC2 = rlm_model.fit(cov=&quot;H2&quot;) 
    &gt;&gt;&gt; rlm_results_HC2.params 
    array([  0.82938433,   0.92606597,  -0.12784672, -41.02649835]) 
    &gt;&gt;&gt; rlm_results_HC2.bse 
    array([ 0.11945975,  0.32235497,  0.11796313,  9.08950419]) 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; rlm_hamp_hub = sm.RLM(data.endog, data.exog, \ 
                          M=sm.robust.norms.Hampel()).fit( \ 
                          sm.robust.scale.HuberScale()) 
     
    &gt;&gt;&gt; rlm_hamp_hub.params 
    array([  0.73175452,   1.25082038,  -0.14794399, -40.27122257]) 
     
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">M=norms.HuberT()):</span>
        <span class="s1">self.M = M</span>
        <span class="s1">self.endog = np.asarray(endog)</span>
        <span class="s1">self.exog = np.asarray(exog)</span>
        <span class="s1">self._initialize()</span>

    <span class="s2">def </span><span class="s1">_initialize(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Initializes the model for the IRLS fit. 
 
        Resets the history and number of iterations. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.history = {</span><span class="s3">'deviance' </span><span class="s1">: [np.inf]</span><span class="s2">, </span><span class="s3">'params' </span><span class="s1">: [np.inf]</span><span class="s2">, </span>
            <span class="s3">'weights' </span><span class="s1">: [np.inf]</span><span class="s2">, </span><span class="s3">'sresid' </span><span class="s1">: [np.inf]</span><span class="s2">, </span><span class="s3">'scale' </span><span class="s1">: []}</span>
        <span class="s1">self.iteration = </span><span class="s4">0</span>
        <span class="s1">self.pinv_wexog = np.linalg.pinv(self.exog)</span>
        <span class="s1">self.normalized_cov_params = np.dot(self.pinv_wexog</span><span class="s2">,</span>
                                        <span class="s1">np.transpose(self.pinv_wexog))</span>
        <span class="s1">self.df_resid = np.float(self.exog.shape[</span><span class="s4">0</span><span class="s1">] - tools.rank(self.exog))</span>
        <span class="s1">self.df_model = np.float(tools.rank(self.exog)-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">self.nobs = float(self.endog.shape[</span><span class="s4">0</span><span class="s1">])</span>

    <span class="s2">def </span><span class="s1">score(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError</span>

    <span class="s2">def </span><span class="s1">information(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError</span>

    <span class="s2">def </span><span class="s1">loglike(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError</span>

    <span class="s2">def </span><span class="s1">deviance(self</span><span class="s2">, </span><span class="s1">tmp_results):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the (unnormalized) log-likelihood from the M estimator. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.M((self.endog - tmp_results.fittedvalues)/\</span>
                    <span class="s1">tmp_results.scale).sum()</span>
    
    <span class="s2">def </span><span class="s1">_update_history(self</span><span class="s2">, </span><span class="s1">tmp_results):</span>
        <span class="s1">self.history[</span><span class="s3">'deviance'</span><span class="s1">].append(self.deviance(tmp_results))</span>
        <span class="s1">self.history[</span><span class="s3">'params'</span><span class="s1">].append(tmp_results.params)</span>
        <span class="s1">self.history[</span><span class="s3">'scale'</span><span class="s1">].append(tmp_results.scale)</span>
        <span class="s1">self.history[</span><span class="s3">'sresid'</span><span class="s1">].append(tmp_results.resid/tmp_results.scale)</span>
        <span class="s1">self.history[</span><span class="s3">'weights'</span><span class="s1">].append(tmp_results.model.weights)</span>

    <span class="s2">def </span><span class="s1">_estimate_scale(self</span><span class="s2">, </span><span class="s1">resid):</span>
        <span class="s0">&quot;&quot;&quot; 
        Estimates the scale based on the option provided to the fit method. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">isinstance(self.scale_est</span><span class="s2">, </span><span class="s1">str):</span>
            <span class="s2">if </span><span class="s1">self.scale_est.lower() == </span><span class="s3">'mad'</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">scale.mad(resid)</span>
            <span class="s2">if </span><span class="s1">self.scale_est.lower() == </span><span class="s3">'stand_mad'</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">scale.stand_mad(resid)</span>
        <span class="s2">elif </span><span class="s1">isinstance(self.scale_est</span><span class="s2">, </span><span class="s1">scale.HuberScale):</span>
            <span class="s2">return </span><span class="s1">scale.hubers_scale(self.df_resid</span><span class="s2">, </span><span class="s1">self.nobs</span><span class="s2">, </span><span class="s1">resid)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">scale.scale_est(self</span><span class="s2">, </span><span class="s1">resid)**</span><span class="s4">2</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">tol=</span><span class="s4">1e-8</span><span class="s2">, </span><span class="s1">scale_est=</span><span class="s3">'mad'</span><span class="s2">, </span><span class="s1">init=</span><span class="s2">None, </span><span class="s1">cov=</span><span class="s3">'H1'</span><span class="s2">,</span>
            <span class="s1">update_scale=</span><span class="s2">True, </span><span class="s1">conv=</span><span class="s3">'dev'</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fits the model using iteratively reweighted least squares. 
 
        The IRLS routine runs until the specified objective converges to `tol` 
        or `maxiter` has been reached. 
 
        Parameters 
        ---------- 
        conv : string 
            Indicates the convergence criteria. 
            Available options are &quot;coefs&quot; (the coefficients), &quot;weights&quot; (the  
            weights in the iteration), &quot;resids&quot; (the standardized residuals), 
            and &quot;dev&quot; (the un-normalized log-likelihood for the M 
            estimator).  The default is &quot;dev&quot;. 
        cov : string, optional 
            'H1', 'H2', or 'H3' 
            Indicates how the covariance matrix is estimated.  Default is 'H1'. 
            See rlm.RLMResults for more information. 
        init : string 
            Specifies method for the initial estimates of the parameters. 
            Default is None, which means that the least squares estimate 
            is used.  Currently it is the only available choice. 
        maxiter : int 
            The maximum number of iterations to try. Default is 50. 
        scale_est : string or HuberScale() 
            'mad', 'stand_mad', or HuberScale() 
            Indicates the estimate to use for scaling the weights in the IRLS. 
            The default is 'mad' (median absolute deviation.  Other options are 
            use 'stand_mad' for the median absolute deviation standardized 
            around the median and 'HuberScale' for Huber's proposal 2.   
            Huber's proposal 2 has optional keyword arguments d, tol, and  
            maxiter for specifying the tuning constant, the convergence  
            tolerance, and the maximum number of iterations. 
            See models.robust.scale for more information.       
        tol : float 
            The convergence tolerance of the estimate.  Default is 1e-8. 
        update_scale : Bool 
            If `update_scale` is False then the scale estimate for the 
            weights is held constant over the iteration.  Otherwise, it 
            is updated for each fit in the iteration.  Default is True. 
 
        Returns 
        ------- 
        results : object 
            scikits.statsmodels.rlm.RLMresults 
        &quot;&quot;&quot;</span>
        <span class="s2">if not </span><span class="s1">cov.upper() </span><span class="s2">in </span><span class="s1">[</span><span class="s3">&quot;H1&quot;</span><span class="s2">,</span><span class="s3">&quot;H2&quot;</span><span class="s2">,</span><span class="s3">&quot;H3&quot;</span><span class="s1">]:</span>
            <span class="s2">raise </span><span class="s1">AttributeError</span><span class="s2">, </span><span class="s3">&quot;Covariance matrix %s not understood&quot; </span><span class="s1">% cov</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self.cov = cov.upper()</span>
        <span class="s1">conv = conv.lower()</span>
        <span class="s2">if not </span><span class="s1">conv </span><span class="s2">in </span><span class="s1">[</span><span class="s3">&quot;weights&quot;</span><span class="s2">,</span><span class="s3">&quot;coefs&quot;</span><span class="s2">,</span><span class="s3">&quot;dev&quot;</span><span class="s2">,</span><span class="s3">&quot;resid&quot;</span><span class="s1">]:</span>
            <span class="s2">raise </span><span class="s1">AttributeError</span><span class="s2">, </span><span class="s3">&quot;Convergence argument %s not understood&quot; </span><span class="s1">\</span>
                <span class="s1">% conv</span>
        <span class="s1">self.scale_est = scale_est</span>
        <span class="s1">wls_results = WLS(self.endog</span><span class="s2">, </span><span class="s1">self.exog).fit()</span>
        <span class="s2">if not </span><span class="s1">init: </span>
            <span class="s1">self.scale = self._estimate_scale(wls_results.resid)</span>
        <span class="s1">self._update_history(wls_results)</span>
        <span class="s1">self.iteration = </span><span class="s4">1</span>
        <span class="s2">if </span><span class="s1">conv == </span><span class="s3">'coefs'</span><span class="s1">:</span>
            <span class="s1">criterion = self.history[</span><span class="s3">'params'</span><span class="s1">]</span>
        <span class="s2">elif </span><span class="s1">conv == </span><span class="s3">'dev'</span><span class="s1">:</span>
            <span class="s1">criterion = self.history[</span><span class="s3">'deviance'</span><span class="s1">]</span>
        <span class="s2">elif </span><span class="s1">conv == </span><span class="s3">'resid'</span><span class="s1">:</span>
            <span class="s1">criterion = self.history[</span><span class="s3">'sresid'</span><span class="s1">]</span>
        <span class="s2">elif </span><span class="s1">conv == </span><span class="s3">'weights'</span><span class="s1">:</span>
            <span class="s1">criterion = self.history[</span><span class="s3">'weights'</span><span class="s1">]</span>
        <span class="s2">while </span><span class="s1">(np.all(np.fabs(criterion[self.iteration]-\</span>
                <span class="s1">criterion[self.iteration-</span><span class="s4">1</span><span class="s1">]) &gt; tol) </span><span class="s2">and </span><span class="s1">\</span>
                <span class="s1">self.iteration &lt; maxiter):</span>
<span class="s5">#            self.weights = self.M.weights((self.endog - \</span>
<span class="s5">#                    wls_results.fittedvalues)/self.scale)</span>
            <span class="s1">self.weights = self.M.weights(wls_results.resid/self.scale)</span>
            <span class="s1">wls_results = WLS(self.endog</span><span class="s2">, </span><span class="s1">self.exog</span><span class="s2">, </span>
                                    <span class="s1">weights=self.weights).fit()</span>
            <span class="s2">if </span><span class="s1">update_scale </span><span class="s2">is True</span><span class="s1">:</span>
                <span class="s1">self.scale = self._estimate_scale(wls_results.resid)</span>
            <span class="s1">self._update_history(wls_results)</span>
            <span class="s1">self.iteration += </span><span class="s4">1</span>
        <span class="s1">self.results = RLMResults(self</span><span class="s2">, </span><span class="s1">wls_results.params</span><span class="s2">, </span>
                            <span class="s1">self.normalized_cov_params</span><span class="s2">, </span><span class="s1">self.scale)</span>
        <span class="s2">return </span><span class="s1">self.results</span>

<span class="s2">class </span><span class="s1">RLMResults(LikelihoodModelResults):</span>
    <span class="s0">&quot;&quot;&quot; 
    Class to contain RLM results 
 
    **Attributes** 
     
    bcov_scaled : array 
        p x p scaled covariance matrix specified in the model fit method. 
        The default is H1. H1 is defined as 
        k**2 * (1/df_resid*sum(M.psi(sresid)**2)*scale**2)/ 
        ((1/nobs*sum(M.psi_deriv(sresid)))**2) * (X.T X)^(-1) 
         
        where k = 1 + (df_model +1)/nobs * var_psiprime/m**2 
        where m = mean(M.psi_deriv(sresid)) and 
        var_psiprime = var(M.psi_deriv(sresid)) 
 
        H2 is defined as 
        k * (1/df_resid) * sum(M.psi(sresid)**2) *scale**2/ 
        ((1/nobs)*sum(M.psi_deriv(sresid)))*W_inv 
 
        H3 is defined as 
        1/k * (1/df_resid * sum(M.psi(sresid)**2)*scale**2 * 
        (W_inv X.T X W_inv)) 
 
        where k is defined as above and  
        W_inv = (M.psi_deriv(sresid) exog.T exog)^(-1) 
 
        See the technical documentation for cleaner formulae. 
    bcov_unscaled : array 
        The usual p x p covariance matrix with scale set equal to 1.  It 
        is then just equivalent to normalized_cov_params. 
    bse : array 
        An array of the standard errors of the parameters.  The standard 
        errors are taken from the robust covariance matrix specified in the 
        argument to fit. 
    chisq : array 
        An array of the chi-squared values of the paramter estimates. 
    df_model 
        See RLM.df_model 
    df_resid 
        See RLM.df_resid 
    fittedvalues : array 
        The linear predicted values.  dot(exog, params) 
    model : scikits.statsmodels.rlm.RLM 
        A reference to the model instance 
    nobs : float 
        The number of observations n 
    normalized_cov_params : array 
        See RLM.normalized_cov_params 
    params : array 
        The coefficients of the fitted model 
    pinv_wexog : array 
        See RLM.pinv_wexog 
    resid : array 
        The residuals of the fitted model.  endog - fittedvalues 
    scale : float 
        The type of scale is determined in the arguments to the fit method in 
        RLM.  The reported scale is taken from the residuals of the weighted  
        least squares in the last IRLS iteration if update_scale is True.  If 
        update_scale is False, then it is the scale given by the first OLS 
        fit before the IRLS iterations. 
    sresid : array 
        The scaled residuals. 
    weights : array 
        The reported weights are determined by passing the scaled residuals  
        from the last weighted least squares fit in the IRLS algortihm. 
 
    See also 
    -------- 
    scikits.statsmodels.model.LikelihoodModelResults 
    &quot;&quot;&quot;</span>


    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">model</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">normalized_cov_params</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s1">super(RLMResults</span><span class="s2">, </span><span class="s1">self).__init__(model</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span>
                <span class="s1">normalized_cov_params</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s1">self._get_results(model)</span>

    <span class="s2">def </span><span class="s1">_get_results(self</span><span class="s2">, </span><span class="s1">model):</span>
        <span class="s5">#TODO: &quot;pvals&quot; should come from chisq on bse?</span>
        <span class="s1">self.df_model = model.df_model</span>
        <span class="s1">self.df_resid = model.df_resid</span>
        <span class="s1">self.fittedvalues = np.dot(model.exog</span><span class="s2">, </span><span class="s1">self.params)</span>
        <span class="s1">self.resid = model.endog - self.fittedvalues   </span><span class="s5"># before bcov</span>
        <span class="s1">self.sresid = self.resid/self.scale</span>
        <span class="s1">self.pinv_wexog = model.pinv_wexog    </span><span class="s5"># for bcov, </span>
                                                <span class="s5"># this is getting sloppy</span>
        <span class="s1">self.bcov_unscaled = self.cov_params(scale=</span><span class="s4">1.</span><span class="s1">)</span>
        <span class="s1">self.nobs = model.nobs</span>
        <span class="s1">self.weights = model.weights</span>
        <span class="s1">m = np.mean(model.M.psi_deriv(self.sresid))</span>
        <span class="s1">var_psiprime = np.var(model.M.psi_deriv(self.sresid))</span>
        <span class="s1">k = </span><span class="s4">1 </span><span class="s1">+ (self.df_model+</span><span class="s4">1</span><span class="s1">)/self.nobs * var_psiprime/m**</span><span class="s4">2</span>
        
        <span class="s2">if </span><span class="s1">model.cov == </span><span class="s3">&quot;H1&quot;</span><span class="s1">:</span>
            <span class="s1">self.bcov_scaled = k**</span><span class="s4">2 </span><span class="s1">* (</span><span class="s4">1</span><span class="s1">/self.df_resid*\</span>
                <span class="s1">np.sum(model.M.psi(self.sresid)**</span><span class="s4">2</span><span class="s1">)*self.scale**</span><span class="s4">2</span><span class="s1">)\</span>
                <span class="s1">/((</span><span class="s4">1</span><span class="s1">/self.nobs*np.sum(model.M.psi_deriv(self.sresid)))**</span><span class="s4">2</span><span class="s1">)\</span>
                <span class="s1">*model.normalized_cov_params</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">W = np.dot(model.M.psi_deriv(self.sresid)*model.exog.T</span><span class="s2">,</span><span class="s1">model.exog)</span>
            <span class="s1">W_inv = np.linalg.inv(W)</span>
<span class="s5"># [W_jk]^-1 = [SUM(psi_deriv(Sr_i)*x_ij*x_jk)]^-1</span>
<span class="s5"># where Sr are the standardized residuals</span>
            <span class="s2">if </span><span class="s1">model.cov == </span><span class="s3">&quot;H2&quot;</span><span class="s1">:</span>
<span class="s5"># These are correct, based on Huber (1973) 8.13</span>
                <span class="s1">self.bcov_scaled = k*(</span><span class="s4">1</span><span class="s1">/self.df_resid)*np.sum(\</span>
                        <span class="s1">model.M.psi(self.sresid)**</span><span class="s4">2</span><span class="s1">)*self.scale**</span><span class="s4">2</span><span class="s1">\</span>
                        <span class="s1">/((</span><span class="s4">1</span><span class="s1">/self.nobs)*np.sum(\</span>
                        <span class="s1">model.M.psi_deriv(self.sresid)))*W_inv</span>
            <span class="s2">elif </span><span class="s1">model.cov == </span><span class="s3">&quot;H3&quot;</span><span class="s1">:</span>
                <span class="s1">self.bcov_scaled = k**-</span><span class="s4">1</span><span class="s1">*</span><span class="s4">1</span><span class="s1">/self.df_resid*np.sum(\</span>
                    <span class="s1">model.M.psi(self.sresid)**</span><span class="s4">2</span><span class="s1">)*self.scale**</span><span class="s4">2</span><span class="s1">\</span>
                    <span class="s1">*np.dot(np.dot(W_inv</span><span class="s2">, </span><span class="s1">np.dot(model.exog.T</span><span class="s2">,</span><span class="s1">model.exog))</span><span class="s2">,</span><span class="s1">\</span>
                    <span class="s1">W_inv)</span>
        <span class="s1">self.bse = np.sqrt(np.diag(self.bcov_scaled))</span>
        <span class="s1">self.chisq = (self.params/self.bse)**</span><span class="s4">2</span>

<span class="s2">if </span><span class="s1">__name__==</span><span class="s3">&quot;__main__&quot;</span><span class="s1">:</span>
<span class="s5">#NOTE: This is to be removed</span>
<span class="s5">#Delivery Time Data is taken from Montgomery and Peck</span>
    <span class="s2">import </span><span class="s1">models</span>

<span class="s5">#delivery time(minutes)</span>
    <span class="s1">endog = np.array([</span><span class="s4">16.68</span><span class="s2">, </span><span class="s4">11.50</span><span class="s2">, </span><span class="s4">12.03</span><span class="s2">, </span><span class="s4">14.88</span><span class="s2">, </span><span class="s4">13.75</span><span class="s2">, </span><span class="s4">18.11</span><span class="s2">, </span><span class="s4">8.00</span><span class="s2">, </span><span class="s4">17.83</span><span class="s2">, </span>
    <span class="s4">79.24</span><span class="s2">, </span><span class="s4">21.50</span><span class="s2">, </span><span class="s4">40.33</span><span class="s2">, </span><span class="s4">21.00</span><span class="s2">, </span><span class="s4">13.50</span><span class="s2">, </span><span class="s4">19.75</span><span class="s2">, </span><span class="s4">24.00</span><span class="s2">, </span><span class="s4">29.00</span><span class="s2">, </span><span class="s4">15.35</span><span class="s2">, </span><span class="s4">19.00</span><span class="s2">, </span>
    <span class="s4">9.50</span><span class="s2">, </span><span class="s4">35.10</span><span class="s2">, </span><span class="s4">17.90</span><span class="s2">, </span><span class="s4">52.32</span><span class="s2">, </span><span class="s4">18.75</span><span class="s2">, </span><span class="s4">19.83</span><span class="s2">, </span><span class="s4">10.75</span><span class="s1">])</span>

<span class="s5">#number of cases, distance (Feet)</span>
    <span class="s1">exog = np.array([[</span><span class="s4">7</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">7</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">7</span><span class="s2">, </span><span class="s4">30</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">16</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">9</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span>
    <span class="s4">7</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">17</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">26</span><span class="s2">, </span><span class="s4">9</span><span class="s2">, </span><span class="s4">8</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">560</span><span class="s2">, </span><span class="s4">220</span><span class="s2">, </span><span class="s4">340</span><span class="s2">, </span><span class="s4">80</span><span class="s2">, </span><span class="s4">150</span><span class="s2">, </span><span class="s4">330</span><span class="s2">, </span><span class="s4">110</span><span class="s2">, </span><span class="s4">210</span><span class="s2">, </span><span class="s4">1460</span><span class="s2">, </span>
    <span class="s4">605</span><span class="s2">, </span><span class="s4">688</span><span class="s2">, </span><span class="s4">215</span><span class="s2">, </span><span class="s4">255</span><span class="s2">, </span><span class="s4">462</span><span class="s2">, </span><span class="s4">448</span><span class="s2">, </span><span class="s4">776</span><span class="s2">, </span><span class="s4">200</span><span class="s2">, </span><span class="s4">132</span><span class="s2">, </span><span class="s4">36</span><span class="s2">, </span><span class="s4">770</span><span class="s2">, </span><span class="s4">140</span><span class="s2">, </span><span class="s4">810</span><span class="s2">, </span><span class="s4">450</span><span class="s2">, </span><span class="s4">635</span><span class="s2">,</span>
    <span class="s4">150</span><span class="s1">]])</span>
    <span class="s1">exog = exog.T</span>
    <span class="s1">exog = models.tools.add_constant(exog)</span>

<span class="s5">#    model_ols = models.regression.OLS(endog, exog)</span>
<span class="s5">#    results_ols = model_ols.fit()</span>

<span class="s5">#    model_huber = RLM(endog, exog, M=norms.HuberT(t=2.)) </span>
<span class="s5">#    results_huber = model_huber.fit(scale_est=&quot;stand_mad&quot;, update_scale=False) </span>

<span class="s5">#    model_ramsaysE = RLM(endog, exog, M=norms.RamsayE())</span>
<span class="s5">#    results_ramsaysE = model_ramsaysE.fit(update_scale=False)</span>
    
<span class="s5">#    model_andrewWave = RLM(endog, exog, M=norms.AndrewWave())</span>
<span class="s5">#    results_andrewWave = model_andrewWave.fit(update_scale=False)</span>

<span class="s5">#    model_hampel = RLM(endog, exog, M=norms.Hampel(a=1.7,b=3.4,c=8.5)) # convergence problems with scale changed, not with 2,4,8 though?</span>
<span class="s5">#    results_hampel = model_hampel.fit(update_scale=False)</span>

<span class="s5">#######################</span>
<span class="s5">### Stack Loss Data ###</span>
<span class="s5">#######################</span>
    <span class="s2">from </span><span class="s1">models.datasets.stackloss </span><span class="s2">import </span><span class="s1">Load</span>
    <span class="s1">data = Load()</span>
    <span class="s1">data.exog = models.tools.add_constant(data.exog)</span>
<span class="s5">#############</span>
<span class="s5">### Huber ###</span>
<span class="s5">#############</span>
<span class="s5">#    m1_Huber = RLM(data.endog, data.exog, M=norms.HuberT())</span>
<span class="s5">#    results_Huber1 = m1_Huber.fit()</span>
<span class="s5">#    m2_Huber = RLM(data.endog, data.exog, M=norms.HuberT())</span>
<span class="s5">#    results_Huber2 = m2_Huber.fit(cov=&quot;H2&quot;)</span>
<span class="s5">#    m3_Huber = RLM(data.endog, data.exog, M=norms.HuberT())</span>
<span class="s5">#    results_Huber3 = m3_Huber.fit(cov=&quot;H3&quot;)</span>
<span class="s5">##############</span>
<span class="s5">### Hampel ###</span>
<span class="s5">##############</span>
<span class="s5">#    m1_Hampel = RLM(data.endog, data.exog, M=norms.Hampel())</span>
<span class="s5">#    results_Hampel1 = m1_Hampel.fit()</span>
<span class="s5">#    m2_Hampel = RLM(data.endog, data.exog, M=norms.Hampel())</span>
<span class="s5">#    results_Hampel2 = m2_Hampel.fit(cov=&quot;H2&quot;)</span>
<span class="s5">#    m3_Hampel = RLM(data.endog, data.exog, M=norms.Hampel())</span>
<span class="s5">#    results_Hampel3 = m3_Hampel.fit(cov=&quot;H3&quot;)</span>
<span class="s5">################</span>
<span class="s5">### Bisquare ###</span>
<span class="s5">################</span>
<span class="s5">#    m1_Bisquare = RLM(data.endog, data.exog, M=norms.TukeyBiweight())</span>
<span class="s5">#    results_Bisquare1 = m1_Bisquare.fit()</span>
<span class="s5">#    m2_Bisquare = RLM(data.endog, data.exog, M=norms.TukeyBiweight())</span>
<span class="s5">#    results_Bisquare2 = m2_Bisquare.fit(cov=&quot;H2&quot;)</span>
<span class="s5">#    m3_Bisquare = RLM(data.endog, data.exog, M=norms.TukeyBiweight())</span>
<span class="s5">#    results_Bisquare3 = m3_Bisquare.fit(cov=&quot;H3&quot;)</span>


<span class="s5">##############################################</span>
<span class="s5"># Huber's Proposal 2 scaling                 #</span>
<span class="s5">##############################################</span>

<span class="s5">################</span>
<span class="s5">### Huber'sT ###</span>
<span class="s5">################</span>
    <span class="s1">m1_Huber_H = RLM(data.endog</span><span class="s2">, </span><span class="s1">data.exog</span><span class="s2">, </span><span class="s1">M=norms.HuberT())</span>
    <span class="s1">results_Huber1_H = m1_Huber_H.fit(scale_est=scale.HuberScale())</span>
<span class="s5">#    m2_Huber_H</span>
<span class="s5">#    m3_Huber_H</span>
<span class="s5">#    m4 = RLM(data.endog, data.exog, M=norms.HuberT())</span>
<span class="s5">#    results4 = m1.fit(scale_est=&quot;Huber&quot;)</span>
<span class="s5">#    m5 = RLM(data.endog, data.exog, M=norms.Hampel())</span>
<span class="s5">#    results5 = m2.fit(scale_est=&quot;Huber&quot;)</span>
<span class="s5">#    m6 = RLM(data.endog, data.exog, M=norms.TukeyBiweight())</span>
<span class="s5">#    results6 = m3.fit(scale_est=&quot;Huber&quot;)</span>


    

<span class="s5">#    print &quot;&quot;&quot;Least squares fit</span>
<span class="s5">#%s  </span>
<span class="s5">#Huber Params, t = 2.</span>
<span class="s5">#%s</span>
<span class="s5">#Ramsay's E Params</span>
<span class="s5">#%s</span>
<span class="s5">#Andrew's Wave Params</span>
<span class="s5">#%s</span>
<span class="s5">#Hampel's 17A Function</span>
<span class="s5">#%s</span>
<span class="s5">#&quot;&quot;&quot; % (results_ols.params, results_huber.params, results_ramsaysE.params, </span>
<span class="s5">#            results_andrewWave.params, results_hampel.params)</span>

</pre>
</body>
</html>