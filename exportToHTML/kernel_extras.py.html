<html>
<head>
<title>kernel_extras.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
kernel_extras.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Multivariate Conditional and Unconditional Kernel Density Estimation 
with Mixed Data Types 
 
References 
---------- 
[1] Racine, J., Li, Q. Nonparametric econometrics: theory and practice. 
    Princeton University Press. (2007) 
[2] Racine, Jeff. &quot;Nonparametric Econometrics: A Primer,&quot; Foundation 
    and Trends in Econometrics: Vol 3: No 1, pp1-88. (2008) 
    http://dx.doi.org/10.1561/0800000009 
[3] Racine, J., Li, Q. &quot;Nonparametric Estimation of Distributions 
    with Categorical and Continuous Data.&quot; Working Paper. (2000) 
[4] Racine, J. Li, Q. &quot;Kernel Estimation of Multivariate Conditional 
    Distributions Annals of Economics and Finance 5, 211-235 (2004) 
[5] Liu, R., Yang, L. &quot;Kernel estimation of multivariate 
    cumulative distribution function.&quot; 
    Journal of Nonparametric Statistics (2008) 
[6] Li, R., Ju, G. &quot;Nonparametric Estimation of Multivariate CDF 
    with Categorical and Continuous Data.&quot; Working Paper 
[7] Li, Q., Racine, J. &quot;Cross-validated local linear nonparametric 
    regression&quot; Statistica Sinica 14(2004), pp. 485-512 
[8] Racine, J.: &quot;Consistent Significance Testing for Nonparametric 
        Regression&quot; Journal of Business &amp; Economics Statistics 
[9] Racine, J., Hart, J., Li, Q., &quot;Testing the Significance of 
        Categorical Predictor Variables in Nonparametric Regression 
        Models&quot;, 2006, Econometric Reviews 25, 523-544 
 
&quot;&quot;&quot;</span>

<span class="s2"># TODO: make default behavior efficient=True above a certain n_obs</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">optimize</span>
<span class="s3">from </span><span class="s1">scipy.stats.mstats </span><span class="s3">import </span><span class="s1">mquantiles</span>

<span class="s3">from </span><span class="s1">statsmodels.nonparametric.api </span><span class="s3">import </span><span class="s1">KDEMultivariate</span><span class="s3">, </span><span class="s1">KernelReg</span>
<span class="s3">from </span><span class="s1">statsmodels.nonparametric._kernel_base </span><span class="s3">import </span><span class="s1">\</span>
    <span class="s1">gpke</span><span class="s3">, </span><span class="s1">LeaveOneOut</span><span class="s3">, </span><span class="s1">_get_type_pos</span><span class="s3">, </span><span class="s1">_adjust_shape</span>


<span class="s1">__all__ = [</span><span class="s4">'SingleIndexModel'</span><span class="s3">, </span><span class="s4">'SemiLinear'</span><span class="s3">, </span><span class="s4">'TestFForm'</span><span class="s1">]</span>


<span class="s3">class </span><span class="s1">TestFForm:</span>
    <span class="s0">&quot;&quot;&quot; 
    Nonparametric test for functional form. 
 
    Parameters 
    ---------- 
    endog : list 
        Dependent variable (training set) 
    exog : list of array_like objects 
        The independent (right-hand-side) variables 
    bw : array_like, str 
        Bandwidths for exog or specify method for bandwidth selection 
    fform : function 
        The functional form ``y = g(b, x)`` to be tested. Takes as inputs 
        the RHS variables `exog` and the coefficients ``b`` (betas) 
        and returns a fitted ``y_hat``. 
    var_type : str 
        The type of the independent `exog` variables: 
 
            - c: continuous 
            - o: ordered 
            - u: unordered 
 
    estimator : function 
        Must return the estimated coefficients b (betas). Takes as inputs 
        ``(endog, exog)``.  E.g. least square estimator:: 
 
            lambda (x,y): np.dot(np.pinv(np.dot(x.T, x)), np.dot(x.T, y)) 
 
    References 
    ---------- 
    See Racine, J.: &quot;Consistent Significance Testing for Nonparametric 
    Regression&quot; Journal of Business &amp; Economics Statistics. 
 
    See chapter 12 in [1]  pp. 355-357. 
    &quot;&quot;&quot;</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">bw</span><span class="s3">, </span><span class="s1">var_type</span><span class="s3">, </span><span class="s1">fform</span><span class="s3">, </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">nboot=</span><span class="s5">100</span><span class="s1">):</span>
        <span class="s1">self.endog = endog</span>
        <span class="s1">self.exog = exog</span>
        <span class="s1">self.var_type = var_type</span>
        <span class="s1">self.fform = fform</span>
        <span class="s1">self.estimator = estimator</span>
        <span class="s1">self.nboot = nboot</span>
        <span class="s1">self.bw = KDEMultivariate(exog</span><span class="s3">, </span><span class="s1">bw=bw</span><span class="s3">, </span><span class="s1">var_type=var_type).bw</span>
        <span class="s1">self.sig = self._compute_sig()</span>

    <span class="s3">def </span><span class="s1">_compute_sig(self):</span>
        <span class="s1">Y = self.endog</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">b = self.estimator(Y</span><span class="s3">, </span><span class="s1">X)</span>
        <span class="s1">m = self.fform(X</span><span class="s3">, </span><span class="s1">b)</span>
        <span class="s1">n = np.shape(X)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">resid = Y - m</span>
        <span class="s1">resid = resid - np.mean(resid)  </span><span class="s2"># center residuals</span>
        <span class="s1">self.test_stat = self._compute_test_stat(resid)</span>
        <span class="s1">sqrt5 = np.sqrt(</span><span class="s5">5.</span><span class="s1">)</span>
        <span class="s1">fct1 = (</span><span class="s5">1 </span><span class="s1">- sqrt5) / </span><span class="s5">2.</span>
        <span class="s1">fct2 = (</span><span class="s5">1 </span><span class="s1">+ sqrt5) / </span><span class="s5">2.</span>
        <span class="s1">u1 = fct1 * resid</span>
        <span class="s1">u2 = fct2 * resid</span>
        <span class="s1">r = fct2 / sqrt5</span>
        <span class="s1">I_dist = np.empty((self.nboot</span><span class="s3">,</span><span class="s5">1</span><span class="s1">))</span>
        <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(self.nboot):</span>
            <span class="s1">u_boot = u2.copy()</span>

            <span class="s1">prob = np.random.uniform(</span><span class="s5">0</span><span class="s3">,</span><span class="s5">1</span><span class="s3">, </span><span class="s1">size = (n</span><span class="s3">,</span><span class="s1">))</span>
            <span class="s1">ind = prob &lt; r</span>
            <span class="s1">u_boot[ind] = u1[ind]</span>
            <span class="s1">Y_boot = m + u_boot</span>
            <span class="s1">b_hat = self.estimator(Y_boot</span><span class="s3">, </span><span class="s1">X)</span>
            <span class="s1">m_hat = self.fform(X</span><span class="s3">, </span><span class="s1">b_hat)</span>
            <span class="s1">u_boot_hat = Y_boot - m_hat</span>
            <span class="s1">I_dist[j] = self._compute_test_stat(u_boot_hat)</span>

        <span class="s1">self.boots_results = I_dist</span>
        <span class="s1">sig = </span><span class="s4">&quot;Not Significant&quot;</span>
        <span class="s3">if </span><span class="s1">self.test_stat &gt; mquantiles(I_dist</span><span class="s3">, </span><span class="s5">0.9</span><span class="s1">):</span>
            <span class="s1">sig = </span><span class="s4">&quot;*&quot;</span>
        <span class="s3">if </span><span class="s1">self.test_stat &gt; mquantiles(I_dist</span><span class="s3">, </span><span class="s5">0.95</span><span class="s1">):</span>
            <span class="s1">sig = </span><span class="s4">&quot;**&quot;</span>
        <span class="s3">if </span><span class="s1">self.test_stat &gt; mquantiles(I_dist</span><span class="s3">, </span><span class="s5">0.99</span><span class="s1">):</span>
            <span class="s1">sig = </span><span class="s4">&quot;***&quot;</span>
        <span class="s3">return </span><span class="s1">sig</span>

    <span class="s3">def </span><span class="s1">_compute_test_stat(self</span><span class="s3">, </span><span class="s1">u):</span>
        <span class="s1">n = np.shape(u)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">XLOO = LeaveOneOut(self.exog)</span>
        <span class="s1">uLOO = LeaveOneOut(u[:</span><span class="s3">,None</span><span class="s1">]).__iter__()</span>
        <span class="s1">ival = </span><span class="s5">0</span>
        <span class="s1">S2 = </span><span class="s5">0</span>
        <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">X_not_i </span><span class="s3">in </span><span class="s1">enumerate(XLOO):</span>
            <span class="s1">u_j = next(uLOO)</span>
            <span class="s1">u_j = np.squeeze(u_j)</span>
            <span class="s2"># See Bootstrapping procedure on p. 357 in [1]</span>
            <span class="s1">K = gpke(self.bw</span><span class="s3">, </span><span class="s1">data=-X_not_i</span><span class="s3">, </span><span class="s1">data_predict=-self.exog[i</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                     <span class="s1">var_type=self.var_type</span><span class="s3">, </span><span class="s1">tosum=</span><span class="s3">False</span><span class="s1">)</span>
            <span class="s1">f_i = (u[i] * u_j * K)</span>
            <span class="s3">assert </span><span class="s1">u_j.shape == K.shape</span>
            <span class="s1">ival += f_i.sum()  </span><span class="s2"># See eq. 12.7 on p. 355 in [1]</span>
            <span class="s1">S2 += (f_i**</span><span class="s5">2</span><span class="s1">).sum()  </span><span class="s2"># See Theorem 12.1 on p.356 in [1]</span>
            <span class="s3">assert </span><span class="s1">np.size(ival) == </span><span class="s5">1</span>
            <span class="s3">assert </span><span class="s1">np.size(S2) == </span><span class="s5">1</span>

        <span class="s1">ival *= </span><span class="s5">1. </span><span class="s1">/ (n * (n - </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">ix_cont = _get_type_pos(self.var_type)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">hp = self.bw[ix_cont].prod()</span>
        <span class="s1">S2 *= </span><span class="s5">2 </span><span class="s1">* hp / (n * (n - </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">T = n * ival * np.sqrt(hp / S2)</span>
        <span class="s3">return </span><span class="s1">T</span>


<span class="s3">class </span><span class="s1">SingleIndexModel(KernelReg):</span>
    <span class="s0">&quot;&quot;&quot; 
    Single index semiparametric model ``y = g(X * b) + e``. 
 
    Parameters 
    ---------- 
    endog : array_like 
        The dependent variable 
    exog : array_like 
        The independent variable(s) 
    var_type : str 
        The type of variables in X: 
 
            - c: continuous 
            - o: ordered 
            - u: unordered 
 
    Attributes 
    ---------- 
    b : array_like 
        The linear coefficients b (betas) 
    bw : array_like 
        Bandwidths 
 
    Methods 
    ------- 
    fit(): Computes the fitted values ``E[Y|X] = g(X * b)`` 
           and the marginal effects ``dY/dX``. 
 
    References 
    ---------- 
    See chapter on semiparametric models in [1] 
 
    Notes 
    ----- 
    This model resembles the binary choice models. The user knows 
    that X and b interact linearly, but ``g(X * b)`` is unknown. 
    In the parametric binary choice models the user usually assumes 
    some distribution of g() such as normal or logistic. 
    &quot;&quot;&quot;</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">var_type):</span>
        <span class="s1">self.var_type = var_type</span>
        <span class="s1">self.K = len(var_type)</span>
        <span class="s1">self.var_type = self.var_type[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">self.endog = _adjust_shape(endog</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">self.exog = _adjust_shape(exog</span><span class="s3">, </span><span class="s1">self.K)</span>
        <span class="s1">self.nobs = np.shape(self.exog)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">self.data_type = self.var_type</span>
        <span class="s1">self.ckertype = </span><span class="s4">'gaussian'</span>
        <span class="s1">self.okertype = </span><span class="s4">'wangryzin'</span>
        <span class="s1">self.ukertype = </span><span class="s4">'aitchisonaitken'</span>
        <span class="s1">self.func = self._est_loc_linear</span>

        <span class="s1">self.b</span><span class="s3">, </span><span class="s1">self.bw = self._est_b_bw()</span>

    <span class="s3">def </span><span class="s1">_est_b_bw(self):</span>
        <span class="s1">params0 = np.random.uniform(size=(self.K + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">))</span>
        <span class="s1">b_bw = optimize.fmin(self.cv_loo</span><span class="s3">, </span><span class="s1">params0</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">b = b_bw[</span><span class="s5">0</span><span class="s1">:self.K]</span>
        <span class="s1">bw = b_bw[self.K:]</span>
        <span class="s1">bw = self._set_bw_bounds(bw)</span>
        <span class="s3">return </span><span class="s1">b</span><span class="s3">, </span><span class="s1">bw</span>

    <span class="s3">def </span><span class="s1">cv_loo(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2"># See p. 254 in Textbook</span>
        <span class="s1">params = np.asarray(params)</span>
        <span class="s1">b = params[</span><span class="s5">0 </span><span class="s1">: self.K]</span>
        <span class="s1">bw = params[self.K:]</span>
        <span class="s1">LOO_X = LeaveOneOut(self.exog)</span>
        <span class="s1">LOO_Y = LeaveOneOut(self.endog).__iter__()</span>
        <span class="s1">L = </span><span class="s5">0</span>
        <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">X_not_i </span><span class="s3">in </span><span class="s1">enumerate(LOO_X):</span>
            <span class="s1">Y = next(LOO_Y)</span>
            <span class="s2">#print b.shape, np.dot(self.exog[i:i+1, :], b).shape, bw,</span>
            <span class="s1">G = self.func(bw</span><span class="s3">, </span><span class="s1">endog=Y</span><span class="s3">, </span><span class="s1">exog=-np.dot(X_not_i</span><span class="s3">, </span><span class="s1">b)[:</span><span class="s3">,None</span><span class="s1">]</span><span class="s3">,</span>
                          <span class="s2">#data_predict=-b*self.exog[i, :])[0]</span>
                          <span class="s1">data_predict=-np.dot(self.exog[i:i+</span><span class="s5">1</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">b))[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s2">#print G.shape</span>
            <span class="s1">L += (self.endog[i] - G) ** </span><span class="s5">2</span>

        <span class="s2"># Note: There might be a way to vectorize this. See p.72 in [1]</span>
        <span class="s3">return </span><span class="s1">L / self.nobs</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">data_predict=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s3">if </span><span class="s1">data_predict </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">data_predict = self.exog</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">data_predict = _adjust_shape(data_predict</span><span class="s3">, </span><span class="s1">self.K)</span>

        <span class="s1">N_data_predict = np.shape(data_predict)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">mean = np.empty((N_data_predict</span><span class="s3">,</span><span class="s1">))</span>
        <span class="s1">mfx = np.empty((N_data_predict</span><span class="s3">, </span><span class="s1">self.K))</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(N_data_predict):</span>
            <span class="s1">mean_mfx = self.func(self.bw</span><span class="s3">, </span><span class="s1">self.endog</span><span class="s3">,</span>
                                 <span class="s1">np.dot(self.exog</span><span class="s3">, </span><span class="s1">self.b)[:</span><span class="s3">,None</span><span class="s1">]</span><span class="s3">,</span>
                                 <span class="s1">data_predict=np.dot(data_predict[i:i+</span><span class="s5">1</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span><span class="s1">self.b))</span>
            <span class="s1">mean[i] = mean_mfx[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">mfx_c = np.squeeze(mean_mfx[</span><span class="s5">1</span><span class="s1">])</span>
            <span class="s1">mfx[i</span><span class="s3">, </span><span class="s1">:] = mfx_c</span>

        <span class="s3">return </span><span class="s1">mean</span><span class="s3">, </span><span class="s1">mfx</span>

    <span class="s3">def </span><span class="s1">__repr__(self):</span>
        <span class="s0">&quot;&quot;&quot;Provide something sane to print.&quot;&quot;&quot;</span>
        <span class="s1">repr = </span><span class="s4">&quot;Single Index Model </span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">repr += </span><span class="s4">&quot;Number of variables: K = &quot; </span><span class="s1">+ str(self.K) + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">repr += </span><span class="s4">&quot;Number of samples:   nobs = &quot; </span><span class="s1">+ str(self.nobs) + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">repr += </span><span class="s4">&quot;Variable types:      &quot; </span><span class="s1">+ self.var_type + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">repr += </span><span class="s4">&quot;BW selection method: cv_ls&quot; </span><span class="s1">+ </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">repr += </span><span class="s4">&quot;Estimator type: local constant&quot; </span><span class="s1">+ </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s3">return </span><span class="s1">repr</span>


<span class="s3">class </span><span class="s1">SemiLinear(KernelReg):</span>
    <span class="s0">&quot;&quot;&quot; 
    Semiparametric partially linear model, ``Y = Xb + g(Z) + e``. 
 
    Parameters 
    ---------- 
    endog : array_like 
        The dependent variable 
    exog : array_like 
        The linear component in the regression 
    exog_nonparametric : array_like 
        The nonparametric component in the regression 
    var_type : str 
        The type of the variables in the nonparametric component; 
 
            - c: continuous 
            - o: ordered 
            - u: unordered 
 
    k_linear : int 
        The number of variables that comprise the linear component. 
 
    Attributes 
    ---------- 
    bw : array_like 
        Bandwidths for the nonparametric component exog_nonparametric 
    b : array_like 
        Coefficients in the linear component 
    nobs : int 
        The number of observations. 
    k_linear : int 
        The number of variables that comprise the linear component. 
 
    Methods 
    ------- 
    fit 
        Returns the fitted mean and marginal effects dy/dz 
 
    Notes 
    ----- 
    This model uses only the local constant regression estimator 
 
    References 
    ---------- 
    See chapter on Semiparametric Models in [1] 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">exog_nonparametric</span><span class="s3">, </span><span class="s1">var_type</span><span class="s3">, </span><span class="s1">k_linear):</span>
        <span class="s1">self.endog = _adjust_shape(endog</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">self.exog = _adjust_shape(exog</span><span class="s3">, </span><span class="s1">k_linear)</span>
        <span class="s1">self.K = len(var_type)</span>
        <span class="s1">self.exog_nonparametric = _adjust_shape(exog_nonparametric</span><span class="s3">, </span><span class="s1">self.K)</span>
        <span class="s1">self.k_linear = k_linear</span>
        <span class="s1">self.nobs = np.shape(self.exog)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">self.var_type = var_type</span>
        <span class="s1">self.data_type = self.var_type</span>
        <span class="s1">self.ckertype = </span><span class="s4">'gaussian'</span>
        <span class="s1">self.okertype = </span><span class="s4">'wangryzin'</span>
        <span class="s1">self.ukertype = </span><span class="s4">'aitchisonaitken'</span>
        <span class="s1">self.func = self._est_loc_linear</span>

        <span class="s1">self.b</span><span class="s3">, </span><span class="s1">self.bw = self._est_b_bw()</span>

    <span class="s3">def </span><span class="s1">_est_b_bw(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Computes the (beta) coefficients and the bandwidths. 
 
        Minimizes ``cv_loo`` with respect to ``b`` and ``bw``. 
        &quot;&quot;&quot;</span>
        <span class="s1">params0 = np.random.uniform(size=(self.k_linear + self.K</span><span class="s3">, </span><span class="s1">))</span>
        <span class="s1">b_bw = optimize.fmin(self.cv_loo</span><span class="s3">, </span><span class="s1">params0</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">b = b_bw[</span><span class="s5">0 </span><span class="s1">: self.k_linear]</span>
        <span class="s1">bw = b_bw[self.k_linear:]</span>
        <span class="s2">#bw = self._set_bw_bounds(np.asarray(bw))</span>
        <span class="s3">return </span><span class="s1">b</span><span class="s3">, </span><span class="s1">bw</span>

    <span class="s3">def </span><span class="s1">cv_loo(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Similar to the cross validation leave-one-out estimator. 
 
        Modified to reflect the linear components. 
 
        Parameters 
        ---------- 
        params : array_like 
            Vector consisting of the coefficients (b) and the bandwidths (bw). 
            The first ``k_linear`` elements are the coefficients. 
 
        Returns 
        ------- 
        L : float 
            The value of the objective function 
 
        References 
        ---------- 
        See p.254 in [1] 
        &quot;&quot;&quot;</span>
        <span class="s1">params = np.asarray(params)</span>
        <span class="s1">b = params[</span><span class="s5">0 </span><span class="s1">: self.k_linear]</span>
        <span class="s1">bw = params[self.k_linear:]</span>
        <span class="s1">LOO_X = LeaveOneOut(self.exog)</span>
        <span class="s1">LOO_Y = LeaveOneOut(self.endog).__iter__()</span>
        <span class="s1">LOO_Z = LeaveOneOut(self.exog_nonparametric).__iter__()</span>
        <span class="s1">Xb = np.dot(self.exog</span><span class="s3">, </span><span class="s1">b)[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s1">L = </span><span class="s5">0</span>
        <span class="s3">for </span><span class="s1">ii</span><span class="s3">, </span><span class="s1">X_not_i </span><span class="s3">in </span><span class="s1">enumerate(LOO_X):</span>
            <span class="s1">Y = next(LOO_Y)</span>
            <span class="s1">Z = next(LOO_Z)</span>
            <span class="s1">Xb_j = np.dot(X_not_i</span><span class="s3">, </span><span class="s1">b)[:</span><span class="s3">,None</span><span class="s1">]</span>
            <span class="s1">Yx = Y - Xb_j</span>
            <span class="s1">G = self.func(bw</span><span class="s3">, </span><span class="s1">endog=Yx</span><span class="s3">, </span><span class="s1">exog=-Z</span><span class="s3">,</span>
                          <span class="s1">data_predict=-self.exog_nonparametric[ii</span><span class="s3">, </span><span class="s1">:])[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">lt = Xb[ii</span><span class="s3">, </span><span class="s1">:] </span><span class="s2">#.sum()  # linear term</span>
            <span class="s1">L += (self.endog[ii] - lt - G) ** </span><span class="s5">2</span>

        <span class="s3">return </span><span class="s1">L</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">exog_predict=</span><span class="s3">None, </span><span class="s1">exog_nonparametric_predict=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Computes fitted values and marginal effects&quot;&quot;&quot;</span>

        <span class="s3">if </span><span class="s1">exog_predict </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog_predict = self.exog</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">exog_predict = _adjust_shape(exog_predict</span><span class="s3">, </span><span class="s1">self.k_linear)</span>

        <span class="s3">if </span><span class="s1">exog_nonparametric_predict </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog_nonparametric_predict = self.exog_nonparametric</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">exog_nonparametric_predict = _adjust_shape(exog_nonparametric_predict</span><span class="s3">, </span><span class="s1">self.K)</span>

        <span class="s1">N_data_predict = np.shape(exog_nonparametric_predict)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">mean = np.empty((N_data_predict</span><span class="s3">,</span><span class="s1">))</span>
        <span class="s1">mfx = np.empty((N_data_predict</span><span class="s3">, </span><span class="s1">self.K))</span>
        <span class="s1">Y = self.endog - np.dot(exog_predict</span><span class="s3">, </span><span class="s1">self.b)[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(N_data_predict):</span>
            <span class="s1">mean_mfx = self.func(self.bw</span><span class="s3">, </span><span class="s1">Y</span><span class="s3">, </span><span class="s1">self.exog_nonparametric</span><span class="s3">,</span>
                                 <span class="s1">data_predict=exog_nonparametric_predict[i</span><span class="s3">, </span><span class="s1">:])</span>
            <span class="s1">mean[i] = mean_mfx[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">mfx_c = np.squeeze(mean_mfx[</span><span class="s5">1</span><span class="s1">])</span>
            <span class="s1">mfx[i</span><span class="s3">, </span><span class="s1">:] = mfx_c</span>

        <span class="s3">return </span><span class="s1">mean</span><span class="s3">, </span><span class="s1">mfx</span>

    <span class="s3">def </span><span class="s1">__repr__(self):</span>
        <span class="s0">&quot;&quot;&quot;Provide something sane to print.&quot;&quot;&quot;</span>
        <span class="s1">repr = </span><span class="s4">&quot;Semiparamatric Partially Linear Model </span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">repr += </span><span class="s4">&quot;Number of variables: K = &quot; </span><span class="s1">+ str(self.K) + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">repr += </span><span class="s4">&quot;Number of samples:   N = &quot; </span><span class="s1">+ str(self.nobs) + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">repr += </span><span class="s4">&quot;Variable types:      &quot; </span><span class="s1">+ self.var_type + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">repr += </span><span class="s4">&quot;BW selection method: cv_ls&quot; </span><span class="s1">+ </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">repr += </span><span class="s4">&quot;Estimator type: local constant&quot; </span><span class="s1">+ </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s3">return </span><span class="s1">repr</span>
</pre>
</body>
</html>