<html>
<head>
<title>optimize.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
optimize.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Our own implementation of the Newton algorithm 
 
Unlike the scipy.optimize version, this version of the Newton conjugate 
gradient solver uses only one function call to retrieve the 
func value, the gradient value and a callable for the Hessian matvec 
product. If the function call is very expensive (e.g. for logistic 
regression with large design matrix), this approach gives very 
significant speedups. 
&quot;&quot;&quot;</span>
<span class="s2"># This is a modified file from scipy.optimize</span>
<span class="s2"># Original authors: Travis Oliphant, Eric Jones</span>
<span class="s2"># Modifications by Gael Varoquaux, Mathieu Blondel and Tom Dupre la Tour</span>
<span class="s2"># License: BSD</span>

<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">..exceptions </span><span class="s3">import </span><span class="s1">ConvergenceWarning</span>
<span class="s3">from </span><span class="s1">.fixes </span><span class="s3">import </span><span class="s1">line_search_wolfe1</span><span class="s3">, </span><span class="s1">line_search_wolfe2</span>


<span class="s3">class </span><span class="s1">_LineSearchError(RuntimeError):</span>
    <span class="s3">pass</span>


<span class="s3">def </span><span class="s1">_line_search_wolfe12(f</span><span class="s3">, </span><span class="s1">fprime</span><span class="s3">, </span><span class="s1">xk</span><span class="s3">, </span><span class="s1">pk</span><span class="s3">, </span><span class="s1">gfk</span><span class="s3">, </span><span class="s1">old_fval</span><span class="s3">, </span><span class="s1">old_old_fval</span><span class="s3">, </span><span class="s1">**kwargs):</span>
    <span class="s0">&quot;&quot;&quot; 
    Same as line_search_wolfe1, but fall back to line_search_wolfe2 if 
    suitable step length is not found, and raise an exception if a 
    suitable step length is not found. 
 
    Raises 
    ------ 
    _LineSearchError 
        If no suitable step size is found. 
 
    &quot;&quot;&quot;</span>
    <span class="s1">ret = line_search_wolfe1(f</span><span class="s3">, </span><span class="s1">fprime</span><span class="s3">, </span><span class="s1">xk</span><span class="s3">, </span><span class="s1">pk</span><span class="s3">, </span><span class="s1">gfk</span><span class="s3">, </span><span class="s1">old_fval</span><span class="s3">, </span><span class="s1">old_old_fval</span><span class="s3">, </span><span class="s1">**kwargs)</span>

    <span class="s3">if </span><span class="s1">ret[</span><span class="s4">0</span><span class="s1">] </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s2"># line search failed: try different one.</span>
        <span class="s1">ret = line_search_wolfe2(</span>
            <span class="s1">f</span><span class="s3">, </span><span class="s1">fprime</span><span class="s3">, </span><span class="s1">xk</span><span class="s3">, </span><span class="s1">pk</span><span class="s3">, </span><span class="s1">gfk</span><span class="s3">, </span><span class="s1">old_fval</span><span class="s3">, </span><span class="s1">old_old_fval</span><span class="s3">, </span><span class="s1">**kwargs</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">ret[</span><span class="s4">0</span><span class="s1">] </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">_LineSearchError()</span>

    <span class="s3">return </span><span class="s1">ret</span>


<span class="s3">def </span><span class="s1">_cg(fhess_p</span><span class="s3">, </span><span class="s1">fgrad</span><span class="s3">, </span><span class="s1">maxiter</span><span class="s3">, </span><span class="s1">tol):</span>
    <span class="s0">&quot;&quot;&quot; 
    Solve iteratively the linear system 'fhess_p . xsupi = fgrad' 
    with a conjugate gradient descent. 
 
    Parameters 
    ---------- 
    fhess_p : callable 
        Function that takes the gradient as a parameter and returns the 
        matrix product of the Hessian and gradient. 
 
    fgrad : ndarray of shape (n_features,) or (n_features + 1,) 
        Gradient vector. 
 
    maxiter : int 
        Number of CG iterations. 
 
    tol : float 
        Stopping criterion. 
 
    Returns 
    ------- 
    xsupi : ndarray of shape (n_features,) or (n_features + 1,) 
        Estimated solution. 
    &quot;&quot;&quot;</span>
    <span class="s1">xsupi = np.zeros(len(fgrad)</span><span class="s3">, </span><span class="s1">dtype=fgrad.dtype)</span>
    <span class="s1">ri = fgrad</span>
    <span class="s1">psupi = -ri</span>
    <span class="s1">i = </span><span class="s4">0</span>
    <span class="s1">dri0 = np.dot(ri</span><span class="s3">, </span><span class="s1">ri)</span>

    <span class="s3">while </span><span class="s1">i &lt;= maxiter:</span>
        <span class="s3">if </span><span class="s1">np.sum(np.abs(ri)) &lt;= tol:</span>
            <span class="s3">break</span>

        <span class="s1">Ap = fhess_p(psupi)</span>
        <span class="s2"># check curvature</span>
        <span class="s1">curv = np.dot(psupi</span><span class="s3">, </span><span class="s1">Ap)</span>
        <span class="s3">if </span><span class="s4">0 </span><span class="s1">&lt;= curv &lt;= </span><span class="s4">3 </span><span class="s1">* np.finfo(np.float64).eps:</span>
            <span class="s3">break</span>
        <span class="s3">elif </span><span class="s1">curv &lt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">i &gt; </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s3">break</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s2"># fall back to steepest descent direction</span>
                <span class="s1">xsupi += dri0 / curv * psupi</span>
                <span class="s3">break</span>
        <span class="s1">alphai = dri0 / curv</span>
        <span class="s1">xsupi += alphai * psupi</span>
        <span class="s1">ri = ri + alphai * Ap</span>
        <span class="s1">dri1 = np.dot(ri</span><span class="s3">, </span><span class="s1">ri)</span>
        <span class="s1">betai = dri1 / dri0</span>
        <span class="s1">psupi = -ri + betai * psupi</span>
        <span class="s1">i = i + </span><span class="s4">1</span>
        <span class="s1">dri0 = dri1  </span><span class="s2"># update np.dot(ri,ri) for next time.</span>

    <span class="s3">return </span><span class="s1">xsupi</span>


<span class="s3">def </span><span class="s1">_newton_cg(</span>
    <span class="s1">grad_hess</span><span class="s3">,</span>
    <span class="s1">func</span><span class="s3">,</span>
    <span class="s1">grad</span><span class="s3">,</span>
    <span class="s1">x0</span><span class="s3">,</span>
    <span class="s1">args=()</span><span class="s3">,</span>
    <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
    <span class="s1">maxiter=</span><span class="s4">100</span><span class="s3">,</span>
    <span class="s1">maxinner=</span><span class="s4">200</span><span class="s3">,</span>
    <span class="s1">line_search=</span><span class="s3">True,</span>
    <span class="s1">warn=</span><span class="s3">True,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Minimization of scalar function of one or more variables using the 
    Newton-CG algorithm. 
 
    Parameters 
    ---------- 
    grad_hess : callable 
        Should return the gradient and a callable returning the matvec product 
        of the Hessian. 
 
    func : callable 
        Should return the value of the function. 
 
    grad : callable 
        Should return the function value and the gradient. This is used 
        by the linesearch functions. 
 
    x0 : array of float 
        Initial guess. 
 
    args : tuple, default=() 
        Arguments passed to func_grad_hess, func and grad. 
 
    tol : float, default=1e-4 
        Stopping criterion. The iteration will stop when 
        ``max{|g_i | i = 1, ..., n} &lt;= tol`` 
        where ``g_i`` is the i-th component of the gradient. 
 
    maxiter : int, default=100 
        Number of Newton iterations. 
 
    maxinner : int, default=200 
        Number of CG iterations. 
 
    line_search : bool, default=True 
        Whether to use a line search or not. 
 
    warn : bool, default=True 
        Whether to warn when didn't converge. 
 
    Returns 
    ------- 
    xk : ndarray of float 
        Estimated minimum. 
    &quot;&quot;&quot;</span>
    <span class="s1">x0 = np.asarray(x0).flatten()</span>
    <span class="s1">xk = x0</span>
    <span class="s1">k = </span><span class="s4">0</span>

    <span class="s3">if </span><span class="s1">line_search:</span>
        <span class="s1">old_fval = func(x0</span><span class="s3">, </span><span class="s1">*args)</span>
        <span class="s1">old_old_fval = </span><span class="s3">None</span>

    <span class="s2"># Outer loop: our Newton iteration</span>
    <span class="s3">while </span><span class="s1">k &lt; maxiter:</span>
        <span class="s2"># Compute a search direction pk by applying the CG method to</span>
        <span class="s2">#  del2 f(xk) p = - fgrad f(xk) starting from 0.</span>
        <span class="s1">fgrad</span><span class="s3">, </span><span class="s1">fhess_p = grad_hess(xk</span><span class="s3">, </span><span class="s1">*args)</span>

        <span class="s1">absgrad = np.abs(fgrad)</span>
        <span class="s3">if </span><span class="s1">np.max(absgrad) &lt;= tol:</span>
            <span class="s3">break</span>

        <span class="s1">maggrad = np.sum(absgrad)</span>
        <span class="s1">eta = min([</span><span class="s4">0.5</span><span class="s3">, </span><span class="s1">np.sqrt(maggrad)])</span>
        <span class="s1">termcond = eta * maggrad</span>

        <span class="s2"># Inner loop: solve the Newton update by conjugate gradient, to</span>
        <span class="s2"># avoid inverting the Hessian</span>
        <span class="s1">xsupi = _cg(fhess_p</span><span class="s3">, </span><span class="s1">fgrad</span><span class="s3">, </span><span class="s1">maxiter=maxinner</span><span class="s3">, </span><span class="s1">tol=termcond)</span>

        <span class="s1">alphak = </span><span class="s4">1.0</span>

        <span class="s3">if </span><span class="s1">line_search:</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s1">alphak</span><span class="s3">, </span><span class="s1">fc</span><span class="s3">, </span><span class="s1">gc</span><span class="s3">, </span><span class="s1">old_fval</span><span class="s3">, </span><span class="s1">old_old_fval</span><span class="s3">, </span><span class="s1">gfkp1 = _line_search_wolfe12(</span>
                    <span class="s1">func</span><span class="s3">, </span><span class="s1">grad</span><span class="s3">, </span><span class="s1">xk</span><span class="s3">, </span><span class="s1">xsupi</span><span class="s3">, </span><span class="s1">fgrad</span><span class="s3">, </span><span class="s1">old_fval</span><span class="s3">, </span><span class="s1">old_old_fval</span><span class="s3">, </span><span class="s1">args=args</span>
                <span class="s1">)</span>
            <span class="s3">except </span><span class="s1">_LineSearchError:</span>
                <span class="s1">warnings.warn(</span><span class="s5">&quot;Line Search failed&quot;</span><span class="s1">)</span>
                <span class="s3">break</span>

        <span class="s1">xk = xk + alphak * xsupi  </span><span class="s2"># upcast if necessary</span>
        <span class="s1">k += </span><span class="s4">1</span>

    <span class="s3">if </span><span class="s1">warn </span><span class="s3">and </span><span class="s1">k &gt;= maxiter:</span>
        <span class="s1">warnings.warn(</span>
            <span class="s5">&quot;newton-cg failed to converge. Increase the number of iterations.&quot;</span><span class="s3">,</span>
            <span class="s1">ConvergenceWarning</span><span class="s3">,</span>
        <span class="s1">)</span>
    <span class="s3">return </span><span class="s1">xk</span><span class="s3">, </span><span class="s1">k</span>


<span class="s3">def </span><span class="s1">_check_optimize_result(solver</span><span class="s3">, </span><span class="s1">result</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s3">None, </span><span class="s1">extra_warning_msg=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Check the OptimizeResult for successful convergence 
 
    Parameters 
    ---------- 
    solver : str 
       Solver name. Currently only `lbfgs` is supported. 
 
    result : OptimizeResult 
       Result of the scipy.optimize.minimize function. 
 
    max_iter : int, default=None 
       Expected maximum number of iterations. 
 
    extra_warning_msg : str, default=None 
        Extra warning message. 
 
    Returns 
    ------- 
    n_iter : int 
       Number of iterations. 
    &quot;&quot;&quot;</span>
    <span class="s2"># handle both scipy and scikit-learn solver names</span>
    <span class="s3">if </span><span class="s1">solver == </span><span class="s5">&quot;lbfgs&quot;</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">result.status != </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s2"># The message is already decoded in scipy&gt;=1.6.0</span>
                <span class="s1">result_message = result.message.decode(</span><span class="s5">&quot;latin1&quot;</span><span class="s1">)</span>
            <span class="s3">except </span><span class="s1">AttributeError:</span>
                <span class="s1">result_message = result.message</span>
            <span class="s1">warning_msg = (</span>
                <span class="s5">&quot;{} failed to converge (status={}):</span><span class="s3">\n</span><span class="s5">{}.</span><span class="s3">\n\n</span><span class="s5">&quot;</span>
                <span class="s5">&quot;Increase the number of iterations (max_iter) &quot;</span>
                <span class="s5">&quot;or scale the data as shown in:</span><span class="s3">\n</span><span class="s5">&quot;</span>
                <span class="s5">&quot;    https://scikit-learn.org/stable/modules/&quot;</span>
                <span class="s5">&quot;preprocessing.html&quot;</span>
            <span class="s1">).format(solver</span><span class="s3">, </span><span class="s1">result.status</span><span class="s3">, </span><span class="s1">result_message)</span>
            <span class="s3">if </span><span class="s1">extra_warning_msg </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">warning_msg += </span><span class="s5">&quot;</span><span class="s3">\n</span><span class="s5">&quot; </span><span class="s1">+ extra_warning_msg</span>
            <span class="s1">warnings.warn(warning_msg</span><span class="s3">, </span><span class="s1">ConvergenceWarning</span><span class="s3">, </span><span class="s1">stacklevel=</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">max_iter </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s2"># In scipy &lt;= 1.0.0, nit may exceed maxiter for lbfgs.</span>
            <span class="s2"># See https://github.com/scipy/scipy/issues/7854</span>
            <span class="s1">n_iter_i = min(result.nit</span><span class="s3">, </span><span class="s1">max_iter)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">n_iter_i = result.nit</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">return </span><span class="s1">n_iter_i</span>
</pre>
</body>
</html>