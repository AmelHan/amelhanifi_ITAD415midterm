<html>
<head>
<title>test_theil.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_theil.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot; 
Created on Mon May 05 17:29:56 2014 
 
Author: Josef Perktold 
&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">os</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">pandas </span><span class="s3">as </span><span class="s1">pd</span>
<span class="s3">import </span><span class="s1">pytest</span>

<span class="s3">from </span><span class="s1">numpy.testing </span><span class="s3">import </span><span class="s1">assert_allclose</span>

<span class="s3">from </span><span class="s1">statsmodels.regression.linear_model </span><span class="s3">import </span><span class="s1">OLS</span><span class="s3">, </span><span class="s1">GLS</span>

<span class="s3">from </span><span class="s1">statsmodels.sandbox.regression.penalized </span><span class="s3">import </span><span class="s1">TheilGLS</span>


<span class="s3">class </span><span class="s1">TestTheilTextile:</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>

        <span class="s1">cur_dir = os.path.dirname(os.path.abspath(__file__))</span>
        <span class="s1">filepath = os.path.join(cur_dir</span><span class="s3">, </span><span class="s4">&quot;results&quot;</span><span class="s3">,</span>
                                <span class="s4">&quot;theil_textile_predict.csv&quot;</span><span class="s1">)</span>
        <span class="s1">cls.res_predict = pd.read_csv(filepath</span><span class="s3">, </span><span class="s1">sep=</span><span class="s4">&quot;,&quot;</span><span class="s1">)</span>

        <span class="s1">names = </span><span class="s4">&quot;year lconsump    lincome lprice&quot;</span><span class="s1">.split()</span>

        <span class="s1">data = np.array(</span><span class="s4">'''</span><span class="s3">\ 
        </span><span class="s4">1923    1.99651 1.98543 2.00432 
        1924    1.99564 1.99167 2.00043 
        1925    2   2   2 
        1926    2.04766 2.02078 1.95713 
        1927    2.08707 2.02078 1.93702 
        1928    2.07041 2.03941 1.95279 
        1929    2.08314 2.04454 1.95713 
        1930    2.13354 2.05038 1.91803 
        1931    2.18808 2.03862 1.84572 
        1932    2.18639 2.02243 1.81558 
        1933    2.20003 2.00732 1.78746 
        1934    2.14799 1.97955 1.79588 
        1935    2.13418 1.98408 1.80346 
        1936    2.22531 1.98945 1.72099 
        1937    2.18837 2.0103  1.77597 
        1938    2.17319 2.00689 1.77452 
        1939    2.2188  2.0162  1.78746'''</span><span class="s1">.split()</span><span class="s3">, </span><span class="s1">float).reshape(-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">4</span><span class="s1">)</span>


        <span class="s1">endog = data[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]</span>
        <span class="s0"># constant at the end to match Stata</span>
        <span class="s1">exog = np.column_stack((data[:</span><span class="s3">, </span><span class="s5">2</span><span class="s1">:]</span><span class="s3">, </span><span class="s1">np.ones(endog.shape[</span><span class="s5">0</span><span class="s1">])))</span>

        <span class="s0">#prior(lprice -0.7 0.15 lincome 1 0.15) cov(lprice lincome -0.01)</span>
        <span class="s1">r_matrix = np.array([[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]])</span>
        <span class="s1">r_mean = [</span><span class="s5">1</span><span class="s3">, </span><span class="s1">-</span><span class="s5">0.7</span><span class="s1">]</span>

        <span class="s1">cov_r = np.array([[</span><span class="s5">0.15</span><span class="s1">**</span><span class="s5">2</span><span class="s3">, </span><span class="s1">-</span><span class="s5">0.01</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[-</span><span class="s5">0.01</span><span class="s3">, </span><span class="s5">0.15</span><span class="s1">**</span><span class="s5">2</span><span class="s1">]])</span>
        <span class="s1">mod = TheilGLS(endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">r_matrix</span><span class="s3">, </span><span class="s1">q_matrix=r_mean</span><span class="s3">, </span><span class="s1">sigma_prior=cov_r)</span>
        <span class="s1">cls.res1 = mod.fit(cov_type=</span><span class="s4">'data-prior'</span><span class="s3">, </span><span class="s1">use_t=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s0">#cls.res1._cache['scale'] = 0.0001852252884817586 # from tg_mixed</span>
        <span class="s1">cls.res1._cache[</span><span class="s4">'scale'</span><span class="s1">] = </span><span class="s5">0.00018334123641580062 </span><span class="s0"># from OLS</span>
        <span class="s3">from </span><span class="s1">.results </span><span class="s3">import </span><span class="s1">results_theil_textile </span><span class="s3">as </span><span class="s1">resmodule</span>
        <span class="s1">cls.res2 = resmodule.results_theil_textile</span>


    <span class="s3">def </span><span class="s1">test_basic(self):</span>
        <span class="s1">pt = self.res2.params_table[:</span><span class="s3">,</span><span class="s1">:</span><span class="s5">6</span><span class="s1">].T</span>
        <span class="s1">params2</span><span class="s3">, </span><span class="s1">bse2</span><span class="s3">, </span><span class="s1">tvalues2</span><span class="s3">, </span><span class="s1">pvalues2</span><span class="s3">, </span><span class="s1">ci_low</span><span class="s3">, </span><span class="s1">ci_upp = pt</span>
        <span class="s1">assert_allclose(self.res1.params</span><span class="s3">, </span><span class="s1">params2</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">2e-6</span><span class="s1">)</span>

        <span class="s0">#TODO tgmixed seems to use scale from initial OLS, not from final res</span>
        <span class="s0"># np.sqrt(res.scale / res_ols.scale)</span>
        <span class="s0"># see below mse_resid which is equal to scale</span>
        <span class="s1">corr_fact = </span><span class="s5">0.9836026210570028</span>
        <span class="s1">corr_fact = </span><span class="s5">0.97376865041463734</span>
        <span class="s1">corr_fact = </span><span class="s5">1</span>
        <span class="s1">assert_allclose(self.res1.bse / corr_fact</span><span class="s3">, </span><span class="s1">bse2</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">2e-6</span><span class="s1">)</span>
        <span class="s1">assert_allclose(self.res1.tvalues  * corr_fact</span><span class="s3">, </span><span class="s1">tvalues2</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">2e-6</span><span class="s1">)</span>
        <span class="s0"># pvalues are very small</span>
        <span class="s0">#assert_allclose(self.res1.pvalues, pvalues2, atol=2e-6)</span>
        <span class="s0">#assert_allclose(self.res1.pvalues, pvalues2, rtol=0.7)</span>
        <span class="s1">ci = self.res1.conf_int()</span>
        <span class="s0"># not scale corrected</span>
        <span class="s1">assert_allclose(ci[:</span><span class="s3">,</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">ci_low</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">0.01</span><span class="s1">)</span>
        <span class="s1">assert_allclose(ci[:</span><span class="s3">,</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">ci_upp</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">0.01</span><span class="s1">)</span>
        <span class="s1">assert_allclose(self.res1.rsquared</span><span class="s3">, </span><span class="s1">self.res2.r2</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">2e-6</span><span class="s1">)</span>

        <span class="s0"># Note: tgmixed is using k_exog for df_resid</span>
        <span class="s1">corr_fact = self.res1.df_resid / self.res2.df_r</span>
        <span class="s1">assert_allclose(np.sqrt(self.res1.mse_resid * corr_fact)</span><span class="s3">,</span>
                        <span class="s1">self.res2.rmse</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">2e-6</span><span class="s1">)</span>

        <span class="s1">assert_allclose(self.res1.fittedvalues</span><span class="s3">,</span>
                        <span class="s1">self.res_predict[</span><span class="s4">'fittedvalues'</span><span class="s1">]</span><span class="s3">, </span><span class="s1">atol=</span><span class="s5">5e7</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">test_other(self):</span>
        <span class="s1">tc = self.res1.test_compatibility()</span>
        <span class="s1">assert_allclose(np.squeeze(tc[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">, </span><span class="s1">self.res2.compat</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">2e-6</span><span class="s1">)</span>
        <span class="s1">assert_allclose(np.squeeze(tc[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">self.res2.pvalue</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">2e-6</span><span class="s1">)</span>

        <span class="s1">frac = self.res1.share_data()</span>
        <span class="s0"># TODO check again, I guess tgmixed uses final scale in hatmatrix</span>
        <span class="s0"># but I'm not sure, it passed in previous version, but now we override</span>
        <span class="s0"># scale with OLS scale</span>
        <span class="s0"># assert_allclose(frac, self.res2.frac_sample, rtol=2e-6)</span>
        <span class="s0"># regression tests:</span>
        <span class="s1">assert_allclose(frac</span><span class="s3">, </span><span class="s5">0.6946116246864239</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">2e-6</span><span class="s1">)</span>


    <span class="s3">def </span><span class="s1">test_no_penalization(self):</span>
        <span class="s1">res_ols = OLS(self.res1.model.endog</span><span class="s3">, </span><span class="s1">self.res1.model.exog).fit()</span>
        <span class="s1">res_theil = self.res1.model.fit(pen_weight=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">cov_type=</span><span class="s4">'data-prior'</span><span class="s1">)</span>
        <span class="s1">assert_allclose(res_theil.params</span><span class="s3">, </span><span class="s1">res_ols.params</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-10</span><span class="s1">)</span>
        <span class="s1">assert_allclose(res_theil.bse</span><span class="s3">, </span><span class="s1">res_ols.bse</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-10</span><span class="s1">)</span>

    <span class="s1">@pytest.mark.smoke</span>
    <span class="s3">def </span><span class="s1">test_summary(self):</span>
        <span class="s3">with </span><span class="s1">pytest.warns(UserWarning):</span>
            <span class="s1">self.res1.summary()</span>


<span class="s3">class </span><span class="s1">CheckEquivalenceMixin:</span>

    <span class="s1">tol = {</span><span class="s4">'default'</span><span class="s1">: (</span><span class="s5">1e-4</span><span class="s3">, </span><span class="s5">1e-20</span><span class="s1">)}</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">get_sample(cls):</span>
        <span class="s1">np.random.seed(</span><span class="s5">987456</span><span class="s1">)</span>
        <span class="s1">nobs</span><span class="s3">, </span><span class="s1">k_vars = </span><span class="s5">200</span><span class="s3">, </span><span class="s5">5</span>
        <span class="s1">beta = </span><span class="s5">0.5 </span><span class="s1">* np.array([</span><span class="s5">0.1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span>
        <span class="s1">x = np.random.randn(nobs</span><span class="s3">, </span><span class="s1">k_vars)</span>
        <span class="s1">x[:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = </span><span class="s5">1</span>
        <span class="s1">y = np.dot(x</span><span class="s3">, </span><span class="s1">beta) + </span><span class="s5">2 </span><span class="s1">* np.random.randn(nobs)</span>
        <span class="s3">return </span><span class="s1">y</span><span class="s3">, </span><span class="s1">x</span>

    <span class="s3">def </span><span class="s1">test_attributes(self):</span>

        <span class="s1">attributes_fit = [</span><span class="s4">'params'</span><span class="s3">, </span><span class="s4">'rsquared'</span><span class="s3">, </span><span class="s4">'df_resid'</span><span class="s3">, </span><span class="s4">'df_model'</span><span class="s3">,</span>
                          <span class="s4">'llf'</span><span class="s3">, </span><span class="s4">'aic'</span><span class="s3">, </span><span class="s4">'bic'</span>
                          <span class="s0">#'fittedvalues', 'resid'</span>
                          <span class="s1">]</span>
        <span class="s1">attributes_inference = [</span><span class="s4">'bse'</span><span class="s3">, </span><span class="s4">'tvalues'</span><span class="s3">, </span><span class="s4">'pvalues'</span><span class="s1">]</span>
        <span class="s3">import </span><span class="s1">copy</span>
        <span class="s1">attributes = copy.copy(attributes_fit)</span>

        <span class="s3">if not </span><span class="s1">getattr(self</span><span class="s3">, </span><span class="s4">'skip_inference'</span><span class="s3">, False</span><span class="s1">):</span>
            <span class="s1">attributes.extend(attributes_inference)</span>

        <span class="s3">for </span><span class="s1">att </span><span class="s3">in </span><span class="s1">attributes:</span>
            <span class="s1">r1 = getattr(self.res1</span><span class="s3">, </span><span class="s1">att)</span>
            <span class="s1">r2 = getattr(self.res2</span><span class="s3">, </span><span class="s1">att)</span>
            <span class="s3">if not </span><span class="s1">np.size(r1) == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">r1 = r1[:len(r2)]</span>

            <span class="s0"># check if we have overwritten tolerance</span>
            <span class="s1">rtol</span><span class="s3">, </span><span class="s1">atol = self.tol.get(att</span><span class="s3">, </span><span class="s1">self.tol[</span><span class="s4">'default'</span><span class="s1">])</span>
            <span class="s1">message = </span><span class="s4">'attribute: ' </span><span class="s1">+ att </span><span class="s0">#+ '\n%r\n\%r' % (r1, r2)</span>
            <span class="s1">assert_allclose(r1</span><span class="s3">, </span><span class="s1">r2</span><span class="s3">, </span><span class="s1">rtol=rtol</span><span class="s3">, </span><span class="s1">atol=atol</span><span class="s3">, </span><span class="s1">err_msg=message)</span>

        <span class="s0"># models are not close enough for some attributes at high precision</span>
        <span class="s1">assert_allclose(self.res1.fittedvalues</span><span class="s3">, </span><span class="s1">self.res1.fittedvalues</span><span class="s3">,</span>
                        <span class="s1">rtol=</span><span class="s5">1e-3</span><span class="s3">, </span><span class="s1">atol=</span><span class="s5">1e-4</span><span class="s1">)</span>
        <span class="s1">assert_allclose(self.res1.resid</span><span class="s3">, </span><span class="s1">self.res1.resid</span><span class="s3">,</span>
                        <span class="s1">rtol=</span><span class="s5">1e-3</span><span class="s3">, </span><span class="s1">atol=</span><span class="s5">1e-4</span><span class="s1">)</span>


<span class="s3">class </span><span class="s1">TestTheil1(CheckEquivalenceMixin):</span>
    <span class="s0"># penalize last two parameters to zero</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">y</span><span class="s3">, </span><span class="s1">x = cls.get_sample()</span>
        <span class="s1">mod1 = TheilGLS(y</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">sigma_prior=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1.</span><span class="s3">, </span><span class="s5">1.</span><span class="s1">])</span>
        <span class="s1">cls.res1 = mod1.fit(</span><span class="s5">200000</span><span class="s1">)</span>
        <span class="s1">cls.res2 = OLS(y</span><span class="s3">, </span><span class="s1">x[:</span><span class="s3">, </span><span class="s1">:</span><span class="s5">3</span><span class="s1">]).fit()</span>

<span class="s3">class </span><span class="s1">TestTheil2(CheckEquivalenceMixin):</span>
    <span class="s0"># no penalization = same as OLS</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">y</span><span class="s3">, </span><span class="s1">x = cls.get_sample()</span>
        <span class="s1">mod1 = TheilGLS(y</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">sigma_prior=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1.</span><span class="s3">, </span><span class="s5">1.</span><span class="s1">])</span>
        <span class="s1">cls.res1 = mod1.fit(</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">cls.res2 = OLS(y</span><span class="s3">, </span><span class="s1">x).fit()</span>


<span class="s3">class </span><span class="s1">TestTheil3(CheckEquivalenceMixin):</span>
    <span class="s0"># perfect multicollinearity = same as OLS in terms of fit</span>
    <span class="s0"># inference: bse, ... is different</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">cls.skip_inference = </span><span class="s3">True</span>
        <span class="s1">y</span><span class="s3">, </span><span class="s1">x = cls.get_sample()</span>
        <span class="s1">xd = np.column_stack((x</span><span class="s3">, </span><span class="s1">x))</span>
        <span class="s0">#sp = np.zeros(5), np.ones(5)</span>
        <span class="s1">r_matrix = np.eye(</span><span class="s5">5</span><span class="s3">, </span><span class="s5">10</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span>
        <span class="s1">mod1 = TheilGLS(y</span><span class="s3">, </span><span class="s1">xd</span><span class="s3">, </span><span class="s1">r_matrix=r_matrix) </span><span class="s0">#sigma_prior=[0, 0, 1., 1.])</span>
        <span class="s1">cls.res1 = mod1.fit(</span><span class="s5">0.001</span><span class="s3">, </span><span class="s1">cov_type=</span><span class="s4">'data-prior'</span><span class="s1">)</span>
        <span class="s1">cls.res2 = OLS(y</span><span class="s3">, </span><span class="s1">x).fit()</span>


<span class="s3">class </span><span class="s1">TestTheilGLS(CheckEquivalenceMixin):</span>
    <span class="s0"># penalize last two parameters to zero</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">y</span><span class="s3">, </span><span class="s1">x = cls.get_sample()</span>
        <span class="s1">nobs = len(y)</span>
        <span class="s1">weights = (np.arange(nobs) &lt; (nobs // </span><span class="s5">2</span><span class="s1">)) + </span><span class="s5">0.5</span>
        <span class="s1">mod1 = TheilGLS(y</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">sigma=weights</span><span class="s3">, </span><span class="s1">sigma_prior=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1.</span><span class="s3">, </span><span class="s5">1.</span><span class="s1">])</span>
        <span class="s1">cls.res1 = mod1.fit(</span><span class="s5">200000</span><span class="s1">)</span>
        <span class="s1">cls.res2 = GLS(y</span><span class="s3">, </span><span class="s1">x[:</span><span class="s3">, </span><span class="s1">:</span><span class="s5">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">sigma=weights).fit()</span>


<span class="s3">class </span><span class="s1">TestTheilLinRestriction(CheckEquivalenceMixin):</span>
    <span class="s0"># impose linear restriction with small uncertainty - close to OLS</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">y</span><span class="s3">, </span><span class="s1">x = cls.get_sample()</span>
        <span class="s0">#merge var1 and var2</span>
        <span class="s1">x2 = x[:</span><span class="s3">, </span><span class="s1">:</span><span class="s5">2</span><span class="s1">].copy()</span>
        <span class="s1">x2[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">] += x[:</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span>
        <span class="s0">#mod1 = TheilGLS(y, x, r_matrix =[[0, 1, -1, 0, 0]])</span>
        <span class="s1">mod1 = TheilGLS(y</span><span class="s3">, </span><span class="s1">x[:</span><span class="s3">, </span><span class="s1">:</span><span class="s5">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">r_matrix =[[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">]])</span>
        <span class="s1">cls.res1 = mod1.fit(</span><span class="s5">200000</span><span class="s1">)</span>
        <span class="s1">cls.res2 = OLS(y</span><span class="s3">, </span><span class="s1">x2).fit()</span>

        <span class="s0"># adjust precision, careful: cls.tol is mutable</span>
        <span class="s1">tol = {</span><span class="s4">'pvalues'</span><span class="s1">: (</span><span class="s5">1e-4</span><span class="s3">, </span><span class="s5">2e-7</span><span class="s1">)</span><span class="s3">,</span>
               <span class="s4">'tvalues'</span><span class="s1">: (</span><span class="s5">5e-4</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)}</span>
        <span class="s1">tol.update(cls.tol)</span>
        <span class="s1">cls.tol = tol</span>


<span class="s3">class </span><span class="s1">TestTheilLinRestrictionApprox(CheckEquivalenceMixin):</span>
    <span class="s0"># impose linear restriction with some uncertainty</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">y</span><span class="s3">, </span><span class="s1">x = cls.get_sample()</span>
        <span class="s0">#merge var1 and var2</span>
        <span class="s1">x2 = x[:</span><span class="s3">, </span><span class="s1">:</span><span class="s5">2</span><span class="s1">].copy()</span>
        <span class="s1">x2[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">] += x[:</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span>
        <span class="s0">#mod1 = TheilGLS(y, x, r_matrix =[[0, 1, -1, 0, 0]])</span>
        <span class="s1">mod1 = TheilGLS(y</span><span class="s3">, </span><span class="s1">x[:</span><span class="s3">, </span><span class="s1">:</span><span class="s5">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">r_matrix =[[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">]])</span>
        <span class="s1">cls.res1 = mod1.fit(</span><span class="s5">100</span><span class="s1">)</span>
        <span class="s1">cls.res2 = OLS(y</span><span class="s3">, </span><span class="s1">x2).fit()</span>

        <span class="s0"># adjust precision, careful: cls.tol is mutable</span>
        <span class="s3">import </span><span class="s1">copy</span>
        <span class="s1">tol = copy.copy(cls.tol)</span>
        <span class="s1">tol2 = {</span><span class="s4">'default'</span><span class="s1">: (</span><span class="s5">0.15</span><span class="s3">,  </span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s4">'params'</span><span class="s1">:  (</span><span class="s5">0.05</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s4">'pvalues'</span><span class="s1">: (</span><span class="s5">0.02</span><span class="s3">, </span><span class="s5">0.001</span><span class="s1">)</span><span class="s3">,</span>
                <span class="s1">}</span>
        <span class="s1">tol.update(tol2)</span>
        <span class="s1">cls.tol = tol</span>


<span class="s3">class </span><span class="s1">TestTheilPanel:</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">#example 3</span>
        <span class="s1">nobs = </span><span class="s5">300</span>
        <span class="s1">nobs_i = </span><span class="s5">5</span>
        <span class="s1">n_groups = nobs // nobs_i</span>
        <span class="s1">k_vars = </span><span class="s5">3</span>

        <span class="s3">from </span><span class="s1">statsmodels.sandbox.panel.random_panel </span><span class="s3">import </span><span class="s1">PanelSample</span>
        <span class="s1">dgp = PanelSample(nobs</span><span class="s3">, </span><span class="s1">k_vars</span><span class="s3">, </span><span class="s1">n_groups</span><span class="s3">, </span><span class="s1">seed=</span><span class="s5">303305</span><span class="s1">)</span>
        <span class="s0"># add random intercept, using same RandomState</span>
        <span class="s1">dgp.group_means = </span><span class="s5">2 </span><span class="s1">+ dgp.random_state.randn(n_groups)</span>
        <span class="s1">print(</span><span class="s4">'seed'</span><span class="s3">, </span><span class="s1">dgp.seed)</span>
        <span class="s1">y = dgp.generate_panel()</span>
        <span class="s1">x = np.column_stack((dgp.exog[:</span><span class="s3">,</span><span class="s5">1</span><span class="s1">:]</span><span class="s3">,</span>
                             <span class="s1">dgp.groups[:</span><span class="s3">,None</span><span class="s1">] == np.arange(n_groups)))</span>
        <span class="s1">cls.dgp = dgp</span>
        <span class="s1">cls.endog = y</span>
        <span class="s1">cls.exog = x</span>
        <span class="s1">cls.res_ols = OLS(y</span><span class="s3">, </span><span class="s1">x).fit()</span>

    <span class="s3">def </span><span class="s1">test_regression(self):</span>
        <span class="s1">y = self.endog</span>
        <span class="s1">x = self.exog</span>
        <span class="s1">n_groups</span><span class="s3">, </span><span class="s1">k_vars = self.dgp.n_groups</span><span class="s3">, </span><span class="s1">self.dgp.k_vars</span>

        <span class="s1">Rg = (np.eye(n_groups-</span><span class="s5">1</span><span class="s1">) - </span><span class="s5">1. </span><span class="s1">/ n_groups *</span>
                <span class="s1">np.ones((n_groups - </span><span class="s5">1</span><span class="s3">, </span><span class="s1">n_groups-</span><span class="s5">1</span><span class="s1">)))</span>
        <span class="s1">R = np.c_[np.zeros((n_groups - </span><span class="s5">1</span><span class="s3">, </span><span class="s1">k_vars))</span><span class="s3">, </span><span class="s1">Rg]</span>
        <span class="s1">r = np.zeros(n_groups - </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">R[:</span><span class="s3">, </span><span class="s1">k_vars-</span><span class="s5">1</span><span class="s1">] = -</span><span class="s5">1</span>

        <span class="s1">lambd = </span><span class="s5">1 </span><span class="s0">#1e-4</span>
        <span class="s1">mod = TheilGLS(y</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">r_matrix=R</span><span class="s3">, </span><span class="s1">q_matrix=r</span><span class="s3">, </span><span class="s1">sigma_prior=lambd)</span>
        <span class="s1">res = mod.fit()</span>

        <span class="s0"># regression test</span>
        <span class="s1">params1 = np.array([</span>
            <span class="s5">0.9751655 </span><span class="s3">,  </span><span class="s5">1.05215277</span><span class="s3">,  </span><span class="s5">0.37135028</span><span class="s3">,  </span><span class="s5">2.0492626 </span><span class="s3">,  </span><span class="s5">2.82062503</span><span class="s3">,</span>
            <span class="s5">2.82139775</span><span class="s3">,  </span><span class="s5">1.92940468</span><span class="s3">,  </span><span class="s5">2.96942081</span><span class="s3">,  </span><span class="s5">2.86349583</span><span class="s3">,  </span><span class="s5">3.20695368</span><span class="s3">,</span>
            <span class="s5">4.04516422</span><span class="s3">,  </span><span class="s5">3.04918839</span><span class="s3">,  </span><span class="s5">4.54748808</span><span class="s3">,  </span><span class="s5">3.49026961</span><span class="s3">,  </span><span class="s5">3.15529618</span><span class="s3">,</span>
            <span class="s5">4.25552932</span><span class="s3">,  </span><span class="s5">2.65471759</span><span class="s3">,  </span><span class="s5">3.62328747</span><span class="s3">,  </span><span class="s5">3.07283053</span><span class="s3">,  </span><span class="s5">3.49485898</span><span class="s3">,</span>
            <span class="s5">3.42301424</span><span class="s3">,  </span><span class="s5">2.94677593</span><span class="s3">,  </span><span class="s5">2.81549427</span><span class="s3">,  </span><span class="s5">2.24895113</span><span class="s3">,  </span><span class="s5">2.29222784</span><span class="s3">,</span>
            <span class="s5">2.89194946</span><span class="s3">,  </span><span class="s5">3.17052308</span><span class="s3">,  </span><span class="s5">2.37754241</span><span class="s3">,  </span><span class="s5">3.54358533</span><span class="s3">,  </span><span class="s5">3.79838425</span><span class="s3">,</span>
            <span class="s5">1.91189071</span><span class="s3">,  </span><span class="s5">1.15976407</span><span class="s3">,  </span><span class="s5">4.05629691</span><span class="s3">,  </span><span class="s5">1.58556827</span><span class="s3">,  </span><span class="s5">4.49941666</span><span class="s3">,</span>
            <span class="s5">4.08608599</span><span class="s3">,  </span><span class="s5">3.1889269 </span><span class="s3">,  </span><span class="s5">2.86203652</span><span class="s3">,  </span><span class="s5">3.06785013</span><span class="s3">,  </span><span class="s5">1.9376162 </span><span class="s3">,</span>
            <span class="s5">2.90657681</span><span class="s3">,  </span><span class="s5">3.71910592</span><span class="s3">,  </span><span class="s5">3.15607617</span><span class="s3">,  </span><span class="s5">3.58464547</span><span class="s3">,  </span><span class="s5">2.15466323</span><span class="s3">,</span>
            <span class="s5">4.87026717</span><span class="s3">,  </span><span class="s5">2.92909833</span><span class="s3">,  </span><span class="s5">2.64998337</span><span class="s3">,  </span><span class="s5">2.891171  </span><span class="s3">,  </span><span class="s5">4.04422964</span><span class="s3">,</span>
            <span class="s5">3.54616122</span><span class="s3">,  </span><span class="s5">4.12135273</span><span class="s3">,  </span><span class="s5">3.70232028</span><span class="s3">,  </span><span class="s5">3.8314497 </span><span class="s3">,  </span><span class="s5">2.2591451 </span><span class="s3">,</span>
            <span class="s5">2.39321422</span><span class="s3">,  </span><span class="s5">3.13064532</span><span class="s3">,  </span><span class="s5">2.1569678 </span><span class="s3">,  </span><span class="s5">2.04667506</span><span class="s3">,  </span><span class="s5">3.92064689</span><span class="s3">,</span>
            <span class="s5">3.66243644</span><span class="s3">,  </span><span class="s5">3.11742725</span><span class="s1">])</span>
        <span class="s1">assert_allclose(res.params</span><span class="s3">, </span><span class="s1">params1)</span>

        <span class="s1">pen_weight_aicc = mod.select_pen_weight(method=</span><span class="s4">'aicc'</span><span class="s1">)</span>
        <span class="s1">pen_weight_gcv = mod.select_pen_weight(method=</span><span class="s4">'gcv'</span><span class="s1">)</span>
        <span class="s1">pen_weight_cv = mod.select_pen_weight(method=</span><span class="s4">'cv'</span><span class="s1">)</span>
        <span class="s1">pen_weight_bic = mod.select_pen_weight(method=</span><span class="s4">'bic'</span><span class="s1">)</span>
        <span class="s1">assert_allclose(pen_weight_gcv</span><span class="s3">, </span><span class="s1">pen_weight_aicc</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">0.1</span><span class="s1">)</span>
        <span class="s0"># regression tests:</span>
        <span class="s1">assert_allclose(pen_weight_aicc</span><span class="s3">, </span><span class="s5">4.77333984</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-4</span><span class="s1">)</span>
        <span class="s1">assert_allclose(pen_weight_gcv</span><span class="s3">,  </span><span class="s5">4.45546875</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-4</span><span class="s1">)</span>
        <span class="s1">assert_allclose(pen_weight_bic</span><span class="s3">, </span><span class="s5">9.35957031</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-4</span><span class="s1">)</span>
        <span class="s1">assert_allclose(pen_weight_cv</span><span class="s3">, </span><span class="s5">1.99277344</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-4</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">test_combine_subset_regression(self):</span>
        <span class="s0"># split sample into two, use first sample as prior for second</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">exog = self.exog</span>
        <span class="s1">nobs = len(endog)</span>

        <span class="s1">n05 = nobs // </span><span class="s5">2</span>
        <span class="s1">np.random.seed(</span><span class="s5">987125</span><span class="s1">)</span>
        <span class="s0"># shuffle to get random subsamples</span>
        <span class="s1">shuffle_idx = np.random.permutation(np.arange(nobs))</span>
        <span class="s1">ys = endog[shuffle_idx]</span>
        <span class="s1">xs = exog[shuffle_idx]</span>
        <span class="s1">k = </span><span class="s5">10</span>
        <span class="s1">res_ols0 = OLS(ys[:n05]</span><span class="s3">, </span><span class="s1">xs[:n05</span><span class="s3">, </span><span class="s1">:k]).fit()</span>
        <span class="s1">res_ols1 = OLS(ys[n05:]</span><span class="s3">, </span><span class="s1">xs[n05:</span><span class="s3">, </span><span class="s1">:k]).fit()</span>

        <span class="s1">w = res_ols1.scale / res_ols0.scale   </span><span class="s0">#1.01</span>
        <span class="s1">mod_1 = TheilGLS(ys[n05:]</span><span class="s3">, </span><span class="s1">xs[n05:</span><span class="s3">, </span><span class="s1">:k]</span><span class="s3">, </span><span class="s1">r_matrix=np.eye(k)</span><span class="s3">,</span>
                         <span class="s1">q_matrix=res_ols0.params</span><span class="s3">,</span>
                         <span class="s1">sigma_prior=w * res_ols0.cov_params())</span>
        <span class="s1">res_1p = mod_1.fit(cov_type=</span><span class="s4">'data-prior'</span><span class="s1">)</span>
        <span class="s1">res_1s = mod_1.fit(cov_type=</span><span class="s4">'sandwich'</span><span class="s1">)</span>
        <span class="s1">res_olsf = OLS(ys</span><span class="s3">, </span><span class="s1">xs[:</span><span class="s3">, </span><span class="s1">:k]).fit()</span>

        <span class="s1">assert_allclose(res_1p.params</span><span class="s3">, </span><span class="s1">res_olsf.params</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-9</span><span class="s1">)</span>
        <span class="s1">corr_fact = np.sqrt(res_1p.scale / res_olsf.scale)</span>
        <span class="s0"># corrct for differences in scale computation</span>
        <span class="s1">assert_allclose(res_1p.bse</span><span class="s3">, </span><span class="s1">res_olsf.bse * corr_fact</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-3</span><span class="s1">)</span>

        <span class="s0"># regression test, does not verify numbers</span>
        <span class="s0"># especially why are these smaller than OLS on full sample</span>
        <span class="s0"># in larger sample, nobs=600, those were close to full OLS</span>
        <span class="s1">bse1 = np.array([</span>
            <span class="s5">0.26589869</span><span class="s3">,  </span><span class="s5">0.15224812</span><span class="s3">,  </span><span class="s5">0.38407399</span><span class="s3">,  </span><span class="s5">0.75679949</span><span class="s3">,  </span><span class="s5">0.66084200</span><span class="s3">,</span>
            <span class="s5">0.54174080</span><span class="s3">,  </span><span class="s5">0.53697607</span><span class="s3">,  </span><span class="s5">0.66006377</span><span class="s3">,  </span><span class="s5">0.38228551</span><span class="s3">,  </span><span class="s5">0.53920485</span><span class="s1">])</span>
        <span class="s1">assert_allclose(res_1s.bse</span><span class="s3">, </span><span class="s1">bse1</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-7</span><span class="s1">)</span>
</pre>
</body>
</html>