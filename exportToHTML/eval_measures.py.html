<html>
<head>
<title>eval_measures.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
eval_measures.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot;some measures for evaluation of prediction, tests and model selection 
 
Created on Tue Nov 08 15:23:20 2011 
Updated on Wed Jun 03 10:42:20 2020 
 
Authors: Josef Perktold &amp; Peter Prescott 
License: BSD-3 
 
&quot;&quot;&quot;</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">statsmodels.tools.validation </span><span class="s3">import </span><span class="s1">array_like</span>


<span class="s3">def </span><span class="s1">mse(x1</span><span class="s3">, </span><span class="s1">x2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;mean squared error 
 
    Parameters 
    ---------- 
    x1, x2 : array_like 
       The performance measure depends on the difference between these two 
       arrays. 
    axis : int 
       axis along which the summary statistic is calculated 
 
    Returns 
    ------- 
    mse : ndarray or float 
       mean squared error along given axis. 
 
    Notes 
    ----- 
    If ``x1`` and ``x2`` have different shapes, then they need to broadcast. 
    This uses ``numpy.asanyarray`` to convert the input. Whether this is the 
    desired result or not depends on the array subclass, for example 
    numpy matrices will silently produce an incorrect result. 
    &quot;&quot;&quot;</span>
    <span class="s1">x1 = np.asanyarray(x1)</span>
    <span class="s1">x2 = np.asanyarray(x2)</span>
    <span class="s3">return </span><span class="s1">np.mean((x1 - x2) ** </span><span class="s4">2</span><span class="s3">, </span><span class="s1">axis=axis)</span>


<span class="s3">def </span><span class="s1">rmse(x1</span><span class="s3">, </span><span class="s1">x2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;root mean squared error 
 
    Parameters 
    ---------- 
    x1, x2 : array_like 
       The performance measure depends on the difference between these two 
       arrays. 
    axis : int 
       axis along which the summary statistic is calculated 
 
    Returns 
    ------- 
    rmse : ndarray or float 
       root mean squared error along given axis. 
 
    Notes 
    ----- 
    If ``x1`` and ``x2`` have different shapes, then they need to broadcast. 
    This uses ``numpy.asanyarray`` to convert the input. Whether this is the 
    desired result or not depends on the array subclass, for example 
    numpy matrices will silently produce an incorrect result. 
    &quot;&quot;&quot;</span>
    <span class="s1">x1 = np.asanyarray(x1)</span>
    <span class="s1">x2 = np.asanyarray(x2)</span>
    <span class="s3">return </span><span class="s1">np.sqrt(mse(x1</span><span class="s3">, </span><span class="s1">x2</span><span class="s3">, </span><span class="s1">axis=axis))</span>


<span class="s3">def </span><span class="s1">rmspe(y</span><span class="s3">, </span><span class="s1">y_hat</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">zeros=np.nan):</span>
    <span class="s2">&quot;&quot;&quot; 
    Root Mean Squared Percentage Error 
 
    Parameters 
    ---------- 
    y : array_like 
      The actual value. 
    y_hat : array_like 
       The predicted value. 
    axis : int 
       Axis along which the summary statistic is calculated 
    zeros : float 
       Value to assign to error where y is zero 
 
    Returns 
    ------- 
    rmspe : ndarray or float 
       Root Mean Squared Percentage Error along given axis. 
    &quot;&quot;&quot;</span>
    <span class="s1">y_hat = np.asarray(y_hat)</span>
    <span class="s1">y = np.asarray(y)</span>
    <span class="s1">error = y - y_hat</span>
    <span class="s1">loc = y != </span><span class="s4">0</span>
    <span class="s1">loc = loc.ravel()</span>
    <span class="s1">percentage_error = np.full_like(error</span><span class="s3">, </span><span class="s1">zeros)</span>
    <span class="s1">percentage_error.flat[loc] = error.flat[loc] / y.flat[loc]</span>
    <span class="s1">mspe = np.nanmean(percentage_error ** </span><span class="s4">2</span><span class="s3">, </span><span class="s1">axis=axis) * </span><span class="s4">100</span>
    <span class="s3">return </span><span class="s1">np.sqrt(mspe)</span>


<span class="s3">def </span><span class="s1">maxabs(x1</span><span class="s3">, </span><span class="s1">x2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;maximum absolute error 
 
    Parameters 
    ---------- 
    x1, x2 : array_like 
       The performance measure depends on the difference between these two 
       arrays. 
    axis : int 
       axis along which the summary statistic is calculated 
 
    Returns 
    ------- 
    maxabs : ndarray or float 
       maximum absolute difference along given axis. 
 
    Notes 
    ----- 
    If ``x1`` and ``x2`` have different shapes, then they need to broadcast. 
    This uses ``numpy.asanyarray`` to convert the input. Whether this is the 
    desired result or not depends on the array subclass. 
    &quot;&quot;&quot;</span>
    <span class="s1">x1 = np.asanyarray(x1)</span>
    <span class="s1">x2 = np.asanyarray(x2)</span>
    <span class="s3">return </span><span class="s1">np.max(np.abs(x1 - x2)</span><span class="s3">, </span><span class="s1">axis=axis)</span>


<span class="s3">def </span><span class="s1">meanabs(x1</span><span class="s3">, </span><span class="s1">x2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;mean absolute error 
 
    Parameters 
    ---------- 
    x1, x2 : array_like 
       The performance measure depends on the difference between these two 
       arrays. 
    axis : int 
       axis along which the summary statistic is calculated 
 
    Returns 
    ------- 
    meanabs : ndarray or float 
       mean absolute difference along given axis. 
 
    Notes 
    ----- 
    If ``x1`` and ``x2`` have different shapes, then they need to broadcast. 
    This uses ``numpy.asanyarray`` to convert the input. Whether this is the 
    desired result or not depends on the array subclass. 
    &quot;&quot;&quot;</span>
    <span class="s1">x1 = np.asanyarray(x1)</span>
    <span class="s1">x2 = np.asanyarray(x2)</span>
    <span class="s3">return </span><span class="s1">np.mean(np.abs(x1 - x2)</span><span class="s3">, </span><span class="s1">axis=axis)</span>


<span class="s3">def </span><span class="s1">medianabs(x1</span><span class="s3">, </span><span class="s1">x2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;median absolute error 
 
    Parameters 
    ---------- 
    x1, x2 : array_like 
       The performance measure depends on the difference between these two 
       arrays. 
    axis : int 
       axis along which the summary statistic is calculated 
 
    Returns 
    ------- 
    medianabs : ndarray or float 
       median absolute difference along given axis. 
 
    Notes 
    ----- 
    If ``x1`` and ``x2`` have different shapes, then they need to broadcast. 
    This uses ``numpy.asanyarray`` to convert the input. Whether this is the 
    desired result or not depends on the array subclass. 
    &quot;&quot;&quot;</span>
    <span class="s1">x1 = np.asanyarray(x1)</span>
    <span class="s1">x2 = np.asanyarray(x2)</span>
    <span class="s3">return </span><span class="s1">np.median(np.abs(x1 - x2)</span><span class="s3">, </span><span class="s1">axis=axis)</span>


<span class="s3">def </span><span class="s1">bias(x1</span><span class="s3">, </span><span class="s1">x2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;bias, mean error 
 
    Parameters 
    ---------- 
    x1, x2 : array_like 
       The performance measure depends on the difference between these two 
       arrays. 
    axis : int 
       axis along which the summary statistic is calculated 
 
    Returns 
    ------- 
    bias : ndarray or float 
       bias, or mean difference along given axis. 
 
    Notes 
    ----- 
    If ``x1`` and ``x2`` have different shapes, then they need to broadcast. 
    This uses ``numpy.asanyarray`` to convert the input. Whether this is the 
    desired result or not depends on the array subclass. 
    &quot;&quot;&quot;</span>
    <span class="s1">x1 = np.asanyarray(x1)</span>
    <span class="s1">x2 = np.asanyarray(x2)</span>
    <span class="s3">return </span><span class="s1">np.mean(x1 - x2</span><span class="s3">, </span><span class="s1">axis=axis)</span>


<span class="s3">def </span><span class="s1">medianbias(x1</span><span class="s3">, </span><span class="s1">x2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;median bias, median error 
 
    Parameters 
    ---------- 
    x1, x2 : array_like 
       The performance measure depends on the difference between these two 
       arrays. 
    axis : int 
       axis along which the summary statistic is calculated 
 
    Returns 
    ------- 
    medianbias : ndarray or float 
       median bias, or median difference along given axis. 
 
    Notes 
    ----- 
    If ``x1`` and ``x2`` have different shapes, then they need to broadcast. 
    This uses ``numpy.asanyarray`` to convert the input. Whether this is the 
    desired result or not depends on the array subclass. 
    &quot;&quot;&quot;</span>
    <span class="s1">x1 = np.asanyarray(x1)</span>
    <span class="s1">x2 = np.asanyarray(x2)</span>
    <span class="s3">return </span><span class="s1">np.median(x1 - x2</span><span class="s3">, </span><span class="s1">axis=axis)</span>


<span class="s3">def </span><span class="s1">vare(x1</span><span class="s3">, </span><span class="s1">x2</span><span class="s3">, </span><span class="s1">ddof=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;variance of error 
 
    Parameters 
    ---------- 
    x1, x2 : array_like 
       The performance measure depends on the difference between these two 
       arrays. 
    axis : int 
       axis along which the summary statistic is calculated 
 
    Returns 
    ------- 
    vare : ndarray or float 
       variance of difference along given axis. 
 
    Notes 
    ----- 
    If ``x1`` and ``x2`` have different shapes, then they need to broadcast. 
    This uses ``numpy.asanyarray`` to convert the input. Whether this is the 
    desired result or not depends on the array subclass. 
    &quot;&quot;&quot;</span>
    <span class="s1">x1 = np.asanyarray(x1)</span>
    <span class="s1">x2 = np.asanyarray(x2)</span>
    <span class="s3">return </span><span class="s1">np.var(x1 - x2</span><span class="s3">, </span><span class="s1">ddof=ddof</span><span class="s3">, </span><span class="s1">axis=axis)</span>


<span class="s3">def </span><span class="s1">stde(x1</span><span class="s3">, </span><span class="s1">x2</span><span class="s3">, </span><span class="s1">ddof=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;standard deviation of error 
 
    Parameters 
    ---------- 
    x1, x2 : array_like 
       The performance measure depends on the difference between these two 
       arrays. 
    axis : int 
       axis along which the summary statistic is calculated 
 
    Returns 
    ------- 
    stde : ndarray or float 
       standard deviation of difference along given axis. 
 
    Notes 
    ----- 
    If ``x1`` and ``x2`` have different shapes, then they need to broadcast. 
    This uses ``numpy.asanyarray`` to convert the input. Whether this is the 
    desired result or not depends on the array subclass. 
    &quot;&quot;&quot;</span>
    <span class="s1">x1 = np.asanyarray(x1)</span>
    <span class="s1">x2 = np.asanyarray(x2)</span>
    <span class="s3">return </span><span class="s1">np.std(x1 - x2</span><span class="s3">, </span><span class="s1">ddof=ddof</span><span class="s3">, </span><span class="s1">axis=axis)</span>


<span class="s3">def </span><span class="s1">iqr(x1</span><span class="s3">, </span><span class="s1">x2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot; 
    Interquartile range of error 
 
    Parameters 
    ---------- 
    x1 : array_like 
       One of the inputs into the IQR calculation. 
    x2 : array_like 
       The other input into the IQR calculation. 
    axis : {None, int} 
       axis along which the summary statistic is calculated 
 
    Returns 
    ------- 
    irq : {float, ndarray} 
       Interquartile range along given axis. 
 
    Notes 
    ----- 
    If ``x1`` and ``x2`` have different shapes, then they must broadcast. 
    &quot;&quot;&quot;</span>
    <span class="s1">x1 = array_like(x1</span><span class="s3">, </span><span class="s5">&quot;x1&quot;</span><span class="s3">, </span><span class="s1">dtype=</span><span class="s3">None, </span><span class="s1">ndim=</span><span class="s3">None</span><span class="s1">)</span>
    <span class="s1">x2 = array_like(x2</span><span class="s3">, </span><span class="s5">&quot;x1&quot;</span><span class="s3">, </span><span class="s1">dtype=</span><span class="s3">None, </span><span class="s1">ndim=</span><span class="s3">None</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">axis </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">x1 = x1.ravel()</span>
        <span class="s1">x2 = x2.ravel()</span>
        <span class="s1">axis = </span><span class="s4">0</span>
    <span class="s1">xdiff = np.sort(x1 - x2</span><span class="s3">, </span><span class="s1">axis=axis)</span>
    <span class="s1">nobs = x1.shape[axis]</span>
    <span class="s1">idx = np.round((nobs - </span><span class="s4">1</span><span class="s1">) * np.array([</span><span class="s4">0.25</span><span class="s3">, </span><span class="s4">0.75</span><span class="s1">])).astype(int)</span>
    <span class="s1">sl = [slice(</span><span class="s3">None</span><span class="s1">)] * xdiff.ndim</span>
    <span class="s1">sl[axis] = idx</span>
    <span class="s1">iqr = np.diff(xdiff[tuple(sl)]</span><span class="s3">, </span><span class="s1">axis=axis)</span>
    <span class="s1">iqr = np.squeeze(iqr)  </span><span class="s0"># drop reduced dimension</span>
    <span class="s3">return </span><span class="s1">iqr</span>


<span class="s0"># Information Criteria</span>
<span class="s0"># ---------------------</span>


<span class="s3">def </span><span class="s1">aic(llf</span><span class="s3">, </span><span class="s1">nobs</span><span class="s3">, </span><span class="s1">df_modelwc):</span>
    <span class="s2">&quot;&quot;&quot; 
    Akaike information criterion 
 
    Parameters 
    ---------- 
    llf : {float, array_like} 
        value of the loglikelihood 
    nobs : int 
        number of observations 
    df_modelwc : int 
        number of parameters including constant 
 
    Returns 
    ------- 
    aic : float 
        information criterion 
 
    References 
    ---------- 
    https://en.wikipedia.org/wiki/Akaike_information_criterion 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">-</span><span class="s4">2.0 </span><span class="s1">* llf + </span><span class="s4">2.0 </span><span class="s1">* df_modelwc</span>


<span class="s3">def </span><span class="s1">aicc(llf</span><span class="s3">, </span><span class="s1">nobs</span><span class="s3">, </span><span class="s1">df_modelwc):</span>
    <span class="s2">&quot;&quot;&quot; 
    Akaike information criterion (AIC) with small sample correction 
 
    Parameters 
    ---------- 
    llf : {float, array_like} 
        value of the loglikelihood 
    nobs : int 
        number of observations 
    df_modelwc : int 
        number of parameters including constant 
 
    Returns 
    ------- 
    aicc : float 
        information criterion 
 
    References 
    ---------- 
    https://en.wikipedia.org/wiki/Akaike_information_criterion#AICc 
 
    Notes 
    ----- 
    Returns +inf if the effective degrees of freedom, defined as 
    ``nobs - df_modelwc - 1.0``, is &lt;= 0. 
    &quot;&quot;&quot;</span>
    <span class="s1">dof_eff = nobs - df_modelwc - </span><span class="s4">1.0</span>
    <span class="s3">if </span><span class="s1">dof_eff &gt; </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">-</span><span class="s4">2.0 </span><span class="s1">* llf + </span><span class="s4">2.0 </span><span class="s1">* df_modelwc * nobs / dof_eff</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">np.inf</span>


<span class="s3">def </span><span class="s1">bic(llf</span><span class="s3">, </span><span class="s1">nobs</span><span class="s3">, </span><span class="s1">df_modelwc):</span>
    <span class="s2">&quot;&quot;&quot; 
    Bayesian information criterion (BIC) or Schwarz criterion 
 
    Parameters 
    ---------- 
    llf : {float, array_like} 
        value of the loglikelihood 
    nobs : int 
        number of observations 
    df_modelwc : int 
        number of parameters including constant 
 
    Returns 
    ------- 
    bic : float 
        information criterion 
 
    References 
    ---------- 
    https://en.wikipedia.org/wiki/Bayesian_information_criterion 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">-</span><span class="s4">2.0 </span><span class="s1">* llf + np.log(nobs) * df_modelwc</span>


<span class="s3">def </span><span class="s1">hqic(llf</span><span class="s3">, </span><span class="s1">nobs</span><span class="s3">, </span><span class="s1">df_modelwc):</span>
    <span class="s2">&quot;&quot;&quot; 
    Hannan-Quinn information criterion (HQC) 
 
    Parameters 
    ---------- 
    llf : {float, array_like} 
        value of the loglikelihood 
    nobs : int 
        number of observations 
    df_modelwc : int 
        number of parameters including constant 
 
    Returns 
    ------- 
    hqic : float 
        information criterion 
 
    References 
    ---------- 
    Wikipedia does not say much 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">-</span><span class="s4">2.0 </span><span class="s1">* llf + </span><span class="s4">2 </span><span class="s1">* np.log(np.log(nobs)) * df_modelwc</span>


<span class="s0"># IC based on residual sigma</span>


<span class="s3">def </span><span class="s1">aic_sigma(sigma2</span><span class="s3">, </span><span class="s1">nobs</span><span class="s3">, </span><span class="s1">df_modelwc</span><span class="s3">, </span><span class="s1">islog=</span><span class="s3">False</span><span class="s1">):</span>
    <span class="s2">r&quot;&quot;&quot; 
    Akaike information criterion 
 
    Parameters 
    ---------- 
    sigma2 : float 
        estimate of the residual variance or determinant of Sigma_hat in the 
        multivariate case. If islog is true, then it is assumed that sigma 
        is already log-ed, for example logdetSigma. 
    nobs : int 
        number of observations 
    df_modelwc : int 
        number of parameters including constant 
 
    Returns 
    ------- 
    aic : float 
        information criterion 
 
    Notes 
    ----- 
    A constant has been dropped in comparison to the loglikelihood base 
    information criteria. The information criteria should be used to compare 
    only comparable models. 
 
    For example, AIC is defined in terms of the loglikelihood as 
 
    :math:`-2 llf + 2 k` 
 
    in terms of :math:`\hat{\sigma}^2` 
 
    :math:`log(\hat{\sigma}^2) + 2 k / n` 
 
    in terms of the determinant of :math:`\hat{\Sigma}` 
 
    :math:`log(\|\hat{\Sigma}\|) + 2 k / n` 
 
    Note: In our definition we do not divide by n in the log-likelihood 
    version. 
 
    TODO: Latex math 
 
    reference for example lecture notes by Herman Bierens 
 
    See Also 
    -------- 
 
    References 
    ---------- 
    https://en.wikipedia.org/wiki/Akaike_information_criterion 
    &quot;&quot;&quot;</span>
    <span class="s3">if not </span><span class="s1">islog:</span>
        <span class="s1">sigma2 = np.log(sigma2)</span>
    <span class="s3">return </span><span class="s1">sigma2 + aic(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">nobs</span><span class="s3">, </span><span class="s1">df_modelwc) / nobs</span>


<span class="s3">def </span><span class="s1">aicc_sigma(sigma2</span><span class="s3">, </span><span class="s1">nobs</span><span class="s3">, </span><span class="s1">df_modelwc</span><span class="s3">, </span><span class="s1">islog=</span><span class="s3">False</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot; 
    Akaike information criterion (AIC) with small sample correction 
 
    Parameters 
    ---------- 
    sigma2 : float 
        estimate of the residual variance or determinant of Sigma_hat in the 
        multivariate case. If islog is true, then it is assumed that sigma 
        is already log-ed, for example logdetSigma. 
    nobs : int 
        number of observations 
    df_modelwc : int 
        number of parameters including constant 
 
    Returns 
    ------- 
    aicc : float 
        information criterion 
 
    Notes 
    ----- 
    A constant has been dropped in comparison to the loglikelihood base 
    information criteria. These should be used to compare for comparable 
    models. 
 
    References 
    ---------- 
    https://en.wikipedia.org/wiki/Akaike_information_criterion#AICc 
    &quot;&quot;&quot;</span>
    <span class="s3">if not </span><span class="s1">islog:</span>
        <span class="s1">sigma2 = np.log(sigma2)</span>
    <span class="s3">return </span><span class="s1">sigma2 + aicc(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">nobs</span><span class="s3">, </span><span class="s1">df_modelwc) / nobs</span>


<span class="s3">def </span><span class="s1">bic_sigma(sigma2</span><span class="s3">, </span><span class="s1">nobs</span><span class="s3">, </span><span class="s1">df_modelwc</span><span class="s3">, </span><span class="s1">islog=</span><span class="s3">False</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;Bayesian information criterion (BIC) or Schwarz criterion 
 
    Parameters 
    ---------- 
    sigma2 : float 
        estimate of the residual variance or determinant of Sigma_hat in the 
        multivariate case. If islog is true, then it is assumed that sigma 
        is already log-ed, for example logdetSigma. 
    nobs : int 
        number of observations 
    df_modelwc : int 
        number of parameters including constant 
 
    Returns 
    ------- 
    bic : float 
        information criterion 
 
    Notes 
    ----- 
    A constant has been dropped in comparison to the loglikelihood base 
    information criteria. These should be used to compare for comparable 
    models. 
 
    References 
    ---------- 
    https://en.wikipedia.org/wiki/Bayesian_information_criterion 
    &quot;&quot;&quot;</span>
    <span class="s3">if not </span><span class="s1">islog:</span>
        <span class="s1">sigma2 = np.log(sigma2)</span>
    <span class="s3">return </span><span class="s1">sigma2 + bic(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">nobs</span><span class="s3">, </span><span class="s1">df_modelwc) / nobs</span>


<span class="s3">def </span><span class="s1">hqic_sigma(sigma2</span><span class="s3">, </span><span class="s1">nobs</span><span class="s3">, </span><span class="s1">df_modelwc</span><span class="s3">, </span><span class="s1">islog=</span><span class="s3">False</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;Hannan-Quinn information criterion (HQC) 
 
    Parameters 
    ---------- 
    sigma2 : float 
        estimate of the residual variance or determinant of Sigma_hat in the 
        multivariate case. If islog is true, then it is assumed that sigma 
        is already log-ed, for example logdetSigma. 
    nobs : int 
        number of observations 
    df_modelwc : int 
        number of parameters including constant 
 
    Returns 
    ------- 
    hqic : float 
        information criterion 
 
    Notes 
    ----- 
    A constant has been dropped in comparison to the loglikelihood base 
    information criteria. These should be used to compare for comparable 
    models. 
 
    References 
    ---------- 
    xxx 
    &quot;&quot;&quot;</span>
    <span class="s3">if not </span><span class="s1">islog:</span>
        <span class="s1">sigma2 = np.log(sigma2)</span>
    <span class="s3">return </span><span class="s1">sigma2 + hqic(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">nobs</span><span class="s3">, </span><span class="s1">df_modelwc) / nobs</span>


<span class="s0"># from var_model.py, VAR only? separates neqs and k_vars per equation</span>
<span class="s0"># def fpe_sigma():</span>
<span class="s0">#     ((nobs + self.df_model) / self.df_resid) ** neqs * np.exp(ld)</span>


<span class="s1">__all__ = [</span>
    <span class="s1">maxabs</span><span class="s3">,</span>
    <span class="s1">meanabs</span><span class="s3">,</span>
    <span class="s1">medianabs</span><span class="s3">,</span>
    <span class="s1">medianbias</span><span class="s3">,</span>
    <span class="s1">mse</span><span class="s3">,</span>
    <span class="s1">rmse</span><span class="s3">,</span>
    <span class="s1">rmspe</span><span class="s3">,</span>
    <span class="s1">stde</span><span class="s3">,</span>
    <span class="s1">vare</span><span class="s3">,</span>
    <span class="s1">aic</span><span class="s3">,</span>
    <span class="s1">aic_sigma</span><span class="s3">,</span>
    <span class="s1">aicc</span><span class="s3">,</span>
    <span class="s1">aicc_sigma</span><span class="s3">,</span>
    <span class="s1">bias</span><span class="s3">,</span>
    <span class="s1">bic</span><span class="s3">,</span>
    <span class="s1">bic_sigma</span><span class="s3">,</span>
    <span class="s1">hqic</span><span class="s3">,</span>
    <span class="s1">hqic_sigma</span><span class="s3">,</span>
    <span class="s1">iqr</span><span class="s3">,</span>
<span class="s1">]</span>
</pre>
</body>
</html>