<html>
<head>
<title>test_fastica.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_fastica.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Test the fastica algorithm. 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">itertools</span>
<span class="s2">import </span><span class="s1">os</span>
<span class="s2">import </span><span class="s1">warnings</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">stats</span>

<span class="s2">from </span><span class="s1">sklearn.decomposition </span><span class="s2">import </span><span class="s1">PCA</span><span class="s2">, </span><span class="s1">FastICA</span><span class="s2">, </span><span class="s1">fastica</span>
<span class="s2">from </span><span class="s1">sklearn.decomposition._fastica </span><span class="s2">import </span><span class="s1">_gs_decorrelation</span>
<span class="s2">from </span><span class="s1">sklearn.exceptions </span><span class="s2">import </span><span class="s1">ConvergenceWarning</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">assert_allclose</span>


<span class="s2">def </span><span class="s1">center_and_norm(x</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s3">1</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Centers and norms x **in place** 
 
    Parameters 
    ----------- 
    x: ndarray 
        Array with an axis of observations (statistical units) measured on 
        random variables. 
    axis: int, optional 
        Axis along which the mean and variance are calculated. 
    &quot;&quot;&quot;</span>
    <span class="s1">x = np.rollaxis(x</span><span class="s2">, </span><span class="s1">axis)</span>
    <span class="s1">x -= x.mean(axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">x /= x.std(axis=</span><span class="s3">0</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_gs():</span>
    <span class="s4"># Test gram schmidt orthonormalization</span>
    <span class="s4"># generate a random orthogonal  matrix</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">W</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = np.linalg.svd(rng.randn(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">10</span><span class="s1">))</span>
    <span class="s1">w = rng.randn(</span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">_gs_decorrelation(w</span><span class="s2">, </span><span class="s1">W</span><span class="s2">, </span><span class="s3">10</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">(w**</span><span class="s3">2</span><span class="s1">).sum() &lt; </span><span class="s3">1.0e-10</span>
    <span class="s1">w = rng.randn(</span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">u = _gs_decorrelation(w</span><span class="s2">, </span><span class="s1">W</span><span class="s2">, </span><span class="s3">5</span><span class="s1">)</span>
    <span class="s1">tmp = np.dot(u</span><span class="s2">, </span><span class="s1">W.T)</span>
    <span class="s2">assert </span><span class="s1">(tmp[:</span><span class="s3">5</span><span class="s1">] ** </span><span class="s3">2</span><span class="s1">).sum() &lt; </span><span class="s3">1.0e-10</span>


<span class="s2">def </span><span class="s1">test_fastica_attributes_dtypes(global_dtype):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.random_sample((</span><span class="s3">100</span><span class="s2">, </span><span class="s3">10</span><span class="s1">)).astype(global_dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">fica = FastICA(</span>
        <span class="s1">n_components=</span><span class="s3">5</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s3">1000</span><span class="s2">, </span><span class="s1">whiten=</span><span class="s5">&quot;unit-variance&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span>
    <span class="s1">).fit(X)</span>
    <span class="s2">assert </span><span class="s1">fica.components_.dtype == global_dtype</span>
    <span class="s2">assert </span><span class="s1">fica.mixing_.dtype == global_dtype</span>
    <span class="s2">assert </span><span class="s1">fica.mean_.dtype == global_dtype</span>
    <span class="s2">assert </span><span class="s1">fica.whitening_.dtype == global_dtype</span>


<span class="s2">def </span><span class="s1">test_fastica_return_dtypes(global_dtype):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.random_sample((</span><span class="s3">100</span><span class="s2">, </span><span class="s3">10</span><span class="s1">)).astype(global_dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">k_</span><span class="s2">, </span><span class="s1">mixing_</span><span class="s2">, </span><span class="s1">s_ = fastica(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s3">1000</span><span class="s2">, </span><span class="s1">whiten=</span><span class="s5">&quot;unit-variance&quot;</span><span class="s2">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">k_.dtype == global_dtype</span>
    <span class="s2">assert </span><span class="s1">mixing_.dtype == global_dtype</span>
    <span class="s2">assert </span><span class="s1">s_.dtype == global_dtype</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;add_noise&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_fastica_simple(add_noise</span><span class="s2">, </span><span class="s1">global_random_seed</span><span class="s2">, </span><span class="s1">global_dtype):</span>
    <span class="s2">if </span><span class="s1">(</span>
        <span class="s1">global_random_seed == </span><span class="s3">20</span>
        <span class="s2">and </span><span class="s1">global_dtype == np.float32</span>
        <span class="s2">and not </span><span class="s1">add_noise</span>
        <span class="s2">and </span><span class="s1">os.getenv(</span><span class="s5">&quot;DISTRIB&quot;</span><span class="s1">) == </span><span class="s5">&quot;ubuntu&quot;</span>
    <span class="s1">):</span>
        <span class="s1">pytest.xfail(</span>
            <span class="s5">&quot;FastICA instability with Ubuntu Atlas build with float32 &quot;</span>
            <span class="s5">&quot;global_dtype. For more details, see &quot;</span>
            <span class="s5">&quot;https://github.com/scikit-learn/scikit-learn/issues/24131#issuecomment-1208091119&quot;  </span><span class="s4"># noqa</span>
        <span class="s1">)</span>

    <span class="s4"># Test the FastICA algorithm on very simple data.</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">n_samples = </span><span class="s3">1000</span>
    <span class="s4"># Generate two sources:</span>
    <span class="s1">s1 = (</span><span class="s3">2 </span><span class="s1">* np.sin(np.linspace(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">100</span><span class="s2">, </span><span class="s1">n_samples)) &gt; </span><span class="s3">0</span><span class="s1">) - </span><span class="s3">1</span>
    <span class="s1">s2 = stats.t.rvs(</span><span class="s3">1</span><span class="s2">, </span><span class="s1">size=n_samples</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s1">s = np.c_[s1</span><span class="s2">, </span><span class="s1">s2].T</span>
    <span class="s1">center_and_norm(s)</span>
    <span class="s1">s = s.astype(global_dtype)</span>
    <span class="s1">s1</span><span class="s2">, </span><span class="s1">s2 = s</span>

    <span class="s4"># Mixing angle</span>
    <span class="s1">phi = </span><span class="s3">0.6</span>
    <span class="s1">mixing = np.array([[np.cos(phi)</span><span class="s2">, </span><span class="s1">np.sin(phi)]</span><span class="s2">, </span><span class="s1">[np.sin(phi)</span><span class="s2">, </span><span class="s1">-np.cos(phi)]])</span>
    <span class="s1">mixing = mixing.astype(global_dtype)</span>
    <span class="s1">m = np.dot(mixing</span><span class="s2">, </span><span class="s1">s)</span>

    <span class="s2">if </span><span class="s1">add_noise:</span>
        <span class="s1">m += </span><span class="s3">0.1 </span><span class="s1">* rng.randn(</span><span class="s3">2</span><span class="s2">, </span><span class="s3">1000</span><span class="s1">)</span>

    <span class="s1">center_and_norm(m)</span>

    <span class="s4"># function as fun arg</span>
    <span class="s2">def </span><span class="s1">g_test(x):</span>
        <span class="s2">return </span><span class="s1">x**</span><span class="s3">3</span><span class="s2">, </span><span class="s1">(</span><span class="s3">3 </span><span class="s1">* x**</span><span class="s3">2</span><span class="s1">).mean(axis=-</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">algos = [</span><span class="s5">&quot;parallel&quot;</span><span class="s2">, </span><span class="s5">&quot;deflation&quot;</span><span class="s1">]</span>
    <span class="s1">nls = [</span><span class="s5">&quot;logcosh&quot;</span><span class="s2">, </span><span class="s5">&quot;exp&quot;</span><span class="s2">, </span><span class="s5">&quot;cube&quot;</span><span class="s2">, </span><span class="s1">g_test]</span>
    <span class="s1">whitening = [</span><span class="s5">&quot;arbitrary-variance&quot;</span><span class="s2">, </span><span class="s5">&quot;unit-variance&quot;</span><span class="s2">, False</span><span class="s1">]</span>
    <span class="s2">for </span><span class="s1">algo</span><span class="s2">, </span><span class="s1">nl</span><span class="s2">, </span><span class="s1">whiten </span><span class="s2">in </span><span class="s1">itertools.product(algos</span><span class="s2">, </span><span class="s1">nls</span><span class="s2">, </span><span class="s1">whitening):</span>
        <span class="s2">if </span><span class="s1">whiten:</span>
            <span class="s1">k_</span><span class="s2">, </span><span class="s1">mixing_</span><span class="s2">, </span><span class="s1">s_ = fastica(</span>
                <span class="s1">m.T</span><span class="s2">, </span><span class="s1">fun=nl</span><span class="s2">, </span><span class="s1">whiten=whiten</span><span class="s2">, </span><span class="s1">algorithm=algo</span><span class="s2">, </span><span class="s1">random_state=rng</span>
            <span class="s1">)</span>
            <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
                <span class="s1">fastica(m.T</span><span class="s2">, </span><span class="s1">fun=np.tanh</span><span class="s2">, </span><span class="s1">whiten=whiten</span><span class="s2">, </span><span class="s1">algorithm=algo)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">pca = PCA(n_components=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">whiten=</span><span class="s2">True, </span><span class="s1">random_state=rng)</span>
            <span class="s1">X = pca.fit_transform(m.T)</span>
            <span class="s1">k_</span><span class="s2">, </span><span class="s1">mixing_</span><span class="s2">, </span><span class="s1">s_ = fastica(</span>
                <span class="s1">X</span><span class="s2">, </span><span class="s1">fun=nl</span><span class="s2">, </span><span class="s1">algorithm=algo</span><span class="s2">, </span><span class="s1">whiten=</span><span class="s2">False, </span><span class="s1">random_state=rng</span>
            <span class="s1">)</span>
            <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
                <span class="s1">fastica(X</span><span class="s2">, </span><span class="s1">fun=np.tanh</span><span class="s2">, </span><span class="s1">algorithm=algo)</span>
        <span class="s1">s_ = s_.T</span>
        <span class="s4"># Check that the mixing model described in the docstring holds:</span>
        <span class="s2">if </span><span class="s1">whiten:</span>
            <span class="s4"># XXX: exact reconstruction to standard relative tolerance is not</span>
            <span class="s4"># possible. This is probably expected when add_noise is True but we</span>
            <span class="s4"># also need a non-trivial atol in float32 when add_noise is False.</span>
            <span class="s4">#</span>
            <span class="s4"># Note that the 2 sources are non-Gaussian in this test.</span>
            <span class="s1">atol = </span><span class="s3">1e-5 </span><span class="s2">if </span><span class="s1">global_dtype == np.float32 </span><span class="s2">else </span><span class="s3">0</span>
            <span class="s1">assert_allclose(np.dot(np.dot(mixing_</span><span class="s2">, </span><span class="s1">k_)</span><span class="s2">, </span><span class="s1">m)</span><span class="s2">, </span><span class="s1">s_</span><span class="s2">, </span><span class="s1">atol=atol)</span>

        <span class="s1">center_and_norm(s_)</span>
        <span class="s1">s1_</span><span class="s2">, </span><span class="s1">s2_ = s_</span>
        <span class="s4"># Check to see if the sources have been estimated</span>
        <span class="s4"># in the wrong order</span>
        <span class="s2">if </span><span class="s1">abs(np.dot(s1_</span><span class="s2">, </span><span class="s1">s2)) &gt; abs(np.dot(s1_</span><span class="s2">, </span><span class="s1">s1)):</span>
            <span class="s1">s2_</span><span class="s2">, </span><span class="s1">s1_ = s_</span>
        <span class="s1">s1_ *= np.sign(np.dot(s1_</span><span class="s2">, </span><span class="s1">s1))</span>
        <span class="s1">s2_ *= np.sign(np.dot(s2_</span><span class="s2">, </span><span class="s1">s2))</span>

        <span class="s4"># Check that we have estimated the original sources</span>
        <span class="s2">if not </span><span class="s1">add_noise:</span>
            <span class="s1">assert_allclose(np.dot(s1_</span><span class="s2">, </span><span class="s1">s1) / n_samples</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">1e-2</span><span class="s1">)</span>
            <span class="s1">assert_allclose(np.dot(s2_</span><span class="s2">, </span><span class="s1">s2) / n_samples</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">1e-2</span><span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">assert_allclose(np.dot(s1_</span><span class="s2">, </span><span class="s1">s1) / n_samples</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">1e-1</span><span class="s1">)</span>
            <span class="s1">assert_allclose(np.dot(s2_</span><span class="s2">, </span><span class="s1">s2) / n_samples</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">1e-1</span><span class="s1">)</span>

    <span class="s4"># Test FastICA class</span>
    <span class="s1">_</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">sources_fun = fastica(</span>
        <span class="s1">m.T</span><span class="s2">, </span><span class="s1">fun=nl</span><span class="s2">, </span><span class="s1">algorithm=algo</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>
    <span class="s1">ica = FastICA(fun=nl</span><span class="s2">, </span><span class="s1">algorithm=algo</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s1">sources = ica.fit_transform(m.T)</span>
    <span class="s2">assert </span><span class="s1">ica.components_.shape == (</span><span class="s3">2</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">sources.shape == (</span><span class="s3">1000</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span>

    <span class="s1">assert_allclose(sources_fun</span><span class="s2">, </span><span class="s1">sources)</span>
    <span class="s4"># Set atol to account for the different magnitudes of the elements in sources</span>
    <span class="s4"># (from 1e-4 to 1e1).</span>
    <span class="s1">atol = np.max(np.abs(sources)) * (</span><span class="s3">1e-5 </span><span class="s2">if </span><span class="s1">global_dtype == np.float32 </span><span class="s2">else </span><span class="s3">1e-7</span><span class="s1">)</span>
    <span class="s1">assert_allclose(sources</span><span class="s2">, </span><span class="s1">ica.transform(m.T)</span><span class="s2">, </span><span class="s1">atol=atol)</span>

    <span class="s2">assert </span><span class="s1">ica.mixing_.shape == (</span><span class="s3">2</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span>

    <span class="s1">ica = FastICA(fun=np.tanh</span><span class="s2">, </span><span class="s1">algorithm=algo)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">ica.fit(m.T)</span>


<span class="s2">def </span><span class="s1">test_fastica_nowhiten():</span>
    <span class="s1">m = [[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]]</span>

    <span class="s4"># test for issue #697</span>
    <span class="s1">ica = FastICA(n_components=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">whiten=</span><span class="s2">False, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">warn_msg = </span><span class="s5">&quot;Ignoring n_components with whiten=False.&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s2">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">ica.fit(m)</span>
    <span class="s2">assert </span><span class="s1">hasattr(ica</span><span class="s2">, </span><span class="s5">&quot;mixing_&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_fastica_convergence_fail():</span>
    <span class="s4"># Test the FastICA algorithm on very simple data</span>
    <span class="s4"># (see test_non_square_fastica).</span>
    <span class="s4"># Ensure a ConvergenceWarning raised if the tolerance is sufficiently low.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">n_samples = </span><span class="s3">1000</span>
    <span class="s4"># Generate two sources:</span>
    <span class="s1">t = np.linspace(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">100</span><span class="s2">, </span><span class="s1">n_samples)</span>
    <span class="s1">s1 = np.sin(t)</span>
    <span class="s1">s2 = np.ceil(np.sin(np.pi * t))</span>
    <span class="s1">s = np.c_[s1</span><span class="s2">, </span><span class="s1">s2].T</span>
    <span class="s1">center_and_norm(s)</span>

    <span class="s4"># Mixing matrix</span>
    <span class="s1">mixing = rng.randn(</span><span class="s3">6</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">m = np.dot(mixing</span><span class="s2">, </span><span class="s1">s)</span>

    <span class="s4"># Do fastICA with tolerance 0. to ensure failing convergence</span>
    <span class="s1">warn_msg = (</span>
        <span class="s5">&quot;FastICA did not converge. Consider increasing tolerance &quot;</span>
        <span class="s5">&quot;or the maximum number of iterations.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s2">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">ica = FastICA(</span>
            <span class="s1">algorithm=</span><span class="s5">&quot;parallel&quot;</span><span class="s2">, </span><span class="s1">n_components=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">random_state=rng</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">tol=</span><span class="s3">0.0</span>
        <span class="s1">)</span>
        <span class="s1">ica.fit(m.T)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;add_noise&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_non_square_fastica(add_noise):</span>
    <span class="s4"># Test the FastICA algorithm on very simple data.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">n_samples = </span><span class="s3">1000</span>
    <span class="s4"># Generate two sources:</span>
    <span class="s1">t = np.linspace(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">100</span><span class="s2">, </span><span class="s1">n_samples)</span>
    <span class="s1">s1 = np.sin(t)</span>
    <span class="s1">s2 = np.ceil(np.sin(np.pi * t))</span>
    <span class="s1">s = np.c_[s1</span><span class="s2">, </span><span class="s1">s2].T</span>
    <span class="s1">center_and_norm(s)</span>
    <span class="s1">s1</span><span class="s2">, </span><span class="s1">s2 = s</span>

    <span class="s4"># Mixing matrix</span>
    <span class="s1">mixing = rng.randn(</span><span class="s3">6</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">m = np.dot(mixing</span><span class="s2">, </span><span class="s1">s)</span>

    <span class="s2">if </span><span class="s1">add_noise:</span>
        <span class="s1">m += </span><span class="s3">0.1 </span><span class="s1">* rng.randn(</span><span class="s3">6</span><span class="s2">, </span><span class="s1">n_samples)</span>

    <span class="s1">center_and_norm(m)</span>

    <span class="s1">k_</span><span class="s2">, </span><span class="s1">mixing_</span><span class="s2">, </span><span class="s1">s_ = fastica(</span>
        <span class="s1">m.T</span><span class="s2">, </span><span class="s1">n_components=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">whiten=</span><span class="s5">&quot;unit-variance&quot;</span><span class="s2">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">s_ = s_.T</span>

    <span class="s4"># Check that the mixing model described in the docstring holds:</span>
    <span class="s1">assert_allclose(s_</span><span class="s2">, </span><span class="s1">np.dot(np.dot(mixing_</span><span class="s2">, </span><span class="s1">k_)</span><span class="s2">, </span><span class="s1">m))</span>

    <span class="s1">center_and_norm(s_)</span>
    <span class="s1">s1_</span><span class="s2">, </span><span class="s1">s2_ = s_</span>
    <span class="s4"># Check to see if the sources have been estimated</span>
    <span class="s4"># in the wrong order</span>
    <span class="s2">if </span><span class="s1">abs(np.dot(s1_</span><span class="s2">, </span><span class="s1">s2)) &gt; abs(np.dot(s1_</span><span class="s2">, </span><span class="s1">s1)):</span>
        <span class="s1">s2_</span><span class="s2">, </span><span class="s1">s1_ = s_</span>
    <span class="s1">s1_ *= np.sign(np.dot(s1_</span><span class="s2">, </span><span class="s1">s1))</span>
    <span class="s1">s2_ *= np.sign(np.dot(s2_</span><span class="s2">, </span><span class="s1">s2))</span>

    <span class="s4"># Check that we have estimated the original sources</span>
    <span class="s2">if not </span><span class="s1">add_noise:</span>
        <span class="s1">assert_allclose(np.dot(s1_</span><span class="s2">, </span><span class="s1">s1) / n_samples</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">1e-3</span><span class="s1">)</span>
        <span class="s1">assert_allclose(np.dot(s2_</span><span class="s2">, </span><span class="s1">s2) / n_samples</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">1e-3</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_fit_transform(global_random_seed</span><span class="s2">, </span><span class="s1">global_dtype):</span>
    <span class="s0">&quot;&quot;&quot;Test unit variance of transformed data using FastICA algorithm. 
 
    Check that `fit_transform` gives the same result as applying 
    `fit` and then `transform`. 
 
    Bug #13056 
    &quot;&quot;&quot;</span>
    <span class="s4"># multivariate uniform data in [0, 1]</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">X = rng.random_sample((</span><span class="s3">100</span><span class="s2">, </span><span class="s3">10</span><span class="s1">)).astype(global_dtype)</span>
    <span class="s1">max_iter = </span><span class="s3">300</span>
    <span class="s2">for </span><span class="s1">whiten</span><span class="s2">, </span><span class="s1">n_components </span><span class="s2">in </span><span class="s1">[[</span><span class="s5">&quot;unit-variance&quot;</span><span class="s2">, </span><span class="s3">5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s2">False, None</span><span class="s1">]]:</span>
        <span class="s1">n_components_ = n_components </span><span class="s2">if </span><span class="s1">n_components </span><span class="s2">is not None else </span><span class="s1">X.shape[</span><span class="s3">1</span><span class="s1">]</span>

        <span class="s1">ica = FastICA(</span>
            <span class="s1">n_components=n_components</span><span class="s2">, </span><span class="s1">max_iter=max_iter</span><span class="s2">, </span><span class="s1">whiten=whiten</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span>
        <span class="s1">)</span>
        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s4"># make sure that numerical errors do not cause sqrt of negative</span>
            <span class="s4"># values</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;error&quot;</span><span class="s2">, </span><span class="s1">RuntimeWarning)</span>
            <span class="s4"># XXX: for some seeds, the model does not converge.</span>
            <span class="s4"># However this is not what we test here.</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s2">, </span><span class="s1">ConvergenceWarning)</span>
            <span class="s1">Xt = ica.fit_transform(X)</span>
        <span class="s2">assert </span><span class="s1">ica.components_.shape == (n_components_</span><span class="s2">, </span><span class="s3">10</span><span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">Xt.shape == (X.shape[</span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">n_components_)</span>

        <span class="s1">ica2 = FastICA(</span>
            <span class="s1">n_components=n_components</span><span class="s2">, </span><span class="s1">max_iter=max_iter</span><span class="s2">, </span><span class="s1">whiten=whiten</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span>
        <span class="s1">)</span>
        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s4"># make sure that numerical errors do not cause sqrt of negative</span>
            <span class="s4"># values</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;error&quot;</span><span class="s2">, </span><span class="s1">RuntimeWarning)</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s2">, </span><span class="s1">ConvergenceWarning)</span>
            <span class="s1">ica2.fit(X)</span>
        <span class="s2">assert </span><span class="s1">ica2.components_.shape == (n_components_</span><span class="s2">, </span><span class="s3">10</span><span class="s1">)</span>
        <span class="s1">Xt2 = ica2.transform(X)</span>

        <span class="s4"># XXX: we have to set atol for this test to pass for all seeds when</span>
        <span class="s4"># fitting with float32 data. Is this revealing a bug?</span>
        <span class="s2">if </span><span class="s1">global_dtype:</span>
            <span class="s1">atol = np.abs(Xt2).mean() / </span><span class="s3">1e6</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">atol = </span><span class="s3">0.0  </span><span class="s4"># the default rtol is enough for float64 data</span>
        <span class="s1">assert_allclose(Xt</span><span class="s2">, </span><span class="s1">Xt2</span><span class="s2">, </span><span class="s1">atol=atol)</span>


<span class="s1">@pytest.mark.filterwarnings(</span><span class="s5">&quot;ignore:Ignoring n_components with whiten=False.&quot;</span><span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;whiten, n_components, expected_mixing_shape&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s5">&quot;arbitrary-variance&quot;</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s1">(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">5</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s5">&quot;arbitrary-variance&quot;</span><span class="s2">, </span><span class="s3">10</span><span class="s2">, </span><span class="s1">(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">10</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s5">&quot;unit-variance&quot;</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s1">(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">5</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s5">&quot;unit-variance&quot;</span><span class="s2">, </span><span class="s3">10</span><span class="s2">, </span><span class="s1">(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">10</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s2">False, </span><span class="s3">5</span><span class="s2">, </span><span class="s1">(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">10</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s2">False, </span><span class="s3">10</span><span class="s2">, </span><span class="s1">(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">10</span><span class="s1">))</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_inverse_transform(</span>
    <span class="s1">whiten</span><span class="s2">, </span><span class="s1">n_components</span><span class="s2">, </span><span class="s1">expected_mixing_shape</span><span class="s2">, </span><span class="s1">global_random_seed</span><span class="s2">, </span><span class="s1">global_dtype</span>
<span class="s1">):</span>
    <span class="s4"># Test FastICA.inverse_transform</span>
    <span class="s1">n_samples = </span><span class="s3">100</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">X = rng.random_sample((n_samples</span><span class="s2">, </span><span class="s3">10</span><span class="s1">)).astype(global_dtype)</span>

    <span class="s1">ica = FastICA(n_components=n_components</span><span class="s2">, </span><span class="s1">random_state=rng</span><span class="s2">, </span><span class="s1">whiten=whiten)</span>
    <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s4"># For some dataset (depending on the value of global_dtype) the model</span>
        <span class="s4"># can fail to converge but this should not impact the definition of</span>
        <span class="s4"># a valid inverse transform.</span>
        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s2">, </span><span class="s1">ConvergenceWarning)</span>
        <span class="s1">Xt = ica.fit_transform(X)</span>
    <span class="s2">assert </span><span class="s1">ica.mixing_.shape == expected_mixing_shape</span>
    <span class="s1">X2 = ica.inverse_transform(Xt)</span>
    <span class="s2">assert </span><span class="s1">X.shape == X2.shape</span>

    <span class="s4"># reversibility test in non-reduction case</span>
    <span class="s2">if </span><span class="s1">n_components == X.shape[</span><span class="s3">1</span><span class="s1">]:</span>
        <span class="s4"># XXX: we have to set atol for this test to pass for all seeds when</span>
        <span class="s4"># fitting with float32 data. Is this revealing a bug?</span>
        <span class="s2">if </span><span class="s1">global_dtype:</span>
            <span class="s4"># XXX: dividing by a smaller number makes</span>
            <span class="s4"># tests fail for some seeds.</span>
            <span class="s1">atol = np.abs(X2).mean() / </span><span class="s3">1e5</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">atol = </span><span class="s3">0.0  </span><span class="s4"># the default rtol is enough for float64 data</span>
        <span class="s1">assert_allclose(X</span><span class="s2">, </span><span class="s1">X2</span><span class="s2">, </span><span class="s1">atol=atol)</span>


<span class="s2">def </span><span class="s1">test_fastica_errors():</span>
    <span class="s1">n_features = </span><span class="s3">3</span>
    <span class="s1">n_samples = </span><span class="s3">10</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.random_sample((n_samples</span><span class="s2">, </span><span class="s1">n_features))</span>
    <span class="s1">w_init = rng.randn(n_features + </span><span class="s3">1</span><span class="s2">, </span><span class="s1">n_features + </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">r&quot;alpha must be in \[1,2\]&quot;</span><span class="s1">):</span>
        <span class="s1">fastica(X</span><span class="s2">, </span><span class="s1">fun_args={</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: </span><span class="s3">0</span><span class="s1">})</span>
    <span class="s2">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;w_init has invalid shape.+&quot; r&quot;should be \(3L?, 3L?\)&quot;</span>
    <span class="s1">):</span>
        <span class="s1">fastica(X</span><span class="s2">, </span><span class="s1">w_init=w_init)</span>


<span class="s2">def </span><span class="s1">test_fastica_whiten_unit_variance():</span>
    <span class="s0">&quot;&quot;&quot;Test unit variance of transformed data using FastICA algorithm. 
 
    Bug #13056 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.random_sample((</span><span class="s3">100</span><span class="s2">, </span><span class="s3">10</span><span class="s1">))</span>
    <span class="s1">n_components = X.shape[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">ica = FastICA(n_components=n_components</span><span class="s2">, </span><span class="s1">whiten=</span><span class="s5">&quot;unit-variance&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">Xt = ica.fit_transform(X)</span>

    <span class="s2">assert </span><span class="s1">np.var(Xt) == pytest.approx(</span><span class="s3">1.0</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;whiten&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">&quot;arbitrary-variance&quot;</span><span class="s2">, </span><span class="s5">&quot;unit-variance&quot;</span><span class="s2">, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;return_X_mean&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;return_n_iter&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_fastica_output_shape(whiten</span><span class="s2">, </span><span class="s1">return_X_mean</span><span class="s2">, </span><span class="s1">return_n_iter):</span>
    <span class="s1">n_features = </span><span class="s3">3</span>
    <span class="s1">n_samples = </span><span class="s3">10</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.random_sample((n_samples</span><span class="s2">, </span><span class="s1">n_features))</span>

    <span class="s1">expected_len = </span><span class="s3">3 </span><span class="s1">+ return_X_mean + return_n_iter</span>

    <span class="s1">out = fastica(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">whiten=whiten</span><span class="s2">, </span><span class="s1">return_n_iter=return_n_iter</span><span class="s2">, </span><span class="s1">return_X_mean=return_X_mean</span>
    <span class="s1">)</span>

    <span class="s2">assert </span><span class="s1">len(out) == expected_len</span>
    <span class="s2">if not </span><span class="s1">whiten:</span>
        <span class="s2">assert </span><span class="s1">out[</span><span class="s3">0</span><span class="s1">] </span><span class="s2">is None</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;add_noise&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_fastica_simple_different_solvers(add_noise</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Test FastICA is consistent between whiten_solvers.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">n_samples = </span><span class="s3">1000</span>
    <span class="s4"># Generate two sources:</span>
    <span class="s1">s1 = (</span><span class="s3">2 </span><span class="s1">* np.sin(np.linspace(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">100</span><span class="s2">, </span><span class="s1">n_samples)) &gt; </span><span class="s3">0</span><span class="s1">) - </span><span class="s3">1</span>
    <span class="s1">s2 = stats.t.rvs(</span><span class="s3">1</span><span class="s2">, </span><span class="s1">size=n_samples</span><span class="s2">, </span><span class="s1">random_state=rng)</span>
    <span class="s1">s = np.c_[s1</span><span class="s2">, </span><span class="s1">s2].T</span>
    <span class="s1">center_and_norm(s)</span>
    <span class="s1">s1</span><span class="s2">, </span><span class="s1">s2 = s</span>

    <span class="s4"># Mixing angle</span>
    <span class="s1">phi = rng.rand() * </span><span class="s3">2 </span><span class="s1">* np.pi</span>
    <span class="s1">mixing = np.array([[np.cos(phi)</span><span class="s2">, </span><span class="s1">np.sin(phi)]</span><span class="s2">, </span><span class="s1">[np.sin(phi)</span><span class="s2">, </span><span class="s1">-np.cos(phi)]])</span>
    <span class="s1">m = np.dot(mixing</span><span class="s2">, </span><span class="s1">s)</span>

    <span class="s2">if </span><span class="s1">add_noise:</span>
        <span class="s1">m += </span><span class="s3">0.1 </span><span class="s1">* rng.randn(</span><span class="s3">2</span><span class="s2">, </span><span class="s3">1000</span><span class="s1">)</span>

    <span class="s1">center_and_norm(m)</span>

    <span class="s1">outs = {}</span>
    <span class="s2">for </span><span class="s1">solver </span><span class="s2">in </span><span class="s1">(</span><span class="s5">&quot;svd&quot;</span><span class="s2">, </span><span class="s5">&quot;eigh&quot;</span><span class="s1">):</span>
        <span class="s1">ica = FastICA(random_state=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">whiten=</span><span class="s5">&quot;unit-variance&quot;</span><span class="s2">, </span><span class="s1">whiten_solver=solver)</span>
        <span class="s1">sources = ica.fit_transform(m.T)</span>
        <span class="s1">outs[solver] = sources</span>
        <span class="s2">assert </span><span class="s1">ica.components_.shape == (</span><span class="s3">2</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">sources.shape == (</span><span class="s3">1000</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span>

    <span class="s4"># compared numbers are not all on the same magnitude. Using a small atol to</span>
    <span class="s4"># make the test less brittle</span>
    <span class="s1">assert_allclose(outs[</span><span class="s5">&quot;eigh&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">outs[</span><span class="s5">&quot;svd&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">1e-12</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_fastica_eigh_low_rank_warning(global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Test FastICA eigh solver raises warning for low-rank data.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">A = rng.randn(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">X = A @ A.T</span>
    <span class="s1">ica = FastICA(random_state=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">whiten=</span><span class="s5">&quot;unit-variance&quot;</span><span class="s2">, </span><span class="s1">whiten_solver=</span><span class="s5">&quot;eigh&quot;</span><span class="s1">)</span>
    <span class="s1">msg = </span><span class="s5">&quot;There are some small singular values&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">ica.fit(X)</span>
</pre>
</body>
</html>