<html>
<head>
<title>dist_dependence_measures.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
dist_dependence_measures.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot; Distance dependence measure and the dCov test. 
 
Implementation of Sz√©kely et al. (2007) calculation of distance 
dependence statistics, including the Distance covariance (dCov) test 
for independence of random vectors of arbitrary length. 
 
Author: Ron Itzikovitch 
 
References 
---------- 
.. Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007) 
   &quot;Measuring and testing dependence by correlation of distances&quot;. 
   Annals of Statistics, Vol. 35 No. 6, pp. 2769-2794. 
 
&quot;&quot;&quot;</span>
<span class="s3">from </span><span class="s1">collections </span><span class="s3">import </span><span class="s1">namedtuple</span>
<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy.spatial.distance </span><span class="s3">import </span><span class="s1">pdist</span><span class="s3">, </span><span class="s1">squareform</span>
<span class="s3">from </span><span class="s1">scipy.stats </span><span class="s3">import </span><span class="s1">norm</span>

<span class="s3">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s3">import </span><span class="s1">HypothesisTestWarning</span>

<span class="s1">DistDependStat = namedtuple(</span>
    <span class="s4">&quot;DistDependStat&quot;</span><span class="s3">,</span>
    <span class="s1">[</span><span class="s4">&quot;test_statistic&quot;</span><span class="s3">, </span><span class="s4">&quot;distance_correlation&quot;</span><span class="s3">,</span>
     <span class="s4">&quot;distance_covariance&quot;</span><span class="s3">, </span><span class="s4">&quot;dvar_x&quot;</span><span class="s3">, </span><span class="s4">&quot;dvar_y&quot;</span><span class="s3">, </span><span class="s4">&quot;S&quot;</span><span class="s1">]</span><span class="s3">,</span>
<span class="s1">)</span>


<span class="s3">def </span><span class="s1">distance_covariance_test(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">B=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s4">&quot;auto&quot;</span><span class="s1">):</span>
    <span class="s2">r&quot;&quot;&quot;The Distance Covariance (dCov) test 
 
    Apply the Distance Covariance (dCov) test of independence to `x` and `y`. 
    This test was introduced in [1]_, and is based on the distance covariance 
    statistic. The test is applicable to random vectors of arbitrary length 
    (see the notes section for more details). 
 
    Parameters 
    ---------- 
    x : array_like, 1-D or 2-D 
        If `x` is 1-D than it is assumed to be a vector of observations of a 
        single random variable. If `x` is 2-D than the rows should be 
        observations and the columns are treated as the components of a 
        random vector, i.e., each column represents a different component of 
        the random vector `x`. 
    y : array_like, 1-D or 2-D 
        Same as `x`, but only the number of observation has to match that of 
        `x`. If `y` is 2-D note that the number of columns of `y` (i.e., the 
        number of components in the random vector) does not need to match 
        the number of columns in `x`. 
    B : int, optional, default=`None` 
        The number of iterations to perform when evaluating the null 
        distribution of the test statistic when the `emp` method is 
        applied (see below). if `B` is `None` than as in [1]_ we set 
        `B` to be ``B = 200 + 5000/n``, where `n` is the number of 
        observations. 
    method : {'auto', 'emp', 'asym'}, optional, default=auto 
        The method by which to obtain the p-value for the test. 
 
        - `auto` : Default method. The number of observations will be used to 
          determine the method. 
        - `emp` : Empirical evaluation of the p-value using permutations of 
          the rows of `y` to obtain the null distribution. 
        - `asym` : An asymptotic approximation of the distribution of the test 
          statistic is used to find the p-value. 
 
    Returns 
    ------- 
    test_statistic : float 
        The value of the test statistic used in the test. 
    pval : float 
        The p-value. 
    chosen_method : str 
        The method that was used to obtain the p-value. Mostly relevant when 
        the function is called with `method='auto'`. 
 
    Notes 
    ----- 
    The test applies to random vectors of arbitrary dimensions, i.e., `x` 
    can be a 1-D vector of observations for a single random variable while 
    `y` can be a `k` by `n` 2-D array (where `k &gt; 1`). In other words, it 
    is also possible for `x` and `y` to both be 2-D arrays and have the 
    same number of rows (observations) while differing in the number of 
    columns. 
 
    As noted in [1]_ the statistics are sensitive to all types of departures 
    from independence, including nonlinear or nonmonotone dependence 
    structure. 
 
    References 
    ---------- 
    .. [1] Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007) 
       &quot;Measuring and testing by correlation of distances&quot;. 
       Annals of Statistics, Vol. 35 No. 6, pp. 2769-2794. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from statsmodels.stats.dist_dependence_measures import 
    ... distance_covariance_test 
    &gt;&gt;&gt; data = np.random.rand(1000, 10) 
    &gt;&gt;&gt; x, y = data[:, :3], data[:, 3:] 
    &gt;&gt;&gt; x.shape 
    (1000, 3) 
    &gt;&gt;&gt; y.shape 
    (1000, 7) 
    &gt;&gt;&gt; distance_covariance_test(x, y) 
    (1.0426404792714983, 0.2971148340813543, 'asym') 
    # (test_statistic, pval, chosen_method) 
 
    &quot;&quot;&quot;</span>
    <span class="s1">x</span><span class="s3">, </span><span class="s1">y = _validate_and_tranform_x_and_y(x</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">n = x.shape[</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">stats = distance_statistics(x</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s3">if </span><span class="s1">method == </span><span class="s4">&quot;auto&quot; </span><span class="s3">and </span><span class="s1">n &lt;= </span><span class="s5">500 </span><span class="s3">or </span><span class="s1">method == </span><span class="s4">&quot;emp&quot;</span><span class="s1">:</span>
        <span class="s1">chosen_method = </span><span class="s4">&quot;emp&quot;</span>
        <span class="s1">test_statistic</span><span class="s3">, </span><span class="s1">pval = _empirical_pvalue(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">stats)</span>

    <span class="s3">elif </span><span class="s1">method == </span><span class="s4">&quot;auto&quot; </span><span class="s3">and </span><span class="s1">n &gt; </span><span class="s5">500 </span><span class="s3">or </span><span class="s1">method == </span><span class="s4">&quot;asym&quot;</span><span class="s1">:</span>
        <span class="s1">chosen_method = </span><span class="s4">&quot;asym&quot;</span>
        <span class="s1">test_statistic</span><span class="s3">, </span><span class="s1">pval = _asymptotic_pvalue(stats)</span>

    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Unknown 'method' parameter: {}&quot;</span><span class="s1">.format(method))</span>

    <span class="s0"># In case we got an extreme p-value (0 or 1) when using the empirical</span>
    <span class="s0"># distribution of the test statistic under the null, we fall back</span>
    <span class="s0"># to the asymptotic approximation.</span>
    <span class="s3">if </span><span class="s1">chosen_method == </span><span class="s4">&quot;emp&quot; </span><span class="s3">and </span><span class="s1">pval </span><span class="s3">in </span><span class="s1">[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]:</span>
        <span class="s1">msg = (</span>
            <span class="s4">f&quot;p-value was </span><span class="s3">{</span><span class="s1">pval</span><span class="s3">} </span><span class="s4">when using the empirical method. &quot;</span>
            <span class="s4">&quot;The asymptotic approximation will be used instead&quot;</span>
        <span class="s1">)</span>
        <span class="s1">warnings.warn(msg</span><span class="s3">, </span><span class="s1">HypothesisTestWarning)</span>
        <span class="s1">_</span><span class="s3">, </span><span class="s1">pval = _asymptotic_pvalue(stats)</span>

    <span class="s3">return </span><span class="s1">test_statistic</span><span class="s3">, </span><span class="s1">pval</span><span class="s3">, </span><span class="s1">chosen_method</span>


<span class="s3">def </span><span class="s1">_validate_and_tranform_x_and_y(x</span><span class="s3">, </span><span class="s1">y):</span>
    <span class="s2">r&quot;&quot;&quot;Ensure `x` and `y` have proper shape and transform/reshape them if 
    required. 
 
    Parameters 
    ---------- 
    x : array_like, 1-D or 2-D 
        If `x` is 1-D than it is assumed to be a vector of observations of a 
        single random variable. If `x` is 2-D than the rows should be 
        observations and the columns are treated as the components of a 
        random vector, i.e., each column represents a different component of 
        the random vector `x`. 
    y : array_like, 1-D or 2-D 
        Same as `x`, but only the number of observation has to match that of 
        `x`. If `y` is 2-D note that the number of columns of `y` (i.e., the 
        number of components in the random vector) does not need to match 
        the number of columns in `x`. 
 
    Returns 
    ------- 
    x : array_like, 1-D or 2-D 
    y : array_like, 1-D or 2-D 
 
    Raises 
    ------ 
    ValueError 
        If `x` and `y` have a different number of observations. 
 
    &quot;&quot;&quot;</span>
    <span class="s1">x = np.asanyarray(x)</span>
    <span class="s1">y = np.asanyarray(y)</span>

    <span class="s3">if </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">] != y.shape[</span><span class="s5">0</span><span class="s1">]:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;x and y must have the same number of observations (rows).&quot;</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">len(x.shape) == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">x = x.reshape((x.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>

    <span class="s3">if </span><span class="s1">len(y.shape) == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">y = y.reshape((y.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>

    <span class="s3">return </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y</span>


<span class="s3">def </span><span class="s1">_empirical_pvalue(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">stats):</span>
    <span class="s2">r&quot;&quot;&quot;Calculate the empirical p-value based on permutations of `y`'s rows 
 
    Parameters 
    ---------- 
    x : array_like, 1-D or 2-D 
        If `x` is 1-D than it is assumed to be a vector of observations of a 
        single random variable. If `x` is 2-D than the rows should be 
        observations and the columns are treated as the components of a 
        random vector, i.e., each column represents a different component of 
        the random vector `x`. 
    y : array_like, 1-D or 2-D 
        Same as `x`, but only the number of observation has to match that of 
        `x`. If `y` is 2-D note that the number of columns of `y` (i.e., the 
        number of components in the random vector) does not need to match 
        the number of columns in `x`. 
    B : int 
        The number of iterations when evaluating the null distribution. 
    n : Number of observations found in each of `x` and `y`. 
    stats: namedtuple 
        The result obtained from calling ``distance_statistics(x, y)``. 
 
    Returns 
    ------- 
    test_statistic : float 
        The empirical test statistic. 
    pval : float 
        The empirical p-value. 
 
    &quot;&quot;&quot;</span>
    <span class="s1">B = int(B) </span><span class="s3">if </span><span class="s1">B </span><span class="s3">else </span><span class="s1">int(np.floor(</span><span class="s5">200 </span><span class="s1">+ </span><span class="s5">5000 </span><span class="s1">/ n))</span>
    <span class="s1">empirical_dist = _get_test_statistic_distribution(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">B)</span>
    <span class="s1">pval = </span><span class="s5">1 </span><span class="s1">- np.searchsorted(</span>
        <span class="s1">sorted(empirical_dist)</span><span class="s3">, </span><span class="s1">stats.test_statistic</span>
    <span class="s1">) / len(empirical_dist)</span>
    <span class="s1">test_statistic = stats.test_statistic</span>

    <span class="s3">return </span><span class="s1">test_statistic</span><span class="s3">, </span><span class="s1">pval</span>


<span class="s3">def </span><span class="s1">_asymptotic_pvalue(stats):</span>
    <span class="s2">r&quot;&quot;&quot;Calculate the p-value based on an approximation of the distribution of 
    the test statistic under the null. 
 
    Parameters 
    ---------- 
    stats: namedtuple 
        The result obtained from calling ``distance_statistics(x, y)``. 
 
    Returns 
    ------- 
    test_statistic : float 
        The test statistic. 
    pval : float 
        The asymptotic p-value. 
 
    &quot;&quot;&quot;</span>
    <span class="s1">test_statistic = np.sqrt(stats.test_statistic / stats.S)</span>
    <span class="s1">pval = (</span><span class="s5">1 </span><span class="s1">- norm.cdf(test_statistic)) * </span><span class="s5">2</span>

    <span class="s3">return </span><span class="s1">test_statistic</span><span class="s3">, </span><span class="s1">pval</span>


<span class="s3">def </span><span class="s1">_get_test_statistic_distribution(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">B):</span>
    <span class="s2">r&quot;&quot;&quot; 
    Parameters 
    ---------- 
    x : array_like, 1-D or 2-D 
        If `x` is 1-D than it is assumed to be a vector of observations of a 
        single random variable. If `x` is 2-D than the rows should be 
        observations and the columns are treated as the components of a 
        random vector, i.e., each column represents a different component of 
        the random vector `x`. 
    y : array_like, 1-D or 2-D 
        Same as `x`, but only the number of observation has to match that of 
        `x`. If `y` is 2-D note that the number of columns of `y` (i.e., the 
        number of components in the random vector) does not need to match 
        the number of columns in `x`. 
    B : int 
        The number of iterations to perform when evaluating the null 
        distribution. 
 
    Returns 
    ------- 
    emp_dist : array_like 
        The empirical distribution of the test statistic. 
 
    &quot;&quot;&quot;</span>
    <span class="s1">y = y.copy()</span>
    <span class="s1">emp_dist = np.zeros(B)</span>
    <span class="s1">x_dist = squareform(pdist(x</span><span class="s3">, </span><span class="s4">&quot;euclidean&quot;</span><span class="s1">))</span>

    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(B):</span>
        <span class="s1">np.random.shuffle(y)</span>
        <span class="s1">emp_dist[i] = distance_statistics(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">x_dist=x_dist).test_statistic</span>

    <span class="s3">return </span><span class="s1">emp_dist</span>


<span class="s3">def </span><span class="s1">distance_statistics(x</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">x_dist=</span><span class="s3">None, </span><span class="s1">y_dist=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s2">r&quot;&quot;&quot;Calculate various distance dependence statistics. 
 
    Calculate several distance dependence statistics as described in [1]_. 
 
    Parameters 
    ---------- 
    x : array_like, 1-D or 2-D 
        If `x` is 1-D than it is assumed to be a vector of observations of a 
        single random variable. If `x` is 2-D than the rows should be 
        observations and the columns are treated as the components of a 
        random vector, i.e., each column represents a different component of 
        the random vector `x`. 
    y : array_like, 1-D or 2-D 
        Same as `x`, but only the number of observation has to match that of 
        `x`. If `y` is 2-D note that the number of columns of `y` (i.e., the 
        number of components in the random vector) does not need to match 
        the number of columns in `x`. 
    x_dist : array_like, 2-D, optional 
        A square 2-D array_like object whose values are the euclidean 
        distances between `x`'s rows. 
    y_dist : array_like, 2-D, optional 
        A square 2-D array_like object whose values are the euclidean 
        distances between `y`'s rows. 
 
    Returns 
    ------- 
    namedtuple 
        A named tuple of distance dependence statistics (DistDependStat) with 
        the following values: 
 
        - test_statistic : float - The &quot;basic&quot; test statistic (i.e., the one 
          used when the `emp` method is chosen when calling 
          ``distance_covariance_test()`` 
        - distance_correlation : float - The distance correlation 
          between `x` and `y`. 
        - distance_covariance : float - The distance covariance of 
          `x` and `y`. 
        - dvar_x : float - The distance variance of `x`. 
        - dvar_y : float - The distance variance of `y`. 
        - S : float - The mean of the euclidean distances in `x` multiplied 
          by those of `y`. Mostly used internally. 
 
    References 
    ---------- 
    .. [1] Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007) 
       &quot;Measuring and testing dependence by correlation of distances&quot;. 
       Annals of Statistics, Vol. 35 No. 6, pp. 2769-2794. 
 
    Examples 
    -------- 
 
    &gt;&gt;&gt; from statsmodels.stats.dist_dependence_measures import 
    ... distance_statistics 
    &gt;&gt;&gt; distance_statistics(np.random.random(1000), np.random.random(1000)) 
    DistDependStat(test_statistic=0.07948284320205831, 
    distance_correlation=0.04269511890990793, 
    distance_covariance=0.008915315092696293, 
    dvar_x=0.20719027438266704, dvar_y=0.21044934264957588, 
    S=0.10892061635588891) 
 
    &quot;&quot;&quot;</span>
    <span class="s1">x</span><span class="s3">, </span><span class="s1">y = _validate_and_tranform_x_and_y(x</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">n = x.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s1">a = x_dist </span><span class="s3">if </span><span class="s1">x_dist </span><span class="s3">is not None else </span><span class="s1">squareform(pdist(x</span><span class="s3">, </span><span class="s4">&quot;euclidean&quot;</span><span class="s1">))</span>
    <span class="s1">b = y_dist </span><span class="s3">if </span><span class="s1">y_dist </span><span class="s3">is not None else </span><span class="s1">squareform(pdist(y</span><span class="s3">, </span><span class="s4">&quot;euclidean&quot;</span><span class="s1">))</span>

    <span class="s1">a_row_means = a.mean(axis=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">b_row_means = b.mean(axis=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">a_col_means = a.mean(axis=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">b_col_means = b.mean(axis=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">keepdims=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">a_mean = a.mean()</span>
    <span class="s1">b_mean = b.mean()</span>

    <span class="s1">A = a - a_row_means - a_col_means + a_mean</span>
    <span class="s1">B = b - b_row_means - b_col_means + b_mean</span>

    <span class="s1">S = a_mean * b_mean</span>
    <span class="s1">dcov = np.sqrt(np.multiply(A</span><span class="s3">, </span><span class="s1">B).mean())</span>
    <span class="s1">dvar_x = np.sqrt(np.multiply(A</span><span class="s3">, </span><span class="s1">A).mean())</span>
    <span class="s1">dvar_y = np.sqrt(np.multiply(B</span><span class="s3">, </span><span class="s1">B).mean())</span>
    <span class="s1">dcor = dcov / np.sqrt(dvar_x * dvar_y)</span>

    <span class="s1">test_statistic = n * dcov ** </span><span class="s5">2</span>

    <span class="s3">return </span><span class="s1">DistDependStat(</span>
        <span class="s1">test_statistic=test_statistic</span><span class="s3">,</span>
        <span class="s1">distance_correlation=dcor</span><span class="s3">,</span>
        <span class="s1">distance_covariance=dcov</span><span class="s3">,</span>
        <span class="s1">dvar_x=dvar_x</span><span class="s3">,</span>
        <span class="s1">dvar_y=dvar_y</span><span class="s3">,</span>
        <span class="s1">S=S</span><span class="s3">,</span>
    <span class="s1">)</span>


<span class="s3">def </span><span class="s1">distance_covariance(x</span><span class="s3">, </span><span class="s1">y):</span>
    <span class="s2">r&quot;&quot;&quot;Distance covariance. 
 
    Calculate the empirical distance covariance as described in [1]_. 
 
    Parameters 
    ---------- 
    x : array_like, 1-D or 2-D 
        If `x` is 1-D than it is assumed to be a vector of observations of a 
        single random variable. If `x` is 2-D than the rows should be 
        observations and the columns are treated as the components of a 
        random vector, i.e., each column represents a different component of 
        the random vector `x`. 
    y : array_like, 1-D or 2-D 
        Same as `x`, but only the number of observation has to match that of 
        `x`. If `y` is 2-D note that the number of columns of `y` (i.e., the 
        number of components in the random vector) does not need to match 
        the number of columns in `x`. 
 
    Returns 
    ------- 
    float 
        The empirical distance covariance between `x` and `y`. 
 
    References 
    ---------- 
    .. [1] Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007) 
       &quot;Measuring and testing dependence by correlation of distances&quot;. 
       Annals of Statistics, Vol. 35 No. 6, pp. 2769-2794. 
 
    Examples 
    -------- 
 
    &gt;&gt;&gt; from statsmodels.stats.dist_dependence_measures import 
    ... distance_covariance 
    &gt;&gt;&gt; distance_covariance(np.random.random(1000), np.random.random(1000)) 
    0.007575063951951362 
 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">distance_statistics(x</span><span class="s3">, </span><span class="s1">y).distance_covariance</span>


<span class="s3">def </span><span class="s1">distance_variance(x):</span>
    <span class="s2">r&quot;&quot;&quot;Distance variance. 
 
    Calculate the empirical distance variance as described in [1]_. 
 
    Parameters 
    ---------- 
    x : array_like, 1-D or 2-D 
        If `x` is 1-D than it is assumed to be a vector of observations of a 
        single random variable. If `x` is 2-D than the rows should be 
        observations and the columns are treated as the components of a 
        random vector, i.e., each column represents a different component of 
        the random vector `x`. 
 
    Returns 
    ------- 
    float 
        The empirical distance variance of `x`. 
 
    References 
    ---------- 
    .. [1] Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007) 
       &quot;Measuring and testing dependence by correlation of distances&quot;. 
       Annals of Statistics, Vol. 35 No. 6, pp. 2769-2794. 
 
    Examples 
    -------- 
 
    &gt;&gt;&gt; from statsmodels.stats.dist_dependence_measures import 
    ... distance_variance 
    &gt;&gt;&gt; distance_variance(np.random.random(1000)) 
    0.21732609190659702 
 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">distance_covariance(x</span><span class="s3">, </span><span class="s1">x)</span>


<span class="s3">def </span><span class="s1">distance_correlation(x</span><span class="s3">, </span><span class="s1">y):</span>
    <span class="s2">r&quot;&quot;&quot;Distance correlation. 
 
    Calculate the empirical distance correlation as described in [1]_. 
    This statistic is analogous to product-moment correlation and describes 
    the dependence between `x` and `y`, which are random vectors of 
    arbitrary length. The statistics' values range between 0 (implies 
    independence) and 1 (implies complete dependence). 
 
    Parameters 
    ---------- 
    x : array_like, 1-D or 2-D 
        If `x` is 1-D than it is assumed to be a vector of observations of a 
        single random variable. If `x` is 2-D than the rows should be 
        observations and the columns are treated as the components of a 
        random vector, i.e., each column represents a different component of 
        the random vector `x`. 
    y : array_like, 1-D or 2-D 
        Same as `x`, but only the number of observation has to match that of 
        `x`. If `y` is 2-D note that the number of columns of `y` (i.e., the 
        number of components in the random vector) does not need to match 
        the number of columns in `x`. 
 
    Returns 
    ------- 
    float 
        The empirical distance correlation between `x` and `y`. 
 
    References 
    ---------- 
    .. [1] Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007) 
       &quot;Measuring and testing dependence by correlation of distances&quot;. 
       Annals of Statistics, Vol. 35 No. 6, pp. 2769-2794. 
 
    Examples 
    -------- 
 
    &gt;&gt;&gt; from statsmodels.stats.dist_dependence_measures import 
    ... distance_correlation 
    &gt;&gt;&gt; distance_correlation(np.random.random(1000), np.random.random(1000)) 
    0.04060497840149489 
 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">distance_statistics(x</span><span class="s3">, </span><span class="s1">y).distance_correlation</span>
</pre>
</body>
</html>