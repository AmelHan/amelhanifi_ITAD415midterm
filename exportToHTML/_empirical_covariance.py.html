<html>
<head>
<title>_empirical_covariance.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_empirical_covariance.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Maximum likelihood covariance estimator. 
 
&quot;&quot;&quot;</span>

<span class="s2"># Author: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr&gt;</span>
<span class="s2">#         Gael Varoquaux &lt;gael.varoquaux@normalesup.org&gt;</span>
<span class="s2">#         Virgile Fritsch &lt;virgile.fritsch@inria.fr&gt;</span>
<span class="s2">#</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s2"># avoid division truncation</span>
<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">linalg</span>

<span class="s3">from </span><span class="s1">.. </span><span class="s3">import </span><span class="s1">config_context</span>
<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">BaseEstimator</span><span class="s3">, </span><span class="s1">_fit_context</span>
<span class="s3">from </span><span class="s1">..metrics.pairwise </span><span class="s3">import </span><span class="s1">pairwise_distances</span>
<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">check_array</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">validate_params</span>
<span class="s3">from </span><span class="s1">..utils.extmath </span><span class="s3">import </span><span class="s1">fast_logdet</span>


<span class="s3">def </span><span class="s1">log_likelihood(emp_cov</span><span class="s3">, </span><span class="s1">precision):</span>
    <span class="s0">&quot;&quot;&quot;Compute the sample mean of the log_likelihood under a covariance model. 
 
    Computes the empirical expected log-likelihood, allowing for universal 
    comparison (beyond this software package), and accounts for normalization 
    terms and scaling. 
 
    Parameters 
    ---------- 
    emp_cov : ndarray of shape (n_features, n_features) 
        Maximum Likelihood Estimator of covariance. 
 
    precision : ndarray of shape (n_features, n_features) 
        The precision matrix of the covariance model to be tested. 
 
    Returns 
    ------- 
    log_likelihood_ : float 
        Sample mean of the log-likelihood. 
    &quot;&quot;&quot;</span>
    <span class="s1">p = precision.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">log_likelihood_ = -np.sum(emp_cov * precision) + fast_logdet(precision)</span>
    <span class="s1">log_likelihood_ -= p * np.log(</span><span class="s4">2 </span><span class="s1">* np.pi)</span>
    <span class="s1">log_likelihood_ /= </span><span class="s4">2.0</span>
    <span class="s3">return </span><span class="s1">log_likelihood_</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s5">&quot;X&quot;</span><span class="s1">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;assume_centered&quot;</span><span class="s1">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">True,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">empirical_covariance(X</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">assume_centered=</span><span class="s3">False</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the Maximum likelihood covariance estimator. 
 
    Parameters 
    ---------- 
    X : ndarray of shape (n_samples, n_features) 
        Data from which to compute the covariance estimate. 
 
    assume_centered : bool, default=False 
        If `True`, data will not be centered before computation. 
        Useful when working with data whose mean is almost, but not exactly 
        zero. 
        If `False`, data will be centered before computation. 
 
    Returns 
    ------- 
    covariance : ndarray of shape (n_features, n_features) 
        Empirical covariance (Maximum Likelihood Estimator). 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.covariance import empirical_covariance 
    &gt;&gt;&gt; X = [[1,1,1],[1,1,1],[1,1,1], 
    ...      [0,0,0],[0,0,0],[0,0,0]] 
    &gt;&gt;&gt; empirical_covariance(X) 
    array([[0.25, 0.25, 0.25], 
           [0.25, 0.25, 0.25], 
           [0.25, 0.25, 0.25]]) 
    &quot;&quot;&quot;</span>
    <span class="s1">X = check_array(X</span><span class="s3">, </span><span class="s1">ensure_2d=</span><span class="s3">False, </span><span class="s1">force_all_finite=</span><span class="s3">False</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">X.ndim == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">X = np.reshape(X</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">))</span>

    <span class="s3">if </span><span class="s1">X.shape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">warnings.warn(</span>
            <span class="s5">&quot;Only one sample available. You may want to reshape your data array&quot;</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">assume_centered:</span>
        <span class="s1">covariance = np.dot(X.T</span><span class="s3">, </span><span class="s1">X) / X.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">covariance = np.cov(X.T</span><span class="s3">, </span><span class="s1">bias=</span><span class="s4">1</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">covariance.ndim == </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s1">covariance = np.array([[covariance]])</span>
    <span class="s3">return </span><span class="s1">covariance</span>


<span class="s3">class </span><span class="s1">EmpiricalCovariance(BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Maximum likelihood covariance estimator. 
 
    Read more in the :ref:`User Guide &lt;covariance&gt;`. 
 
    Parameters 
    ---------- 
    store_precision : bool, default=True 
        Specifies if the estimated precision is stored. 
 
    assume_centered : bool, default=False 
        If True, data are not centered before computation. 
        Useful when working with data whose mean is almost, but not exactly 
        zero. 
        If False (default), data are centered before computation. 
 
    Attributes 
    ---------- 
    location_ : ndarray of shape (n_features,) 
        Estimated location, i.e. the estimated mean. 
 
    covariance_ : ndarray of shape (n_features, n_features) 
        Estimated covariance matrix 
 
    precision_ : ndarray of shape (n_features, n_features) 
        Estimated pseudo-inverse matrix. 
        (stored only if store_precision is True) 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    EllipticEnvelope : An object for detecting outliers in 
        a Gaussian distributed dataset. 
    GraphicalLasso : Sparse inverse covariance estimation 
        with an l1-penalized estimator. 
    LedoitWolf : LedoitWolf Estimator. 
    MinCovDet : Minimum Covariance Determinant 
        (robust estimator of covariance). 
    OAS : Oracle Approximating Shrinkage Estimator. 
    ShrunkCovariance : Covariance estimator with shrinkage. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.covariance import EmpiricalCovariance 
    &gt;&gt;&gt; from sklearn.datasets import make_gaussian_quantiles 
    &gt;&gt;&gt; real_cov = np.array([[.8, .3], 
    ...                      [.3, .4]]) 
    &gt;&gt;&gt; rng = np.random.RandomState(0) 
    &gt;&gt;&gt; X = rng.multivariate_normal(mean=[0, 0], 
    ...                             cov=real_cov, 
    ...                             size=500) 
    &gt;&gt;&gt; cov = EmpiricalCovariance().fit(X) 
    &gt;&gt;&gt; cov.covariance_ 
    array([[0.7569..., 0.2818...], 
           [0.2818..., 0.3928...]]) 
    &gt;&gt;&gt; cov.location_ 
    array([0.0622..., 0.0193...]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s5">&quot;store_precision&quot;</span><span class="s1">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;assume_centered&quot;</span><span class="s1">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">store_precision=</span><span class="s3">True, </span><span class="s1">assume_centered=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s1">self.store_precision = store_precision</span>
        <span class="s1">self.assume_centered = assume_centered</span>

    <span class="s3">def </span><span class="s1">_set_covariance(self</span><span class="s3">, </span><span class="s1">covariance):</span>
        <span class="s0">&quot;&quot;&quot;Saves the covariance and precision estimates 
 
        Storage is done accordingly to `self.store_precision`. 
        Precision stored only if invertible. 
 
        Parameters 
        ---------- 
        covariance : array-like of shape (n_features, n_features) 
            Estimated covariance matrix to be stored, and from which precision 
            is computed. 
        &quot;&quot;&quot;</span>
        <span class="s1">covariance = check_array(covariance)</span>
        <span class="s2"># set covariance</span>
        <span class="s1">self.covariance_ = covariance</span>
        <span class="s2"># set precision</span>
        <span class="s3">if </span><span class="s1">self.store_precision:</span>
            <span class="s1">self.precision_ = linalg.pinvh(covariance</span><span class="s3">, </span><span class="s1">check_finite=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.precision_ = </span><span class="s3">None</span>

    <span class="s3">def </span><span class="s1">get_precision(self):</span>
        <span class="s0">&quot;&quot;&quot;Getter for the precision matrix. 
 
        Returns 
        ------- 
        precision_ : array-like of shape (n_features, n_features) 
            The precision matrix associated to the current covariance object. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self.store_precision:</span>
            <span class="s1">precision = self.precision_</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">precision = linalg.pinvh(self.covariance_</span><span class="s3">, </span><span class="s1">check_finite=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">precision</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the maximum likelihood covariance estimator to X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
          Training data, where `n_samples` is the number of samples and 
          `n_features` is the number of features. 
 
        y : Ignored 
            Not used, present for API consistency by convention. 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X)</span>
        <span class="s3">if </span><span class="s1">self.assume_centered:</span>
            <span class="s1">self.location_ = np.zeros(X.shape[</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.location_ = X.mean(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">covariance = empirical_covariance(X</span><span class="s3">, </span><span class="s1">assume_centered=self.assume_centered)</span>
        <span class="s1">self._set_covariance(covariance)</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Compute the log-likelihood of `X_test` under the estimated Gaussian model. 
 
        The Gaussian model is defined by its mean and covariance matrix which are 
        represented respectively by `self.location_` and `self.covariance_`. 
 
        Parameters 
        ---------- 
        X_test : array-like of shape (n_samples, n_features) 
            Test data of which we compute the likelihood, where `n_samples` is 
            the number of samples and `n_features` is the number of features. 
            `X_test` is assumed to be drawn from the same distribution than 
            the data used in fit (including centering). 
 
        y : Ignored 
            Not used, present for API consistency by convention. 
 
        Returns 
        ------- 
        res : float 
            The log-likelihood of `X_test` with `self.location_` and `self.covariance_` 
            as estimators of the Gaussian model mean and covariance matrix respectively. 
        &quot;&quot;&quot;</span>
        <span class="s1">X_test = self._validate_data(X_test</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s2"># compute empirical covariance of the test set</span>
        <span class="s1">test_cov = empirical_covariance(X_test - self.location_</span><span class="s3">, </span><span class="s1">assume_centered=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s2"># compute log likelihood</span>
        <span class="s1">res = log_likelihood(test_cov</span><span class="s3">, </span><span class="s1">self.get_precision())</span>

        <span class="s3">return </span><span class="s1">res</span>

    <span class="s3">def </span><span class="s1">error_norm(self</span><span class="s3">, </span><span class="s1">comp_cov</span><span class="s3">, </span><span class="s1">norm=</span><span class="s5">&quot;frobenius&quot;</span><span class="s3">, </span><span class="s1">scaling=</span><span class="s3">True, </span><span class="s1">squared=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Compute the Mean Squared Error between two covariance estimators. 
 
        Parameters 
        ---------- 
        comp_cov : array-like of shape (n_features, n_features) 
            The covariance to compare with. 
 
        norm : {&quot;frobenius&quot;, &quot;spectral&quot;}, default=&quot;frobenius&quot; 
            The type of norm used to compute the error. Available error types: 
            - 'frobenius' (default): sqrt(tr(A^t.A)) 
            - 'spectral': sqrt(max(eigenvalues(A^t.A)) 
            where A is the error ``(comp_cov - self.covariance_)``. 
 
        scaling : bool, default=True 
            If True (default), the squared error norm is divided by n_features. 
            If False, the squared error norm is not rescaled. 
 
        squared : bool, default=True 
            Whether to compute the squared error norm or the error norm. 
            If True (default), the squared error norm is returned. 
            If False, the error norm is returned. 
 
        Returns 
        ------- 
        result : float 
            The Mean Squared Error (in the sense of the Frobenius norm) between 
            `self` and `comp_cov` covariance estimators. 
        &quot;&quot;&quot;</span>
        <span class="s2"># compute the error</span>
        <span class="s1">error = comp_cov - self.covariance_</span>
        <span class="s2"># compute the error norm</span>
        <span class="s3">if </span><span class="s1">norm == </span><span class="s5">&quot;frobenius&quot;</span><span class="s1">:</span>
            <span class="s1">squared_norm = np.sum(error**</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">norm == </span><span class="s5">&quot;spectral&quot;</span><span class="s1">:</span>
            <span class="s1">squared_norm = np.amax(linalg.svdvals(np.dot(error.T</span><span class="s3">, </span><span class="s1">error)))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
                <span class="s5">&quot;Only spectral and frobenius norms are implemented&quot;</span>
            <span class="s1">)</span>
        <span class="s2"># optionally scale the error norm</span>
        <span class="s3">if </span><span class="s1">scaling:</span>
            <span class="s1">squared_norm = squared_norm / error.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s2"># finally get either the squared norm or the norm</span>
        <span class="s3">if </span><span class="s1">squared:</span>
            <span class="s1">result = squared_norm</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">result = np.sqrt(squared_norm)</span>

        <span class="s3">return </span><span class="s1">result</span>

    <span class="s3">def </span><span class="s1">mahalanobis(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Compute the squared Mahalanobis distances of given observations. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The observations, the Mahalanobis distances of the which we 
            compute. Observations are assumed to be drawn from the same 
            distribution than the data used in fit. 
 
        Returns 
        ------- 
        dist : ndarray of shape (n_samples,) 
            Squared Mahalanobis distances of the observations. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False</span><span class="s1">)</span>

        <span class="s1">precision = self.get_precision()</span>
        <span class="s3">with </span><span class="s1">config_context(assume_finite=</span><span class="s3">True</span><span class="s1">):</span>
            <span class="s2"># compute mahalanobis distances</span>
            <span class="s1">dist = pairwise_distances(</span>
                <span class="s1">X</span><span class="s3">, </span><span class="s1">self.location_[np.newaxis</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">metric=</span><span class="s5">&quot;mahalanobis&quot;</span><span class="s3">, </span><span class="s1">VI=precision</span>
            <span class="s1">)</span>

        <span class="s3">return </span><span class="s1">np.reshape(dist</span><span class="s3">, </span><span class="s1">(len(X)</span><span class="s3">,</span><span class="s1">)) ** </span><span class="s4">2</span>
</pre>
</body>
</html>