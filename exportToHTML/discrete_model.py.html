<html>
<head>
<title>discrete_model.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #cc7832;}
.s4 { color: #808080;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
discrete_model.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Limited dependent variable and qualitative variables. 
 
Includes binary outcomes, count data, (ordered) ordinal data and limited 
dependent variables. 
 
General References 
-------------------- 
 
A.C. Cameron and P.K. Trivedi.  `Regression Analysis of Count Data`. 
    Cambridge, 1998 
 
G.S. Madalla. `Limited-Dependent and Qualitative Variables in Econometrics`. 
    Cambridge, 1983. 
 
W. Greene. `Econometric Analysis`. Prentice Hall, 5th. edition. 2003. 
&quot;&quot;&quot;</span>
<span class="s1">__all__ = [</span><span class="s2">&quot;Poisson&quot;</span><span class="s3">, </span><span class="s2">&quot;Logit&quot;</span><span class="s3">, </span><span class="s2">&quot;Probit&quot;</span><span class="s3">, </span><span class="s2">&quot;MNLogit&quot;</span><span class="s3">, </span><span class="s2">&quot;NegativeBinomial&quot;</span><span class="s3">,</span>
           <span class="s2">&quot;GeneralizedPoisson&quot;</span><span class="s3">, </span><span class="s2">&quot;NegativeBinomialP&quot;</span><span class="s3">, </span><span class="s2">&quot;CountModel&quot;</span><span class="s1">]</span>

<span class="s3">from </span><span class="s1">statsmodels.compat.pandas </span><span class="s3">import </span><span class="s1">Appender</span>

<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">pandas </span><span class="s3">import </span><span class="s1">MultiIndex</span><span class="s3">, </span><span class="s1">get_dummies</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">special</span><span class="s3">, </span><span class="s1">stats</span>
<span class="s3">from </span><span class="s1">scipy.special </span><span class="s3">import </span><span class="s1">digamma</span><span class="s3">, </span><span class="s1">gammaln</span><span class="s3">, </span><span class="s1">loggamma</span><span class="s3">, </span><span class="s1">polygamma</span>
<span class="s3">from </span><span class="s1">scipy.stats </span><span class="s3">import </span><span class="s1">nbinom</span>

<span class="s3">from </span><span class="s1">statsmodels.base.data </span><span class="s3">import </span><span class="s1">handle_data  </span><span class="s4"># for mnlogit</span>
<span class="s3">from </span><span class="s1">statsmodels.base.l1_slsqp </span><span class="s3">import </span><span class="s1">fit_l1_slsqp</span>
<span class="s3">import </span><span class="s1">statsmodels.base.model </span><span class="s3">as </span><span class="s1">base</span>
<span class="s3">import </span><span class="s1">statsmodels.base.wrapper </span><span class="s3">as </span><span class="s1">wrap</span>
<span class="s3">from </span><span class="s1">statsmodels.base._constraints </span><span class="s3">import </span><span class="s1">fit_constrained_wrap</span>
<span class="s3">import </span><span class="s1">statsmodels.base._parameter_inference </span><span class="s3">as </span><span class="s1">pinfer</span>
<span class="s3">from </span><span class="s1">statsmodels.base </span><span class="s3">import </span><span class="s1">_prediction_inference </span><span class="s3">as </span><span class="s1">pred</span>
<span class="s3">from </span><span class="s1">statsmodels.distributions </span><span class="s3">import </span><span class="s1">genpoisson_p</span>
<span class="s3">import </span><span class="s1">statsmodels.regression.linear_model </span><span class="s3">as </span><span class="s1">lm</span>
<span class="s3">from </span><span class="s1">statsmodels.tools </span><span class="s3">import </span><span class="s1">data </span><span class="s3">as </span><span class="s1">data_tools</span><span class="s3">, </span><span class="s1">tools</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.decorators </span><span class="s3">import </span><span class="s1">cache_readonly</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.numdiff </span><span class="s3">import </span><span class="s1">approx_fprime_cs</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">PerfectSeparationError</span><span class="s3">,</span>
    <span class="s1">PerfectSeparationWarning</span><span class="s3">,</span>
    <span class="s1">SpecificationWarning</span><span class="s3">,</span>
    <span class="s1">)</span>


<span class="s3">try</span><span class="s1">:</span>
    <span class="s3">import </span><span class="s1">cvxopt  </span><span class="s4"># noqa:F401</span>
    <span class="s1">have_cvxopt = </span><span class="s3">True</span>
<span class="s3">except </span><span class="s1">ImportError:</span>
    <span class="s1">have_cvxopt = </span><span class="s3">False</span>


<span class="s4"># TODO: When we eventually get user-settable precision, we need to change</span>
<span class="s4">#       this</span>
<span class="s1">FLOAT_EPS = np.finfo(float).eps</span>

<span class="s4"># Limit for exponentials to avoid overflow</span>
<span class="s1">EXP_UPPER_LIMIT = np.log(np.finfo(np.float64).max) - </span><span class="s5">1.0</span>

<span class="s4"># TODO: add options for the parameter covariance/variance</span>
<span class="s4">#       ie., OIM, EIM, and BHHH see Green 21.4</span>

<span class="s1">_discrete_models_docs = </span><span class="s2">&quot;&quot;&quot; 
&quot;&quot;&quot;</span>

<span class="s1">_discrete_results_docs = </span><span class="s2">&quot;&quot;&quot; 
    %(one_line_description)s 
 
    Parameters 
    ---------- 
    model : A DiscreteModel instance 
    params : array_like 
        The parameters of a fitted model. 
    hessian : array_like 
        The hessian of the fitted model. 
    scale : float 
        A scale parameter for the covariance matrix. 
 
    Attributes 
    ---------- 
    df_resid : float 
        See model definition. 
    df_model : float 
        See model definition. 
    llf : float 
        Value of the loglikelihood 
    %(extra_attr)s&quot;&quot;&quot;</span>

<span class="s1">_l1_results_attr = </span><span class="s2">&quot;&quot;&quot;    nnz_params : int 
        The number of nonzero parameters in the model.  Train with 
        trim_params == True or else numerical error will distort this. 
    trimmed : bool array 
        trimmed[i] == True if the ith parameter was trimmed from the model.&quot;&quot;&quot;</span>

<span class="s1">_get_start_params_null_docs = </span><span class="s2">&quot;&quot;&quot; 
Compute one-step moment estimator for null (constant-only) model 
 
This is a preliminary estimator used as start_params. 
 
Returns 
------- 
params : ndarray 
    parameter estimate based one one-step moment matching 
 
&quot;&quot;&quot;</span>

<span class="s1">_check_rank_doc = </span><span class="s2">&quot;&quot;&quot; 
    check_rank : bool 
        Check exog rank to determine model degrees of freedom. Default is 
        True. Setting to False reduces model initialization time when 
        exog.shape[1] is large. 
    &quot;&quot;&quot;</span>


<span class="s4"># helper for MNLogit (will be generally useful later)</span>
<span class="s3">def </span><span class="s1">_numpy_to_dummies(endog):</span>
    <span class="s3">if </span><span class="s1">endog.ndim == </span><span class="s5">2 </span><span class="s3">and </span><span class="s1">endog.dtype.kind </span><span class="s3">not in </span><span class="s1">[</span><span class="s2">&quot;S&quot;</span><span class="s3">, </span><span class="s2">&quot;O&quot;</span><span class="s1">]:</span>
        <span class="s1">endog_dummies = endog</span>
        <span class="s1">ynames = range(endog.shape[</span><span class="s5">1</span><span class="s1">])</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">dummies = get_dummies(endog</span><span class="s3">, </span><span class="s1">drop_first=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">ynames = {i: dummies.columns[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(dummies.shape[</span><span class="s5">1</span><span class="s1">])}</span>
        <span class="s1">endog_dummies = np.asarray(dummies</span><span class="s3">, </span><span class="s1">dtype=float)</span>

        <span class="s3">return </span><span class="s1">endog_dummies</span><span class="s3">, </span><span class="s1">ynames</span>

    <span class="s3">return </span><span class="s1">endog_dummies</span><span class="s3">, </span><span class="s1">ynames</span>


<span class="s3">def </span><span class="s1">_pandas_to_dummies(endog):</span>
    <span class="s3">if </span><span class="s1">endog.ndim == </span><span class="s5">2</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">endog.shape[</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">yname = endog.columns[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">endog_dummies = get_dummies(endog.iloc[:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:  </span><span class="s4"># assume already dummies</span>
            <span class="s1">yname = </span><span class="s2">'y'</span>
            <span class="s1">endog_dummies = endog</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">yname = endog.name</span>
        <span class="s3">if </span><span class="s1">yname </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">yname = </span><span class="s2">'y'</span>
        <span class="s1">endog_dummies = get_dummies(endog)</span>
    <span class="s1">ynames = endog_dummies.columns.tolist()</span>

    <span class="s3">return </span><span class="s1">endog_dummies</span><span class="s3">, </span><span class="s1">ynames</span><span class="s3">, </span><span class="s1">yname</span>


<span class="s3">def </span><span class="s1">_validate_l1_method(method):</span>
    <span class="s0">&quot;&quot;&quot; 
    As of 0.10.0, the supported values for `method` in `fit_regularized` 
    are &quot;l1&quot; and &quot;l1_cvxopt_cp&quot;.  If an invalid value is passed, raise 
    with a helpful error message 
 
    Parameters 
    ---------- 
    method : str 
 
    Raises 
    ------ 
    ValueError 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">method </span><span class="s3">not in </span><span class="s1">[</span><span class="s2">'l1'</span><span class="s3">, </span><span class="s2">'l1_cvxopt_cp'</span><span class="s1">]:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s2">'`method` = {method} is not supported, use either '</span>
                         <span class="s2">'&quot;l1&quot; or &quot;l1_cvxopt_cp&quot;'</span><span class="s1">.format(method=method))</span>


<span class="s4">#### Private Model Classes ####</span>


<span class="s3">class </span><span class="s1">DiscreteModel(base.LikelihoodModel):</span>
    <span class="s0">&quot;&quot;&quot; 
    Abstract class for discrete choice models. 
 
    This class does not do anything itself but lays out the methods and 
    call signature expected of child classes in addition to those of 
    statsmodels.model.LikelihoodModel. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">check_rank=</span><span class="s3">True, </span><span class="s1">**kwargs):</span>
        <span class="s1">self._check_rank = check_rank</span>
        <span class="s1">super().__init__(endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s1">self.raise_on_perfect_prediction = </span><span class="s3">False  </span><span class="s4"># keep for backwards compat</span>
        <span class="s1">self.k_extra = </span><span class="s5">0</span>

    <span class="s3">def </span><span class="s1">initialize(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Initialize is called by 
        statsmodels.model.LikelihoodModel.__init__ 
        and should contain any preprocessing that needs to be done for a model. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._check_rank:</span>
            <span class="s4"># assumes constant</span>
            <span class="s1">rank = tools.matrix_rank(self.exog</span><span class="s3">, </span><span class="s1">method=</span><span class="s2">&quot;qr&quot;</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s4"># If rank check is skipped, assume full</span>
            <span class="s1">rank = self.exog.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">self.df_model = float(rank - </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">self.df_resid = float(self.exog.shape[</span><span class="s5">0</span><span class="s1">] - rank)</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        The cumulative distribution function of the model. 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        The probability density (mass) function of the model. 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">def </span><span class="s1">_check_perfect_pred(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">*args):</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">fittedvalues = self.predict(params)</span>
        <span class="s3">if </span><span class="s1">np.allclose(fittedvalues - endog</span><span class="s3">, </span><span class="s5">0</span><span class="s1">):</span>
            <span class="s3">if </span><span class="s1">self.raise_on_perfect_prediction:</span>
                <span class="s4"># backwards compatibility for attr raise_on_perfect_prediction</span>
                <span class="s1">msg = </span><span class="s2">&quot;Perfect separation detected, results not available&quot;</span>
                <span class="s3">raise </span><span class="s1">PerfectSeparationError(msg)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">msg = (</span><span class="s2">&quot;Perfect separation or prediction detected, &quot;</span>
                       <span class="s2">&quot;parameter may not be identified&quot;</span><span class="s1">)</span>
                <span class="s1">warnings.warn(msg</span><span class="s3">, </span><span class="s1">category=PerfectSeparationWarning)</span>

    <span class="s1">@Appender(base.LikelihoodModel.fit.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'newton'</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s5">35</span><span class="s3">,</span>
            <span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fit the model using maximum likelihood. 
 
        The rest of the docstring is from 
        statsmodels.base.model.LikelihoodModel.fit 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">callback </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">callback = self._check_perfect_pred</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">pass  </span><span class="s4"># TODO: make a function factory to have multiple call-backs</span>

        <span class="s1">mlefit = super().fit(start_params=start_params</span><span class="s3">,</span>
                             <span class="s1">method=method</span><span class="s3">,</span>
                             <span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                             <span class="s1">full_output=full_output</span><span class="s3">,</span>
                             <span class="s1">disp=disp</span><span class="s3">,</span>
                             <span class="s1">callback=callback</span><span class="s3">,</span>
                             <span class="s1">**kwargs)</span>

        <span class="s3">return </span><span class="s1">mlefit  </span><span class="s4"># It is up to subclasses to wrap results</span>

    <span class="s3">def </span><span class="s1">fit_regularized(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'l1'</span><span class="s3">,</span>
                        <span class="s1">maxiter=</span><span class="s2">'defined_by_method'</span><span class="s3">, </span><span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s3">True,</span>
                        <span class="s1">callback=</span><span class="s3">None, </span><span class="s1">alpha=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">trim_mode=</span><span class="s2">'auto'</span><span class="s3">,</span>
                        <span class="s1">auto_trim_tol=</span><span class="s5">0.01</span><span class="s3">, </span><span class="s1">size_trim_tol=</span><span class="s5">1e-4</span><span class="s3">, </span><span class="s1">qc_tol=</span><span class="s5">0.03</span><span class="s3">,</span>
                        <span class="s1">qc_verbose=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fit the model using a regularized maximum likelihood. 
 
        The regularization method AND the solver used is determined by the 
        argument method. 
 
        Parameters 
        ---------- 
        start_params : array_like, optional 
            Initial guess of the solution for the loglikelihood maximization. 
            The default is an array of zeros. 
        method : 'l1' or 'l1_cvxopt_cp' 
            See notes for details. 
        maxiter : {int, 'defined_by_method'} 
            Maximum number of iterations to perform. 
            If 'defined_by_method', then use method defaults (see notes). 
        full_output : bool 
            Set to True to have all available output in the Results object's 
            mle_retvals attribute. The output is dependent on the solver. 
            See LikelihoodModelResults notes section for more information. 
        disp : bool 
            Set to True to print convergence messages. 
        fargs : tuple 
            Extra arguments passed to the likelihood function, i.e., 
            loglike(x,*args). 
        callback : callable callback(xk) 
            Called after each iteration, as callback(xk), where xk is the 
            current parameter vector. 
        retall : bool 
            Set to True to return list of solutions at each iteration. 
            Available in Results object's mle_retvals attribute. 
        alpha : non-negative scalar or numpy array (same size as parameters) 
            The weight multiplying the l1 penalty term. 
        trim_mode : 'auto, 'size', or 'off' 
            If not 'off', trim (set to zero) parameters that would have been 
            zero if the solver reached the theoretical minimum. 
            If 'auto', trim params using the Theory above. 
            If 'size', trim params if they have very small absolute value. 
        size_trim_tol : float or 'auto' (default = 'auto') 
            Tolerance used when trim_mode == 'size'. 
        auto_trim_tol : float 
            Tolerance used when trim_mode == 'auto'. 
        qc_tol : float 
            Print warning and do not allow auto trim when (ii) (above) is 
            violated by this much. 
        qc_verbose : bool 
            If true, print out a full QC report upon failure. 
        **kwargs 
            Additional keyword arguments used when fitting the model. 
 
        Returns 
        ------- 
        Results 
            A results instance. 
 
        Notes 
        ----- 
        Using 'l1_cvxopt_cp' requires the cvxopt module. 
 
        Extra parameters are not penalized if alpha is given as a scalar. 
        An example is the shape parameter in NegativeBinomial `nb1` and `nb2`. 
 
        Optional arguments for the solvers (available in Results.mle_settings):: 
 
            'l1' 
                acc : float (default 1e-6) 
                    Requested accuracy as used by slsqp 
            'l1_cvxopt_cp' 
                abstol : float 
                    absolute accuracy (default: 1e-7). 
                reltol : float 
                    relative accuracy (default: 1e-6). 
                feastol : float 
                    tolerance for feasibility conditions (default: 1e-7). 
                refinement : int 
                    number of iterative refinement steps when solving KKT 
                    equations (default: 1). 
 
        Optimization methodology 
 
        With :math:`L` the negative log likelihood, we solve the convex but 
        non-smooth problem 
 
        .. math:: \\min_\\beta L(\\beta) + \\sum_k\\alpha_k |\\beta_k| 
 
        via the transformation to the smooth, convex, constrained problem 
        in twice as many variables (adding the &quot;added variables&quot; :math:`u_k`) 
 
        .. math:: \\min_{\\beta,u} L(\\beta) + \\sum_k\\alpha_k u_k, 
 
        subject to 
 
        .. math:: -u_k \\leq \\beta_k \\leq u_k. 
 
        With :math:`\\partial_k L` the derivative of :math:`L` in the 
        :math:`k^{th}` parameter direction, theory dictates that, at the 
        minimum, exactly one of two conditions holds: 
 
        (i) :math:`|\\partial_k L| = \\alpha_k`  and  :math:`\\beta_k \\neq 0` 
        (ii) :math:`|\\partial_k L| \\leq \\alpha_k`  and  :math:`\\beta_k = 0` 
        &quot;&quot;&quot;</span>
        <span class="s1">_validate_l1_method(method)</span>
        <span class="s4"># Set attributes based on method</span>
        <span class="s1">cov_params_func = self.cov_params_func_l1</span>

        <span class="s4">### Bundle up extra kwargs for the dictionary kwargs.  These are</span>
        <span class="s4">### passed through super(...).fit() as kwargs and unpacked at</span>
        <span class="s4">### appropriate times</span>
        <span class="s1">alpha = np.array(alpha)</span>
        <span class="s3">assert </span><span class="s1">alpha.min() &gt;= </span><span class="s5">0</span>
        <span class="s3">try</span><span class="s1">:</span>
            <span class="s1">kwargs[</span><span class="s2">'alpha'</span><span class="s1">] = alpha</span>
        <span class="s3">except </span><span class="s1">TypeError:</span>
            <span class="s1">kwargs = dict(alpha=alpha)</span>
        <span class="s1">kwargs[</span><span class="s2">'alpha_rescaled'</span><span class="s1">] = kwargs[</span><span class="s2">'alpha'</span><span class="s1">] / float(self.endog.shape[</span><span class="s5">0</span><span class="s1">])</span>
        <span class="s1">kwargs[</span><span class="s2">'trim_mode'</span><span class="s1">] = trim_mode</span>
        <span class="s1">kwargs[</span><span class="s2">'size_trim_tol'</span><span class="s1">] = size_trim_tol</span>
        <span class="s1">kwargs[</span><span class="s2">'auto_trim_tol'</span><span class="s1">] = auto_trim_tol</span>
        <span class="s1">kwargs[</span><span class="s2">'qc_tol'</span><span class="s1">] = qc_tol</span>
        <span class="s1">kwargs[</span><span class="s2">'qc_verbose'</span><span class="s1">] = qc_verbose</span>

        <span class="s4">### Define default keyword arguments to be passed to super(...).fit()</span>
        <span class="s3">if </span><span class="s1">maxiter == </span><span class="s2">'defined_by_method'</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">method == </span><span class="s2">'l1'</span><span class="s1">:</span>
                <span class="s1">maxiter = </span><span class="s5">1000</span>
            <span class="s3">elif </span><span class="s1">method == </span><span class="s2">'l1_cvxopt_cp'</span><span class="s1">:</span>
                <span class="s1">maxiter = </span><span class="s5">70</span>

        <span class="s4">## Parameters to pass to super(...).fit()</span>
        <span class="s4"># For the 'extra' parameters, pass all that are available,</span>
        <span class="s4"># even if we know (at this point) we will only use one.</span>
        <span class="s1">extra_fit_funcs = {</span><span class="s2">'l1'</span><span class="s1">: fit_l1_slsqp}</span>
        <span class="s3">if </span><span class="s1">have_cvxopt </span><span class="s3">and </span><span class="s1">method == </span><span class="s2">'l1_cvxopt_cp'</span><span class="s1">:</span>
            <span class="s3">from </span><span class="s1">statsmodels.base.l1_cvxopt </span><span class="s3">import </span><span class="s1">fit_l1_cvxopt_cp</span>
            <span class="s1">extra_fit_funcs[</span><span class="s2">'l1_cvxopt_cp'</span><span class="s1">] = fit_l1_cvxopt_cp</span>
        <span class="s3">elif </span><span class="s1">method.lower() == </span><span class="s2">'l1_cvxopt_cp'</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;Cannot use l1_cvxopt_cp as cvxopt &quot;</span>
                             <span class="s2">&quot;was not found (install it, or use method='l1' instead)&quot;</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">callback </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">callback = self._check_perfect_pred</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">pass  </span><span class="s4"># make a function factory to have multiple call-backs</span>

        <span class="s1">mlefit = super().fit(start_params=start_params</span><span class="s3">,</span>
                             <span class="s1">method=method</span><span class="s3">,</span>
                             <span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                             <span class="s1">full_output=full_output</span><span class="s3">,</span>
                             <span class="s1">disp=disp</span><span class="s3">,</span>
                             <span class="s1">callback=callback</span><span class="s3">,</span>
                             <span class="s1">extra_fit_funcs=extra_fit_funcs</span><span class="s3">,</span>
                             <span class="s1">cov_params_func=cov_params_func</span><span class="s3">,</span>
                             <span class="s1">**kwargs)</span>

        <span class="s3">return </span><span class="s1">mlefit  </span><span class="s4"># up to subclasses to wrap results</span>

    <span class="s3">def </span><span class="s1">cov_params_func_l1(self</span><span class="s3">, </span><span class="s1">likelihood_model</span><span class="s3">, </span><span class="s1">xopt</span><span class="s3">, </span><span class="s1">retvals):</span>
        <span class="s0">&quot;&quot;&quot; 
        Computes cov_params on a reduced parameter space 
        corresponding to the nonzero parameters resulting from the 
        l1 regularized fit. 
 
        Returns a full cov_params matrix, with entries corresponding 
        to zero'd values set to np.nan. 
        &quot;&quot;&quot;</span>
        <span class="s1">H = likelihood_model.hessian(xopt)</span>
        <span class="s1">trimmed = retvals[</span><span class="s2">'trimmed'</span><span class="s1">]</span>
        <span class="s1">nz_idx = np.nonzero(~trimmed)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">nnz_params = (~trimmed).sum()</span>
        <span class="s3">if </span><span class="s1">nnz_params &gt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">H_restricted = H[nz_idx[:</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">nz_idx]</span>
            <span class="s4"># Covariance estimate for the nonzero params</span>
            <span class="s1">H_restricted_inv = np.linalg.inv(-H_restricted)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">H_restricted_inv = np.zeros(</span><span class="s5">0</span><span class="s1">)</span>

        <span class="s1">cov_params = np.nan * np.ones(H.shape)</span>
        <span class="s1">cov_params[nz_idx[:</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">nz_idx] = H_restricted_inv</span>

        <span class="s3">return </span><span class="s1">cov_params</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">which=</span><span class="s2">&quot;mean&quot;</span><span class="s3">, </span><span class="s1">linear=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Predict response variable of a model given exogenous variables. 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">def </span><span class="s1">_derivative_exog(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">dummy_idx=</span><span class="s3">None,</span>
                         <span class="s1">count_idx=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        This should implement the derivative of the non-linear function 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">def </span><span class="s1">_derivative_exog_helper(self</span><span class="s3">, </span><span class="s1">margeff</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">dummy_idx</span><span class="s3">,</span>
                                <span class="s1">count_idx</span><span class="s3">, </span><span class="s1">transform):</span>
        <span class="s0">&quot;&quot;&quot; 
        Helper for _derivative_exog to wrap results appropriately 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">.discrete_margins </span><span class="s3">import </span><span class="s1">_get_count_effects</span><span class="s3">, </span><span class="s1">_get_dummy_effects</span>

        <span class="s3">if </span><span class="s1">count_idx </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">margeff = _get_count_effects(margeff</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">count_idx</span><span class="s3">, </span><span class="s1">transform</span><span class="s3">,</span>
                                         <span class="s1">self</span><span class="s3">, </span><span class="s1">params)</span>
        <span class="s3">if </span><span class="s1">dummy_idx </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">margeff = _get_dummy_effects(margeff</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">dummy_idx</span><span class="s3">, </span><span class="s1">transform</span><span class="s3">,</span>
                                         <span class="s1">self</span><span class="s3">, </span><span class="s1">params)</span>

        <span class="s3">return </span><span class="s1">margeff</span>


<span class="s3">class </span><span class="s1">BinaryModel(DiscreteModel):</span>
    <span class="s1">_continuous_ok = </span><span class="s3">False</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">offset=</span><span class="s3">None, </span><span class="s1">check_rank=</span><span class="s3">True, </span><span class="s1">**kwargs):</span>
        <span class="s4"># unconditional check, requires no extra kwargs added by subclasses</span>
        <span class="s1">self._check_kwargs(kwargs)</span>
        <span class="s1">super().__init__(endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">offset=offset</span><span class="s3">, </span><span class="s1">check_rank=check_rank</span><span class="s3">,</span>
                         <span class="s1">**kwargs)</span>
        <span class="s3">if not </span><span class="s1">issubclass(self.__class__</span><span class="s3">, </span><span class="s1">MultinomialModel):</span>
            <span class="s3">if not </span><span class="s1">np.all((self.endog &gt;= </span><span class="s5">0</span><span class="s1">) &amp; (self.endog &lt;= </span><span class="s5">1</span><span class="s1">)):</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;endog must be in the unit interval.&quot;</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">offset </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">delattr(self</span><span class="s3">, </span><span class="s2">'offset'</span><span class="s1">)</span>

            <span class="s3">if </span><span class="s1">(</span><span class="s3">not </span><span class="s1">self._continuous_ok </span><span class="s3">and</span>
                    <span class="s1">np.any(self.endog != np.round(self.endog))):</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;endog must be binary, either 0 or 1&quot;</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">which=</span><span class="s2">&quot;mean&quot;</span><span class="s3">, </span><span class="s1">linear=</span><span class="s3">None,</span>
                <span class="s1">offset=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Predict response variable of a model given exogenous variables. 
 
        Parameters 
        ---------- 
        params : array_like 
            Fitted parameters of the model. 
        exog : array_like 
            1d or 2d array of exogenous values.  If not supplied, the 
            whole exog attribute of the model is used. 
        which : {'mean', 'linear', 'var', 'prob'}, optional 
            Statistic to predict. Default is 'mean'. 
 
            - 'mean' returns the conditional expectation of endog E(y | x), 
              i.e. exp of linear predictor. 
            - 'linear' returns the linear predictor of the mean function. 
            - 'var' returns the estimated variance of endog implied by the 
              model. 
 
            .. versionadded: 0.14 
 
               ``which`` replaces and extends the deprecated ``linear`` 
               argument. 
 
        linear : bool 
            If True, returns the linear predicted values.  If False or None, 
            then the statistic specified by ``which`` will be returned. 
 
            .. deprecated: 0.14 
 
               The ``linear` keyword is deprecated and will be removed, 
               use ``which`` keyword instead. 
 
        Returns 
        ------- 
        array 
            Fitted values at exog. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">linear </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">msg = </span><span class="s2">'linear keyword is deprecated, use which=&quot;linear&quot;'</span>
            <span class="s1">warnings.warn(msg</span><span class="s3">, </span><span class="s1">FutureWarning)</span>
            <span class="s3">if </span><span class="s1">linear </span><span class="s3">is True</span><span class="s1">:</span>
                <span class="s1">which = </span><span class="s2">&quot;linear&quot;</span>

        <span class="s4"># Use fit offset if appropriate</span>
        <span class="s3">if </span><span class="s1">offset </span><span class="s3">is None and </span><span class="s1">exog </span><span class="s3">is None and </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s2">'offset'</span><span class="s1">):</span>
            <span class="s1">offset = self.offset</span>
        <span class="s3">elif </span><span class="s1">offset </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">offset = </span><span class="s5">0.</span>

        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>

        <span class="s1">linpred = np.dot(exog</span><span class="s3">, </span><span class="s1">params) + offset</span>

        <span class="s3">if </span><span class="s1">which == </span><span class="s2">&quot;mean&quot;</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">self.cdf(linpred)</span>
        <span class="s3">elif </span><span class="s1">which == </span><span class="s2">&quot;linear&quot;</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">linpred</span>
        <span class="s3">if </span><span class="s1">which == </span><span class="s2">&quot;var&quot;</span><span class="s1">:</span>
            <span class="s1">mu = self.cdf(linpred)</span>
            <span class="s1">var_ = mu * (</span><span class="s5">1 </span><span class="s1">- mu)</span>
            <span class="s3">return </span><span class="s1">var_</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s2">'Only `which` is &quot;mean&quot;, &quot;linear&quot; or &quot;var&quot; are'</span>
                             <span class="s2">' available.'</span><span class="s1">)</span>

    <span class="s1">@Appender(DiscreteModel.fit_regularized.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit_regularized(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'l1'</span><span class="s3">,</span>
            <span class="s1">maxiter=</span><span class="s2">'defined_by_method'</span><span class="s3">, </span><span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None,</span>
            <span class="s1">alpha=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">trim_mode=</span><span class="s2">'auto'</span><span class="s3">, </span><span class="s1">auto_trim_tol=</span><span class="s5">0.01</span><span class="s3">, </span><span class="s1">size_trim_tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
            <span class="s1">qc_tol=</span><span class="s5">0.03</span><span class="s3">, </span><span class="s1">**kwargs):</span>

        <span class="s1">_validate_l1_method(method)</span>

        <span class="s1">bnryfit = super().fit_regularized(start_params=start_params</span><span class="s3">,</span>
                                          <span class="s1">method=method</span><span class="s3">,</span>
                                          <span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                                          <span class="s1">full_output=full_output</span><span class="s3">,</span>
                                          <span class="s1">disp=disp</span><span class="s3">,</span>
                                          <span class="s1">callback=callback</span><span class="s3">,</span>
                                          <span class="s1">alpha=alpha</span><span class="s3">,</span>
                                          <span class="s1">trim_mode=trim_mode</span><span class="s3">,</span>
                                          <span class="s1">auto_trim_tol=auto_trim_tol</span><span class="s3">,</span>
                                          <span class="s1">size_trim_tol=size_trim_tol</span><span class="s3">,</span>
                                          <span class="s1">qc_tol=qc_tol</span><span class="s3">,</span>
                                          <span class="s1">**kwargs)</span>

        <span class="s1">discretefit = L1BinaryResults(self</span><span class="s3">, </span><span class="s1">bnryfit)</span>
        <span class="s3">return </span><span class="s1">L1BinaryResultsWrapper(discretefit)</span>

    <span class="s3">def </span><span class="s1">fit_constrained(self</span><span class="s3">, </span><span class="s1">constraints</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">**fit_kwds):</span>

        <span class="s1">res = fit_constrained_wrap(self</span><span class="s3">, </span><span class="s1">constraints</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None,</span>
                                   <span class="s1">**fit_kwds)</span>
        <span class="s3">return </span><span class="s1">res</span>

    <span class="s1">fit_constrained.__doc__ = fit_constrained_wrap.__doc__</span>

    <span class="s3">def </span><span class="s1">_derivative_predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">transform=</span><span class="s2">'dydx'</span><span class="s3">,</span>
                            <span class="s1">offset=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        For computing marginal effects standard errors. 
 
        This is used only in the case of discrete and count regressors to 
        get the variance-covariance of the marginal effects. It returns 
        [d F / d params] where F is the predict. 
 
        Transform can be 'dydx' or 'eydx'. Checking is done in margeff 
        computations for appropriate transform. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>
        <span class="s1">linpred = self.predict(params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">offset=offset</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s1">dF = self.pdf(linpred)[:</span><span class="s3">,None</span><span class="s1">] * exog</span>
        <span class="s3">if </span><span class="s2">'ey' </span><span class="s3">in </span><span class="s1">transform:</span>
            <span class="s1">dF /= self.predict(params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">offset=offset)[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s3">return </span><span class="s1">dF</span>

    <span class="s3">def </span><span class="s1">_derivative_exog(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">transform=</span><span class="s2">'dydx'</span><span class="s3">,</span>
                         <span class="s1">dummy_idx=</span><span class="s3">None, </span><span class="s1">count_idx=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        For computing marginal effects returns dF(XB) / dX where F(.) is 
        the predicted probabilities 
 
        transform can be 'dydx', 'dyex', 'eydx', or 'eyex'. 
 
        Not all of these make sense in the presence of discrete regressors, 
        but checks are done in the results in get_margeff. 
        &quot;&quot;&quot;</span>
        <span class="s4"># Note: this form should be appropriate for</span>
        <span class="s4">#   group 1 probit, logit, logistic, cloglog, heckprob, xtprobit</span>
        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>

        <span class="s1">linpred = self.predict(params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">offset=offset</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s1">margeff = np.dot(self.pdf(linpred)[:</span><span class="s3">,None</span><span class="s1">]</span><span class="s3">,</span>
                         <span class="s1">params[</span><span class="s3">None,</span><span class="s1">:])</span>

        <span class="s3">if </span><span class="s2">'ex' </span><span class="s3">in </span><span class="s1">transform:</span>
            <span class="s1">margeff *= exog</span>
        <span class="s3">if </span><span class="s2">'ey' </span><span class="s3">in </span><span class="s1">transform:</span>
            <span class="s1">margeff /= self.predict(params</span><span class="s3">, </span><span class="s1">exog)[:</span><span class="s3">, None</span><span class="s1">]</span>

        <span class="s3">return </span><span class="s1">self._derivative_exog_helper(margeff</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">,</span>
                                            <span class="s1">dummy_idx</span><span class="s3">, </span><span class="s1">count_idx</span><span class="s3">, </span><span class="s1">transform)</span>

    <span class="s3">def </span><span class="s1">_deriv_mean_dparams(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Derivative of the expected endog with respect to the parameters. 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which score is evaluated 
 
        Returns 
        ------- 
        The value of the derivative of the expected endog with respect 
        to the parameter vector. 
        &quot;&quot;&quot;</span>
        <span class="s1">link = self.link</span>
        <span class="s1">lin_pred = self.predict(params</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s1">idl = link.inverse_deriv(lin_pred)</span>
        <span class="s1">dmat = self.exog * idl[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s3">return </span><span class="s1">dmat</span>

    <span class="s3">def </span><span class="s1">get_distribution(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Get frozen instance of distribution based on predicted parameters. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
        exog : ndarray, optional 
            Explanatory variables for the main count model. 
            If ``exog`` is None, then the data from the model will be used. 
        offset : ndarray, optional 
            Offset is added to the linear predictor of the mean function with 
            coefficient equal to 1. 
            Default is zero if exog is not None, and the model offset if exog 
            is None. 
        exposure : ndarray, optional 
            Log(exposure) is added to the linear predictor  of the mean 
            function with coefficient equal to 1. If exposure is specified, 
            then it will be logged by the method. The user does not need to 
            log it first. 
            Default is one if exog is is not None, and it is the model exposure 
            if exog is None. 
 
        Returns 
        ------- 
        Instance of frozen scipy distribution. 
        &quot;&quot;&quot;</span>
        <span class="s1">mu = self.predict(params</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">offset=offset)</span>
        <span class="s4"># distr = stats.bernoulli(mu[:, None])</span>
        <span class="s1">distr = stats.bernoulli(mu)</span>
        <span class="s3">return </span><span class="s1">distr</span>


<span class="s3">class </span><span class="s1">MultinomialModel(BinaryModel):</span>

    <span class="s3">def </span><span class="s1">_handle_data(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">missing</span><span class="s3">, </span><span class="s1">hasconst</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s3">if </span><span class="s1">data_tools._is_using_ndarray_type(endog</span><span class="s3">, None</span><span class="s1">):</span>
            <span class="s1">endog_dummies</span><span class="s3">, </span><span class="s1">ynames = _numpy_to_dummies(endog)</span>
            <span class="s1">yname = </span><span class="s2">'y'</span>
        <span class="s3">elif </span><span class="s1">data_tools._is_using_pandas(endog</span><span class="s3">, None</span><span class="s1">):</span>
            <span class="s1">endog_dummies</span><span class="s3">, </span><span class="s1">ynames</span><span class="s3">, </span><span class="s1">yname = _pandas_to_dummies(endog)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">endog = np.asarray(endog)</span>
            <span class="s1">endog_dummies</span><span class="s3">, </span><span class="s1">ynames = _numpy_to_dummies(endog)</span>
            <span class="s1">yname = </span><span class="s2">'y'</span>

        <span class="s3">if not </span><span class="s1">isinstance(ynames</span><span class="s3">, </span><span class="s1">dict):</span>
            <span class="s1">ynames = dict(zip(range(endog_dummies.shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">ynames))</span>

        <span class="s1">self._ynames_map = ynames</span>
        <span class="s1">data = handle_data(endog_dummies</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">missing</span><span class="s3">, </span><span class="s1">hasconst</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s1">data.ynames = yname  </span><span class="s4"># overwrite this to single endog name</span>
        <span class="s1">data.orig_endog = endog</span>
        <span class="s1">self.wendog = data.endog</span>

        <span class="s4"># repeating from upstream...</span>
        <span class="s3">for </span><span class="s1">key </span><span class="s3">in </span><span class="s1">kwargs:</span>
            <span class="s3">if </span><span class="s1">key </span><span class="s3">in </span><span class="s1">[</span><span class="s2">'design_info'</span><span class="s3">, </span><span class="s2">'formula'</span><span class="s1">]:  </span><span class="s4"># leave attached to data</span>
                <span class="s3">continue</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s1">setattr(self</span><span class="s3">, </span><span class="s1">key</span><span class="s3">, </span><span class="s1">data.__dict__.pop(key))</span>
            <span class="s3">except </span><span class="s1">KeyError:</span>
                <span class="s3">pass</span>
        <span class="s3">return </span><span class="s1">data</span>

    <span class="s3">def </span><span class="s1">initialize(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Preprocesses the data for MNLogit. 
        &quot;&quot;&quot;</span>
        <span class="s1">super().initialize()</span>
        <span class="s4"># This is also a &quot;whiten&quot; method in other models (eg regression)</span>
        <span class="s1">self.endog = self.endog.argmax(</span><span class="s5">1</span><span class="s1">)  </span><span class="s4"># turn it into an array of col idx</span>
        <span class="s1">self.J = self.wendog.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">self.K = self.exog.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">self.df_model *= (self.J-</span><span class="s5">1</span><span class="s1">)  </span><span class="s4"># for each J - 1 equation.</span>
        <span class="s1">self.df_resid = self.exog.shape[</span><span class="s5">0</span><span class="s1">] - self.df_model - (self.J-</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">which=</span><span class="s2">&quot;mean&quot;</span><span class="s3">, </span><span class="s1">linear=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Predict response variable of a model given exogenous variables. 
 
        Parameters 
        ---------- 
        params : array_like 
            2d array of fitted parameters of the model. Should be in the 
            order returned from the model. 
        exog : array_like 
            1d or 2d array of exogenous values.  If not supplied, the 
            whole exog attribute of the model is used. If a 1d array is given 
            it assumed to be 1 row of exogenous variables. If you only have 
            one regressor and would like to do prediction, you must provide 
            a 2d array with shape[1] == 1. 
        which : {'mean', 'linear', 'var', 'prob'}, optional 
            Statistic to predict. Default is 'mean'. 
 
            - 'mean' returns the conditional expectation of endog E(y | x), 
              i.e. exp of linear predictor. 
            - 'linear' returns the linear predictor of the mean function. 
            - 'var' returns the estimated variance of endog implied by the 
              model. 
 
            .. versionadded: 0.14 
 
               ``which`` replaces and extends the deprecated ``linear`` 
               argument. 
 
        linear : bool 
            If True, returns the linear predicted values.  If False or None, 
            then the statistic specified by ``which`` will be returned. 
 
            .. deprecated: 0.14 
 
               The ``linear` keyword is deprecated and will be removed, 
               use ``which`` keyword instead. 
 
        Notes 
        ----- 
        Column 0 is the base case, the rest conform to the rows of params 
        shifted up one for the base case. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">linear </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">msg = </span><span class="s2">'linear keyword is deprecated, use which=&quot;linear&quot;'</span>
            <span class="s1">warnings.warn(msg</span><span class="s3">, </span><span class="s1">FutureWarning)</span>
            <span class="s3">if </span><span class="s1">linear </span><span class="s3">is True</span><span class="s1">:</span>
                <span class="s1">which = </span><span class="s2">&quot;linear&quot;</span>

        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">: </span><span class="s4"># do here to accommodate user-given exog</span>
            <span class="s1">exog = self.exog</span>
        <span class="s3">if </span><span class="s1">exog.ndim == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">exog = exog[</span><span class="s3">None</span><span class="s1">]</span>

        <span class="s1">pred = super().predict(params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">which=which)</span>
        <span class="s3">if </span><span class="s1">which == </span><span class="s2">&quot;linear&quot;</span><span class="s1">:</span>
            <span class="s1">pred = np.column_stack((np.zeros(len(exog))</span><span class="s3">, </span><span class="s1">pred))</span>
        <span class="s3">return </span><span class="s1">pred</span>

    <span class="s1">@Appender(DiscreteModel.fit.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'newton'</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s5">35</span><span class="s3">,</span>
            <span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s3">if </span><span class="s1">start_params </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">start_params = np.zeros((self.K * (self.J-</span><span class="s5">1</span><span class="s1">)))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">start_params = np.asarray(start_params)</span>

        <span class="s3">if </span><span class="s1">callback </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s4"># placeholder until check_perfect_pred</span>
            <span class="s1">callback = </span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">*args : </span><span class="s3">None</span>
        <span class="s4"># skip calling super to handle results from LikelihoodModel</span>
        <span class="s1">mnfit = base.LikelihoodModel.fit(self</span><span class="s3">, </span><span class="s1">start_params = start_params</span><span class="s3">,</span>
                <span class="s1">method=method</span><span class="s3">, </span><span class="s1">maxiter=maxiter</span><span class="s3">, </span><span class="s1">full_output=full_output</span><span class="s3">,</span>
                <span class="s1">disp=disp</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s1">mnfit.params = mnfit.params.reshape(self.K</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">order=</span><span class="s2">'F'</span><span class="s1">)</span>
        <span class="s1">mnfit = MultinomialResults(self</span><span class="s3">, </span><span class="s1">mnfit)</span>
        <span class="s3">return </span><span class="s1">MultinomialResultsWrapper(mnfit)</span>

    <span class="s1">@Appender(DiscreteModel.fit_regularized.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit_regularized(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'l1'</span><span class="s3">,</span>
            <span class="s1">maxiter=</span><span class="s2">'defined_by_method'</span><span class="s3">, </span><span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None,</span>
            <span class="s1">alpha=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">trim_mode=</span><span class="s2">'auto'</span><span class="s3">, </span><span class="s1">auto_trim_tol=</span><span class="s5">0.01</span><span class="s3">, </span><span class="s1">size_trim_tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
            <span class="s1">qc_tol=</span><span class="s5">0.03</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s3">if </span><span class="s1">start_params </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">start_params = np.zeros((self.K * (self.J-</span><span class="s5">1</span><span class="s1">)))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">start_params = np.asarray(start_params)</span>
        <span class="s1">mnfit = DiscreteModel.fit_regularized(</span>
                <span class="s1">self</span><span class="s3">, </span><span class="s1">start_params=start_params</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                <span class="s1">full_output=full_output</span><span class="s3">, </span><span class="s1">disp=disp</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">,</span>
                <span class="s1">alpha=alpha</span><span class="s3">, </span><span class="s1">trim_mode=trim_mode</span><span class="s3">, </span><span class="s1">auto_trim_tol=auto_trim_tol</span><span class="s3">,</span>
                <span class="s1">size_trim_tol=size_trim_tol</span><span class="s3">, </span><span class="s1">qc_tol=qc_tol</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s1">mnfit.params = mnfit.params.reshape(self.K</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">order=</span><span class="s2">'F'</span><span class="s1">)</span>
        <span class="s1">mnfit = L1MultinomialResults(self</span><span class="s3">, </span><span class="s1">mnfit)</span>
        <span class="s3">return </span><span class="s1">L1MultinomialResultsWrapper(mnfit)</span>

    <span class="s3">def </span><span class="s1">_derivative_predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">transform=</span><span class="s2">'dydx'</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        For computing marginal effects standard errors. 
 
        This is used only in the case of discrete and count regressors to 
        get the variance-covariance of the marginal effects. It returns 
        [d F / d params] where F is the predicted probabilities for each 
        choice. dFdparams is of shape nobs x (J*K) x (J-1)*K. 
        The zero derivatives for the base category are not included. 
 
        Transform can be 'dydx' or 'eydx'. Checking is done in margeff 
        computations for appropriate transform. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>
        <span class="s3">if </span><span class="s1">params.ndim == </span><span class="s5">1</span><span class="s1">: </span><span class="s4"># will get flatted from approx_fprime</span>
            <span class="s1">params = params.reshape(self.K</span><span class="s3">, </span><span class="s1">self.J-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">order=</span><span class="s2">'F'</span><span class="s1">)</span>

        <span class="s1">eXB = np.exp(np.dot(exog</span><span class="s3">, </span><span class="s1">params))</span>
        <span class="s1">sum_eXB = (</span><span class="s5">1 </span><span class="s1">+ eXB.sum(</span><span class="s5">1</span><span class="s1">))[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s1">J = int(self.J)</span>
        <span class="s1">K = int(self.K)</span>
        <span class="s1">repeat_eXB = np.repeat(eXB</span><span class="s3">, </span><span class="s1">J</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">X = np.tile(exog</span><span class="s3">, </span><span class="s1">J-</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s4"># this is the derivative wrt the base level</span>
        <span class="s1">F0 = -repeat_eXB * X / sum_eXB ** </span><span class="s5">2</span>
        <span class="s4"># this is the derivative wrt the other levels when</span>
        <span class="s4"># dF_j / dParams_j (ie., own equation)</span>
        <span class="s4">#NOTE: this computes too much, any easy way to cut down?</span>
        <span class="s1">F1 = eXB.T[:</span><span class="s3">,</span><span class="s1">:</span><span class="s3">,None</span><span class="s1">]*X * (sum_eXB - repeat_eXB) / (sum_eXB**</span><span class="s5">2</span><span class="s1">)</span>
        <span class="s1">F1 = F1.transpose((</span><span class="s5">1</span><span class="s3">,</span><span class="s5">0</span><span class="s3">,</span><span class="s5">2</span><span class="s1">)) </span><span class="s4"># put the nobs index first</span>

        <span class="s4"># other equation index</span>
        <span class="s1">other_idx = ~np.kron(np.eye(J-</span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.ones(K)).astype(bool)</span>
        <span class="s1">F1[:</span><span class="s3">, </span><span class="s1">other_idx] = (-eXB.T[:</span><span class="s3">,</span><span class="s1">:</span><span class="s3">,None</span><span class="s1">]*X*repeat_eXB / \</span>
                           <span class="s1">(sum_eXB**</span><span class="s5">2</span><span class="s1">)).transpose((</span><span class="s5">1</span><span class="s3">,</span><span class="s5">0</span><span class="s3">,</span><span class="s5">2</span><span class="s1">))[:</span><span class="s3">, </span><span class="s1">other_idx]</span>
        <span class="s1">dFdX = np.concatenate((F0[:</span><span class="s3">, None,</span><span class="s1">:]</span><span class="s3">, </span><span class="s1">F1)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s2">'ey' </span><span class="s3">in </span><span class="s1">transform:</span>
            <span class="s1">dFdX /= self.predict(params</span><span class="s3">, </span><span class="s1">exog)[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s3">return </span><span class="s1">dFdX</span>

    <span class="s3">def </span><span class="s1">_derivative_exog(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">transform=</span><span class="s2">'dydx'</span><span class="s3">,</span>
                         <span class="s1">dummy_idx=</span><span class="s3">None, </span><span class="s1">count_idx=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        For computing marginal effects returns dF(XB) / dX where F(.) is 
        the predicted probabilities 
 
        transform can be 'dydx', 'dyex', 'eydx', or 'eyex'. 
 
        Not all of these make sense in the presence of discrete regressors, 
        but checks are done in the results in get_margeff. 
 
        For Multinomial models the marginal effects are 
 
        P[j] * (params[j] - sum_k P[k]*params[k]) 
 
        It is returned unshaped, so that each row contains each of the J 
        equations. This makes it easier to take derivatives of this for 
        standard errors. If you want average marginal effects you can do 
        margeff.reshape(nobs, K, J, order='F).mean(0) and the marginal effects 
        for choice J are in column J 
        &quot;&quot;&quot;</span>
        <span class="s1">J = int(self.J)  </span><span class="s4"># number of alternative choices</span>
        <span class="s1">K = int(self.K)  </span><span class="s4"># number of variables</span>
        <span class="s4"># Note: this form should be appropriate for</span>
        <span class="s4">#   group 1 probit, logit, logistic, cloglog, heckprob, xtprobit</span>
        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>
        <span class="s3">if </span><span class="s1">params.ndim == </span><span class="s5">1</span><span class="s1">:  </span><span class="s4"># will get flatted from approx_fprime</span>
            <span class="s1">params = params.reshape(K</span><span class="s3">, </span><span class="s1">J-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">order=</span><span class="s2">'F'</span><span class="s1">)</span>

        <span class="s1">zeroparams = np.c_[np.zeros(K)</span><span class="s3">, </span><span class="s1">params]  </span><span class="s4"># add base in</span>

        <span class="s1">cdf = self.cdf(np.dot(exog</span><span class="s3">, </span><span class="s1">params))</span>

        <span class="s4"># TODO: meaningful interpretation for `iterm`?</span>
        <span class="s1">iterm = np.array([cdf[:</span><span class="s3">, </span><span class="s1">[i]] * zeroparams[:</span><span class="s3">, </span><span class="s1">i]</span>
                          <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(int(J))]).sum(</span><span class="s5">0</span><span class="s1">)</span>

        <span class="s1">margeff = np.array([cdf[:</span><span class="s3">, </span><span class="s1">[j]] * (zeroparams[:</span><span class="s3">, </span><span class="s1">j] - iterm)</span>
                            <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(J)])</span>

        <span class="s4"># swap the axes to make sure margeff are in order nobs, K, J</span>
        <span class="s1">margeff = np.transpose(margeff</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">0</span><span class="s1">))</span>

        <span class="s3">if </span><span class="s2">'ex' </span><span class="s3">in </span><span class="s1">transform:</span>
            <span class="s1">margeff *= exog</span>
        <span class="s3">if </span><span class="s2">'ey' </span><span class="s3">in </span><span class="s1">transform:</span>
            <span class="s1">margeff /= self.predict(params</span><span class="s3">, </span><span class="s1">exog)[:</span><span class="s3">,None,</span><span class="s1">:]</span>

        <span class="s1">margeff = self._derivative_exog_helper(margeff</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">,</span>
                                               <span class="s1">dummy_idx</span><span class="s3">, </span><span class="s1">count_idx</span><span class="s3">, </span><span class="s1">transform)</span>
        <span class="s3">return </span><span class="s1">margeff.reshape(len(exog)</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">order=</span><span class="s2">'F'</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">get_distribution(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;get frozen instance of distribution 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>


<span class="s3">class </span><span class="s1">CountModel(DiscreteModel):</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">offset=</span><span class="s3">None, </span><span class="s1">exposure=</span><span class="s3">None, </span><span class="s1">missing=</span><span class="s2">'none'</span><span class="s3">,</span>
                 <span class="s1">check_rank=</span><span class="s3">True, </span><span class="s1">**kwargs):</span>
        <span class="s1">self._check_kwargs(kwargs)</span>
        <span class="s1">super().__init__(endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">check_rank</span><span class="s3">, </span><span class="s1">missing=missing</span><span class="s3">,</span>
                         <span class="s1">offset=offset</span><span class="s3">, </span><span class="s1">exposure=exposure</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s3">if </span><span class="s1">exposure </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">self.exposure = np.asarray(self.exposure)</span>
            <span class="s1">self.exposure = np.log(self.exposure)</span>
        <span class="s3">if </span><span class="s1">offset </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">self.offset = np.asarray(self.offset)</span>
        <span class="s1">self._check_inputs(self.offset</span><span class="s3">, </span><span class="s1">self.exposure</span><span class="s3">, </span><span class="s1">self.endog)</span>
        <span class="s3">if </span><span class="s1">offset </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">delattr(self</span><span class="s3">, </span><span class="s2">'offset'</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">exposure </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">delattr(self</span><span class="s3">, </span><span class="s2">'exposure'</span><span class="s1">)</span>

        <span class="s4"># promote dtype to float64 if needed</span>
        <span class="s1">dt = np.promote_types(self.endog.dtype</span><span class="s3">, </span><span class="s1">np.float64)</span>
        <span class="s1">self.endog = np.asarray(self.endog</span><span class="s3">, </span><span class="s1">dt)</span>
        <span class="s1">dt = np.promote_types(self.exog.dtype</span><span class="s3">, </span><span class="s1">np.float64)</span>
        <span class="s1">self.exog = np.asarray(self.exog</span><span class="s3">, </span><span class="s1">dt)</span>


    <span class="s3">def </span><span class="s1">_check_inputs(self</span><span class="s3">, </span><span class="s1">offset</span><span class="s3">, </span><span class="s1">exposure</span><span class="s3">, </span><span class="s1">endog):</span>
        <span class="s3">if </span><span class="s1">offset </span><span class="s3">is not None and </span><span class="s1">offset.shape[</span><span class="s5">0</span><span class="s1">] != endog.shape[</span><span class="s5">0</span><span class="s1">]:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;offset is not the same length as endog&quot;</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">exposure </span><span class="s3">is not None and </span><span class="s1">exposure.shape[</span><span class="s5">0</span><span class="s1">] != endog.shape[</span><span class="s5">0</span><span class="s1">]:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;exposure is not the same length as endog&quot;</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_get_init_kwds(self):</span>
        <span class="s4"># this is a temporary fixup because exposure has been transformed</span>
        <span class="s4"># see #1609</span>
        <span class="s1">kwds = super()._get_init_kwds()</span>
        <span class="s3">if </span><span class="s2">'exposure' </span><span class="s3">in </span><span class="s1">kwds </span><span class="s3">and </span><span class="s1">kwds[</span><span class="s2">'exposure'</span><span class="s1">] </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">kwds[</span><span class="s2">'exposure'</span><span class="s1">] = np.exp(kwds[</span><span class="s2">'exposure'</span><span class="s1">])</span>
        <span class="s3">return </span><span class="s1">kwds</span>

    <span class="s3">def </span><span class="s1">_get_predict_arrays(self</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None, </span><span class="s1">exposure=</span><span class="s3">None</span><span class="s1">):</span>

        <span class="s4"># convert extras if not None</span>
        <span class="s3">if </span><span class="s1">exposure </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">exposure = np.log(np.asarray(exposure))</span>
        <span class="s3">if </span><span class="s1">offset </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">offset = np.asarray(offset)</span>

        <span class="s4"># get defaults</span>
        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s4"># prediction is in-sample</span>
            <span class="s1">exog = self.exog</span>
            <span class="s3">if </span><span class="s1">exposure </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">exposure = getattr(self</span><span class="s3">, </span><span class="s2">'exposure'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">offset </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">'offset'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s4"># user specified</span>
            <span class="s1">exog = np.asarray(exog)</span>
            <span class="s3">if </span><span class="s1">exposure </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">exposure = </span><span class="s5">0</span>
            <span class="s3">if </span><span class="s1">offset </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">offset = </span><span class="s5">0</span>

        <span class="s3">return </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">offset</span><span class="s3">, </span><span class="s1">exposure</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exposure=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None,</span>
                <span class="s1">which=</span><span class="s2">'mean'</span><span class="s3">, </span><span class="s1">linear=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Predict response variable of a count model given exogenous variables 
 
        Parameters 
        ---------- 
        params : array_like 
            Model parameters 
        exog : array_like, optional 
            Design / exogenous data. Is exog is None, model exog is used. 
        exposure : array_like, optional 
            Log(exposure) is added to the linear prediction with 
            coefficient equal to 1. If exposure is not provided and exog 
            is None, uses the model's exposure if present.  If not, uses 
            0 as the default value. 
        offset : array_like, optional 
            Offset is added to the linear prediction with coefficient 
            equal to 1. If offset is not provided and exog 
            is None, uses the model's offset if present.  If not, uses 
            0 as the default value. 
        which : 'mean', 'linear', 'var', 'prob' (optional) 
            Statitistic to predict. Default is 'mean'. 
 
            - 'mean' returns the conditional expectation of endog E(y | x), 
              i.e. exp of linear predictor. 
            - 'linear' returns the linear predictor of the mean function. 
            - 'var' variance of endog implied by the likelihood model 
            - 'prob' predicted probabilities for counts. 
 
            .. versionadded: 0.14 
 
               ``which`` replaces and extends the deprecated ``linear`` 
               argument. 
 
        linear : bool 
            If True, returns the linear predicted values.  If False or None, 
            then the statistic specified by ``which`` will be returned. 
 
            .. deprecated: 0.14 
 
               The ``linear` keyword is deprecated and will be removed, 
               use ``which`` keyword instead. 
 
 
        Notes 
        ----- 
        If exposure is specified, then it will be logged by the method. 
        The user does not need to log it first. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">linear </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">msg = </span><span class="s2">'linear keyword is deprecated, use which=&quot;linear&quot;'</span>
            <span class="s1">warnings.warn(msg</span><span class="s3">, </span><span class="s1">FutureWarning)</span>
            <span class="s3">if </span><span class="s1">linear </span><span class="s3">is True</span><span class="s1">:</span>
                <span class="s1">which = </span><span class="s2">&quot;linear&quot;</span>

        <span class="s4"># the following is copied from GLM predict (without family/link check)</span>
        <span class="s4"># Use fit offset if appropriate</span>
        <span class="s3">if </span><span class="s1">offset </span><span class="s3">is None and </span><span class="s1">exog </span><span class="s3">is None and </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s2">'offset'</span><span class="s1">):</span>
            <span class="s1">offset = self.offset</span>
        <span class="s3">elif </span><span class="s1">offset </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">offset = </span><span class="s5">0.</span>

        <span class="s4"># Use fit exposure if appropriate</span>
        <span class="s3">if </span><span class="s1">exposure </span><span class="s3">is None and </span><span class="s1">exog </span><span class="s3">is None and </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s2">'exposure'</span><span class="s1">):</span>
            <span class="s4"># Already logged</span>
            <span class="s1">exposure = self.exposure</span>
        <span class="s3">elif </span><span class="s1">exposure </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exposure = </span><span class="s5">0.</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">exposure = np.log(exposure)</span>

        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>

        <span class="s1">fitted = np.dot(exog</span><span class="s3">, </span><span class="s1">params[:exog.shape[</span><span class="s5">1</span><span class="s1">]])</span>
        <span class="s1">linpred = fitted + exposure + offset</span>
        <span class="s3">if </span><span class="s1">which == </span><span class="s2">&quot;mean&quot;</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">np.exp(linpred)</span>
        <span class="s3">elif </span><span class="s1">which.startswith(</span><span class="s2">&quot;lin&quot;</span><span class="s1">):</span>
            <span class="s3">return </span><span class="s1">linpred</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s2">'keyword which has to be &quot;mean&quot; and &quot;linear&quot;'</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_derivative_predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">transform=</span><span class="s2">'dydx'</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        For computing marginal effects standard errors. 
 
        This is used only in the case of discrete and count regressors to 
        get the variance-covariance of the marginal effects. It returns 
        [d F / d params] where F is the predict. 
 
        Transform can be 'dydx' or 'eydx'. Checking is done in margeff 
        computations for appropriate transform. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>
        <span class="s4">#NOTE: this handles offset and exposure</span>
        <span class="s1">dF = self.predict(params</span><span class="s3">, </span><span class="s1">exog)[:</span><span class="s3">,None</span><span class="s1">] * exog</span>
        <span class="s3">if </span><span class="s2">'ey' </span><span class="s3">in </span><span class="s1">transform:</span>
            <span class="s1">dF /= self.predict(params</span><span class="s3">, </span><span class="s1">exog)[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s3">return </span><span class="s1">dF</span>

    <span class="s3">def </span><span class="s1">_derivative_exog(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">transform=</span><span class="s2">&quot;dydx&quot;</span><span class="s3">,</span>
                         <span class="s1">dummy_idx=</span><span class="s3">None, </span><span class="s1">count_idx=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        For computing marginal effects. These are the marginal effects 
        d F(XB) / dX 
        For the Poisson model F(XB) is the predicted counts rather than 
        the probabilities. 
 
        transform can be 'dydx', 'dyex', 'eydx', or 'eyex'. 
 
        Not all of these make sense in the presence of discrete regressors, 
        but checks are done in the results in get_margeff. 
        &quot;&quot;&quot;</span>
        <span class="s4"># group 3 poisson, nbreg, zip, zinb</span>
        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>
        <span class="s1">k_extra = getattr(self</span><span class="s3">, </span><span class="s2">'k_extra'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">params_exog = params </span><span class="s3">if </span><span class="s1">k_extra == </span><span class="s5">0 </span><span class="s3">else </span><span class="s1">params[:-k_extra]</span>
        <span class="s1">margeff = self.predict(params</span><span class="s3">, </span><span class="s1">exog)[:</span><span class="s3">,None</span><span class="s1">] * params_exog[</span><span class="s3">None,</span><span class="s1">:]</span>
        <span class="s3">if </span><span class="s2">'ex' </span><span class="s3">in </span><span class="s1">transform:</span>
            <span class="s1">margeff *= exog</span>
        <span class="s3">if </span><span class="s2">'ey' </span><span class="s3">in </span><span class="s1">transform:</span>
            <span class="s1">margeff /= self.predict(params</span><span class="s3">, </span><span class="s1">exog)[:</span><span class="s3">,None</span><span class="s1">]</span>

        <span class="s3">return </span><span class="s1">self._derivative_exog_helper(margeff</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">,</span>
                                            <span class="s1">dummy_idx</span><span class="s3">, </span><span class="s1">count_idx</span><span class="s3">, </span><span class="s1">transform)</span>

    <span class="s3">def </span><span class="s1">_deriv_mean_dparams(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Derivative of the expected endog with respect to the parameters. 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which score is evaluated 
 
        Returns 
        ------- 
        The value of the derivative of the expected endog with respect 
        to the parameter vector. 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.genmod.families </span><span class="s3">import </span><span class="s1">links</span>
        <span class="s1">link = links.Log()</span>
        <span class="s1">lin_pred = self.predict(params</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s1">idl = link.inverse_deriv(lin_pred)</span>
        <span class="s1">dmat = self.exog * idl[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s3">if </span><span class="s1">self.k_extra &gt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">dmat_extra = np.zeros((dmat.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">self.k_extra))</span>
            <span class="s1">dmat = np.column_stack((dmat</span><span class="s3">, </span><span class="s1">dmat_extra))</span>
        <span class="s3">return </span><span class="s1">dmat</span>


    <span class="s1">@Appender(DiscreteModel.fit.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'newton'</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s5">35</span><span class="s3">,</span>
            <span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s1">cntfit = super().fit(start_params=start_params</span><span class="s3">,</span>
                             <span class="s1">method=method</span><span class="s3">,</span>
                             <span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                             <span class="s1">full_output=full_output</span><span class="s3">,</span>
                             <span class="s1">disp=disp</span><span class="s3">,</span>
                             <span class="s1">callback=callback</span><span class="s3">,</span>
                             <span class="s1">**kwargs)</span>
        <span class="s1">discretefit = CountResults(self</span><span class="s3">, </span><span class="s1">cntfit)</span>
        <span class="s3">return </span><span class="s1">CountResultsWrapper(discretefit)</span>

    <span class="s1">@Appender(DiscreteModel.fit_regularized.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit_regularized(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'l1'</span><span class="s3">,</span>
            <span class="s1">maxiter=</span><span class="s2">'defined_by_method'</span><span class="s3">, </span><span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None,</span>
            <span class="s1">alpha=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">trim_mode=</span><span class="s2">'auto'</span><span class="s3">, </span><span class="s1">auto_trim_tol=</span><span class="s5">0.01</span><span class="s3">, </span><span class="s1">size_trim_tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
            <span class="s1">qc_tol=</span><span class="s5">0.03</span><span class="s3">, </span><span class="s1">**kwargs):</span>

        <span class="s1">_validate_l1_method(method)</span>

        <span class="s1">cntfit = super().fit_regularized(start_params=start_params</span><span class="s3">,</span>
                                         <span class="s1">method=method</span><span class="s3">,</span>
                                         <span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                                         <span class="s1">full_output=full_output</span><span class="s3">,</span>
                                         <span class="s1">disp=disp</span><span class="s3">,</span>
                                         <span class="s1">callback=callback</span><span class="s3">,</span>
                                         <span class="s1">alpha=alpha</span><span class="s3">,</span>
                                         <span class="s1">trim_mode=trim_mode</span><span class="s3">, </span><span class="s1">auto_trim_tol=auto_trim_tol</span><span class="s3">,</span>
                                         <span class="s1">size_trim_tol=size_trim_tol</span><span class="s3">,</span>
                                         <span class="s1">qc_tol=qc_tol</span><span class="s3">,</span>
                                         <span class="s1">**kwargs)</span>

        <span class="s1">discretefit = L1CountResults(self</span><span class="s3">, </span><span class="s1">cntfit)</span>
        <span class="s3">return </span><span class="s1">L1CountResultsWrapper(discretefit)</span>


<span class="s4"># Public Model Classes</span>


<span class="s3">class </span><span class="s1">Poisson(CountModel):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Poisson Model 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : ndarray 
        A reference to the endogenous response variable 
    exog : ndarray 
        A reference to the exogenous design. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s3">,</span>
           <span class="s2">'extra_params'</span><span class="s1">:</span>
           <span class="s2">&quot;&quot;&quot;offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    exposure : array_like 
        Log(exposure) is added to the linear prediction with coefficient 
        equal to 1. 
        &quot;&quot;&quot; </span><span class="s1">+ base._missing_param_doc + _check_rank_doc}</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">family(self):</span>
        <span class="s3">from </span><span class="s1">statsmodels.genmod </span><span class="s3">import </span><span class="s1">families</span>
        <span class="s3">return </span><span class="s1">families.Poisson()</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        Poisson model cumulative distribution function 
 
        Parameters 
        ---------- 
        X : array_like 
            `X` is the linear predictor of the model.  See notes. 
 
        Returns 
        ------- 
        The value of the Poisson CDF at each point. 
 
        Notes 
        ----- 
        The CDF is defined as 
 
        .. math:: \\exp\\left(-\\lambda\\right)\\sum_{i=0}^{y}\\frac{\\lambda^{i}}{i!} 
 
        where :math:`\\lambda` assumes the loglinear model. I.e., 
 
        .. math:: \\ln\\lambda_{i}=X\\beta 
 
        The parameter `X` is :math:`X\\beta` in the above formula. 
        &quot;&quot;&quot;</span>
        <span class="s1">y = self.endog</span>
        <span class="s3">return </span><span class="s1">stats.poisson.cdf(y</span><span class="s3">, </span><span class="s1">np.exp(X))</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        Poisson model probability mass function 
 
        Parameters 
        ---------- 
        X : array_like 
            `X` is the linear predictor of the model.  See notes. 
 
        Returns 
        ------- 
        pdf : ndarray 
            The value of the Poisson probability mass function, PMF, for each 
            point of X. 
 
        Notes 
        ----- 
        The PMF is defined as 
 
        .. math:: \\frac{e^{-\\lambda_{i}}\\lambda_{i}^{y_{i}}}{y_{i}!} 
 
        where :math:`\\lambda` assumes the loglinear model. I.e., 
 
        .. math:: \\ln\\lambda_{i}=x_{i}\\beta 
 
        The parameter `X` is :math:`x_{i}\\beta` in the above formula. 
        &quot;&quot;&quot;</span>
        <span class="s1">y = self.endog</span>
        <span class="s3">return </span><span class="s1">np.exp(stats.poisson.logpmf(y</span><span class="s3">, </span><span class="s1">np.exp(X)))</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Loglikelihood of Poisson model 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike : float 
            The log-likelihood function of the model evaluated at `params`. 
            See notes. 
 
        Notes 
        ----- 
        .. math:: \\ln L=\\sum_{i=1}^{n}\\left[-\\lambda_{i}+y_{i}x_{i}^{\\prime}\\beta-\\ln y_{i}!\\right] 
        &quot;&quot;&quot;</span>
        <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">exposure = getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">XB = np.dot(self.exog</span><span class="s3">, </span><span class="s1">params) + offset + exposure</span>
        <span class="s1">endog = self.endog</span>
        <span class="s3">return </span><span class="s1">np.sum(</span>
            <span class="s1">-np.exp(np.clip(XB</span><span class="s3">, None, </span><span class="s1">EXP_UPPER_LIMIT))</span>
            <span class="s1">+ endog * XB</span>
            <span class="s1">- gammaln(endog + </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">loglikeobs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Loglikelihood for observations of Poisson model 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike : array_like 
            The log likelihood for each observation of the model evaluated 
            at `params`. See Notes 
 
        Notes 
        ----- 
        .. math:: \\ln L_{i}=\\left[-\\lambda_{i}+y_{i}x_{i}^{\\prime}\\beta-\\ln y_{i}!\\right] 
 
        for observations :math:`i=1,...,n` 
        &quot;&quot;&quot;</span>
        <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">exposure = getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">XB = np.dot(self.exog</span><span class="s3">, </span><span class="s1">params) + offset + exposure</span>
        <span class="s1">endog = self.endog</span>
        <span class="s4">#np.sum(stats.poisson.logpmf(endog, np.exp(XB)))</span>
        <span class="s3">return </span><span class="s1">-np.exp(XB) +  endog*XB - gammaln(endog+</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s1">@Appender(_get_start_params_null_docs)</span>
    <span class="s3">def </span><span class="s1">_get_start_params_null(self):</span>
        <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">exposure = getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">const = (self.endog / np.exp(offset + exposure)).mean()</span>
        <span class="s1">params = [np.log(const)]</span>
        <span class="s3">return </span><span class="s1">params</span>

    <span class="s1">@Appender(DiscreteModel.fit.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'newton'</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s5">35</span><span class="s3">,</span>
            <span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>

        <span class="s3">if </span><span class="s1">start_params </span><span class="s3">is None and </span><span class="s1">self.data.const_idx </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s4"># k_params or k_exog not available?</span>
            <span class="s1">start_params = </span><span class="s5">0.001 </span><span class="s1">* np.ones(self.exog.shape[</span><span class="s5">1</span><span class="s1">])</span>
            <span class="s1">start_params[self.data.const_idx] = self._get_start_params_null()[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s1">kwds = {}</span>
        <span class="s3">if </span><span class="s1">kwargs.get(</span><span class="s2">'cov_type'</span><span class="s1">) </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">kwds[</span><span class="s2">'cov_type'</span><span class="s1">] = kwargs.get(</span><span class="s2">'cov_type'</span><span class="s1">)</span>
            <span class="s1">kwds[</span><span class="s2">'cov_kwds'</span><span class="s1">] = kwargs.get(</span><span class="s2">'cov_kwds'</span><span class="s3">, </span><span class="s1">{})</span>

        <span class="s1">cntfit = super(CountModel</span><span class="s3">, </span><span class="s1">self).fit(start_params=start_params</span><span class="s3">,</span>
                                             <span class="s1">method=method</span><span class="s3">,</span>
                                             <span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                                             <span class="s1">full_output=full_output</span><span class="s3">,</span>
                                             <span class="s1">disp=disp</span><span class="s3">,</span>
                                             <span class="s1">callback=callback</span><span class="s3">,</span>
                                             <span class="s1">**kwargs)</span>

        <span class="s1">discretefit = PoissonResults(self</span><span class="s3">, </span><span class="s1">cntfit</span><span class="s3">, </span><span class="s1">**kwds)</span>
        <span class="s3">return </span><span class="s1">PoissonResultsWrapper(discretefit)</span>

    <span class="s1">@Appender(DiscreteModel.fit_regularized.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit_regularized(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'l1'</span><span class="s3">,</span>
            <span class="s1">maxiter=</span><span class="s2">'defined_by_method'</span><span class="s3">, </span><span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None,</span>
            <span class="s1">alpha=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">trim_mode=</span><span class="s2">'auto'</span><span class="s3">, </span><span class="s1">auto_trim_tol=</span><span class="s5">0.01</span><span class="s3">, </span><span class="s1">size_trim_tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
            <span class="s1">qc_tol=</span><span class="s5">0.03</span><span class="s3">, </span><span class="s1">**kwargs):</span>

        <span class="s1">_validate_l1_method(method)</span>

        <span class="s1">cntfit = super(CountModel</span><span class="s3">, </span><span class="s1">self).fit_regularized(</span>
                <span class="s1">start_params=start_params</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                <span class="s1">full_output=full_output</span><span class="s3">, </span><span class="s1">disp=disp</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">,</span>
                <span class="s1">alpha=alpha</span><span class="s3">, </span><span class="s1">trim_mode=trim_mode</span><span class="s3">, </span><span class="s1">auto_trim_tol=auto_trim_tol</span><span class="s3">,</span>
                <span class="s1">size_trim_tol=size_trim_tol</span><span class="s3">, </span><span class="s1">qc_tol=qc_tol</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s1">discretefit = L1PoissonResults(self</span><span class="s3">, </span><span class="s1">cntfit)</span>
        <span class="s3">return </span><span class="s1">L1PoissonResultsWrapper(discretefit)</span>

    <span class="s3">def </span><span class="s1">fit_constrained(self</span><span class="s3">, </span><span class="s1">constraints</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">**fit_kwds):</span>
        <span class="s0">&quot;&quot;&quot;fit the model subject to linear equality constraints 
 
        The constraints are of the form   `R params = q` 
        where R is the constraint_matrix and q is the vector of 
        constraint_values. 
 
        The estimation creates a new model with transformed design matrix, 
        exog, and converts the results back to the original parameterization. 
 
        Parameters 
        ---------- 
        constraints : formula expression or tuple 
            If it is a tuple, then the constraint needs to be given by two 
            arrays (constraint_matrix, constraint_value), i.e. (R, q). 
            Otherwise, the constraints can be given as strings or list of 
            strings. 
            see t_test for details 
        start_params : None or array_like 
            starting values for the optimization. `start_params` needs to be 
            given in the original parameter space and are internally 
            transformed. 
        **fit_kwds : keyword arguments 
            fit_kwds are used in the optimization of the transformed model. 
 
        Returns 
        ------- 
        results : Results instance 
        &quot;&quot;&quot;</span>

        <span class="s4">#constraints = (R, q)</span>
        <span class="s4"># TODO: temporary trailing underscore to not overwrite the monkey</span>
        <span class="s4">#       patched version</span>
        <span class="s4"># TODO: decide whether to move the imports</span>
        <span class="s3">from </span><span class="s1">patsy </span><span class="s3">import </span><span class="s1">DesignInfo</span>
        <span class="s3">from </span><span class="s1">statsmodels.base._constraints </span><span class="s3">import </span><span class="s1">(fit_constrained</span><span class="s3">,</span>
                                                   <span class="s1">LinearConstraints)</span>

        <span class="s4"># same pattern as in base.LikelihoodModel.t_test</span>
        <span class="s1">lc = DesignInfo(self.exog_names).linear_constraint(constraints)</span>
        <span class="s1">R</span><span class="s3">, </span><span class="s1">q = lc.coefs</span><span class="s3">, </span><span class="s1">lc.constants</span>

        <span class="s4"># TODO: add start_params option, need access to tranformation</span>
        <span class="s4">#       fit_constrained needs to do the transformation</span>
        <span class="s1">params</span><span class="s3">, </span><span class="s1">cov</span><span class="s3">, </span><span class="s1">res_constr = fit_constrained(self</span><span class="s3">, </span><span class="s1">R</span><span class="s3">, </span><span class="s1">q</span><span class="s3">,</span>
                                                  <span class="s1">start_params=start_params</span><span class="s3">,</span>
                                                  <span class="s1">fit_kwds=fit_kwds)</span>
        <span class="s4">#create dummy results Instance, TODO: wire up properly</span>
        <span class="s1">res = self.fit(maxiter=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">method=</span><span class="s2">'nm'</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">0</span><span class="s3">,</span>
                       <span class="s1">warn_convergence=</span><span class="s3">False</span><span class="s1">) </span><span class="s4"># we get a wrapper back</span>
        <span class="s1">res.mle_retvals[</span><span class="s2">'fcall'</span><span class="s1">] = res_constr.mle_retvals.get(</span><span class="s2">'fcall'</span><span class="s3">, </span><span class="s1">np.nan)</span>
        <span class="s1">res.mle_retvals[</span><span class="s2">'iterations'</span><span class="s1">] = res_constr.mle_retvals.get(</span>
                                                        <span class="s2">'iterations'</span><span class="s3">, </span><span class="s1">np.nan)</span>
        <span class="s1">res.mle_retvals[</span><span class="s2">'converged'</span><span class="s1">] = res_constr.mle_retvals[</span><span class="s2">'converged'</span><span class="s1">]</span>
        <span class="s1">res._results.params = params</span>
        <span class="s1">res._results.cov_params_default = cov</span>
        <span class="s1">cov_type = fit_kwds.get(</span><span class="s2">'cov_type'</span><span class="s3">, </span><span class="s2">'nonrobust'</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">cov_type != </span><span class="s2">'nonrobust'</span><span class="s1">:</span>
            <span class="s1">res._results.normalized_cov_params = cov </span><span class="s4"># assume scale=1</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">res._results.normalized_cov_params = </span><span class="s3">None</span>
        <span class="s1">k_constr = len(q)</span>
        <span class="s1">res._results.df_resid += k_constr</span>
        <span class="s1">res._results.df_model -= k_constr</span>
        <span class="s1">res._results.constraints = LinearConstraints.from_patsy(lc)</span>
        <span class="s1">res._results.k_constr = k_constr</span>
        <span class="s1">res._results.results_constrained = res_constr</span>
        <span class="s3">return </span><span class="s1">res</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Poisson model score (gradient) vector of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        score : ndarray, 1-D 
            The score vector of the model, i.e. the first derivative of the 
            loglikelihood function, evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L}{\\partial\\beta}=\\sum_{i=1}^{n}\\left(y_{i}-\\lambda_{i}\\right)x_{i} 
 
        where the loglinear model is assumed 
 
        .. math:: \\ln\\lambda_{i}=x_{i}\\beta 
        &quot;&quot;&quot;</span>
        <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">exposure = getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">L = np.exp(np.dot(X</span><span class="s3">,</span><span class="s1">params) + offset + exposure)</span>
        <span class="s3">return </span><span class="s1">np.dot(self.endog - L</span><span class="s3">, </span><span class="s1">X)</span>

    <span class="s3">def </span><span class="s1">score_obs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Poisson model Jacobian of the log-likelihood for each observation 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        score : array_like 
            The score vector (nobs, k_vars) of the model evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L_{i}}{\\partial\\beta}=\\left(y_{i}-\\lambda_{i}\\right)x_{i} 
 
        for observations :math:`i=1,...,n` 
 
        where the loglinear model is assumed 
 
        .. math:: \\ln\\lambda_{i}=x_{i}\\beta 
        &quot;&quot;&quot;</span>
        <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">exposure = getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">L = np.exp(np.dot(X</span><span class="s3">,</span><span class="s1">params) + offset + exposure)</span>
        <span class="s3">return </span><span class="s1">(self.endog - L)[:</span><span class="s3">,None</span><span class="s1">] * X</span>

    <span class="s3">def </span><span class="s1">score_factor(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Poisson model score_factor for each observation 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        score : array_like 
            The score factor (nobs, ) of the model evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L_{i}}{\\partial\\beta}=\\left(y_{i}-\\lambda_{i}\\right) 
 
        for observations :math:`i=1,...,n` 
 
        where the loglinear model is assumed 
 
        .. math:: \\ln\\lambda_{i}=x_{i}\\beta 
        &quot;&quot;&quot;</span>
        <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">exposure = getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">L = np.exp(np.dot(X</span><span class="s3">,</span><span class="s1">params) + offset + exposure)</span>
        <span class="s3">return </span><span class="s1">(self.endog - L)</span>

    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Poisson model Hessian matrix of the loglikelihood 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        hess : ndarray, (k_vars, k_vars) 
            The Hessian, second derivative of loglikelihood function, 
            evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial^{2}\\ln L}{\\partial\\beta\\partial\\beta^{\\prime}}=-\\sum_{i=1}^{n}\\lambda_{i}x_{i}x_{i}^{\\prime} 
 
        where the loglinear model is assumed 
 
        .. math:: \\ln\\lambda_{i}=x_{i}\\beta 
        &quot;&quot;&quot;</span>
        <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">exposure = getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">L = np.exp(np.dot(X</span><span class="s3">,</span><span class="s1">params) + exposure + offset)</span>
        <span class="s3">return </span><span class="s1">-np.dot(L*X.T</span><span class="s3">, </span><span class="s1">X)</span>

    <span class="s3">def </span><span class="s1">hessian_factor(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Poisson model Hessian factor 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        hess : ndarray, (nobs,) 
            The Hessian factor, second derivative of loglikelihood function 
            with respect to the linear predictor evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial^{2}\\ln L}{\\partial\\beta\\partial\\beta^{\\prime}}=-\\sum_{i=1}^{n}\\lambda_{i} 
 
        where the loglinear model is assumed 
 
        .. math:: \\ln\\lambda_{i}=x_{i}\\beta 
        &quot;&quot;&quot;</span>
        <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">exposure = getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">L = np.exp(np.dot(X</span><span class="s3">,</span><span class="s1">params) + exposure + offset)</span>
        <span class="s3">return </span><span class="s1">-L</span>

    <span class="s3">def </span><span class="s1">_deriv_score_obs_dendog(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">scale=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;derivative of score_obs w.r.t. endog 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which score is evaluated 
        scale : None or float 
            If scale is None, then the default scale will be calculated. 
            Default scale is defined by `self.scaletype` and set in fit. 
            If scale is not None, then it is used as a fixed scale. 
 
        Returns 
        ------- 
        derivative : ndarray_2d 
            The derivative of the score_obs with respect to endog. This 
            can is given by `score_factor0[:, None] * exog` where 
            `score_factor0` is the score_factor without the residual. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.exog</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exposure=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None,</span>
                <span class="s1">which=</span><span class="s2">'mean'</span><span class="s3">, </span><span class="s1">linear=</span><span class="s3">None, </span><span class="s1">y_values=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Predict response variable of a model given exogenous variables. 
 
        Parameters 
        ---------- 
        params : array_like 
            2d array of fitted parameters of the model. Should be in the 
            order returned from the model. 
        exog : array_like, optional 
            1d or 2d array of exogenous values.  If not supplied, then the 
            exog attribute of the model is used. If a 1d array is given 
            it assumed to be 1 row of exogenous variables. If you only have 
            one regressor and would like to do prediction, you must provide 
            a 2d array with shape[1] == 1. 
        offset : array_like, optional 
            Offset is added to the linear predictor with coefficient equal 
            to 1. 
            Default is zero if exog is not None, and the model offset if exog 
            is None. 
        exposure : array_like, optional 
            Log(exposure) is added to the linear prediction with coefficient 
            equal to 1. 
            Default is one if exog is is not None, and is the model exposure 
            if exog is None. 
        which : 'mean', 'linear', 'var', 'prob' (optional) 
            Statitistic to predict. Default is 'mean'. 
 
            - 'mean' returns the conditional expectation of endog E(y | x), 
              i.e. exp of linear predictor. 
            - 'linear' returns the linear predictor of the mean function. 
            - 'var' returns the estimated variance of endog implied by the 
              model. 
            - 'prob' return probabilities for counts from 0 to max(endog) or 
              for y_values if those are provided. 
 
            .. versionadded: 0.14 
 
               ``which`` replaces and extends the deprecated ``linear`` 
               argument. 
 
        linear : bool 
            The ``linear` keyword is deprecated and will be removed, 
            use ``which`` keyword instead. 
            If True, returns the linear predicted values.  If False or None, 
            then the statistic specified by ``which`` will be returned. 
 
            .. deprecated: 0.14 
 
               The ``linear` keyword is deprecated and will be removed, 
               use ``which`` keyword instead. 
 
        y_values : array_like 
            Values of the random variable endog at which pmf is evaluated. 
            Only used if ``which=&quot;prob&quot;`` 
        &quot;&quot;&quot;</span>
        <span class="s4"># Note docstring is reused by other count models</span>

        <span class="s3">if </span><span class="s1">linear </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">msg = </span><span class="s2">'linear keyword is deprecated, use which=&quot;linear&quot;'</span>
            <span class="s1">warnings.warn(msg</span><span class="s3">, </span><span class="s1">FutureWarning)</span>
            <span class="s3">if </span><span class="s1">linear </span><span class="s3">is True</span><span class="s1">:</span>
                <span class="s1">which = </span><span class="s2">&quot;linear&quot;</span>

        <span class="s3">if </span><span class="s1">which.startswith(</span><span class="s2">&quot;lin&quot;</span><span class="s1">):</span>
            <span class="s1">which = </span><span class="s2">&quot;linear&quot;</span>
        <span class="s3">if </span><span class="s1">which </span><span class="s3">in </span><span class="s1">[</span><span class="s2">&quot;mean&quot;</span><span class="s3">, </span><span class="s2">&quot;linear&quot;</span><span class="s1">]:</span>
            <span class="s3">return </span><span class="s1">super().predict(params</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">exposure=exposure</span><span class="s3">,</span>
                                   <span class="s1">offset=offset</span><span class="s3">,</span>
                                   <span class="s1">which=which</span><span class="s3">, </span><span class="s1">linear=linear)</span>
        <span class="s4"># TODO: add full set of which</span>
        <span class="s3">elif </span><span class="s1">which == </span><span class="s2">&quot;var&quot;</span><span class="s1">:</span>
            <span class="s1">mu = self.predict(params</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">,</span>
                              <span class="s1">exposure=exposure</span><span class="s3">, </span><span class="s1">offset=offset</span><span class="s3">,</span>
                              <span class="s1">)</span>
            <span class="s3">return </span><span class="s1">mu</span>
        <span class="s3">elif </span><span class="s1">which == </span><span class="s2">&quot;prob&quot;</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">y_values </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">y_values = np.atleast_2d(y_values)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">y_values = np.atleast_2d(</span>
                    <span class="s1">np.arange(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">np.max(self.endog) + </span><span class="s5">1</span><span class="s1">))</span>
            <span class="s1">mu = self.predict(params</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">,</span>
                              <span class="s1">exposure=exposure</span><span class="s3">, </span><span class="s1">offset=offset</span><span class="s3">,</span>
                              <span class="s1">)[:</span><span class="s3">, None</span><span class="s1">]</span>
            <span class="s4"># uses broadcasting</span>
            <span class="s3">return </span><span class="s1">stats.poisson._pmf(y_values</span><span class="s3">, </span><span class="s1">mu)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s2">'Value of the `which` option is not recognized'</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_prob_nonzero(self</span><span class="s3">, </span><span class="s1">mu</span><span class="s3">, </span><span class="s1">params=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Probability that count is not zero 
 
        internal use in Censored model, will be refactored or removed 
        &quot;&quot;&quot;</span>
        <span class="s1">prob_nz = - np.expm1(-mu)</span>
        <span class="s3">return </span><span class="s1">prob_nz</span>

    <span class="s3">def </span><span class="s1">_var(self</span><span class="s3">, </span><span class="s1">mu</span><span class="s3">, </span><span class="s1">params=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;variance implied by the distribution 
 
        internal use, will be refactored or removed 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">mu</span>

    <span class="s3">def </span><span class="s1">get_distribution(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exposure=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Get frozen instance of distribution based on predicted parameters. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
        exog : ndarray, optional 
            Explanatory variables for the main count model. 
            If ``exog`` is None, then the data from the model will be used. 
        offset : ndarray, optional 
            Offset is added to the linear predictor of the mean function with 
            coefficient equal to 1. 
            Default is zero if exog is not None, and the model offset if exog 
            is None. 
        exposure : ndarray, optional 
            Log(exposure) is added to the linear predictor  of the mean 
            function with coefficient equal to 1. If exposure is specified, 
            then it will be logged by the method. The user does not need to 
            log it first. 
            Default is one if exog is is not None, and it is the model exposure 
            if exog is None. 
 
        Returns 
        ------- 
        Instance of frozen scipy distribution subclass. 
        &quot;&quot;&quot;</span>
        <span class="s1">mu = self.predict(params</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">exposure=exposure</span><span class="s3">, </span><span class="s1">offset=offset)</span>
        <span class="s1">distr = stats.poisson(mu)</span>
        <span class="s3">return </span><span class="s1">distr</span>


<span class="s3">class </span><span class="s1">GeneralizedPoisson(CountModel):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Generalized Poisson Model 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : ndarray 
        A reference to the endogenous response variable 
    exog : ndarray 
        A reference to the exogenous design. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s3">,</span>
           <span class="s2">'extra_params'</span><span class="s1">:</span>
               <span class="s2">&quot;&quot;&quot; 
    p : scalar 
        P denotes parameterizations for GP regression. p=1 for GP-1 and 
        p=2 for GP-2. Default is p=1. 
    offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    exposure : array_like 
        Log(exposure) is added to the linear prediction with coefficient 
        equal to 1.&quot;&quot;&quot; </span><span class="s1">+ base._missing_param_doc + _check_rank_doc}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">p=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">offset=</span><span class="s3">None,</span>
                 <span class="s1">exposure=</span><span class="s3">None, </span><span class="s1">missing=</span><span class="s2">'none'</span><span class="s3">, </span><span class="s1">check_rank=</span><span class="s3">True, </span><span class="s1">**kwargs):</span>
        <span class="s1">super().__init__(endog</span><span class="s3">,</span>
                         <span class="s1">exog</span><span class="s3">,</span>
                         <span class="s1">offset=offset</span><span class="s3">,</span>
                         <span class="s1">exposure=exposure</span><span class="s3">,</span>
                         <span class="s1">missing=missing</span><span class="s3">,</span>
                         <span class="s1">check_rank=check_rank</span><span class="s3">,</span>
                         <span class="s1">**kwargs)</span>
        <span class="s1">self.parameterization = p - </span><span class="s5">1</span>
        <span class="s1">self.exog_names.append(</span><span class="s2">'alpha'</span><span class="s1">)</span>
        <span class="s1">self.k_extra = </span><span class="s5">1</span>
        <span class="s1">self._transparams = </span><span class="s3">False</span>

    <span class="s3">def </span><span class="s1">_get_init_kwds(self):</span>
        <span class="s1">kwds = super()._get_init_kwds()</span>
        <span class="s1">kwds[</span><span class="s2">'p'</span><span class="s1">] = self.parameterization + </span><span class="s5">1</span>
        <span class="s3">return </span><span class="s1">kwds</span>

    <span class="s3">def </span><span class="s1">_get_exogs(self):</span>
        <span class="s3">return </span><span class="s1">(self.exog</span><span class="s3">, None</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Loglikelihood of Generalized Poisson model 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike : float 
            The log-likelihood function of the model evaluated at `params`. 
            See notes. 
 
        Notes 
        ----- 
        .. math:: \\ln L=\\sum_{i=1}^{n}\\left[\\mu_{i}+(y_{i}-1)*ln(\\mu_{i}+ 
            \\alpha*\\mu_{i}^{p-1}*y_{i})-y_{i}*ln(1+\\alpha*\\mu_{i}^{p-1})- 
            ln(y_{i}!)-\\frac{\\mu_{i}+\\alpha*\\mu_{i}^{p-1}*y_{i}}{1+\\alpha* 
            \\mu_{i}^{p-1}}\\right] 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.sum(self.loglikeobs(params))</span>

    <span class="s3">def </span><span class="s1">loglikeobs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Loglikelihood for observations of Generalized Poisson model 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike : ndarray 
            The log likelihood for each observation of the model evaluated 
            at `params`. See Notes 
 
        Notes 
        ----- 
        .. math:: \\ln L=\\sum_{i=1}^{n}\\left[\\mu_{i}+(y_{i}-1)*ln(\\mu_{i}+ 
            \\alpha*\\mu_{i}^{p-1}*y_{i})-y_{i}*ln(1+\\alpha*\\mu_{i}^{p-1})- 
            ln(y_{i}!)-\\frac{\\mu_{i}+\\alpha*\\mu_{i}^{p-1}*y_{i}}{1+\\alpha* 
            \\mu_{i}^{p-1}}\\right] 
 
        for observations :math:`i=1,...,n` 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._transparams:</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">p = self.parameterization</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">mu = self.predict(params)</span>
        <span class="s1">mu_p = np.power(mu</span><span class="s3">, </span><span class="s1">p)</span>
        <span class="s1">a1 = </span><span class="s5">1 </span><span class="s1">+ alpha * mu_p</span>
        <span class="s1">a2 = mu + (a1 - </span><span class="s5">1</span><span class="s1">) * endog</span>
        <span class="s1">a1 = np.maximum(</span><span class="s5">1e-20</span><span class="s3">, </span><span class="s1">a1)</span>
        <span class="s1">a2 = np.maximum(</span><span class="s5">1e-20</span><span class="s3">, </span><span class="s1">a2)</span>
        <span class="s3">return </span><span class="s1">(np.log(mu) + (endog - </span><span class="s5">1</span><span class="s1">) * np.log(a2) - endog *</span>
                <span class="s1">np.log(a1) - gammaln(endog + </span><span class="s5">1</span><span class="s1">) - a2 / a1)</span>

    <span class="s1">@Appender(_get_start_params_null_docs)</span>
    <span class="s3">def </span><span class="s1">_get_start_params_null(self):</span>
        <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">exposure = getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>

        <span class="s1">const = (self.endog / np.exp(offset + exposure)).mean()</span>
        <span class="s1">params = [np.log(const)]</span>
        <span class="s1">mu = const * np.exp(offset + exposure)</span>
        <span class="s1">resid = self.endog - mu</span>
        <span class="s1">a = self._estimate_dispersion(mu</span><span class="s3">, </span><span class="s1">resid</span><span class="s3">, </span><span class="s1">df_resid=resid.shape[</span><span class="s5">0</span><span class="s1">] - </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">params.append(a)</span>

        <span class="s3">return </span><span class="s1">np.array(params)</span>

    <span class="s3">def </span><span class="s1">_estimate_dispersion(self</span><span class="s3">, </span><span class="s1">mu</span><span class="s3">, </span><span class="s1">resid</span><span class="s3">, </span><span class="s1">df_resid=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">q = self.parameterization</span>
        <span class="s3">if </span><span class="s1">df_resid </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">df_resid = resid.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">a = ((np.abs(resid) / np.sqrt(mu) - </span><span class="s5">1</span><span class="s1">) * mu**(-q)).sum() / df_resid</span>
        <span class="s3">return </span><span class="s1">a</span>


    <span class="s1">@Appender(</span>
        <span class="s2">&quot;&quot;&quot; 
        use_transparams : bool 
            This parameter enable internal transformation to impose 
            non-negativity. True to enable. Default is False. 
            use_transparams=True imposes the no underdispersion (alpha &gt; 0) 
            constraint. In case use_transparams=True and method=&quot;newton&quot; or 
            &quot;ncg&quot; transformation is ignored. 
        &quot;&quot;&quot;</span><span class="s1">)</span>
    <span class="s1">@Appender(DiscreteModel.fit.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'bfgs'</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s5">35</span><span class="s3">,</span>
            <span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None, </span><span class="s1">use_transparams=</span><span class="s3">False,</span>
            <span class="s1">cov_type=</span><span class="s2">'nonrobust'</span><span class="s3">, </span><span class="s1">cov_kwds=</span><span class="s3">None, </span><span class="s1">use_t=</span><span class="s3">None, </span><span class="s1">optim_kwds_prelim=</span><span class="s3">None,</span>
            <span class="s1">**kwargs):</span>
        <span class="s3">if </span><span class="s1">use_transparams </span><span class="s3">and </span><span class="s1">method </span><span class="s3">not in </span><span class="s1">[</span><span class="s2">'newton'</span><span class="s3">, </span><span class="s2">'ncg'</span><span class="s1">]:</span>
            <span class="s1">self._transparams = </span><span class="s3">True</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">use_transparams:</span>
                <span class="s1">warnings.warn(</span><span class="s2">'Parameter &quot;use_transparams&quot; is ignored'</span><span class="s3">,</span>
                              <span class="s1">RuntimeWarning)</span>
            <span class="s1">self._transparams = </span><span class="s3">False</span>

        <span class="s3">if </span><span class="s1">start_params </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">) + getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">np.size(offset) == </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">offset == </span><span class="s5">0</span><span class="s1">:</span>
                <span class="s1">offset = </span><span class="s3">None</span>
            <span class="s1">kwds_prelim = {</span><span class="s2">'disp'</span><span class="s1">: </span><span class="s5">0</span><span class="s3">, </span><span class="s2">'skip_hessian'</span><span class="s1">: </span><span class="s3">True,</span>
                           <span class="s2">'warn_convergence'</span><span class="s1">: </span><span class="s3">False</span><span class="s1">}</span>
            <span class="s3">if </span><span class="s1">optim_kwds_prelim </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">kwds_prelim.update(optim_kwds_prelim)</span>
            <span class="s1">mod_poi = Poisson(self.endog</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">, </span><span class="s1">offset=offset)</span>
            <span class="s3">with </span><span class="s1">warnings.catch_warnings():</span>
                <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;always&quot;</span><span class="s1">)</span>
                <span class="s1">res_poi = mod_poi.fit(**kwds_prelim)</span>
            <span class="s1">start_params = res_poi.params</span>
            <span class="s1">a = self._estimate_dispersion(res_poi.predict()</span><span class="s3">, </span><span class="s1">res_poi.resid</span><span class="s3">,</span>
                                          <span class="s1">df_resid=res_poi.df_resid)</span>
            <span class="s1">start_params = np.append(start_params</span><span class="s3">, </span><span class="s1">max(-</span><span class="s5">0.1</span><span class="s3">, </span><span class="s1">a))</span>

        <span class="s3">if </span><span class="s1">callback </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s4"># work around perfect separation callback #3895</span>
            <span class="s1">callback = </span><span class="s3">lambda </span><span class="s1">*x: x</span>

        <span class="s1">mlefit = super().fit(start_params=start_params</span><span class="s3">,</span>
                             <span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                             <span class="s1">method=method</span><span class="s3">,</span>
                             <span class="s1">disp=disp</span><span class="s3">,</span>
                             <span class="s1">full_output=full_output</span><span class="s3">,</span>
                             <span class="s1">callback=callback</span><span class="s3">,</span>
                             <span class="s1">**kwargs)</span>
        <span class="s3">if </span><span class="s1">optim_kwds_prelim </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">mlefit.mle_settings[</span><span class="s2">&quot;optim_kwds_prelim&quot;</span><span class="s1">] = optim_kwds_prelim</span>
        <span class="s3">if </span><span class="s1">use_transparams </span><span class="s3">and </span><span class="s1">method </span><span class="s3">not in </span><span class="s1">[</span><span class="s2">&quot;newton&quot;</span><span class="s3">, </span><span class="s2">&quot;ncg&quot;</span><span class="s1">]:</span>
            <span class="s1">self._transparams = </span><span class="s3">False</span>
            <span class="s1">mlefit._results.params[-</span><span class="s5">1</span><span class="s1">] = np.exp(mlefit._results.params[-</span><span class="s5">1</span><span class="s1">])</span>

        <span class="s1">gpfit = GeneralizedPoissonResults(self</span><span class="s3">, </span><span class="s1">mlefit._results)</span>
        <span class="s1">result = GeneralizedPoissonResultsWrapper(gpfit)</span>

        <span class="s3">if </span><span class="s1">cov_kwds </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">cov_kwds = {}</span>

        <span class="s1">result._get_robustcov_results(cov_type=cov_type</span><span class="s3">,</span>
                                      <span class="s1">use_self=</span><span class="s3">True, </span><span class="s1">use_t=use_t</span><span class="s3">, </span><span class="s1">**cov_kwds)</span>
        <span class="s3">return </span><span class="s1">result</span>

    <span class="s1">@Appender(DiscreteModel.fit_regularized.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit_regularized(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'l1'</span><span class="s3">,</span>
            <span class="s1">maxiter=</span><span class="s2">'defined_by_method'</span><span class="s3">, </span><span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None,</span>
            <span class="s1">alpha=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">trim_mode=</span><span class="s2">'auto'</span><span class="s3">, </span><span class="s1">auto_trim_tol=</span><span class="s5">0.01</span><span class="s3">, </span><span class="s1">size_trim_tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
            <span class="s1">qc_tol=</span><span class="s5">0.03</span><span class="s3">, </span><span class="s1">**kwargs):</span>

        <span class="s1">_validate_l1_method(method)</span>

        <span class="s3">if </span><span class="s1">np.size(alpha) == </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">alpha != </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">k_params = self.exog.shape[</span><span class="s5">1</span><span class="s1">] + self.k_extra</span>
            <span class="s1">alpha = alpha * np.ones(k_params)</span>
            <span class="s1">alpha[-</span><span class="s5">1</span><span class="s1">] = </span><span class="s5">0</span>

        <span class="s1">alpha_p = alpha[:-</span><span class="s5">1</span><span class="s1">] </span><span class="s3">if </span><span class="s1">(self.k_extra </span><span class="s3">and </span><span class="s1">np.size(alpha) &gt; </span><span class="s5">1</span><span class="s1">) </span><span class="s3">else </span><span class="s1">alpha</span>
        <span class="s1">self._transparams = </span><span class="s3">False</span>
        <span class="s3">if </span><span class="s1">start_params </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">) + getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">np.size(offset) == </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">offset == </span><span class="s5">0</span><span class="s1">:</span>
                <span class="s1">offset = </span><span class="s3">None</span>
            <span class="s1">mod_poi = Poisson(self.endog</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">, </span><span class="s1">offset=offset)</span>
            <span class="s3">with </span><span class="s1">warnings.catch_warnings():</span>
                <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;always&quot;</span><span class="s1">)</span>
                <span class="s1">start_params = mod_poi.fit_regularized(</span>
                    <span class="s1">start_params=start_params</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                    <span class="s1">full_output=full_output</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">,</span>
                    <span class="s1">alpha=alpha_p</span><span class="s3">, </span><span class="s1">trim_mode=trim_mode</span><span class="s3">,</span>
                    <span class="s1">auto_trim_tol=auto_trim_tol</span><span class="s3">, </span><span class="s1">size_trim_tol=size_trim_tol</span><span class="s3">,</span>
                    <span class="s1">qc_tol=qc_tol</span><span class="s3">, </span><span class="s1">**kwargs).params</span>
            <span class="s1">start_params = np.append(start_params</span><span class="s3">, </span><span class="s5">0.1</span><span class="s1">)</span>

        <span class="s1">cntfit = super(CountModel</span><span class="s3">, </span><span class="s1">self).fit_regularized(</span>
                <span class="s1">start_params=start_params</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                <span class="s1">full_output=full_output</span><span class="s3">, </span><span class="s1">disp=disp</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">,</span>
                <span class="s1">alpha=alpha</span><span class="s3">, </span><span class="s1">trim_mode=trim_mode</span><span class="s3">, </span><span class="s1">auto_trim_tol=auto_trim_tol</span><span class="s3">,</span>
                <span class="s1">size_trim_tol=size_trim_tol</span><span class="s3">, </span><span class="s1">qc_tol=qc_tol</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s1">discretefit = L1GeneralizedPoissonResults(self</span><span class="s3">, </span><span class="s1">cntfit)</span>
        <span class="s3">return </span><span class="s1">L1GeneralizedPoissonResultsWrapper(discretefit)</span>

    <span class="s3">def </span><span class="s1">score_obs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">if </span><span class="s1">self._transparams:</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">p = self.parameterization</span>
        <span class="s1">exog = self.exog</span>
        <span class="s1">y = self.endog[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s1">mu = self.predict(params)[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s1">mu_p = np.power(mu</span><span class="s3">, </span><span class="s1">p)</span>
        <span class="s1">a1 = </span><span class="s5">1 </span><span class="s1">+ alpha * mu_p</span>
        <span class="s1">a2 = mu + alpha * mu_p * y</span>
        <span class="s1">a3 = alpha * p * mu ** (p - </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">a4 = a3 * y</span>
        <span class="s1">dmudb = mu * exog</span>

        <span class="s1">dalpha = (mu_p * (y * ((y - </span><span class="s5">1</span><span class="s1">) / a2 - </span><span class="s5">2 </span><span class="s1">/ a1) + a2 / a1**</span><span class="s5">2</span><span class="s1">))</span>
        <span class="s1">dparams = dmudb * (-a4 / a1 +</span>
                           <span class="s1">a3 * a2 / (a1 ** </span><span class="s5">2</span><span class="s1">) +</span>
                           <span class="s1">(</span><span class="s5">1 </span><span class="s1">+ a4) * ((y - </span><span class="s5">1</span><span class="s1">) / a2 - </span><span class="s5">1 </span><span class="s1">/ a1) +</span>
                           <span class="s5">1 </span><span class="s1">/ mu)</span>

        <span class="s3">return </span><span class="s1">np.concatenate((dparams</span><span class="s3">, </span><span class="s1">np.atleast_2d(dalpha))</span><span class="s3">,</span>
                              <span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s1">score = np.sum(self.score_obs(params)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">self._transparams:</span>
            <span class="s1">score[-</span><span class="s5">1</span><span class="s1">] == score[-</span><span class="s5">1</span><span class="s1">] ** </span><span class="s5">2</span>
            <span class="s3">return </span><span class="s1">score</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">score</span>

    <span class="s3">def </span><span class="s1">score_factor(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">endog=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s3">if </span><span class="s1">self._transparams:</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">p = self.parameterization</span>
        <span class="s1">y = self.endog </span><span class="s3">if </span><span class="s1">endog </span><span class="s3">is None else </span><span class="s1">endog</span>

        <span class="s1">mu = self.predict(params)</span>
        <span class="s1">mu_p = np.power(mu</span><span class="s3">, </span><span class="s1">p)</span>
        <span class="s1">a1 = </span><span class="s5">1 </span><span class="s1">+ alpha * mu_p</span>
        <span class="s1">a2 = mu + alpha * mu_p * y</span>
        <span class="s1">a3 = alpha * p * mu ** (p - </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">a4 = a3 * y</span>
        <span class="s1">dmudb = mu</span>

        <span class="s1">dalpha = (mu_p * (y * ((y - </span><span class="s5">1</span><span class="s1">) / a2 - </span><span class="s5">2 </span><span class="s1">/ a1) + a2 / a1**</span><span class="s5">2</span><span class="s1">))</span>
        <span class="s1">dparams = dmudb * (-a4 / a1 +</span>
                           <span class="s1">a3 * a2 / (a1 ** </span><span class="s5">2</span><span class="s1">) +</span>
                           <span class="s1">(</span><span class="s5">1 </span><span class="s1">+ a4) * ((y - </span><span class="s5">1</span><span class="s1">) / a2 - </span><span class="s5">1 </span><span class="s1">/ a1) +</span>
                           <span class="s5">1 </span><span class="s1">/ mu)</span>

        <span class="s3">return </span><span class="s1">dparams</span><span class="s3">, </span><span class="s1">dalpha</span>

    <span class="s3">def </span><span class="s1">_score_p(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Generalized Poisson model derivative of the log-likelihood by p-parameter 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        dldp : float 
            dldp is first derivative of the loglikelihood function, 
        evaluated at `p-parameter`. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._transparams:</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">p = self.parameterization</span>
        <span class="s1">y = self.endog[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s1">mu = self.predict(params)[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s1">mu_p = np.power(mu</span><span class="s3">, </span><span class="s1">p)</span>
        <span class="s1">a1 = </span><span class="s5">1 </span><span class="s1">+ alpha * mu_p</span>
        <span class="s1">a2 = mu + alpha * mu_p * y</span>

        <span class="s1">dp = np.sum((np.log(mu) * ((a2 - mu) * ((y - </span><span class="s5">1</span><span class="s1">) / a2 - </span><span class="s5">2 </span><span class="s1">/ a1) +</span>
                                   <span class="s1">(a1 - </span><span class="s5">1</span><span class="s1">) * a2 / a1 ** </span><span class="s5">2</span><span class="s1">)))</span>
        <span class="s3">return </span><span class="s1">dp</span>

    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Generalized Poisson model Hessian matrix of the loglikelihood 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        hess : ndarray, (k_vars, k_vars) 
            The Hessian, second derivative of loglikelihood function, 
            evaluated at `params` 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._transparams:</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">p = self.parameterization</span>
        <span class="s1">exog = self.exog</span>
        <span class="s1">y = self.endog[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s1">mu = self.predict(params)[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s1">mu_p = np.power(mu</span><span class="s3">, </span><span class="s1">p)</span>
        <span class="s1">a1 = </span><span class="s5">1 </span><span class="s1">+ alpha * mu_p</span>
        <span class="s1">a2 = mu + alpha * mu_p * y</span>
        <span class="s1">a3 = alpha * p * mu ** (p - </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">a4 = a3 * y</span>
        <span class="s1">a5 = p * mu ** (p - </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">dmudb = mu * exog</span>

        <span class="s4"># for dl/dparams dparams</span>
        <span class="s1">dim = exog.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">hess_arr = np.empty((dim+</span><span class="s5">1</span><span class="s3">,</span><span class="s1">dim+</span><span class="s5">1</span><span class="s1">))</span>

        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(dim):</span>
            <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(i + </span><span class="s5">1</span><span class="s1">):</span>
                <span class="s1">hess_val = np.sum(mu * exog[:</span><span class="s3">,</span><span class="s1">i</span><span class="s3">,None</span><span class="s1">] * exog[:</span><span class="s3">,</span><span class="s1">j</span><span class="s3">,None</span><span class="s1">] *</span>
                    <span class="s1">(mu * (a3 * a4 / a1**</span><span class="s5">2 </span><span class="s1">-</span>
                           <span class="s5">2 </span><span class="s1">* a3**</span><span class="s5">2 </span><span class="s1">* a2 / a1**</span><span class="s5">3 </span><span class="s1">+</span>
                           <span class="s5">2 </span><span class="s1">* a3 * (a4 + </span><span class="s5">1</span><span class="s1">) / a1**</span><span class="s5">2 </span><span class="s1">-</span>
                           <span class="s1">a4 * p / (mu * a1) +</span>
                           <span class="s1">a3 * p * a2 / (mu * a1**</span><span class="s5">2</span><span class="s1">) +</span>
                           <span class="s1">(y - </span><span class="s5">1</span><span class="s1">) * a4 * (p - </span><span class="s5">1</span><span class="s1">) / (a2 * mu) -</span>
                           <span class="s1">(y - </span><span class="s5">1</span><span class="s1">) * (</span><span class="s5">1 </span><span class="s1">+ a4)**</span><span class="s5">2 </span><span class="s1">/ a2**</span><span class="s5">2 </span><span class="s1">-</span>
                           <span class="s1">a4 * (p - </span><span class="s5">1</span><span class="s1">) / (a1 * mu)) +</span>
                     <span class="s1">((y - </span><span class="s5">1</span><span class="s1">) * (</span><span class="s5">1 </span><span class="s1">+ a4) / a2 -</span>
                      <span class="s1">(</span><span class="s5">1 </span><span class="s1">+ a4) / a1))</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
                <span class="s1">hess_arr[i</span><span class="s3">, </span><span class="s1">j] = np.squeeze(hess_val)</span>
        <span class="s1">tri_idx = np.triu_indices(dim</span><span class="s3">, </span><span class="s1">k=</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">hess_arr[tri_idx] = hess_arr.T[tri_idx]</span>

        <span class="s4"># for dl/dparams dalpha</span>
        <span class="s1">dldpda = np.sum((</span><span class="s5">2 </span><span class="s1">* a4 * mu_p / a1**</span><span class="s5">2 </span><span class="s1">-</span>
                         <span class="s5">2 </span><span class="s1">* a3 * mu_p * a2 / a1**</span><span class="s5">3 </span><span class="s1">-</span>
                         <span class="s1">mu_p * y * (y - </span><span class="s5">1</span><span class="s1">) * (</span><span class="s5">1 </span><span class="s1">+ a4) / a2**</span><span class="s5">2 </span><span class="s1">+</span>
                         <span class="s1">mu_p * (</span><span class="s5">1 </span><span class="s1">+ a4) / a1**</span><span class="s5">2 </span><span class="s1">+</span>
                         <span class="s1">a5 * y * (y - </span><span class="s5">1</span><span class="s1">) / a2 -</span>
                         <span class="s5">2 </span><span class="s1">* a5 * y / a1 +</span>
                         <span class="s1">a5 * a2 / a1**</span><span class="s5">2</span><span class="s1">) * dmudb</span><span class="s3">,</span>
                        <span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>

        <span class="s1">hess_arr[-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">:-</span><span class="s5">1</span><span class="s1">] = dldpda</span>
        <span class="s1">hess_arr[:-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">-</span><span class="s5">1</span><span class="s1">] = dldpda</span>

        <span class="s4"># for dl/dalpha dalpha</span>
        <span class="s1">dldada = mu_p**</span><span class="s5">2 </span><span class="s1">* (</span><span class="s5">3 </span><span class="s1">* y / a1**</span><span class="s5">2 </span><span class="s1">-</span>
                            <span class="s1">(y / a2)**</span><span class="s5">2. </span><span class="s1">* (y - </span><span class="s5">1</span><span class="s1">) -</span>
                            <span class="s5">2 </span><span class="s1">* a2 / a1**</span><span class="s5">3</span><span class="s1">)</span>

        <span class="s1">hess_arr[-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">-</span><span class="s5">1</span><span class="s1">] = dldada.sum()</span>

        <span class="s3">return </span><span class="s1">hess_arr</span>

    <span class="s3">def </span><span class="s1">hessian_factor(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Generalized Poisson model Hessian matrix of the loglikelihood 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        hess : ndarray, (nobs, 3) 
            The Hessian factor, second derivative of loglikelihood function 
            with respect to linear predictor and dispersion parameter 
            evaluated at `params` 
            The first column contains the second derivative w.r.t. linpred, 
            the second column contains the cross derivative, and the 
            third column contains the second derivative w.r.t. the dispersion 
            parameter. 
 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._transparams:</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">p = self.parameterization</span>
        <span class="s1">y = self.endog</span>
        <span class="s1">mu = self.predict(params)</span>
        <span class="s1">mu_p = np.power(mu</span><span class="s3">, </span><span class="s1">p)</span>
        <span class="s1">a1 = </span><span class="s5">1 </span><span class="s1">+ alpha * mu_p</span>
        <span class="s1">a2 = mu + alpha * mu_p * y</span>
        <span class="s1">a3 = alpha * p * mu ** (p - </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">a4 = a3 * y</span>
        <span class="s1">a5 = p * mu ** (p - </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">dmudb = mu</span>

        <span class="s1">dbb = mu * (</span>
             <span class="s1">mu * (a3 * a4 / a1**</span><span class="s5">2 </span><span class="s1">-</span>
                   <span class="s5">2 </span><span class="s1">* a3**</span><span class="s5">2 </span><span class="s1">* a2 / a1**</span><span class="s5">3 </span><span class="s1">+</span>
                   <span class="s5">2 </span><span class="s1">* a3 * (a4 + </span><span class="s5">1</span><span class="s1">) / a1**</span><span class="s5">2 </span><span class="s1">-</span>
                   <span class="s1">a4 * p / (mu * a1) +</span>
                   <span class="s1">a3 * p * a2 / (mu * a1**</span><span class="s5">2</span><span class="s1">) +</span>
                   <span class="s1">a4 / (mu * a1) -</span>
                   <span class="s1">a3 * a2 / (mu * a1**</span><span class="s5">2</span><span class="s1">) +</span>
                   <span class="s1">(y - </span><span class="s5">1</span><span class="s1">) * a4 * (p - </span><span class="s5">1</span><span class="s1">) / (a2 * mu) -</span>
                   <span class="s1">(y - </span><span class="s5">1</span><span class="s1">) * (</span><span class="s5">1 </span><span class="s1">+ a4)**</span><span class="s5">2 </span><span class="s1">/ a2**</span><span class="s5">2 </span><span class="s1">-</span>
                   <span class="s1">a4 * (p - </span><span class="s5">1</span><span class="s1">) / (a1 * mu) -</span>
                   <span class="s5">1 </span><span class="s1">/ mu**</span><span class="s5">2</span><span class="s1">) +</span>
             <span class="s1">(-a4 / a1 +</span>
              <span class="s1">a3 * a2 / a1**</span><span class="s5">2 </span><span class="s1">+</span>
              <span class="s1">(y - </span><span class="s5">1</span><span class="s1">) * (</span><span class="s5">1 </span><span class="s1">+ a4) / a2 -</span>
              <span class="s1">(</span><span class="s5">1 </span><span class="s1">+ a4) / a1 +</span>
              <span class="s5">1 </span><span class="s1">/ mu))</span>

        <span class="s4"># for dl/dlinpred dalpha</span>
        <span class="s1">dba = ((</span><span class="s5">2 </span><span class="s1">* a4 * mu_p / a1**</span><span class="s5">2 </span><span class="s1">-</span>
                         <span class="s5">2 </span><span class="s1">* a3 * mu_p * a2 / a1**</span><span class="s5">3 </span><span class="s1">-</span>
                         <span class="s1">mu_p * y * (y - </span><span class="s5">1</span><span class="s1">) * (</span><span class="s5">1 </span><span class="s1">+ a4) / a2**</span><span class="s5">2 </span><span class="s1">+</span>
                         <span class="s1">mu_p * (</span><span class="s5">1 </span><span class="s1">+ a4) / a1**</span><span class="s5">2 </span><span class="s1">+</span>
                         <span class="s1">a5 * y * (y - </span><span class="s5">1</span><span class="s1">) / a2 -</span>
                         <span class="s5">2 </span><span class="s1">* a5 * y / a1 +</span>
                         <span class="s1">a5 * a2 / a1**</span><span class="s5">2</span><span class="s1">) * dmudb)</span>

        <span class="s4"># for dl/dalpha dalpha</span>
        <span class="s1">daa = mu_p**</span><span class="s5">2 </span><span class="s1">* (</span><span class="s5">3 </span><span class="s1">* y / a1**</span><span class="s5">2 </span><span class="s1">-</span>
                            <span class="s1">(y / a2)**</span><span class="s5">2. </span><span class="s1">* (y - </span><span class="s5">1</span><span class="s1">) -</span>
                            <span class="s5">2 </span><span class="s1">* a2 / a1**</span><span class="s5">3</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">dbb</span><span class="s3">, </span><span class="s1">dba</span><span class="s3">, </span><span class="s1">daa</span>

    <span class="s1">@Appender(Poisson.predict.__doc__)</span>
    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exposure=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None,</span>
                <span class="s1">which=</span><span class="s2">'mean'</span><span class="s3">, </span><span class="s1">y_values=</span><span class="s3">None</span><span class="s1">):</span>

        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>

        <span class="s3">if </span><span class="s1">exposure </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exposure = getattr(self</span><span class="s3">, </span><span class="s2">'exposure'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">exposure != </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">exposure = np.log(exposure)</span>

        <span class="s3">if </span><span class="s1">offset </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">'offset'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>

        <span class="s1">fitted = np.dot(exog</span><span class="s3">, </span><span class="s1">params[:exog.shape[</span><span class="s5">1</span><span class="s1">]])</span>
        <span class="s1">linpred = fitted + exposure + offset</span>

        <span class="s3">if </span><span class="s1">which == </span><span class="s2">'mean'</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">np.exp(linpred)</span>
        <span class="s3">elif </span><span class="s1">which == </span><span class="s2">'linear'</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">linpred</span>
        <span class="s3">elif </span><span class="s1">which == </span><span class="s2">'var'</span><span class="s1">:</span>
            <span class="s1">mean = np.exp(linpred)</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s1">pm1 = self.parameterization  </span><span class="s4"># `p - 1` in GPP</span>
            <span class="s1">var_ = mean * (</span><span class="s5">1 </span><span class="s1">+ alpha * mean**pm1)**</span><span class="s5">2</span>
            <span class="s3">return </span><span class="s1">var_</span>
        <span class="s3">elif </span><span class="s1">which == </span><span class="s2">'prob'</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">y_values </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">y_values = np.atleast_2d(np.arange(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">np.max(self.endog)+</span><span class="s5">1</span><span class="s1">))</span>
            <span class="s1">mu = self.predict(params</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">exposure=exposure</span><span class="s3">,</span>
                              <span class="s1">offset=offset)[:</span><span class="s3">, None</span><span class="s1">]</span>
            <span class="s3">return </span><span class="s1">genpoisson_p.pmf(y_values</span><span class="s3">, </span><span class="s1">mu</span><span class="s3">, </span><span class="s1">params[-</span><span class="s5">1</span><span class="s1">]</span><span class="s3">,</span>
                                    <span class="s1">self.parameterization + </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s2">'keyword </span><span class="s3">\'</span><span class="s2">which</span><span class="s3">\' </span><span class="s2">not recognized'</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_deriv_score_obs_dendog(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot;derivative of score_obs w.r.t. endog 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which score is evaluated 
 
        Returns 
        ------- 
        derivative : ndarray_2d 
            The derivative of the score_obs with respect to endog. 
        &quot;&quot;&quot;</span>
        <span class="s4"># code duplication with NegativeBinomialP</span>
        <span class="s3">from </span><span class="s1">statsmodels.tools.numdiff </span><span class="s3">import </span><span class="s1">_approx_fprime_cs_scalar</span>

        <span class="s3">def </span><span class="s1">f(y):</span>
            <span class="s3">if </span><span class="s1">y.ndim == </span><span class="s5">2 </span><span class="s3">and </span><span class="s1">y.shape[</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">y = y[:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">sf = self.score_factor(params</span><span class="s3">, </span><span class="s1">endog=y)</span>
            <span class="s3">return </span><span class="s1">np.column_stack(sf)</span>

        <span class="s1">dsf = _approx_fprime_cs_scalar(self.endog[:</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">f)</span>
        <span class="s4"># deriv is 2d vector</span>
        <span class="s1">d1 = dsf[:</span><span class="s3">, </span><span class="s1">:</span><span class="s5">1</span><span class="s1">] * self.exog</span>
        <span class="s1">d2 = dsf[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">:</span><span class="s5">2</span><span class="s1">]</span>

        <span class="s3">return </span><span class="s1">np.column_stack((d1</span><span class="s3">, </span><span class="s1">d2))</span>

    <span class="s3">def </span><span class="s1">_var(self</span><span class="s3">, </span><span class="s1">mu</span><span class="s3">, </span><span class="s1">params=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;variance implied by the distribution 
 
        internal use, will be refactored or removed 
        &quot;&quot;&quot;</span>
        <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">pm1 = self.parameterization  </span><span class="s4"># `p-1` in GPP</span>
        <span class="s1">var_ = mu * (</span><span class="s5">1 </span><span class="s1">+ alpha * mu**pm1)**</span><span class="s5">2</span>
        <span class="s3">return </span><span class="s1">var_</span>

    <span class="s3">def </span><span class="s1">_prob_nonzero(self</span><span class="s3">, </span><span class="s1">mu</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot;Probability that count is not zero 
 
        internal use in Censored model, will be refactored or removed 
        &quot;&quot;&quot;</span>
        <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">pm1 = self.parameterization  </span><span class="s4"># p-1 in GPP</span>
        <span class="s1">prob_zero = np.exp(- mu / (</span><span class="s5">1 </span><span class="s1">+ alpha * mu**pm1))</span>
        <span class="s1">prob_nz = </span><span class="s5">1 </span><span class="s1">- prob_zero</span>
        <span class="s3">return </span><span class="s1">prob_nz</span>

    <span class="s1">@Appender(Poisson.get_distribution.__doc__)</span>
    <span class="s3">def </span><span class="s1">get_distribution(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exposure=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;get frozen instance of distribution 
        &quot;&quot;&quot;</span>
        <span class="s1">mu = self.predict(params</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">exposure=exposure</span><span class="s3">, </span><span class="s1">offset=offset)</span>
        <span class="s1">p = self.parameterization + </span><span class="s5">1</span>
        <span class="s4"># distr = genpoisson_p(mu[:, None], params[-1], p)</span>
        <span class="s1">distr = genpoisson_p(mu</span><span class="s3">, </span><span class="s1">params[-</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">p)</span>
        <span class="s3">return </span><span class="s1">distr</span>


<span class="s3">class </span><span class="s1">Logit(BinaryModel):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Logit Model 
 
    %(params)s 
    offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : ndarray 
        A reference to the endogenous response variable 
    exog : ndarray 
        A reference to the exogenous design. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s3">,</span>
           <span class="s2">'extra_params'</span><span class="s1">: base._missing_param_doc + _check_rank_doc}</span>

    <span class="s1">_continuous_ok = </span><span class="s3">True</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">link(self):</span>
        <span class="s3">from </span><span class="s1">statsmodels.genmod.families </span><span class="s3">import </span><span class="s1">links</span>
        <span class="s1">link = links.Logit()</span>
        <span class="s3">return </span><span class="s1">link</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        The logistic cumulative distribution function 
 
        Parameters 
        ---------- 
        X : array_like 
            `X` is the linear predictor of the logit model.  See notes. 
 
        Returns 
        ------- 
        1/(1 + exp(-X)) 
 
        Notes 
        ----- 
        In the logit model, 
 
        .. math:: \\Lambda\\left(x^{\\prime}\\beta\\right)= 
                  \\text{Prob}\\left(Y=1|x\\right)= 
                  \\frac{e^{x^{\\prime}\\beta}}{1+e^{x^{\\prime}\\beta}} 
        &quot;&quot;&quot;</span>
        <span class="s1">X = np.asarray(X)</span>
        <span class="s3">return </span><span class="s5">1</span><span class="s1">/(</span><span class="s5">1</span><span class="s1">+np.exp(-X))</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        The logistic probability density function 
 
        Parameters 
        ---------- 
        X : array_like 
            `X` is the linear predictor of the logit model.  See notes. 
 
        Returns 
        ------- 
        pdf : ndarray 
            The value of the Logit probability mass function, PMF, for each 
            point of X. ``np.exp(-x)/(1+np.exp(-X))**2`` 
 
        Notes 
        ----- 
        In the logit model, 
 
        .. math:: \\lambda\\left(x^{\\prime}\\beta\\right)=\\frac{e^{-x^{\\prime}\\beta}}{\\left(1+e^{-x^{\\prime}\\beta}\\right)^{2}} 
        &quot;&quot;&quot;</span>
        <span class="s1">X = np.asarray(X)</span>
        <span class="s3">return </span><span class="s1">np.exp(-X)/(</span><span class="s5">1</span><span class="s1">+np.exp(-X))**</span><span class="s5">2</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">family(self):</span>
        <span class="s3">from </span><span class="s1">statsmodels.genmod </span><span class="s3">import </span><span class="s1">families</span>
        <span class="s3">return </span><span class="s1">families.Binomial()</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Log-likelihood of logit model. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the logit model. 
 
        Returns 
        ------- 
        loglike : float 
            The log-likelihood function of the model evaluated at `params`. 
            See notes. 
 
        Notes 
        ----- 
        .. math:: 
 
           \\ln L=\\sum_{i}\\ln\\Lambda 
           \\left(q_{i}x_{i}^{\\prime}\\beta\\right) 
 
        Where :math:`q=2y-1`. This simplification comes from the fact that the 
        logistic distribution is symmetric. 
        &quot;&quot;&quot;</span>
        <span class="s1">q = </span><span class="s5">2</span><span class="s1">*self.endog - </span><span class="s5">1</span>
        <span class="s1">linpred = self.predict(params</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">np.sum(np.log(self.cdf(q * linpred)))</span>

    <span class="s3">def </span><span class="s1">loglikeobs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Log-likelihood of logit model for each observation. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the logit model. 
 
        Returns 
        ------- 
        loglike : ndarray 
            The log likelihood for each observation of the model evaluated 
            at `params`. See Notes 
 
        Notes 
        ----- 
        .. math:: 
 
           \\ln L=\\sum_{i}\\ln\\Lambda 
           \\left(q_{i}x_{i}^{\\prime}\\beta\\right) 
 
        for observations :math:`i=1,...,n` 
 
        where :math:`q=2y-1`. This simplification comes from the fact that the 
        logistic distribution is symmetric. 
        &quot;&quot;&quot;</span>
        <span class="s1">q = </span><span class="s5">2</span><span class="s1">*self.endog - </span><span class="s5">1</span>
        <span class="s1">linpred = self.predict(params</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">np.log(self.cdf(q * linpred))</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Logit model score (gradient) vector of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        score : ndarray, 1-D 
            The score vector of the model, i.e. the first derivative of the 
            loglikelihood function, evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L}{\\partial\\beta}=\\sum_{i=1}^{n}\\left(y_{i}-\\Lambda_{i}\\right)x_{i} 
        &quot;&quot;&quot;</span>

        <span class="s1">y = self.endog</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">fitted = self.predict(params)</span>
        <span class="s3">return </span><span class="s1">np.dot(y - fitted</span><span class="s3">, </span><span class="s1">X)</span>

    <span class="s3">def </span><span class="s1">score_obs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Logit model Jacobian of the log-likelihood for each observation 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        jac : array_like 
            The derivative of the loglikelihood for each observation evaluated 
            at `params`. 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L_{i}}{\\partial\\beta}=\\left(y_{i}-\\Lambda_{i}\\right)x_{i} 
 
        for observations :math:`i=1,...,n` 
        &quot;&quot;&quot;</span>

        <span class="s1">y = self.endog</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">fitted = self.predict(params)</span>
        <span class="s3">return </span><span class="s1">(y - fitted)[:</span><span class="s3">,None</span><span class="s1">] * X</span>

    <span class="s3">def </span><span class="s1">score_factor(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Logit model derivative of the log-likelihood with respect to linpred. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        score_factor : array_like 
            The derivative of the loglikelihood for each observation evaluated 
            at `params`. 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L_{i}}{\\partial\\beta}=\\left(y_{i}-\\lambda_{i}\\right) 
 
        for observations :math:`i=1,...,n` 
 
        where the loglinear model is assumed 
 
        .. math:: \\ln\\lambda_{i}=x_{i}\\beta 
        &quot;&quot;&quot;</span>
        <span class="s1">y = self.endog</span>
        <span class="s1">fitted = self.predict(params)</span>
        <span class="s3">return </span><span class="s1">(y - fitted)</span>

    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Logit model Hessian matrix of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        hess : ndarray, (k_vars, k_vars) 
            The Hessian, second derivative of loglikelihood function, 
            evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial^{2}\\ln L}{\\partial\\beta\\partial\\beta^{\\prime}}=-\\sum_{i}\\Lambda_{i}\\left(1-\\Lambda_{i}\\right)x_{i}x_{i}^{\\prime} 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">L = self.predict(params)</span>
        <span class="s3">return </span><span class="s1">-np.dot(L*(</span><span class="s5">1</span><span class="s1">-L)*X.T</span><span class="s3">,</span><span class="s1">X)</span>

    <span class="s3">def </span><span class="s1">hessian_factor(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Logit model Hessian factor 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        hess : ndarray, (nobs,) 
            The Hessian factor, second derivative of loglikelihood function 
            with respect to the linear predictor evaluated at `params` 
        &quot;&quot;&quot;</span>
        <span class="s1">L = self.predict(params)</span>
        <span class="s3">return </span><span class="s1">-L * (</span><span class="s5">1 </span><span class="s1">- L)</span>

    <span class="s1">@Appender(DiscreteModel.fit.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'newton'</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s5">35</span><span class="s3">,</span>
            <span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s1">bnryfit = super().fit(start_params=start_params</span><span class="s3">,</span>
                              <span class="s1">method=method</span><span class="s3">,</span>
                              <span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                              <span class="s1">full_output=full_output</span><span class="s3">,</span>
                              <span class="s1">disp=disp</span><span class="s3">,</span>
                              <span class="s1">callback=callback</span><span class="s3">,</span>
                              <span class="s1">**kwargs)</span>

        <span class="s1">discretefit = LogitResults(self</span><span class="s3">, </span><span class="s1">bnryfit)</span>
        <span class="s3">return </span><span class="s1">BinaryResultsWrapper(discretefit)</span>

    <span class="s3">def </span><span class="s1">_deriv_score_obs_dendog(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot;derivative of score_obs w.r.t. endog 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which score is evaluated 
 
        Returns 
        ------- 
        derivative : ndarray_2d 
            The derivative of the score_obs with respect to endog. This 
            can is given by `score_factor0[:, None] * exog` where 
            `score_factor0` is the score_factor without the residual. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.exog</span>


<span class="s3">class </span><span class="s1">Probit(BinaryModel):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Probit Model 
 
    %(params)s 
    offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : ndarray 
        A reference to the endogenous response variable 
    exog : ndarray 
        A reference to the exogenous design. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s3">,</span>
           <span class="s2">'extra_params'</span><span class="s1">: base._missing_param_doc + _check_rank_doc}</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">link(self):</span>
        <span class="s3">from </span><span class="s1">statsmodels.genmod.families </span><span class="s3">import </span><span class="s1">links</span>
        <span class="s1">link = links.Probit()</span>
        <span class="s3">return </span><span class="s1">link</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        Probit (Normal) cumulative distribution function 
 
        Parameters 
        ---------- 
        X : array_like 
            The linear predictor of the model (XB). 
 
        Returns 
        ------- 
        cdf : ndarray 
            The cdf evaluated at `X`. 
 
        Notes 
        ----- 
        This function is just an alias for scipy.stats.norm.cdf 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">stats.norm._cdf(X)</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        Probit (Normal) probability density function 
 
        Parameters 
        ---------- 
        X : array_like 
            The linear predictor of the model (XB). 
 
        Returns 
        ------- 
        pdf : ndarray 
            The value of the normal density function for each point of X. 
 
        Notes 
        ----- 
        This function is just an alias for scipy.stats.norm.pdf 
        &quot;&quot;&quot;</span>
        <span class="s1">X = np.asarray(X)</span>
        <span class="s3">return </span><span class="s1">stats.norm._pdf(X)</span>


    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Log-likelihood of probit model (i.e., the normal distribution). 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike : float 
            The log-likelihood function of the model evaluated at `params`. 
            See notes. 
 
        Notes 
        ----- 
        .. math:: \\ln L=\\sum_{i}\\ln\\Phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right) 
 
        Where :math:`q=2y-1`. This simplification comes from the fact that the 
        normal distribution is symmetric. 
        &quot;&quot;&quot;</span>

        <span class="s1">q = </span><span class="s5">2</span><span class="s1">*self.endog - </span><span class="s5">1</span>
        <span class="s1">linpred = self.predict(params</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">np.sum(np.log(np.clip(self.cdf(q * linpred)</span><span class="s3">, </span><span class="s1">FLOAT_EPS</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)))</span>

    <span class="s3">def </span><span class="s1">loglikeobs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Log-likelihood of probit model for each observation 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike : array_like 
            The log likelihood for each observation of the model evaluated 
            at `params`. See Notes 
 
        Notes 
        ----- 
        .. math:: \\ln L_{i}=\\ln\\Phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right) 
 
        for observations :math:`i=1,...,n` 
 
        where :math:`q=2y-1`. This simplification comes from the fact that the 
        normal distribution is symmetric. 
        &quot;&quot;&quot;</span>

        <span class="s1">q = </span><span class="s5">2</span><span class="s1">*self.endog - </span><span class="s5">1</span>
        <span class="s1">linpred = self.predict(params</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">np.log(np.clip(self.cdf(q*linpred)</span><span class="s3">, </span><span class="s1">FLOAT_EPS</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>


    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Probit model score (gradient) vector 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        score : ndarray, 1-D 
            The score vector of the model, i.e. the first derivative of the 
            loglikelihood function, evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L}{\\partial\\beta}=\\sum_{i=1}^{n}\\left[\\frac{q_{i}\\phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right)}{\\Phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right)}\\right]x_{i} 
 
        Where :math:`q=2y-1`. This simplification comes from the fact that the 
        normal distribution is symmetric. 
        &quot;&quot;&quot;</span>
        <span class="s1">y = self.endog</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">XB = self.predict(params</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s1">q = </span><span class="s5">2</span><span class="s1">*y - </span><span class="s5">1</span>
        <span class="s4"># clip to get rid of invalid divide complaint</span>
        <span class="s1">L = q*self.pdf(q*XB)/np.clip(self.cdf(q*XB)</span><span class="s3">, </span><span class="s1">FLOAT_EPS</span><span class="s3">, </span><span class="s5">1 </span><span class="s1">- FLOAT_EPS)</span>
        <span class="s3">return </span><span class="s1">np.dot(L</span><span class="s3">,</span><span class="s1">X)</span>

    <span class="s3">def </span><span class="s1">score_obs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Probit model Jacobian for each observation 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        jac : array_like 
            The derivative of the loglikelihood for each observation evaluated 
            at `params`. 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L_{i}}{\\partial\\beta}=\\left[\\frac{q_{i}\\phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right)}{\\Phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right)}\\right]x_{i} 
 
        for observations :math:`i=1,...,n` 
 
        Where :math:`q=2y-1`. This simplification comes from the fact that the 
        normal distribution is symmetric. 
        &quot;&quot;&quot;</span>
        <span class="s1">y = self.endog</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">XB = self.predict(params</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s1">q = </span><span class="s5">2</span><span class="s1">*y - </span><span class="s5">1</span>
        <span class="s4"># clip to get rid of invalid divide complaint</span>
        <span class="s1">L = q*self.pdf(q*XB)/np.clip(self.cdf(q*XB)</span><span class="s3">, </span><span class="s1">FLOAT_EPS</span><span class="s3">, </span><span class="s5">1 </span><span class="s1">- FLOAT_EPS)</span>
        <span class="s3">return </span><span class="s1">L[:</span><span class="s3">,None</span><span class="s1">] * X</span>

    <span class="s3">def </span><span class="s1">score_factor(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Probit model Jacobian for each observation 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        score_factor : array_like (nobs,) 
            The derivative of the loglikelihood function for each observation 
            with respect to linear predictor evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L_{i}}{\\partial\\beta}=\\left[\\frac{q_{i}\\phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right)}{\\Phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right)}\\right]x_{i} 
 
        for observations :math:`i=1,...,n` 
 
        Where :math:`q=2y-1`. This simplification comes from the fact that the 
        normal distribution is symmetric. 
        &quot;&quot;&quot;</span>
        <span class="s1">y = self.endog</span>
        <span class="s1">XB = self.predict(params</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s1">q = </span><span class="s5">2</span><span class="s1">*y - </span><span class="s5">1</span>
        <span class="s4"># clip to get rid of invalid divide complaint</span>
        <span class="s1">L = q*self.pdf(q*XB)/np.clip(self.cdf(q*XB)</span><span class="s3">, </span><span class="s1">FLOAT_EPS</span><span class="s3">, </span><span class="s5">1 </span><span class="s1">- FLOAT_EPS)</span>
        <span class="s3">return </span><span class="s1">L</span>


    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Probit model Hessian matrix of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        hess : ndarray, (k_vars, k_vars) 
            The Hessian, second derivative of loglikelihood function, 
            evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial^{2}\\ln L}{\\partial\\beta\\partial\\beta^{\\prime}}=-\\lambda_{i}\\left(\\lambda_{i}+x_{i}^{\\prime}\\beta\\right)x_{i}x_{i}^{\\prime} 
 
        where 
 
        .. math:: \\lambda_{i}=\\frac{q_{i}\\phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right)}{\\Phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right)} 
 
        and :math:`q=2y-1` 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">XB = self.predict(params</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s1">q = </span><span class="s5">2</span><span class="s1">*self.endog - </span><span class="s5">1</span>
        <span class="s1">L = q*self.pdf(q*XB)/self.cdf(q*XB)</span>
        <span class="s3">return </span><span class="s1">np.dot(-L*(L+XB)*X.T</span><span class="s3">,</span><span class="s1">X)</span>

    <span class="s3">def </span><span class="s1">hessian_factor(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Probit model Hessian factor of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        hess : ndarray, (nobs,) 
            The Hessian factor, second derivative of loglikelihood function 
            with respect to linear predictor evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial^{2}\\ln L}{\\partial\\beta\\partial\\beta^{\\prime}}=-\\lambda_{i}\\left(\\lambda_{i}+x_{i}^{\\prime}\\beta\\right)x_{i}x_{i}^{\\prime} 
 
        where 
 
        .. math:: \\lambda_{i}=\\frac{q_{i}\\phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right)}{\\Phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right)} 
 
        and :math:`q=2y-1` 
        &quot;&quot;&quot;</span>
        <span class="s1">XB = self.predict(params</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s1">q = </span><span class="s5">2 </span><span class="s1">* self.endog - </span><span class="s5">1</span>
        <span class="s1">L = q * self.pdf(q * XB) / self.cdf(q * XB)</span>
        <span class="s3">return </span><span class="s1">-L * (L + XB)</span>

    <span class="s1">@Appender(DiscreteModel.fit.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'newton'</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s5">35</span><span class="s3">,</span>
            <span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s1">bnryfit = super().fit(start_params=start_params</span><span class="s3">,</span>
                              <span class="s1">method=method</span><span class="s3">,</span>
                              <span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                              <span class="s1">full_output=full_output</span><span class="s3">,</span>
                              <span class="s1">disp=disp</span><span class="s3">,</span>
                              <span class="s1">callback=callback</span><span class="s3">,</span>
                              <span class="s1">**kwargs)</span>
        <span class="s1">discretefit = ProbitResults(self</span><span class="s3">, </span><span class="s1">bnryfit)</span>
        <span class="s3">return </span><span class="s1">BinaryResultsWrapper(discretefit)</span>

    <span class="s3">def </span><span class="s1">_deriv_score_obs_dendog(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot;derivative of score_obs w.r.t. endog 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which score is evaluated 
 
        Returns 
        ------- 
        derivative : ndarray_2d 
            The derivative of the score_obs with respect to endog. This 
            can is given by `score_factor0[:, None] * exog` where 
            `score_factor0` is the score_factor without the residual. 
        &quot;&quot;&quot;</span>

        <span class="s1">linpred = self.predict(params</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>

        <span class="s1">pdf_ = self.pdf(linpred)</span>
        <span class="s4"># clip to get rid of invalid divide complaint</span>
        <span class="s1">cdf_ = np.clip(self.cdf(linpred)</span><span class="s3">, </span><span class="s1">FLOAT_EPS</span><span class="s3">, </span><span class="s5">1 </span><span class="s1">- FLOAT_EPS)</span>
        <span class="s1">deriv = pdf_ / cdf_ / (</span><span class="s5">1 </span><span class="s1">- cdf_)  </span><span class="s4"># deriv factor</span>
        <span class="s3">return </span><span class="s1">deriv[:</span><span class="s3">, None</span><span class="s1">] * self.exog</span>


<span class="s3">class </span><span class="s1">MNLogit(MultinomialModel):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Multinomial Logit Model 
 
    Parameters 
    ---------- 
    endog : array_like 
        `endog` is an 1-d vector of the endogenous response.  `endog` can 
        contain strings, ints, or floats or may be a pandas Categorical Series. 
        Note that if it contains strings, every distinct string will be a 
        category.  No stripping of whitespace is done. 
    exog : array_like 
        A nobs x k array where `nobs` is the number of observations and `k` 
        is the number of regressors. An intercept is not included by default 
        and should be added by the user. See `statsmodels.tools.add_constant`. 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : ndarray 
        A reference to the endogenous response variable 
    exog : ndarray 
        A reference to the exogenous design. 
    J : float 
        The number of choices for the endogenous variable. Note that this 
        is zero-indexed. 
    K : float 
        The actual number of parameters for the exogenous design.  Includes 
        the constant if the design has one. 
    names : dict 
        A dictionary mapping the column number in `wendog` to the variables 
        in `endog`. 
    wendog : ndarray 
        An n x j array where j is the number of unique categories in `endog`. 
        Each column of j is a dummy variable indicating the category of 
        each observation. See `names` for a dictionary mapping each column to 
        its category. 
 
    Notes 
    ----- 
    See developer notes for further information on `MNLogit` internals. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'extra_params'</span><span class="s1">: base._missing_param_doc + _check_rank_doc}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">check_rank=</span><span class="s3">True, </span><span class="s1">**kwargs):</span>
        <span class="s1">super().__init__(endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">check_rank=check_rank</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s4"># Override cov_names since multivariate model</span>
        <span class="s1">yname = self.endog_names</span>
        <span class="s1">ynames = self._ynames_map</span>
        <span class="s1">ynames = MultinomialResults._maybe_convert_ynames_int(ynames)</span>
        <span class="s4"># use range below to ensure sortedness</span>
        <span class="s1">ynames = [ynames[key] </span><span class="s3">for </span><span class="s1">key </span><span class="s3">in </span><span class="s1">range(int(self.J))]</span>
        <span class="s1">idx = MultiIndex.from_product((ynames[</span><span class="s5">1</span><span class="s1">:]</span><span class="s3">, </span><span class="s1">self.data.xnames)</span><span class="s3">,</span>
                                      <span class="s1">names=(yname</span><span class="s3">, None</span><span class="s1">))</span>
        <span class="s1">self.data.cov_names = idx</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">eXB):</span>
        <span class="s0">&quot;&quot;&quot; 
        NotImplemented 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        Multinomial logit cumulative distribution function. 
 
        Parameters 
        ---------- 
        X : ndarray 
            The linear predictor of the model XB. 
 
        Returns 
        ------- 
        cdf : ndarray 
            The cdf evaluated at `X`. 
 
        Notes 
        ----- 
        In the multinomial logit model. 
        .. math:: \\frac{\\exp\\left(\\beta_{j}^{\\prime}x_{i}\\right)}{\\sum_{k=0}^{J}\\exp\\left(\\beta_{k}^{\\prime}x_{i}\\right)} 
        &quot;&quot;&quot;</span>
        <span class="s1">eXB = np.column_stack((np.ones(len(X))</span><span class="s3">, </span><span class="s1">np.exp(X)))</span>
        <span class="s3">return </span><span class="s1">eXB/eXB.sum(</span><span class="s5">1</span><span class="s1">)[:</span><span class="s3">,None</span><span class="s1">]</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Log-likelihood of the multinomial logit model. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the multinomial logit model. 
 
        Returns 
        ------- 
        loglike : float 
            The log-likelihood function of the model evaluated at `params`. 
            See notes. 
 
        Notes 
        ----- 
        .. math:: 
 
           \\ln L=\\sum_{i=1}^{n}\\sum_{j=0}^{J}d_{ij}\\ln 
           \\left(\\frac{\\exp\\left(\\beta_{j}^{\\prime}x_{i}\\right)} 
           {\\sum_{k=0}^{J} 
           \\exp\\left(\\beta_{k}^{\\prime}x_{i}\\right)}\\right) 
 
        where :math:`d_{ij}=1` if individual `i` chose alternative `j` and 0 
        if not. 
        &quot;&quot;&quot;</span>
        <span class="s1">params = params.reshape(self.K</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">order=</span><span class="s2">'F'</span><span class="s1">)</span>
        <span class="s1">d = self.wendog</span>
        <span class="s1">logprob = np.log(self.cdf(np.dot(self.exog</span><span class="s3">,</span><span class="s1">params)))</span>
        <span class="s3">return </span><span class="s1">np.sum(d * logprob)</span>

    <span class="s3">def </span><span class="s1">loglikeobs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Log-likelihood of the multinomial logit model for each observation. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the multinomial logit model. 
 
        Returns 
        ------- 
        loglike : array_like 
            The log likelihood for each observation of the model evaluated 
            at `params`. See Notes 
 
        Notes 
        ----- 
        .. math:: 
 
           \\ln L_{i}=\\sum_{j=0}^{J}d_{ij}\\ln 
           \\left(\\frac{\\exp\\left(\\beta_{j}^{\\prime}x_{i}\\right)} 
           {\\sum_{k=0}^{J} 
           \\exp\\left(\\beta_{k}^{\\prime}x_{i}\\right)}\\right) 
 
        for observations :math:`i=1,...,n` 
 
        where :math:`d_{ij}=1` if individual `i` chose alternative `j` and 0 
        if not. 
        &quot;&quot;&quot;</span>
        <span class="s1">params = params.reshape(self.K</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">order=</span><span class="s2">'F'</span><span class="s1">)</span>
        <span class="s1">d = self.wendog</span>
        <span class="s1">logprob = np.log(self.cdf(np.dot(self.exog</span><span class="s3">,</span><span class="s1">params)))</span>
        <span class="s3">return </span><span class="s1">d * logprob</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Score matrix for multinomial logit model log-likelihood 
 
        Parameters 
        ---------- 
        params : ndarray 
            The parameters of the multinomial logit model. 
 
        Returns 
        ------- 
        score : ndarray, (K * (J-1),) 
            The 2-d score vector, i.e. the first derivative of the 
            loglikelihood function, of the multinomial logit model evaluated at 
            `params`. 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L}{\\partial\\beta_{j}}=\\sum_{i}\\left(d_{ij}-\\frac{\\exp\\left(\\beta_{j}^{\\prime}x_{i}\\right)}{\\sum_{k=0}^{J}\\exp\\left(\\beta_{k}^{\\prime}x_{i}\\right)}\\right)x_{i} 
 
        for :math:`j=1,...,J` 
 
        In the multinomial model the score matrix is K x J-1 but is returned 
        as a flattened array to work with the solvers. 
        &quot;&quot;&quot;</span>
        <span class="s1">params = params.reshape(self.K</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">order=</span><span class="s2">'F'</span><span class="s1">)</span>
        <span class="s1">firstterm = self.wendog[:</span><span class="s3">,</span><span class="s5">1</span><span class="s1">:] - self.cdf(np.dot(self.exog</span><span class="s3">,</span>
                                                  <span class="s1">params))[:</span><span class="s3">,</span><span class="s5">1</span><span class="s1">:]</span>
        <span class="s4">#NOTE: might need to switch terms if params is reshaped</span>
        <span class="s3">return </span><span class="s1">np.dot(firstterm.T</span><span class="s3">, </span><span class="s1">self.exog).flatten()</span>

    <span class="s3">def </span><span class="s1">loglike_and_score(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns log likelihood and score, efficiently reusing calculations. 
 
        Note that both of these returned quantities will need to be negated 
        before being minimized by the maximum likelihood fitting machinery. 
        &quot;&quot;&quot;</span>
        <span class="s1">params = params.reshape(self.K</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">order=</span><span class="s2">'F'</span><span class="s1">)</span>
        <span class="s1">cdf_dot_exog_params = self.cdf(np.dot(self.exog</span><span class="s3">, </span><span class="s1">params))</span>
        <span class="s1">loglike_value = np.sum(self.wendog * np.log(cdf_dot_exog_params))</span>
        <span class="s1">firstterm = self.wendog[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">:] - cdf_dot_exog_params[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">:]</span>
        <span class="s1">score_array = np.dot(firstterm.T</span><span class="s3">, </span><span class="s1">self.exog).flatten()</span>
        <span class="s3">return </span><span class="s1">loglike_value</span><span class="s3">, </span><span class="s1">score_array</span>

    <span class="s3">def </span><span class="s1">score_obs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Jacobian matrix for multinomial logit model log-likelihood 
 
        Parameters 
        ---------- 
        params : ndarray 
            The parameters of the multinomial logit model. 
 
        Returns 
        ------- 
        jac : array_like 
            The derivative of the loglikelihood for each observation evaluated 
            at `params` . 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L_{i}}{\\partial\\beta_{j}}=\\left(d_{ij}-\\frac{\\exp\\left(\\beta_{j}^{\\prime}x_{i}\\right)}{\\sum_{k=0}^{J}\\exp\\left(\\beta_{k}^{\\prime}x_{i}\\right)}\\right)x_{i} 
 
        for :math:`j=1,...,J`, for observations :math:`i=1,...,n` 
 
        In the multinomial model the score vector is K x (J-1) but is returned 
        as a flattened array. The Jacobian has the observations in rows and 
        the flattened array of derivatives in columns. 
        &quot;&quot;&quot;</span>
        <span class="s1">params = params.reshape(self.K</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">order=</span><span class="s2">'F'</span><span class="s1">)</span>
        <span class="s1">firstterm = self.wendog[:</span><span class="s3">,</span><span class="s5">1</span><span class="s1">:] - self.cdf(np.dot(self.exog</span><span class="s3">,</span>
                                                  <span class="s1">params))[:</span><span class="s3">,</span><span class="s5">1</span><span class="s1">:]</span>
        <span class="s4">#NOTE: might need to switch terms if params is reshaped</span>
        <span class="s3">return </span><span class="s1">(firstterm[:</span><span class="s3">,</span><span class="s1">:</span><span class="s3">,None</span><span class="s1">] * self.exog[:</span><span class="s3">,None,</span><span class="s1">:]).reshape(self.exog.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Multinomial logit Hessian matrix of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        hess : ndarray, (J*K, J*K) 
            The Hessian, second derivative of loglikelihood function with 
            respect to the flattened parameters, evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial^{2}\\ln L}{\\partial\\beta_{j}\\partial\\beta_{l}}=-\\sum_{i=1}^{n}\\frac{\\exp\\left(\\beta_{j}^{\\prime}x_{i}\\right)}{\\sum_{k=0}^{J}\\exp\\left(\\beta_{k}^{\\prime}x_{i}\\right)}\\left[\\boldsymbol{1}\\left(j=l\\right)-\\frac{\\exp\\left(\\beta_{l}^{\\prime}x_{i}\\right)}{\\sum_{k=0}^{J}\\exp\\left(\\beta_{k}^{\\prime}x_{i}\\right)}\\right]x_{i}x_{l}^{\\prime} 
 
        where 
        :math:`\\boldsymbol{1}\\left(j=l\\right)` equals 1 if `j` = `l` and 0 
        otherwise. 
 
        The actual Hessian matrix has J**2 * K x K elements. Our Hessian 
        is reshaped to be square (J*K, J*K) so that the solvers can use it. 
 
        This implementation does not take advantage of the symmetry of 
        the Hessian and could probably be refactored for speed. 
        &quot;&quot;&quot;</span>
        <span class="s1">params = params.reshape(self.K</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">order=</span><span class="s2">'F'</span><span class="s1">)</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">pr = self.cdf(np.dot(X</span><span class="s3">,</span><span class="s1">params))</span>
        <span class="s1">partials = []</span>
        <span class="s1">J = self.J</span>
        <span class="s1">K = self.K</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(J-</span><span class="s5">1</span><span class="s1">):</span>
            <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(J-</span><span class="s5">1</span><span class="s1">): </span><span class="s4"># this loop assumes we drop the first col.</span>
                <span class="s3">if </span><span class="s1">i == j:</span>
                    <span class="s1">partials.append(\</span>
                        <span class="s1">-np.dot(((pr[:</span><span class="s3">,</span><span class="s1">i+</span><span class="s5">1</span><span class="s1">]*(</span><span class="s5">1</span><span class="s1">-pr[:</span><span class="s3">,</span><span class="s1">j+</span><span class="s5">1</span><span class="s1">]))[:</span><span class="s3">,None</span><span class="s1">]*X).T</span><span class="s3">,</span><span class="s1">X))</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">partials.append(-np.dot(((pr[:</span><span class="s3">,</span><span class="s1">i+</span><span class="s5">1</span><span class="s1">]*-pr[:</span><span class="s3">,</span><span class="s1">j+</span><span class="s5">1</span><span class="s1">])[:</span><span class="s3">,None</span><span class="s1">]*X).T</span><span class="s3">,</span><span class="s1">X))</span>
        <span class="s1">H = np.array(partials)</span>
        <span class="s4"># the developer's notes on multinomial should clear this math up</span>
        <span class="s1">H = np.transpose(H.reshape(J-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">J-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">K</span><span class="s3">, </span><span class="s1">K)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)).reshape((J-</span><span class="s5">1</span><span class="s1">)*K</span><span class="s3">, </span><span class="s1">(J-</span><span class="s5">1</span><span class="s1">)*K)</span>
        <span class="s3">return </span><span class="s1">H</span>


<span class="s4">#TODO: Weibull can replaced by a survival analsysis function</span>
<span class="s4"># like stat's streg (The cox model as well)</span>
<span class="s4">#class Weibull(DiscreteModel):</span>
<span class="s4">#    &quot;&quot;&quot;</span>
<span class="s4">#    Binary choice Weibull model</span>
<span class="s4">#</span>
<span class="s4">#    Notes</span>
<span class="s4">#    ------</span>
<span class="s4">#    This is unfinished and untested.</span>
<span class="s4">#    &quot;&quot;&quot;</span>
<span class="s4">##TODO: add analytic hessian for Weibull</span>
<span class="s4">#    def initialize(self):</span>
<span class="s4">#        pass</span>
<span class="s4">#</span>
<span class="s4">#    def cdf(self, X):</span>
<span class="s4">#        &quot;&quot;&quot;</span>
<span class="s4">#        Gumbell (Log Weibull) cumulative distribution function</span>
<span class="s4">#        &quot;&quot;&quot;</span>
<span class="s4">##        return np.exp(-np.exp(-X))</span>
<span class="s4">#        return stats.gumbel_r.cdf(X)</span>
<span class="s4">#        # these two are equivalent.</span>
<span class="s4">#        # Greene table and discussion is incorrect.</span>
<span class="s4">#</span>
<span class="s4">#    def pdf(self, X):</span>
<span class="s4">#        &quot;&quot;&quot;</span>
<span class="s4">#        Gumbell (LogWeibull) probability distribution function</span>
<span class="s4">#        &quot;&quot;&quot;</span>
<span class="s4">#        return stats.gumbel_r.pdf(X)</span>
<span class="s4">#</span>
<span class="s4">#    def loglike(self, params):</span>
<span class="s4">#        &quot;&quot;&quot;</span>
<span class="s4">#        Loglikelihood of Weibull distribution</span>
<span class="s4">#        &quot;&quot;&quot;</span>
<span class="s4">#        X = self.exog</span>
<span class="s4">#        cdf = self.cdf(np.dot(X,params))</span>
<span class="s4">#        y = self.endog</span>
<span class="s4">#        return np.sum(y*np.log(cdf) + (1-y)*np.log(1-cdf))</span>
<span class="s4">#</span>
<span class="s4">#    def score(self, params):</span>
<span class="s4">#        y = self.endog</span>
<span class="s4">#        X = self.exog</span>
<span class="s4">#        F = self.cdf(np.dot(X,params))</span>
<span class="s4">#        f = self.pdf(np.dot(X,params))</span>
<span class="s4">#        term = (y*f/F + (1 - y)*-f/(1-F))</span>
<span class="s4">#        return np.dot(term,X)</span>
<span class="s4">#</span>
<span class="s4">#    def hessian(self, params):</span>
<span class="s4">#        hess = nd.Jacobian(self.score)</span>
<span class="s4">#        return hess(params)</span>
<span class="s4">#</span>
<span class="s4">#    def fit(self, start_params=None, method='newton', maxiter=35, tol=1e-08):</span>
<span class="s4">## The example had problems with all zero start values, Hessian = 0</span>
<span class="s4">#        if start_params is None:</span>
<span class="s4">#            start_params = OLS(self.endog, self.exog).fit().params</span>
<span class="s4">#        mlefit = super(Weibull, self).fit(start_params=start_params,</span>
<span class="s4">#                method=method, maxiter=maxiter, tol=tol)</span>
<span class="s4">#        return mlefit</span>
<span class="s4">#</span>


<span class="s3">class </span><span class="s1">NegativeBinomial(CountModel):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Negative Binomial Model 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : ndarray 
        A reference to the endogenous response variable 
    exog : ndarray 
        A reference to the exogenous design. 
 
    References 
    ---------- 
    Greene, W. 2008. &quot;Functional forms for the negative binomial model 
        for count data&quot;. Economics Letters. Volume 99, Number 3, pp.585-590. 
    Hilbe, J.M. 2011. &quot;Negative binomial regression&quot;. Cambridge University 
        Press. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s3">,</span>
           <span class="s2">'extra_params'</span><span class="s1">:</span>
           <span class="s2">&quot;&quot;&quot;loglike_method : str 
        Log-likelihood type. 'nb2','nb1', or 'geometric'. 
        Fitted value :math:`</span><span class="s3">\\</span><span class="s2">mu` 
        Heterogeneity parameter :math:`</span><span class="s3">\\</span><span class="s2">alpha` 
 
        - nb2: Variance equal to :math:`</span><span class="s3">\\</span><span class="s2">mu + </span><span class="s3">\\</span><span class="s2">alpha</span><span class="s3">\\</span><span class="s2">mu^2` (most common) 
        - nb1: Variance equal to :math:`</span><span class="s3">\\</span><span class="s2">mu + </span><span class="s3">\\</span><span class="s2">alpha</span><span class="s3">\\</span><span class="s2">mu` 
        - geometric: Variance equal to :math:`</span><span class="s3">\\</span><span class="s2">mu + </span><span class="s3">\\</span><span class="s2">mu^2` 
    offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    exposure : array_like 
        Log(exposure) is added to the linear prediction with coefficient 
        equal to 1. 
    &quot;&quot;&quot; </span><span class="s1">+ base._missing_param_doc + _check_rank_doc}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">loglike_method=</span><span class="s2">'nb2'</span><span class="s3">, </span><span class="s1">offset=</span><span class="s3">None,</span>
                 <span class="s1">exposure=</span><span class="s3">None, </span><span class="s1">missing=</span><span class="s2">'none'</span><span class="s3">, </span><span class="s1">check_rank=</span><span class="s3">True, </span><span class="s1">**kwargs):</span>
        <span class="s1">super().__init__(endog</span><span class="s3">,</span>
                         <span class="s1">exog</span><span class="s3">,</span>
                         <span class="s1">offset=offset</span><span class="s3">,</span>
                         <span class="s1">exposure=exposure</span><span class="s3">,</span>
                         <span class="s1">missing=missing</span><span class="s3">,</span>
                         <span class="s1">check_rank=check_rank</span><span class="s3">,</span>
                         <span class="s1">**kwargs)</span>
        <span class="s1">self.loglike_method = loglike_method</span>
        <span class="s1">self._initialize()</span>
        <span class="s3">if </span><span class="s1">loglike_method </span><span class="s3">in </span><span class="s1">[</span><span class="s2">'nb2'</span><span class="s3">, </span><span class="s2">'nb1'</span><span class="s1">]:</span>
            <span class="s1">self.exog_names.append(</span><span class="s2">'alpha'</span><span class="s1">)</span>
            <span class="s1">self.k_extra = </span><span class="s5">1</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.k_extra = </span><span class="s5">0</span>
        <span class="s4"># store keys for extras if we need to recreate model instance</span>
        <span class="s4"># we need to append keys that do not go to super</span>
        <span class="s1">self._init_keys.append(</span><span class="s2">'loglike_method'</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_initialize(self):</span>
        <span class="s3">if </span><span class="s1">self.loglike_method == </span><span class="s2">'nb2'</span><span class="s1">:</span>
            <span class="s1">self.hessian = self._hessian_nb2</span>
            <span class="s1">self.score = self._score_nbin</span>
            <span class="s1">self.loglikeobs = self._ll_nb2</span>
            <span class="s1">self._transparams = </span><span class="s3">True  </span><span class="s4"># transform lnalpha -&gt; alpha in fit</span>
        <span class="s3">elif </span><span class="s1">self.loglike_method == </span><span class="s2">'nb1'</span><span class="s1">:</span>
            <span class="s1">self.hessian = self._hessian_nb1</span>
            <span class="s1">self.score = self._score_nb1</span>
            <span class="s1">self.loglikeobs = self._ll_nb1</span>
            <span class="s1">self._transparams = </span><span class="s3">True  </span><span class="s4"># transform lnalpha -&gt; alpha in fit</span>
        <span class="s3">elif </span><span class="s1">self.loglike_method == </span><span class="s2">'geometric'</span><span class="s1">:</span>
            <span class="s1">self.hessian = self._hessian_geom</span>
            <span class="s1">self.score = self._score_geom</span>
            <span class="s1">self.loglikeobs = self._ll_geometric</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s2">'Likelihood type must &quot;nb1&quot;, &quot;nb2&quot; '</span>
                             <span class="s2">'or &quot;geometric&quot;'</span><span class="s1">)</span>

    <span class="s4"># Workaround to pickle instance methods</span>
    <span class="s3">def </span><span class="s1">__getstate__(self):</span>
        <span class="s1">odict = self.__dict__.copy()  </span><span class="s4"># copy the dict since we change it</span>
        <span class="s3">del </span><span class="s1">odict[</span><span class="s2">'hessian'</span><span class="s1">]</span>
        <span class="s3">del </span><span class="s1">odict[</span><span class="s2">'score'</span><span class="s1">]</span>
        <span class="s3">del </span><span class="s1">odict[</span><span class="s2">'loglikeobs'</span><span class="s1">]</span>
        <span class="s3">return </span><span class="s1">odict</span>

    <span class="s3">def </span><span class="s1">__setstate__(self</span><span class="s3">, </span><span class="s1">indict):</span>
        <span class="s1">self.__dict__.update(indict)</span>
        <span class="s1">self._initialize()</span>

    <span class="s3">def </span><span class="s1">_ll_nbin(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">Q=</span><span class="s5">0</span><span class="s1">):</span>
        <span class="s3">if </span><span class="s1">np.any(np.iscomplex(params)) </span><span class="s3">or </span><span class="s1">np.iscomplex(alpha):</span>
            <span class="s1">gamma_ln = loggamma</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">gamma_ln = gammaln</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">mu = self.predict(params)</span>
        <span class="s1">size = </span><span class="s5">1</span><span class="s1">/alpha * mu**Q</span>
        <span class="s1">prob = size/(size+mu)</span>
        <span class="s1">coeff = (gamma_ln(size+endog) - gamma_ln(endog+</span><span class="s5">1</span><span class="s1">) -</span>
                 <span class="s1">gamma_ln(size))</span>
        <span class="s1">llf = coeff + size*np.log(prob) + endog*np.log(</span><span class="s5">1</span><span class="s1">-prob)</span>
        <span class="s3">return </span><span class="s1">llf</span>

    <span class="s3">def </span><span class="s1">_ll_nb2(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">if </span><span class="s1">self._transparams:  </span><span class="s4"># got lnalpha during fit</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">return </span><span class="s1">self._ll_nbin(params[:-</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">Q=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_ll_nb1(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">if </span><span class="s1">self._transparams:  </span><span class="s4"># got lnalpha during fit</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">return </span><span class="s1">self._ll_nbin(params[:-</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">Q=</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_ll_geometric(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s4"># we give alpha of 1 because it's actually log(alpha) where alpha=0</span>
        <span class="s3">return </span><span class="s1">self._ll_nbin(params</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">r&quot;&quot;&quot; 
        Loglikelihood for negative binomial model 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. If `loglike_method` is nb1 or 
            nb2, then the ancillary parameter is expected to be the 
            last element. 
 
        Returns 
        ------- 
        llf : float 
            The loglikelihood value at `params` 
 
        Notes 
        ----- 
        Following notation in Greene (2008), with negative binomial 
        heterogeneity parameter :math:`\alpha`: 
 
        .. math:: 
 
           \lambda_i &amp;= exp(X\beta) \\ 
           \theta &amp;= 1 / \alpha \\ 
           g_i &amp;= \theta \lambda_i^Q \\ 
           w_i &amp;= g_i/(g_i + \lambda_i) \\ 
           r_i &amp;= \theta / (\theta+\lambda_i) \\ 
           ln \mathcal{L}_i &amp;= ln \Gamma(y_i+g_i) - ln \Gamma(1+y_i) + g_iln (r_i) + y_i ln(1-r_i) 
 
        where :math`Q=0` for NB2 and geometric and :math:`Q=1` for NB1. 
        For the geometric, :math:`\alpha=0` as well. 
        &quot;&quot;&quot;</span>
        <span class="s1">llf = np.sum(self.loglikeobs(params))</span>
        <span class="s3">return </span><span class="s1">llf</span>

    <span class="s3">def </span><span class="s1">_score_geom(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s1">exog = self.exog</span>
        <span class="s1">y = self.endog[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s1">mu = self.predict(params)[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s1">dparams = exog * (y-mu)/(mu+</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">dparams.sum(</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_score_nbin(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">Q=</span><span class="s5">0</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Score vector for NB2 model 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._transparams: </span><span class="s4"># lnalpha came in during fit</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">exog = self.exog</span>
        <span class="s1">y = self.endog[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s1">mu = self.predict(params)[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s1">a1 = </span><span class="s5">1</span><span class="s1">/alpha * mu**Q</span>
        <span class="s1">prob = a1 / (a1 + mu)  </span><span class="s4"># a1 aka &quot;size&quot; in _ll_nbin</span>
        <span class="s3">if </span><span class="s1">Q == </span><span class="s5">1</span><span class="s1">:  </span><span class="s4"># nb1</span>
            <span class="s4"># Q == 1 --&gt; a1 = mu / alpha --&gt; prob = 1 / (alpha + 1)</span>
            <span class="s1">dgpart = digamma(y + a1) - digamma(a1)</span>
            <span class="s1">dparams = exog * a1 * (np.log(prob) +</span>
                       <span class="s1">dgpart)</span>
            <span class="s1">dalpha = ((alpha * (y - mu * np.log(prob) -</span>
                              <span class="s1">mu*(dgpart + </span><span class="s5">1</span><span class="s1">)) -</span>
                       <span class="s1">mu * (np.log(prob) +</span>
                           <span class="s1">dgpart))/</span>
                       <span class="s1">(alpha**</span><span class="s5">2</span><span class="s1">*(alpha + </span><span class="s5">1</span><span class="s1">))).sum()</span>

        <span class="s3">elif </span><span class="s1">Q == </span><span class="s5">0</span><span class="s1">:  </span><span class="s4"># nb2</span>
            <span class="s1">dgpart = digamma(y + a1) - digamma(a1)</span>
            <span class="s1">dparams = exog*a1 * (y-mu)/(mu+a1)</span>
            <span class="s1">da1 = -alpha**-</span><span class="s5">2</span>
            <span class="s1">dalpha = (dgpart + np.log(a1)</span>
                        <span class="s1">- np.log(a1+mu) - (y-mu)/(a1+mu)).sum() * da1</span>

        <span class="s4">#multiply above by constant outside sum to reduce rounding error</span>
        <span class="s3">if </span><span class="s1">self._transparams:</span>
            <span class="s3">return </span><span class="s1">np.r_[dparams.sum(</span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dalpha*alpha]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">np.r_[dparams.sum(</span><span class="s5">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dalpha]</span>

    <span class="s3">def </span><span class="s1">_score_nb1(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">return </span><span class="s1">self._score_nbin(params</span><span class="s3">, </span><span class="s1">Q=</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_hessian_geom(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s1">exog = self.exog</span>
        <span class="s1">y = self.endog[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s1">mu = self.predict(params)[:</span><span class="s3">,None</span><span class="s1">]</span>

        <span class="s4"># for dl/dparams dparams</span>
        <span class="s1">dim = exog.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">hess_arr = np.empty((dim</span><span class="s3">, </span><span class="s1">dim))</span>
        <span class="s1">const_arr = mu*(</span><span class="s5">1</span><span class="s1">+y)/(mu+</span><span class="s5">1</span><span class="s1">)**</span><span class="s5">2</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(dim):</span>
            <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(dim):</span>
                <span class="s3">if </span><span class="s1">j &gt; i:</span>
                    <span class="s3">continue</span>
                <span class="s1">hess_arr[i</span><span class="s3">,</span><span class="s1">j] = np.squeeze(</span>
                    <span class="s1">np.sum(-exog[:</span><span class="s3">,</span><span class="s1">i</span><span class="s3">,None</span><span class="s1">] * exog[:</span><span class="s3">,</span><span class="s1">j</span><span class="s3">,None</span><span class="s1">] * const_arr</span><span class="s3">,</span>
                           <span class="s1">axis=</span><span class="s5">0</span>
                           <span class="s1">)</span>
                <span class="s1">)</span>
        <span class="s1">tri_idx = np.triu_indices(dim</span><span class="s3">, </span><span class="s1">k=</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">hess_arr[tri_idx] = hess_arr.T[tri_idx]</span>
        <span class="s3">return </span><span class="s1">hess_arr</span>


    <span class="s3">def </span><span class="s1">_hessian_nb1(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Hessian of NB1 model. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._transparams: </span><span class="s4"># lnalpha came in during fit</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">exog = self.exog</span>
        <span class="s1">y = self.endog[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s1">mu = self.predict(params)[:</span><span class="s3">,None</span><span class="s1">]</span>

        <span class="s1">a1 = mu/alpha</span>
        <span class="s1">dgpart = digamma(y + a1) - digamma(a1)</span>
        <span class="s1">prob = </span><span class="s5">1 </span><span class="s1">/ (</span><span class="s5">1 </span><span class="s1">+ alpha)  </span><span class="s4"># equiv: a1 / (a1 + mu)</span>

        <span class="s4"># for dl/dparams dparams</span>
        <span class="s1">dim = exog.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">hess_arr = np.empty((dim+</span><span class="s5">1</span><span class="s3">,</span><span class="s1">dim+</span><span class="s5">1</span><span class="s1">))</span>
        <span class="s4">#const_arr = a1*mu*(a1+y)/(mu+a1)**2</span>
        <span class="s4"># not all of dparams</span>
        <span class="s1">dparams = exog / alpha * (np.log(prob) +</span>
                                  <span class="s1">dgpart)</span>

        <span class="s1">dmudb = exog*mu</span>
        <span class="s1">xmu_alpha = exog * a1</span>
        <span class="s1">trigamma = (special.polygamma(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">a1 + y) -</span>
                    <span class="s1">special.polygamma(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">a1))</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(dim):</span>
            <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(dim):</span>
                <span class="s3">if </span><span class="s1">j &gt; i:</span>
                    <span class="s3">continue</span>
                <span class="s1">hess_arr[i</span><span class="s3">,</span><span class="s1">j] = np.squeeze(</span>
                    <span class="s1">np.sum(</span>
                        <span class="s1">dparams[:</span><span class="s3">,</span><span class="s1">i</span><span class="s3">,None</span><span class="s1">] * dmudb[:</span><span class="s3">,</span><span class="s1">j</span><span class="s3">,None</span><span class="s1">] +</span>
                        <span class="s1">xmu_alpha[:</span><span class="s3">,</span><span class="s1">i</span><span class="s3">,None</span><span class="s1">] * xmu_alpha[:</span><span class="s3">,</span><span class="s1">j</span><span class="s3">,None</span><span class="s1">] * trigamma</span><span class="s3">,</span>
                        <span class="s1">axis=</span><span class="s5">0</span>
                    <span class="s1">)</span>
                <span class="s1">)</span>
        <span class="s1">tri_idx = np.triu_indices(dim</span><span class="s3">, </span><span class="s1">k=</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">hess_arr[tri_idx] = hess_arr.T[tri_idx]</span>

        <span class="s4"># for dl/dparams dalpha</span>
        <span class="s4"># da1 = -alpha**-2</span>
        <span class="s1">dldpda = np.sum(-a1 * dparams + exog * a1 *</span>
                        <span class="s1">(-trigamma*mu/alpha**</span><span class="s5">2 </span><span class="s1">- prob)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>

        <span class="s1">hess_arr[-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">:-</span><span class="s5">1</span><span class="s1">] = dldpda</span>
        <span class="s1">hess_arr[:-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">-</span><span class="s5">1</span><span class="s1">] = dldpda</span>

        <span class="s1">log_alpha = np.log(prob)</span>
        <span class="s1">alpha3 = alpha**</span><span class="s5">3</span>
        <span class="s1">alpha2 = alpha**</span><span class="s5">2</span>
        <span class="s1">mu2 = mu**</span><span class="s5">2</span>
        <span class="s1">dada = ((alpha3*mu*(</span><span class="s5">2</span><span class="s1">*log_alpha + </span><span class="s5">2</span><span class="s1">*dgpart + </span><span class="s5">3</span><span class="s1">) -</span>
                 <span class="s5">2</span><span class="s1">*alpha3*y +</span>
                 <span class="s5">4</span><span class="s1">*alpha2*mu*(log_alpha + dgpart) +</span>
                 <span class="s1">alpha2 * (</span><span class="s5">2</span><span class="s1">*mu - y) +</span>
                 <span class="s5">2</span><span class="s1">*alpha*mu2*trigamma + mu2 * trigamma + alpha2 * mu2 * trigamma +</span>
                 <span class="s5">2</span><span class="s1">*alpha*mu*(log_alpha + dgpart)</span>
                 <span class="s1">)/(alpha**</span><span class="s5">4</span><span class="s1">*(alpha2 + </span><span class="s5">2</span><span class="s1">*alpha + </span><span class="s5">1</span><span class="s1">)))</span>
        <span class="s1">hess_arr[-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">-</span><span class="s5">1</span><span class="s1">] = dada.sum()</span>

        <span class="s3">return </span><span class="s1">hess_arr</span>

    <span class="s3">def </span><span class="s1">_hessian_nb2(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Hessian of NB2 model. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._transparams: </span><span class="s4"># lnalpha came in during fit</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">a1 = </span><span class="s5">1</span><span class="s1">/alpha</span>
        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">exog = self.exog</span>
        <span class="s1">y = self.endog[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s1">mu = self.predict(params)[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s1">prob = a1 / (a1 + mu)</span>
        <span class="s1">dgpart = digamma(a1 + y) - digamma(a1)</span>

        <span class="s4"># for dl/dparams dparams</span>
        <span class="s1">dim = exog.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">hess_arr = np.empty((dim+</span><span class="s5">1</span><span class="s3">,</span><span class="s1">dim+</span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">const_arr = a1*mu*(a1+y)/(mu+a1)**</span><span class="s5">2</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(dim):</span>
            <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(dim):</span>
                <span class="s3">if </span><span class="s1">j &gt; i:</span>
                    <span class="s3">continue</span>
                <span class="s1">hess_arr[i</span><span class="s3">,</span><span class="s1">j] = np.sum(-exog[:</span><span class="s3">,</span><span class="s1">i</span><span class="s3">,None</span><span class="s1">] * exog[:</span><span class="s3">,</span><span class="s1">j</span><span class="s3">,None</span><span class="s1">] *</span>
                                       <span class="s1">const_arr</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">).squeeze()</span>
        <span class="s1">tri_idx = np.triu_indices(dim</span><span class="s3">, </span><span class="s1">k=</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">hess_arr[tri_idx] = hess_arr.T[tri_idx]</span>

        <span class="s4"># for dl/dparams dalpha</span>
        <span class="s1">da1 = -alpha**-</span><span class="s5">2</span>
        <span class="s1">dldpda = -np.sum(mu*exog*(y-mu)*a1**</span><span class="s5">2</span><span class="s1">/(mu+a1)**</span><span class="s5">2 </span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">hess_arr[-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">:-</span><span class="s5">1</span><span class="s1">] = dldpda</span>
        <span class="s1">hess_arr[:-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">-</span><span class="s5">1</span><span class="s1">] = dldpda</span>

        <span class="s4"># for dl/dalpha dalpha</span>
        <span class="s4">#NOTE: polygamma(1,x) is the trigamma function</span>
        <span class="s1">da2 = </span><span class="s5">2</span><span class="s1">*alpha**-</span><span class="s5">3</span>
        <span class="s1">dalpha = da1 * (dgpart +</span>
                    <span class="s1">np.log(prob) - (y - mu)/(a1+mu))</span>
        <span class="s1">dada = (da2 * dalpha/da1 + da1**</span><span class="s5">2 </span><span class="s1">* (special.polygamma(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">a1+y) -</span>
                    <span class="s1">special.polygamma(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">a1) + </span><span class="s5">1</span><span class="s1">/a1 - </span><span class="s5">1</span><span class="s1">/(a1 + mu) +</span>
                    <span class="s1">(y - mu)/(mu + a1)**</span><span class="s5">2</span><span class="s1">)).sum()</span>
        <span class="s1">hess_arr[-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">-</span><span class="s5">1</span><span class="s1">] = dada</span>

        <span class="s3">return </span><span class="s1">hess_arr</span>

    <span class="s4">#TODO: replace this with analytic where is it used?</span>
    <span class="s3">def </span><span class="s1">score_obs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s1">sc = approx_fprime_cs(params</span><span class="s3">, </span><span class="s1">self.loglikeobs)</span>
        <span class="s3">return </span><span class="s1">sc</span>

    <span class="s1">@Appender(Poisson.predict.__doc__)</span>
    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exposure=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None,</span>
                <span class="s1">which=</span><span class="s2">'mean'</span><span class="s3">, </span><span class="s1">linear=</span><span class="s3">None, </span><span class="s1">y_values=</span><span class="s3">None</span><span class="s1">):</span>

        <span class="s3">if </span><span class="s1">linear </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">msg = </span><span class="s2">'linear keyword is deprecated, use which=&quot;linear&quot;'</span>
            <span class="s1">warnings.warn(msg</span><span class="s3">, </span><span class="s1">FutureWarning)</span>
            <span class="s3">if </span><span class="s1">linear </span><span class="s3">is True</span><span class="s1">:</span>
                <span class="s1">which = </span><span class="s2">&quot;linear&quot;</span>

        <span class="s4"># avoid duplicate computation for get-distribution</span>
        <span class="s3">if </span><span class="s1">which == </span><span class="s2">&quot;prob&quot;</span><span class="s1">:</span>
            <span class="s1">distr = self.get_distribution(</span>
                <span class="s1">params</span><span class="s3">,</span>
                <span class="s1">exog=exog</span><span class="s3">,</span>
                <span class="s1">exposure=exposure</span><span class="s3">,</span>
                <span class="s1">offset=offset</span>
                <span class="s1">)</span>
            <span class="s3">if </span><span class="s1">y_values </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">y_values = np.arange(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">np.max(self.endog) + </span><span class="s5">1</span><span class="s1">)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">y_values = np.asarray(y_values)</span>

            <span class="s3">assert </span><span class="s1">y_values.ndim == </span><span class="s5">1</span>
            <span class="s1">y_values = y_values[...</span><span class="s3">, None</span><span class="s1">]</span>
            <span class="s3">return </span><span class="s1">distr.pmf(y_values).T</span>

        <span class="s1">exog</span><span class="s3">, </span><span class="s1">offset</span><span class="s3">, </span><span class="s1">exposure = self._get_predict_arrays(</span>
            <span class="s1">exog=exog</span><span class="s3">,</span>
            <span class="s1">offset=offset</span><span class="s3">,</span>
            <span class="s1">exposure=exposure</span>
            <span class="s1">)</span>

        <span class="s1">fitted = np.dot(exog</span><span class="s3">, </span><span class="s1">params[:exog.shape[</span><span class="s5">1</span><span class="s1">]])</span>
        <span class="s1">linpred = fitted + exposure + offset</span>
        <span class="s3">if </span><span class="s1">which == </span><span class="s2">&quot;mean&quot;</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">np.exp(linpred)</span>
        <span class="s3">elif </span><span class="s1">which.startswith(</span><span class="s2">&quot;lin&quot;</span><span class="s1">):</span>
            <span class="s3">return </span><span class="s1">linpred</span>
        <span class="s3">elif </span><span class="s1">which == </span><span class="s2">&quot;var&quot;</span><span class="s1">:</span>
            <span class="s1">mu = np.exp(linpred)</span>
            <span class="s3">if </span><span class="s1">self.loglike_method == </span><span class="s2">'geometric'</span><span class="s1">:</span>
                <span class="s1">var_ = mu * (</span><span class="s5">1 </span><span class="s1">+ mu)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s3">if </span><span class="s1">self.loglike_method == </span><span class="s2">'nb2'</span><span class="s1">:</span>
                    <span class="s1">p = </span><span class="s5">2</span>
                <span class="s3">elif </span><span class="s1">self.loglike_method == </span><span class="s2">'nb1'</span><span class="s1">:</span>
                    <span class="s1">p = </span><span class="s5">1</span>
                <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
                <span class="s1">var_ = mu * (</span><span class="s5">1 </span><span class="s1">+ alpha * mu**(p - </span><span class="s5">1</span><span class="s1">))</span>
            <span class="s3">return </span><span class="s1">var_</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s2">'keyword which has to be &quot;mean&quot; and &quot;linear&quot;'</span><span class="s1">)</span>

    <span class="s1">@Appender(_get_start_params_null_docs)</span>
    <span class="s3">def </span><span class="s1">_get_start_params_null(self):</span>
        <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">exposure = getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">const = (self.endog / np.exp(offset + exposure)).mean()</span>
        <span class="s1">params = [np.log(const)]</span>
        <span class="s1">mu = const * np.exp(offset + exposure)</span>
        <span class="s1">resid = self.endog - mu</span>
        <span class="s1">a = self._estimate_dispersion(mu</span><span class="s3">, </span><span class="s1">resid</span><span class="s3">, </span><span class="s1">df_resid=resid.shape[</span><span class="s5">0</span><span class="s1">] - </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">params.append(a)</span>
        <span class="s3">return </span><span class="s1">np.array(params)</span>

    <span class="s3">def </span><span class="s1">_estimate_dispersion(self</span><span class="s3">, </span><span class="s1">mu</span><span class="s3">, </span><span class="s1">resid</span><span class="s3">, </span><span class="s1">df_resid=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s3">if </span><span class="s1">df_resid </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">df_resid = resid.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s3">if </span><span class="s1">self.loglike_method == </span><span class="s2">'nb2'</span><span class="s1">:</span>
            <span class="s4">#params.append(np.linalg.pinv(mu[:,None]).dot(resid**2 / mu - 1))</span>
            <span class="s1">a = ((resid**</span><span class="s5">2 </span><span class="s1">/ mu - </span><span class="s5">1</span><span class="s1">) / mu).sum() / df_resid</span>
        <span class="s3">else</span><span class="s1">: </span><span class="s4">#self.loglike_method == 'nb1':</span>
            <span class="s1">a = (resid**</span><span class="s5">2 </span><span class="s1">/ mu - </span><span class="s5">1</span><span class="s1">).sum() / df_resid</span>
        <span class="s3">return </span><span class="s1">a</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'bfgs'</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s5">35</span><span class="s3">,</span>
            <span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None,</span>
            <span class="s1">cov_type=</span><span class="s2">'nonrobust'</span><span class="s3">, </span><span class="s1">cov_kwds=</span><span class="s3">None, </span><span class="s1">use_t=</span><span class="s3">None,</span>
            <span class="s1">optim_kwds_prelim=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>

        <span class="s4"># Note: do not let super handle robust covariance because it has</span>
        <span class="s4"># transformed params</span>
        <span class="s1">self._transparams = </span><span class="s3">False </span><span class="s4"># always define attribute</span>
        <span class="s3">if </span><span class="s1">self.loglike_method.startswith(</span><span class="s2">'nb'</span><span class="s1">) </span><span class="s3">and </span><span class="s1">method </span><span class="s3">not in </span><span class="s1">[</span><span class="s2">'newton'</span><span class="s3">,</span>
                                                                   <span class="s2">'ncg'</span><span class="s1">]:</span>
            <span class="s1">self._transparams = </span><span class="s3">True </span><span class="s4"># in case same Model instance is refit</span>
        <span class="s3">elif </span><span class="s1">self.loglike_method.startswith(</span><span class="s2">'nb'</span><span class="s1">): </span><span class="s4"># method is newton/ncg</span>
            <span class="s1">self._transparams = </span><span class="s3">False </span><span class="s4"># because we need to step in alpha space</span>

        <span class="s3">if </span><span class="s1">start_params </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s4"># Use poisson fit as first guess.</span>
            <span class="s4">#TODO, Warning: this assumes exposure is logged</span>
            <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">) + getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">np.size(offset) == </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">offset == </span><span class="s5">0</span><span class="s1">:</span>
                <span class="s1">offset = </span><span class="s3">None</span>
            <span class="s1">kwds_prelim = {</span><span class="s2">'disp'</span><span class="s1">: </span><span class="s5">0</span><span class="s3">, </span><span class="s2">'skip_hessian'</span><span class="s1">: </span><span class="s3">True, </span><span class="s2">'warn_convergence'</span><span class="s1">: </span><span class="s3">False</span><span class="s1">}</span>
            <span class="s3">if </span><span class="s1">optim_kwds_prelim </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">kwds_prelim.update(optim_kwds_prelim)</span>
            <span class="s1">mod_poi = Poisson(self.endog</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">, </span><span class="s1">offset=offset)</span>
            <span class="s3">with </span><span class="s1">warnings.catch_warnings():</span>
                <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;always&quot;</span><span class="s1">)</span>
                <span class="s1">res_poi = mod_poi.fit(**kwds_prelim)</span>
            <span class="s1">start_params = res_poi.params</span>
            <span class="s3">if </span><span class="s1">self.loglike_method.startswith(</span><span class="s2">'nb'</span><span class="s1">):</span>
                <span class="s1">a = self._estimate_dispersion(res_poi.predict()</span><span class="s3">, </span><span class="s1">res_poi.resid</span><span class="s3">,</span>
                                              <span class="s1">df_resid=res_poi.df_resid)</span>
                <span class="s1">start_params = np.append(start_params</span><span class="s3">, </span><span class="s1">max(</span><span class="s5">0.05</span><span class="s3">, </span><span class="s1">a))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self._transparams </span><span class="s3">is True</span><span class="s1">:</span>
                <span class="s4"># transform user provided start_params dispersion, see #3918</span>
                <span class="s1">start_params = np.array(start_params</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">True</span><span class="s1">)</span>
                <span class="s1">start_params[-</span><span class="s5">1</span><span class="s1">] = np.log(start_params[-</span><span class="s5">1</span><span class="s1">])</span>

        <span class="s3">if </span><span class="s1">callback </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s4"># work around perfect separation callback #3895</span>
            <span class="s1">callback = </span><span class="s3">lambda </span><span class="s1">*x: x</span>

        <span class="s1">mlefit = super().fit(start_params=start_params</span><span class="s3">,</span>
                             <span class="s1">maxiter=maxiter</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">disp=disp</span><span class="s3">,</span>
                             <span class="s1">full_output=full_output</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">,</span>
                             <span class="s1">**kwargs)</span>
        <span class="s3">if </span><span class="s1">optim_kwds_prelim </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">mlefit.mle_settings[</span><span class="s2">&quot;optim_kwds_prelim&quot;</span><span class="s1">] = optim_kwds_prelim</span>
        <span class="s4"># TODO: Fix NBin _check_perfect_pred</span>
        <span class="s3">if </span><span class="s1">self.loglike_method.startswith(</span><span class="s2">'nb'</span><span class="s1">):</span>
            <span class="s4"># mlefit is a wrapped counts results</span>
            <span class="s1">self._transparams = </span><span class="s3">False </span><span class="s4"># do not need to transform anymore now</span>
            <span class="s4"># change from lnalpha to alpha</span>
            <span class="s3">if </span><span class="s1">method </span><span class="s3">not in </span><span class="s1">[</span><span class="s2">&quot;newton&quot;</span><span class="s3">, </span><span class="s2">&quot;ncg&quot;</span><span class="s1">]:</span>
                <span class="s1">mlefit._results.params[-</span><span class="s5">1</span><span class="s1">] = np.exp(mlefit._results.params[-</span><span class="s5">1</span><span class="s1">])</span>

            <span class="s1">nbinfit = NegativeBinomialResults(self</span><span class="s3">, </span><span class="s1">mlefit._results)</span>
            <span class="s1">result = NegativeBinomialResultsWrapper(nbinfit)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">result = mlefit</span>

        <span class="s3">if </span><span class="s1">cov_kwds </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">cov_kwds = {}  </span><span class="s4">#TODO: make this unnecessary ?</span>
        <span class="s1">result._get_robustcov_results(cov_type=cov_type</span><span class="s3">, </span><span class="s1">use_self=</span><span class="s3">True, </span><span class="s1">use_t=use_t</span><span class="s3">, </span><span class="s1">**cov_kwds)</span>
        <span class="s3">return </span><span class="s1">result</span>


    <span class="s3">def </span><span class="s1">fit_regularized(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'l1'</span><span class="s3">,</span>
            <span class="s1">maxiter=</span><span class="s2">'defined_by_method'</span><span class="s3">, </span><span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None,</span>
            <span class="s1">alpha=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">trim_mode=</span><span class="s2">'auto'</span><span class="s3">, </span><span class="s1">auto_trim_tol=</span><span class="s5">0.01</span><span class="s3">, </span><span class="s1">size_trim_tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
            <span class="s1">qc_tol=</span><span class="s5">0.03</span><span class="s3">, </span><span class="s1">**kwargs):</span>

        <span class="s1">_validate_l1_method(method)</span>

        <span class="s3">if </span><span class="s1">self.loglike_method.startswith(</span><span class="s2">'nb'</span><span class="s1">) </span><span class="s3">and </span><span class="s1">(np.size(alpha) == </span><span class="s5">1 </span><span class="s3">and</span>
                                                     <span class="s1">alpha != </span><span class="s5">0</span><span class="s1">):</span>
            <span class="s4"># do not penalize alpha if alpha is scalar</span>
            <span class="s1">k_params = self.exog.shape[</span><span class="s5">1</span><span class="s1">] + self.k_extra</span>
            <span class="s1">alpha = alpha * np.ones(k_params)</span>
            <span class="s1">alpha[-</span><span class="s5">1</span><span class="s1">] = </span><span class="s5">0</span>

        <span class="s4"># alpha for regularized poisson to get starting values</span>
        <span class="s1">alpha_p = alpha[:-</span><span class="s5">1</span><span class="s1">] </span><span class="s3">if </span><span class="s1">(self.k_extra </span><span class="s3">and </span><span class="s1">np.size(alpha) &gt; </span><span class="s5">1</span><span class="s1">) </span><span class="s3">else </span><span class="s1">alpha</span>

        <span class="s1">self._transparams = </span><span class="s3">False</span>
        <span class="s3">if </span><span class="s1">start_params </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s4"># Use poisson fit as first guess.</span>
            <span class="s4">#TODO, Warning: this assumes exposure is logged</span>
            <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">) + getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">np.size(offset) == </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">offset == </span><span class="s5">0</span><span class="s1">:</span>
                <span class="s1">offset = </span><span class="s3">None</span>
            <span class="s1">mod_poi = Poisson(self.endog</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">, </span><span class="s1">offset=offset)</span>
            <span class="s3">with </span><span class="s1">warnings.catch_warnings():</span>
                <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;always&quot;</span><span class="s1">)</span>
                <span class="s1">start_params = mod_poi.fit_regularized(</span>
                    <span class="s1">start_params=start_params</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                    <span class="s1">full_output=full_output</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">,</span>
                    <span class="s1">alpha=alpha_p</span><span class="s3">, </span><span class="s1">trim_mode=trim_mode</span><span class="s3">,</span>
                    <span class="s1">auto_trim_tol=auto_trim_tol</span><span class="s3">, </span><span class="s1">size_trim_tol=size_trim_tol</span><span class="s3">,</span>
                    <span class="s1">qc_tol=qc_tol</span><span class="s3">, </span><span class="s1">**kwargs).params</span>
            <span class="s3">if </span><span class="s1">self.loglike_method.startswith(</span><span class="s2">'nb'</span><span class="s1">):</span>
                <span class="s1">start_params = np.append(start_params</span><span class="s3">, </span><span class="s5">0.1</span><span class="s1">)</span>

        <span class="s1">cntfit = super(CountModel</span><span class="s3">, </span><span class="s1">self).fit_regularized(</span>
                <span class="s1">start_params=start_params</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                <span class="s1">full_output=full_output</span><span class="s3">, </span><span class="s1">disp=disp</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">,</span>
                <span class="s1">alpha=alpha</span><span class="s3">, </span><span class="s1">trim_mode=trim_mode</span><span class="s3">, </span><span class="s1">auto_trim_tol=auto_trim_tol</span><span class="s3">,</span>
                <span class="s1">size_trim_tol=size_trim_tol</span><span class="s3">, </span><span class="s1">qc_tol=qc_tol</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s1">discretefit = L1NegativeBinomialResults(self</span><span class="s3">, </span><span class="s1">cntfit)</span>
        <span class="s3">return </span><span class="s1">L1NegativeBinomialResultsWrapper(discretefit)</span>

    <span class="s1">@Appender(Poisson.get_distribution.__doc__)</span>
    <span class="s3">def </span><span class="s1">get_distribution(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exposure=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;get frozen instance of distribution 
        &quot;&quot;&quot;</span>
        <span class="s1">mu = self.predict(params</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">exposure=exposure</span><span class="s3">, </span><span class="s1">offset=offset)</span>
        <span class="s3">if </span><span class="s1">self.loglike_method == </span><span class="s2">'geometric'</span><span class="s1">:</span>
            <span class="s4"># distr = stats.geom(1 / (1 + mu[:, None]), loc=-1)</span>
            <span class="s1">distr = stats.geom(</span><span class="s5">1 </span><span class="s1">/ (</span><span class="s5">1 </span><span class="s1">+ mu)</span><span class="s3">, </span><span class="s1">loc=-</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self.loglike_method == </span><span class="s2">'nb2'</span><span class="s1">:</span>
                <span class="s1">p = </span><span class="s5">2</span>
            <span class="s3">elif </span><span class="s1">self.loglike_method == </span><span class="s2">'nb1'</span><span class="s1">:</span>
                <span class="s1">p = </span><span class="s5">1</span>

            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s1">q = </span><span class="s5">2 </span><span class="s1">- p</span>
            <span class="s1">size = </span><span class="s5">1. </span><span class="s1">/ alpha * mu**q</span>
            <span class="s1">prob = size / (size + mu)</span>
            <span class="s4"># distr = nbinom(size[:, None], prob[:, None])</span>
            <span class="s1">distr = nbinom(size</span><span class="s3">, </span><span class="s1">prob)</span>

        <span class="s3">return </span><span class="s1">distr</span>


<span class="s3">class </span><span class="s1">NegativeBinomialP(CountModel):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Generalized Negative Binomial (NB-P) Model 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : ndarray 
        A reference to the endogenous response variable 
    exog : ndarray 
        A reference to the exogenous design. 
    p : scalar 
        P denotes parameterizations for NB-P regression. p=1 for NB-1 and 
        p=2 for NB-2. Default is p=1. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s3">,</span>
           <span class="s2">'extra_params'</span><span class="s1">:</span>
               <span class="s2">&quot;&quot;&quot;p : scalar 
        P denotes parameterizations for NB regression. p=1 for NB-1 and 
        p=2 for NB-2. Default is p=2. 
    offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    exposure : array_like 
        Log(exposure) is added to the linear prediction with coefficient 
        equal to 1. 
        &quot;&quot;&quot; </span><span class="s1">+ base._missing_param_doc + _check_rank_doc}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">p=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">offset=</span><span class="s3">None,</span>
                 <span class="s1">exposure=</span><span class="s3">None, </span><span class="s1">missing=</span><span class="s2">'none'</span><span class="s3">, </span><span class="s1">check_rank=</span><span class="s3">True,</span>
                 <span class="s1">**kwargs):</span>
        <span class="s1">super().__init__(endog</span><span class="s3">,</span>
                         <span class="s1">exog</span><span class="s3">,</span>
                         <span class="s1">offset=offset</span><span class="s3">,</span>
                         <span class="s1">exposure=exposure</span><span class="s3">,</span>
                         <span class="s1">missing=missing</span><span class="s3">,</span>
                         <span class="s1">check_rank=check_rank</span><span class="s3">,</span>
                         <span class="s1">**kwargs)</span>
        <span class="s1">self.parameterization = p</span>
        <span class="s1">self.exog_names.append(</span><span class="s2">'alpha'</span><span class="s1">)</span>
        <span class="s1">self.k_extra = </span><span class="s5">1</span>
        <span class="s1">self._transparams = </span><span class="s3">False</span>

    <span class="s3">def </span><span class="s1">_get_init_kwds(self):</span>
        <span class="s1">kwds = super()._get_init_kwds()</span>
        <span class="s1">kwds[</span><span class="s2">'p'</span><span class="s1">] = self.parameterization</span>
        <span class="s3">return </span><span class="s1">kwds</span>

    <span class="s3">def </span><span class="s1">_get_exogs(self):</span>
        <span class="s3">return </span><span class="s1">(self.exog</span><span class="s3">, None</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Loglikelihood of Generalized Negative Binomial (NB-P) model 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike : float 
            The log-likelihood function of the model evaluated at `params`. 
            See notes. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.sum(self.loglikeobs(params))</span>

    <span class="s3">def </span><span class="s1">loglikeobs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Loglikelihood for observations of Generalized Negative Binomial (NB-P) model 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike : ndarray 
            The log likelihood for each observation of the model evaluated 
            at `params`. See Notes 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._transparams:</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">p = self.parameterization</span>
        <span class="s1">y = self.endog</span>

        <span class="s1">mu = self.predict(params)</span>
        <span class="s1">mu_p = mu**(</span><span class="s5">2 </span><span class="s1">- p)</span>
        <span class="s1">a1 = mu_p / alpha</span>
        <span class="s1">a2 = mu + a1</span>

        <span class="s1">llf = (gammaln(y + a1) - gammaln(y + </span><span class="s5">1</span><span class="s1">) - gammaln(a1) +</span>
               <span class="s1">a1 * np.log(a1) + y * np.log(mu) -</span>
               <span class="s1">(y + a1) * np.log(a2))</span>

        <span class="s3">return </span><span class="s1">llf</span>

    <span class="s3">def </span><span class="s1">score_obs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Generalized Negative Binomial (NB-P) model score (gradient) vector of the log-likelihood for each observations. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        score : ndarray, 1-D 
            The score vector of the model, i.e. the first derivative of the 
            loglikelihood function, evaluated at `params` 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._transparams:</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">p = </span><span class="s5">2 </span><span class="s1">- self.parameterization</span>
        <span class="s1">y = self.endog</span>

        <span class="s1">mu = self.predict(params)</span>
        <span class="s1">mu_p = mu**p</span>
        <span class="s1">a1 = mu_p / alpha</span>
        <span class="s1">a2 = mu + a1</span>
        <span class="s1">a3 = y + a1</span>
        <span class="s1">a4 = p * a1 / mu</span>

        <span class="s1">dgpart = digamma(a3) - digamma(a1)</span>
        <span class="s1">dgterm = dgpart + np.log(a1 / a2) + </span><span class="s5">1 </span><span class="s1">- a3 / a2</span>
        <span class="s4"># TODO: better name/interpretation for dgterm?</span>

        <span class="s1">dparams = (a4 * dgterm -</span>
                   <span class="s1">a3 / a2 +</span>
                   <span class="s1">y / mu)</span>
        <span class="s1">dparams = (self.exog.T * mu * dparams).T</span>
        <span class="s1">dalpha = -a1 / alpha * dgterm</span>

        <span class="s3">return </span><span class="s1">np.concatenate((dparams</span><span class="s3">, </span><span class="s1">np.atleast_2d(dalpha).T)</span><span class="s3">,</span>
                              <span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Generalized Negative Binomial (NB-P) model score (gradient) vector of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        score : ndarray, 1-D 
            The score vector of the model, i.e. the first derivative of the 
            loglikelihood function, evaluated at `params` 
        &quot;&quot;&quot;</span>
        <span class="s1">score = np.sum(self.score_obs(params)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">self._transparams:</span>
            <span class="s1">score[-</span><span class="s5">1</span><span class="s1">] == score[-</span><span class="s5">1</span><span class="s1">] ** </span><span class="s5">2</span>
            <span class="s3">return </span><span class="s1">score</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">score</span>

    <span class="s3">def </span><span class="s1">score_factor(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">endog=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Generalized Negative Binomial (NB-P) model score (gradient) vector of the log-likelihood for each observations. 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        score : ndarray, 1-D 
            The score vector of the model, i.e. the first derivative of the 
            loglikelihood function, evaluated at `params` 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._transparams:</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">p = </span><span class="s5">2 </span><span class="s1">- self.parameterization</span>
        <span class="s1">y = self.endog </span><span class="s3">if </span><span class="s1">endog </span><span class="s3">is None else </span><span class="s1">endog</span>

        <span class="s1">mu = self.predict(params)</span>
        <span class="s1">mu_p = mu**p</span>
        <span class="s1">a1 = mu_p / alpha</span>
        <span class="s1">a2 = mu + a1</span>
        <span class="s1">a3 = y + a1</span>
        <span class="s1">a4 = p * a1 / mu</span>

        <span class="s1">dgpart = digamma(a3) - digamma(a1)</span>

        <span class="s1">dparams = ((a4 * dgpart -</span>
                   <span class="s1">a3 / a2) +</span>
                   <span class="s1">y / mu + a4 * (</span><span class="s5">1 </span><span class="s1">- a3 / a2 + np.log(a1 / a2)))</span>
        <span class="s1">dparams = (mu * dparams).T</span>
        <span class="s1">dalpha = (-a1 / alpha * (dgpart +</span>
                                 <span class="s1">np.log(a1 / a2) +</span>
                                 <span class="s5">1 </span><span class="s1">- a3 / a2))</span>

        <span class="s3">return </span><span class="s1">dparams</span><span class="s3">, </span><span class="s1">dalpha</span>

    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Generalized Negative Binomial (NB-P) model hessian maxtrix of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model 
 
        Returns 
        ------- 
        hessian : ndarray, 2-D 
            The hessian matrix of the model. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._transparams:</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">p = </span><span class="s5">2 </span><span class="s1">- self.parameterization</span>
        <span class="s1">y = self.endog</span>
        <span class="s1">exog = self.exog</span>
        <span class="s1">mu = self.predict(params)</span>

        <span class="s1">mu_p = mu**p</span>
        <span class="s1">a1 = mu_p / alpha</span>
        <span class="s1">a2 = mu + a1</span>
        <span class="s1">a3 = y + a1</span>
        <span class="s1">a4 = p * a1 / mu</span>

        <span class="s1">prob = a1 / a2</span>
        <span class="s1">lprob = np.log(prob)</span>
        <span class="s1">dgpart = digamma(a3) - digamma(a1)</span>
        <span class="s1">pgpart = polygamma(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">a3) - polygamma(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">a1)</span>

        <span class="s1">dim = exog.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">hess_arr = np.zeros((dim + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">dim + </span><span class="s5">1</span><span class="s1">))</span>

        <span class="s1">coeff = mu**</span><span class="s5">2 </span><span class="s1">* (((</span><span class="s5">1 </span><span class="s1">+ a4)**</span><span class="s5">2 </span><span class="s1">* a3 / a2**</span><span class="s5">2 </span><span class="s1">-</span>
                          <span class="s1">a3 / a2 * (p - </span><span class="s5">1</span><span class="s1">) * a4 / mu -</span>
                          <span class="s1">y / mu**</span><span class="s5">2 </span><span class="s1">-</span>
                          <span class="s5">2 </span><span class="s1">* a4 * (</span><span class="s5">1 </span><span class="s1">+ a4) / a2 +</span>
                          <span class="s1">p * a4 / mu * (lprob + dgpart + </span><span class="s5">2</span><span class="s1">) -</span>
                          <span class="s1">a4 / mu * (lprob + dgpart + </span><span class="s5">1</span><span class="s1">) +</span>
                          <span class="s1">a4**</span><span class="s5">2 </span><span class="s1">* pgpart) +</span>
                         <span class="s1">(-(</span><span class="s5">1 </span><span class="s1">+ a4) * a3 / a2 +</span>
                          <span class="s1">y / mu +</span>
                          <span class="s1">a4 * (lprob + dgpart + </span><span class="s5">1</span><span class="s1">)) / mu)</span>

        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(dim):</span>
            <span class="s1">hess_arr[i</span><span class="s3">, </span><span class="s1">:-</span><span class="s5">1</span><span class="s1">] = np.sum(self.exog[:</span><span class="s3">, </span><span class="s1">:].T * self.exog[:</span><span class="s3">, </span><span class="s1">i] * coeff</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>


        <span class="s1">hess_arr[-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">:-</span><span class="s5">1</span><span class="s1">] = (self.exog[:</span><span class="s3">, </span><span class="s1">:].T * mu * a1 *</span>
                <span class="s1">((</span><span class="s5">1 </span><span class="s1">+ a4) * (</span><span class="s5">1 </span><span class="s1">- a3 / a2) / a2 -</span>
                 <span class="s1">p * (lprob + dgpart + </span><span class="s5">2</span><span class="s1">) / mu +</span>
                 <span class="s1">p / mu * (a3 + p * a1) / a2 -</span>
                 <span class="s1">a4 * pgpart) / alpha).sum(axis=</span><span class="s5">1</span><span class="s1">)</span>


        <span class="s1">da2 = (a1 * (</span><span class="s5">2 </span><span class="s1">* lprob +</span>
                     <span class="s5">2 </span><span class="s1">* dgpart + </span><span class="s5">3 </span><span class="s1">-</span>
                     <span class="s5">2 </span><span class="s1">* a3 / a2</span>
                     <span class="s1">+ a1 * pgpart</span>
                     <span class="s1">- </span><span class="s5">2 </span><span class="s1">* prob +</span>
                     <span class="s1">prob * a3 / a2) / alpha**</span><span class="s5">2</span><span class="s1">)</span>

        <span class="s1">hess_arr[-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">] = da2.sum()</span>

        <span class="s1">tri_idx = np.triu_indices(dim + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">k=</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">hess_arr[tri_idx] = hess_arr.T[tri_idx]</span>

        <span class="s3">return </span><span class="s1">hess_arr</span>

    <span class="s3">def </span><span class="s1">hessian_factor(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Generalized Negative Binomial (NB-P) model hessian maxtrix of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        hessian : ndarray, 2-D 
            The hessian matrix of the model. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._transparams:</span>
            <span class="s1">alpha = np.exp(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">p = </span><span class="s5">2 </span><span class="s1">- self.parameterization</span>
        <span class="s1">y = self.endog</span>
        <span class="s1">mu = self.predict(params)</span>

        <span class="s1">mu_p = mu**p</span>
        <span class="s1">a1 = mu_p / alpha</span>
        <span class="s1">a2 = mu + a1</span>
        <span class="s1">a3 = y + a1</span>
        <span class="s1">a4 = p * a1 / mu</span>
        <span class="s1">a5 = a4 * p / mu</span>

        <span class="s1">dgpart = digamma(a3) - digamma(a1)</span>

        <span class="s1">coeff = mu**</span><span class="s5">2 </span><span class="s1">* (((</span><span class="s5">1 </span><span class="s1">+ a4)**</span><span class="s5">2 </span><span class="s1">* a3 / a2**</span><span class="s5">2 </span><span class="s1">-</span>
                          <span class="s1">a3 * (a5 - a4 / mu) / a2 -</span>
                          <span class="s1">y / mu**</span><span class="s5">2 </span><span class="s1">-</span>
                          <span class="s5">2 </span><span class="s1">* a4 * (</span><span class="s5">1 </span><span class="s1">+ a4) / a2 +</span>
                          <span class="s1">a5 * (np.log(a1) - np.log(a2) + dgpart + </span><span class="s5">2</span><span class="s1">) -</span>
                          <span class="s1">a4 * (np.log(a1) - np.log(a2) + dgpart + </span><span class="s5">1</span><span class="s1">) / mu -</span>
                          <span class="s1">a4**</span><span class="s5">2 </span><span class="s1">* (polygamma(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">a1) - polygamma(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">a3))) +</span>
                         <span class="s1">(-(</span><span class="s5">1 </span><span class="s1">+ a4) * a3 / a2 +</span>
                          <span class="s1">y / mu +</span>
                          <span class="s1">a4 * (np.log(a1) - np.log(a2) + dgpart + </span><span class="s5">1</span><span class="s1">)) / mu)</span>

        <span class="s1">hfbb = coeff</span>

        <span class="s1">hfba = (mu * a1 *</span>
                <span class="s1">((</span><span class="s5">1 </span><span class="s1">+ a4) * (</span><span class="s5">1 </span><span class="s1">- a3 / a2) / a2 -</span>
                 <span class="s1">p * (np.log(a1 / a2) + dgpart + </span><span class="s5">2</span><span class="s1">) / mu +</span>
                 <span class="s1">p * (a3 / mu + a4) / a2 +</span>
                 <span class="s1">a4 * (polygamma(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">a1) - polygamma(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">a3))) / alpha)</span>

        <span class="s1">hfaa = (a1 * (</span><span class="s5">2 </span><span class="s1">* np.log(a1 / a2) +</span>
                     <span class="s5">2 </span><span class="s1">* dgpart + </span><span class="s5">3 </span><span class="s1">-</span>
                     <span class="s5">2 </span><span class="s1">* a3 / a2 - a1 * polygamma(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">a1) +</span>
                     <span class="s1">a1 * polygamma(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">a3) - </span><span class="s5">2 </span><span class="s1">* a1 / a2 +</span>
                     <span class="s1">a1 * a3 / a2**</span><span class="s5">2</span><span class="s1">) / alpha**</span><span class="s5">2</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">hfbb</span><span class="s3">, </span><span class="s1">hfba</span><span class="s3">, </span><span class="s1">hfaa</span>

    <span class="s1">@Appender(_get_start_params_null_docs)</span>
    <span class="s3">def </span><span class="s1">_get_start_params_null(self):</span>
        <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">exposure = getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>

        <span class="s1">const = (self.endog / np.exp(offset + exposure)).mean()</span>
        <span class="s1">params = [np.log(const)]</span>
        <span class="s1">mu = const * np.exp(offset + exposure)</span>
        <span class="s1">resid = self.endog - mu</span>
        <span class="s1">a = self._estimate_dispersion(mu</span><span class="s3">, </span><span class="s1">resid</span><span class="s3">, </span><span class="s1">df_resid=resid.shape[</span><span class="s5">0</span><span class="s1">] - </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">params.append(a)</span>

        <span class="s3">return </span><span class="s1">np.array(params)</span>

    <span class="s3">def </span><span class="s1">_estimate_dispersion(self</span><span class="s3">, </span><span class="s1">mu</span><span class="s3">, </span><span class="s1">resid</span><span class="s3">, </span><span class="s1">df_resid=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">q = self.parameterization - </span><span class="s5">1</span>
        <span class="s3">if </span><span class="s1">df_resid </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">df_resid = resid.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">a = ((resid**</span><span class="s5">2 </span><span class="s1">/ mu - </span><span class="s5">1</span><span class="s1">) * mu**(-q)).sum() / df_resid</span>
        <span class="s3">return </span><span class="s1">a</span>

    <span class="s1">@Appender(DiscreteModel.fit.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'bfgs'</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s5">35</span><span class="s3">,</span>
            <span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None, </span><span class="s1">use_transparams=</span><span class="s3">False,</span>
            <span class="s1">cov_type=</span><span class="s2">'nonrobust'</span><span class="s3">, </span><span class="s1">cov_kwds=</span><span class="s3">None, </span><span class="s1">use_t=</span><span class="s3">None,</span>
            <span class="s1">optim_kwds_prelim=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s4"># TODO: Fix doc string</span>
        <span class="s0">&quot;&quot;&quot; 
        use_transparams : bool 
            This parameter enable internal transformation to impose 
            non-negativity. True to enable. Default is False. 
            use_transparams=True imposes the no underdispersion (alpha &gt; 0) 
            constraint. In case use_transparams=True and method=&quot;newton&quot; or 
            &quot;ncg&quot; transformation is ignored. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">use_transparams </span><span class="s3">and </span><span class="s1">method </span><span class="s3">not in </span><span class="s1">[</span><span class="s2">'newton'</span><span class="s3">, </span><span class="s2">'ncg'</span><span class="s1">]:</span>
            <span class="s1">self._transparams = </span><span class="s3">True</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">use_transparams:</span>
                <span class="s1">warnings.warn(</span><span class="s2">'Parameter &quot;use_transparams&quot; is ignored'</span><span class="s3">,</span>
                              <span class="s1">RuntimeWarning)</span>
            <span class="s1">self._transparams = </span><span class="s3">False</span>
        <span class="s3">if </span><span class="s1">start_params </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">) + getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">np.size(offset) == </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">offset == </span><span class="s5">0</span><span class="s1">:</span>
                <span class="s1">offset = </span><span class="s3">None</span>
            <span class="s1">kwds_prelim = {</span><span class="s2">'disp'</span><span class="s1">: </span><span class="s5">0</span><span class="s3">, </span><span class="s2">'skip_hessian'</span><span class="s1">: </span><span class="s3">True, </span><span class="s2">'warn_convergence'</span><span class="s1">: </span><span class="s3">False</span><span class="s1">}</span>
            <span class="s3">if </span><span class="s1">optim_kwds_prelim </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">kwds_prelim.update(optim_kwds_prelim)</span>
            <span class="s1">mod_poi = Poisson(self.endog</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">, </span><span class="s1">offset=offset)</span>
            <span class="s3">with </span><span class="s1">warnings.catch_warnings():</span>
                <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;always&quot;</span><span class="s1">)</span>
                <span class="s1">res_poi = mod_poi.fit(**kwds_prelim)</span>
            <span class="s1">start_params = res_poi.params</span>
            <span class="s1">a = self._estimate_dispersion(res_poi.predict()</span><span class="s3">, </span><span class="s1">res_poi.resid</span><span class="s3">,</span>
                                          <span class="s1">df_resid=res_poi.df_resid)</span>
            <span class="s1">start_params = np.append(start_params</span><span class="s3">, </span><span class="s1">max(</span><span class="s5">0.05</span><span class="s3">, </span><span class="s1">a))</span>

        <span class="s3">if </span><span class="s1">callback </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s4"># work around perfect separation callback #3895</span>
            <span class="s1">callback = </span><span class="s3">lambda </span><span class="s1">*x: x</span>

        <span class="s1">mlefit = super(NegativeBinomialP</span><span class="s3">, </span><span class="s1">self).fit(start_params=start_params</span><span class="s3">,</span>
                        <span class="s1">maxiter=maxiter</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">disp=disp</span><span class="s3">,</span>
                        <span class="s1">full_output=full_output</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">,</span>
                        <span class="s1">**kwargs)</span>
        <span class="s3">if </span><span class="s1">optim_kwds_prelim </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">mlefit.mle_settings[</span><span class="s2">&quot;optim_kwds_prelim&quot;</span><span class="s1">] = optim_kwds_prelim</span>
        <span class="s3">if </span><span class="s1">use_transparams </span><span class="s3">and </span><span class="s1">method </span><span class="s3">not in </span><span class="s1">[</span><span class="s2">&quot;newton&quot;</span><span class="s3">, </span><span class="s2">&quot;ncg&quot;</span><span class="s1">]:</span>
            <span class="s1">self._transparams = </span><span class="s3">False</span>
            <span class="s1">mlefit._results.params[-</span><span class="s5">1</span><span class="s1">] = np.exp(mlefit._results.params[-</span><span class="s5">1</span><span class="s1">])</span>

        <span class="s1">nbinfit = NegativeBinomialPResults(self</span><span class="s3">, </span><span class="s1">mlefit._results)</span>
        <span class="s1">result = NegativeBinomialPResultsWrapper(nbinfit)</span>

        <span class="s3">if </span><span class="s1">cov_kwds </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">cov_kwds = {}</span>
        <span class="s1">result._get_robustcov_results(cov_type=cov_type</span><span class="s3">,</span>
                                    <span class="s1">use_self=</span><span class="s3">True, </span><span class="s1">use_t=use_t</span><span class="s3">, </span><span class="s1">**cov_kwds)</span>
        <span class="s3">return </span><span class="s1">result</span>

    <span class="s1">@Appender(DiscreteModel.fit_regularized.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit_regularized(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s2">'l1'</span><span class="s3">,</span>
            <span class="s1">maxiter=</span><span class="s2">'defined_by_method'</span><span class="s3">, </span><span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None,</span>
            <span class="s1">alpha=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">trim_mode=</span><span class="s2">'auto'</span><span class="s3">, </span><span class="s1">auto_trim_tol=</span><span class="s5">0.01</span><span class="s3">, </span><span class="s1">size_trim_tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
            <span class="s1">qc_tol=</span><span class="s5">0.03</span><span class="s3">, </span><span class="s1">**kwargs):</span>

        <span class="s1">_validate_l1_method(method)</span>

        <span class="s3">if </span><span class="s1">np.size(alpha) == </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">alpha != </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">k_params = self.exog.shape[</span><span class="s5">1</span><span class="s1">] + self.k_extra</span>
            <span class="s1">alpha = alpha * np.ones(k_params)</span>
            <span class="s1">alpha[-</span><span class="s5">1</span><span class="s1">] = </span><span class="s5">0</span>

        <span class="s1">alpha_p = alpha[:-</span><span class="s5">1</span><span class="s1">] </span><span class="s3">if </span><span class="s1">(self.k_extra </span><span class="s3">and </span><span class="s1">np.size(alpha) &gt; </span><span class="s5">1</span><span class="s1">) </span><span class="s3">else </span><span class="s1">alpha</span>

        <span class="s1">self._transparams = </span><span class="s3">False</span>
        <span class="s3">if </span><span class="s1">start_params </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">) + getattr(self</span><span class="s3">, </span><span class="s2">&quot;exposure&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">np.size(offset) == </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">offset == </span><span class="s5">0</span><span class="s1">:</span>
                <span class="s1">offset = </span><span class="s3">None</span>
            <span class="s1">mod_poi = Poisson(self.endog</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">, </span><span class="s1">offset=offset)</span>
            <span class="s3">with </span><span class="s1">warnings.catch_warnings():</span>
                <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;always&quot;</span><span class="s1">)</span>
                <span class="s1">start_params = mod_poi.fit_regularized(</span>
                    <span class="s1">start_params=start_params</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                    <span class="s1">full_output=full_output</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">,</span>
                    <span class="s1">alpha=alpha_p</span><span class="s3">, </span><span class="s1">trim_mode=trim_mode</span><span class="s3">,</span>
                    <span class="s1">auto_trim_tol=auto_trim_tol</span><span class="s3">, </span><span class="s1">size_trim_tol=size_trim_tol</span><span class="s3">,</span>
                    <span class="s1">qc_tol=qc_tol</span><span class="s3">, </span><span class="s1">**kwargs).params</span>
            <span class="s1">start_params = np.append(start_params</span><span class="s3">, </span><span class="s5">0.1</span><span class="s1">)</span>

        <span class="s1">cntfit = super(CountModel</span><span class="s3">, </span><span class="s1">self).fit_regularized(</span>
                <span class="s1">start_params=start_params</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                <span class="s1">full_output=full_output</span><span class="s3">, </span><span class="s1">disp=disp</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">,</span>
                <span class="s1">alpha=alpha</span><span class="s3">, </span><span class="s1">trim_mode=trim_mode</span><span class="s3">, </span><span class="s1">auto_trim_tol=auto_trim_tol</span><span class="s3">,</span>
                <span class="s1">size_trim_tol=size_trim_tol</span><span class="s3">, </span><span class="s1">qc_tol=qc_tol</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s1">discretefit = L1NegativeBinomialResults(self</span><span class="s3">, </span><span class="s1">cntfit)</span>

        <span class="s3">return </span><span class="s1">L1NegativeBinomialResultsWrapper(discretefit)</span>

    <span class="s1">@Appender(Poisson.predict.__doc__)</span>
    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exposure=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None,</span>
                <span class="s1">which=</span><span class="s2">'mean'</span><span class="s3">, </span><span class="s1">y_values=</span><span class="s3">None</span><span class="s1">):</span>

        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>

        <span class="s3">if </span><span class="s1">exposure </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exposure = getattr(self</span><span class="s3">, </span><span class="s2">'exposure'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">exposure != </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">exposure = np.log(exposure)</span>

        <span class="s3">if </span><span class="s1">offset </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">offset = getattr(self</span><span class="s3">, </span><span class="s2">'offset'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>

        <span class="s1">fitted = np.dot(exog</span><span class="s3">, </span><span class="s1">params[:exog.shape[</span><span class="s5">1</span><span class="s1">]])</span>
        <span class="s1">linpred = fitted + exposure + offset</span>

        <span class="s3">if </span><span class="s1">which == </span><span class="s2">'mean'</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">np.exp(linpred)</span>
        <span class="s3">elif </span><span class="s1">which == </span><span class="s2">'linear'</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">linpred</span>
        <span class="s3">elif </span><span class="s1">which == </span><span class="s2">'var'</span><span class="s1">:</span>
            <span class="s1">mean = np.exp(linpred)</span>
            <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s1">p = self.parameterization  </span><span class="s4"># no `-1` as in GPP</span>
            <span class="s1">var_ = mean * (</span><span class="s5">1 </span><span class="s1">+ alpha * mean**(p - </span><span class="s5">1</span><span class="s1">))</span>
            <span class="s3">return </span><span class="s1">var_</span>
        <span class="s3">elif </span><span class="s1">which == </span><span class="s2">'prob'</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">y_values </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">y_values = np.atleast_2d(np.arange(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">np.max(self.endog)+</span><span class="s5">1</span><span class="s1">))</span>

            <span class="s1">mu = self.predict(params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">exposure</span><span class="s3">, </span><span class="s1">offset)</span>
            <span class="s1">size</span><span class="s3">, </span><span class="s1">prob = self.convert_params(params</span><span class="s3">, </span><span class="s1">mu)</span>
            <span class="s3">return </span><span class="s1">nbinom.pmf(y_values</span><span class="s3">, </span><span class="s1">size[:</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">prob[:</span><span class="s3">, None</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s2">'keyword &quot;which&quot; = %s not recognized' </span><span class="s1">% which)</span>

    <span class="s3">def </span><span class="s1">convert_params(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">mu):</span>
        <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">p = </span><span class="s5">2 </span><span class="s1">- self.parameterization</span>

        <span class="s1">size = </span><span class="s5">1. </span><span class="s1">/ alpha * mu**p</span>
        <span class="s1">prob = size / (size + mu)</span>

        <span class="s3">return </span><span class="s1">(size</span><span class="s3">, </span><span class="s1">prob)</span>

    <span class="s3">def </span><span class="s1">_deriv_score_obs_dendog(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot;derivative of score_obs w.r.t. endog 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which score is evaluated 
 
        Returns 
        ------- 
        derivative : ndarray_2d 
            The derivative of the score_obs with respect to endog. 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.tools.numdiff </span><span class="s3">import </span><span class="s1">_approx_fprime_cs_scalar</span>

        <span class="s3">def </span><span class="s1">f(y):</span>
            <span class="s3">if </span><span class="s1">y.ndim == </span><span class="s5">2 </span><span class="s3">and </span><span class="s1">y.shape[</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">y = y[:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">sf = self.score_factor(params</span><span class="s3">, </span><span class="s1">endog=y)</span>
            <span class="s3">return </span><span class="s1">np.column_stack(sf)</span>

        <span class="s1">dsf = _approx_fprime_cs_scalar(self.endog[:</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">f)</span>
        <span class="s4"># deriv is 2d vector</span>
        <span class="s1">d1 = dsf[:</span><span class="s3">, </span><span class="s1">:</span><span class="s5">1</span><span class="s1">] * self.exog</span>
        <span class="s1">d2 = dsf[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">:</span><span class="s5">2</span><span class="s1">]</span>

        <span class="s3">return </span><span class="s1">np.column_stack((d1</span><span class="s3">, </span><span class="s1">d2))</span>

    <span class="s3">def </span><span class="s1">_var(self</span><span class="s3">, </span><span class="s1">mu</span><span class="s3">, </span><span class="s1">params=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;variance implied by the distribution 
 
        internal use, will be refactored or removed 
        &quot;&quot;&quot;</span>
        <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">p = self.parameterization  </span><span class="s4"># no `-1` as in GPP</span>
        <span class="s1">var_ = mu * (</span><span class="s5">1 </span><span class="s1">+ alpha * mu**(p - </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s3">return </span><span class="s1">var_</span>

    <span class="s3">def </span><span class="s1">_prob_nonzero(self</span><span class="s3">, </span><span class="s1">mu</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot;Probability that count is not zero 
 
        internal use in Censored model, will be refactored or removed 
        &quot;&quot;&quot;</span>
        <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">p = self.parameterization</span>
        <span class="s1">prob_nz = </span><span class="s5">1 </span><span class="s1">- (</span><span class="s5">1 </span><span class="s1">+ alpha * mu**(p-</span><span class="s5">1</span><span class="s1">))**(- </span><span class="s5">1 </span><span class="s1">/ alpha)</span>
        <span class="s3">return </span><span class="s1">prob_nz</span>

    <span class="s1">@Appender(Poisson.get_distribution.__doc__)</span>
    <span class="s3">def </span><span class="s1">get_distribution(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exposure=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;get frozen instance of distribution 
        &quot;&quot;&quot;</span>
        <span class="s1">mu = self.predict(params</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">exposure=exposure</span><span class="s3">, </span><span class="s1">offset=offset)</span>
        <span class="s1">size</span><span class="s3">, </span><span class="s1">prob = self.convert_params(params</span><span class="s3">, </span><span class="s1">mu)</span>
        <span class="s4"># distr = nbinom(size[:, None], prob[:, None])</span>
        <span class="s1">distr = nbinom(size</span><span class="s3">, </span><span class="s1">prob)</span>
        <span class="s3">return </span><span class="s1">distr</span>


<span class="s4">### Results Class ###</span>

<span class="s3">class </span><span class="s1">DiscreteResults(base.LikelihoodModelResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span><span class="s2">&quot;one_line_description&quot; </span><span class="s1">:</span>
        <span class="s2">&quot;A results class for the discrete dependent variable models.&quot;</span><span class="s3">,</span>
        <span class="s2">&quot;extra_attr&quot; </span><span class="s1">: </span><span class="s2">&quot;&quot;</span><span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">model</span><span class="s3">, </span><span class="s1">mlefit</span><span class="s3">, </span><span class="s1">cov_type=</span><span class="s2">'nonrobust'</span><span class="s3">, </span><span class="s1">cov_kwds=</span><span class="s3">None,</span>
                 <span class="s1">use_t=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s4">#super(DiscreteResults, self).__init__(model, params,</span>
        <span class="s4">#        np.linalg.inv(-hessian), scale=1.)</span>
        <span class="s1">self.model = model</span>
        <span class="s1">self.method = </span><span class="s2">&quot;MLE&quot;</span>
        <span class="s1">self.df_model = model.df_model</span>
        <span class="s1">self.df_resid = model.df_resid</span>
        <span class="s1">self._cache = {}</span>
        <span class="s1">self.nobs = model.exog.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">self.__dict__.update(mlefit.__dict__)</span>
        <span class="s1">self.converged = mlefit.mle_retvals[</span><span class="s2">&quot;converged&quot;</span><span class="s1">]</span>

        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s2">'cov_type'</span><span class="s1">):</span>
            <span class="s4"># do this only if super, i.e. mlefit did not already add cov_type</span>
            <span class="s4"># robust covariance</span>
            <span class="s3">if </span><span class="s1">use_t </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">self.use_t = use_t</span>
            <span class="s3">if </span><span class="s1">cov_type == </span><span class="s2">'nonrobust'</span><span class="s1">:</span>
                <span class="s1">self.cov_type = </span><span class="s2">'nonrobust'</span>
                <span class="s1">self.cov_kwds = {</span><span class="s2">'description' </span><span class="s1">: </span><span class="s2">'Standard Errors assume that the ' </span><span class="s1">+</span>
                                 <span class="s2">'covariance matrix of the errors is correctly ' </span><span class="s1">+</span>
                                 <span class="s2">'specified.'</span><span class="s1">}</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s3">if </span><span class="s1">cov_kwds </span><span class="s3">is None</span><span class="s1">:</span>
                    <span class="s1">cov_kwds = {}</span>
                <span class="s3">from </span><span class="s1">statsmodels.base.covtype </span><span class="s3">import </span><span class="s1">get_robustcov_results</span>
                <span class="s1">get_robustcov_results(self</span><span class="s3">, </span><span class="s1">cov_type=cov_type</span><span class="s3">, </span><span class="s1">use_self=</span><span class="s3">True,</span>
                                           <span class="s1">**cov_kwds)</span>


    <span class="s3">def </span><span class="s1">__getstate__(self):</span>
        <span class="s4"># remove unpicklable methods</span>
        <span class="s1">mle_settings = getattr(self</span><span class="s3">, </span><span class="s2">'mle_settings'</span><span class="s3">, None</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">mle_settings </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s2">'callback' </span><span class="s3">in </span><span class="s1">mle_settings:</span>
                <span class="s1">mle_settings[</span><span class="s2">'callback'</span><span class="s1">] = </span><span class="s3">None</span>
            <span class="s3">if </span><span class="s2">'cov_params_func' </span><span class="s3">in </span><span class="s1">mle_settings:</span>
                <span class="s1">mle_settings[</span><span class="s2">'cov_params_func'</span><span class="s1">] = </span><span class="s3">None</span>
        <span class="s3">return </span><span class="s1">self.__dict__</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">prsquared(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        McFadden's pseudo-R-squared. `1 - (llf / llnull)` 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s5">1 </span><span class="s1">- self.llf/self.llnull</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">llr(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Likelihood ratio chi-squared statistic; `-2*(llnull - llf)` 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">-</span><span class="s5">2</span><span class="s1">*(self.llnull - self.llf)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">llr_pvalue(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        The chi-squared probability of getting a log-likelihood ratio 
        statistic greater than llr.  llr has a chi-squared distribution 
        with degrees of freedom `df_model`. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">stats.distributions.chi2.sf(self.llr</span><span class="s3">, </span><span class="s1">self.df_model)</span>

    <span class="s3">def </span><span class="s1">set_null_options(self</span><span class="s3">, </span><span class="s1">llnull=</span><span class="s3">None, </span><span class="s1">attach_results=</span><span class="s3">True, </span><span class="s1">**kwargs):</span>
        <span class="s0">&quot;&quot;&quot; 
        Set the fit options for the Null (constant-only) model. 
 
        This resets the cache for related attributes which is potentially 
        fragile. This only sets the option, the null model is estimated 
        when llnull is accessed, if llnull is not yet in cache. 
 
        Parameters 
        ---------- 
        llnull : {None, float} 
            If llnull is not None, then the value will be directly assigned to 
            the cached attribute &quot;llnull&quot;. 
        attach_results : bool 
            Sets an internal flag whether the results instance of the null 
            model should be attached. By default without calling this method, 
            thenull model results are not attached and only the loglikelihood 
            value llnull is stored. 
        **kwargs 
            Additional keyword arguments used as fit keyword arguments for the 
            null model. The override and model default values. 
 
        Notes 
        ----- 
        Modifies attributes of this instance, and so has no return. 
        &quot;&quot;&quot;</span>
        <span class="s4"># reset cache, note we need to add here anything that depends on</span>
        <span class="s4"># llnullor the null model. If something is missing, then the attribute</span>
        <span class="s4"># might be incorrect.</span>
        <span class="s1">self._cache.pop(</span><span class="s2">'llnull'</span><span class="s3">, None</span><span class="s1">)</span>
        <span class="s1">self._cache.pop(</span><span class="s2">'llr'</span><span class="s3">, None</span><span class="s1">)</span>
        <span class="s1">self._cache.pop(</span><span class="s2">'llr_pvalue'</span><span class="s3">, None</span><span class="s1">)</span>
        <span class="s1">self._cache.pop(</span><span class="s2">'prsquared'</span><span class="s3">, None</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s2">'res_null'</span><span class="s1">):</span>
            <span class="s3">del </span><span class="s1">self.res_null</span>

        <span class="s3">if </span><span class="s1">llnull </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">self._cache[</span><span class="s2">'llnull'</span><span class="s1">] = llnull</span>
        <span class="s1">self._attach_nullmodel = attach_results</span>
        <span class="s1">self._optim_kwds_null = kwargs</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">llnull(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Value of the constant-only loglikelihood 
        &quot;&quot;&quot;</span>
        <span class="s1">model = self.model</span>
        <span class="s1">kwds = model._get_init_kwds().copy()</span>
        <span class="s3">for </span><span class="s1">key </span><span class="s3">in </span><span class="s1">getattr(model</span><span class="s3">, </span><span class="s2">'_null_drop_keys'</span><span class="s3">, </span><span class="s1">[]):</span>
            <span class="s3">del </span><span class="s1">kwds[key]</span>
        <span class="s4"># TODO: what parameters to pass to fit?</span>
        <span class="s1">mod_null = model.__class__(model.endog</span><span class="s3">, </span><span class="s1">np.ones(self.nobs)</span><span class="s3">, </span><span class="s1">**kwds)</span>
        <span class="s4"># TODO: consider catching and warning on convergence failure?</span>
        <span class="s4"># in the meantime, try hard to converge. see</span>
        <span class="s4"># TestPoissonConstrained1a.test_smoke</span>

        <span class="s1">optim_kwds = getattr(self</span><span class="s3">, </span><span class="s2">'_optim_kwds_null'</span><span class="s3">, </span><span class="s1">{}).copy()</span>

        <span class="s3">if </span><span class="s2">'start_params' </span><span class="s3">in </span><span class="s1">optim_kwds:</span>
            <span class="s4"># user provided</span>
            <span class="s1">sp_null = optim_kwds.pop(</span><span class="s2">'start_params'</span><span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">hasattr(model</span><span class="s3">, </span><span class="s2">'_get_start_params_null'</span><span class="s1">):</span>
            <span class="s4"># get moment estimates if available</span>
            <span class="s1">sp_null = model._get_start_params_null()</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">sp_null = </span><span class="s3">None</span>

        <span class="s1">opt_kwds = dict(method=</span><span class="s2">'bfgs'</span><span class="s3">, </span><span class="s1">warn_convergence=</span><span class="s3">False, </span><span class="s1">maxiter=</span><span class="s5">10000</span><span class="s3">,</span>
                        <span class="s1">disp=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">opt_kwds.update(optim_kwds)</span>

        <span class="s3">if </span><span class="s1">optim_kwds:</span>
            <span class="s1">res_null = mod_null.fit(start_params=sp_null</span><span class="s3">, </span><span class="s1">**opt_kwds)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s4"># this should be a reasonably method case across versions</span>
            <span class="s1">res_null = mod_null.fit(start_params=sp_null</span><span class="s3">, </span><span class="s1">method=</span><span class="s2">'nm'</span><span class="s3">,</span>
                                    <span class="s1">warn_convergence=</span><span class="s3">False,</span>
                                    <span class="s1">maxiter=</span><span class="s5">10000</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">0</span><span class="s1">)</span>
            <span class="s1">res_null = mod_null.fit(start_params=res_null.params</span><span class="s3">, </span><span class="s1">method=</span><span class="s2">'bfgs'</span><span class="s3">,</span>
                                    <span class="s1">warn_convergence=</span><span class="s3">False,</span>
                                    <span class="s1">maxiter=</span><span class="s5">10000</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">0</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">getattr(self</span><span class="s3">, </span><span class="s2">'_attach_nullmodel'</span><span class="s3">, False</span><span class="s1">) </span><span class="s3">is not False</span><span class="s1">:</span>
            <span class="s1">self.res_null = res_null</span>

        <span class="s3">return </span><span class="s1">res_null.llf</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">fittedvalues(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Linear predictor XB. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.dot(self.model.exog</span><span class="s3">, </span><span class="s1">self.params[:self.model.exog.shape[</span><span class="s5">1</span><span class="s1">]])</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid_response(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Respnose residuals. The response residuals are defined as 
        `endog - fittedvalues` 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.model.endog - self.predict()</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid_pearson(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Pearson residuals defined as response residuals divided by standard 
        deviation implied by the model. 
        &quot;&quot;&quot;</span>
        <span class="s1">var_ = self.predict(which=</span><span class="s2">&quot;var&quot;</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">self.resid_response / np.sqrt(var_)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">aic(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Akaike information criterion.  `-2*(llf - p)` where `p` is the number 
        of regressors including the intercept. 
        &quot;&quot;&quot;</span>
        <span class="s1">k_extra = getattr(self.model</span><span class="s3">, </span><span class="s2">'k_extra'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">-</span><span class="s5">2</span><span class="s1">*(self.llf - (self.df_model + </span><span class="s5">1 </span><span class="s1">+ k_extra))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">bic(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Bayesian information criterion. `-2*llf + ln(nobs)*p` where `p` is the 
        number of regressors including the intercept. 
        &quot;&quot;&quot;</span>
        <span class="s1">k_extra = getattr(self.model</span><span class="s3">, </span><span class="s2">'k_extra'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">-</span><span class="s5">2</span><span class="s1">*self.llf + np.log(self.nobs)*(self.df_model + </span><span class="s5">1 </span><span class="s1">+ k_extra)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">im_ratio(self):</span>
        <span class="s3">return </span><span class="s1">pinfer.im_ratio(self)</span>

    <span class="s3">def </span><span class="s1">info_criteria(self</span><span class="s3">, </span><span class="s1">crit</span><span class="s3">, </span><span class="s1">dk_params=</span><span class="s5">0</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Return an information criterion for the model. 
 
        Parameters 
        ---------- 
        crit : string 
            One of 'aic', 'bic', 'tic' or 'gbic'. 
        dk_params : int or float 
            Correction to the number of parameters used in the information 
            criterion. 
 
        Returns 
        ------- 
        Value of information criterion. 
 
        Notes 
        ----- 
        Tic and gbic 
 
        References 
        ---------- 
        Burnham KP, Anderson KR (2002). Model Selection and Multimodel 
        Inference; Springer New York. 
        &quot;&quot;&quot;</span>
        <span class="s1">crit = crit.lower()</span>
        <span class="s1">k_extra = getattr(self.model</span><span class="s3">, </span><span class="s2">'k_extra'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">k_params = self.df_model + </span><span class="s5">1 </span><span class="s1">+ k_extra + dk_params</span>

        <span class="s3">if </span><span class="s1">crit == </span><span class="s2">&quot;aic&quot;</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">-</span><span class="s5">2 </span><span class="s1">* self.llf + </span><span class="s5">2 </span><span class="s1">* k_params</span>
        <span class="s3">elif </span><span class="s1">crit == </span><span class="s2">&quot;bic&quot;</span><span class="s1">:</span>
            <span class="s1">nobs = self.df_model + self.df_resid + </span><span class="s5">1</span>
            <span class="s1">bic = -</span><span class="s5">2</span><span class="s1">*self.llf + k_params*np.log(nobs)</span>
            <span class="s3">return </span><span class="s1">bic</span>
        <span class="s3">elif </span><span class="s1">crit == </span><span class="s2">&quot;tic&quot;</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">pinfer.tic(self)</span>
        <span class="s3">elif </span><span class="s1">crit == </span><span class="s2">&quot;gbic&quot;</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">pinfer.gbic(self)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;Name of information criterion not recognized.&quot;</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">score_test(self</span><span class="s3">, </span><span class="s1">exog_extra=</span><span class="s3">None, </span><span class="s1">params_constrained=</span><span class="s3">None,</span>
                   <span class="s1">hypothesis=</span><span class="s2">'joint'</span><span class="s3">, </span><span class="s1">cov_type=</span><span class="s3">None, </span><span class="s1">cov_kwds=</span><span class="s3">None,</span>
                   <span class="s1">k_constraints=</span><span class="s3">None, </span><span class="s1">observed=</span><span class="s3">True</span><span class="s1">):</span>

        <span class="s1">res = pinfer.score_test(self</span><span class="s3">, </span><span class="s1">exog_extra=exog_extra</span><span class="s3">,</span>
                                <span class="s1">params_constrained=params_constrained</span><span class="s3">,</span>
                                <span class="s1">hypothesis=hypothesis</span><span class="s3">,</span>
                                <span class="s1">cov_type=cov_type</span><span class="s3">, </span><span class="s1">cov_kwds=cov_kwds</span><span class="s3">,</span>
                                <span class="s1">k_constraints=k_constraints</span><span class="s3">,</span>
                                <span class="s1">observed=observed)</span>
        <span class="s3">return </span><span class="s1">res</span>

    <span class="s1">score_test.__doc__ = pinfer.score_test.__doc__</span>

    <span class="s3">def </span><span class="s1">get_prediction(self</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None,</span>
                       <span class="s1">transform=</span><span class="s3">True, </span><span class="s1">which=</span><span class="s2">&quot;mean&quot;</span><span class="s3">, </span><span class="s1">linear=</span><span class="s3">None,</span>
                       <span class="s1">row_labels=</span><span class="s3">None, </span><span class="s1">average=</span><span class="s3">False,</span>
                       <span class="s1">agg_weights=</span><span class="s3">None, </span><span class="s1">y_values=</span><span class="s3">None,</span>
                       <span class="s1">**kwargs):</span>
        <span class="s0">&quot;&quot;&quot; 
        Compute prediction results when endpoint transformation is valid. 
 
        Parameters 
        ---------- 
        exog : array_like, optional 
            The values for which you want to predict. 
        transform : bool, optional 
            If the model was fit via a formula, do you want to pass 
            exog through the formula. Default is True. E.g., if you fit 
            a model y ~ log(x1) + log(x2), and transform is True, then 
            you can pass a data structure that contains x1 and x2 in 
            their original form. Otherwise, you'd need to log the data 
            first. 
        which : str 
            Which statistic is to be predicted. Default is &quot;mean&quot;. 
            The available statistics and options depend on the model. 
            see the model.predict docstring 
        linear : bool 
            Linear has been replaced by the `which` keyword and will be 
            deprecated. 
            If linear is True, then `which` is ignored and the linear 
            prediction is returned. 
        row_labels : list of str or None 
            If row_lables are provided, then they will replace the generated 
            labels. 
        average : bool 
            If average is True, then the mean prediction is computed, that is, 
            predictions are computed for individual exog and then the average 
            over observation is used. 
            If average is False, then the results are the predictions for all 
            observations, i.e. same length as ``exog``. 
        agg_weights : ndarray, optional 
            Aggregation weights, only used if average is True. 
            The weights are not normalized. 
        y_values : None or nd_array 
            Some predictive statistics like which=&quot;prob&quot; are computed at 
            values of the response variable. If y_values is not None, then 
            it will be used instead of the default set of y_values. 
 
            **Warning:** ``which=&quot;prob&quot;`` for count models currently computes 
            the pmf for all y=k up to max(endog). This can be a large array if 
            the observed endog values are large. 
            This will likely change so that the set of y_values will be chosen 
            to limit the array size. 
        **kwargs : 
            Some models can take additional keyword arguments, such as offset, 
            exposure or additional exog in multi-part models like zero inflated 
            models. 
            See the predict method of the model for the details. 
 
        Returns 
        ------- 
        prediction_results : PredictionResults 
            The prediction results instance contains prediction and prediction 
            variance and can on demand calculate confidence intervals and 
            summary dataframe for the prediction. 
 
        Notes 
        ----- 
        Status: new in 0.14, experimental 
        &quot;&quot;&quot;</span>

        <span class="s3">if </span><span class="s1">linear </span><span class="s3">is True</span><span class="s1">:</span>
            <span class="s4"># compatibility with old keyword</span>
            <span class="s1">which = </span><span class="s2">&quot;linear&quot;</span>

        <span class="s1">pred_kwds = kwargs</span>
        <span class="s4"># y_values is explicit so we can add it to the docstring</span>
        <span class="s3">if </span><span class="s1">y_values </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">pred_kwds[</span><span class="s2">&quot;y_values&quot;</span><span class="s1">] = y_values</span>

        <span class="s1">res = pred.get_prediction(</span>
            <span class="s1">self</span><span class="s3">,</span>
            <span class="s1">exog=exog</span><span class="s3">,</span>
            <span class="s1">which=which</span><span class="s3">,</span>
            <span class="s1">transform=transform</span><span class="s3">,</span>
            <span class="s1">row_labels=row_labels</span><span class="s3">,</span>
            <span class="s1">average=average</span><span class="s3">,</span>
            <span class="s1">agg_weights=agg_weights</span><span class="s3">,</span>
            <span class="s1">pred_kwds=pred_kwds</span>
            <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">res</span>

    <span class="s3">def </span><span class="s1">get_distribution(self</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">transform=</span><span class="s3">True, </span><span class="s1">**kwargs):</span>

        <span class="s1">exog</span><span class="s3">, </span><span class="s1">_ = self._transform_predict_exog(exog</span><span class="s3">, </span><span class="s1">transform=transform)</span>
        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">exog = np.asarray(exog)</span>
        <span class="s1">distr = self.model.get_distribution(self.params</span><span class="s3">,</span>
                                            <span class="s1">exog=exog</span><span class="s3">,</span>
                                            <span class="s1">**kwargs</span>
                                            <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">distr</span>

    <span class="s3">def </span><span class="s1">_get_endog_name(self</span><span class="s3">, </span><span class="s1">yname</span><span class="s3">, </span><span class="s1">yname_list):</span>
        <span class="s3">if </span><span class="s1">yname </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">yname = self.model.endog_names</span>
        <span class="s3">if </span><span class="s1">yname_list </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">yname_list = self.model.endog_names</span>
        <span class="s3">return </span><span class="s1">yname</span><span class="s3">, </span><span class="s1">yname_list</span>

    <span class="s3">def </span><span class="s1">get_margeff(self</span><span class="s3">, </span><span class="s1">at=</span><span class="s2">'overall'</span><span class="s3">, </span><span class="s1">method=</span><span class="s2">'dydx'</span><span class="s3">, </span><span class="s1">atexog=</span><span class="s3">None,</span>
            <span class="s1">dummy=</span><span class="s3">False, </span><span class="s1">count=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Get marginal effects of the fitted model. 
 
        Parameters 
        ---------- 
        at : str, optional 
            Options are: 
 
            - 'overall', The average of the marginal effects at each 
              observation. 
            - 'mean', The marginal effects at the mean of each regressor. 
            - 'median', The marginal effects at the median of each regressor. 
            - 'zero', The marginal effects at zero for each regressor. 
            - 'all', The marginal effects at each observation. If `at` is all 
              only margeff will be available from the returned object. 
 
            Note that if `exog` is specified, then marginal effects for all 
            variables not specified by `exog` are calculated using the `at` 
            option. 
        method : str, optional 
            Options are: 
 
            - 'dydx' - dy/dx - No transformation is made and marginal effects 
              are returned.  This is the default. 
            - 'eyex' - estimate elasticities of variables in `exog` -- 
              d(lny)/d(lnx) 
            - 'dyex' - estimate semi-elasticity -- dy/d(lnx) 
            - 'eydx' - estimate semi-elasticity -- d(lny)/dx 
 
            Note that tranformations are done after each observation is 
            calculated.  Semi-elasticities for binary variables are computed 
            using the midpoint method. 'dyex' and 'eyex' do not make sense 
            for discrete variables. For interpretations of these methods 
            see notes below. 
        atexog : array_like, optional 
            Optionally, you can provide the exogenous variables over which to 
            get the marginal effects.  This should be a dictionary with the key 
            as the zero-indexed column number and the value of the dictionary. 
            Default is None for all independent variables less the constant. 
        dummy : bool, optional 
            If False, treats binary variables (if present) as continuous.  This 
            is the default.  Else if True, treats binary variables as 
            changing from 0 to 1.  Note that any variable that is either 0 or 1 
            is treated as binary.  Each binary variable is treated separately 
            for now. 
        count : bool, optional 
            If False, treats count variables (if present) as continuous.  This 
            is the default.  Else if True, the marginal effect is the 
            change in probabilities when each observation is increased by one. 
 
        Returns 
        ------- 
        DiscreteMargins : marginal effects instance 
            Returns an object that holds the marginal effects, standard 
            errors, confidence intervals, etc. See 
            `statsmodels.discrete.discrete_margins.DiscreteMargins` for more 
            information. 
 
        Notes 
        ----- 
        Interpretations of methods: 
 
        - 'dydx' - change in `endog` for a change in `exog`. 
        - 'eyex' - proportional change in `endog` for a proportional change 
          in `exog`. 
        - 'dyex' - change in `endog` for a proportional change in `exog`. 
        - 'eydx' - proportional change in `endog` for a change in `exog`. 
 
        When using after Poisson, returns the expected number of events per 
        period, assuming that the model is loglinear. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">getattr(self.model</span><span class="s3">, </span><span class="s2">&quot;offset&quot;</span><span class="s3">, None</span><span class="s1">) </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s2">&quot;Margins with offset are not available.&quot;</span><span class="s1">)</span>
        <span class="s3">from </span><span class="s1">statsmodels.discrete.discrete_margins </span><span class="s3">import </span><span class="s1">DiscreteMargins</span>
        <span class="s3">return </span><span class="s1">DiscreteMargins(self</span><span class="s3">, </span><span class="s1">(at</span><span class="s3">, </span><span class="s1">method</span><span class="s3">, </span><span class="s1">atexog</span><span class="s3">, </span><span class="s1">dummy</span><span class="s3">, </span><span class="s1">count))</span>

    <span class="s3">def </span><span class="s1">get_influence(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Get an instance of MLEInfluence with influence and outlier measures 
 
        Returns 
        ------- 
        infl : MLEInfluence instance 
            The instance has methods to calculate the main influence and 
            outlier measures as attributes. 
 
        See Also 
        -------- 
        statsmodels.stats.outliers_influence.MLEInfluence 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.stats.outliers_influence </span><span class="s3">import </span><span class="s1">MLEInfluence</span>
        <span class="s3">return </span><span class="s1">MLEInfluence(self)</span>

    <span class="s3">def </span><span class="s1">summary(self</span><span class="s3">, </span><span class="s1">yname=</span><span class="s3">None, </span><span class="s1">xname=</span><span class="s3">None, </span><span class="s1">title=</span><span class="s3">None, </span><span class="s1">alpha=</span><span class="s5">.05</span><span class="s3">,</span>
                <span class="s1">yname_list=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Summarize the Regression Results. 
 
        Parameters 
        ---------- 
        yname : str, optional 
            The name of the endog variable in the tables. The default is `y`. 
        xname : list[str], optional 
            The names for the exogenous variables, default is &quot;var_xx&quot;. 
            Must match the number of parameters in the model. 
        title : str, optional 
            Title for the top table. If not None, then this replaces the 
            default title. 
        alpha : float 
            The significance level for the confidence intervals. 
 
        Returns 
        ------- 
        Summary 
            Class that holds the summary tables and text, which can be printed 
            or converted to various output formats. 
 
        See Also 
        -------- 
        statsmodels.iolib.summary.Summary : Class that hold summary results. 
        &quot;&quot;&quot;</span>

        <span class="s1">top_left = [(</span><span class="s2">'Dep. Variable:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s2">'Model:'</span><span class="s3">, </span><span class="s1">[self.model.__class__.__name__])</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s2">'Method:'</span><span class="s3">, </span><span class="s1">[self.method])</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s2">'Date:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s2">'Time:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s2">'converged:'</span><span class="s3">, </span><span class="s1">[</span><span class="s2">&quot;%s&quot; </span><span class="s1">% self.mle_retvals[</span><span class="s2">'converged'</span><span class="s1">]])</span><span class="s3">,</span>
                    <span class="s1">]</span>

        <span class="s1">top_right = [(</span><span class="s2">'No. Observations:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s2">'Df Residuals:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s2">'Df Model:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s2">'Pseudo R-squ.:'</span><span class="s3">, </span><span class="s1">[</span><span class="s2">&quot;%#6.4g&quot; </span><span class="s1">% self.prsquared])</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s2">'Log-Likelihood:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s2">'LL-Null:'</span><span class="s3">, </span><span class="s1">[</span><span class="s2">&quot;%#8.5g&quot; </span><span class="s1">% self.llnull])</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s2">'LLR p-value:'</span><span class="s3">, </span><span class="s1">[</span><span class="s2">&quot;%#6.4g&quot; </span><span class="s1">% self.llr_pvalue])</span>
                     <span class="s1">]</span>

        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s2">'cov_type'</span><span class="s1">):</span>
            <span class="s1">top_left.append((</span><span class="s2">'Covariance Type:'</span><span class="s3">, </span><span class="s1">[self.cov_type]))</span>

        <span class="s3">if </span><span class="s1">title </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">title = self.model.__class__.__name__ + </span><span class="s2">' ' </span><span class="s1">+ </span><span class="s2">&quot;Regression Results&quot;</span>

        <span class="s4"># boiler plate</span>
        <span class="s3">from </span><span class="s1">statsmodels.iolib.summary </span><span class="s3">import </span><span class="s1">Summary</span>
        <span class="s1">smry = Summary()</span>
        <span class="s1">yname</span><span class="s3">, </span><span class="s1">yname_list = self._get_endog_name(yname</span><span class="s3">, </span><span class="s1">yname_list)</span>

        <span class="s4"># for top of table</span>
        <span class="s1">smry.add_table_2cols(self</span><span class="s3">, </span><span class="s1">gleft=top_left</span><span class="s3">, </span><span class="s1">gright=top_right</span><span class="s3">,</span>
                             <span class="s1">yname=yname</span><span class="s3">, </span><span class="s1">xname=xname</span><span class="s3">, </span><span class="s1">title=title)</span>

        <span class="s4"># for parameters, etc</span>
        <span class="s1">smry.add_table_params(self</span><span class="s3">, </span><span class="s1">yname=yname_list</span><span class="s3">, </span><span class="s1">xname=xname</span><span class="s3">, </span><span class="s1">alpha=alpha</span><span class="s3">,</span>
                              <span class="s1">use_t=self.use_t)</span>

        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s2">'constraints'</span><span class="s1">):</span>
            <span class="s1">smry.add_extra_txt([</span><span class="s2">'Model has been estimated subject to linear '</span>
                                <span class="s2">'equality constraints.'</span><span class="s1">])</span>

        <span class="s3">return </span><span class="s1">smry</span>

    <span class="s3">def </span><span class="s1">summary2(self</span><span class="s3">, </span><span class="s1">yname=</span><span class="s3">None, </span><span class="s1">xname=</span><span class="s3">None, </span><span class="s1">title=</span><span class="s3">None, </span><span class="s1">alpha=</span><span class="s5">.05</span><span class="s3">,</span>
                 <span class="s1">float_format=</span><span class="s2">&quot;%.4f&quot;</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Experimental function to summarize regression results. 
 
        Parameters 
        ---------- 
        yname : str 
            Name of the dependent variable (optional). 
        xname : list[str], optional 
            List of strings of length equal to the number of parameters 
            Names of the independent variables (optional). 
        title : str, optional 
            Title for the top table. If not None, then this replaces the 
            default title. 
        alpha : float 
            The significance level for the confidence intervals. 
        float_format : str 
            The print format for floats in parameters summary. 
 
        Returns 
        ------- 
        Summary 
            Instance that contains the summary tables and text, which can be 
            printed or converted to various output formats. 
 
        See Also 
        -------- 
        statsmodels.iolib.summary2.Summary : Class that holds summary results. 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.iolib </span><span class="s3">import </span><span class="s1">summary2</span>
        <span class="s1">smry = summary2.Summary()</span>
        <span class="s1">smry.add_base(results=self</span><span class="s3">, </span><span class="s1">alpha=alpha</span><span class="s3">, </span><span class="s1">float_format=float_format</span><span class="s3">,</span>
                      <span class="s1">xname=xname</span><span class="s3">, </span><span class="s1">yname=yname</span><span class="s3">, </span><span class="s1">title=title)</span>

        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s2">'constraints'</span><span class="s1">):</span>
            <span class="s1">smry.add_text(</span><span class="s2">'Model has been estimated subject to linear '</span>
                          <span class="s2">'equality constraints.'</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">smry</span>


<span class="s3">class </span><span class="s1">CountResults(DiscreteResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span>
        <span class="s2">&quot;one_line_description&quot;</span><span class="s1">: </span><span class="s2">&quot;A results class for count data&quot;</span><span class="s3">,</span>
        <span class="s2">&quot;extra_attr&quot;</span><span class="s1">: </span><span class="s2">&quot;&quot;</span><span class="s1">}</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Residuals 
 
        Notes 
        ----- 
        The residuals for Count models are defined as 
 
        .. math:: y - p 
 
        where :math:`p = \\exp(X\\beta)`. Any exposure and offset variables 
        are also handled. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.model.endog - self.predict()</span>

    <span class="s3">def </span><span class="s1">get_diagnostic(self</span><span class="s3">, </span><span class="s1">y_max=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Get instance of class with specification and diagnostic methods. 
 
        experimental, API of Diagnostic classes will change 
 
        Returns 
        ------- 
        CountDiagnostic instance 
            The instance has methods to perform specification and diagnostic 
            tesst and plots 
 
        See Also 
        -------- 
        statsmodels.statsmodels.discrete.diagnostic.CountDiagnostic 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.discrete.diagnostic </span><span class="s3">import </span><span class="s1">CountDiagnostic</span>
        <span class="s3">return </span><span class="s1">CountDiagnostic(self</span><span class="s3">, </span><span class="s1">y_max=y_max)</span>


<span class="s3">class </span><span class="s1">NegativeBinomialResults(CountResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span>
        <span class="s2">&quot;one_line_description&quot;</span><span class="s1">: </span><span class="s2">&quot;A results class for NegativeBinomial 1 and 2&quot;</span><span class="s3">,</span>
        <span class="s2">&quot;extra_attr&quot;</span><span class="s1">: </span><span class="s2">&quot;&quot;</span><span class="s1">}</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">lnalpha(self):</span>
        <span class="s0">&quot;&quot;&quot;Natural log of alpha&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.log(self.params[-</span><span class="s5">1</span><span class="s1">])</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">lnalpha_std_err(self):</span>
        <span class="s0">&quot;&quot;&quot;Natural log of standardized error&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.bse[-</span><span class="s5">1</span><span class="s1">] / self.params[-</span><span class="s5">1</span><span class="s1">]</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">aic(self):</span>
        <span class="s4"># + 1 because we estimate alpha</span>
        <span class="s1">k_extra = getattr(self.model</span><span class="s3">, </span><span class="s2">'k_extra'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">-</span><span class="s5">2</span><span class="s1">*(self.llf - (self.df_model + self.k_constant + k_extra))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">bic(self):</span>
        <span class="s4"># + 1 because we estimate alpha</span>
        <span class="s1">k_extra = getattr(self.model</span><span class="s3">, </span><span class="s2">'k_extra'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">-</span><span class="s5">2</span><span class="s1">*self.llf + np.log(self.nobs)*(self.df_model +</span>
                                                <span class="s1">self.k_constant + k_extra)</span>


<span class="s3">class </span><span class="s1">NegativeBinomialPResults(NegativeBinomialResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span>
        <span class="s2">&quot;one_line_description&quot;</span><span class="s1">: </span><span class="s2">&quot;A results class for NegativeBinomialP&quot;</span><span class="s3">,</span>
        <span class="s2">&quot;extra_attr&quot;</span><span class="s1">: </span><span class="s2">&quot;&quot;</span><span class="s1">}</span>


<span class="s3">class </span><span class="s1">GeneralizedPoissonResults(NegativeBinomialResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span>
        <span class="s2">&quot;one_line_description&quot;</span><span class="s1">: </span><span class="s2">&quot;A results class for Generalized Poisson&quot;</span><span class="s3">,</span>
        <span class="s2">&quot;extra_attr&quot;</span><span class="s1">: </span><span class="s2">&quot;&quot;</span><span class="s1">}</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">_dispersion_factor(self):</span>
        <span class="s1">p = getattr(self.model</span><span class="s3">, </span><span class="s2">'parameterization'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">mu = self.predict()</span>
        <span class="s3">return </span><span class="s1">(</span><span class="s5">1 </span><span class="s1">+ self.params[-</span><span class="s5">1</span><span class="s1">] * mu**p)**</span><span class="s5">2</span>


<span class="s3">class </span><span class="s1">L1CountResults(DiscreteResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span><span class="s2">&quot;one_line_description&quot; </span><span class="s1">:</span>
            <span class="s2">&quot;A results class for count data fit by l1 regularization&quot;</span><span class="s3">,</span>
            <span class="s2">&quot;extra_attr&quot; </span><span class="s1">: _l1_results_attr}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">model</span><span class="s3">, </span><span class="s1">cntfit):</span>
        <span class="s1">super(L1CountResults</span><span class="s3">, </span><span class="s1">self).__init__(model</span><span class="s3">, </span><span class="s1">cntfit)</span>
        <span class="s4"># self.trimmed is a boolean array with T/F telling whether or not that</span>
        <span class="s4"># entry in params has been set zero'd out.</span>
        <span class="s1">self.trimmed = cntfit.mle_retvals[</span><span class="s2">'trimmed'</span><span class="s1">]</span>
        <span class="s1">self.nnz_params = (~self.trimmed).sum()</span>

        <span class="s4"># Set degrees of freedom.  In doing so,</span>
        <span class="s4"># adjust for extra parameter in NegativeBinomial nb1 and nb2</span>
        <span class="s4"># extra parameter is not included in df_model</span>
        <span class="s1">k_extra = getattr(self.model</span><span class="s3">, </span><span class="s2">'k_extra'</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>

        <span class="s1">self.df_model = self.nnz_params - </span><span class="s5">1 </span><span class="s1">- k_extra</span>
        <span class="s1">self.df_resid = float(self.model.endog.shape[</span><span class="s5">0</span><span class="s1">] - self.nnz_params) + k_extra</span>


<span class="s3">class </span><span class="s1">PoissonResults(CountResults):</span>

    <span class="s3">def </span><span class="s1">predict_prob(self</span><span class="s3">, </span><span class="s1">n=</span><span class="s3">None, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exposure=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None,</span>
                     <span class="s1">transform=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return predicted probability of each count level for each observation 
 
        Parameters 
        ---------- 
        n : array_like or int 
            The counts for which you want the probabilities. If n is None 
            then the probabilities for each count from 0 to max(y) are 
            given. 
 
        Returns 
        ------- 
        ndarray 
            A nobs x n array where len(`n`) columns are indexed by the count 
            n. If n is None, then column 0 is the probability that each 
            observation is 0, column 1 is the probability that each 
            observation is 1, etc. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">n </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">counts = np.atleast_2d(n)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">counts = np.atleast_2d(np.arange(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">np.max(self.model.endog)+</span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">mu = self.predict(exog=exog</span><span class="s3">, </span><span class="s1">exposure=exposure</span><span class="s3">, </span><span class="s1">offset=offset</span><span class="s3">,</span>
                          <span class="s1">transform=transform</span><span class="s3">, </span><span class="s1">which=</span><span class="s2">&quot;mean&quot;</span><span class="s1">)[:</span><span class="s3">,None</span><span class="s1">]</span>
        <span class="s4"># uses broadcasting</span>
        <span class="s3">return </span><span class="s1">stats.poisson.pmf(counts</span><span class="s3">, </span><span class="s1">mu)</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">resid_pearson(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Pearson residuals 
 
        Notes 
        ----- 
        Pearson residuals are defined to be 
 
        .. math:: r_j = \\frac{(y - M_jp_j)}{\\sqrt{M_jp_j(1-p_j)}} 
 
        where :math:`p_j=cdf(X\\beta)` and :math:`M_j` is the total number of 
        observations sharing the covariate pattern :math:`j`. 
 
        For now :math:`M_j` is always set to 1. 
        &quot;&quot;&quot;</span>
        <span class="s4"># Pearson residuals</span>
        <span class="s1">p = self.predict()  </span><span class="s4"># fittedvalues is still linear</span>
        <span class="s3">return </span><span class="s1">(self.model.endog - p)/np.sqrt(p)</span>

    <span class="s3">def </span><span class="s1">get_influence(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Get an instance of MLEInfluence with influence and outlier measures 
 
        Returns 
        ------- 
        infl : MLEInfluence instance 
            The instance has methods to calculate the main influence and 
            outlier measures as attributes. 
 
        See Also 
        -------- 
        statsmodels.stats.outliers_influence.MLEInfluence 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.stats.outliers_influence </span><span class="s3">import </span><span class="s1">MLEInfluence</span>
        <span class="s3">return </span><span class="s1">MLEInfluence(self)</span>

    <span class="s3">def </span><span class="s1">get_diagnostic(self</span><span class="s3">, </span><span class="s1">y_max=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Get instance of class with specification and diagnostic methods 
 
        experimental, API of Diagnostic classes will change 
 
        Returns 
        ------- 
        PoissonDiagnostic instance 
            The instance has methods to perform specification and diagnostic 
            tesst and plots 
 
        See Also 
        -------- 
        statsmodels.statsmodels.discrete.diagnostic.PoissonDiagnostic 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.discrete.diagnostic </span><span class="s3">import </span><span class="s1">(</span>
            <span class="s1">PoissonDiagnostic)</span>
        <span class="s3">return </span><span class="s1">PoissonDiagnostic(self</span><span class="s3">, </span><span class="s1">y_max=y_max)</span>


<span class="s3">class </span><span class="s1">L1PoissonResults(L1CountResults</span><span class="s3">, </span><span class="s1">PoissonResults):</span>
    <span class="s3">pass</span>

<span class="s3">class </span><span class="s1">L1NegativeBinomialResults(L1CountResults</span><span class="s3">, </span><span class="s1">NegativeBinomialResults):</span>
    <span class="s3">pass</span>

<span class="s3">class </span><span class="s1">L1GeneralizedPoissonResults(L1CountResults</span><span class="s3">, </span><span class="s1">GeneralizedPoissonResults):</span>
    <span class="s3">pass</span>

<span class="s3">class </span><span class="s1">OrderedResults(DiscreteResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span><span class="s2">&quot;one_line_description&quot; </span><span class="s1">: </span><span class="s2">&quot;A results class for ordered discrete data.&quot; </span><span class="s3">, </span><span class="s2">&quot;extra_attr&quot; </span><span class="s1">: </span><span class="s2">&quot;&quot;</span><span class="s1">}</span>
    <span class="s3">pass</span>

<span class="s3">class </span><span class="s1">BinaryResults(DiscreteResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span><span class="s2">&quot;one_line_description&quot; </span><span class="s1">: </span><span class="s2">&quot;A results class for binary data&quot;</span><span class="s3">, </span><span class="s2">&quot;extra_attr&quot; </span><span class="s1">: </span><span class="s2">&quot;&quot;</span><span class="s1">}</span>

    <span class="s3">def </span><span class="s1">pred_table(self</span><span class="s3">, </span><span class="s1">threshold=</span><span class="s5">.5</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Prediction table 
 
        Parameters 
        ---------- 
        threshold : scalar 
            Number between 0 and 1. Threshold above which a prediction is 
            considered 1 and below which a prediction is considered 0. 
 
        Notes 
        ----- 
        pred_table[i,j] refers to the number of times &quot;i&quot; was observed and 
        the model predicted &quot;j&quot;. Correct predictions are along the diagonal. 
        &quot;&quot;&quot;</span>
        <span class="s1">model = self.model</span>
        <span class="s1">actual = model.endog</span>
        <span class="s1">pred = np.array(self.predict() &gt; threshold</span><span class="s3">, </span><span class="s1">dtype=float)</span>
        <span class="s1">bins = np.array([</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0.5</span><span class="s3">, </span><span class="s5">1</span><span class="s1">])</span>
        <span class="s3">return </span><span class="s1">np.histogram2d(actual</span><span class="s3">, </span><span class="s1">pred</span><span class="s3">, </span><span class="s1">bins=bins)[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s1">@Appender(DiscreteResults.summary.__doc__)</span>
    <span class="s3">def </span><span class="s1">summary(self</span><span class="s3">, </span><span class="s1">yname=</span><span class="s3">None, </span><span class="s1">xname=</span><span class="s3">None, </span><span class="s1">title=</span><span class="s3">None, </span><span class="s1">alpha=</span><span class="s5">.05</span><span class="s3">,</span>
                <span class="s1">yname_list=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">smry = super(BinaryResults</span><span class="s3">, </span><span class="s1">self).summary(yname</span><span class="s3">, </span><span class="s1">xname</span><span class="s3">, </span><span class="s1">title</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">,</span>
                                                  <span class="s1">yname_list)</span>
        <span class="s1">fittedvalues = self.model.cdf(self.fittedvalues)</span>
        <span class="s1">absprederror = np.abs(self.model.endog - fittedvalues)</span>
        <span class="s1">predclose_sum = (absprederror &lt; </span><span class="s5">1e-4</span><span class="s1">).sum()</span>
        <span class="s1">predclose_frac = predclose_sum / len(fittedvalues)</span>

        <span class="s4"># add warnings/notes</span>
        <span class="s1">etext = []</span>
        <span class="s3">if </span><span class="s1">predclose_sum == len(fittedvalues):  </span><span class="s4"># TODO: nobs?</span>
            <span class="s1">wstr = </span><span class="s2">&quot;Complete Separation: The results show that there is&quot;</span>
            <span class="s1">wstr += </span><span class="s2">&quot;complete separation or perfect prediction.</span><span class="s3">\n</span><span class="s2">&quot;</span>
            <span class="s1">wstr += </span><span class="s2">&quot;In this case the Maximum Likelihood Estimator does &quot;</span>
            <span class="s1">wstr += </span><span class="s2">&quot;not exist and the parameters</span><span class="s3">\n</span><span class="s2">&quot;</span>
            <span class="s1">wstr += </span><span class="s2">&quot;are not identified.&quot;</span>
            <span class="s1">etext.append(wstr)</span>
        <span class="s3">elif </span><span class="s1">predclose_frac &gt; </span><span class="s5">0.1</span><span class="s1">:  </span><span class="s4"># TODO: get better diagnosis</span>
            <span class="s1">wstr = </span><span class="s2">&quot;Possibly complete quasi-separation: A fraction &quot;</span>
            <span class="s1">wstr += </span><span class="s2">&quot;%4.2f of observations can be</span><span class="s3">\n</span><span class="s2">&quot; </span><span class="s1">% predclose_frac</span>
            <span class="s1">wstr += </span><span class="s2">&quot;perfectly predicted. This might indicate that there &quot;</span>
            <span class="s1">wstr += </span><span class="s2">&quot;is complete</span><span class="s3">\n</span><span class="s2">quasi-separation. In this case some &quot;</span>
            <span class="s1">wstr += </span><span class="s2">&quot;parameters will not be identified.&quot;</span>
            <span class="s1">etext.append(wstr)</span>
        <span class="s3">if </span><span class="s1">etext:</span>
            <span class="s1">smry.add_extra_txt(etext)</span>
        <span class="s3">return </span><span class="s1">smry</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid_dev(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Deviance residuals 
 
        Notes 
        ----- 
        Deviance residuals are defined 
 
        .. math:: d_j = \\pm\\left(2\\left[Y_j\\ln\\left(\\frac{Y_j}{M_jp_j}\\right) + (M_j - Y_j\\ln\\left(\\frac{M_j-Y_j}{M_j(1-p_j)} \\right) \\right] \\right)^{1/2} 
 
        where 
 
        :math:`p_j = cdf(X\\beta)` and :math:`M_j` is the total number of 
        observations sharing the covariate pattern :math:`j`. 
 
        For now :math:`M_j` is always set to 1. 
        &quot;&quot;&quot;</span>
        <span class="s4">#These are the deviance residuals</span>
        <span class="s4">#model = self.model</span>
        <span class="s1">endog = self.model.endog</span>
        <span class="s4">#exog = model.exog</span>
        <span class="s4"># M = # of individuals that share a covariate pattern</span>
        <span class="s4"># so M[i] = 2 for i = two share a covariate pattern</span>
        <span class="s1">M = </span><span class="s5">1</span>
        <span class="s1">p = self.predict()</span>
        <span class="s4">#Y_0 = np.where(exog == 0)</span>
        <span class="s4">#Y_M = np.where(exog == M)</span>
        <span class="s4">#NOTE: Common covariate patterns are not yet handled</span>
        <span class="s1">res = -(</span><span class="s5">1</span><span class="s1">-endog)*np.sqrt(</span><span class="s5">2</span><span class="s1">*M*np.abs(np.log(</span><span class="s5">1</span><span class="s1">-p))) + \</span>
                <span class="s1">endog*np.sqrt(</span><span class="s5">2</span><span class="s1">*M*np.abs(np.log(p)))</span>
        <span class="s3">return </span><span class="s1">res</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid_pearson(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Pearson residuals 
 
        Notes 
        ----- 
        Pearson residuals are defined to be 
 
        .. math:: r_j = \\frac{(y - M_jp_j)}{\\sqrt{M_jp_j(1-p_j)}} 
 
        where :math:`p_j=cdf(X\\beta)` and :math:`M_j` is the total number of 
        observations sharing the covariate pattern :math:`j`. 
 
        For now :math:`M_j` is always set to 1. 
        &quot;&quot;&quot;</span>
        <span class="s4"># Pearson residuals</span>
        <span class="s4">#model = self.model</span>
        <span class="s1">endog = self.model.endog</span>
        <span class="s4">#exog = model.exog</span>
        <span class="s4"># M = # of individuals that share a covariate pattern</span>
        <span class="s4"># so M[i] = 2 for i = two share a covariate pattern</span>
        <span class="s4"># use unique row pattern?</span>
        <span class="s1">M = </span><span class="s5">1</span>
        <span class="s1">p = self.predict()</span>
        <span class="s3">return </span><span class="s1">(endog - M*p)/np.sqrt(M*p*(</span><span class="s5">1</span><span class="s1">-p))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid_response(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        The response residuals 
 
        Notes 
        ----- 
        Response residuals are defined to be 
 
        .. math:: y - p 
 
        where :math:`p=cdf(X\\beta)`. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.model.endog - self.predict()</span>


<span class="s3">class </span><span class="s1">LogitResults(BinaryResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span>
        <span class="s2">&quot;one_line_description&quot;</span><span class="s1">: </span><span class="s2">&quot;A results class for Logit Model&quot;</span><span class="s3">,</span>
        <span class="s2">&quot;extra_attr&quot;</span><span class="s1">: </span><span class="s2">&quot;&quot;</span><span class="s1">}</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid_generalized(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Generalized residuals 
 
        Notes 
        ----- 
        The generalized residuals for the Logit model are defined 
 
        .. math:: y - p 
 
        where :math:`p=cdf(X\\beta)`. This is the same as the `resid_response` 
        for the Logit model. 
        &quot;&quot;&quot;</span>
        <span class="s4"># Generalized residuals</span>
        <span class="s3">return </span><span class="s1">self.model.endog - self.predict()</span>

    <span class="s3">def </span><span class="s1">get_influence(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Get an instance of MLEInfluence with influence and outlier measures 
 
        Returns 
        ------- 
        infl : MLEInfluence instance 
            The instance has methods to calculate the main influence and 
            outlier measures as attributes. 
 
        See Also 
        -------- 
        statsmodels.stats.outliers_influence.MLEInfluence 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.stats.outliers_influence </span><span class="s3">import </span><span class="s1">MLEInfluence</span>
        <span class="s3">return </span><span class="s1">MLEInfluence(self)</span>


<span class="s3">class </span><span class="s1">ProbitResults(BinaryResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span>
        <span class="s2">&quot;one_line_description&quot;</span><span class="s1">: </span><span class="s2">&quot;A results class for Probit Model&quot;</span><span class="s3">,</span>
        <span class="s2">&quot;extra_attr&quot;</span><span class="s1">: </span><span class="s2">&quot;&quot;</span><span class="s1">}</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid_generalized(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Generalized residuals 
 
        Notes 
        ----- 
        The generalized residuals for the Probit model are defined 
 
        .. math:: y\\frac{\\phi(X\\beta)}{\\Phi(X\\beta)}-(1-y)\\frac{\\phi(X\\beta)}{1-\\Phi(X\\beta)} 
        &quot;&quot;&quot;</span>
        <span class="s4"># generalized residuals</span>
        <span class="s1">model = self.model</span>
        <span class="s1">endog = model.endog</span>
        <span class="s1">XB = self.predict(which=</span><span class="s2">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s1">pdf = model.pdf(XB)</span>
        <span class="s1">cdf = model.cdf(XB)</span>
        <span class="s3">return </span><span class="s1">endog * pdf/cdf - (</span><span class="s5">1</span><span class="s1">-endog)*pdf/(</span><span class="s5">1</span><span class="s1">-cdf)</span>

<span class="s3">class </span><span class="s1">L1BinaryResults(BinaryResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span><span class="s2">&quot;one_line_description&quot; </span><span class="s1">:</span>
    <span class="s2">&quot;Results instance for binary data fit by l1 regularization&quot;</span><span class="s3">,</span>
    <span class="s2">&quot;extra_attr&quot; </span><span class="s1">: _l1_results_attr}</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">model</span><span class="s3">, </span><span class="s1">bnryfit):</span>
        <span class="s1">super(L1BinaryResults</span><span class="s3">, </span><span class="s1">self).__init__(model</span><span class="s3">, </span><span class="s1">bnryfit)</span>
        <span class="s4"># self.trimmed is a boolean array with T/F telling whether or not that</span>
        <span class="s4"># entry in params has been set zero'd out.</span>
        <span class="s1">self.trimmed = bnryfit.mle_retvals[</span><span class="s2">'trimmed'</span><span class="s1">]</span>
        <span class="s1">self.nnz_params = (~self.trimmed).sum()</span>
        <span class="s1">self.df_model = self.nnz_params - </span><span class="s5">1</span>
        <span class="s1">self.df_resid = float(self.model.endog.shape[</span><span class="s5">0</span><span class="s1">] - self.nnz_params)</span>


<span class="s3">class </span><span class="s1">MultinomialResults(DiscreteResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span><span class="s2">&quot;one_line_description&quot; </span><span class="s1">:</span>
            <span class="s2">&quot;A results class for multinomial data&quot;</span><span class="s3">, </span><span class="s2">&quot;extra_attr&quot; </span><span class="s1">: </span><span class="s2">&quot;&quot;</span><span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">model</span><span class="s3">, </span><span class="s1">mlefit):</span>
        <span class="s1">super(MultinomialResults</span><span class="s3">, </span><span class="s1">self).__init__(model</span><span class="s3">, </span><span class="s1">mlefit)</span>
        <span class="s1">self.J = model.J</span>
        <span class="s1">self.K = model.K</span>

    <span class="s1">@staticmethod</span>
    <span class="s3">def </span><span class="s1">_maybe_convert_ynames_int(ynames):</span>
        <span class="s4"># see if they're integers</span>
        <span class="s1">issue_warning = </span><span class="s3">False</span>
        <span class="s1">msg = (</span><span class="s2">'endog contains values are that not int-like. Uses string '</span>
               <span class="s2">'representation of value. Use integer-valued endog to '</span>
               <span class="s2">'suppress this warning.'</span><span class="s1">)</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">ynames:</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s3">if </span><span class="s1">ynames[i] % </span><span class="s5">1 </span><span class="s1">== </span><span class="s5">0</span><span class="s1">:</span>
                    <span class="s1">ynames[i] = str(int(ynames[i]))</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">issue_warning = </span><span class="s3">True</span>
                    <span class="s1">ynames[i] = str(ynames[i])</span>
            <span class="s3">except </span><span class="s1">TypeError:</span>
                <span class="s1">ynames[i] = str(ynames[i])</span>
        <span class="s3">if </span><span class="s1">issue_warning:</span>
            <span class="s1">warnings.warn(msg</span><span class="s3">, </span><span class="s1">SpecificationWarning)</span>

        <span class="s3">return </span><span class="s1">ynames</span>

    <span class="s3">def </span><span class="s1">_get_endog_name(self</span><span class="s3">, </span><span class="s1">yname</span><span class="s3">, </span><span class="s1">yname_list</span><span class="s3">, </span><span class="s1">all=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        If all is False, the first variable name is dropped 
        &quot;&quot;&quot;</span>
        <span class="s1">model = self.model</span>
        <span class="s3">if </span><span class="s1">yname </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">yname = model.endog_names</span>
        <span class="s3">if </span><span class="s1">yname_list </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">ynames = model._ynames_map</span>
            <span class="s1">ynames = self._maybe_convert_ynames_int(ynames)</span>
            <span class="s4"># use range below to ensure sortedness</span>
            <span class="s1">ynames = [ynames[key] </span><span class="s3">for </span><span class="s1">key </span><span class="s3">in </span><span class="s1">range(int(model.J))]</span>
            <span class="s1">ynames = [</span><span class="s2">'='</span><span class="s1">.join([yname</span><span class="s3">, </span><span class="s1">name]) </span><span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">ynames]</span>
            <span class="s3">if not </span><span class="s1">all:</span>
                <span class="s1">yname_list = ynames[</span><span class="s5">1</span><span class="s1">:] </span><span class="s4"># assumes first variable is dropped</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">yname_list = ynames</span>
        <span class="s3">return </span><span class="s1">yname</span><span class="s3">, </span><span class="s1">yname_list</span>

    <span class="s3">def </span><span class="s1">pred_table(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the J x J prediction table. 
 
        Notes 
        ----- 
        pred_table[i,j] refers to the number of times &quot;i&quot; was observed and 
        the model predicted &quot;j&quot;. Correct predictions are along the diagonal. 
        &quot;&quot;&quot;</span>
        <span class="s1">ju = self.model.J - </span><span class="s5">1  </span><span class="s4"># highest index</span>
        <span class="s4"># these are the actual, predicted indices</span>
        <span class="s4">#idx = lzip(self.model.endog, self.predict().argmax(1))</span>
        <span class="s1">bins = np.concatenate(([</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">np.linspace(</span><span class="s5">0.5</span><span class="s3">, </span><span class="s1">ju - </span><span class="s5">0.5</span><span class="s3">, </span><span class="s1">ju)</span><span class="s3">, </span><span class="s1">[ju]))</span>
        <span class="s3">return </span><span class="s1">np.histogram2d(self.model.endog</span><span class="s3">, </span><span class="s1">self.predict().argmax(</span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                              <span class="s1">bins=bins)[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">bse(self):</span>
        <span class="s1">bse = np.sqrt(np.diag(self.cov_params()))</span>
        <span class="s3">return </span><span class="s1">bse.reshape(self.params.shape</span><span class="s3">, </span><span class="s1">order=</span><span class="s2">'F'</span><span class="s1">)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">aic(self):</span>
        <span class="s3">return </span><span class="s1">-</span><span class="s5">2</span><span class="s1">*(self.llf - (self.df_model+self.model.J-</span><span class="s5">1</span><span class="s1">))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">bic(self):</span>
        <span class="s3">return </span><span class="s1">-</span><span class="s5">2</span><span class="s1">*self.llf + np.log(self.nobs)*(self.df_model+self.model.J-</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">conf_int(self</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s5">.05</span><span class="s3">, </span><span class="s1">cols=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">confint = super(DiscreteResults</span><span class="s3">, </span><span class="s1">self).conf_int(alpha=alpha</span><span class="s3">,</span>
                                                            <span class="s1">cols=cols)</span>
        <span class="s3">return </span><span class="s1">confint.transpose(</span><span class="s5">2</span><span class="s3">,</span><span class="s5">0</span><span class="s3">,</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">get_prediction(self):</span>
        <span class="s0">&quot;&quot;&quot;Not implemented for Multinomial 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">def </span><span class="s1">margeff(self):</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s2">&quot;Use get_margeff instead&quot;</span><span class="s1">)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid_misclassified(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Residuals indicating which observations are misclassified. 
 
        Notes 
        ----- 
        The residuals for the multinomial model are defined as 
 
        .. math:: argmax(y_i) \\neq argmax(p_i) 
 
        where :math:`argmax(y_i)` is the index of the category for the 
        endogenous variable and :math:`argmax(p_i)` is the index of the 
        predicted probabilities for each category. That is, the residual 
        is a binary indicator that is 0 if the category with the highest 
        predicted probability is the same as that of the observed variable 
        and 1 otherwise. 
        &quot;&quot;&quot;</span>
        <span class="s4"># it's 0 or 1 - 0 for correct prediction and 1 for a missed one</span>
        <span class="s3">return </span><span class="s1">(self.model.wendog.argmax(</span><span class="s5">1</span><span class="s1">) !=</span>
                <span class="s1">self.predict().argmax(</span><span class="s5">1</span><span class="s1">)).astype(float)</span>

    <span class="s3">def </span><span class="s1">summary2(self</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s5">0.05</span><span class="s3">, </span><span class="s1">float_format=</span><span class="s2">&quot;%.4f&quot;</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Experimental function to summarize regression results 
 
        Parameters 
        ---------- 
        alpha : float 
            significance level for the confidence intervals 
        float_format : str 
            print format for floats in parameters summary 
 
        Returns 
        ------- 
        smry : Summary instance 
            this holds the summary tables and text, which can be printed or 
            converted to various output formats. 
 
        See Also 
        -------- 
        statsmodels.iolib.summary2.Summary : class to hold summary results 
        &quot;&quot;&quot;</span>

        <span class="s3">from </span><span class="s1">statsmodels.iolib </span><span class="s3">import </span><span class="s1">summary2</span>
        <span class="s1">smry = summary2.Summary()</span>
        <span class="s1">smry.add_dict(summary2.summary_model(self))</span>
        <span class="s4"># One data frame per value of endog</span>
        <span class="s1">eqn = self.params.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">confint = self.conf_int(alpha)</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(eqn):</span>
            <span class="s1">coefs = summary2.summary_params((self</span><span class="s3">, </span><span class="s1">self.params[:</span><span class="s3">, </span><span class="s1">i]</span><span class="s3">,</span>
                                             <span class="s1">self.bse[:</span><span class="s3">, </span><span class="s1">i]</span><span class="s3">,</span>
                                             <span class="s1">self.tvalues[:</span><span class="s3">, </span><span class="s1">i]</span><span class="s3">,</span>
                                             <span class="s1">self.pvalues[:</span><span class="s3">, </span><span class="s1">i]</span><span class="s3">,</span>
                                             <span class="s1">confint[i])</span><span class="s3">,</span>
                                            <span class="s1">alpha=alpha)</span>
            <span class="s4"># Header must show value of endog</span>
            <span class="s1">level_str =  self.model.endog_names + </span><span class="s2">' = ' </span><span class="s1">+ str(i)</span>
            <span class="s1">coefs[level_str] = coefs.index</span>
            <span class="s1">coefs = coefs.iloc[:</span><span class="s3">, </span><span class="s1">[-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">]]</span>
            <span class="s1">smry.add_df(coefs</span><span class="s3">, </span><span class="s1">index=</span><span class="s3">False, </span><span class="s1">header=</span><span class="s3">True,</span>
                        <span class="s1">float_format=float_format)</span>
            <span class="s1">smry.add_title(results=self)</span>
        <span class="s3">return </span><span class="s1">smry</span>


<span class="s3">class </span><span class="s1">L1MultinomialResults(MultinomialResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span><span class="s2">&quot;one_line_description&quot; </span><span class="s1">:</span>
        <span class="s2">&quot;A results class for multinomial data fit by l1 regularization&quot;</span><span class="s3">,</span>
        <span class="s2">&quot;extra_attr&quot; </span><span class="s1">: _l1_results_attr}</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">model</span><span class="s3">, </span><span class="s1">mlefit):</span>
        <span class="s1">super(L1MultinomialResults</span><span class="s3">, </span><span class="s1">self).__init__(model</span><span class="s3">, </span><span class="s1">mlefit)</span>
        <span class="s4"># self.trimmed is a boolean array with T/F telling whether or not that</span>
        <span class="s4"># entry in params has been set zero'd out.</span>
        <span class="s1">self.trimmed = mlefit.mle_retvals[</span><span class="s2">'trimmed'</span><span class="s1">]</span>
        <span class="s1">self.nnz_params = (~self.trimmed).sum()</span>

        <span class="s4"># Note: J-1 constants</span>
        <span class="s1">self.df_model = self.nnz_params - (self.model.J - </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">self.df_resid = float(self.model.endog.shape[</span><span class="s5">0</span><span class="s1">] - self.nnz_params)</span>


<span class="s4">#### Results Wrappers ####</span>

<span class="s3">class </span><span class="s1">OrderedResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s3">pass</span>


<span class="s1">wrap.populate_wrapper(OrderedResultsWrapper</span><span class="s3">, </span><span class="s1">OrderedResults)</span>


<span class="s3">class </span><span class="s1">CountResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s3">pass</span>


<span class="s1">wrap.populate_wrapper(CountResultsWrapper</span><span class="s3">, </span><span class="s1">CountResults)</span>


<span class="s3">class </span><span class="s1">NegativeBinomialResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s3">pass</span>


<span class="s1">wrap.populate_wrapper(NegativeBinomialResultsWrapper</span><span class="s3">,</span>
                      <span class="s1">NegativeBinomialResults)</span>


<span class="s3">class </span><span class="s1">NegativeBinomialPResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s3">pass</span>


<span class="s1">wrap.populate_wrapper(NegativeBinomialPResultsWrapper</span><span class="s3">,</span>
                      <span class="s1">NegativeBinomialPResults)</span>


<span class="s3">class </span><span class="s1">GeneralizedPoissonResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s3">pass</span>


<span class="s1">wrap.populate_wrapper(GeneralizedPoissonResultsWrapper</span><span class="s3">,</span>
                      <span class="s1">GeneralizedPoissonResults)</span>


<span class="s3">class </span><span class="s1">PoissonResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s3">pass</span>


<span class="s1">wrap.populate_wrapper(PoissonResultsWrapper</span><span class="s3">, </span><span class="s1">PoissonResults)</span>


<span class="s3">class </span><span class="s1">L1CountResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s3">pass</span>


<span class="s3">class </span><span class="s1">L1PoissonResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s3">pass</span>


<span class="s1">wrap.populate_wrapper(L1PoissonResultsWrapper</span><span class="s3">, </span><span class="s1">L1PoissonResults)</span>


<span class="s3">class </span><span class="s1">L1NegativeBinomialResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s3">pass</span>


<span class="s1">wrap.populate_wrapper(L1NegativeBinomialResultsWrapper</span><span class="s3">,</span>
                      <span class="s1">L1NegativeBinomialResults)</span>


<span class="s3">class </span><span class="s1">L1GeneralizedPoissonResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s3">pass</span>


<span class="s1">wrap.populate_wrapper(L1GeneralizedPoissonResultsWrapper</span><span class="s3">,</span>
                      <span class="s1">L1GeneralizedPoissonResults)</span>


<span class="s3">class </span><span class="s1">BinaryResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s1">_attrs = {</span><span class="s2">&quot;resid_dev&quot;</span><span class="s1">: </span><span class="s2">&quot;rows&quot;</span><span class="s3">,</span>
              <span class="s2">&quot;resid_generalized&quot;</span><span class="s1">: </span><span class="s2">&quot;rows&quot;</span><span class="s3">,</span>
              <span class="s2">&quot;resid_pearson&quot;</span><span class="s1">: </span><span class="s2">&quot;rows&quot;</span><span class="s3">,</span>
              <span class="s2">&quot;resid_response&quot;</span><span class="s1">: </span><span class="s2">&quot;rows&quot;</span>
              <span class="s1">}</span>
    <span class="s1">_wrap_attrs = wrap.union_dicts(lm.RegressionResultsWrapper._wrap_attrs</span><span class="s3">,</span>
                                   <span class="s1">_attrs)</span>


<span class="s1">wrap.populate_wrapper(BinaryResultsWrapper</span><span class="s3">, </span><span class="s1">BinaryResults)</span>


<span class="s3">class </span><span class="s1">L1BinaryResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s3">pass</span>


<span class="s1">wrap.populate_wrapper(L1BinaryResultsWrapper</span><span class="s3">, </span><span class="s1">L1BinaryResults)</span>


<span class="s3">class </span><span class="s1">MultinomialResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s1">_attrs = {</span><span class="s2">&quot;resid_misclassified&quot;</span><span class="s1">: </span><span class="s2">&quot;rows&quot;</span><span class="s1">}</span>
    <span class="s1">_wrap_attrs = wrap.union_dicts(lm.RegressionResultsWrapper._wrap_attrs</span><span class="s3">,</span>
                                   <span class="s1">_attrs)</span>
    <span class="s1">_methods = {</span><span class="s2">'conf_int'</span><span class="s1">: </span><span class="s2">'multivariate_confint'</span><span class="s1">}</span>
    <span class="s1">_wrap_methods = wrap.union_dicts(lm.RegressionResultsWrapper._wrap_methods</span><span class="s3">,</span>
                                     <span class="s1">_methods)</span>


<span class="s1">wrap.populate_wrapper(MultinomialResultsWrapper</span><span class="s3">, </span><span class="s1">MultinomialResults)</span>


<span class="s3">class </span><span class="s1">L1MultinomialResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s3">pass</span>


<span class="s1">wrap.populate_wrapper(L1MultinomialResultsWrapper</span><span class="s3">, </span><span class="s1">L1MultinomialResults)</span>
</pre>
</body>
</html>