<html>
<head>
<title>kernel_density.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
kernel_density.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Multivariate Conditional and Unconditional Kernel Density Estimation 
with Mixed Data Types. 
 
References 
---------- 
[1] Racine, J., Li, Q. Nonparametric econometrics: theory and practice. 
    Princeton University Press. (2007) 
[2] Racine, Jeff. &quot;Nonparametric Econometrics: A Primer,&quot; Foundation 
    and Trends in Econometrics: Vol 3: No 1, pp1-88. (2008) 
    http://dx.doi.org/10.1561/0800000009 
[3] Racine, J., Li, Q. &quot;Nonparametric Estimation of Distributions 
    with Categorical and Continuous Data.&quot; Working Paper. (2000) 
[4] Racine, J. Li, Q. &quot;Kernel Estimation of Multivariate Conditional 
    Distributions Annals of Economics and Finance 5, 211-235 (2004) 
[5] Liu, R., Yang, L. &quot;Kernel estimation of multivariate 
    cumulative distribution function.&quot; 
    Journal of Nonparametric Statistics (2008) 
[6] Li, R., Ju, G. &quot;Nonparametric Estimation of Multivariate CDF 
    with Categorical and Continuous Data.&quot; Working Paper 
[7] Li, Q., Racine, J. &quot;Cross-validated local linear nonparametric 
    regression&quot; Statistica Sinica 14(2004), pp. 485-512 
[8] Racine, J.: &quot;Consistent Significance Testing for Nonparametric 
        Regression&quot; Journal of Business &amp; Economics Statistics 
[9] Racine, J., Hart, J., Li, Q., &quot;Testing the Significance of 
        Categorical Predictor Variables in Nonparametric Regression 
        Models&quot;, 2006, Econometric Reviews 25, 523-544 
 
&quot;&quot;&quot;</span>
<span class="s2"># TODO: make default behavior efficient=True above a certain n_obs</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">. </span><span class="s3">import </span><span class="s1">kernels</span>
<span class="s3">from </span><span class="s1">._kernel_base </span><span class="s3">import </span><span class="s1">GenericKDE</span><span class="s3">, </span><span class="s1">EstimatorSettings</span><span class="s3">, </span><span class="s1">gpke</span><span class="s3">, </span><span class="s1">\</span>
    <span class="s1">LeaveOneOut</span><span class="s3">, </span><span class="s1">_adjust_shape</span>


<span class="s1">__all__ = [</span><span class="s4">'KDEMultivariate'</span><span class="s3">, </span><span class="s4">'KDEMultivariateConditional'</span><span class="s3">, </span><span class="s4">'EstimatorSettings'</span><span class="s1">]</span>


<span class="s3">class </span><span class="s1">KDEMultivariate(GenericKDE):</span>
    <span class="s0">&quot;&quot;&quot; 
    Multivariate kernel density estimator. 
 
    This density estimator can handle univariate as well as multivariate data, 
    including mixed continuous / ordered discrete / unordered discrete data. 
    It also provides cross-validated bandwidth selection methods (least 
    squares, maximum likelihood). 
 
    Parameters 
    ---------- 
    data : list of ndarrays or 2-D ndarray 
        The training data for the Kernel Density Estimation, used to determine 
        the bandwidth(s).  If a 2-D array, should be of shape 
        (num_observations, num_variables).  If a list, each list element is a 
        separate observation. 
    var_type : str 
        The type of the variables: 
 
            - c : continuous 
            - u : unordered (discrete) 
            - o : ordered (discrete) 
 
        The string should contain a type specifier for each variable, so for 
        example ``var_type='ccuo'``. 
    bw : array_like or str, optional 
        If an array, it is a fixed user-specified bandwidth.  If a string, 
        should be one of: 
 
            - normal_reference: normal reference rule of thumb (default) 
            - cv_ml: cross validation maximum likelihood 
            - cv_ls: cross validation least squares 
 
    defaults : EstimatorSettings instance, optional 
        The default values for (efficient) bandwidth estimation. 
 
    Attributes 
    ---------- 
    bw : array_like 
        The bandwidth parameters. 
 
    See Also 
    -------- 
    KDEMultivariateConditional 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import statsmodels.api as sm 
    &gt;&gt;&gt; nobs = 300 
    &gt;&gt;&gt; np.random.seed(1234)  # Seed random generator 
    &gt;&gt;&gt; c1 = np.random.normal(size=(nobs,1)) 
    &gt;&gt;&gt; c2 = np.random.normal(2, 1, size=(nobs,1)) 
 
    Estimate a bivariate distribution and display the bandwidth found: 
 
    &gt;&gt;&gt; dens_u = sm.nonparametric.KDEMultivariate(data=[c1,c2], 
    ...     var_type='cc', bw='normal_reference') 
    &gt;&gt;&gt; dens_u.bw 
    array([ 0.39967419,  0.38423292]) 
    &quot;&quot;&quot;</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">var_type</span><span class="s3">, </span><span class="s1">bw=</span><span class="s3">None, </span><span class="s1">defaults=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">self.var_type = var_type</span>
        <span class="s1">self.k_vars = len(self.var_type)</span>
        <span class="s1">self.data = _adjust_shape(data</span><span class="s3">, </span><span class="s1">self.k_vars)</span>
        <span class="s1">self.data_type = var_type</span>
        <span class="s1">self.nobs</span><span class="s3">, </span><span class="s1">self.k_vars = np.shape(self.data)</span>
        <span class="s3">if </span><span class="s1">self.nobs &lt;= self.k_vars:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;The number of observations must be larger &quot; </span><span class="s1">\</span>
                             <span class="s4">&quot;than the number of variables.&quot;</span><span class="s1">)</span>
        <span class="s1">defaults = EstimatorSettings() </span><span class="s3">if </span><span class="s1">defaults </span><span class="s3">is None else </span><span class="s1">defaults</span>
        <span class="s1">self._set_defaults(defaults)</span>
        <span class="s3">if not </span><span class="s1">self.efficient:</span>
            <span class="s1">self.bw = self._compute_bw(bw)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.bw = self._compute_efficient(bw)</span>

    <span class="s3">def </span><span class="s1">__repr__(self):</span>
        <span class="s0">&quot;&quot;&quot;Provide something sane to print.&quot;&quot;&quot;</span>
        <span class="s1">rpr = </span><span class="s4">&quot;KDE instance</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Number of variables: k_vars = &quot; </span><span class="s1">+ str(self.k_vars) + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Number of samples:   nobs = &quot; </span><span class="s1">+ str(self.nobs) + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Variable types:      &quot; </span><span class="s1">+ self.var_type + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;BW selection method: &quot; </span><span class="s1">+ self._bw_method + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s3">return </span><span class="s1">rpr</span>

    <span class="s3">def </span><span class="s1">loo_likelihood(self</span><span class="s3">, </span><span class="s1">bw</span><span class="s3">, </span><span class="s1">func=</span><span class="s3">lambda </span><span class="s1">x: x):</span>
        <span class="s0">r&quot;&quot;&quot; 
        Returns the leave-one-out likelihood function. 
 
        The leave-one-out likelihood function for the unconditional KDE. 
 
        Parameters 
        ---------- 
        bw : array_like 
            The value for the bandwidth parameter(s). 
        func : callable, optional 
            Function to transform the likelihood values (before summing); for 
            the log likelihood, use ``func=np.log``.  Default is ``f(x) = x``. 
 
        Notes 
        ----- 
        The leave-one-out kernel estimator of :math:`f_{-i}` is: 
 
        .. math:: f_{-i}(X_{i})=\frac{1}{(n-1)h} 
                    \sum_{j=1,j\neq i}K_{h}(X_{i},X_{j}) 
 
        where :math:`K_{h}` represents the generalized product kernel 
        estimator: 
 
        .. math:: K_{h}(X_{i},X_{j}) = 
            \prod_{s=1}^{q}h_{s}^{-1}k\left(\frac{X_{is}-X_{js}}{h_{s}}\right) 
        &quot;&quot;&quot;</span>
        <span class="s1">LOO = LeaveOneOut(self.data)</span>
        <span class="s1">L = </span><span class="s5">0</span>
        <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">X_not_i </span><span class="s3">in </span><span class="s1">enumerate(LOO):</span>
            <span class="s1">f_i = gpke(bw</span><span class="s3">, </span><span class="s1">data=-X_not_i</span><span class="s3">, </span><span class="s1">data_predict=-self.data[i</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                       <span class="s1">var_type=self.var_type)</span>
            <span class="s1">L += func(f_i)</span>

        <span class="s3">return </span><span class="s1">-L</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">data_predict=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">r&quot;&quot;&quot; 
        Evaluate the probability density function. 
 
        Parameters 
        ---------- 
        data_predict : array_like, optional 
            Points to evaluate at.  If unspecified, the training data is used. 
 
        Returns 
        ------- 
        pdf_est : array_like 
            Probability density function evaluated at `data_predict`. 
 
        Notes 
        ----- 
        The probability density is given by the generalized product kernel 
        estimator: 
 
        .. math:: K_{h}(X_{i},X_{j}) = 
            \prod_{s=1}^{q}h_{s}^{-1}k\left(\frac{X_{is}-X_{js}}{h_{s}}\right) 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">data_predict </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">data_predict = self.data</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">data_predict = _adjust_shape(data_predict</span><span class="s3">, </span><span class="s1">self.k_vars)</span>

        <span class="s1">pdf_est = []</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(np.shape(data_predict)[</span><span class="s5">0</span><span class="s1">]):</span>
            <span class="s1">pdf_est.append(gpke(self.bw</span><span class="s3">, </span><span class="s1">data=self.data</span><span class="s3">,</span>
                                <span class="s1">data_predict=data_predict[i</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                                <span class="s1">var_type=self.var_type) / self.nobs)</span>

        <span class="s1">pdf_est = np.squeeze(pdf_est)</span>
        <span class="s3">return </span><span class="s1">pdf_est</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">data_predict=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">r&quot;&quot;&quot; 
        Evaluate the cumulative distribution function. 
 
        Parameters 
        ---------- 
        data_predict : array_like, optional 
            Points to evaluate at.  If unspecified, the training data is used. 
 
        Returns 
        ------- 
        cdf_est : array_like 
            The estimate of the cdf. 
 
        Notes 
        ----- 
        See https://en.wikipedia.org/wiki/Cumulative_distribution_function 
        For more details on the estimation see Ref. [5] in module docstring. 
 
        The multivariate CDF for mixed data (continuous and ordered/unordered 
        discrete) is estimated by: 
 
        .. math:: 
 
            F(x^{c},x^{d})=n^{-1}\sum_{i=1}^{n}\left[G(\frac{x^{c}-X_{i}}{h})\sum_{u\leq x^{d}}L(X_{i}^{d},x_{i}^{d}, \lambda)\right] 
 
        where G() is the product kernel CDF estimator for the continuous 
        and L() for the discrete variables. 
 
        Used bandwidth is ``self.bw``. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">data_predict </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">data_predict = self.data</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">data_predict = _adjust_shape(data_predict</span><span class="s3">, </span><span class="s1">self.k_vars)</span>

        <span class="s1">cdf_est = []</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(np.shape(data_predict)[</span><span class="s5">0</span><span class="s1">]):</span>
            <span class="s1">cdf_est.append(gpke(self.bw</span><span class="s3">, </span><span class="s1">data=self.data</span><span class="s3">,</span>
                                <span class="s1">data_predict=data_predict[i</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                                <span class="s1">var_type=self.var_type</span><span class="s3">,</span>
                                <span class="s1">ckertype=</span><span class="s4">&quot;gaussian_cdf&quot;</span><span class="s3">,</span>
                                <span class="s1">ukertype=</span><span class="s4">&quot;aitchisonaitken_cdf&quot;</span><span class="s3">,</span>
                                <span class="s1">okertype=</span><span class="s4">'wangryzin_cdf'</span><span class="s1">) / self.nobs)</span>

        <span class="s1">cdf_est = np.squeeze(cdf_est)</span>
        <span class="s3">return </span><span class="s1">cdf_est</span>

    <span class="s3">def </span><span class="s1">imse(self</span><span class="s3">, </span><span class="s1">bw):</span>
        <span class="s0">r&quot;&quot;&quot; 
        Returns the Integrated Mean Square Error for the unconditional KDE. 
 
        Parameters 
        ---------- 
        bw : array_like 
            The bandwidth parameter(s). 
 
        Returns 
        ------- 
        CV : float 
            The cross-validation objective function. 
 
        Notes 
        ----- 
        See p. 27 in [1]_ for details on how to handle the multivariate 
        estimation with mixed data types see p.6 in [2]_. 
 
        The formula for the cross-validation objective function is: 
 
        .. math:: CV=\frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{N} 
            \bar{K}_{h}(X_{i},X_{j})-\frac{2}{n(n-1)}\sum_{i=1}^{n} 
            \sum_{j=1,j\neq i}^{N}K_{h}(X_{i},X_{j}) 
 
        Where :math:`\bar{K}_{h}` is the multivariate product convolution 
        kernel (consult [2]_ for mixed data types). 
 
        References 
        ---------- 
        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and 
                practice. Princeton University Press. (2007) 
        .. [2] Racine, J., Li, Q. &quot;Nonparametric Estimation of Distributions 
                with Categorical and Continuous Data.&quot; Working Paper. (2000) 
        &quot;&quot;&quot;</span>
        <span class="s2">#F = 0</span>
        <span class="s2">#for i in range(self.nobs):</span>
        <span class="s2">#    k_bar_sum = gpke(bw, data=-self.data,</span>
        <span class="s2">#                     data_predict=-self.data[i, :],</span>
        <span class="s2">#                     var_type=self.var_type,</span>
        <span class="s2">#                     ckertype='gauss_convolution',</span>
        <span class="s2">#                     okertype='wangryzin_convolution',</span>
        <span class="s2">#                     ukertype='aitchisonaitken_convolution')</span>
        <span class="s2">#    F += k_bar_sum</span>
        <span class="s2">## there is a + because loo_likelihood returns the negative</span>
        <span class="s2">#return (F / self.nobs**2 + self.loo_likelihood(bw) * \</span>
        <span class="s2">#        2 / ((self.nobs) * (self.nobs - 1)))</span>

        <span class="s2"># The code below is equivalent to the commented-out code above.  It's</span>
        <span class="s2"># about 20% faster due to some code being moved outside the for-loops</span>
        <span class="s2"># and shared by gpke() and loo_likelihood().</span>
        <span class="s1">F = </span><span class="s5">0</span>
        <span class="s1">kertypes = dict(c=kernels.gaussian_convolution</span><span class="s3">,</span>
                        <span class="s1">o=kernels.wang_ryzin_convolution</span><span class="s3">,</span>
                        <span class="s1">u=kernels.aitchison_aitken_convolution)</span>
        <span class="s1">nobs = self.nobs</span>
        <span class="s1">data = -self.data</span>
        <span class="s1">var_type = self.var_type</span>
        <span class="s1">ix_cont = np.array([c == </span><span class="s4">'c' </span><span class="s3">for </span><span class="s1">c </span><span class="s3">in </span><span class="s1">var_type])</span>
        <span class="s1">_bw_cont_product = bw[ix_cont].prod()</span>
        <span class="s1">Kval = np.empty(data.shape)</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(nobs):</span>
            <span class="s3">for </span><span class="s1">ii</span><span class="s3">, </span><span class="s1">vtype </span><span class="s3">in </span><span class="s1">enumerate(var_type):</span>
                <span class="s1">Kval[:</span><span class="s3">, </span><span class="s1">ii] = kertypes[vtype](bw[ii]</span><span class="s3">,</span>
                                              <span class="s1">data[:</span><span class="s3">, </span><span class="s1">ii]</span><span class="s3">,</span>
                                              <span class="s1">data[i</span><span class="s3">, </span><span class="s1">ii])</span>

            <span class="s1">dens = Kval.prod(axis=</span><span class="s5">1</span><span class="s1">) / _bw_cont_product</span>
            <span class="s1">k_bar_sum = dens.sum(axis=</span><span class="s5">0</span><span class="s1">)</span>
            <span class="s1">F += k_bar_sum  </span><span class="s2"># sum of prod kernel over nobs</span>

        <span class="s1">kertypes = dict(c=kernels.gaussian</span><span class="s3">,</span>
                        <span class="s1">o=kernels.wang_ryzin</span><span class="s3">,</span>
                        <span class="s1">u=kernels.aitchison_aitken)</span>
        <span class="s1">LOO = LeaveOneOut(self.data)</span>
        <span class="s1">L = </span><span class="s5">0   </span><span class="s2"># leave-one-out likelihood</span>
        <span class="s1">Kval = np.empty((data.shape[</span><span class="s5">0</span><span class="s1">]-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">data.shape[</span><span class="s5">1</span><span class="s1">]))</span>
        <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">X_not_i </span><span class="s3">in </span><span class="s1">enumerate(LOO):</span>
            <span class="s3">for </span><span class="s1">ii</span><span class="s3">, </span><span class="s1">vtype </span><span class="s3">in </span><span class="s1">enumerate(var_type):</span>
                <span class="s1">Kval[:</span><span class="s3">, </span><span class="s1">ii] = kertypes[vtype](bw[ii]</span><span class="s3">,</span>
                                              <span class="s1">-X_not_i[:</span><span class="s3">, </span><span class="s1">ii]</span><span class="s3">,</span>
                                              <span class="s1">data[i</span><span class="s3">, </span><span class="s1">ii])</span>
            <span class="s1">dens = Kval.prod(axis=</span><span class="s5">1</span><span class="s1">) / _bw_cont_product</span>
            <span class="s1">L += dens.sum(axis=</span><span class="s5">0</span><span class="s1">)</span>

        <span class="s2"># CV objective function, eq. (2.4) of Ref. [3]</span>
        <span class="s3">return </span><span class="s1">(F / nobs**</span><span class="s5">2 </span><span class="s1">- </span><span class="s5">2 </span><span class="s1">* L / (nobs * (nobs - </span><span class="s5">1</span><span class="s1">)))</span>

    <span class="s3">def </span><span class="s1">_get_class_vars_type(self):</span>
        <span class="s0">&quot;&quot;&quot;Helper method to be able to pass needed vars to _compute_subset.&quot;&quot;&quot;</span>
        <span class="s1">class_type = </span><span class="s4">'KDEMultivariate'</span>
        <span class="s1">class_vars = (self.var_type</span><span class="s3">, </span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">class_type</span><span class="s3">, </span><span class="s1">class_vars</span>


<span class="s3">class </span><span class="s1">KDEMultivariateConditional(GenericKDE):</span>
    <span class="s0">&quot;&quot;&quot; 
    Conditional multivariate kernel density estimator. 
 
    Calculates ``P(Y_1,Y_2,...Y_n | X_1,X_2...X_m) = 
    P(X_1, X_2,...X_n, Y_1, Y_2,..., Y_m)/P(X_1, X_2,..., X_m)``. 
    The conditional density is by definition the ratio of the two densities, 
    see [1]_. 
 
    Parameters 
    ---------- 
    endog : list of ndarrays or 2-D ndarray 
        The training data for the dependent variables, used to determine 
        the bandwidth(s).  If a 2-D array, should be of shape 
        (num_observations, num_variables).  If a list, each list element is a 
        separate observation. 
    exog : list of ndarrays or 2-D ndarray 
        The training data for the independent variable; same shape as `endog`. 
    dep_type : str 
        The type of the dependent variables: 
 
            c : Continuous 
            u : Unordered (Discrete) 
            o : Ordered (Discrete) 
 
        The string should contain a type specifier for each variable, so for 
        example ``dep_type='ccuo'``. 
    indep_type : str 
        The type of the independent variables; specified like `dep_type`. 
    bw : array_like or str, optional 
        If an array, it is a fixed user-specified bandwidth.  If a string, 
        should be one of: 
 
            - normal_reference: normal reference rule of thumb (default) 
            - cv_ml: cross validation maximum likelihood 
            - cv_ls: cross validation least squares 
 
    defaults : Instance of class EstimatorSettings 
        The default values for the efficient bandwidth estimation 
 
    Attributes 
    ---------- 
    bw : array_like 
        The bandwidth parameters 
 
    See Also 
    -------- 
    KDEMultivariate 
 
    References 
    ---------- 
    .. [1] https://en.wikipedia.org/wiki/Conditional_probability_distribution 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import statsmodels.api as sm 
    &gt;&gt;&gt; nobs = 300 
    &gt;&gt;&gt; c1 = np.random.normal(size=(nobs,1)) 
    &gt;&gt;&gt; c2 = np.random.normal(2,1,size=(nobs,1)) 
 
    &gt;&gt;&gt; dens_c = sm.nonparametric.KDEMultivariateConditional(endog=[c1], 
    ...     exog=[c2], dep_type='c', indep_type='c', bw='normal_reference') 
    &gt;&gt;&gt; dens_c.bw   # show computed bandwidth 
    array([ 0.41223484,  0.40976931]) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">dep_type</span><span class="s3">, </span><span class="s1">indep_type</span><span class="s3">, </span><span class="s1">bw</span><span class="s3">,</span>
                 <span class="s1">defaults=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">self.dep_type = dep_type</span>
        <span class="s1">self.indep_type = indep_type</span>
        <span class="s1">self.data_type = dep_type + indep_type</span>
        <span class="s1">self.k_dep = len(self.dep_type)</span>
        <span class="s1">self.k_indep = len(self.indep_type)</span>
        <span class="s1">self.endog = _adjust_shape(endog</span><span class="s3">, </span><span class="s1">self.k_dep)</span>
        <span class="s1">self.exog = _adjust_shape(exog</span><span class="s3">, </span><span class="s1">self.k_indep)</span>
        <span class="s1">self.nobs</span><span class="s3">, </span><span class="s1">self.k_dep = np.shape(self.endog)</span>
        <span class="s1">self.data = np.column_stack((self.endog</span><span class="s3">, </span><span class="s1">self.exog))</span>
        <span class="s1">self.k_vars = np.shape(self.data)[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">defaults = EstimatorSettings() </span><span class="s3">if </span><span class="s1">defaults </span><span class="s3">is None else </span><span class="s1">defaults</span>
        <span class="s1">self._set_defaults(defaults)</span>
        <span class="s3">if not </span><span class="s1">self.efficient:</span>
            <span class="s1">self.bw = self._compute_bw(bw)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.bw = self._compute_efficient(bw)</span>

    <span class="s3">def </span><span class="s1">__repr__(self):</span>
        <span class="s0">&quot;&quot;&quot;Provide something sane to print.&quot;&quot;&quot;</span>
        <span class="s1">rpr = </span><span class="s4">&quot;KDEMultivariateConditional instance</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Number of independent variables: k_indep = &quot; </span><span class="s1">+ \</span>
               <span class="s1">str(self.k_indep) + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Number of dependent variables: k_dep = &quot; </span><span class="s1">+ \</span>
               <span class="s1">str(self.k_dep) + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Number of observations: nobs = &quot; </span><span class="s1">+ str(self.nobs) + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Independent variable types:      &quot; </span><span class="s1">+ self.indep_type + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Dependent variable types:      &quot; </span><span class="s1">+ self.dep_type + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;BW selection method: &quot; </span><span class="s1">+ self._bw_method + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s3">return </span><span class="s1">rpr</span>

    <span class="s3">def </span><span class="s1">loo_likelihood(self</span><span class="s3">, </span><span class="s1">bw</span><span class="s3">, </span><span class="s1">func=</span><span class="s3">lambda </span><span class="s1">x: x):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the leave-one-out conditional likelihood of the data. 
 
        If `func` is not equal to the default, what's calculated is a function 
        of the leave-one-out conditional likelihood. 
 
        Parameters 
        ---------- 
        bw : array_like 
            The bandwidth parameter(s). 
        func : callable, optional 
            Function to transform the likelihood values (before summing); for 
            the log likelihood, use ``func=np.log``.  Default is ``f(x) = x``. 
 
        Returns 
        ------- 
        L : float 
            The value of the leave-one-out function for the data. 
 
        Notes 
        ----- 
        Similar to ``KDE.loo_likelihood`, but substitute ``f(y|x)=f(x,y)/f(x)`` 
        for ``f(x)``. 
        &quot;&quot;&quot;</span>
        <span class="s1">yLOO = LeaveOneOut(self.data)</span>
        <span class="s1">xLOO = LeaveOneOut(self.exog).__iter__()</span>
        <span class="s1">L = </span><span class="s5">0</span>
        <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">Y_j </span><span class="s3">in </span><span class="s1">enumerate(yLOO):</span>
            <span class="s1">X_not_i = next(xLOO)</span>
            <span class="s1">f_yx = gpke(bw</span><span class="s3">, </span><span class="s1">data=-Y_j</span><span class="s3">, </span><span class="s1">data_predict=-self.data[i</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                        <span class="s1">var_type=(self.dep_type + self.indep_type))</span>
            <span class="s1">f_x = gpke(bw[self.k_dep:]</span><span class="s3">, </span><span class="s1">data=-X_not_i</span><span class="s3">,</span>
                       <span class="s1">data_predict=-self.exog[i</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                       <span class="s1">var_type=self.indep_type)</span>
            <span class="s1">f_i = f_yx / f_x</span>
            <span class="s1">L += func(f_i)</span>

        <span class="s3">return </span><span class="s1">-L</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">endog_predict=</span><span class="s3">None, </span><span class="s1">exog_predict=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">r&quot;&quot;&quot; 
        Evaluate the probability density function. 
 
        Parameters 
        ---------- 
        endog_predict : array_like, optional 
            Evaluation data for the dependent variables.  If unspecified, the 
            training data is used. 
        exog_predict : array_like, optional 
            Evaluation data for the independent variables. 
 
        Returns 
        ------- 
        pdf : array_like 
            The value of the probability density at `endog_predict` and `exog_predict`. 
 
        Notes 
        ----- 
        The formula for the conditional probability density is: 
 
        .. math:: f(y|x)=\frac{f(x,y)}{f(x)} 
 
        with 
 
        .. math:: f(x)=\prod_{s=1}^{q}h_{s}^{-1}k 
                            \left(\frac{x_{is}-x_{js}}{h_{s}}\right) 
 
        where :math:`k` is the appropriate kernel for each variable. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">endog_predict </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">endog_predict = self.endog</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">endog_predict = _adjust_shape(endog_predict</span><span class="s3">, </span><span class="s1">self.k_dep)</span>
        <span class="s3">if </span><span class="s1">exog_predict </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog_predict = self.exog</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">exog_predict = _adjust_shape(exog_predict</span><span class="s3">, </span><span class="s1">self.k_indep)</span>

        <span class="s1">pdf_est = []</span>
        <span class="s1">data_predict = np.column_stack((endog_predict</span><span class="s3">, </span><span class="s1">exog_predict))</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(np.shape(data_predict)[</span><span class="s5">0</span><span class="s1">]):</span>
            <span class="s1">f_yx = gpke(self.bw</span><span class="s3">, </span><span class="s1">data=self.data</span><span class="s3">,</span>
                        <span class="s1">data_predict=data_predict[i</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                        <span class="s1">var_type=(self.dep_type + self.indep_type))</span>
            <span class="s1">f_x = gpke(self.bw[self.k_dep:]</span><span class="s3">, </span><span class="s1">data=self.exog</span><span class="s3">,</span>
                       <span class="s1">data_predict=exog_predict[i</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                       <span class="s1">var_type=self.indep_type)</span>
            <span class="s1">pdf_est.append(f_yx / f_x)</span>

        <span class="s3">return </span><span class="s1">np.squeeze(pdf_est)</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">endog_predict=</span><span class="s3">None, </span><span class="s1">exog_predict=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">r&quot;&quot;&quot; 
        Cumulative distribution function for the conditional density. 
 
        Parameters 
        ---------- 
        endog_predict : array_like, optional 
            The evaluation dependent variables at which the cdf is estimated. 
            If not specified the training dependent variables are used. 
        exog_predict : array_like, optional 
            The evaluation independent variables at which the cdf is estimated. 
            If not specified the training independent variables are used. 
 
        Returns 
        ------- 
        cdf_est : array_like 
            The estimate of the cdf. 
 
        Notes 
        ----- 
        For more details on the estimation see [2]_, and p.181 in [1]_. 
 
        The multivariate conditional CDF for mixed data (continuous and 
        ordered/unordered discrete) is estimated by: 
 
        .. math:: 
 
            F(y|x)=\frac{n^{-1}\sum_{i=1}^{n}G(\frac{y-Y_{i}}{h_{0}}) W_{h}(X_{i},x)}{\widehat{\mu}(x)} 
 
        where G() is the product kernel CDF estimator for the dependent (y) 
        variable(s) and W() is the product kernel CDF estimator for the 
        independent variable(s). 
 
        References 
        ---------- 
        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and 
                practice. Princeton University Press. (2007) 
        .. [2] Liu, R., Yang, L. &quot;Kernel estimation of multivariate cumulative 
                    distribution function.&quot; Journal of Nonparametric 
                    Statistics (2008) 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">endog_predict </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">endog_predict = self.endog</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">endog_predict = _adjust_shape(endog_predict</span><span class="s3">, </span><span class="s1">self.k_dep)</span>
        <span class="s3">if </span><span class="s1">exog_predict </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog_predict = self.exog</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">exog_predict = _adjust_shape(exog_predict</span><span class="s3">, </span><span class="s1">self.k_indep)</span>

        <span class="s1">N_data_predict = np.shape(exog_predict)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">cdf_est = np.empty(N_data_predict)</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(N_data_predict):</span>
            <span class="s1">mu_x = gpke(self.bw[self.k_dep:]</span><span class="s3">, </span><span class="s1">data=self.exog</span><span class="s3">,</span>
                        <span class="s1">data_predict=exog_predict[i</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                        <span class="s1">var_type=self.indep_type) / self.nobs</span>
            <span class="s1">mu_x = np.squeeze(mu_x)</span>
            <span class="s1">cdf_endog = gpke(self.bw[</span><span class="s5">0</span><span class="s1">:self.k_dep]</span><span class="s3">, </span><span class="s1">data=self.endog</span><span class="s3">,</span>
                             <span class="s1">data_predict=endog_predict[i</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                             <span class="s1">var_type=self.dep_type</span><span class="s3">,</span>
                             <span class="s1">ckertype=</span><span class="s4">&quot;gaussian_cdf&quot;</span><span class="s3">,</span>
                             <span class="s1">ukertype=</span><span class="s4">&quot;aitchisonaitken_cdf&quot;</span><span class="s3">,</span>
                             <span class="s1">okertype=</span><span class="s4">'wangryzin_cdf'</span><span class="s3">, </span><span class="s1">tosum=</span><span class="s3">False</span><span class="s1">)</span>

            <span class="s1">cdf_exog = gpke(self.bw[self.k_dep:]</span><span class="s3">, </span><span class="s1">data=self.exog</span><span class="s3">,</span>
                            <span class="s1">data_predict=exog_predict[i</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                            <span class="s1">var_type=self.indep_type</span><span class="s3">, </span><span class="s1">tosum=</span><span class="s3">False</span><span class="s1">)</span>
            <span class="s1">S = (cdf_endog * cdf_exog).sum(axis=</span><span class="s5">0</span><span class="s1">)</span>
            <span class="s1">cdf_est[i] = S / (self.nobs * mu_x)</span>

        <span class="s3">return </span><span class="s1">cdf_est</span>

    <span class="s3">def </span><span class="s1">imse(self</span><span class="s3">, </span><span class="s1">bw):</span>
        <span class="s0">r&quot;&quot;&quot; 
        The integrated mean square error for the conditional KDE. 
 
        Parameters 
        ---------- 
        bw : array_like 
            The bandwidth parameter(s). 
 
        Returns 
        ------- 
        CV : float 
            The cross-validation objective function. 
 
        Notes 
        ----- 
        For more details see pp. 156-166 in [1]_. For details on how to 
        handle the mixed variable types see [2]_. 
 
        The formula for the cross-validation objective function for mixed 
        variable types is: 
 
        .. math:: CV(h,\lambda)=\frac{1}{n}\sum_{l=1}^{n} 
            \frac{G_{-l}(X_{l})}{\left[\mu_{-l}(X_{l})\right]^{2}}- 
            \frac{2}{n}\sum_{l=1}^{n}\frac{f_{-l}(X_{l},Y_{l})}{\mu_{-l}(X_{l})} 
 
        where 
 
        .. math:: G_{-l}(X_{l}) = n^{-2}\sum_{i\neq l}\sum_{j\neq l} 
                        K_{X_{i},X_{l}} K_{X_{j},X_{l}}K_{Y_{i},Y_{j}}^{(2)} 
 
        where :math:`K_{X_{i},X_{l}}` is the multivariate product kernel and 
        :math:`\mu_{-l}(X_{l})` is the leave-one-out estimator of the pdf. 
 
        :math:`K_{Y_{i},Y_{j}}^{(2)}` is the convolution kernel. 
 
        The value of the function is minimized by the ``_cv_ls`` method of the 
        `GenericKDE` class to return the bw estimates that minimize the 
        distance between the estimated and &quot;true&quot; probability density. 
 
        References 
        ---------- 
        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and 
                practice. Princeton University Press. (2007) 
        .. [2] Racine, J., Li, Q. &quot;Nonparametric Estimation of Distributions 
                with Categorical and Continuous Data.&quot; Working Paper. (2000) 
        &quot;&quot;&quot;</span>
        <span class="s1">zLOO = LeaveOneOut(self.data)</span>
        <span class="s1">CV = </span><span class="s5">0</span>
        <span class="s1">nobs = float(self.nobs)</span>
        <span class="s1">expander = np.ones((self.nobs - </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s3">for </span><span class="s1">ii</span><span class="s3">, </span><span class="s1">Z </span><span class="s3">in </span><span class="s1">enumerate(zLOO):</span>
            <span class="s1">X = Z[:</span><span class="s3">, </span><span class="s1">self.k_dep:]</span>
            <span class="s1">Y = Z[:</span><span class="s3">, </span><span class="s1">:self.k_dep]</span>
            <span class="s1">Ye_L = np.kron(Y</span><span class="s3">, </span><span class="s1">expander)</span>
            <span class="s1">Ye_R = np.kron(expander</span><span class="s3">, </span><span class="s1">Y)</span>
            <span class="s1">Xe_L = np.kron(X</span><span class="s3">, </span><span class="s1">expander)</span>
            <span class="s1">Xe_R = np.kron(expander</span><span class="s3">, </span><span class="s1">X)</span>
            <span class="s1">K_Xi_Xl = gpke(bw[self.k_dep:]</span><span class="s3">, </span><span class="s1">data=Xe_L</span><span class="s3">,</span>
                           <span class="s1">data_predict=self.exog[ii</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                           <span class="s1">var_type=self.indep_type</span><span class="s3">, </span><span class="s1">tosum=</span><span class="s3">False</span><span class="s1">)</span>
            <span class="s1">K_Xj_Xl = gpke(bw[self.k_dep:]</span><span class="s3">, </span><span class="s1">data=Xe_R</span><span class="s3">,</span>
                           <span class="s1">data_predict=self.exog[ii</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                           <span class="s1">var_type=self.indep_type</span><span class="s3">, </span><span class="s1">tosum=</span><span class="s3">False</span><span class="s1">)</span>
            <span class="s1">K2_Yi_Yj = gpke(bw[</span><span class="s5">0</span><span class="s1">:self.k_dep]</span><span class="s3">, </span><span class="s1">data=Ye_L</span><span class="s3">,</span>
                            <span class="s1">data_predict=Ye_R</span><span class="s3">, </span><span class="s1">var_type=self.dep_type</span><span class="s3">,</span>
                            <span class="s1">ckertype=</span><span class="s4">'gauss_convolution'</span><span class="s3">,</span>
                            <span class="s1">okertype=</span><span class="s4">'wangryzin_convolution'</span><span class="s3">,</span>
                            <span class="s1">ukertype=</span><span class="s4">'aitchisonaitken_convolution'</span><span class="s3">,</span>
                            <span class="s1">tosum=</span><span class="s3">False</span><span class="s1">)</span>
            <span class="s1">G = (K_Xi_Xl * K_Xj_Xl * K2_Yi_Yj).sum() / nobs**</span><span class="s5">2</span>
            <span class="s1">f_X_Y = gpke(bw</span><span class="s3">, </span><span class="s1">data=-Z</span><span class="s3">, </span><span class="s1">data_predict=-self.data[ii</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                         <span class="s1">var_type=(self.dep_type + self.indep_type)) / nobs</span>
            <span class="s1">m_x = gpke(bw[self.k_dep:]</span><span class="s3">, </span><span class="s1">data=-X</span><span class="s3">,</span>
                       <span class="s1">data_predict=-self.exog[ii</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                       <span class="s1">var_type=self.indep_type) / nobs</span>
            <span class="s1">CV += (G / m_x ** </span><span class="s5">2</span><span class="s1">) - </span><span class="s5">2 </span><span class="s1">* (f_X_Y / m_x)</span>

        <span class="s3">return </span><span class="s1">CV / nobs</span>

    <span class="s3">def </span><span class="s1">_get_class_vars_type(self):</span>
        <span class="s0">&quot;&quot;&quot;Helper method to be able to pass needed vars to _compute_subset.&quot;&quot;&quot;</span>
        <span class="s1">class_type = </span><span class="s4">'KDEMultivariateConditional'</span>
        <span class="s1">class_vars = (self.k_dep</span><span class="s3">, </span><span class="s1">self.dep_type</span><span class="s3">, </span><span class="s1">self.indep_type)</span>
        <span class="s3">return </span><span class="s1">class_type</span><span class="s3">, </span><span class="s1">class_vars</span>
</pre>
</body>
</html>