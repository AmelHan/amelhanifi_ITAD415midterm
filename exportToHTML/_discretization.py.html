<html>
<head>
<title>_discretization.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_discretization.py</font>
</center></td></tr></table>
<pre><span class="s0"># Author: Henry Lin &lt;hlin117@gmail.com&gt;</span>
<span class="s0">#         Tom Dupr√© la Tour</span>

<span class="s0"># License: BSD</span>


<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">numbers </span><span class="s2">import </span><span class="s1">Integral</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>

<span class="s2">from </span><span class="s1">..base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">TransformerMixin</span><span class="s2">, </span><span class="s1">_fit_context</span>
<span class="s2">from </span><span class="s1">..utils </span><span class="s2">import </span><span class="s1">_safe_indexing</span>
<span class="s2">from </span><span class="s1">..utils._param_validation </span><span class="s2">import </span><span class="s1">Hidden</span><span class="s2">, </span><span class="s1">Interval</span><span class="s2">, </span><span class="s1">Options</span><span class="s2">, </span><span class="s1">StrOptions</span>
<span class="s2">from </span><span class="s1">..utils.stats </span><span class="s2">import </span><span class="s1">_weighted_percentile</span>
<span class="s2">from </span><span class="s1">..utils.validation </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">_check_feature_names_in</span><span class="s2">,</span>
    <span class="s1">_check_sample_weight</span><span class="s2">,</span>
    <span class="s1">check_array</span><span class="s2">,</span>
    <span class="s1">check_is_fitted</span><span class="s2">,</span>
    <span class="s1">check_random_state</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">._encoders </span><span class="s2">import </span><span class="s1">OneHotEncoder</span>


<span class="s2">class </span><span class="s1">KBinsDiscretizer(TransformerMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
    <span class="s3">&quot;&quot;&quot; 
    Bin continuous data into intervals. 
 
    Read more in the :ref:`User Guide &lt;preprocessing_discretization&gt;`. 
 
    .. versionadded:: 0.20 
 
    Parameters 
    ---------- 
    n_bins : int or array-like of shape (n_features,), default=5 
        The number of bins to produce. Raises ValueError if ``n_bins &lt; 2``. 
 
    encode : {'onehot', 'onehot-dense', 'ordinal'}, default='onehot' 
        Method used to encode the transformed result. 
 
        - 'onehot': Encode the transformed result with one-hot encoding 
          and return a sparse matrix. Ignored features are always 
          stacked to the right. 
        - 'onehot-dense': Encode the transformed result with one-hot encoding 
          and return a dense array. Ignored features are always 
          stacked to the right. 
        - 'ordinal': Return the bin identifier encoded as an integer value. 
 
    strategy : {'uniform', 'quantile', 'kmeans'}, default='quantile' 
        Strategy used to define the widths of the bins. 
 
        - 'uniform': All bins in each feature have identical widths. 
        - 'quantile': All bins in each feature have the same number of points. 
        - 'kmeans': Values in each bin have the same nearest center of a 1D 
          k-means cluster. 
 
        For an example of the different strategies see: 
        :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_strategies.py`. 
 
    dtype : {np.float32, np.float64}, default=None 
        The desired data-type for the output. If None, output dtype is 
        consistent with input dtype. Only np.float32 and np.float64 are 
        supported. 
 
        .. versionadded:: 0.24 
 
    subsample : int or None, default='warn' 
        Maximum number of samples, used to fit the model, for computational 
        efficiency. Defaults to 200_000 when `strategy='quantile'` and to `None` 
        when `strategy='uniform'` or `strategy='kmeans'`. 
        `subsample=None` means that all the training samples are used when 
        computing the quantiles that determine the binning thresholds. 
        Since quantile computation relies on sorting each column of `X` and 
        that sorting has an `n log(n)` time complexity, 
        it is recommended to use subsampling on datasets with a 
        very large number of samples. 
 
        .. versionchanged:: 1.3 
            The default value of `subsample` changed from `None` to `200_000` when 
            `strategy=&quot;quantile&quot;`. 
 
        .. versionchanged:: 1.5 
            The default value of `subsample` changed from `None` to `200_000` when 
            `strategy=&quot;uniform&quot;` or `strategy=&quot;kmeans&quot;`. 
 
    random_state : int, RandomState instance or None, default=None 
        Determines random number generation for subsampling. 
        Pass an int for reproducible results across multiple function calls. 
        See the `subsample` parameter for more details. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
        .. versionadded:: 1.1 
 
    Attributes 
    ---------- 
    bin_edges_ : ndarray of ndarray of shape (n_features,) 
        The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )`` 
        Ignored features will have empty arrays. 
 
    n_bins_ : ndarray of shape (n_features,), dtype=np.int_ 
        Number of bins per feature. Bins whose width are too small 
        (i.e., &lt;= 1e-8) are removed with a warning. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    Binarizer : Class used to bin values as ``0`` or 
        ``1`` based on a parameter ``threshold``. 
 
    Notes 
    ----- 
 
    For a visualization of discretization on different datasets refer to 
    :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_classification.py`. 
    On the effect of discretization on linear models see: 
    :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization.py`. 
 
    In bin edges for feature ``i``, the first and last values are used only for 
    ``inverse_transform``. During transform, bin edges are extended to:: 
 
      np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf]) 
 
    You can combine ``KBinsDiscretizer`` with 
    :class:`~sklearn.compose.ColumnTransformer` if you only want to preprocess 
    part of the features. 
 
    ``KBinsDiscretizer`` might produce constant features (e.g., when 
    ``encode = 'onehot'`` and certain bins do not contain any data). 
    These features can be removed with feature selection algorithms 
    (e.g., :class:`~sklearn.feature_selection.VarianceThreshold`). 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.preprocessing import KBinsDiscretizer 
    &gt;&gt;&gt; X = [[-2, 1, -4,   -1], 
    ...      [-1, 2, -3, -0.5], 
    ...      [ 0, 3, -2,  0.5], 
    ...      [ 1, 4, -1,    2]] 
    &gt;&gt;&gt; est = KBinsDiscretizer( 
    ...     n_bins=3, encode='ordinal', strategy='uniform', subsample=None 
    ... ) 
    &gt;&gt;&gt; est.fit(X) 
    KBinsDiscretizer(...) 
    &gt;&gt;&gt; Xt = est.transform(X) 
    &gt;&gt;&gt; Xt  # doctest: +SKIP 
    array([[ 0., 0., 0., 0.], 
           [ 1., 1., 1., 0.], 
           [ 2., 2., 2., 1.], 
           [ 2., 2., 2., 2.]]) 
 
    Sometimes it may be useful to convert the data back into the original 
    feature space. The ``inverse_transform`` function converts the binned 
    data into the original feature space. Each value will be equal to the mean 
    of the two bin edges. 
 
    &gt;&gt;&gt; est.bin_edges_[0] 
    array([-2., -1.,  0.,  1.]) 
    &gt;&gt;&gt; est.inverse_transform(Xt) 
    array([[-1.5,  1.5, -3.5, -0.5], 
           [-0.5,  2.5, -2.5, -0.5], 
           [ 0.5,  3.5, -1.5,  0.5], 
           [ 0.5,  3.5, -1.5,  1.5]]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;n_bins&quot;</span><span class="s1">: [Interval(Integral</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s4">&quot;array-like&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s4">&quot;encode&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;onehot&quot;</span><span class="s2">, </span><span class="s4">&quot;onehot-dense&quot;</span><span class="s2">, </span><span class="s4">&quot;ordinal&quot;</span><span class="s1">})]</span><span class="s2">,</span>
        <span class="s4">&quot;strategy&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;uniform&quot;</span><span class="s2">, </span><span class="s4">&quot;quantile&quot;</span><span class="s2">, </span><span class="s4">&quot;kmeans&quot;</span><span class="s1">})]</span><span class="s2">,</span>
        <span class="s4">&quot;dtype&quot;</span><span class="s1">: [Options(type</span><span class="s2">, </span><span class="s1">{np.float64</span><span class="s2">, </span><span class="s1">np.float32})</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s4">&quot;subsample&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Integral</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s2">None,</span>
            <span class="s1">Hidden(StrOptions({</span><span class="s4">&quot;warn&quot;</span><span class="s1">}))</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">n_bins=</span><span class="s5">5</span><span class="s2">,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">encode=</span><span class="s4">&quot;onehot&quot;</span><span class="s2">,</span>
        <span class="s1">strategy=</span><span class="s4">&quot;quantile&quot;</span><span class="s2">,</span>
        <span class="s1">dtype=</span><span class="s2">None,</span>
        <span class="s1">subsample=</span><span class="s4">&quot;warn&quot;</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s2">None,</span>
    <span class="s1">):</span>
        <span class="s1">self.n_bins = n_bins</span>
        <span class="s1">self.encode = encode</span>
        <span class="s1">self.strategy = strategy</span>
        <span class="s1">self.dtype = dtype</span>
        <span class="s1">self.subsample = subsample</span>
        <span class="s1">self.random_state = random_state</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s3">&quot;&quot;&quot; 
        Fit the estimator. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Data to be discretized. 
 
        y : None 
            Ignored. This parameter exists only for compatibility with 
            :class:`~sklearn.pipeline.Pipeline`. 
 
        sample_weight : ndarray of shape (n_samples,) 
            Contains weight values to be associated with each sample. 
            Only possible when `strategy` is set to `&quot;quantile&quot;`. 
 
            .. versionadded:: 1.3 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X</span><span class="s2">, </span><span class="s1">dtype=</span><span class="s4">&quot;numeric&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">self.dtype </span><span class="s2">in </span><span class="s1">(np.float64</span><span class="s2">, </span><span class="s1">np.float32):</span>
            <span class="s1">output_dtype = self.dtype</span>
        <span class="s2">else</span><span class="s1">:  </span><span class="s0"># self.dtype is None</span>
            <span class="s1">output_dtype = X.dtype</span>

        <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X.shape</span>

        <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is not None and </span><span class="s1">self.strategy == </span><span class="s4">&quot;uniform&quot;</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;`sample_weight` was provided but it cannot be &quot;</span>
                <span class="s4">&quot;used with strategy='uniform'. Got strategy=&quot;</span>
                <span class="s4">f&quot;</span><span class="s2">{</span><span class="s1">self.strategy</span><span class="s2">!r} </span><span class="s4">instead.&quot;</span>
            <span class="s1">)</span>

        <span class="s2">if </span><span class="s1">self.strategy </span><span class="s2">in </span><span class="s1">(</span><span class="s4">&quot;uniform&quot;</span><span class="s2">, </span><span class="s4">&quot;kmeans&quot;</span><span class="s1">) </span><span class="s2">and </span><span class="s1">self.subsample == </span><span class="s4">&quot;warn&quot;</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s4">&quot;In version 1.5 onwards, subsample=200_000 &quot;</span>
                    <span class="s4">&quot;will be used by default. Set subsample explicitly to &quot;</span>
                    <span class="s4">&quot;silence this warning in the mean time. Set &quot;</span>
                    <span class="s4">&quot;subsample=None to disable subsampling explicitly.&quot;</span>
                <span class="s1">)</span><span class="s2">,</span>
                <span class="s1">FutureWarning</span><span class="s2">,</span>
            <span class="s1">)</span>

        <span class="s1">subsample = self.subsample</span>
        <span class="s2">if </span><span class="s1">subsample == </span><span class="s4">&quot;warn&quot;</span><span class="s1">:</span>
            <span class="s1">subsample = </span><span class="s5">200000 </span><span class="s2">if </span><span class="s1">self.strategy == </span><span class="s4">&quot;quantile&quot; </span><span class="s2">else None</span>
        <span class="s2">if </span><span class="s1">subsample </span><span class="s2">is not None and </span><span class="s1">n_samples &gt; subsample:</span>
            <span class="s1">rng = check_random_state(self.random_state)</span>
            <span class="s1">subsample_idx = rng.choice(n_samples</span><span class="s2">, </span><span class="s1">size=subsample</span><span class="s2">, </span><span class="s1">replace=</span><span class="s2">False</span><span class="s1">)</span>
            <span class="s1">X = _safe_indexing(X</span><span class="s2">, </span><span class="s1">subsample_idx)</span>

        <span class="s1">n_features = X.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">n_bins = self._validate_n_bins(n_features)</span>

        <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s1">bin_edges = np.zeros(n_features</span><span class="s2">, </span><span class="s1">dtype=object)</span>
        <span class="s2">for </span><span class="s1">jj </span><span class="s2">in </span><span class="s1">range(n_features):</span>
            <span class="s1">column = X[:</span><span class="s2">, </span><span class="s1">jj]</span>
            <span class="s1">col_min</span><span class="s2">, </span><span class="s1">col_max = column.min()</span><span class="s2">, </span><span class="s1">column.max()</span>

            <span class="s2">if </span><span class="s1">col_min == col_max:</span>
                <span class="s1">warnings.warn(</span>
                    <span class="s4">&quot;Feature %d is constant and will be replaced with 0.&quot; </span><span class="s1">% jj</span>
                <span class="s1">)</span>
                <span class="s1">n_bins[jj] = </span><span class="s5">1</span>
                <span class="s1">bin_edges[jj] = np.array([-np.inf</span><span class="s2">, </span><span class="s1">np.inf])</span>
                <span class="s2">continue</span>

            <span class="s2">if </span><span class="s1">self.strategy == </span><span class="s4">&quot;uniform&quot;</span><span class="s1">:</span>
                <span class="s1">bin_edges[jj] = np.linspace(col_min</span><span class="s2">, </span><span class="s1">col_max</span><span class="s2">, </span><span class="s1">n_bins[jj] + </span><span class="s5">1</span><span class="s1">)</span>

            <span class="s2">elif </span><span class="s1">self.strategy == </span><span class="s4">&quot;quantile&quot;</span><span class="s1">:</span>
                <span class="s1">quantiles = np.linspace(</span><span class="s5">0</span><span class="s2">, </span><span class="s5">100</span><span class="s2">, </span><span class="s1">n_bins[jj] + </span><span class="s5">1</span><span class="s1">)</span>
                <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is None</span><span class="s1">:</span>
                    <span class="s1">bin_edges[jj] = np.asarray(np.percentile(column</span><span class="s2">, </span><span class="s1">quantiles))</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">bin_edges[jj] = np.asarray(</span>
                        <span class="s1">[</span>
                            <span class="s1">_weighted_percentile(column</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">q)</span>
                            <span class="s2">for </span><span class="s1">q </span><span class="s2">in </span><span class="s1">quantiles</span>
                        <span class="s1">]</span><span class="s2">,</span>
                        <span class="s1">dtype=np.float64</span><span class="s2">,</span>
                    <span class="s1">)</span>
            <span class="s2">elif </span><span class="s1">self.strategy == </span><span class="s4">&quot;kmeans&quot;</span><span class="s1">:</span>
                <span class="s2">from </span><span class="s1">..cluster </span><span class="s2">import </span><span class="s1">KMeans  </span><span class="s0"># fixes import loops</span>

                <span class="s0"># Deterministic initialization with uniform spacing</span>
                <span class="s1">uniform_edges = np.linspace(col_min</span><span class="s2">, </span><span class="s1">col_max</span><span class="s2">, </span><span class="s1">n_bins[jj] + </span><span class="s5">1</span><span class="s1">)</span>
                <span class="s1">init = (uniform_edges[</span><span class="s5">1</span><span class="s1">:] + uniform_edges[:-</span><span class="s5">1</span><span class="s1">])[:</span><span class="s2">, None</span><span class="s1">] * </span><span class="s5">0.5</span>

                <span class="s0"># 1D k-means procedure</span>
                <span class="s1">km = KMeans(n_clusters=n_bins[jj]</span><span class="s2">, </span><span class="s1">init=init</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s1">)</span>
                <span class="s1">centers = km.fit(</span>
                    <span class="s1">column[:</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight</span>
                <span class="s1">).cluster_centers_[:</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span>
                <span class="s0"># Must sort, centers may be unsorted even with sorted init</span>
                <span class="s1">centers.sort()</span>
                <span class="s1">bin_edges[jj] = (centers[</span><span class="s5">1</span><span class="s1">:] + centers[:-</span><span class="s5">1</span><span class="s1">]) * </span><span class="s5">0.5</span>
                <span class="s1">bin_edges[jj] = np.r_[col_min</span><span class="s2">, </span><span class="s1">bin_edges[jj]</span><span class="s2">, </span><span class="s1">col_max]</span>

            <span class="s0"># Remove bins whose width are too small (i.e., &lt;= 1e-8)</span>
            <span class="s2">if </span><span class="s1">self.strategy </span><span class="s2">in </span><span class="s1">(</span><span class="s4">&quot;quantile&quot;</span><span class="s2">, </span><span class="s4">&quot;kmeans&quot;</span><span class="s1">):</span>
                <span class="s1">mask = np.ediff1d(bin_edges[jj]</span><span class="s2">, </span><span class="s1">to_begin=np.inf) &gt; </span><span class="s5">1e-8</span>
                <span class="s1">bin_edges[jj] = bin_edges[jj][mask]</span>
                <span class="s2">if </span><span class="s1">len(bin_edges[jj]) - </span><span class="s5">1 </span><span class="s1">!= n_bins[jj]:</span>
                    <span class="s1">warnings.warn(</span>
                        <span class="s4">&quot;Bins whose width are too small (i.e., &lt;= &quot;</span>
                        <span class="s4">&quot;1e-8) in feature %d are removed. Consider &quot;</span>
                        <span class="s4">&quot;decreasing the number of bins.&quot; </span><span class="s1">% jj</span>
                    <span class="s1">)</span>
                    <span class="s1">n_bins[jj] = len(bin_edges[jj]) - </span><span class="s5">1</span>

        <span class="s1">self.bin_edges_ = bin_edges</span>
        <span class="s1">self.n_bins_ = n_bins</span>

        <span class="s2">if </span><span class="s4">&quot;onehot&quot; </span><span class="s2">in </span><span class="s1">self.encode:</span>
            <span class="s1">self._encoder = OneHotEncoder(</span>
                <span class="s1">categories=[np.arange(i) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">self.n_bins_]</span><span class="s2">,</span>
                <span class="s1">sparse_output=self.encode == </span><span class="s4">&quot;onehot&quot;</span><span class="s2">,</span>
                <span class="s1">dtype=output_dtype</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s0"># Fit the OneHotEncoder with toy datasets</span>
            <span class="s0"># so that it's ready for use after the KBinsDiscretizer is fitted</span>
            <span class="s1">self._encoder.fit(np.zeros((</span><span class="s5">1</span><span class="s2">, </span><span class="s1">len(self.n_bins_))))</span>

        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">_validate_n_bins(self</span><span class="s2">, </span><span class="s1">n_features):</span>
        <span class="s3">&quot;&quot;&quot;Returns n_bins_, the number of bins per feature.&quot;&quot;&quot;</span>
        <span class="s1">orig_bins = self.n_bins</span>
        <span class="s2">if </span><span class="s1">isinstance(orig_bins</span><span class="s2">, </span><span class="s1">Integral):</span>
            <span class="s2">return </span><span class="s1">np.full(n_features</span><span class="s2">, </span><span class="s1">orig_bins</span><span class="s2">, </span><span class="s1">dtype=int)</span>

        <span class="s1">n_bins = check_array(orig_bins</span><span class="s2">, </span><span class="s1">dtype=int</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True, </span><span class="s1">ensure_2d=</span><span class="s2">False</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">n_bins.ndim &gt; </span><span class="s5">1 </span><span class="s2">or </span><span class="s1">n_bins.shape[</span><span class="s5">0</span><span class="s1">] != n_features:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;n_bins must be a scalar or array of shape (n_features,).&quot;</span><span class="s1">)</span>

        <span class="s1">bad_nbins_value = (n_bins &lt; </span><span class="s5">2</span><span class="s1">) | (n_bins != orig_bins)</span>

        <span class="s1">violating_indices = np.where(bad_nbins_value)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s2">if </span><span class="s1">violating_indices.shape[</span><span class="s5">0</span><span class="s1">] &gt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">indices = </span><span class="s4">&quot;, &quot;</span><span class="s1">.join(str(i) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">violating_indices)</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;{} received an invalid number &quot;</span>
                <span class="s4">&quot;of bins at indices {}. Number of bins &quot;</span>
                <span class="s4">&quot;must be at least 2, and must be an int.&quot;</span><span class="s1">.format(</span>
                    <span class="s1">KBinsDiscretizer.__name__</span><span class="s2">, </span><span class="s1">indices</span>
                <span class="s1">)</span>
            <span class="s1">)</span>
        <span class="s2">return </span><span class="s1">n_bins</span>

    <span class="s2">def </span><span class="s1">transform(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s3">&quot;&quot;&quot; 
        Discretize the data. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Data to be discretized. 
 
        Returns 
        ------- 
        Xt : {ndarray, sparse matrix}, dtype={np.float32, np.float64} 
            Data in the binned space. Will be a sparse matrix if 
            `self.encode='onehot'` and ndarray otherwise. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s0"># check input and attribute dtypes</span>
        <span class="s1">dtype = (np.float64</span><span class="s2">, </span><span class="s1">np.float32) </span><span class="s2">if </span><span class="s1">self.dtype </span><span class="s2">is None else </span><span class="s1">self.dtype</span>
        <span class="s1">Xt = self._validate_data(X</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True, </span><span class="s1">dtype=dtype</span><span class="s2">, </span><span class="s1">reset=</span><span class="s2">False</span><span class="s1">)</span>

        <span class="s1">bin_edges = self.bin_edges_</span>
        <span class="s2">for </span><span class="s1">jj </span><span class="s2">in </span><span class="s1">range(Xt.shape[</span><span class="s5">1</span><span class="s1">]):</span>
            <span class="s1">Xt[:</span><span class="s2">, </span><span class="s1">jj] = np.searchsorted(bin_edges[jj][</span><span class="s5">1</span><span class="s1">:-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Xt[:</span><span class="s2">, </span><span class="s1">jj]</span><span class="s2">, </span><span class="s1">side=</span><span class="s4">&quot;right&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">self.encode == </span><span class="s4">&quot;ordinal&quot;</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">Xt</span>

        <span class="s1">dtype_init = </span><span class="s2">None</span>
        <span class="s2">if </span><span class="s4">&quot;onehot&quot; </span><span class="s2">in </span><span class="s1">self.encode:</span>
            <span class="s1">dtype_init = self._encoder.dtype</span>
            <span class="s1">self._encoder.dtype = Xt.dtype</span>
        <span class="s2">try</span><span class="s1">:</span>
            <span class="s1">Xt_enc = self._encoder.transform(Xt)</span>
        <span class="s2">finally</span><span class="s1">:</span>
            <span class="s0"># revert the initial dtype to avoid modifying self.</span>
            <span class="s1">self._encoder.dtype = dtype_init</span>
        <span class="s2">return </span><span class="s1">Xt_enc</span>

    <span class="s2">def </span><span class="s1">inverse_transform(self</span><span class="s2">, </span><span class="s1">Xt):</span>
        <span class="s3">&quot;&quot;&quot; 
        Transform discretized data back to original feature space. 
 
        Note that this function does not regenerate the original data 
        due to discretization rounding. 
 
        Parameters 
        ---------- 
        Xt : array-like of shape (n_samples, n_features) 
            Transformed data in the binned space. 
 
        Returns 
        ------- 
        Xinv : ndarray, dtype={np.float32, np.float64} 
            Data in the original feature space. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s2">if </span><span class="s4">&quot;onehot&quot; </span><span class="s2">in </span><span class="s1">self.encode:</span>
            <span class="s1">Xt = self._encoder.inverse_transform(Xt)</span>

        <span class="s1">Xinv = check_array(Xt</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True, </span><span class="s1">dtype=(np.float64</span><span class="s2">, </span><span class="s1">np.float32))</span>
        <span class="s1">n_features = self.n_bins_.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s2">if </span><span class="s1">Xinv.shape[</span><span class="s5">1</span><span class="s1">] != n_features:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;Incorrect number of features. Expecting {}, received {}.&quot;</span><span class="s1">.format(</span>
                    <span class="s1">n_features</span><span class="s2">, </span><span class="s1">Xinv.shape[</span><span class="s5">1</span><span class="s1">]</span>
                <span class="s1">)</span>
            <span class="s1">)</span>

        <span class="s2">for </span><span class="s1">jj </span><span class="s2">in </span><span class="s1">range(n_features):</span>
            <span class="s1">bin_edges = self.bin_edges_[jj]</span>
            <span class="s1">bin_centers = (bin_edges[</span><span class="s5">1</span><span class="s1">:] + bin_edges[:-</span><span class="s5">1</span><span class="s1">]) * </span><span class="s5">0.5</span>
            <span class="s1">Xinv[:</span><span class="s2">, </span><span class="s1">jj] = bin_centers[np.int_(Xinv[:</span><span class="s2">, </span><span class="s1">jj])]</span>

        <span class="s2">return </span><span class="s1">Xinv</span>

    <span class="s2">def </span><span class="s1">get_feature_names_out(self</span><span class="s2">, </span><span class="s1">input_features=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s3">&quot;&quot;&quot;Get output feature names. 
 
        Parameters 
        ---------- 
        input_features : array-like of str or None, default=None 
            Input features. 
 
            - If `input_features` is `None`, then `feature_names_in_` is 
              used as feature names in. If `feature_names_in_` is not defined, 
              then the following input feature names are generated: 
              `[&quot;x0&quot;, &quot;x1&quot;, ..., &quot;x(n_features_in_ - 1)&quot;]`. 
            - If `input_features` is an array-like, then `input_features` must 
              match `feature_names_in_` if `feature_names_in_` is defined. 
 
        Returns 
        ------- 
        feature_names_out : ndarray of str objects 
            Transformed feature names. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self</span><span class="s2">, </span><span class="s4">&quot;n_features_in_&quot;</span><span class="s1">)</span>
        <span class="s1">input_features = _check_feature_names_in(self</span><span class="s2">, </span><span class="s1">input_features)</span>
        <span class="s2">if </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s4">&quot;_encoder&quot;</span><span class="s1">):</span>
            <span class="s2">return </span><span class="s1">self._encoder.get_feature_names_out(input_features)</span>

        <span class="s0"># ordinal encoding</span>
        <span class="s2">return </span><span class="s1">input_features</span>
</pre>
</body>
</html>