<html>
<head>
<title>_kernel_base.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_kernel_base.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Module containing the base object for multivariate kernel density and 
regression, plus some utilities. 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">copy</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">optimize</span>
<span class="s2">from </span><span class="s1">scipy.stats.mstats </span><span class="s2">import </span><span class="s1">mquantiles</span>

<span class="s2">try</span><span class="s1">:</span>
    <span class="s2">import </span><span class="s1">joblib</span>
    <span class="s1">has_joblib = </span><span class="s2">True</span>
<span class="s2">except </span><span class="s1">ImportError:</span>
    <span class="s1">has_joblib = </span><span class="s2">False</span>

<span class="s2">from </span><span class="s1">. </span><span class="s2">import </span><span class="s1">kernels</span>


<span class="s1">kernel_func = dict(wangryzin=kernels.wang_ryzin</span><span class="s2">,</span>
                   <span class="s1">aitchisonaitken=kernels.aitchison_aitken</span><span class="s2">,</span>
                   <span class="s1">gaussian=kernels.gaussian</span><span class="s2">,</span>
                   <span class="s1">aitchison_aitken_reg = kernels.aitchison_aitken_reg</span><span class="s2">,</span>
                   <span class="s1">wangryzin_reg = kernels.wang_ryzin_reg</span><span class="s2">,</span>
                   <span class="s1">gauss_convolution=kernels.gaussian_convolution</span><span class="s2">,</span>
                   <span class="s1">wangryzin_convolution=kernels.wang_ryzin_convolution</span><span class="s2">,</span>
                   <span class="s1">aitchisonaitken_convolution=kernels.aitchison_aitken_convolution</span><span class="s2">,</span>
                   <span class="s1">gaussian_cdf=kernels.gaussian_cdf</span><span class="s2">,</span>
                   <span class="s1">aitchisonaitken_cdf=kernels.aitchison_aitken_cdf</span><span class="s2">,</span>
                   <span class="s1">wangryzin_cdf=kernels.wang_ryzin_cdf</span><span class="s2">,</span>
                   <span class="s1">d_gaussian=kernels.d_gaussian</span><span class="s2">,</span>
                   <span class="s1">tricube=kernels.tricube)</span>


<span class="s2">def </span><span class="s1">_compute_min_std_IQR(data):</span>
    <span class="s0">&quot;&quot;&quot;Compute minimum of std and IQR for each variable.&quot;&quot;&quot;</span>
    <span class="s1">s1 = np.std(data</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">q75 = mquantiles(data</span><span class="s2">, </span><span class="s3">0.75</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">).data[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">q25 = mquantiles(data</span><span class="s2">, </span><span class="s3">0.25</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">).data[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">s2 = (q75 - q25) / </span><span class="s3">1.349  </span><span class="s4"># IQR</span>
    <span class="s1">dispersion = np.minimum(s1</span><span class="s2">, </span><span class="s1">s2)</span>
    <span class="s2">return </span><span class="s1">dispersion</span>


<span class="s2">def </span><span class="s1">_compute_subset(class_type</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">bw</span><span class="s2">, </span><span class="s1">co</span><span class="s2">, </span><span class="s1">do</span><span class="s2">, </span><span class="s1">n_cvars</span><span class="s2">, </span><span class="s1">ix_ord</span><span class="s2">,</span>
                    <span class="s1">ix_unord</span><span class="s2">, </span><span class="s1">n_sub</span><span class="s2">, </span><span class="s1">class_vars</span><span class="s2">, </span><span class="s1">randomize</span><span class="s2">, </span><span class="s1">bound):</span>
    <span class="s0">&quot;&quot;&quot;&quot;Compute bw on subset of data. 
 
    Called from ``GenericKDE._compute_efficient_*``. 
 
    Notes 
    ----- 
    Needs to be outside the class in order for joblib to be able to pickle it. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">randomize:</span>
        <span class="s1">np.random.shuffle(data)</span>
        <span class="s1">sub_data = data[:n_sub</span><span class="s2">, </span><span class="s1">:]</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">sub_data = data[bound[</span><span class="s3">0</span><span class="s1">]:bound[</span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">:]</span>

    <span class="s2">if </span><span class="s1">class_type == </span><span class="s5">'KDEMultivariate'</span><span class="s1">:</span>
        <span class="s2">from </span><span class="s1">.kernel_density </span><span class="s2">import </span><span class="s1">KDEMultivariate</span>
        <span class="s1">var_type = class_vars[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">sub_model = KDEMultivariate(sub_data</span><span class="s2">, </span><span class="s1">var_type</span><span class="s2">, </span><span class="s1">bw=bw</span><span class="s2">,</span>
                        <span class="s1">defaults=EstimatorSettings(efficient=</span><span class="s2">False</span><span class="s1">))</span>
    <span class="s2">elif </span><span class="s1">class_type == </span><span class="s5">'KDEMultivariateConditional'</span><span class="s1">:</span>
        <span class="s2">from </span><span class="s1">.kernel_density </span><span class="s2">import </span><span class="s1">KDEMultivariateConditional</span>
        <span class="s1">k_dep</span><span class="s2">, </span><span class="s1">dep_type</span><span class="s2">, </span><span class="s1">indep_type = class_vars</span>
        <span class="s1">endog = sub_data[:</span><span class="s2">, </span><span class="s1">:k_dep]</span>
        <span class="s1">exog = sub_data[:</span><span class="s2">, </span><span class="s1">k_dep:]</span>
        <span class="s1">sub_model = KDEMultivariateConditional(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">dep_type</span><span class="s2">,</span>
            <span class="s1">indep_type</span><span class="s2">, </span><span class="s1">bw=bw</span><span class="s2">, </span><span class="s1">defaults=EstimatorSettings(efficient=</span><span class="s2">False</span><span class="s1">))</span>
    <span class="s2">elif </span><span class="s1">class_type == </span><span class="s5">'KernelReg'</span><span class="s1">:</span>
        <span class="s2">from </span><span class="s1">.kernel_regression </span><span class="s2">import </span><span class="s1">KernelReg</span>
        <span class="s1">var_type</span><span class="s2">, </span><span class="s1">k_vars</span><span class="s2">, </span><span class="s1">reg_type = class_vars</span>
        <span class="s1">endog = _adjust_shape(sub_data[:</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">exog = _adjust_shape(sub_data[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">k_vars)</span>
        <span class="s1">sub_model = KernelReg(endog=endog</span><span class="s2">, </span><span class="s1">exog=exog</span><span class="s2">, </span><span class="s1">reg_type=reg_type</span><span class="s2">,</span>
                              <span class="s1">var_type=var_type</span><span class="s2">, </span><span class="s1">bw=bw</span><span class="s2">,</span>
                              <span class="s1">defaults=EstimatorSettings(efficient=</span><span class="s2">False</span><span class="s1">))</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;class_type not recognized, should be one of &quot; </span><span class="s1">\</span>
                 <span class="s5">&quot;{KDEMultivariate, KDEMultivariateConditional, KernelReg}&quot;</span><span class="s1">)</span>

    <span class="s4"># Compute dispersion in next 4 lines</span>
    <span class="s2">if </span><span class="s1">class_type == </span><span class="s5">'KernelReg'</span><span class="s1">:</span>
        <span class="s1">sub_data = sub_data[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">:]</span>

    <span class="s1">dispersion = _compute_min_std_IQR(sub_data)</span>

    <span class="s1">fct = dispersion * n_sub**(-</span><span class="s3">1. </span><span class="s1">/ (n_cvars + co))</span>
    <span class="s1">fct[ix_unord] = n_sub**(-</span><span class="s3">2. </span><span class="s1">/ (n_cvars + do))</span>
    <span class="s1">fct[ix_ord] = n_sub**(-</span><span class="s3">2. </span><span class="s1">/ (n_cvars + do))</span>
    <span class="s1">sample_scale_sub = sub_model.bw / fct  </span><span class="s4">#TODO: check if correct</span>
    <span class="s1">bw_sub = sub_model.bw</span>
    <span class="s2">return </span><span class="s1">sample_scale_sub</span><span class="s2">, </span><span class="s1">bw_sub</span>


<span class="s2">class </span><span class="s1">GenericKDE (object):</span>
    <span class="s0">&quot;&quot;&quot; 
    Base class for density estimation and regression KDE classes. 
    &quot;&quot;&quot;</span>
    <span class="s2">def </span><span class="s1">_compute_bw(self</span><span class="s2">, </span><span class="s1">bw):</span>
        <span class="s0">&quot;&quot;&quot; 
        Computes the bandwidth of the data. 
 
        Parameters 
        ---------- 
        bw : {array_like, str} 
            If array_like: user-specified bandwidth. 
            If a string, should be one of: 
 
                - cv_ml: cross validation maximum likelihood 
                - normal_reference: normal reference rule of thumb 
                - cv_ls: cross validation least squares 
 
        Notes 
        ----- 
        The default values for bw is 'normal_reference'. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">bw </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">bw = </span><span class="s5">'normal_reference'</span>

        <span class="s2">if not </span><span class="s1">isinstance(bw</span><span class="s2">, </span><span class="s1">str):</span>
            <span class="s1">self._bw_method = </span><span class="s5">&quot;user-specified&quot;</span>
            <span class="s1">res = np.asarray(bw)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s4"># The user specified a bandwidth selection method</span>
            <span class="s1">self._bw_method = bw</span>
            <span class="s4"># Workaround to avoid instance methods in __dict__</span>
            <span class="s2">if </span><span class="s1">bw == </span><span class="s5">'normal_reference'</span><span class="s1">:</span>
                <span class="s1">bwfunc = self._normal_reference</span>
            <span class="s2">elif </span><span class="s1">bw == </span><span class="s5">'cv_ml'</span><span class="s1">:</span>
                <span class="s1">bwfunc = self._cv_ml</span>
            <span class="s2">else</span><span class="s1">:  </span><span class="s4"># bw == 'cv_ls'</span>
                <span class="s1">bwfunc = self._cv_ls</span>
            <span class="s1">res = bwfunc()</span>

        <span class="s2">return </span><span class="s1">res</span>

    <span class="s2">def </span><span class="s1">_compute_dispersion(self</span><span class="s2">, </span><span class="s1">data):</span>
        <span class="s0">&quot;&quot;&quot; 
        Computes the measure of dispersion. 
 
        The minimum of the standard deviation and interquartile range / 1.349 
 
        Notes 
        ----- 
        Reimplemented in `KernelReg`, because the first column of `data` has to 
        be removed. 
 
        References 
        ---------- 
        See the user guide for the np package in R. 
        In the notes on bwscaling option in npreg, npudens, npcdens there is 
        a discussion on the measure of dispersion 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">_compute_min_std_IQR(data)</span>

    <span class="s2">def </span><span class="s1">_get_class_vars_type(self):</span>
        <span class="s0">&quot;&quot;&quot;Helper method to be able to pass needed vars to _compute_subset. 
 
        Needs to be implemented by subclasses.&quot;&quot;&quot;</span>
        <span class="s2">pass</span>

    <span class="s2">def </span><span class="s1">_compute_efficient(self</span><span class="s2">, </span><span class="s1">bw):</span>
        <span class="s0">&quot;&quot;&quot; 
        Computes the bandwidth by estimating the scaling factor (c) 
        in n_res resamples of size ``n_sub`` (in `randomize` case), or by 
        dividing ``nobs`` into as many ``n_sub`` blocks as needed (if 
        `randomize` is False). 
 
        References 
        ---------- 
        See p.9 in socserv.mcmaster.ca/racine/np_faq.pdf 
        &quot;&quot;&quot;</span>

        <span class="s2">if </span><span class="s1">bw </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">self._bw_method = </span><span class="s5">'normal_reference'</span>
        <span class="s2">if </span><span class="s1">isinstance(bw</span><span class="s2">, </span><span class="s1">str):</span>
            <span class="s1">self._bw_method = bw</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self._bw_method = </span><span class="s5">&quot;user-specified&quot;</span>
            <span class="s2">return </span><span class="s1">bw</span>

        <span class="s1">nobs = self.nobs</span>
        <span class="s1">n_sub = self.n_sub</span>
        <span class="s1">data = copy.deepcopy(self.data)</span>
        <span class="s1">n_cvars = self.data_type.count(</span><span class="s5">'c'</span><span class="s1">)</span>
        <span class="s1">co = </span><span class="s3">4  </span><span class="s4"># 2*order of continuous kernel</span>
        <span class="s1">do = </span><span class="s3">4  </span><span class="s4"># 2*order of discrete kernel</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">ix_ord</span><span class="s2">, </span><span class="s1">ix_unord = _get_type_pos(self.data_type)</span>

        <span class="s4"># Define bounds for slicing the data</span>
        <span class="s2">if </span><span class="s1">self.randomize:</span>
            <span class="s4"># randomize chooses blocks of size n_sub, independent of nobs</span>
            <span class="s1">bounds = [</span><span class="s2">None</span><span class="s1">] * self.n_res</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">bounds = [(i * n_sub</span><span class="s2">, </span><span class="s1">(i+</span><span class="s3">1</span><span class="s1">) * n_sub) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(nobs // n_sub)]</span>
            <span class="s2">if </span><span class="s1">nobs % n_sub &gt; </span><span class="s3">0</span><span class="s1">:</span>
                <span class="s1">bounds.append((nobs - nobs % n_sub</span><span class="s2">, </span><span class="s1">nobs))</span>

        <span class="s1">n_blocks = self.n_res </span><span class="s2">if </span><span class="s1">self.randomize </span><span class="s2">else </span><span class="s1">len(bounds)</span>
        <span class="s1">sample_scale = np.empty((n_blocks</span><span class="s2">, </span><span class="s1">self.k_vars))</span>
        <span class="s1">only_bw = np.empty((n_blocks</span><span class="s2">, </span><span class="s1">self.k_vars))</span>

        <span class="s1">class_type</span><span class="s2">, </span><span class="s1">class_vars = self._get_class_vars_type()</span>
        <span class="s2">if </span><span class="s1">has_joblib:</span>
            <span class="s4"># `res` is a list of tuples (sample_scale_sub, bw_sub)</span>
            <span class="s1">res = joblib.Parallel(n_jobs=self.n_jobs)(</span>
                <span class="s1">joblib.delayed(_compute_subset)(</span>
                    <span class="s1">class_type</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">bw</span><span class="s2">, </span><span class="s1">co</span><span class="s2">, </span><span class="s1">do</span><span class="s2">, </span><span class="s1">n_cvars</span><span class="s2">, </span><span class="s1">ix_ord</span><span class="s2">, </span><span class="s1">ix_unord</span><span class="s2">, </span><span class="s1">\</span>
                    <span class="s1">n_sub</span><span class="s2">, </span><span class="s1">class_vars</span><span class="s2">, </span><span class="s1">self.randomize</span><span class="s2">, </span><span class="s1">bounds[i]) \</span>
                <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_blocks))</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">res = []</span>
            <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_blocks):</span>
                <span class="s1">res.append(_compute_subset(class_type</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">bw</span><span class="s2">, </span><span class="s1">co</span><span class="s2">, </span><span class="s1">do</span><span class="s2">,</span>
                                           <span class="s1">n_cvars</span><span class="s2">, </span><span class="s1">ix_ord</span><span class="s2">, </span><span class="s1">ix_unord</span><span class="s2">, </span><span class="s1">n_sub</span><span class="s2">,</span>
                                           <span class="s1">class_vars</span><span class="s2">, </span><span class="s1">self.randomize</span><span class="s2">,</span>
                                           <span class="s1">bounds[i]))</span>

        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_blocks):</span>
            <span class="s1">sample_scale[i</span><span class="s2">, </span><span class="s1">:] = res[i][</span><span class="s3">0</span><span class="s1">]</span>
            <span class="s1">only_bw[i</span><span class="s2">, </span><span class="s1">:] = res[i][</span><span class="s3">1</span><span class="s1">]</span>

        <span class="s1">s = self._compute_dispersion(data)</span>
        <span class="s1">order_func = np.median </span><span class="s2">if </span><span class="s1">self.return_median </span><span class="s2">else </span><span class="s1">np.mean</span>
        <span class="s1">m_scale = order_func(sample_scale</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s4"># TODO: Check if 1/5 is correct in line below!</span>
        <span class="s1">bw = m_scale * s * nobs**(-</span><span class="s3">1. </span><span class="s1">/ (n_cvars + co))</span>
        <span class="s1">bw[ix_ord] = m_scale[ix_ord] * nobs**(-</span><span class="s3">2.</span><span class="s1">/ (n_cvars + do))</span>
        <span class="s1">bw[ix_unord] = m_scale[ix_unord] * nobs**(-</span><span class="s3">2.</span><span class="s1">/ (n_cvars + do))</span>

        <span class="s2">if </span><span class="s1">self.return_only_bw:</span>
            <span class="s1">bw = np.median(only_bw</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">bw</span>

    <span class="s2">def </span><span class="s1">_set_defaults(self</span><span class="s2">, </span><span class="s1">defaults):</span>
        <span class="s0">&quot;&quot;&quot;Sets the default values for the efficient estimation&quot;&quot;&quot;</span>
        <span class="s1">self.n_res = defaults.n_res</span>
        <span class="s1">self.n_sub = defaults.n_sub</span>
        <span class="s1">self.randomize = defaults.randomize</span>
        <span class="s1">self.return_median = defaults.return_median</span>
        <span class="s1">self.efficient = defaults.efficient</span>
        <span class="s1">self.return_only_bw = defaults.return_only_bw</span>
        <span class="s1">self.n_jobs = defaults.n_jobs</span>

    <span class="s2">def </span><span class="s1">_normal_reference(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns Scott's normal reference rule of thumb bandwidth parameter. 
 
        Notes 
        ----- 
        See p.13 in [2] for an example and discussion.  The formula for the 
        bandwidth is 
 
        .. math:: h = 1.06n^{-1/(4+q)} 
 
        where ``n`` is the number of observations and ``q`` is the number of 
        variables. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = np.std(self.data</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s3">1.06 </span><span class="s1">* X * self.nobs ** (- </span><span class="s3">1. </span><span class="s1">/ (</span><span class="s3">4 </span><span class="s1">+ self.data.shape[</span><span class="s3">1</span><span class="s1">]))</span>

    <span class="s2">def </span><span class="s1">_set_bw_bounds(self</span><span class="s2">, </span><span class="s1">bw):</span>
        <span class="s0">&quot;&quot;&quot; 
        Sets bandwidth lower bound to effectively zero )1e-10), and for 
        discrete values upper bound to 1. 
        &quot;&quot;&quot;</span>
        <span class="s1">bw[bw &lt; </span><span class="s3">0</span><span class="s1">] = </span><span class="s3">1e-10</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">ix_ord</span><span class="s2">, </span><span class="s1">ix_unord = _get_type_pos(self.data_type)</span>
        <span class="s1">bw[ix_ord] = np.minimum(bw[ix_ord]</span><span class="s2">, </span><span class="s3">1.</span><span class="s1">)</span>
        <span class="s1">bw[ix_unord] = np.minimum(bw[ix_unord]</span><span class="s2">, </span><span class="s3">1.</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">bw</span>

    <span class="s2">def </span><span class="s1">_cv_ml(self):</span>
        <span class="s0">r&quot;&quot;&quot; 
        Returns the cross validation maximum likelihood bandwidth parameter. 
 
        Notes 
        ----- 
        For more details see p.16, 18, 27 in Ref. [1] (see module docstring). 
 
        Returns the bandwidth estimate that maximizes the leave-out-out 
        likelihood.  The leave-one-out log likelihood function is: 
 
        .. math:: \ln L=\sum_{i=1}^{n}\ln f_{-i}(X_{i}) 
 
        The leave-one-out kernel estimator of :math:`f_{-i}` is: 
 
        .. math:: f_{-i}(X_{i})=\frac{1}{(n-1)h} 
                        \sum_{j=1,j\neq i}K_{h}(X_{i},X_{j}) 
 
        where :math:`K_{h}` represents the Generalized product kernel 
        estimator: 
 
        .. math:: K_{h}(X_{i},X_{j})=\prod_{s=1}^ 
                        {q}h_{s}^{-1}k\left(\frac{X_{is}-X_{js}}{h_{s}}\right) 
        &quot;&quot;&quot;</span>
        <span class="s4"># the initial value for the optimization is the normal_reference</span>
        <span class="s1">h0 = self._normal_reference()</span>
        <span class="s1">bw = optimize.fmin(self.loo_likelihood</span><span class="s2">, </span><span class="s1">x0=h0</span><span class="s2">, </span><span class="s1">args=(np.log</span><span class="s2">, </span><span class="s1">)</span><span class="s2">,</span>
                           <span class="s1">maxiter=</span><span class="s3">1e3</span><span class="s2">, </span><span class="s1">maxfun=</span><span class="s3">1e3</span><span class="s2">, </span><span class="s1">disp=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">xtol=</span><span class="s3">1e-3</span><span class="s1">)</span>
        <span class="s1">bw = self._set_bw_bounds(bw)  </span><span class="s4"># bound bw if necessary</span>
        <span class="s2">return </span><span class="s1">bw</span>

    <span class="s2">def </span><span class="s1">_cv_ls(self):</span>
        <span class="s0">r&quot;&quot;&quot; 
        Returns the cross-validation least squares bandwidth parameter(s). 
 
        Notes 
        ----- 
        For more details see pp. 16, 27 in Ref. [1] (see module docstring). 
 
        Returns the value of the bandwidth that maximizes the integrated mean 
        square error between the estimated and actual distribution.  The 
        integrated mean square error (IMSE) is given by: 
 
        .. math:: \int\left[\hat{f}(x)-f(x)\right]^{2}dx 
 
        This is the general formula for the IMSE.  The IMSE differs for 
        conditional (``KDEMultivariateConditional``) and unconditional 
        (``KDEMultivariate``) kernel density estimation. 
        &quot;&quot;&quot;</span>
        <span class="s1">h0 = self._normal_reference()</span>
        <span class="s1">bw = optimize.fmin(self.imse</span><span class="s2">, </span><span class="s1">x0=h0</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s3">1e3</span><span class="s2">, </span><span class="s1">maxfun=</span><span class="s3">1e3</span><span class="s2">, </span><span class="s1">disp=</span><span class="s3">0</span><span class="s2">,</span>
                           <span class="s1">xtol=</span><span class="s3">1e-3</span><span class="s1">)</span>
        <span class="s1">bw = self._set_bw_bounds(bw)  </span><span class="s4"># bound bw if necessary</span>
        <span class="s2">return </span><span class="s1">bw</span>

    <span class="s2">def </span><span class="s1">loo_likelihood(self):</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError</span>


<span class="s2">class </span><span class="s1">EstimatorSettings:</span>
    <span class="s0">&quot;&quot;&quot; 
    Object to specify settings for density estimation or regression. 
 
    `EstimatorSettings` has several properties related to how bandwidth 
    estimation for the `KDEMultivariate`, `KDEMultivariateConditional`, 
    `KernelReg` and `CensoredKernelReg` classes behaves. 
 
    Parameters 
    ---------- 
    efficient : bool, optional 
        If True, the bandwidth estimation is to be performed 
        efficiently -- by taking smaller sub-samples and estimating 
        the scaling factor of each subsample.  This is useful for large 
        samples (nobs &gt;&gt; 300) and/or multiple variables (k_vars &gt; 3). 
        If False (default), all data is used at the same time. 
    randomize : bool, optional 
        If True, the bandwidth estimation is to be performed by 
        taking `n_res` random resamples (with replacement) of size `n_sub` from 
        the full sample.  If set to False (default), the estimation is 
        performed by slicing the full sample in sub-samples of size `n_sub` so 
        that all samples are used once. 
    n_sub : int, optional 
        Size of the sub-samples.  Default is 50. 
    n_res : int, optional 
        The number of random re-samples used to estimate the bandwidth. 
        Only has an effect if ``randomize == True``.  Default value is 25. 
    return_median : bool, optional 
        If True (default), the estimator uses the median of all scaling factors 
        for each sub-sample to estimate the bandwidth of the full sample. 
        If False, the estimator uses the mean. 
    return_only_bw : bool, optional 
        If True, the estimator is to use the bandwidth and not the 
        scaling factor.  This is *not* theoretically justified. 
        Should be used only for experimenting. 
    n_jobs : int, optional 
        The number of jobs to use for parallel estimation with 
        ``joblib.Parallel``.  Default is -1, meaning ``n_cores - 1``, with 
        ``n_cores`` the number of available CPU cores. 
        See the `joblib documentation 
        &lt;https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html&gt;`_ for more details. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; settings = EstimatorSettings(randomize=True, n_jobs=3) 
    &gt;&gt;&gt; k_dens = KDEMultivariate(data, var_type, defaults=settings) 
    &quot;&quot;&quot;</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">efficient=</span><span class="s2">False, </span><span class="s1">randomize=</span><span class="s2">False, </span><span class="s1">n_res=</span><span class="s3">25</span><span class="s2">, </span><span class="s1">n_sub=</span><span class="s3">50</span><span class="s2">,</span>
                 <span class="s1">return_median=</span><span class="s2">True, </span><span class="s1">return_only_bw=</span><span class="s2">False, </span><span class="s1">n_jobs=-</span><span class="s3">1</span><span class="s1">):</span>
        <span class="s1">self.efficient = efficient</span>
        <span class="s1">self.randomize = randomize</span>
        <span class="s1">self.n_res = n_res</span>
        <span class="s1">self.n_sub = n_sub</span>
        <span class="s1">self.return_median = return_median</span>
        <span class="s1">self.return_only_bw = return_only_bw  </span><span class="s4"># TODO: remove this?</span>
        <span class="s1">self.n_jobs = n_jobs</span>


<span class="s2">class </span><span class="s1">LeaveOneOut:</span>
    <span class="s0">&quot;&quot;&quot; 
    Generator to give leave-one-out views on X. 
 
    Parameters 
    ---------- 
    X : array_like 
        2-D array. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; X = np.random.normal(0, 1, [10,2]) 
    &gt;&gt;&gt; loo = LeaveOneOut(X) 
    &gt;&gt;&gt; for x in loo: 
    ...    print x 
 
    Notes 
    ----- 
    A little lighter weight than sklearn LOO. We do not need test index. 
    Also passes views on X, not the index. 
    &quot;&quot;&quot;</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s1">self.X = np.asarray(X)</span>

    <span class="s2">def </span><span class="s1">__iter__(self):</span>
        <span class="s1">X = self.X</span>
        <span class="s1">nobs</span><span class="s2">, </span><span class="s1">k_vars = np.shape(X)</span>

        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(nobs):</span>
            <span class="s1">index = np.ones(nobs</span><span class="s2">, </span><span class="s1">dtype=bool)</span>
            <span class="s1">index[i] = </span><span class="s2">False</span>
            <span class="s2">yield </span><span class="s1">X[index</span><span class="s2">, </span><span class="s1">:]</span>


<span class="s2">def </span><span class="s1">_get_type_pos(var_type):</span>
    <span class="s1">ix_cont = np.array([c == </span><span class="s5">'c' </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">var_type])</span>
    <span class="s1">ix_ord = np.array([c == </span><span class="s5">'o' </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">var_type])</span>
    <span class="s1">ix_unord = np.array([c == </span><span class="s5">'u' </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">var_type])</span>
    <span class="s2">return </span><span class="s1">ix_cont</span><span class="s2">, </span><span class="s1">ix_ord</span><span class="s2">, </span><span class="s1">ix_unord</span>


<span class="s2">def </span><span class="s1">_adjust_shape(dat</span><span class="s2">, </span><span class="s1">k_vars):</span>
    <span class="s0">&quot;&quot;&quot; Returns an array of shape (nobs, k_vars) for use with `gpke`.&quot;&quot;&quot;</span>
    <span class="s1">dat = np.asarray(dat)</span>
    <span class="s2">if </span><span class="s1">dat.ndim &gt; </span><span class="s3">2</span><span class="s1">:</span>
        <span class="s1">dat = np.squeeze(dat)</span>
    <span class="s2">if </span><span class="s1">dat.ndim == </span><span class="s3">1 </span><span class="s2">and </span><span class="s1">k_vars &gt; </span><span class="s3">1</span><span class="s1">:  </span><span class="s4"># one obs many vars</span>
        <span class="s1">nobs = </span><span class="s3">1</span>
    <span class="s2">elif </span><span class="s1">dat.ndim == </span><span class="s3">1 </span><span class="s2">and </span><span class="s1">k_vars == </span><span class="s3">1</span><span class="s1">:  </span><span class="s4"># one obs one var</span>
        <span class="s1">nobs = len(dat)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">if </span><span class="s1">np.shape(dat)[</span><span class="s3">0</span><span class="s1">] == k_vars </span><span class="s2">and </span><span class="s1">np.shape(dat)[</span><span class="s3">1</span><span class="s1">] != k_vars:</span>
            <span class="s1">dat = dat.T</span>

        <span class="s1">nobs = np.shape(dat)[</span><span class="s3">0</span><span class="s1">]  </span><span class="s4"># ndim &gt;1 so many obs many vars</span>

    <span class="s1">dat = np.reshape(dat</span><span class="s2">, </span><span class="s1">(nobs</span><span class="s2">, </span><span class="s1">k_vars))</span>
    <span class="s2">return </span><span class="s1">dat</span>


<span class="s2">def </span><span class="s1">gpke(bw</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">data_predict</span><span class="s2">, </span><span class="s1">var_type</span><span class="s2">, </span><span class="s1">ckertype=</span><span class="s5">'gaussian'</span><span class="s2">,</span>
         <span class="s1">okertype=</span><span class="s5">'wangryzin'</span><span class="s2">, </span><span class="s1">ukertype=</span><span class="s5">'aitchisonaitken'</span><span class="s2">, </span><span class="s1">tosum=</span><span class="s2">True</span><span class="s1">):</span>
    <span class="s0">r&quot;&quot;&quot; 
    Returns the non-normalized Generalized Product Kernel Estimator 
 
    Parameters 
    ---------- 
    bw : 1-D ndarray 
        The user-specified bandwidth parameters. 
    data : 1D or 2-D ndarray 
        The training data. 
    data_predict : 1-D ndarray 
        The evaluation points at which the kernel estimation is performed. 
    var_type : str, optional 
        The variable type (continuous, ordered, unordered). 
    ckertype : str, optional 
        The kernel used for the continuous variables. 
    okertype : str, optional 
        The kernel used for the ordered discrete variables. 
    ukertype : str, optional 
        The kernel used for the unordered discrete variables. 
    tosum : bool, optional 
        Whether or not to sum the calculated array of densities.  Default is 
        True. 
 
    Returns 
    ------- 
    dens : array_like 
        The generalized product kernel density estimator. 
 
    Notes 
    ----- 
    The formula for the multivariate kernel estimator for the pdf is: 
 
    .. math:: f(x)=\frac{1}{nh_{1}...h_{q}}\sum_{i=1}^ 
                        {n}K\left(\frac{X_{i}-x}{h}\right) 
 
    where 
 
    .. math:: K\left(\frac{X_{i}-x}{h}\right) = 
                k\left( \frac{X_{i1}-x_{1}}{h_{1}}\right)\times 
                k\left( \frac{X_{i2}-x_{2}}{h_{2}}\right)\times...\times 
                k\left(\frac{X_{iq}-x_{q}}{h_{q}}\right) 
    &quot;&quot;&quot;</span>
    <span class="s1">kertypes = dict(c=ckertype</span><span class="s2">, </span><span class="s1">o=okertype</span><span class="s2">, </span><span class="s1">u=ukertype)</span>
    <span class="s4">#Kval = []</span>
    <span class="s4">#for ii, vtype in enumerate(var_type):</span>
    <span class="s4">#    func = kernel_func[kertypes[vtype]]</span>
    <span class="s4">#    Kval.append(func(bw[ii], data[:, ii], data_predict[ii]))</span>

    <span class="s4">#Kval = np.column_stack(Kval)</span>

    <span class="s1">Kval = np.empty(data.shape)</span>
    <span class="s2">for </span><span class="s1">ii</span><span class="s2">, </span><span class="s1">vtype </span><span class="s2">in </span><span class="s1">enumerate(var_type):</span>
        <span class="s1">func = kernel_func[kertypes[vtype]]</span>
        <span class="s1">Kval[:</span><span class="s2">, </span><span class="s1">ii] = func(bw[ii]</span><span class="s2">, </span><span class="s1">data[:</span><span class="s2">, </span><span class="s1">ii]</span><span class="s2">, </span><span class="s1">data_predict[ii])</span>

    <span class="s1">iscontinuous = np.array([c == </span><span class="s5">'c' </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">var_type])</span>
    <span class="s1">dens = Kval.prod(axis=</span><span class="s3">1</span><span class="s1">) / np.prod(bw[iscontinuous])</span>
    <span class="s2">if </span><span class="s1">tosum:</span>
        <span class="s2">return </span><span class="s1">dens.sum(axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">dens</span>
</pre>
</body>
</html>