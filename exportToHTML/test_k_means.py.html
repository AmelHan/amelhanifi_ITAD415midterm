<html>
<head>
<title>test_k_means.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_k_means.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Testing for K-means&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">re</span>
<span class="s2">import </span><span class="s1">sys</span>
<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">io </span><span class="s2">import </span><span class="s1">StringIO</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">sparse </span><span class="s2">as </span><span class="s1">sp</span>

<span class="s2">from </span><span class="s1">sklearn.base </span><span class="s2">import </span><span class="s1">clone</span>
<span class="s2">from </span><span class="s1">sklearn.cluster </span><span class="s2">import </span><span class="s1">KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans</span><span class="s2">, </span><span class="s1">k_means</span><span class="s2">, </span><span class="s1">kmeans_plusplus</span>
<span class="s2">from </span><span class="s1">sklearn.cluster._k_means_common </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">_euclidean_dense_dense_wrapper</span><span class="s2">,</span>
    <span class="s1">_euclidean_sparse_dense_wrapper</span><span class="s2">,</span>
    <span class="s1">_inertia_dense</span><span class="s2">,</span>
    <span class="s1">_inertia_sparse</span><span class="s2">,</span>
    <span class="s1">_is_same_clustering</span><span class="s2">,</span>
    <span class="s1">_relocate_empty_clusters_dense</span><span class="s2">,</span>
    <span class="s1">_relocate_empty_clusters_sparse</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.cluster._kmeans </span><span class="s2">import </span><span class="s1">_labels_inertia</span><span class="s2">, </span><span class="s1">_mini_batch_step</span>
<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">make_blobs</span>
<span class="s2">from </span><span class="s1">sklearn.exceptions </span><span class="s2">import </span><span class="s1">ConvergenceWarning</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">pairwise_distances</span><span class="s2">, </span><span class="s1">pairwise_distances_argmin</span>
<span class="s2">from </span><span class="s1">sklearn.metrics.cluster </span><span class="s2">import </span><span class="s1">v_measure_score</span>
<span class="s2">from </span><span class="s1">sklearn.metrics.pairwise </span><span class="s2">import </span><span class="s1">euclidean_distances</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s2">,</span>
    <span class="s1">assert_array_equal</span><span class="s2">,</span>
    <span class="s1">create_memmap_backed_data</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.utils.extmath </span><span class="s2">import </span><span class="s1">row_norms</span>
<span class="s2">from </span><span class="s1">sklearn.utils.fixes </span><span class="s2">import </span><span class="s1">threadpool_limits</span>

<span class="s3"># TODO(1.4): Remove</span>
<span class="s1">msg = (</span>
    <span class="s4">r&quot;The default value of `n_init` will change from \d* to 'auto' in 1.4. Set the&quot;</span>
    <span class="s4">r&quot; value of `n_init` explicitly to suppress the warning:FutureWarning&quot;</span>
<span class="s1">)</span>
<span class="s1">pytestmark = pytest.mark.filterwarnings(</span><span class="s4">&quot;ignore:&quot; </span><span class="s1">+ msg)</span>

<span class="s3"># non centered, sparse centers to check the</span>
<span class="s1">centers = np.array(</span>
    <span class="s1">[</span>
        <span class="s1">[</span><span class="s5">0.0</span><span class="s2">, </span><span class="s5">5.0</span><span class="s2">, </span><span class="s5">0.0</span><span class="s2">, </span><span class="s5">0.0</span><span class="s2">, </span><span class="s5">0.0</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s5">1.0</span><span class="s2">, </span><span class="s5">1.0</span><span class="s2">, </span><span class="s5">4.0</span><span class="s2">, </span><span class="s5">0.0</span><span class="s2">, </span><span class="s5">0.0</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">[</span><span class="s5">1.0</span><span class="s2">, </span><span class="s5">0.0</span><span class="s2">, </span><span class="s5">0.0</span><span class="s2">, </span><span class="s5">5.0</span><span class="s2">, </span><span class="s5">1.0</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">n_samples = </span><span class="s5">100</span>
<span class="s1">n_clusters</span><span class="s2">, </span><span class="s1">n_features = centers.shape</span>
<span class="s1">X</span><span class="s2">, </span><span class="s1">true_labels = make_blobs(</span>
    <span class="s1">n_samples=n_samples</span><span class="s2">, </span><span class="s1">centers=centers</span><span class="s2">, </span><span class="s1">cluster_std=</span><span class="s5">1.0</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">42</span>
<span class="s1">)</span>
<span class="s1">X_csr = sp.csr_matrix(X)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;array_constr&quot;</span><span class="s2">, </span><span class="s1">[np.array</span><span class="s2">, </span><span class="s1">sp.csr_matrix]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;dense&quot;</span><span class="s2">, </span><span class="s4">&quot;sparse&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;algo&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;lloyd&quot;</span><span class="s2">, </span><span class="s4">&quot;elkan&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;dtype&quot;</span><span class="s2">, </span><span class="s1">[np.float32</span><span class="s2">, </span><span class="s1">np.float64])</span>
<span class="s2">def </span><span class="s1">test_kmeans_results(array_constr</span><span class="s2">, </span><span class="s1">algo</span><span class="s2">, </span><span class="s1">dtype):</span>
    <span class="s3"># Checks that KMeans works as intended on toy dataset by comparing with</span>
    <span class="s3"># expected results computed by hand.</span>
    <span class="s1">X = array_constr([[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>
    <span class="s1">sample_weight = [</span><span class="s5">3</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">3</span><span class="s1">]</span>
    <span class="s1">init_centers = np.array([[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>

    <span class="s1">expected_labels = [</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">expected_inertia = </span><span class="s5">0.375</span>
    <span class="s1">expected_centers = np.array([[</span><span class="s5">0.125</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.875</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>
    <span class="s1">expected_n_iter = </span><span class="s5">2</span>

    <span class="s1">kmeans = KMeans(n_clusters=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">init=init_centers</span><span class="s2">, </span><span class="s1">algorithm=algo)</span>
    <span class="s1">kmeans.fit(X</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">assert_array_equal(kmeans.labels_</span><span class="s2">, </span><span class="s1">expected_labels)</span>
    <span class="s1">assert_allclose(kmeans.inertia_</span><span class="s2">, </span><span class="s1">expected_inertia)</span>
    <span class="s1">assert_allclose(kmeans.cluster_centers_</span><span class="s2">, </span><span class="s1">expected_centers)</span>
    <span class="s2">assert </span><span class="s1">kmeans.n_iter_ == expected_n_iter</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;array_constr&quot;</span><span class="s2">, </span><span class="s1">[np.array</span><span class="s2">, </span><span class="s1">sp.csr_matrix]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;dense&quot;</span><span class="s2">, </span><span class="s4">&quot;sparse&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;algo&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;lloyd&quot;</span><span class="s2">, </span><span class="s4">&quot;elkan&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_kmeans_relocated_clusters(array_constr</span><span class="s2">, </span><span class="s1">algo):</span>
    <span class="s3"># check that empty clusters are relocated as expected</span>
    <span class="s1">X = array_constr([[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]])</span>

    <span class="s3"># second center too far from others points will be empty at first iter</span>
    <span class="s1">init_centers = np.array([[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0.5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">3</span><span class="s2">, </span><span class="s5">3</span><span class="s1">]])</span>

    <span class="s1">kmeans = KMeans(n_clusters=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">init=init_centers</span><span class="s2">, </span><span class="s1">algorithm=algo)</span>
    <span class="s1">kmeans.fit(X)</span>

    <span class="s1">expected_n_iter = </span><span class="s5">3</span>
    <span class="s1">expected_inertia = </span><span class="s5">0.25</span>
    <span class="s1">assert_allclose(kmeans.inertia_</span><span class="s2">, </span><span class="s1">expected_inertia)</span>
    <span class="s2">assert </span><span class="s1">kmeans.n_iter_ == expected_n_iter</span>

    <span class="s3"># There are two acceptable ways of relocating clusters in this example, the output</span>
    <span class="s3"># depends on how the argpartition strategy breaks ties. We accept both outputs.</span>
    <span class="s2">try</span><span class="s1">:</span>
        <span class="s1">expected_labels = [</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">expected_centers = [[</span><span class="s5">0.25</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.75</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]]</span>
        <span class="s1">assert_array_equal(kmeans.labels_</span><span class="s2">, </span><span class="s1">expected_labels)</span>
        <span class="s1">assert_allclose(kmeans.cluster_centers_</span><span class="s2">, </span><span class="s1">expected_centers)</span>
    <span class="s2">except </span><span class="s1">AssertionError:</span>
        <span class="s1">expected_labels = [</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">expected_centers = [[</span><span class="s5">0.75</span><span class="s2">, </span><span class="s5">1.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.25</span><span class="s2">, </span><span class="s5">0.0</span><span class="s1">]]</span>
        <span class="s1">assert_array_equal(kmeans.labels_</span><span class="s2">, </span><span class="s1">expected_labels)</span>
        <span class="s1">assert_allclose(kmeans.cluster_centers_</span><span class="s2">, </span><span class="s1">expected_centers)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;array_constr&quot;</span><span class="s2">, </span><span class="s1">[np.array</span><span class="s2">, </span><span class="s1">sp.csr_matrix]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;dense&quot;</span><span class="s2">, </span><span class="s4">&quot;sparse&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_relocate_empty_clusters(array_constr):</span>
    <span class="s3"># test for the _relocate_empty_clusters_(dense/sparse) helpers</span>

    <span class="s3"># Synthetic dataset with 3 obvious clusters of different sizes</span>
    <span class="s1">X = np.array([-</span><span class="s5">10.0</span><span class="s2">, </span><span class="s1">-</span><span class="s5">9.5</span><span class="s2">, </span><span class="s1">-</span><span class="s5">9</span><span class="s2">, </span><span class="s1">-</span><span class="s5">8.5</span><span class="s2">, </span><span class="s1">-</span><span class="s5">8</span><span class="s2">, </span><span class="s1">-</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">9</span><span class="s2">, </span><span class="s5">9.5</span><span class="s2">, </span><span class="s5">10</span><span class="s1">]).reshape(-</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">X = array_constr(X)</span>
    <span class="s1">sample_weight = np.ones(</span><span class="s5">10</span><span class="s1">)</span>

    <span class="s3"># centers all initialized to the first point of X</span>
    <span class="s1">centers_old = np.array([-</span><span class="s5">10.0</span><span class="s2">, </span><span class="s1">-</span><span class="s5">10</span><span class="s2">, </span><span class="s1">-</span><span class="s5">10</span><span class="s1">]).reshape(-</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3"># With this initialization, all points will be assigned to the first center</span>
    <span class="s3"># At this point a center in centers_new is the weighted sum of the points</span>
    <span class="s3"># it contains if it's not empty, otherwise it is the same as before.</span>
    <span class="s1">centers_new = np.array([-</span><span class="s5">16.5</span><span class="s2">, </span><span class="s1">-</span><span class="s5">10</span><span class="s2">, </span><span class="s1">-</span><span class="s5">10</span><span class="s1">]).reshape(-</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">weight_in_clusters = np.array([</span><span class="s5">10.0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">labels = np.zeros(</span><span class="s5">10</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span>

    <span class="s2">if </span><span class="s1">array_constr </span><span class="s2">is </span><span class="s1">np.array:</span>
        <span class="s1">_relocate_empty_clusters_dense(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">centers_old</span><span class="s2">, </span><span class="s1">centers_new</span><span class="s2">, </span><span class="s1">weight_in_clusters</span><span class="s2">, </span><span class="s1">labels</span>
        <span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">_relocate_empty_clusters_sparse(</span>
            <span class="s1">X.data</span><span class="s2">,</span>
            <span class="s1">X.indices</span><span class="s2">,</span>
            <span class="s1">X.indptr</span><span class="s2">,</span>
            <span class="s1">sample_weight</span><span class="s2">,</span>
            <span class="s1">centers_old</span><span class="s2">,</span>
            <span class="s1">centers_new</span><span class="s2">,</span>
            <span class="s1">weight_in_clusters</span><span class="s2">,</span>
            <span class="s1">labels</span><span class="s2">,</span>
        <span class="s1">)</span>

    <span class="s3"># The relocation scheme will take the 2 points farthest from the center and</span>
    <span class="s3"># assign them to the 2 empty clusters, i.e. points at 10 and at 9.9. The</span>
    <span class="s3"># first center will be updated to contain the other 8 points.</span>
    <span class="s1">assert_array_equal(weight_in_clusters</span><span class="s2">, </span><span class="s1">[</span><span class="s5">8</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">])</span>
    <span class="s1">assert_allclose(centers_new</span><span class="s2">, </span><span class="s1">[[-</span><span class="s5">36</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">9.5</span><span class="s1">]])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;distribution&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;normal&quot;</span><span class="s2">, </span><span class="s4">&quot;blobs&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;array_constr&quot;</span><span class="s2">, </span><span class="s1">[np.array</span><span class="s2">, </span><span class="s1">sp.csr_matrix]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;dense&quot;</span><span class="s2">, </span><span class="s4">&quot;sparse&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;tol&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1e-2</span><span class="s2">, </span><span class="s5">1e-8</span><span class="s2">, </span><span class="s5">1e-100</span><span class="s2">, </span><span class="s5">0</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_kmeans_elkan_results(distribution</span><span class="s2">, </span><span class="s1">array_constr</span><span class="s2">, </span><span class="s1">tol</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check that results are identical between lloyd and elkan algorithms</span>
    <span class="s1">rnd = np.random.RandomState(global_random_seed)</span>
    <span class="s2">if </span><span class="s1">distribution == </span><span class="s4">&quot;normal&quot;</span><span class="s1">:</span>
        <span class="s1">X = rnd.normal(size=(</span><span class="s5">5000</span><span class="s2">, </span><span class="s5">10</span><span class="s1">))</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">_ = make_blobs(random_state=rnd)</span>
    <span class="s1">X[X &lt; </span><span class="s5">0</span><span class="s1">] = </span><span class="s5">0</span>
    <span class="s1">X = array_constr(X)</span>

    <span class="s1">km_lloyd = KMeans(n_clusters=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">tol=tol)</span>
    <span class="s1">km_elkan = KMeans(</span>
        <span class="s1">algorithm=</span><span class="s4">&quot;elkan&quot;</span><span class="s2">,</span>
        <span class="s1">n_clusters=</span><span class="s5">5</span><span class="s2">,</span>
        <span class="s1">random_state=global_random_seed</span><span class="s2">,</span>
        <span class="s1">n_init=</span><span class="s5">1</span><span class="s2">,</span>
        <span class="s1">tol=tol</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">km_lloyd.fit(X)</span>
    <span class="s1">km_elkan.fit(X)</span>
    <span class="s1">assert_allclose(km_elkan.cluster_centers_</span><span class="s2">, </span><span class="s1">km_lloyd.cluster_centers_)</span>
    <span class="s1">assert_array_equal(km_elkan.labels_</span><span class="s2">, </span><span class="s1">km_lloyd.labels_)</span>
    <span class="s2">assert </span><span class="s1">km_elkan.n_iter_ == km_lloyd.n_iter_</span>
    <span class="s2">assert </span><span class="s1">km_elkan.inertia_ == pytest.approx(km_lloyd.inertia_</span><span class="s2">, </span><span class="s1">rel=</span><span class="s5">1e-6</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;algorithm&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;lloyd&quot;</span><span class="s2">, </span><span class="s4">&quot;elkan&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_kmeans_convergence(algorithm</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check that KMeans stops when convergence is reached when tol=0. (#16075)</span>
    <span class="s1">rnd = np.random.RandomState(global_random_seed)</span>
    <span class="s1">X = rnd.normal(size=(</span><span class="s5">5000</span><span class="s2">, </span><span class="s5">10</span><span class="s1">))</span>
    <span class="s1">max_iter = </span><span class="s5">300</span>

    <span class="s1">km = KMeans(</span>
        <span class="s1">algorithm=algorithm</span><span class="s2">,</span>
        <span class="s1">n_clusters=</span><span class="s5">5</span><span class="s2">,</span>
        <span class="s1">random_state=global_random_seed</span><span class="s2">,</span>
        <span class="s1">n_init=</span><span class="s5">1</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s5">0</span><span class="s2">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
    <span class="s1">).fit(X)</span>

    <span class="s2">assert </span><span class="s1">km.n_iter_ &lt; max_iter</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;algorithm&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;auto&quot;</span><span class="s2">, </span><span class="s4">&quot;full&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_algorithm_auto_full_deprecation_warning(algorithm):</span>
    <span class="s1">X = np.random.rand(</span><span class="s5">100</span><span class="s2">, </span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">kmeans = KMeans(algorithm=algorithm)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(</span>
        <span class="s1">FutureWarning</span><span class="s2">,</span>
        <span class="s1">match=(</span>
            <span class="s4">f&quot;algorithm='</span><span class="s2">{</span><span class="s1">algorithm</span><span class="s2">}</span><span class="s4">' is deprecated, it will &quot;</span>
            <span class="s4">&quot;be removed in 1.3. Using 'lloyd' instead.&quot;</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">kmeans.fit(X)</span>
        <span class="s2">assert </span><span class="s1">kmeans._algorithm == </span><span class="s4">&quot;lloyd&quot;</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_predict_sample_weight_deprecation_warning(Estimator):</span>
    <span class="s1">X = np.random.rand(</span><span class="s5">100</span><span class="s2">, </span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">sample_weight = np.random.uniform(size=</span><span class="s5">100</span><span class="s1">)</span>
    <span class="s1">kmeans = Estimator()</span>
    <span class="s1">kmeans.fit(X</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">warn_msg = (</span>
        <span class="s4">&quot;'sample_weight' was deprecated in version 1.3 and will be removed in 1.5.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s2">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">kmeans.predict(X</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>


<span class="s2">def </span><span class="s1">test_minibatch_update_consistency(global_random_seed):</span>
    <span class="s3"># Check that dense and sparse minibatch update give the same results</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>

    <span class="s1">centers_old = centers + rng.normal(size=centers.shape)</span>
    <span class="s1">centers_old_csr = centers_old.copy()</span>

    <span class="s1">centers_new = np.zeros_like(centers_old)</span>
    <span class="s1">centers_new_csr = np.zeros_like(centers_old_csr)</span>

    <span class="s1">weight_sums = np.zeros(centers_old.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>
    <span class="s1">weight_sums_csr = np.zeros(centers_old.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s1">sample_weight = np.ones(X.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s3"># extract a small minibatch</span>
    <span class="s1">X_mb = X[:</span><span class="s5">10</span><span class="s1">]</span>
    <span class="s1">X_mb_csr = X_csr[:</span><span class="s5">10</span><span class="s1">]</span>
    <span class="s1">sample_weight_mb = sample_weight[:</span><span class="s5">10</span><span class="s1">]</span>

    <span class="s3"># step 1: compute the dense minibatch update</span>
    <span class="s1">old_inertia = _mini_batch_step(</span>
        <span class="s1">X_mb</span><span class="s2">,</span>
        <span class="s1">sample_weight_mb</span><span class="s2">,</span>
        <span class="s1">centers_old</span><span class="s2">,</span>
        <span class="s1">centers_new</span><span class="s2">,</span>
        <span class="s1">weight_sums</span><span class="s2">,</span>
        <span class="s1">np.random.RandomState(global_random_seed)</span><span class="s2">,</span>
        <span class="s1">random_reassign=</span><span class="s2">False,</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">old_inertia &gt; </span><span class="s5">0.0</span>

    <span class="s3"># compute the new inertia on the same batch to check that it decreased</span>
    <span class="s1">labels</span><span class="s2">, </span><span class="s1">new_inertia = _labels_inertia(X_mb</span><span class="s2">, </span><span class="s1">sample_weight_mb</span><span class="s2">, </span><span class="s1">centers_new)</span>
    <span class="s2">assert </span><span class="s1">new_inertia &gt; </span><span class="s5">0.0</span>
    <span class="s2">assert </span><span class="s1">new_inertia &lt; old_inertia</span>

    <span class="s3"># step 2: compute the sparse minibatch update</span>
    <span class="s1">old_inertia_csr = _mini_batch_step(</span>
        <span class="s1">X_mb_csr</span><span class="s2">,</span>
        <span class="s1">sample_weight_mb</span><span class="s2">,</span>
        <span class="s1">centers_old_csr</span><span class="s2">,</span>
        <span class="s1">centers_new_csr</span><span class="s2">,</span>
        <span class="s1">weight_sums_csr</span><span class="s2">,</span>
        <span class="s1">np.random.RandomState(global_random_seed)</span><span class="s2">,</span>
        <span class="s1">random_reassign=</span><span class="s2">False,</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">old_inertia_csr &gt; </span><span class="s5">0.0</span>

    <span class="s3"># compute the new inertia on the same batch to check that it decreased</span>
    <span class="s1">labels_csr</span><span class="s2">, </span><span class="s1">new_inertia_csr = _labels_inertia(</span>
        <span class="s1">X_mb_csr</span><span class="s2">, </span><span class="s1">sample_weight_mb</span><span class="s2">, </span><span class="s1">centers_new_csr</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">new_inertia_csr &gt; </span><span class="s5">0.0</span>
    <span class="s2">assert </span><span class="s1">new_inertia_csr &lt; old_inertia_csr</span>

    <span class="s3"># step 3: check that sparse and dense updates lead to the same results</span>
    <span class="s1">assert_array_equal(labels</span><span class="s2">, </span><span class="s1">labels_csr)</span>
    <span class="s1">assert_allclose(centers_new</span><span class="s2">, </span><span class="s1">centers_new_csr)</span>
    <span class="s1">assert_allclose(old_inertia</span><span class="s2">, </span><span class="s1">old_inertia_csr)</span>
    <span class="s1">assert_allclose(new_inertia</span><span class="s2">, </span><span class="s1">new_inertia_csr)</span>


<span class="s2">def </span><span class="s1">_check_fitted_model(km):</span>
    <span class="s3"># check that the number of clusters centers and distinct labels match</span>
    <span class="s3"># the expectation</span>
    <span class="s1">centers = km.cluster_centers_</span>
    <span class="s2">assert </span><span class="s1">centers.shape == (n_clusters</span><span class="s2">, </span><span class="s1">n_features)</span>

    <span class="s1">labels = km.labels_</span>
    <span class="s2">assert </span><span class="s1">np.unique(labels).shape[</span><span class="s5">0</span><span class="s1">] == n_clusters</span>

    <span class="s3"># check that the labels assignment are perfect (up to a permutation)</span>
    <span class="s1">assert_allclose(v_measure_score(true_labels</span><span class="s2">, </span><span class="s1">labels)</span><span class="s2">, </span><span class="s5">1.0</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">km.inertia_ &gt; </span><span class="s5">0.0</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;data&quot;</span><span class="s2">, </span><span class="s1">[X</span><span class="s2">, </span><span class="s1">X_csr]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;dense&quot;</span><span class="s2">, </span><span class="s4">&quot;sparse&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;init&quot;</span><span class="s2">,</span>
    <span class="s1">[</span><span class="s4">&quot;random&quot;</span><span class="s2">, </span><span class="s4">&quot;k-means++&quot;</span><span class="s2">, </span><span class="s1">centers</span><span class="s2">, lambda </span><span class="s1">X</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">random_state: centers]</span><span class="s2">,</span>
    <span class="s1">ids=[</span><span class="s4">&quot;random&quot;</span><span class="s2">, </span><span class="s4">&quot;k-means++&quot;</span><span class="s2">, </span><span class="s4">&quot;ndarray&quot;</span><span class="s2">, </span><span class="s4">&quot;callable&quot;</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_all_init(Estimator</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">init):</span>
    <span class="s3"># Check KMeans and MiniBatchKMeans with all possible init.</span>
    <span class="s1">n_init = </span><span class="s5">10 </span><span class="s2">if </span><span class="s1">isinstance(init</span><span class="s2">, </span><span class="s1">str) </span><span class="s2">else </span><span class="s5">1</span>
    <span class="s1">km = Estimator(</span>
        <span class="s1">init=init</span><span class="s2">, </span><span class="s1">n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">42</span><span class="s2">, </span><span class="s1">n_init=n_init</span>
    <span class="s1">).fit(data)</span>
    <span class="s1">_check_fitted_model(km)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;init&quot;</span><span class="s2">,</span>
    <span class="s1">[</span><span class="s4">&quot;random&quot;</span><span class="s2">, </span><span class="s4">&quot;k-means++&quot;</span><span class="s2">, </span><span class="s1">centers</span><span class="s2">, lambda </span><span class="s1">X</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">random_state: centers]</span><span class="s2">,</span>
    <span class="s1">ids=[</span><span class="s4">&quot;random&quot;</span><span class="s2">, </span><span class="s4">&quot;k-means++&quot;</span><span class="s2">, </span><span class="s4">&quot;ndarray&quot;</span><span class="s2">, </span><span class="s4">&quot;callable&quot;</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_minibatch_kmeans_partial_fit_init(init):</span>
    <span class="s3"># Check MiniBatchKMeans init with partial_fit</span>
    <span class="s1">n_init = </span><span class="s5">10 </span><span class="s2">if </span><span class="s1">isinstance(init</span><span class="s2">, </span><span class="s1">str) </span><span class="s2">else </span><span class="s5">1</span>
    <span class="s1">km = MiniBatchKMeans(</span>
        <span class="s1">init=init</span><span class="s2">, </span><span class="s1">n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">n_init=n_init</span>
    <span class="s1">)</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(</span><span class="s5">100</span><span class="s1">):</span>
        <span class="s3"># &quot;random&quot; init requires many batches to recover the true labels.</span>
        <span class="s1">km.partial_fit(X)</span>
    <span class="s1">_check_fitted_model(km)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;init, expected_n_init&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s4">&quot;k-means++&quot;</span><span class="s2">, </span><span class="s5">1</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s4">&quot;random&quot;</span><span class="s2">, </span><span class="s4">&quot;default&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s2">lambda </span><span class="s1">X</span><span class="s2">, </span><span class="s1">n_clusters</span><span class="s2">, </span><span class="s1">random_state: random_state.uniform(</span>
                <span class="s1">size=(n_clusters</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">])</span>
            <span class="s1">)</span><span class="s2">,</span>
            <span class="s4">&quot;default&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s4">&quot;array-like&quot;</span><span class="s2">, </span><span class="s5">1</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_kmeans_init_auto_with_initial_centroids(Estimator</span><span class="s2">, </span><span class="s1">init</span><span class="s2">, </span><span class="s1">expected_n_init):</span>
    <span class="s0">&quot;&quot;&quot;Check that `n_init=&quot;auto&quot;` chooses the right number of initializations. 
    Non-regression test for #26657: 
    https://github.com/scikit-learn/scikit-learn/pull/26657 
    &quot;&quot;&quot;</span>
    <span class="s1">n_sample</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_clusters = </span><span class="s5">100</span><span class="s2">, </span><span class="s5">10</span><span class="s2">, </span><span class="s5">5</span>
    <span class="s1">X = np.random.randn(n_sample</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s2">if </span><span class="s1">init == </span><span class="s4">&quot;array-like&quot;</span><span class="s1">:</span>
        <span class="s1">init = np.random.randn(n_clusters</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s2">if </span><span class="s1">expected_n_init == </span><span class="s4">&quot;default&quot;</span><span class="s1">:</span>
        <span class="s1">expected_n_init = </span><span class="s5">3 </span><span class="s2">if </span><span class="s1">Estimator </span><span class="s2">is </span><span class="s1">MiniBatchKMeans </span><span class="s2">else </span><span class="s5">10</span>

    <span class="s1">kmeans = Estimator(n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">init=init</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s4">&quot;auto&quot;</span><span class="s1">).fit(X)</span>
    <span class="s2">assert </span><span class="s1">kmeans._n_init == expected_n_init</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_fortran_aligned_data(Estimator</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check that KMeans works with fortran-aligned data.</span>
    <span class="s1">X_fortran = np.asfortranarray(X)</span>
    <span class="s1">centers_fortran = np.asfortranarray(centers)</span>

    <span class="s1">km_c = Estimator(</span>
        <span class="s1">n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">init=centers</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">).fit(X)</span>
    <span class="s1">km_f = Estimator(</span>
        <span class="s1">n_clusters=n_clusters</span><span class="s2">,</span>
        <span class="s1">init=centers_fortran</span><span class="s2">,</span>
        <span class="s1">n_init=</span><span class="s5">1</span><span class="s2">,</span>
        <span class="s1">random_state=global_random_seed</span><span class="s2">,</span>
    <span class="s1">).fit(X_fortran)</span>
    <span class="s1">assert_allclose(km_c.cluster_centers_</span><span class="s2">, </span><span class="s1">km_f.cluster_centers_)</span>
    <span class="s1">assert_array_equal(km_c.labels_</span><span class="s2">, </span><span class="s1">km_f.labels_)</span>


<span class="s2">def </span><span class="s1">test_minibatch_kmeans_verbose():</span>
    <span class="s3"># Check verbose mode of MiniBatchKMeans for better coverage.</span>
    <span class="s1">km = MiniBatchKMeans(n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">42</span><span class="s2">, </span><span class="s1">verbose=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">old_stdout = sys.stdout</span>
    <span class="s1">sys.stdout = StringIO()</span>
    <span class="s2">try</span><span class="s1">:</span>
        <span class="s1">km.fit(X)</span>
    <span class="s2">finally</span><span class="s1">:</span>
        <span class="s1">sys.stdout = old_stdout</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;algorithm&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;lloyd&quot;</span><span class="s2">, </span><span class="s4">&quot;elkan&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;tol&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1e-2</span><span class="s2">, </span><span class="s5">0</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_kmeans_verbose(algorithm</span><span class="s2">, </span><span class="s1">tol</span><span class="s2">, </span><span class="s1">capsys):</span>
    <span class="s3"># Check verbose mode of KMeans for better coverage.</span>
    <span class="s1">X = np.random.RandomState(</span><span class="s5">0</span><span class="s1">).normal(size=(</span><span class="s5">5000</span><span class="s2">, </span><span class="s5">10</span><span class="s1">))</span>

    <span class="s1">KMeans(</span>
        <span class="s1">algorithm=algorithm</span><span class="s2">,</span>
        <span class="s1">n_clusters=n_clusters</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s5">42</span><span class="s2">,</span>
        <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s2">,</span>
        <span class="s1">n_init=</span><span class="s5">1</span><span class="s2">,</span>
        <span class="s1">tol=tol</span><span class="s2">,</span>
        <span class="s1">verbose=</span><span class="s5">1</span><span class="s2">,</span>
    <span class="s1">).fit(X)</span>

    <span class="s1">captured = capsys.readouterr()</span>

    <span class="s2">assert </span><span class="s1">re.search(</span><span class="s4">r&quot;Initialization complete&quot;</span><span class="s2">, </span><span class="s1">captured.out)</span>
    <span class="s2">assert </span><span class="s1">re.search(</span><span class="s4">r&quot;Iteration [0-9]+, inertia&quot;</span><span class="s2">, </span><span class="s1">captured.out)</span>

    <span class="s2">if </span><span class="s1">tol == </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s1">re.search(</span><span class="s4">r&quot;strict convergence&quot;</span><span class="s2">, </span><span class="s1">captured.out)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s1">re.search(</span><span class="s4">r&quot;center shift .* within tolerance&quot;</span><span class="s2">, </span><span class="s1">captured.out)</span>


<span class="s2">def </span><span class="s1">test_minibatch_kmeans_warning_init_size():</span>
    <span class="s3"># Check that a warning is raised when init_size is smaller than n_clusters</span>
    <span class="s2">with </span><span class="s1">pytest.warns(</span>
        <span class="s1">RuntimeWarning</span><span class="s2">, </span><span class="s1">match=</span><span class="s4">r&quot;init_size.* should be larger than n_clusters&quot;</span>
    <span class="s1">):</span>
        <span class="s1">MiniBatchKMeans(init_size=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">n_clusters=</span><span class="s5">20</span><span class="s1">).fit(X)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_warning_n_init_precomputed_centers(Estimator):</span>
    <span class="s3"># Check that a warning is raised when n_init &gt; 1 and an array is passed for</span>
    <span class="s3"># the init parameter.</span>
    <span class="s2">with </span><span class="s1">pytest.warns(</span>
        <span class="s1">RuntimeWarning</span><span class="s2">,</span>
        <span class="s1">match=</span><span class="s4">&quot;Explicit initial center position passed: performing only one init&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">Estimator(init=centers</span><span class="s2">, </span><span class="s1">n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">10</span><span class="s1">).fit(X)</span>


<span class="s2">def </span><span class="s1">test_minibatch_sensible_reassign(global_random_seed):</span>
    <span class="s3"># check that identical initial clusters are reassigned</span>
    <span class="s3"># also a regression test for when there are more desired reassignments than</span>
    <span class="s3"># samples.</span>
    <span class="s1">zeroed_X</span><span class="s2">, </span><span class="s1">true_labels = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s5">100</span><span class="s2">, </span><span class="s1">centers=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>
    <span class="s1">zeroed_X[::</span><span class="s5">2</span><span class="s2">, </span><span class="s1">:] = </span><span class="s5">0</span>

    <span class="s1">km = MiniBatchKMeans(</span>
        <span class="s1">n_clusters=</span><span class="s5">20</span><span class="s2">, </span><span class="s1">batch_size=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span><span class="s2">, </span><span class="s1">init=</span><span class="s4">&quot;random&quot;</span>
    <span class="s1">).fit(zeroed_X)</span>
    <span class="s3"># there should not be too many exact zero cluster centers</span>
    <span class="s2">assert </span><span class="s1">km.cluster_centers_.any(axis=</span><span class="s5">1</span><span class="s1">).sum() &gt; </span><span class="s5">10</span>

    <span class="s3"># do the same with batch-size &gt; X.shape[0] (regression test)</span>
    <span class="s1">km = MiniBatchKMeans(</span>
        <span class="s1">n_clusters=</span><span class="s5">20</span><span class="s2">, </span><span class="s1">batch_size=</span><span class="s5">200</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span><span class="s2">, </span><span class="s1">init=</span><span class="s4">&quot;random&quot;</span>
    <span class="s1">).fit(zeroed_X)</span>
    <span class="s3"># there should not be too many exact zero cluster centers</span>
    <span class="s2">assert </span><span class="s1">km.cluster_centers_.any(axis=</span><span class="s5">1</span><span class="s1">).sum() &gt; </span><span class="s5">10</span>

    <span class="s3"># do the same with partial_fit API</span>
    <span class="s1">km = MiniBatchKMeans(n_clusters=</span><span class="s5">20</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span><span class="s2">, </span><span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s1">)</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(</span><span class="s5">100</span><span class="s1">):</span>
        <span class="s1">km.partial_fit(zeroed_X)</span>
    <span class="s3"># there should not be too many exact zero cluster centers</span>
    <span class="s2">assert </span><span class="s1">km.cluster_centers_.any(axis=</span><span class="s5">1</span><span class="s1">).sum() &gt; </span><span class="s5">10</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;data&quot;</span><span class="s2">, </span><span class="s1">[X</span><span class="s2">, </span><span class="s1">X_csr]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;dense&quot;</span><span class="s2">, </span><span class="s4">&quot;sparse&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_minibatch_reassign(data</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check the reassignment part of the minibatch step with very high or very</span>
    <span class="s3"># low reassignment ratio.</span>
    <span class="s1">perfect_centers = np.empty((n_clusters</span><span class="s2">, </span><span class="s1">n_features))</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_clusters):</span>
        <span class="s1">perfect_centers[i] = X[true_labels == i].mean(axis=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">sample_weight = np.ones(n_samples)</span>
    <span class="s1">centers_new = np.empty_like(perfect_centers)</span>

    <span class="s3"># Give a perfect initialization, but a large reassignment_ratio, as a</span>
    <span class="s3"># result many centers should be reassigned and the model should no longer</span>
    <span class="s3"># be good</span>
    <span class="s1">score_before = -_labels_inertia(data</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">perfect_centers</span><span class="s2">, </span><span class="s5">1</span><span class="s1">)[</span><span class="s5">1</span><span class="s1">]</span>

    <span class="s1">_mini_batch_step(</span>
        <span class="s1">data</span><span class="s2">,</span>
        <span class="s1">sample_weight</span><span class="s2">,</span>
        <span class="s1">perfect_centers</span><span class="s2">,</span>
        <span class="s1">centers_new</span><span class="s2">,</span>
        <span class="s1">np.zeros(n_clusters)</span><span class="s2">,</span>
        <span class="s1">np.random.RandomState(global_random_seed)</span><span class="s2">,</span>
        <span class="s1">random_reassign=</span><span class="s2">True,</span>
        <span class="s1">reassignment_ratio=</span><span class="s5">1</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">score_after = -_labels_inertia(data</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">centers_new</span><span class="s2">, </span><span class="s5">1</span><span class="s1">)[</span><span class="s5">1</span><span class="s1">]</span>

    <span class="s2">assert </span><span class="s1">score_before &gt; score_after</span>

    <span class="s3"># Give a perfect initialization, with a small reassignment_ratio,</span>
    <span class="s3"># no center should be reassigned.</span>
    <span class="s1">_mini_batch_step(</span>
        <span class="s1">data</span><span class="s2">,</span>
        <span class="s1">sample_weight</span><span class="s2">,</span>
        <span class="s1">perfect_centers</span><span class="s2">,</span>
        <span class="s1">centers_new</span><span class="s2">,</span>
        <span class="s1">np.zeros(n_clusters)</span><span class="s2">,</span>
        <span class="s1">np.random.RandomState(global_random_seed)</span><span class="s2">,</span>
        <span class="s1">random_reassign=</span><span class="s2">True,</span>
        <span class="s1">reassignment_ratio=</span><span class="s5">1e-15</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">assert_allclose(centers_new</span><span class="s2">, </span><span class="s1">perfect_centers)</span>


<span class="s2">def </span><span class="s1">test_minibatch_with_many_reassignments():</span>
    <span class="s3"># Test for the case that the number of clusters to reassign is bigger</span>
    <span class="s3"># than the batch_size. Run the test with 100 clusters and a batch_size of</span>
    <span class="s3"># 10 because it turned out that these values ensure that the number of</span>
    <span class="s3"># clusters to reassign is always bigger than the batch_size.</span>
    <span class="s1">MiniBatchKMeans(</span>
        <span class="s1">n_clusters=</span><span class="s5">100</span><span class="s2">,</span>
        <span class="s1">batch_size=</span><span class="s5">10</span><span class="s2">,</span>
        <span class="s1">init_size=n_samples</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s5">42</span><span class="s2">,</span>
        <span class="s1">verbose=</span><span class="s2">True,</span>
    <span class="s1">).fit(X)</span>


<span class="s2">def </span><span class="s1">test_minibatch_kmeans_init_size():</span>
    <span class="s3"># Check the internal _init_size attribute of MiniBatchKMeans</span>

    <span class="s3"># default init size should be 3 * batch_size</span>
    <span class="s1">km = MiniBatchKMeans(n_clusters=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">batch_size=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s1">).fit(X)</span>
    <span class="s2">assert </span><span class="s1">km._init_size == </span><span class="s5">15</span>

    <span class="s3"># if 3 * batch size &lt; n_clusters, it should then be 3 * n_clusters</span>
    <span class="s1">km = MiniBatchKMeans(n_clusters=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">batch_size=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s1">).fit(X)</span>
    <span class="s2">assert </span><span class="s1">km._init_size == </span><span class="s5">30</span>

    <span class="s3"># it should not be larger than n_samples</span>
    <span class="s1">km = MiniBatchKMeans(</span>
        <span class="s1">n_clusters=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">batch_size=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">init_size=n_samples + </span><span class="s5">1</span>
    <span class="s1">).fit(X)</span>
    <span class="s2">assert </span><span class="s1">km._init_size == n_samples</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;tol, max_no_improvement&quot;</span><span class="s2">, </span><span class="s1">[(</span><span class="s5">1e-4</span><span class="s2">, None</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">0</span><span class="s2">, </span><span class="s5">10</span><span class="s1">)])</span>
<span class="s2">def </span><span class="s1">test_minibatch_declared_convergence(capsys</span><span class="s2">, </span><span class="s1">tol</span><span class="s2">, </span><span class="s1">max_no_improvement):</span>
    <span class="s3"># Check convergence detection based on ewa batch inertia or on</span>
    <span class="s3"># small center change.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">centers = make_blobs(centers=</span><span class="s5">3</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">return_centers=</span><span class="s2">True</span><span class="s1">)</span>

    <span class="s1">km = MiniBatchKMeans(</span>
        <span class="s1">n_clusters=</span><span class="s5">3</span><span class="s2">,</span>
        <span class="s1">init=centers</span><span class="s2">,</span>
        <span class="s1">batch_size=</span><span class="s5">20</span><span class="s2">,</span>
        <span class="s1">tol=tol</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s5">0</span><span class="s2">,</span>
        <span class="s1">max_iter=</span><span class="s5">10</span><span class="s2">,</span>
        <span class="s1">n_init=</span><span class="s5">1</span><span class="s2">,</span>
        <span class="s1">verbose=</span><span class="s5">1</span><span class="s2">,</span>
        <span class="s1">max_no_improvement=max_no_improvement</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">km.fit(X)</span>
    <span class="s2">assert </span><span class="s5">1 </span><span class="s1">&lt; km.n_iter_ &lt; </span><span class="s5">10</span>

    <span class="s1">captured = capsys.readouterr()</span>
    <span class="s2">if </span><span class="s1">max_no_improvement </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s4">&quot;Converged (small centers change)&quot; </span><span class="s2">in </span><span class="s1">captured.out</span>
    <span class="s2">if </span><span class="s1">tol == </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s4">&quot;Converged (lack of improvement in inertia)&quot; </span><span class="s2">in </span><span class="s1">captured.out</span>


<span class="s2">def </span><span class="s1">test_minibatch_iter_steps():</span>
    <span class="s3"># Check consistency of n_iter_ and n_steps_ attributes.</span>
    <span class="s1">batch_size = </span><span class="s5">30</span>
    <span class="s1">n_samples = X.shape[</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">km = MiniBatchKMeans(n_clusters=</span><span class="s5">3</span><span class="s2">, </span><span class="s1">batch_size=batch_size</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">).fit(X)</span>

    <span class="s3"># n_iter_ is the number of started epochs</span>
    <span class="s2">assert </span><span class="s1">km.n_iter_ == np.ceil((km.n_steps_ * batch_size) / n_samples)</span>
    <span class="s2">assert </span><span class="s1">isinstance(km.n_iter_</span><span class="s2">, </span><span class="s1">int)</span>

    <span class="s3"># without stopping condition, max_iter should be reached</span>
    <span class="s1">km = MiniBatchKMeans(</span>
        <span class="s1">n_clusters=</span><span class="s5">3</span><span class="s2">,</span>
        <span class="s1">batch_size=batch_size</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s5">0</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s5">0</span><span class="s2">,</span>
        <span class="s1">max_no_improvement=</span><span class="s2">None,</span>
        <span class="s1">max_iter=</span><span class="s5">10</span><span class="s2">,</span>
    <span class="s1">).fit(X)</span>

    <span class="s2">assert </span><span class="s1">km.n_iter_ == </span><span class="s5">10</span>
    <span class="s2">assert </span><span class="s1">km.n_steps_ == (</span><span class="s5">10 </span><span class="s1">* n_samples) // batch_size</span>
    <span class="s2">assert </span><span class="s1">isinstance(km.n_steps_</span><span class="s2">, </span><span class="s1">int)</span>


<span class="s2">def </span><span class="s1">test_kmeans_copyx():</span>
    <span class="s3"># Check that copy_x=False returns nearly equal X after de-centering.</span>
    <span class="s1">my_X = X.copy()</span>
    <span class="s1">km = KMeans(copy_x=</span><span class="s2">False, </span><span class="s1">n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">42</span><span class="s1">)</span>
    <span class="s1">km.fit(my_X)</span>
    <span class="s1">_check_fitted_model(km)</span>

    <span class="s3"># check that my_X is de-centered</span>
    <span class="s1">assert_allclose(my_X</span><span class="s2">, </span><span class="s1">X)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_score_max_iter(Estimator</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check that fitting KMeans or MiniBatchKMeans with more iterations gives</span>
    <span class="s3"># better score</span>
    <span class="s1">X = np.random.RandomState(global_random_seed).randn(</span><span class="s5">100</span><span class="s2">, </span><span class="s5">10</span><span class="s1">)</span>

    <span class="s1">km1 = Estimator(n_init=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">s1 = km1.fit(X).score(X)</span>
    <span class="s1">km2 = Estimator(n_init=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s5">10</span><span class="s1">)</span>
    <span class="s1">s2 = km2.fit(X).score(X)</span>
    <span class="s2">assert </span><span class="s1">s2 &gt; s1</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;array_constr&quot;</span><span class="s2">, </span><span class="s1">[np.array</span><span class="s2">, </span><span class="s1">sp.csr_matrix]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;dense&quot;</span><span class="s2">, </span><span class="s4">&quot;sparse&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;Estimator, algorithm&quot;</span><span class="s2">,</span>
    <span class="s1">[(KMeans</span><span class="s2">, </span><span class="s4">&quot;lloyd&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(KMeans</span><span class="s2">, </span><span class="s4">&quot;elkan&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(MiniBatchKMeans</span><span class="s2">, None</span><span class="s1">)]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;max_iter&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">2</span><span class="s2">, </span><span class="s5">100</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_kmeans_predict(</span>
    <span class="s1">Estimator</span><span class="s2">, </span><span class="s1">algorithm</span><span class="s2">, </span><span class="s1">array_constr</span><span class="s2">, </span><span class="s1">max_iter</span><span class="s2">, </span><span class="s1">global_dtype</span><span class="s2">, </span><span class="s1">global_random_seed</span>
<span class="s1">):</span>
    <span class="s3"># Check the predict method and the equivalence between fit.predict and</span>
    <span class="s3"># fit_predict.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">_ = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s5">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">centers=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>
    <span class="s1">X = array_constr(X</span><span class="s2">, </span><span class="s1">dtype=global_dtype)</span>

    <span class="s1">km = Estimator(</span>
        <span class="s1">n_clusters=</span><span class="s5">10</span><span class="s2">,</span>
        <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s2">,</span>
        <span class="s1">n_init=</span><span class="s5">10</span><span class="s2">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
        <span class="s1">random_state=global_random_seed</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">if </span><span class="s1">algorithm </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">km.set_params(algorithm=algorithm)</span>
    <span class="s1">km.fit(X)</span>
    <span class="s1">labels = km.labels_</span>

    <span class="s3"># re-predict labels for training set using predict</span>
    <span class="s1">pred = km.predict(X)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s2">, </span><span class="s1">labels)</span>

    <span class="s3"># re-predict labels for training set using fit_predict</span>
    <span class="s1">pred = km.fit_predict(X)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s2">, </span><span class="s1">labels)</span>

    <span class="s3"># predict centroid labels</span>
    <span class="s1">pred = km.predict(km.cluster_centers_)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s2">, </span><span class="s1">np.arange(</span><span class="s5">10</span><span class="s1">))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_dense_sparse(Estimator</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check that the results are the same for dense and sparse input.</span>
    <span class="s1">sample_weight = np.random.RandomState(global_random_seed).random_sample(</span>
        <span class="s1">(n_samples</span><span class="s2">,</span><span class="s1">)</span>
    <span class="s1">)</span>
    <span class="s1">km_dense = Estimator(</span>
        <span class="s1">n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span>
    <span class="s1">)</span>
    <span class="s1">km_dense.fit(X</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">km_sparse = Estimator(</span>
        <span class="s1">n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span>
    <span class="s1">)</span>
    <span class="s1">km_sparse.fit(X_csr</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">assert_array_equal(km_dense.labels_</span><span class="s2">, </span><span class="s1">km_sparse.labels_)</span>
    <span class="s1">assert_allclose(km_dense.cluster_centers_</span><span class="s2">, </span><span class="s1">km_sparse.cluster_centers_)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;init&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;random&quot;</span><span class="s2">, </span><span class="s4">&quot;k-means++&quot;</span><span class="s2">, </span><span class="s1">centers]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;random&quot;</span><span class="s2">, </span><span class="s4">&quot;k-means++&quot;</span><span class="s2">, </span><span class="s4">&quot;ndarray&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_predict_dense_sparse(Estimator</span><span class="s2">, </span><span class="s1">init):</span>
    <span class="s3"># check that models trained on sparse input also works for dense input at</span>
    <span class="s3"># predict time and vice versa.</span>
    <span class="s1">n_init = </span><span class="s5">10 </span><span class="s2">if </span><span class="s1">isinstance(init</span><span class="s2">, </span><span class="s1">str) </span><span class="s2">else </span><span class="s5">1</span>
    <span class="s1">km = Estimator(n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">init=init</span><span class="s2">, </span><span class="s1">n_init=n_init</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">km.fit(X_csr)</span>
    <span class="s1">assert_array_equal(km.predict(X)</span><span class="s2">, </span><span class="s1">km.labels_)</span>

    <span class="s1">km.fit(X)</span>
    <span class="s1">assert_array_equal(km.predict(X_csr)</span><span class="s2">, </span><span class="s1">km.labels_)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;array_constr&quot;</span><span class="s2">, </span><span class="s1">[np.array</span><span class="s2">, </span><span class="s1">sp.csr_matrix]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;dense&quot;</span><span class="s2">, </span><span class="s4">&quot;sparse&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;dtype&quot;</span><span class="s2">, </span><span class="s1">[np.int32</span><span class="s2">, </span><span class="s1">np.int64])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;init&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;k-means++&quot;</span><span class="s2">, </span><span class="s4">&quot;ndarray&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_integer_input(Estimator</span><span class="s2">, </span><span class="s1">array_constr</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">init</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check that KMeans and MiniBatchKMeans work with integer input.</span>
    <span class="s1">X_dense = np.array([[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">10</span><span class="s2">, </span><span class="s5">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">12</span><span class="s2">, </span><span class="s5">9</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">2</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">8</span><span class="s2">, </span><span class="s5">10</span><span class="s1">]])</span>
    <span class="s1">X = array_constr(X_dense</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>

    <span class="s1">n_init = </span><span class="s5">1 </span><span class="s2">if </span><span class="s1">init == </span><span class="s4">&quot;ndarray&quot; </span><span class="s2">else </span><span class="s5">10</span>
    <span class="s1">init = X_dense[:</span><span class="s5">2</span><span class="s1">] </span><span class="s2">if </span><span class="s1">init == </span><span class="s4">&quot;ndarray&quot; </span><span class="s2">else </span><span class="s1">init</span>

    <span class="s1">km = Estimator(</span>
        <span class="s1">n_clusters=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">init=init</span><span class="s2">, </span><span class="s1">n_init=n_init</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>
    <span class="s2">if </span><span class="s1">Estimator </span><span class="s2">is </span><span class="s1">MiniBatchKMeans:</span>
        <span class="s1">km.set_params(batch_size=</span><span class="s5">2</span><span class="s1">)</span>

    <span class="s1">km.fit(X)</span>

    <span class="s3"># Internally integer input should be converted to float64</span>
    <span class="s2">assert </span><span class="s1">km.cluster_centers_.dtype == np.float64</span>

    <span class="s1">expected_labels = [</span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">assert_allclose(v_measure_score(km.labels_</span><span class="s2">, </span><span class="s1">expected_labels)</span><span class="s2">, </span><span class="s5">1.0</span><span class="s1">)</span>

    <span class="s3"># Same with partial_fit (#14314)</span>
    <span class="s2">if </span><span class="s1">Estimator </span><span class="s2">is </span><span class="s1">MiniBatchKMeans:</span>
        <span class="s1">km = clone(km).partial_fit(X)</span>
        <span class="s2">assert </span><span class="s1">km.cluster_centers_.dtype == np.float64</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_transform(Estimator</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check the transform method</span>
    <span class="s1">km = Estimator(n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">random_state=global_random_seed).fit(X)</span>

    <span class="s3"># Transorfming cluster_centers_ should return the pairwise distances</span>
    <span class="s3"># between centers</span>
    <span class="s1">Xt = km.transform(km.cluster_centers_)</span>
    <span class="s1">assert_allclose(Xt</span><span class="s2">, </span><span class="s1">pairwise_distances(km.cluster_centers_))</span>
    <span class="s3"># In particular, diagonal must be 0</span>
    <span class="s1">assert_array_equal(Xt.diagonal()</span><span class="s2">, </span><span class="s1">np.zeros(n_clusters))</span>

    <span class="s3"># Transorfming X should return the pairwise distances between X and the</span>
    <span class="s3"># centers</span>
    <span class="s1">Xt = km.transform(X)</span>
    <span class="s1">assert_allclose(Xt</span><span class="s2">, </span><span class="s1">pairwise_distances(X</span><span class="s2">, </span><span class="s1">km.cluster_centers_))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_fit_transform(Estimator</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check equivalence between fit.transform and fit_transform</span>
    <span class="s1">X1 = Estimator(random_state=global_random_seed</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s1">).fit(X).transform(X)</span>
    <span class="s1">X2 = Estimator(random_state=global_random_seed</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s1">).fit_transform(X)</span>
    <span class="s1">assert_allclose(X1</span><span class="s2">, </span><span class="s1">X2)</span>


<span class="s2">def </span><span class="s1">test_n_init(global_random_seed):</span>
    <span class="s3"># Check that increasing the number of init increases the quality</span>
    <span class="s1">previous_inertia = np.inf</span>
    <span class="s2">for </span><span class="s1">n_init </span><span class="s2">in </span><span class="s1">[</span><span class="s5">1</span><span class="s2">, </span><span class="s5">5</span><span class="s2">, </span><span class="s5">10</span><span class="s1">]:</span>
        <span class="s3"># set max_iter=1 to avoid finding the global minimum and get the same</span>
        <span class="s3"># inertia each time</span>
        <span class="s1">km = KMeans(</span>
            <span class="s1">n_clusters=n_clusters</span><span class="s2">,</span>
            <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s2">,</span>
            <span class="s1">n_init=n_init</span><span class="s2">,</span>
            <span class="s1">random_state=global_random_seed</span><span class="s2">,</span>
            <span class="s1">max_iter=</span><span class="s5">1</span><span class="s2">,</span>
        <span class="s1">).fit(X)</span>
        <span class="s2">assert </span><span class="s1">km.inertia_ &lt;= previous_inertia</span>


<span class="s2">def </span><span class="s1">test_k_means_function(global_random_seed):</span>
    <span class="s3"># test calling the k_means function directly</span>
    <span class="s1">cluster_centers</span><span class="s2">, </span><span class="s1">labels</span><span class="s2">, </span><span class="s1">inertia = k_means(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>

    <span class="s2">assert </span><span class="s1">cluster_centers.shape == (n_clusters</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s2">assert </span><span class="s1">np.unique(labels).shape[</span><span class="s5">0</span><span class="s1">] == n_clusters</span>

    <span class="s3"># check that the labels assignment are perfect (up to a permutation)</span>
    <span class="s1">assert_allclose(v_measure_score(true_labels</span><span class="s2">, </span><span class="s1">labels)</span><span class="s2">, </span><span class="s5">1.0</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">inertia &gt; </span><span class="s5">0.0</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;data&quot;</span><span class="s2">, </span><span class="s1">[X</span><span class="s2">, </span><span class="s1">X_csr]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;dense&quot;</span><span class="s2">, </span><span class="s4">&quot;sparse&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_float_precision(Estimator</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check that the results are the same for single and double precision.</span>
    <span class="s1">km = Estimator(n_init=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>

    <span class="s1">inertia = {}</span>
    <span class="s1">Xt = {}</span>
    <span class="s1">centers = {}</span>
    <span class="s1">labels = {}</span>

    <span class="s2">for </span><span class="s1">dtype </span><span class="s2">in </span><span class="s1">[np.float64</span><span class="s2">, </span><span class="s1">np.float32]:</span>
        <span class="s1">X = data.astype(dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">km.fit(X)</span>

        <span class="s1">inertia[dtype] = km.inertia_</span>
        <span class="s1">Xt[dtype] = km.transform(X)</span>
        <span class="s1">centers[dtype] = km.cluster_centers_</span>
        <span class="s1">labels[dtype] = km.labels_</span>

        <span class="s3"># dtype of cluster centers has to be the dtype of the input data</span>
        <span class="s2">assert </span><span class="s1">km.cluster_centers_.dtype == dtype</span>

        <span class="s3"># same with partial_fit</span>
        <span class="s2">if </span><span class="s1">Estimator </span><span class="s2">is </span><span class="s1">MiniBatchKMeans:</span>
            <span class="s1">km.partial_fit(X[</span><span class="s5">0</span><span class="s1">:</span><span class="s5">3</span><span class="s1">])</span>
            <span class="s2">assert </span><span class="s1">km.cluster_centers_.dtype == dtype</span>

    <span class="s3"># compare arrays with low precision since the difference between 32 and</span>
    <span class="s3"># 64 bit comes from an accumulation of rounding errors.</span>
    <span class="s1">assert_allclose(inertia[np.float32]</span><span class="s2">, </span><span class="s1">inertia[np.float64]</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-4</span><span class="s1">)</span>
    <span class="s1">assert_allclose(Xt[np.float32]</span><span class="s2">, </span><span class="s1">Xt[np.float64]</span><span class="s2">, </span><span class="s1">atol=Xt[np.float64].max() * </span><span class="s5">1e-4</span><span class="s1">)</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">centers[np.float32]</span><span class="s2">, </span><span class="s1">centers[np.float64]</span><span class="s2">, </span><span class="s1">atol=centers[np.float64].max() * </span><span class="s5">1e-4</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(labels[np.float32]</span><span class="s2">, </span><span class="s1">labels[np.float64])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;dtype&quot;</span><span class="s2">, </span><span class="s1">[np.int32</span><span class="s2">, </span><span class="s1">np.int64</span><span class="s2">, </span><span class="s1">np.float32</span><span class="s2">, </span><span class="s1">np.float64])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_centers_not_mutated(Estimator</span><span class="s2">, </span><span class="s1">dtype):</span>
    <span class="s3"># Check that KMeans and MiniBatchKMeans won't mutate the user provided</span>
    <span class="s3"># init centers silently even if input data and init centers have the same</span>
    <span class="s3"># type.</span>
    <span class="s1">X_new_type = X.astype(dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">centers_new_type = centers.astype(dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s1">km = Estimator(init=centers_new_type</span><span class="s2">, </span><span class="s1">n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">km.fit(X_new_type)</span>

    <span class="s2">assert not </span><span class="s1">np.may_share_memory(km.cluster_centers_</span><span class="s2">, </span><span class="s1">centers_new_type)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;data&quot;</span><span class="s2">, </span><span class="s1">[X</span><span class="s2">, </span><span class="s1">X_csr]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;dense&quot;</span><span class="s2">, </span><span class="s4">&quot;sparse&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_kmeans_init_fitted_centers(data):</span>
    <span class="s3"># Check that starting fitting from a local optimum shouldn't change the</span>
    <span class="s3"># solution</span>
    <span class="s1">km1 = KMeans(n_clusters=n_clusters).fit(data)</span>
    <span class="s1">km2 = KMeans(n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">init=km1.cluster_centers_</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s1">).fit(data)</span>

    <span class="s1">assert_allclose(km1.cluster_centers_</span><span class="s2">, </span><span class="s1">km2.cluster_centers_)</span>


<span class="s2">def </span><span class="s1">test_kmeans_warns_less_centers_than_unique_points(global_random_seed):</span>
    <span class="s3"># Check KMeans when the number of found clusters is smaller than expected</span>
    <span class="s1">X = np.asarray([[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]])  </span><span class="s3"># last point is duplicated</span>
    <span class="s1">km = KMeans(n_clusters=</span><span class="s5">4</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>

    <span class="s3"># KMeans should warn that fewer labels than cluster centers have been used</span>
    <span class="s1">msg = (</span>
        <span class="s4">r&quot;Number of distinct clusters \(3\) found smaller than &quot;</span>
        <span class="s4">r&quot;n_clusters \(4\). Possibly due to duplicate points in X.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">km.fit(X)</span>
        <span class="s3"># only three distinct points, so only three clusters</span>
        <span class="s3"># can have points assigned to them</span>
        <span class="s2">assert </span><span class="s1">set(km.labels_) == set(range(</span><span class="s5">3</span><span class="s1">))</span>


<span class="s2">def </span><span class="s1">_sort_centers(centers):</span>
    <span class="s2">return </span><span class="s1">np.sort(centers</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_weighted_vs_repeated(global_random_seed):</span>
    <span class="s3"># Check that a sample weight of N should yield the same result as an N-fold</span>
    <span class="s3"># repetition of the sample. Valid only if init is precomputed, otherwise</span>
    <span class="s3"># rng produces different results. Not valid for MinibatchKMeans due to rng</span>
    <span class="s3"># to extract minibatches.</span>
    <span class="s1">sample_weight = np.random.RandomState(global_random_seed).randint(</span>
        <span class="s5">1</span><span class="s2">, </span><span class="s5">5</span><span class="s2">, </span><span class="s1">size=n_samples</span>
    <span class="s1">)</span>
    <span class="s1">X_repeat = np.repeat(X</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">km = KMeans(</span>
        <span class="s1">init=centers</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>

    <span class="s1">km_weighted = clone(km).fit(X</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">repeated_labels = np.repeat(km_weighted.labels_</span><span class="s2">, </span><span class="s1">sample_weight)</span>
    <span class="s1">km_repeated = clone(km).fit(X_repeat)</span>

    <span class="s1">assert_array_equal(km_repeated.labels_</span><span class="s2">, </span><span class="s1">repeated_labels)</span>
    <span class="s1">assert_allclose(km_weighted.inertia_</span><span class="s2">, </span><span class="s1">km_repeated.inertia_)</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">_sort_centers(km_weighted.cluster_centers_)</span><span class="s2">,</span>
        <span class="s1">_sort_centers(km_repeated.cluster_centers_)</span><span class="s2">,</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;data&quot;</span><span class="s2">, </span><span class="s1">[X</span><span class="s2">, </span><span class="s1">X_csr]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;dense&quot;</span><span class="s2">, </span><span class="s4">&quot;sparse&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_unit_weights_vs_no_weights(Estimator</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check that not passing sample weights should be equivalent to passing</span>
    <span class="s3"># sample weights all equal to one.</span>
    <span class="s1">sample_weight = np.ones(n_samples)</span>

    <span class="s1">km = Estimator(n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">km_none = clone(km).fit(data</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">)</span>
    <span class="s1">km_ones = clone(km).fit(data</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">assert_array_equal(km_none.labels_</span><span class="s2">, </span><span class="s1">km_ones.labels_)</span>
    <span class="s1">assert_allclose(km_none.cluster_centers_</span><span class="s2">, </span><span class="s1">km_ones.cluster_centers_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;data&quot;</span><span class="s2">, </span><span class="s1">[X</span><span class="s2">, </span><span class="s1">X_csr]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;dense&quot;</span><span class="s2">, </span><span class="s4">&quot;sparse&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_scaled_weights(Estimator</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check that scaling all sample weights by a common factor</span>
    <span class="s3"># shouldn't change the result</span>
    <span class="s1">sample_weight = np.random.RandomState(global_random_seed).uniform(size=n_samples)</span>

    <span class="s1">km = Estimator(n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">km_orig = clone(km).fit(data</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">km_scaled = clone(km).fit(data</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s5">0.5 </span><span class="s1">* sample_weight)</span>

    <span class="s1">assert_array_equal(km_orig.labels_</span><span class="s2">, </span><span class="s1">km_scaled.labels_)</span>
    <span class="s1">assert_allclose(km_orig.cluster_centers_</span><span class="s2">, </span><span class="s1">km_scaled.cluster_centers_)</span>


<span class="s2">def </span><span class="s1">test_kmeans_elkan_iter_attribute():</span>
    <span class="s3"># Regression test on bad n_iter_ value. Previous bug n_iter_ was one off</span>
    <span class="s3"># it's right value (#11340).</span>
    <span class="s1">km = KMeans(algorithm=</span><span class="s4">&quot;elkan&quot;</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s5">1</span><span class="s1">).fit(X)</span>
    <span class="s2">assert </span><span class="s1">km.n_iter_ == </span><span class="s5">1</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;array_constr&quot;</span><span class="s2">, </span><span class="s1">[np.array</span><span class="s2">, </span><span class="s1">sp.csr_matrix]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;dense&quot;</span><span class="s2">, </span><span class="s4">&quot;sparse&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_kmeans_empty_cluster_relocated(array_constr):</span>
    <span class="s3"># check that empty clusters are correctly relocated when using sample</span>
    <span class="s3"># weights (#13486)</span>
    <span class="s1">X = array_constr([[-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">]])</span>
    <span class="s1">sample_weight = [</span><span class="s5">1.9</span><span class="s2">, </span><span class="s5">0.1</span><span class="s1">]</span>
    <span class="s1">init = np.array([[-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">10</span><span class="s1">]])</span>

    <span class="s1">km = KMeans(n_clusters=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">init=init</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">km.fit(X</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s2">assert </span><span class="s1">len(set(km.labels_)) == </span><span class="s5">2</span>
    <span class="s1">assert_allclose(km.cluster_centers_</span><span class="s2">, </span><span class="s1">[[-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">]])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_result_equal_in_diff_n_threads(Estimator</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check that KMeans/MiniBatchKMeans give the same results in parallel mode</span>
    <span class="s3"># than in sequential mode.</span>
    <span class="s1">rnd = np.random.RandomState(global_random_seed)</span>
    <span class="s1">X = rnd.normal(size=(</span><span class="s5">50</span><span class="s2">, </span><span class="s5">10</span><span class="s1">))</span>

    <span class="s2">with </span><span class="s1">threadpool_limits(limits=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">user_api=</span><span class="s4">&quot;openmp&quot;</span><span class="s1">):</span>
        <span class="s1">result_1 = (</span>
            <span class="s1">Estimator(n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>
            <span class="s1">.fit(X)</span>
            <span class="s1">.labels_</span>
        <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">threadpool_limits(limits=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">user_api=</span><span class="s4">&quot;openmp&quot;</span><span class="s1">):</span>
        <span class="s1">result_2 = (</span>
            <span class="s1">Estimator(n_clusters=n_clusters</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>
            <span class="s1">.fit(X)</span>
            <span class="s1">.labels_</span>
        <span class="s1">)</span>
    <span class="s1">assert_array_equal(result_1</span><span class="s2">, </span><span class="s1">result_2)</span>


<span class="s2">def </span><span class="s1">test_warning_elkan_1_cluster():</span>
    <span class="s3"># Check warning messages specific to KMeans</span>
    <span class="s2">with </span><span class="s1">pytest.warns(</span>
        <span class="s1">RuntimeWarning</span><span class="s2">,</span>
        <span class="s1">match=</span><span class="s4">&quot;algorithm='elkan' doesn't make sense for a single cluster&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">KMeans(n_clusters=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">algorithm=</span><span class="s4">&quot;elkan&quot;</span><span class="s1">).fit(X)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;array_constr&quot;</span><span class="s2">, </span><span class="s1">[np.array</span><span class="s2">, </span><span class="s1">sp.csr_matrix]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s4">&quot;dense&quot;</span><span class="s2">, </span><span class="s4">&quot;sparse&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;algo&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;lloyd&quot;</span><span class="s2">, </span><span class="s4">&quot;elkan&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_k_means_1_iteration(array_constr</span><span class="s2">, </span><span class="s1">algo</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># check the results after a single iteration (E-step M-step E-step) by</span>
    <span class="s3"># comparing against a pure python implementation.</span>
    <span class="s1">X = np.random.RandomState(global_random_seed).uniform(size=(</span><span class="s5">100</span><span class="s2">, </span><span class="s5">5</span><span class="s1">))</span>
    <span class="s1">init_centers = X[:</span><span class="s5">5</span><span class="s1">]</span>
    <span class="s1">X = array_constr(X)</span>

    <span class="s2">def </span><span class="s1">py_kmeans(X</span><span class="s2">, </span><span class="s1">init):</span>
        <span class="s1">new_centers = init.copy()</span>
        <span class="s1">labels = pairwise_distances_argmin(X</span><span class="s2">, </span><span class="s1">init)</span>
        <span class="s2">for </span><span class="s1">label </span><span class="s2">in </span><span class="s1">range(init.shape[</span><span class="s5">0</span><span class="s1">]):</span>
            <span class="s1">new_centers[label] = X[labels == label].mean(axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">labels = pairwise_distances_argmin(X</span><span class="s2">, </span><span class="s1">new_centers)</span>
        <span class="s2">return </span><span class="s1">labels</span><span class="s2">, </span><span class="s1">new_centers</span>

    <span class="s1">py_labels</span><span class="s2">, </span><span class="s1">py_centers = py_kmeans(X</span><span class="s2">, </span><span class="s1">init_centers)</span>

    <span class="s1">cy_kmeans = KMeans(</span>
        <span class="s1">n_clusters=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">init=init_centers</span><span class="s2">, </span><span class="s1">algorithm=algo</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s5">1</span>
    <span class="s1">).fit(X)</span>
    <span class="s1">cy_labels = cy_kmeans.labels_</span>
    <span class="s1">cy_centers = cy_kmeans.cluster_centers_</span>

    <span class="s1">assert_array_equal(py_labels</span><span class="s2">, </span><span class="s1">cy_labels)</span>
    <span class="s1">assert_allclose(py_centers</span><span class="s2">, </span><span class="s1">cy_centers)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;dtype&quot;</span><span class="s2">, </span><span class="s1">[np.float32</span><span class="s2">, </span><span class="s1">np.float64])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;squared&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_euclidean_distance(dtype</span><span class="s2">, </span><span class="s1">squared</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check that the _euclidean_(dense/sparse)_dense helpers produce correct</span>
    <span class="s3"># results</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">a_sparse = sp.random(</span>
        <span class="s5">1</span><span class="s2">, </span><span class="s5">100</span><span class="s2">, </span><span class="s1">density=</span><span class="s5">0.5</span><span class="s2">, </span><span class="s1">format=</span><span class="s4">&quot;csr&quot;</span><span class="s2">, </span><span class="s1">random_state=rng</span><span class="s2">, </span><span class="s1">dtype=dtype</span>
    <span class="s1">)</span>
    <span class="s1">a_dense = a_sparse.toarray().reshape(-</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">b = rng.randn(</span><span class="s5">100</span><span class="s1">).astype(dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">b_squared_norm = (b**</span><span class="s5">2</span><span class="s1">).sum()</span>

    <span class="s1">expected = ((a_dense - b) ** </span><span class="s5">2</span><span class="s1">).sum()</span>
    <span class="s1">expected = expected </span><span class="s2">if </span><span class="s1">squared </span><span class="s2">else </span><span class="s1">np.sqrt(expected)</span>

    <span class="s1">distance_dense_dense = _euclidean_dense_dense_wrapper(a_dense</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">squared)</span>
    <span class="s1">distance_sparse_dense = _euclidean_sparse_dense_wrapper(</span>
        <span class="s1">a_sparse.data</span><span class="s2">, </span><span class="s1">a_sparse.indices</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">b_squared_norm</span><span class="s2">, </span><span class="s1">squared</span>
    <span class="s1">)</span>

    <span class="s1">rtol = </span><span class="s5">1e-4 </span><span class="s2">if </span><span class="s1">dtype == np.float32 </span><span class="s2">else </span><span class="s5">1e-7</span>
    <span class="s1">assert_allclose(distance_dense_dense</span><span class="s2">, </span><span class="s1">distance_sparse_dense</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>
    <span class="s1">assert_allclose(distance_dense_dense</span><span class="s2">, </span><span class="s1">expected</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>
    <span class="s1">assert_allclose(distance_sparse_dense</span><span class="s2">, </span><span class="s1">expected</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;dtype&quot;</span><span class="s2">, </span><span class="s1">[np.float32</span><span class="s2">, </span><span class="s1">np.float64])</span>
<span class="s2">def </span><span class="s1">test_inertia(dtype</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check that the _inertia_(dense/sparse) helpers produce correct results.</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">X_sparse = sp.random(</span>
        <span class="s5">100</span><span class="s2">, </span><span class="s5">10</span><span class="s2">, </span><span class="s1">density=</span><span class="s5">0.5</span><span class="s2">, </span><span class="s1">format=</span><span class="s4">&quot;csr&quot;</span><span class="s2">, </span><span class="s1">random_state=rng</span><span class="s2">, </span><span class="s1">dtype=dtype</span>
    <span class="s1">)</span>
    <span class="s1">X_dense = X_sparse.toarray()</span>
    <span class="s1">sample_weight = rng.randn(</span><span class="s5">100</span><span class="s1">).astype(dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">centers = rng.randn(</span><span class="s5">5</span><span class="s2">, </span><span class="s5">10</span><span class="s1">).astype(dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">labels = rng.randint(</span><span class="s5">5</span><span class="s2">, </span><span class="s1">size=</span><span class="s5">100</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span>

    <span class="s1">distances = ((X_dense - centers[labels]) ** </span><span class="s5">2</span><span class="s1">).sum(axis=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">expected = np.sum(distances * sample_weight)</span>

    <span class="s1">inertia_dense = _inertia_dense(X_dense</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">centers</span><span class="s2">, </span><span class="s1">labels</span><span class="s2">, </span><span class="s1">n_threads=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">inertia_sparse = _inertia_sparse(</span>
        <span class="s1">X_sparse</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">centers</span><span class="s2">, </span><span class="s1">labels</span><span class="s2">, </span><span class="s1">n_threads=</span><span class="s5">1</span>
    <span class="s1">)</span>

    <span class="s1">rtol = </span><span class="s5">1e-4 </span><span class="s2">if </span><span class="s1">dtype == np.float32 </span><span class="s2">else </span><span class="s5">1e-6</span>
    <span class="s1">assert_allclose(inertia_dense</span><span class="s2">, </span><span class="s1">inertia_sparse</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>
    <span class="s1">assert_allclose(inertia_dense</span><span class="s2">, </span><span class="s1">expected</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>
    <span class="s1">assert_allclose(inertia_sparse</span><span class="s2">, </span><span class="s1">expected</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>

    <span class="s3"># Check the single_label parameter.</span>
    <span class="s1">label = </span><span class="s5">1</span>
    <span class="s1">mask = labels == label</span>
    <span class="s1">distances = ((X_dense[mask] - centers[label]) ** </span><span class="s5">2</span><span class="s1">).sum(axis=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">expected = np.sum(distances * sample_weight[mask])</span>

    <span class="s1">inertia_dense = _inertia_dense(</span>
        <span class="s1">X_dense</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">centers</span><span class="s2">, </span><span class="s1">labels</span><span class="s2">, </span><span class="s1">n_threads=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">single_label=label</span>
    <span class="s1">)</span>
    <span class="s1">inertia_sparse = _inertia_sparse(</span>
        <span class="s1">X_sparse</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">centers</span><span class="s2">, </span><span class="s1">labels</span><span class="s2">, </span><span class="s1">n_threads=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">single_label=label</span>
    <span class="s1">)</span>

    <span class="s1">assert_allclose(inertia_dense</span><span class="s2">, </span><span class="s1">inertia_sparse</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>
    <span class="s1">assert_allclose(inertia_dense</span><span class="s2">, </span><span class="s1">expected</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>
    <span class="s1">assert_allclose(inertia_sparse</span><span class="s2">, </span><span class="s1">expected</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>


<span class="s3"># TODO(1.4): Remove</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Klass, default_n_init&quot;</span><span class="s2">, </span><span class="s1">[(KMeans</span><span class="s2">, </span><span class="s5">10</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(MiniBatchKMeans</span><span class="s2">, </span><span class="s5">3</span><span class="s1">)])</span>
<span class="s2">def </span><span class="s1">test_change_n_init_future_warning(Klass</span><span class="s2">, </span><span class="s1">default_n_init):</span>
    <span class="s1">est = Klass(n_init=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s4">&quot;error&quot;</span><span class="s2">, </span><span class="s1">FutureWarning)</span>
        <span class="s1">est.fit(X)</span>

    <span class="s1">default_n_init = </span><span class="s5">10 </span><span class="s2">if </span><span class="s1">Klass.__name__ == </span><span class="s4">&quot;KMeans&quot; </span><span class="s2">else </span><span class="s5">3</span>
    <span class="s1">msg = (</span>
        <span class="s4">f&quot;The default value of `n_init` will change from </span><span class="s2">{</span><span class="s1">default_n_init</span><span class="s2">} </span><span class="s4">to 'auto'&quot;</span>
        <span class="s4">&quot; in 1.4&quot;</span>
    <span class="s1">)</span>
    <span class="s1">est = Klass()</span>
    <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">est.fit(X)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Klass, default_n_init&quot;</span><span class="s2">, </span><span class="s1">[(KMeans</span><span class="s2">, </span><span class="s5">10</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(MiniBatchKMeans</span><span class="s2">, </span><span class="s5">3</span><span class="s1">)])</span>
<span class="s2">def </span><span class="s1">test_n_init_auto(Klass</span><span class="s2">, </span><span class="s1">default_n_init):</span>
    <span class="s1">est = Klass(n_init=</span><span class="s4">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">init=</span><span class="s4">&quot;k-means++&quot;</span><span class="s1">)</span>
    <span class="s1">est.fit(X)</span>
    <span class="s2">assert </span><span class="s1">est._n_init == </span><span class="s5">1</span>

    <span class="s1">est = Klass(n_init=</span><span class="s4">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s1">)</span>
    <span class="s1">est.fit(X)</span>
    <span class="s2">assert </span><span class="s1">est._n_init == </span><span class="s5">10 </span><span class="s2">if </span><span class="s1">Klass.__name__ == </span><span class="s4">&quot;KMeans&quot; </span><span class="s2">else </span><span class="s5">3</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s2">def </span><span class="s1">test_sample_weight_unchanged(Estimator):</span>
    <span class="s3"># Check that sample_weight is not modified in place by KMeans (#17204)</span>
    <span class="s1">X = np.array([[</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">4</span><span class="s1">]])</span>
    <span class="s1">sample_weight = np.array([</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0.2</span><span class="s2">, </span><span class="s5">0.3</span><span class="s1">])</span>
    <span class="s1">Estimator(n_clusters=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">assert_array_equal(sample_weight</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0.2</span><span class="s2">, </span><span class="s5">0.3</span><span class="s1">]))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[KMeans</span><span class="s2">, </span><span class="s1">MiniBatchKMeans])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;param, match&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">({</span><span class="s4">&quot;n_clusters&quot;</span><span class="s1">: n_samples + </span><span class="s5">1</span><span class="s1">}</span><span class="s2">, </span><span class="s4">r&quot;n_samples.* should be &gt;= n_clusters&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;init&quot;</span><span class="s1">: X[:</span><span class="s5">2</span><span class="s1">]}</span><span class="s2">,</span>
            <span class="s4">r&quot;The shape of the initial centers .* does not match &quot;</span>
            <span class="s4">r&quot;the number of clusters&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;init&quot;</span><span class="s1">: </span><span class="s2">lambda </span><span class="s1">X_</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">random_state: X_[:</span><span class="s5">2</span><span class="s1">]}</span><span class="s2">,</span>
            <span class="s4">r&quot;The shape of the initial centers .* does not match &quot;</span>
            <span class="s4">r&quot;the number of clusters&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;init&quot;</span><span class="s1">: X[:</span><span class="s5">8</span><span class="s2">, </span><span class="s1">:</span><span class="s5">2</span><span class="s1">]}</span><span class="s2">,</span>
            <span class="s4">r&quot;The shape of the initial centers .* does not match &quot;</span>
            <span class="s4">r&quot;the number of features of the data&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;init&quot;</span><span class="s1">: </span><span class="s2">lambda </span><span class="s1">X_</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">random_state: X_[:</span><span class="s5">8</span><span class="s2">, </span><span class="s1">:</span><span class="s5">2</span><span class="s1">]}</span><span class="s2">,</span>
            <span class="s4">r&quot;The shape of the initial centers .* does not match &quot;</span>
            <span class="s4">r&quot;the number of features of the data&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_wrong_params(Estimator</span><span class="s2">, </span><span class="s1">param</span><span class="s2">, </span><span class="s1">match):</span>
    <span class="s3"># Check that error are raised with clear error message when wrong values</span>
    <span class="s3"># are passed for the parameters</span>
    <span class="s3"># Set n_init=1 by default to avoid warning with precomputed init</span>
    <span class="s1">km = Estimator(n_init=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=match):</span>
        <span class="s1">km.set_params(**param).fit(X)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;param, match&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;x_squared_norms&quot;</span><span class="s1">: X[:</span><span class="s5">2</span><span class="s1">]}</span><span class="s2">,</span>
            <span class="s4">r&quot;The length of x_squared_norms .* should &quot;</span>
            <span class="s4">r&quot;be equal to the length of n_samples&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_kmeans_plusplus_wrong_params(param</span><span class="s2">, </span><span class="s1">match):</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=match):</span>
        <span class="s1">kmeans_plusplus(X</span><span class="s2">, </span><span class="s1">n_clusters</span><span class="s2">, </span><span class="s1">**param)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;data&quot;</span><span class="s2">, </span><span class="s1">[X</span><span class="s2">, </span><span class="s1">X_csr])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;dtype&quot;</span><span class="s2">, </span><span class="s1">[np.float64</span><span class="s2">, </span><span class="s1">np.float32])</span>
<span class="s2">def </span><span class="s1">test_kmeans_plusplus_output(data</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s3"># Check for the correct number of seeds and all positive values</span>
    <span class="s1">data = data.astype(dtype)</span>
    <span class="s1">centers</span><span class="s2">, </span><span class="s1">indices = kmeans_plusplus(</span>
        <span class="s1">data</span><span class="s2">, </span><span class="s1">n_clusters</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>

    <span class="s3"># Check there are the correct number of indices and that all indices are</span>
    <span class="s3"># positive and within the number of samples</span>
    <span class="s2">assert </span><span class="s1">indices.shape[</span><span class="s5">0</span><span class="s1">] == n_clusters</span>
    <span class="s2">assert </span><span class="s1">(indices &gt;= </span><span class="s5">0</span><span class="s1">).all()</span>
    <span class="s2">assert </span><span class="s1">(indices &lt;= data.shape[</span><span class="s5">0</span><span class="s1">]).all()</span>

    <span class="s3"># Check for the correct number of seeds and that they are bound by the data</span>
    <span class="s2">assert </span><span class="s1">centers.shape[</span><span class="s5">0</span><span class="s1">] == n_clusters</span>
    <span class="s2">assert </span><span class="s1">(centers.max(axis=</span><span class="s5">0</span><span class="s1">) &lt;= data.max(axis=</span><span class="s5">0</span><span class="s1">)).all()</span>
    <span class="s2">assert </span><span class="s1">(centers.min(axis=</span><span class="s5">0</span><span class="s1">) &gt;= data.min(axis=</span><span class="s5">0</span><span class="s1">)).all()</span>

    <span class="s3"># Check that indices correspond to reported centers</span>
    <span class="s3"># Use X for comparison rather than data, test still works against centers</span>
    <span class="s3"># calculated with sparse data.</span>
    <span class="s1">assert_allclose(X[indices].astype(dtype)</span><span class="s2">, </span><span class="s1">centers)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;x_squared_norms&quot;</span><span class="s2">, </span><span class="s1">[row_norms(X</span><span class="s2">, </span><span class="s1">squared=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">, None</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_kmeans_plusplus_norms(x_squared_norms):</span>
    <span class="s3"># Check that defining x_squared_norms returns the same as default=None.</span>
    <span class="s1">centers</span><span class="s2">, </span><span class="s1">indices = kmeans_plusplus(X</span><span class="s2">, </span><span class="s1">n_clusters</span><span class="s2">, </span><span class="s1">x_squared_norms=x_squared_norms)</span>

    <span class="s1">assert_allclose(X[indices]</span><span class="s2">, </span><span class="s1">centers)</span>


<span class="s2">def </span><span class="s1">test_kmeans_plusplus_dataorder(global_random_seed):</span>
    <span class="s3"># Check that memory layout does not effect result</span>
    <span class="s1">centers_c</span><span class="s2">, </span><span class="s1">_ = kmeans_plusplus(X</span><span class="s2">, </span><span class="s1">n_clusters</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>

    <span class="s1">X_fortran = np.asfortranarray(X)</span>

    <span class="s1">centers_fortran</span><span class="s2">, </span><span class="s1">_ = kmeans_plusplus(</span>
        <span class="s1">X_fortran</span><span class="s2">, </span><span class="s1">n_clusters</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>

    <span class="s1">assert_allclose(centers_c</span><span class="s2">, </span><span class="s1">centers_fortran)</span>


<span class="s2">def </span><span class="s1">test_is_same_clustering():</span>
    <span class="s3"># Sanity check for the _is_same_clustering utility function</span>
    <span class="s1">labels1 = np.array([</span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span>
    <span class="s2">assert </span><span class="s1">_is_same_clustering(labels1</span><span class="s2">, </span><span class="s1">labels1</span><span class="s2">, </span><span class="s5">3</span><span class="s1">)</span>

    <span class="s3"># these other labels represent the same clustering since we can retrieve the first</span>
    <span class="s3"># labels by simply renaming the labels: 0 -&gt; 1, 1 -&gt; 2, 2 -&gt; 0.</span>
    <span class="s1">labels2 = np.array([</span><span class="s5">0</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span>
    <span class="s2">assert </span><span class="s1">_is_same_clustering(labels1</span><span class="s2">, </span><span class="s1">labels2</span><span class="s2">, </span><span class="s5">3</span><span class="s1">)</span>

    <span class="s3"># these other labels do not represent the same clustering since not all ones are</span>
    <span class="s3"># mapped to a same value</span>
    <span class="s1">labels3 = np.array([</span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span>
    <span class="s2">assert not </span><span class="s1">_is_same_clustering(labels1</span><span class="s2">, </span><span class="s1">labels3</span><span class="s2">, </span><span class="s5">3</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;kwargs&quot;</span><span class="s2">, </span><span class="s1">({</span><span class="s4">&quot;init&quot;</span><span class="s1">: np.str_(</span><span class="s4">&quot;k-means++&quot;</span><span class="s1">)}</span><span class="s2">, </span><span class="s1">{</span><span class="s4">&quot;init&quot;</span><span class="s1">: [[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s4">&quot;n_init&quot;</span><span class="s1">: </span><span class="s5">1</span><span class="s1">})</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_kmeans_with_array_like_or_np_scalar_init(kwargs):</span>
    <span class="s0">&quot;&quot;&quot;Check that init works with numpy scalar strings. 
 
    Non-regression test for #21964. 
    &quot;&quot;&quot;</span>
    <span class="s1">X = np.asarray([[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>

    <span class="s1">clustering = KMeans(n_clusters=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">**kwargs)</span>
    <span class="s3"># Does not raise</span>
    <span class="s1">clustering.fit(X)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;Klass, method&quot;</span><span class="s2">,</span>
    <span class="s1">[(KMeans</span><span class="s2">, </span><span class="s4">&quot;fit&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(MiniBatchKMeans</span><span class="s2">, </span><span class="s4">&quot;fit&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(MiniBatchKMeans</span><span class="s2">, </span><span class="s4">&quot;partial_fit&quot;</span><span class="s1">)]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_feature_names_out(Klass</span><span class="s2">, </span><span class="s1">method):</span>
    <span class="s0">&quot;&quot;&quot;Check `feature_names_out` for `KMeans` and `MiniBatchKMeans`.&quot;&quot;&quot;</span>
    <span class="s1">class_name = Klass.__name__.lower()</span>
    <span class="s1">kmeans = Klass()</span>
    <span class="s1">getattr(kmeans</span><span class="s2">, </span><span class="s1">method)(X)</span>
    <span class="s1">n_clusters = kmeans.cluster_centers_.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s1">names_out = kmeans.get_feature_names_out()</span>
    <span class="s1">assert_array_equal([</span><span class="s4">f&quot;</span><span class="s2">{</span><span class="s1">class_name</span><span class="s2">}{</span><span class="s1">i</span><span class="s2">}</span><span class="s4">&quot; </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_clusters)]</span><span class="s2">, </span><span class="s1">names_out)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;is_sparse&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_predict_does_not_change_cluster_centers(is_sparse):</span>
    <span class="s0">&quot;&quot;&quot;Check that predict does not change cluster centers. 
 
    Non-regression test for gh-24253. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">_ = make_blobs(n_samples=</span><span class="s5">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">centers=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">is_sparse:</span>
        <span class="s1">X = sp.csr_matrix(X)</span>

    <span class="s1">kmeans = KMeans()</span>
    <span class="s1">y_pred1 = kmeans.fit_predict(X)</span>
    <span class="s3"># Make cluster_centers readonly</span>
    <span class="s1">kmeans.cluster_centers_ = create_memmap_backed_data(kmeans.cluster_centers_)</span>
    <span class="s1">kmeans.labels_ = create_memmap_backed_data(kmeans.labels_)</span>

    <span class="s1">y_pred2 = kmeans.predict(X)</span>
    <span class="s1">assert_array_equal(y_pred1</span><span class="s2">, </span><span class="s1">y_pred2)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;init&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;k-means++&quot;</span><span class="s2">, </span><span class="s4">&quot;random&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_sample_weight_init(init</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Check that sample weight is used during init. 
 
    `_init_centroids` is shared across all classes inheriting from _BaseKMeans so 
    it's enough to check for KMeans. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">_ = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s5">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">centers=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>
    <span class="s1">x_squared_norms = row_norms(X</span><span class="s2">, </span><span class="s1">squared=</span><span class="s2">True</span><span class="s1">)</span>

    <span class="s1">kmeans = KMeans()</span>
    <span class="s1">clusters_weighted = kmeans._init_centroids(</span>
        <span class="s1">X=X</span><span class="s2">,</span>
        <span class="s1">x_squared_norms=x_squared_norms</span><span class="s2">,</span>
        <span class="s1">init=init</span><span class="s2">,</span>
        <span class="s1">sample_weight=rng.uniform(size=X.shape[</span><span class="s5">0</span><span class="s1">])</span><span class="s2">,</span>
        <span class="s1">n_centroids=</span><span class="s5">5</span><span class="s2">,</span>
        <span class="s1">random_state=np.random.RandomState(global_random_seed)</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">clusters = kmeans._init_centroids(</span>
        <span class="s1">X=X</span><span class="s2">,</span>
        <span class="s1">x_squared_norms=x_squared_norms</span><span class="s2">,</span>
        <span class="s1">init=init</span><span class="s2">,</span>
        <span class="s1">sample_weight=np.ones(X.shape[</span><span class="s5">0</span><span class="s1">])</span><span class="s2">,</span>
        <span class="s1">n_centroids=</span><span class="s5">5</span><span class="s2">,</span>
        <span class="s1">random_state=np.random.RandomState(global_random_seed)</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(AssertionError):</span>
        <span class="s1">assert_allclose(clusters_weighted</span><span class="s2">, </span><span class="s1">clusters)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;init&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;k-means++&quot;</span><span class="s2">, </span><span class="s4">&quot;random&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_sample_weight_zero(init</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Check that if sample weight is 0, this sample won't be chosen. 
 
    `_init_centroids` is shared across all classes inheriting from _BaseKMeans so 
    it's enough to check for KMeans. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">_ = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s5">100</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">centers=</span><span class="s5">5</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>
    <span class="s1">sample_weight = rng.uniform(size=X.shape[</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">sample_weight[::</span><span class="s5">2</span><span class="s1">] = </span><span class="s5">0</span>
    <span class="s1">x_squared_norms = row_norms(X</span><span class="s2">, </span><span class="s1">squared=</span><span class="s2">True</span><span class="s1">)</span>

    <span class="s1">kmeans = KMeans()</span>
    <span class="s1">clusters_weighted = kmeans._init_centroids(</span>
        <span class="s1">X=X</span><span class="s2">,</span>
        <span class="s1">x_squared_norms=x_squared_norms</span><span class="s2">,</span>
        <span class="s1">init=init</span><span class="s2">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s2">,</span>
        <span class="s1">n_centroids=</span><span class="s5">10</span><span class="s2">,</span>
        <span class="s1">random_state=np.random.RandomState(global_random_seed)</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s3"># No center should be one of the 0 sample weight point</span>
    <span class="s3"># (i.e. be at a distance=0 from it)</span>
    <span class="s1">d = euclidean_distances(X[::</span><span class="s5">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">clusters_weighted)</span>
    <span class="s2">assert not </span><span class="s1">np.any(np.isclose(d</span><span class="s2">, </span><span class="s5">0</span><span class="s1">))</span>
</pre>
</body>
</html>