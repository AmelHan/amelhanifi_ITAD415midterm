<html>
<head>
<title>scale.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
scale.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Support and standalone functions for Robust Linear Models 
 
References 
---------- 
PJ Huber.  'Robust Statistics' John Wiley and Sons, Inc., New York, 1981. 
 
R Venables, B Ripley. 'Modern Applied Statistics in S' 
    Springer, New York, 2002. 
 
C Croux, PJ Rousseeuw, 'Time-efficient algorithms for two highly robust 
estimators of scale' Computational statistics. Physica, Heidelberg, 1992. 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy.stats </span><span class="s2">import </span><span class="s1">norm </span><span class="s2">as </span><span class="s1">Gaussian</span>

<span class="s2">from </span><span class="s1">statsmodels.tools </span><span class="s2">import </span><span class="s1">tools</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.validation </span><span class="s2">import </span><span class="s1">array_like</span><span class="s2">, </span><span class="s1">float_like</span>

<span class="s2">from </span><span class="s1">. </span><span class="s2">import </span><span class="s1">norms</span>
<span class="s2">from </span><span class="s1">._qn </span><span class="s2">import </span><span class="s1">_qn</span>


<span class="s2">def </span><span class="s1">mad(a</span><span class="s2">, </span><span class="s1">c=Gaussian.ppf(</span><span class="s3">3 </span><span class="s1">/ </span><span class="s3">4.0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">center=np.median):</span>
    <span class="s0">&quot;&quot;&quot; 
    The Median Absolute Deviation along given axis of an array 
 
    Parameters 
    ---------- 
    a : array_like 
        Input array. 
    c : float, optional 
        The normalization constant.  Defined as scipy.stats.norm.ppf(3/4.), 
        which is approximately 0.6745. 
    axis : int, optional 
        The default is 0. Can also be None. 
    center : callable or float 
        If a callable is provided, such as the default `np.median` then it 
        is expected to be called center(a). The axis argument will be applied 
        via np.apply_over_axes. Otherwise, provide a float. 
 
    Returns 
    ------- 
    mad : float 
        `mad` = median(abs(`a` - center))/`c` 
    &quot;&quot;&quot;</span>
    <span class="s1">a = array_like(a</span><span class="s2">, </span><span class="s4">&quot;a&quot;</span><span class="s2">, </span><span class="s1">ndim=</span><span class="s2">None</span><span class="s1">)</span>
    <span class="s1">c = float_like(c</span><span class="s2">, </span><span class="s4">&quot;c&quot;</span><span class="s1">)</span>
    <span class="s2">if not </span><span class="s1">a.size:</span>
        <span class="s1">center_val = </span><span class="s3">0.0</span>
    <span class="s2">elif </span><span class="s1">callable(center):</span>
        <span class="s2">if </span><span class="s1">axis </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">center_val = np.apply_over_axes(center</span><span class="s2">, </span><span class="s1">a</span><span class="s2">, </span><span class="s1">axis)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">center_val = center(a.ravel())</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">center_val = float_like(center</span><span class="s2">, </span><span class="s4">&quot;center&quot;</span><span class="s1">)</span>
    <span class="s1">err = (np.abs(a - center_val)) / c</span>
    <span class="s2">if not </span><span class="s1">err.size:</span>
        <span class="s2">if </span><span class="s1">axis </span><span class="s2">is None or </span><span class="s1">err.ndim == </span><span class="s3">1</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">np.nan</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">shape = list(err.shape)</span>
            <span class="s1">shape.pop(axis)</span>
            <span class="s2">return </span><span class="s1">np.empty(shape)</span>
    <span class="s2">return </span><span class="s1">np.median(err</span><span class="s2">, </span><span class="s1">axis=axis)</span>


<span class="s2">def </span><span class="s1">iqr(a</span><span class="s2">, </span><span class="s1">c=Gaussian.ppf(</span><span class="s3">3 </span><span class="s1">/ </span><span class="s3">4</span><span class="s1">) - Gaussian.ppf(</span><span class="s3">1 </span><span class="s1">/ </span><span class="s3">4</span><span class="s1">)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    The normalized interquartile range along given axis of an array 
 
    Parameters 
    ---------- 
    a : array_like 
        Input array. 
    c : float, optional 
        The normalization constant, used to get consistent estimates of the 
        standard deviation at the normal distribution.  Defined as 
        scipy.stats.norm.ppf(3/4.) - scipy.stats.norm.ppf(1/4.), which is 
        approximately 1.349. 
    axis : int, optional 
        The default is 0. Can also be None. 
 
    Returns 
    ------- 
    The normalized interquartile range 
    &quot;&quot;&quot;</span>
    <span class="s1">a = array_like(a</span><span class="s2">, </span><span class="s4">&quot;a&quot;</span><span class="s2">, </span><span class="s1">ndim=</span><span class="s2">None</span><span class="s1">)</span>
    <span class="s1">c = float_like(c</span><span class="s2">, </span><span class="s4">&quot;c&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">a.ndim == </span><span class="s3">0</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;a should have at least one dimension&quot;</span><span class="s1">)</span>
    <span class="s2">elif </span><span class="s1">a.size == </span><span class="s3">0</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">np.nan</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">quantiles = np.quantile(a</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0.25</span><span class="s2">, </span><span class="s3">0.75</span><span class="s1">]</span><span class="s2">, </span><span class="s1">axis=axis)</span>
        <span class="s2">return </span><span class="s1">np.squeeze(np.diff(quantiles</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">) / c)</span>


<span class="s2">def </span><span class="s1">qn_scale(a</span><span class="s2">, </span><span class="s1">c=</span><span class="s3">1 </span><span class="s1">/ (np.sqrt(</span><span class="s3">2</span><span class="s1">) * Gaussian.ppf(</span><span class="s3">5 </span><span class="s1">/ </span><span class="s3">8</span><span class="s1">))</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Computes the Qn robust estimator of scale 
 
    The Qn scale estimator is a more efficient alternative to the MAD. 
    The Qn scale estimator of an array a of length n is defined as 
    c * {abs(a[i] - a[j]): i&lt;j}_(k), for k equal to [n/2] + 1 choose 2. Thus, 
    the Qn estimator is the k-th order statistic of the absolute differences 
    of the array. The optional constant is used to normalize the estimate 
    as explained below. The implementation follows the algorithm described 
    in Croux and Rousseeuw (1992). 
 
    Parameters 
    ---------- 
    a : array_like 
        Input array. 
    c : float, optional 
        The normalization constant. The default value is used to get consistent 
        estimates of the standard deviation at the normal distribution. 
    axis : int, optional 
        The default is 0. 
 
    Returns 
    ------- 
    {float, ndarray} 
        The Qn robust estimator of scale 
    &quot;&quot;&quot;</span>
    <span class="s1">a = array_like(</span>
        <span class="s1">a</span><span class="s2">, </span><span class="s4">&quot;a&quot;</span><span class="s2">, </span><span class="s1">ndim=</span><span class="s2">None, </span><span class="s1">dtype=np.float64</span><span class="s2">, </span><span class="s1">contiguous=</span><span class="s2">True, </span><span class="s1">order=</span><span class="s4">&quot;C&quot;</span>
    <span class="s1">)</span>
    <span class="s1">c = float_like(c</span><span class="s2">, </span><span class="s4">&quot;c&quot;</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">a.ndim == </span><span class="s3">0</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;a should have at least one dimension&quot;</span><span class="s1">)</span>
    <span class="s2">elif </span><span class="s1">a.size == </span><span class="s3">0</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">np.nan</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">out = np.apply_along_axis(_qn</span><span class="s2">, </span><span class="s1">axis=axis</span><span class="s2">, </span><span class="s1">arr=a</span><span class="s2">, </span><span class="s1">c=c)</span>
        <span class="s2">if </span><span class="s1">out.ndim == </span><span class="s3">0</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">float(out)</span>
        <span class="s2">return </span><span class="s1">out</span>


<span class="s2">def </span><span class="s1">_qn_naive(a</span><span class="s2">, </span><span class="s1">c=</span><span class="s3">1 </span><span class="s1">/ (np.sqrt(</span><span class="s3">2</span><span class="s1">) * Gaussian.ppf(</span><span class="s3">5 </span><span class="s1">/ </span><span class="s3">8</span><span class="s1">))):</span>
    <span class="s0">&quot;&quot;&quot; 
    A naive implementation of the Qn robust estimator of scale, used solely 
    to test the faster, more involved one 
 
    Parameters 
    ---------- 
    a : array_like 
        Input array. 
    c : float, optional 
        The normalization constant, used to get consistent estimates of the 
        standard deviation at the normal distribution.  Defined as 
        1/(np.sqrt(2) * scipy.stats.norm.ppf(5/8)), which is 2.219144. 
 
    Returns 
    ------- 
    The Qn robust estimator of scale 
    &quot;&quot;&quot;</span>
    <span class="s1">a = np.squeeze(a)</span>
    <span class="s1">n = a.shape[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s2">if </span><span class="s1">a.size == </span><span class="s3">0</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">np.nan</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">h = int(n // </span><span class="s3">2 </span><span class="s1">+ </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">k = int(h * (h - </span><span class="s3">1</span><span class="s1">) / </span><span class="s3">2</span><span class="s1">)</span>
        <span class="s1">idx = np.triu_indices(n</span><span class="s2">, </span><span class="s1">k=</span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">diffs = np.abs(a[idx[</span><span class="s3">0</span><span class="s1">]] - a[idx[</span><span class="s3">1</span><span class="s1">]])</span>
        <span class="s1">output = np.partition(diffs</span><span class="s2">, </span><span class="s1">kth=k - </span><span class="s3">1</span><span class="s1">)[k - </span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">output = c * output</span>
        <span class="s2">return </span><span class="s1">output</span>


<span class="s2">class </span><span class="s1">Huber:</span>
    <span class="s0">&quot;&quot;&quot; 
    Huber's proposal 2 for estimating location and scale jointly. 
 
    Parameters 
    ---------- 
    c : float, optional 
        Threshold used in threshold for chi=psi**2.  Default value is 1.5. 
    tol : float, optional 
        Tolerance for convergence.  Default value is 1e-08. 
    maxiter : int, optional0 
        Maximum number of iterations.  Default value is 30. 
    norm : statsmodels.robust.norms.RobustNorm, optional 
        A robust norm used in M estimator of location. If None, 
        the location estimator defaults to a one-step 
        fixed point version of the M-estimator using Huber's T. 
 
    call 
        Return joint estimates of Huber's scale and location. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; import statsmodels.api as sm 
    &gt;&gt;&gt; chem_data = np.array([2.20, 2.20, 2.4, 2.4, 2.5, 2.7, 2.8, 2.9, 3.03, 
    ...        3.03, 3.10, 3.37, 3.4, 3.4, 3.4, 3.5, 3.6, 3.7, 3.7, 3.7, 3.7, 
    ...        3.77, 5.28, 28.95]) 
    &gt;&gt;&gt; sm.robust.scale.huber(chem_data) 
    (array(3.2054980819923693), array(0.67365260010478967)) 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">c=</span><span class="s3">1.5</span><span class="s2">, </span><span class="s1">tol=</span><span class="s3">1.0e-08</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s3">30</span><span class="s2">, </span><span class="s1">norm=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self.c = c</span>
        <span class="s1">self.maxiter = maxiter</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.norm = norm</span>
        <span class="s1">tmp = </span><span class="s3">2 </span><span class="s1">* Gaussian.cdf(c) - </span><span class="s3">1</span>
        <span class="s1">self.gamma = tmp + c ** </span><span class="s3">2 </span><span class="s1">* (</span><span class="s3">1 </span><span class="s1">- tmp) - </span><span class="s3">2 </span><span class="s1">* c * Gaussian.pdf(c)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">a</span><span class="s2">, </span><span class="s1">mu=</span><span class="s2">None, </span><span class="s1">initscale=</span><span class="s2">None, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Compute Huber's proposal 2 estimate of scale, using an optional 
        initial value of scale and an optional estimate of mu. If mu 
        is supplied, it is not reestimated. 
 
        Parameters 
        ---------- 
        a : ndarray 
            1d array 
        mu : float or None, optional 
            If the location mu is supplied then it is not reestimated. 
            Default is None, which means that it is estimated. 
        initscale : float or None, optional 
            A first guess on scale.  If initscale is None then the standardized 
            median absolute deviation of a is used. 
 
        Notes 
        ----- 
        `Huber` minimizes the function 
 
        sum(psi((a[i]-mu)/scale)**2) 
 
        as a function of (mu, scale), where 
 
        psi(x) = np.clip(x, -self.c, self.c) 
        &quot;&quot;&quot;</span>
        <span class="s1">a = np.asarray(a)</span>
        <span class="s2">if </span><span class="s1">mu </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">n = a.shape[</span><span class="s3">0</span><span class="s1">] - </span><span class="s3">1</span>
            <span class="s1">mu = np.median(a</span><span class="s2">, </span><span class="s1">axis=axis)</span>
            <span class="s1">est_mu = </span><span class="s2">True</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">n = a.shape[</span><span class="s3">0</span><span class="s1">]</span>
            <span class="s1">mu = mu</span>
            <span class="s1">est_mu = </span><span class="s2">False</span>

        <span class="s2">if </span><span class="s1">initscale </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">scale = mad(a</span><span class="s2">, </span><span class="s1">axis=axis)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">scale = initscale</span>
        <span class="s1">scale = tools.unsqueeze(scale</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s1">a.shape)</span>
        <span class="s1">mu = tools.unsqueeze(mu</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s1">a.shape)</span>
        <span class="s2">return </span><span class="s1">self._estimate_both(a</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s1">est_mu</span><span class="s2">, </span><span class="s1">n)</span>

    <span class="s2">def </span><span class="s1">_estimate_both(self</span><span class="s2">, </span><span class="s1">a</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s1">est_mu</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s4">&quot;&quot;&quot; 
        Estimate scale and location simultaneously with the following 
        pseudo_loop: 
 
        while not_converged: 
            mu, scale = estimate_location(a, scale, mu), estimate_scale(a, scale, mu) 
 
        where estimate_location is an M-estimator and estimate_scale implements 
        the check used in Section 5.5 of Venables &amp; Ripley 
        &quot;&quot;&quot;  </span><span class="s5"># noqa:E501</span>
        <span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(self.maxiter):</span>
            <span class="s5"># Estimate the mean along a given axis</span>
            <span class="s2">if </span><span class="s1">est_mu:</span>
                <span class="s2">if </span><span class="s1">self.norm </span><span class="s2">is None</span><span class="s1">:</span>
                    <span class="s5"># This is a one-step fixed-point estimator</span>
                    <span class="s5"># if self.norm == norms.HuberT</span>
                    <span class="s5"># It should be faster than using norms.HuberT</span>
                    <span class="s1">nmu = (</span>
                        <span class="s1">np.clip(</span>
                            <span class="s1">a</span><span class="s2">, </span><span class="s1">mu - self.c * scale</span><span class="s2">, </span><span class="s1">mu + self.c * scale</span>
                        <span class="s1">).sum(axis)</span>
                        <span class="s1">/ a.shape[axis]</span>
                    <span class="s1">)</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">nmu = norms.estimate_location(</span>
                        <span class="s1">a</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">self.norm</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">self.maxiter</span><span class="s2">, </span><span class="s1">self.tol</span>
                    <span class="s1">)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s5"># Effectively, do nothing</span>
                <span class="s1">nmu = mu.squeeze()</span>
            <span class="s1">nmu = tools.unsqueeze(nmu</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s1">a.shape)</span>

            <span class="s1">subset = np.less_equal(np.abs((a - mu) / scale)</span><span class="s2">, </span><span class="s1">self.c)</span>
            <span class="s1">card = subset.sum(axis)</span>

            <span class="s1">scale_num = np.sum(subset * (a - nmu) ** </span><span class="s3">2</span><span class="s2">, </span><span class="s1">axis)</span>
            <span class="s1">scale_denom = n * self.gamma - (a.shape[axis] - card) * self.c ** </span><span class="s3">2</span>
            <span class="s1">nscale = np.sqrt(scale_num / scale_denom)</span>
            <span class="s1">nscale = tools.unsqueeze(nscale</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s1">a.shape)</span>

            <span class="s1">test1 = np.all(</span>
                <span class="s1">np.less_equal(np.abs(scale - nscale)</span><span class="s2">, </span><span class="s1">nscale * self.tol)</span>
            <span class="s1">)</span>
            <span class="s1">test2 = np.all(</span>
                <span class="s1">np.less_equal(np.abs(mu - nmu)</span><span class="s2">, </span><span class="s1">nscale * self.tol)</span>
            <span class="s1">)</span>
            <span class="s2">if not </span><span class="s1">(test1 </span><span class="s2">and </span><span class="s1">test2):</span>
                <span class="s1">mu = nmu</span>
                <span class="s1">scale = nscale</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">nmu.squeeze()</span><span class="s2">, </span><span class="s1">nscale.squeeze()</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;joint estimation of location and scale failed &quot;</span>
            <span class="s4">&quot;to converge in %d iterations&quot; </span><span class="s1">% self.maxiter</span>
        <span class="s1">)</span>


<span class="s1">huber = Huber()</span>


<span class="s2">class </span><span class="s1">HuberScale:</span>
    <span class="s0">r&quot;&quot;&quot; 
    Huber's scaling for fitting robust linear models. 
 
    Huber's scale is intended to be used as the scale estimate in the 
    IRLS algorithm and is slightly different than the `Huber` class. 
 
    Parameters 
    ---------- 
    d : float, optional 
        d is the tuning constant for Huber's scale.  Default is 2.5 
    tol : float, optional 
        The convergence tolerance 
    maxiter : int, optiona 
        The maximum number of iterations.  The default is 30. 
 
    Methods 
    ------- 
    call 
        Return's Huber's scale computed as below 
 
    Notes 
    ----- 
    Huber's scale is the iterative solution to 
 
    scale_(i+1)**2 = 1/(n*h)*sum(chi(r/sigma_i)*sigma_i**2 
 
    where the Huber function is 
 
    chi(x) = (x**2)/2       for \|x\| &lt; d 
    chi(x) = (d**2)/2       for \|x\| &gt;= d 
 
    and the Huber constant h = (n-p)/n*(d**2 + (1-d**2)* 
    scipy.stats.norm.cdf(d) - .5 - d*sqrt(2*pi)*exp(-0.5*d**2) 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">d=</span><span class="s3">2.5</span><span class="s2">, </span><span class="s1">tol=</span><span class="s3">1e-08</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s3">30</span><span class="s1">):</span>
        <span class="s1">self.d = d</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.maxiter = maxiter</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">df_resid</span><span class="s2">, </span><span class="s1">nobs</span><span class="s2">, </span><span class="s1">resid):</span>
        <span class="s1">h = (</span>
            <span class="s1">df_resid</span>
            <span class="s1">/ nobs</span>
            <span class="s1">* (</span>
                <span class="s1">self.d ** </span><span class="s3">2</span>
                <span class="s1">+ (</span><span class="s3">1 </span><span class="s1">- self.d ** </span><span class="s3">2</span><span class="s1">) * Gaussian.cdf(self.d)</span>
                <span class="s1">- </span><span class="s3">0.5</span>
                <span class="s1">- self.d / (np.sqrt(</span><span class="s3">2 </span><span class="s1">* np.pi)) * np.exp(-</span><span class="s3">0.5 </span><span class="s1">* self.d ** </span><span class="s3">2</span><span class="s1">)</span>
            <span class="s1">)</span>
        <span class="s1">)</span>
        <span class="s1">s = mad(resid)</span>

        <span class="s2">def </span><span class="s1">subset(x):</span>
            <span class="s2">return </span><span class="s1">np.less(np.abs(resid / x)</span><span class="s2">, </span><span class="s1">self.d)</span>

        <span class="s2">def </span><span class="s1">chi(s):</span>
            <span class="s2">return </span><span class="s1">subset(s) * (resid / s) ** </span><span class="s3">2 </span><span class="s1">/ </span><span class="s3">2 </span><span class="s1">+ (</span><span class="s3">1 </span><span class="s1">- subset(s)) * (</span>
                <span class="s1">self.d ** </span><span class="s3">2 </span><span class="s1">/ </span><span class="s3">2</span>
            <span class="s1">)</span>

        <span class="s1">scalehist = [np.inf</span><span class="s2">, </span><span class="s1">s]</span>
        <span class="s1">niter = </span><span class="s3">1</span>
        <span class="s2">while </span><span class="s1">(</span>
            <span class="s1">np.abs(scalehist[niter - </span><span class="s3">1</span><span class="s1">] - scalehist[niter]) &gt; self.tol</span>
            <span class="s2">and </span><span class="s1">niter &lt; self.maxiter</span>
        <span class="s1">):</span>
            <span class="s1">nscale = np.sqrt(</span>
                <span class="s3">1</span>
                <span class="s1">/ (nobs * h)</span>
                <span class="s1">* np.sum(chi(scalehist[-</span><span class="s3">1</span><span class="s1">]))</span>
                <span class="s1">* scalehist[-</span><span class="s3">1</span><span class="s1">] ** </span><span class="s3">2</span>
            <span class="s1">)</span>
            <span class="s1">scalehist.append(nscale)</span>
            <span class="s1">niter += </span><span class="s3">1</span>
            <span class="s5"># TODO: raise on convergence failure?</span>
        <span class="s2">return </span><span class="s1">scalehist[-</span><span class="s3">1</span><span class="s1">]</span>


<span class="s1">hubers_scale = HuberScale()</span>
</pre>
</body>
</html>