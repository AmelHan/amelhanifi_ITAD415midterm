<html>
<head>
<title>gam.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
gam.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Generalized additive models 
 
 
 
Requirements for smoothers 
-------------------------- 
 
smooth(y, weights=xxx) : ? no return ? alias for fit 
predict(x=None) : smoothed values, fittedvalues or for new exog 
df_fit() : degress of freedom of fit ? 
 
 
Notes 
----- 
- using PolySmoother works for AdditiveModel, and GAM with Poisson and Binomial 
- testfailure with Gamma, no other families tested 
- there is still an indeterminacy in the split up of the constant across 
  components (smoothers) and alpha, sum, i.e. constant, looks good. 
  - role of offset, that I have not tried to figure out yet 
 
Refactoring 
----------- 
currently result is attached to model instead of other way around 
split up Result in class for AdditiveModel and for GAM, 
subclass GLMResults, needs verification that result statistics are appropriate 
how much inheritance, double inheritance? 
renamings and cleanup 
interface to other smoothers, scipy splines 
 
basic unittests as support for refactoring exist, but we should have a test 
case for gamma and the others. Advantage of PolySmoother is that we can 
benchmark against the parametric GLM results. 
 
&quot;&quot;&quot;</span>

<span class="s2"># JP:</span>
<span class="s2"># changes: use PolySmoother instead of crashing bspline</span>
<span class="s2"># TODO: check/catalogue required interface of a smoother</span>
<span class="s2"># TODO: replace default smoother by corresponding function to initialize</span>
<span class="s2">#       other smoothers</span>
<span class="s2"># TODO: fix iteration, do not define class with iterator methods, use looping;</span>
<span class="s2">#       add maximum iteration and other optional stop criteria</span>
<span class="s2"># fixed some of the dimension problems in PolySmoother,</span>
<span class="s2">#       now graph for example looks good</span>
<span class="s2"># NOTE: example script is now in examples folder</span>
<span class="s2">#update: I did some of the above, see module docstring</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">statsmodels.genmod </span><span class="s3">import </span><span class="s1">families</span>
<span class="s3">from </span><span class="s1">statsmodels.sandbox.nonparametric.smoothers </span><span class="s3">import </span><span class="s1">PolySmoother</span>
<span class="s3">from </span><span class="s1">statsmodels.genmod.generalized_linear_model </span><span class="s3">import </span><span class="s1">GLM</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s3">import </span><span class="s1">IterationLimitWarning</span><span class="s3">, </span><span class="s1">iteration_limit_doc</span>

<span class="s3">import </span><span class="s1">warnings</span>

<span class="s1">DEBUG = </span><span class="s3">False</span>

<span class="s3">def </span><span class="s1">default_smoother(x</span><span class="s3">, </span><span class="s1">s_arg=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s0">''' 
 
    '''</span>
<span class="s2">#    _x = x.copy()</span>
<span class="s2">#    _x.sort()</span>
    <span class="s1">_x = np.sort(x)</span>
    <span class="s1">n = x.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s2"># taken form smooth.spline in R</span>

    <span class="s2">#if n &lt; 50:</span>
    <span class="s3">if </span><span class="s1">n &lt; </span><span class="s4">500</span><span class="s1">:</span>
        <span class="s1">nknots = n</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">a1 = np.log(</span><span class="s4">50</span><span class="s1">) / np.log(</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">a2 = np.log(</span><span class="s4">100</span><span class="s1">) / np.log(</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">a3 = np.log(</span><span class="s4">140</span><span class="s1">) / np.log(</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">a4 = np.log(</span><span class="s4">200</span><span class="s1">) / np.log(</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">n &lt; </span><span class="s4">200</span><span class="s1">:</span>
            <span class="s1">nknots = </span><span class="s4">2</span><span class="s1">**(a1 + (a2 - a1) * (n - </span><span class="s4">50</span><span class="s1">)/</span><span class="s4">150.</span><span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">n &lt; </span><span class="s4">800</span><span class="s1">:</span>
            <span class="s1">nknots = </span><span class="s4">2</span><span class="s1">**(a2 + (a3 - a2) * (n - </span><span class="s4">200</span><span class="s1">)/</span><span class="s4">600.</span><span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">n &lt; </span><span class="s4">3200</span><span class="s1">:</span>
            <span class="s1">nknots = </span><span class="s4">2</span><span class="s1">**(a3 + (a4 - a3) * (n - </span><span class="s4">800</span><span class="s1">)/</span><span class="s4">2400.</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">nknots = </span><span class="s4">200 </span><span class="s1">+ (n - </span><span class="s4">3200.</span><span class="s1">)**</span><span class="s4">0.2</span>
    <span class="s1">knots = _x[np.linspace(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">n-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">nknots).astype(np.int32)]</span>

    <span class="s2">#s = SmoothingSpline(knots, x=x.copy())</span>
    <span class="s2">#when I set order=2, I get nans in the GAM prediction</span>
    <span class="s3">if </span><span class="s1">s_arg </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">order = </span><span class="s4">3 </span><span class="s2">#what about knots? need smoother *args or **kwds</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">order = s_arg</span>
    <span class="s1">s = PolySmoother(order</span><span class="s3">, </span><span class="s1">x=x.copy())  </span><span class="s2">#TODO: change order, why copy?</span>
<span class="s2">#    s.gram(d=2)</span>
<span class="s2">#    s.target_df = 5</span>
    <span class="s3">return </span><span class="s1">s</span>

<span class="s3">class </span><span class="s1">Offset:</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">fn</span><span class="s3">, </span><span class="s1">offset):</span>
        <span class="s1">self.fn = fn</span>
        <span class="s1">self.offset = offset</span>

    <span class="s3">def </span><span class="s1">__call__(self</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kw):</span>
        <span class="s3">return </span><span class="s1">self.fn(*args</span><span class="s3">, </span><span class="s1">**kw) + self.offset</span>

<span class="s3">class </span><span class="s1">Results:</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">Y</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">smoothers</span><span class="s3">, </span><span class="s1">family</span><span class="s3">, </span><span class="s1">offset):</span>
        <span class="s1">self.nobs</span><span class="s3">, </span><span class="s1">self.k_vars = exog.shape  </span><span class="s2">#assumes exog is 2d</span>
        <span class="s2">#weird: If I put the previous line after the definition of self.mu,</span>
        <span class="s2">#    then the attributed do not get added</span>
        <span class="s1">self.Y = Y</span>
        <span class="s1">self.alpha = alpha</span>
        <span class="s1">self.smoothers = smoothers</span>
        <span class="s1">self.offset = offset</span>
        <span class="s1">self.family = family</span>
        <span class="s1">self.exog = exog</span>
        <span class="s1">self.offset = offset</span>
        <span class="s1">self.mu = self.linkinversepredict(exog)  </span><span class="s2">#TODO: remove __call__</span>



    <span class="s3">def </span><span class="s1">__call__(self</span><span class="s3">, </span><span class="s1">exog):</span>
        <span class="s0">'''expected value ? check new GLM, same as mu for given exog 
        maybe remove this 
        '''</span>
        <span class="s3">return </span><span class="s1">self.linkinversepredict(exog)</span>

    <span class="s3">def </span><span class="s1">linkinversepredict(self</span><span class="s3">, </span><span class="s1">exog):  </span><span class="s2">#TODO what's the name in GLM</span>
        <span class="s0">'''expected value ? check new GLM, same as mu for given exog 
        '''</span>
        <span class="s3">return </span><span class="s1">self.family.link.inverse(self.predict(exog))</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">exog):</span>
        <span class="s0">'''predict response, sum of smoothed components 
        TODO: What's this in the case of GLM, corresponds to X*beta ? 
        '''</span>
        <span class="s2">#note: sum is here over axis=0,</span>
        <span class="s2">#TODO: transpose in smoothed and sum over axis=1</span>

        <span class="s2">#BUG: there is some inconsistent orientation somewhere</span>
        <span class="s2">#temporary hack, will not work for 1d</span>
        <span class="s2">#print dir(self)</span>
        <span class="s2">#print 'self.nobs, self.k_vars', self.nobs, self.k_vars</span>
        <span class="s1">exog_smoothed = self.smoothed(exog)</span>
        <span class="s2">#print 'exog_smoothed.shape', exog_smoothed.shape</span>
        <span class="s3">if </span><span class="s1">exog_smoothed.shape[</span><span class="s4">0</span><span class="s1">] == self.k_vars:</span>
            <span class="s3">import </span><span class="s1">warnings</span>
            <span class="s1">warnings.warn(</span><span class="s5">&quot;old orientation, colvars, will go away&quot;</span><span class="s3">,</span>
                          <span class="s1">FutureWarning)</span>
            <span class="s3">return </span><span class="s1">np.sum(self.smoothed(exog)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">) + self.alpha</span>
        <span class="s3">if </span><span class="s1">exog_smoothed.shape[</span><span class="s4">1</span><span class="s1">] == self.k_vars:</span>
            <span class="s3">return </span><span class="s1">np.sum(exog_smoothed</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">) + self.alpha</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'shape mismatch in predict'</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">smoothed(self</span><span class="s3">, </span><span class="s1">exog):</span>
        <span class="s0">'''get smoothed prediction for each component 
 
        '''</span>
        <span class="s2">#bug: with exog in predict I get a shape error</span>
        <span class="s2">#print 'smoothed', exog.shape, self.smoothers[0].predict(exog).shape</span>
        <span class="s2">#there was a mistake exog did not have column index i</span>
        <span class="s3">return </span><span class="s1">np.array([self.smoothers[i].predict(exog[:</span><span class="s3">,</span><span class="s1">i]) + self.offset[i]</span>
        <span class="s2">#should not be a mistake because exog[:,i] is attached to smoother, but</span>
        <span class="s2">#it is for different exog</span>
        <span class="s2">#return np.array([self.smoothers[i].predict() + self.offset[i]</span>
                         <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(exog.shape[</span><span class="s4">1</span><span class="s1">])]).T</span>

    <span class="s3">def </span><span class="s1">smoothed_demeaned(self</span><span class="s3">, </span><span class="s1">exog):</span>
        <span class="s1">components = self.smoothed(exog)</span>
        <span class="s1">means = components.mean(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">constant = means.sum() + self.alpha</span>
        <span class="s1">components_demeaned = components - means</span>
        <span class="s3">return </span><span class="s1">components_demeaned</span><span class="s3">, </span><span class="s1">constant</span>

<span class="s3">class </span><span class="s1">AdditiveModel:</span>
    <span class="s0">'''additive model with non-parametric, smoothed components 
 
    Parameters 
    ---------- 
    exog : ndarray 
    smoothers : None or list of smoother instances 
        smoother instances not yet checked 
    weights : None or ndarray 
    family : None or family instance 
        I think only used because of shared results with GAM and subclassing. 
        If None, then Gaussian is used. 
    '''</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">smoothers=</span><span class="s3">None, </span><span class="s1">weights=</span><span class="s3">None, </span><span class="s1">family=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">self.exog = exog</span>
        <span class="s3">if </span><span class="s1">weights </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">self.weights = weights</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.weights = np.ones(self.exog.shape[</span><span class="s4">0</span><span class="s1">])</span>

        <span class="s1">self.smoothers = smoothers </span><span class="s3">or </span><span class="s1">[default_smoother(exog[:</span><span class="s3">,</span><span class="s1">i]) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(exog.shape[</span><span class="s4">1</span><span class="s1">])]</span>

        <span class="s2">#TODO: why do we set here df, refactoring temporary?</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(exog.shape[</span><span class="s4">1</span><span class="s1">]):</span>
            <span class="s1">self.smoothers[i].df = </span><span class="s4">10</span>

        <span class="s3">if </span><span class="s1">family </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self.family = families.Gaussian()</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.family = family</span>
        <span class="s2">#self.family = families.Gaussian()</span>

    <span class="s3">def </span><span class="s1">_iter__(self):</span>
        <span class="s0">'''initialize iteration ?, should be removed 
 
        '''</span>
        <span class="s1">self.iter = </span><span class="s4">0</span>
        <span class="s1">self.dev = np.inf</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">next(self):</span>
        <span class="s0">'''internal calculation for one fit iteration 
 
        BUG: I think this does not improve, what is supposed to improve 
            offset does not seem to be used, neither an old alpha 
            The smoothers keep coef/params from previous iteration 
        '''</span>
        <span class="s1">_results = self.results</span>
        <span class="s1">Y = self.results.Y</span>
        <span class="s1">mu = _results.predict(self.exog)</span>
        <span class="s2">#TODO offset is never used ?</span>
        <span class="s1">offset = np.zeros(self.exog.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">np.float64)</span>
        <span class="s1">alpha = (Y * self.weights).sum() / self.weights.sum()</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.exog.shape[</span><span class="s4">1</span><span class="s1">]):</span>
            <span class="s1">tmp = self.smoothers[i].predict()</span>
            <span class="s2">#TODO: check what smooth needs to do</span>
            <span class="s2">#smooth (alias for fit, fit given x to new y and attach</span>
            <span class="s2">#print 'next shape', (Y - alpha - mu + tmp).shape</span>
            <span class="s1">bad = np.isnan(Y - alpha - mu + tmp).any()</span>
            <span class="s3">if </span><span class="s1">bad: </span><span class="s2">#temporary assert while debugging</span>
                <span class="s1">print(Y</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">mu</span><span class="s3">, </span><span class="s1">tmp)</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;nan encountered&quot;</span><span class="s1">)</span>
            <span class="s2">#self.smoothers[i].smooth(Y - alpha - mu + tmp,</span>
            <span class="s1">self.smoothers[i].smooth(Y - mu + tmp</span><span class="s3">,</span>
                                     <span class="s1">weights=self.weights)</span>
            <span class="s1">tmp2 = self.smoothers[i].predict() </span><span class="s2">#fittedvalues of previous smooth/fit</span>
            <span class="s1">self.results.offset[i] = -(tmp2*self.weights).sum() / self.weights.sum()</span>
            <span class="s2">#self.offset used in smoothed</span>
            <span class="s3">if </span><span class="s1">DEBUG:</span>
                <span class="s1">print(self.smoothers[i].params)</span>
            <span class="s1">mu += tmp2 - tmp</span>
        <span class="s2">#change setting offset here: tests still pass, offset equal to constant</span>
        <span class="s2">#in component ??? what's the effect of offset</span>
        <span class="s1">offset = self.results.offset</span>
        <span class="s2">#print self.iter</span>
        <span class="s2">#self.iter += 1 #missing incrementing of iter counter NOT</span>
        <span class="s3">return </span><span class="s1">Results(Y</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">, </span><span class="s1">self.smoothers</span><span class="s3">, </span><span class="s1">self.family</span><span class="s3">, </span><span class="s1">offset)</span>

    <span class="s3">def </span><span class="s1">cont(self):</span>
        <span class="s0">'''condition to continue iteration loop 
 
        Parameters 
        ---------- 
        tol 
 
        Returns 
        ------- 
        cont : bool 
            If true, then iteration should be continued. 
 
        '''</span>
        <span class="s1">self.iter += </span><span class="s4">1 </span><span class="s2">#moved here to always count, not necessary</span>
        <span class="s3">if </span><span class="s1">DEBUG:</span>
            <span class="s1">print(self.iter</span><span class="s3">, </span><span class="s1">self.results.Y.shape)</span>
            <span class="s1">print(self.results.predict(self.exog).shape</span><span class="s3">, </span><span class="s1">self.weights.shape)</span>
        <span class="s1">curdev = (((self.results.Y - self.results.predict(self.exog))**</span><span class="s4">2</span><span class="s1">) * self.weights).sum()</span>

        <span class="s3">if </span><span class="s1">self.iter &gt; self.maxiter: </span><span class="s2">#kill it, no max iterationoption</span>
            <span class="s3">return False</span>
        <span class="s3">if </span><span class="s1">np.fabs((self.dev - curdev) / curdev) &lt; self.rtol:</span>
            <span class="s1">self.dev = curdev</span>
            <span class="s3">return False</span>

        <span class="s2">#self.iter += 1</span>
        <span class="s1">self.dev = curdev</span>
        <span class="s3">return True</span>

    <span class="s3">def </span><span class="s1">df_resid(self):</span>
        <span class="s0">'''degrees of freedom of residuals, ddof is sum of all smoothers df 
        '''</span>
        <span class="s3">return </span><span class="s1">self.results.Y.shape[</span><span class="s4">0</span><span class="s1">] - np.array([self.smoothers[i].df_fit() </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.exog.shape[</span><span class="s4">1</span><span class="s1">])]).sum()</span>

    <span class="s3">def </span><span class="s1">estimate_scale(self):</span>
        <span class="s0">'''estimate standard deviation of residuals 
        '''</span>
        <span class="s2">#TODO: remove use of self.results.__call__</span>
        <span class="s3">return </span><span class="s1">((self.results.Y - self.results(self.exog))**</span><span class="s4">2</span><span class="s1">).sum() / self.df_resid()</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">Y</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1.0e-06</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s4">30</span><span class="s1">):</span>
        <span class="s0">'''fit the model to a given endogenous variable Y 
 
        This needs to change for consistency with statsmodels 
 
        '''</span>
        <span class="s1">self.rtol = rtol</span>
        <span class="s1">self.maxiter = maxiter</span>
        <span class="s2">#iter(self)  # what does this do? anything?</span>
        <span class="s1">self._iter__()</span>
        <span class="s1">mu = </span><span class="s4">0</span>
        <span class="s1">alpha = (Y * self.weights).sum() / self.weights.sum()</span>

        <span class="s1">offset = np.zeros(self.exog.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">np.float64)</span>

        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.exog.shape[</span><span class="s4">1</span><span class="s1">]):</span>
            <span class="s1">self.smoothers[i].smooth(Y - alpha - mu</span><span class="s3">,</span>
                                     <span class="s1">weights=self.weights)</span>
            <span class="s1">tmp = self.smoothers[i].predict()</span>
            <span class="s1">offset[i] = (tmp * self.weights).sum() / self.weights.sum()</span>
            <span class="s1">tmp -= tmp.sum()</span>
            <span class="s1">mu += tmp</span>

        <span class="s1">self.results = Results(Y</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">, </span><span class="s1">self.smoothers</span><span class="s3">, </span><span class="s1">self.family</span><span class="s3">, </span><span class="s1">offset)</span>

        <span class="s3">while </span><span class="s1">self.cont():</span>
            <span class="s1">self.results = self.next()</span>

        <span class="s3">if </span><span class="s1">self.iter &gt;= self.maxiter:</span>
            <span class="s1">warnings.warn(iteration_limit_doc</span><span class="s3">, </span><span class="s1">IterationLimitWarning)</span>

        <span class="s3">return </span><span class="s1">self.results</span>

<span class="s3">class </span><span class="s1">Model(GLM</span><span class="s3">, </span><span class="s1">AdditiveModel):</span>
<span class="s2">#class Model(AdditiveModel):</span>
    <span class="s2">#TODO: what does GLM do? Is it actually used ?</span>
    <span class="s2">#only used in __init__, dropping it does not change results</span>
    <span class="s2">#but where gets family attached now? - weird, it's Gaussian in this case now</span>
    <span class="s2">#also where is the link defined?</span>
    <span class="s2">#AdditiveModel overwrites family and sets it to Gaussian - corrected</span>

    <span class="s2">#I think both GLM and AdditiveModel subclassing is only used in __init__</span>

    <span class="s2">#niter = 2</span>

<span class="s2">#    def __init__(self, exog, smoothers=None, family=family.Gaussian()):</span>
<span class="s2">#        GLM.__init__(self, exog, family=family)</span>
<span class="s2">#        AdditiveModel.__init__(self, exog, smoothers=smoothers)</span>
<span class="s2">#        self.family = family</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">smoothers=</span><span class="s3">None, </span><span class="s1">family=families.Gaussian()):</span>
        <span class="s2">#self.family = family</span>
        <span class="s2">#TODO: inconsistent super __init__</span>
        <span class="s1">AdditiveModel.__init__(self</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">smoothers=smoothers</span><span class="s3">, </span><span class="s1">family=family)</span>
        <span class="s1">GLM.__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">family=family)</span>
        <span class="s3">assert </span><span class="s1">self.family </span><span class="s3">is </span><span class="s1">family  </span><span class="s2">#make sure we got the right family</span>

    <span class="s3">def </span><span class="s1">next(self):</span>
        <span class="s1">_results = self.results</span>
        <span class="s1">Y = _results.Y</span>
        <span class="s3">if </span><span class="s1">np.isnan(self.weights).all():</span>
            <span class="s1">print(</span><span class="s5">&quot;nanweights1&quot;</span><span class="s1">)</span>

        <span class="s1">_results.mu = self.family.link.inverse(_results.predict(self.exog))</span>
        <span class="s2">#eta = _results.predict(self.exog)</span>
        <span class="s2">#_results.mu = self.family.fitted(eta)</span>
        <span class="s1">weights = self.family.weights(_results.mu)</span>
        <span class="s3">if </span><span class="s1">np.isnan(weights).all():</span>
            <span class="s1">self.weights = weights</span>
            <span class="s1">print(</span><span class="s5">&quot;nanweights2&quot;</span><span class="s1">)</span>
        <span class="s1">self.weights = weights</span>
        <span class="s3">if </span><span class="s1">DEBUG:</span>
            <span class="s1">print(</span><span class="s5">'deriv isnan'</span><span class="s3">, </span><span class="s1">np.isnan(self.family.link.deriv(_results.mu)).any())</span>

        <span class="s2">#Z = _results.predict(self.exog) + \</span>
        <span class="s1">Z = _results.predict(self.exog) + \</span>
               <span class="s1">self.family.link.deriv(_results.mu) * (Y - _results.mu) </span><span class="s2">#- _results.alpha #?added alpha</span>

        <span class="s1">m = AdditiveModel(self.exog</span><span class="s3">, </span><span class="s1">smoothers=self.smoothers</span><span class="s3">,</span>
                          <span class="s1">weights=self.weights</span><span class="s3">, </span><span class="s1">family=self.family)</span>

        <span class="s2">#TODO: I do not know what the next two lines do, Z, Y ? which is endog?</span>
        <span class="s2">#Y is original endog, Z is endog for the next step in the iterative solver</span>

        <span class="s1">_results = m.fit(Z)</span>
        <span class="s1">self.history.append([Z</span><span class="s3">, </span><span class="s1">_results.predict(self.exog)])</span>
        <span class="s1">_results.Y = Y</span>
        <span class="s1">_results.mu = self.family.link.inverse(_results.predict(self.exog))</span>
        <span class="s1">self.iter += </span><span class="s4">1</span>
        <span class="s1">self.results = _results</span>

        <span class="s3">return </span><span class="s1">_results</span>

    <span class="s3">def </span><span class="s1">estimate_scale(self</span><span class="s3">, </span><span class="s1">Y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return Pearson\'s X^2 estimate of scale. 
        &quot;&quot;&quot;</span>

        <span class="s3">if </span><span class="s1">Y </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">Y = self.Y</span>
        <span class="s1">resid = Y - self.results.mu</span>
        <span class="s3">return </span><span class="s1">(np.power(resid</span><span class="s3">, </span><span class="s4">2</span><span class="s1">) / self.family.variance(self.results.mu)).sum() \</span>
                    <span class="s1">/ self.df_resid   </span><span class="s2">#TODO check this</span>
                    <span class="s2">#/ AdditiveModel.df_resid(self)  #what is the class doing here?</span>


    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">Y</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1.0e-06</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s4">30</span><span class="s1">):</span>

        <span class="s1">self.rtol = rtol</span>
        <span class="s1">self.maxiter = maxiter</span>

        <span class="s1">self.Y = np.asarray(Y</span><span class="s3">, </span><span class="s1">np.float64)</span>

        <span class="s1">self.history = []</span>

        <span class="s2">#iter(self)</span>
        <span class="s1">self._iter__()</span>

        <span class="s2">#TODO code duplication with next?</span>
        <span class="s1">alpha = self.Y.mean()</span>
        <span class="s1">mu0 = self.family.starting_mu(Y)</span>
        <span class="s2">#Z = self.family.link(alpha) + self.family.link.deriv(alpha) * (Y - alpha)</span>
        <span class="s1">Z = self.family.link(alpha) + self.family.link.deriv(alpha) * (Y - mu0)</span>
        <span class="s1">m = AdditiveModel(self.exog</span><span class="s3">, </span><span class="s1">smoothers=self.smoothers</span><span class="s3">, </span><span class="s1">family=self.family)</span>
        <span class="s1">self.results = m.fit(Z)</span>
        <span class="s1">self.results.mu = self.family.link.inverse(self.results.predict(self.exog))</span>
        <span class="s1">self.results.Y = Y</span>

        <span class="s3">while </span><span class="s1">self.cont():</span>
            <span class="s1">self.results = self.next()</span>
            <span class="s1">self.scale = self.results.scale = self.estimate_scale()</span>

        <span class="s3">if </span><span class="s1">self.iter &gt;= self.maxiter:</span>
            <span class="s3">import </span><span class="s1">warnings</span>
            <span class="s1">warnings.warn(iteration_limit_doc</span><span class="s3">, </span><span class="s1">IterationLimitWarning)</span>

        <span class="s3">return </span><span class="s1">self.results</span>
</pre>
</body>
</html>