<html>
<head>
<title>nonlinls.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
nonlinls.py</font>
</center></td></tr></table>
<pre><span class="s0">'''Non-linear least squares 
 
 
 
Author: Josef Perktold based on scipy.optimize.curve_fit 
 
'''</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">optimize</span>

<span class="s2">from </span><span class="s1">statsmodels.base.model </span><span class="s2">import </span><span class="s1">Model</span>


<span class="s2">class </span><span class="s1">Results:</span>
    <span class="s0">'''just a dummy placeholder for now 
    most results from RegressionResults can be used here 
    '''</span>
    <span class="s2">pass</span>


<span class="s3">##def getjaccov(retval, n):</span>
<span class="s3">##    '''calculate something and raw covariance matrix from return of optimize.leastsq</span>
<span class="s3">##</span>
<span class="s3">##    I cannot figure out how to recover the Jacobian, or whether it is even</span>
<span class="s3">##    possible</span>
<span class="s3">##</span>
<span class="s3">##    this is a partial copy of scipy.optimize.leastsq</span>
<span class="s3">##    '''</span>
<span class="s3">##    info = retval[-1]</span>
<span class="s3">##    #n = len(x0)  #nparams, where do I get this</span>
<span class="s3">##    cov_x = None</span>
<span class="s3">##    if info in [1,2,3,4]:</span>
<span class="s3">##        from numpy.dual import inv</span>
<span class="s3">##        from numpy.linalg import LinAlgError</span>
<span class="s3">##        perm = np.take(np.eye(n), retval[1]['ipvt']-1,0)</span>
<span class="s3">##        r = np.triu(np.transpose(retval[1]['fjac'])[:n,:])</span>
<span class="s3">##        R = np.dot(r, perm)</span>
<span class="s3">##        try:</span>
<span class="s3">##            cov_x = inv(np.dot(np.transpose(R),R))</span>
<span class="s3">##        except LinAlgError:</span>
<span class="s3">##            print 'cov_x not available'</span>
<span class="s3">##            pass</span>
<span class="s3">##        return r, R, cov_x</span>
<span class="s3">##</span>
<span class="s3">##def _general_function(params, xdata, ydata, function):</span>
<span class="s3">##    return function(xdata, *params) - ydata</span>
<span class="s3">##</span>
<span class="s3">##def _weighted_general_function(params, xdata, ydata, function, weights):</span>
<span class="s3">##    return weights * (function(xdata, *params) - ydata)</span>
<span class="s3">##</span>



<span class="s2">class </span><span class="s1">NonlinearLS(Model):  </span><span class="s3">#or subclass a model</span>
    <span class="s0">r'''Base class for estimation of a non-linear model with least squares 
 
    This class is supposed to be subclassed, and the subclass has to provide a method 
    `_predict` that defines the non-linear function `f(params) that is predicting the endogenous 
    variable. The model is assumed to be 
 
    :math: y = f(params) + error 
 
    and the estimator minimizes the sum of squares of the estimated error. 
 
    :math: min_parmas \sum (y - f(params))**2 
 
    f has to return the prediction for each observation. Exogenous or explanatory variables 
    should be accessed as attributes of the class instance, and can be given as arguments 
    when the instance is created. 
 
    Warning: 
    Weights are not correctly handled yet in the results statistics, 
    but included when estimating the parameters. 
 
    similar to scipy.optimize.curve_fit 
    API difference: params are array_like not split up, need n_params information 
 
    includes now weights similar to curve_fit 
    no general sigma yet (OLS and WLS, but no GLS) 
 
    This is currently holding on to intermediate results that are not necessary 
    but useful for testing. 
 
    Fit returns and instance of RegressionResult, in contrast to the linear 
    model, results in this case are based on a local approximation, essentially 
    y = f(X, params) is replaced by y = grad * params where grad is the Gradient 
    or Jacobian with the shape (nobs, nparams). See for example Greene 
 
    Examples 
    -------- 
 
    class Myfunc(NonlinearLS): 
 
        def _predict(self, params): 
            x = self.exog 
            a, b, c = params 
            return a*np.exp(-b*x) + c 
 
    Ff we have data (y, x), we can create an instance and fit it with 
 
    mymod = Myfunc(y, x) 
    myres = mymod.fit(nparams=3) 
 
    and use the non-linear regression results, for example 
 
    myres.params 
    myres.bse 
    myres.tvalues 
 
 
    '''</span>
    <span class="s3">#NOTE: This needs to call super for data checking</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">endog=</span><span class="s2">None, </span><span class="s1">exog=</span><span class="s2">None, </span><span class="s1">weights=</span><span class="s2">None, </span><span class="s1">sigma=</span><span class="s2">None,</span>
            <span class="s1">missing=</span><span class="s4">'none'</span><span class="s1">):</span>
        <span class="s1">self.endog = endog</span>
        <span class="s1">self.exog = exog</span>
        <span class="s2">if </span><span class="s1">sigma </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">sigma = np.asarray(sigma)</span>
            <span class="s2">if </span><span class="s1">sigma.ndim &lt; </span><span class="s5">2</span><span class="s1">:</span>
                <span class="s1">self.sigma = sigma</span>
                <span class="s1">self.weights = </span><span class="s5">1.</span><span class="s1">/sigma</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">'correlated errors are not handled yet'</span><span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self.weights = </span><span class="s2">None</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">params=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s3">#copied from GLS, Model has different signature</span>
        <span class="s2">return </span><span class="s1">self._predict(params)</span>


    <span class="s2">def </span><span class="s1">_predict(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s2">pass</span>

    <span class="s2">def </span><span class="s1">start_value(self):</span>
        <span class="s2">return None</span>

    <span class="s2">def </span><span class="s1">geterrors(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">weights=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">if </span><span class="s1">weights </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">self.weights </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">self.endog - self._predict(params)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">weights = self.weights</span>
        <span class="s2">return </span><span class="s1">weights * (self.endog - self._predict(params))</span>

    <span class="s2">def </span><span class="s1">errorsumsquares(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s2">return </span><span class="s1">(self.geterrors(params)**</span><span class="s5">2</span><span class="s1">).sum()</span>


    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">start_value=</span><span class="s2">None, </span><span class="s1">nparams=</span><span class="s2">None, </span><span class="s1">**kw):</span>
        <span class="s3">#if hasattr(self, 'start_value'):</span>
        <span class="s3">#I added start_value even if it's empty, not sure about it</span>
        <span class="s3">#but it makes a visible placeholder</span>

        <span class="s2">if </span><span class="s1">start_value </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">p0 = start_value</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s3">#nesting so that start_value is only calculated if it is needed</span>
            <span class="s1">p0 = self.start_value()</span>
            <span class="s2">if </span><span class="s1">p0 </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s2">pass</span>
            <span class="s2">elif </span><span class="s1">nparams </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">p0 = </span><span class="s5">0.1 </span><span class="s1">* np.ones(nparams)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">'need information about start values for' </span><span class="s1">+</span>
                             <span class="s4">'optimization'</span><span class="s1">)</span>

        <span class="s1">func = self.geterrors</span>
        <span class="s1">res = optimize.leastsq(func</span><span class="s2">, </span><span class="s1">p0</span><span class="s2">, </span><span class="s1">full_output=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">**kw)</span>
        <span class="s1">(popt</span><span class="s2">, </span><span class="s1">pcov</span><span class="s2">, </span><span class="s1">infodict</span><span class="s2">, </span><span class="s1">errmsg</span><span class="s2">, </span><span class="s1">ier) = res</span>

        <span class="s2">if </span><span class="s1">ier </span><span class="s2">not in </span><span class="s1">[</span><span class="s5">1</span><span class="s2">,</span><span class="s5">2</span><span class="s2">,</span><span class="s5">3</span><span class="s2">,</span><span class="s5">4</span><span class="s1">]:</span>
            <span class="s1">msg = </span><span class="s4">&quot;Optimal parameters not found: &quot; </span><span class="s1">+ errmsg</span>
            <span class="s2">raise </span><span class="s1">RuntimeError(msg)</span>

        <span class="s1">err = infodict[</span><span class="s4">'fvec'</span><span class="s1">]</span>

        <span class="s1">ydata = self.endog</span>
        <span class="s2">if </span><span class="s1">(len(ydata) &gt; len(p0)) </span><span class="s2">and </span><span class="s1">pcov </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s3">#this can use the returned errors instead of recalculating</span>

            <span class="s1">s_sq = (err**</span><span class="s5">2</span><span class="s1">).sum()/(len(ydata)-len(p0))</span>
            <span class="s1">pcov = pcov * s_sq</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">pcov = </span><span class="s2">None</span>

        <span class="s1">self.df_resid = len(ydata)-len(p0)</span>
        <span class="s1">self.df_model = len(p0)</span>
        <span class="s1">fitres = Results()</span>
        <span class="s1">fitres.params = popt</span>
        <span class="s1">fitres.pcov = pcov</span>
        <span class="s1">fitres.rawres = res</span>
        <span class="s1">self.wendog = self.endog  </span><span class="s3">#add weights</span>
        <span class="s1">self.wexog = self.jac_predict(popt)</span>
        <span class="s1">pinv_wexog = np.linalg.pinv(self.wexog)</span>
        <span class="s1">self.normalized_cov_params = np.dot(pinv_wexog</span><span class="s2">,</span>
                                         <span class="s1">np.transpose(pinv_wexog))</span>

        <span class="s3">#TODO: check effect of `weights` on result statistics</span>
        <span class="s3">#I think they are correctly included in cov_params</span>
        <span class="s3">#maybe not anymore, I'm not using pcov of leastsq</span>
        <span class="s3">#direct calculation with jac_predict misses the weights</span>

<span class="s3">##        if not weights is None</span>
<span class="s3">##            fitres.wexogw = self.weights * self.jacpredict(popt)</span>
        <span class="s2">from </span><span class="s1">statsmodels.regression </span><span class="s2">import </span><span class="s1">RegressionResults</span>
        <span class="s1">results = RegressionResults</span>

        <span class="s1">beta = popt</span>
        <span class="s1">lfit = RegressionResults(self</span><span class="s2">, </span><span class="s1">beta</span><span class="s2">,</span>
                       <span class="s1">normalized_cov_params=self.normalized_cov_params)</span>

        <span class="s1">lfit.fitres = fitres   </span><span class="s3">#mainly for testing</span>
        <span class="s1">self._results = lfit</span>
        <span class="s2">return </span><span class="s1">lfit</span>

    <span class="s2">def </span><span class="s1">fit_minimal(self</span><span class="s2">, </span><span class="s1">start_value</span><span class="s2">, </span><span class="s1">**kwargs):</span>
        <span class="s0">'''minimal fitting with no extra calculations'''</span>
        <span class="s1">func = self.geterrors</span>
        <span class="s1">res = optimize.leastsq(func</span><span class="s2">, </span><span class="s1">start_value</span><span class="s2">, </span><span class="s1">full_output=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">**kwargs)</span>
        <span class="s2">return </span><span class="s1">res</span>

    <span class="s2">def </span><span class="s1">fit_random(self</span><span class="s2">, </span><span class="s1">ntries=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">rvs_generator=</span><span class="s2">None, </span><span class="s1">nparams=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">'''fit with random starting values 
 
        this could be replaced with a global fitter 
 
        '''</span>

        <span class="s2">if </span><span class="s1">nparams </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">nparams = self.nparams</span>
        <span class="s2">if </span><span class="s1">rvs_generator </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">rvs = np.random.uniform(low=-</span><span class="s5">10</span><span class="s2">, </span><span class="s1">high=</span><span class="s5">10</span><span class="s2">, </span><span class="s1">size=(ntries</span><span class="s2">, </span><span class="s1">nparams))</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">rvs = rvs_generator(size=(ntries</span><span class="s2">, </span><span class="s1">nparams))</span>

        <span class="s1">results = np.array([np.r_[self.fit_minimal(rv)</span><span class="s2">,  </span><span class="s1">rv] </span><span class="s2">for </span><span class="s1">rv </span><span class="s2">in </span><span class="s1">rvs])</span>
        <span class="s3">#selct best results and check how many solutions are within 1e-6 of best</span>
        <span class="s3">#not sure what leastsq returns</span>
        <span class="s2">return </span><span class="s1">results</span>

    <span class="s2">def </span><span class="s1">jac_predict(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s0">'''jacobian of prediction function using complex step derivative 
 
        This assumes that the predict function does not use complex variable 
        but is designed to do so. 
 
        '''</span>
        <span class="s2">from </span><span class="s1">statsmodels.tools.numdiff </span><span class="s2">import </span><span class="s1">approx_fprime_cs</span>

        <span class="s1">jaccs_err = approx_fprime_cs(params</span><span class="s2">, </span><span class="s1">self._predict)</span>
        <span class="s2">return </span><span class="s1">jaccs_err</span>


<span class="s2">class </span><span class="s1">Myfunc(NonlinearLS):</span>

    <span class="s3">#predict model.Model has a different signature</span>
<span class="s3">##    def predict(self, params, exog=None):</span>
<span class="s3">##        if not exog is None:</span>
<span class="s3">##            x = exog</span>
<span class="s3">##        else:</span>
<span class="s3">##            x = self.exog</span>
<span class="s3">##        a, b, c = params</span>
<span class="s3">##        return a*np.exp(-b*x) + c</span>

    <span class="s2">def </span><span class="s1">_predict(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s1">x = self.exog</span>
        <span class="s1">a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">c = params</span>
        <span class="s2">return </span><span class="s1">a*np.exp(-b*x) + c</span>





<span class="s2">if </span><span class="s1">__name__ == </span><span class="s4">'__main__'</span><span class="s1">:</span>
    <span class="s2">def </span><span class="s1">func0(x</span><span class="s2">, </span><span class="s1">a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">c):</span>
        <span class="s2">return </span><span class="s1">a*np.exp(-b*x) + c</span>

    <span class="s2">def </span><span class="s1">func(params</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s1">a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">c = params</span>
        <span class="s2">return </span><span class="s1">a*np.exp(-b*x) + c</span>

    <span class="s2">def </span><span class="s1">error(params</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">return </span><span class="s1">y - func(params</span><span class="s2">, </span><span class="s1">x)</span>

    <span class="s2">def </span><span class="s1">error2(params</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">return </span><span class="s1">(y - func(params</span><span class="s2">, </span><span class="s1">x))**</span><span class="s5">2</span>




    <span class="s1">x = np.linspace(</span><span class="s5">0</span><span class="s2">,</span><span class="s5">4</span><span class="s2">,</span><span class="s5">50</span><span class="s1">)</span>
    <span class="s1">params = np.array([</span><span class="s5">2.5</span><span class="s2">, </span><span class="s5">1.3</span><span class="s2">, </span><span class="s5">0.5</span><span class="s1">])</span>
    <span class="s1">y0 = func(params</span><span class="s2">, </span><span class="s1">x)</span>
    <span class="s1">y = y0 + </span><span class="s5">0.2</span><span class="s1">*np.random.normal(size=len(x))</span>

    <span class="s1">res = optimize.leastsq(error</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">args=(x</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">, </span><span class="s1">full_output=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s3">##    r, R, c = getjaccov(res[1:], 3)</span>

    <span class="s1">mod = Myfunc(y</span><span class="s2">, </span><span class="s1">x)</span>
    <span class="s1">resmy = mod.fit(nparams=</span><span class="s5">3</span><span class="s1">)</span>

    <span class="s1">cf_params</span><span class="s2">, </span><span class="s1">cf_pcov = optimize.curve_fit(func0</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">cf_bse = np.sqrt(np.diag(cf_pcov))</span>
    <span class="s1">print(res[</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">print(cf_params)</span>
    <span class="s1">print(resmy.params)</span>
    <span class="s1">print(cf_bse)</span>
    <span class="s1">print(resmy.bse)</span>
</pre>
</body>
</html>