<html>
<head>
<title>_diagnostic_other.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_diagnostic_other.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot;Score, lagrange multiplier and conditional moment tests 
robust to misspecification or without specification of higher moments 
 
Created on Thu Oct 30 00:42:38 2014 
 
Author: Josef Perktold 
License: BSD-3 
 
Notes 
----- 
 
This module is a mixture of very general and very specific functions for 
hypothesis testing in general models, targeted mainly to non-normal models. 
 
Some of the options or versions of these tests are mainly intented for 
cross-checking and to replicate different examples in references. 
 
We need clean versions with good defaults for those functions that are 
intended for the user. 
 
 
 
References 
---------- 
 
The following references are collected after my intitial implementation and is 
most likely not exactly what I used. 
 
The main articles on which the functions are directly based upon, are Boos 1992, 
Tauchen 1985 and Whitney 1985a. Wooldrige artificial regression is 
based on several articles and his text book. 
Background reading are the textbooks by Cameron and Trivedi, Wooldridge and 
Davidson and MacKinnon. 
Newey and MacFadden 1994 provide some of the theoretical background. 
 
Poisson dispersion tests are based on Dean 1992 and articles and text books by 
Cameron and Trivedi. 
 
The references currently do not include the literature on LM-test for 
specification and diagnostic testing, like Pagan, Bera, Bera and Yoon and 
many others, except those for the Poisson excess dispersion case and Pagan 
and Vella. 
 
 
Boos, Dennis D. 1992. “On Generalized Score Tests.” The American Statistician 46 
(4): 327–33. https://doi.org/10.2307/2685328. 
 
Breslow, Norman. 1989. “Score Tests in Overdispersed GLM’s.” In Statistical 
Modelling, edited by Adriano Decarli, Brian J. Francis, Robert Gilchrist, and 
Gilg U. H. Seeber, 64–74. Lecture Notes in Statistics 57. Springer New York. 
http://link.springer.com/chapter/10.1007/978-1-4612-3680-1_8. 
 
Breslow, Norman. 1990. “Tests of Hypotheses in Overdispersed Poisson Regression 
and Other Quasi- Likelihood Models.” Journal of the American Statistical 
Association 85 (410): 565–71. https://doi.org/10.2307/2289799. 
 
Cameron, A. Colin, and Pravin K. Trivedi. 1986. “Econometric Models Based on 
Count Data. Comparisons and Applications of Some Estimators and Tests.” Journal 
of Applied Econometrics 1 (1): 29–53. https://doi.org/10.1002/jae.3950010104. 
 
Cameron, A. Colin, and Pravin K. Trivedi. 1990a. “Conditional Moment Tests and 
Orthogonal Polynomials.” Indiana University, Department of Economics, Working 
Paper, 90–051. 
 
Cameron, A. Colin, and Pravin K. Trivedi. 1990b. “Regression-Based Tests for 
Overdispersion in the Poisson Model.” Journal of Econometrics 46 (3): 347–64. 
https://doi.org/10.1016/0304-4076(90)90014-K. 
 
Cameron, A. Colin, and Pravin K. Trivedi. Microeconometrics: methods and 
applications. Cambridge university press, 2005. 
 
Cameron, A. Colin, and Pravin K. Trivedi. Regression analysis of count data. 
Vol. 53. Cambridge university press, 2013. 
 
Davidson, Russell, and James G. MacKinnon. 1981. “Several Tests for Model 
Specification in the Presence of Alternative Hypotheses.” Econometrica 49 (3): 
781–93. https://doi.org/10.2307/1911522. 
 
Davidson, Russell, and James G. MacKinnon. 1990. “Specification Tests Based on 
Artificial Regressions.” Journal of the American Statistical Association 85 
(409): 220–27. https://doi.org/10.2307/2289548. 
 
Davidson, Russell, and James G. MacKinnon. 1991. “Artificial Regressions and C 
(α) Tests.” Economics Letters 35 (2): 149–53. 
https://doi.org/10.1016/0165-1765(91)90162-E. 
 
Davidson, Russell, and James G. MacKinnon. Econometric theory and methods. Vol. 
5. New York: Oxford University Press, 2004. 
 
Dean, C. B. 1992. “Testing for Overdispersion in Poisson and Binomial Regression 
Models.” Journal of the American Statistical Association 87 (418): 451–57. 
https://doi.org/10.2307/2290276. 
 
Dean, C., and J. F. Lawless. 1989. “Tests for Detecting Overdispersion in 
Poisson Regression Models.” Journal of the American Statistical Association 84 
(406): 467–72. https://doi.org/10.2307/2289931. 
 
Newey, Whitney K. 1985a. “Generalized Method of Moments Specification Testing.” 
Journal of Econometrics 29 (3): 229–56. 
https://doi.org/10.1016/0304-4076(85)90154-X. 
 
Newey, Whitney K. 1985b. “Maximum Likelihood Specification Testing and 
Conditional Moment Tests.” Econometrica 53 (5): 1047–70. 
https://doi.org/10.2307/1911011. 
 
Newey, Whitney K. and Kenneth D. West. 1987. “Hypothesis Testing with Efficient 
Method of Moments Estimation.” International Economic Review 28 (3): 777–87. 
https://doi.org/10.2307/2526578. 
 
Newey, Whitney K. and Daniel McFadden. 1994 &quot;Large sample estimation and 
hypothesis testing.&quot; Handbook of econometrics 4: 2111-2245. 
 
Pagan, Adrian, and Frank Vella. 1989. “Diagnostic Tests for Models Based on 
Individual Data: A Survey.” Journal of Applied Econometrics 4 (S1): S29–59. 
https://doi.org/10.1002/jae.3950040504. 
 
Tauchen, George. 1985. “Diagnostic Testing and Evaluation of Maximum Likelihood 
Models.” Journal of Econometrics 30 (1–2): 415–43. 
https://doi.org/10.1016/0304-4076(85)90149-6. 
 
White, Halbert. 1981. “Consequences and Detection of Misspecified Nonlinear 
Regression Models.” Journal of the American Statistical Association 76 (374): 
419–33. https://doi.org/10.2307/2287845. 
 
White, Halbert. 1983. “Maximum Likelihood Estimation of Misspecified Models.” 
Econometrica 51 (2): 513. https://doi.org/10.2307/1912004. 
 
White, Halbert. 1994. Estimation, Inference and Specification Analysis. 
Cambridge: Cambridge University Press. https://doi.org/10.1017/CCOL0521252806. 
 
Wooldridge, Jeffrey M. 1991. “Specification Testing and Quasi-Maximum- 
Likelihood Estimation.” Journal of Econometrics 48 (1–2): 29–55. 
https://doi.org/10.1016/0304-4076(91)90031-8. 
 
Wooldridge, Jeffrey M. 1990. “A Unified Approach to Robust, Regression-Based 
Specification Tests.” Econometric Theory 6 (1): 17–43. 
 
Wooldridge, Jeffrey M. 1991a. “On the Application of Robust, Regression- Based 
Diagnostics to Models of Conditional Means and Conditional Variances.” Journal 
of Econometrics 47 (1): 5–46. https://doi.org/10.1016/0304-4076(91)90076-P. 
 
Wooldridge, Jeffrey M. 1991b. “On the Application of Robust, Regression- Based 
Diagnostics to Models of Conditional Means and Conditional Variances.” Journal 
of Econometrics 47 (1): 5–46. https://doi.org/10.1016/0304-4076(91)90076-P. 
 
Wooldridge, Jeffrey M. 1991c. “Specification Testing and Quasi-Maximum- 
Likelihood Estimation.” Journal of Econometrics 48 (1–2): 29–55. 
https://doi.org/10.1016/0304-4076(91)90031-8. 
 
Wooldridge, Jeffrey M. 1994. “On the Limits of GLM for Specification Testing: A 
Comment on Gurmu and Trivedi.” Econometric Theory 10 (2): 409–18. 
https://doi.org/10.2307/3532875. 
 
Wooldridge, Jeffrey M. 1997. “Quasi-Likelihood Methods for Count Data.” Handbook 
of Applied Econometrics 2: 352–406. 
 
Wooldridge, Jeffrey M. Econometric analysis of cross section and panel data. MIT 
press, 2010. 
 
&quot;&quot;&quot;</span>
<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">stats</span>

<span class="s3">from </span><span class="s1">statsmodels.tools.decorators </span><span class="s3">import </span><span class="s1">cache_readonly</span>
<span class="s3">from </span><span class="s1">statsmodels.regression.linear_model </span><span class="s3">import </span><span class="s1">OLS</span>


<span class="s0"># deprecated dispersion functions, moved to discrete._diagnostic_count</span>


<span class="s3">def </span><span class="s1">dispersion_poisson(results):</span>
    <span class="s2">&quot;&quot;&quot;Score/LM type tests for Poisson variance assumptions 
 
    .. deprecated:: 0.14 
 
       dispersion_poisson moved to discrete._diagnostic_count 
 
    Null Hypothesis is 
 
    H0: var(y) = E(y) and assuming E(y) is correctly specified 
    H1: var(y) ~= E(y) 
 
    The tests are based on the constrained model, i.e. the Poisson model. 
    The tests differ in their assumed alternatives, and in their maintained 
    assumptions. 
 
    Parameters 
    ---------- 
    results : Poisson results instance 
        This can be a results instance for either a discrete Poisson or a GLM 
        with family Poisson. 
 
    Returns 
    ------- 
    res : ndarray, shape (7, 2) 
       each row contains the test statistic and p-value for one of the 7 tests 
       computed here. 
    description : 2-D list of strings 
       Each test has two strings a descriptive name and a string for the 
       alternative hypothesis. 
    &quot;&quot;&quot;</span>
    <span class="s1">msg = (</span>
        <span class="s4">'dispersion_poisson here is deprecated, use the version in '</span>
        <span class="s4">'discrete._diagnostic_count'</span>
    <span class="s1">)</span>
    <span class="s1">warnings.warn(msg</span><span class="s3">, </span><span class="s1">FutureWarning)</span>

    <span class="s3">from </span><span class="s1">statsmodels.discrete._diagnostics_count </span><span class="s3">import </span><span class="s1">test_poisson_dispersion</span>
    <span class="s3">return </span><span class="s1">test_poisson_dispersion(results</span><span class="s3">, </span><span class="s1">_old=</span><span class="s3">True</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">dispersion_poisson_generic(results</span><span class="s3">, </span><span class="s1">exog_new_test</span><span class="s3">, </span><span class="s1">exog_new_control=</span><span class="s3">None,</span>
                               <span class="s1">include_score=</span><span class="s3">False, </span><span class="s1">use_endog=</span><span class="s3">True,</span>
                               <span class="s1">cov_type=</span><span class="s4">'HC3'</span><span class="s3">, </span><span class="s1">cov_kwds=</span><span class="s3">None, </span><span class="s1">use_t=</span><span class="s3">False</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;A variable addition test for the variance function 
 
    .. deprecated:: 0.14 
 
       dispersion_poisson_generic moved to discrete._diagnostic_count 
 
    This uses an artificial regression to calculate a variant of an LM or 
    generalized score test for the specification of the variance assumption 
    in a Poisson model. The performed test is a Wald test on the coefficients 
    of the `exog_new_test`. 
 
    Warning: insufficiently tested, especially for options 
    &quot;&quot;&quot;</span>
    <span class="s1">msg = (</span>
        <span class="s4">'dispersion_poisson_generic here is deprecated, use the version in '</span>
        <span class="s4">'discrete._diagnostic_count'</span>
    <span class="s1">)</span>
    <span class="s1">warnings.warn(msg</span><span class="s3">, </span><span class="s1">FutureWarning)</span>

    <span class="s3">from </span><span class="s1">statsmodels.discrete._diagnostics_count </span><span class="s3">import </span><span class="s1">(</span>
        <span class="s1">_test_poisson_dispersion_generic</span>
        <span class="s1">)</span>

    <span class="s1">res_test = _test_poisson_dispersion_generic(</span>
        <span class="s1">results</span><span class="s3">, </span><span class="s1">exog_new_test</span><span class="s3">, </span><span class="s1">exog_new_control= exog_new_control</span><span class="s3">,</span>
        <span class="s1">include_score=include_score</span><span class="s3">, </span><span class="s1">use_endog=use_endog</span><span class="s3">,</span>
        <span class="s1">cov_type=cov_type</span><span class="s3">, </span><span class="s1">cov_kwds=cov_kwds</span><span class="s3">, </span><span class="s1">use_t=use_t</span><span class="s3">,</span>
        <span class="s1">)</span>
    <span class="s3">return </span><span class="s1">res_test</span>


<span class="s3">class </span><span class="s1">ResultsGeneric:</span>


    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">**kwds):</span>
        <span class="s1">self.__dict__.update(kwds)</span>


<span class="s3">class </span><span class="s1">TestResults(ResultsGeneric):</span>

    <span class="s3">def </span><span class="s1">summary(self):</span>
        <span class="s1">txt = </span><span class="s4">'Specification Test (LM, score)</span><span class="s3">\n</span><span class="s4">'</span>
        <span class="s1">stat = [self.c1</span><span class="s3">, </span><span class="s1">self.c2</span><span class="s3">, </span><span class="s1">self.c3]</span>
        <span class="s1">pval = [self.pval1</span><span class="s3">, </span><span class="s1">self.pval2</span><span class="s3">, </span><span class="s1">self.pval3]</span>
        <span class="s1">description = [</span><span class="s4">'nonrobust'</span><span class="s3">, </span><span class="s4">'dispersed'</span><span class="s3">, </span><span class="s4">'HC'</span><span class="s1">]</span>

        <span class="s3">for </span><span class="s1">row </span><span class="s3">in </span><span class="s1">zip(description</span><span class="s3">, </span><span class="s1">stat</span><span class="s3">, </span><span class="s1">pval):</span>
            <span class="s1">txt += </span><span class="s4">'%-12s  statistic = %6.4f  pvalue = %6.5f</span><span class="s3">\n</span><span class="s4">' </span><span class="s1">% row</span>

        <span class="s1">txt += </span><span class="s4">'</span><span class="s3">\n</span><span class="s4">Assumptions:</span><span class="s3">\n</span><span class="s4">'</span>
        <span class="s1">txt += </span><span class="s4">'nonrobust: variance is correctly specified</span><span class="s3">\n</span><span class="s4">'</span>
        <span class="s1">txt += </span><span class="s4">'dispersed: variance correctly specified up to scale factor</span><span class="s3">\n</span><span class="s4">'</span>
        <span class="s1">txt += </span><span class="s4">'HC       : robust to any heteroscedasticity</span><span class="s3">\n</span><span class="s4">'</span>
        <span class="s1">txt += </span><span class="s4">'test is not robust to correlation across observations'</span>

        <span class="s3">return </span><span class="s1">txt</span>


<span class="s3">def </span><span class="s1">lm_test_glm(result</span><span class="s3">, </span><span class="s1">exog_extra</span><span class="s3">, </span><span class="s1">mean_deriv=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s2">'''score/lagrange multiplier test for GLM 
 
    Wooldridge procedure for test of mean function in GLM 
 
    Parameters 
    ---------- 
    results : GLMResults instance 
        results instance with the constrained model 
    exog_extra : ndarray or None 
        additional exogenous variables for variable addition test 
        This can be set to None if mean_deriv is provided. 
    mean_deriv : None or ndarray 
        Extra moment condition that correspond to the partial derivative of 
        a mean function with respect to some parameters. 
 
    Returns 
    ------- 
    test_results : Results instance 
        The results instance has the following attributes which are score 
        statistic and p-value for 3 versions of the score test. 
 
        c1, pval1 : nonrobust score_test results 
        c2, pval2 : score test results robust to over or under dispersion 
        c3, pval3 : score test results fully robust to any heteroscedasticity 
 
        The test results instance also has a simple summary method. 
 
    Notes 
    ----- 
    TODO: add `df` to results and make df detection more robust 
 
    This implements the auxiliary regression procedure of Wooldridge, 
    implemented based on the presentation in chapter 8 in Handbook of 
    Applied Econometrics 2. 
 
    References 
    ---------- 
    Wooldridge, Jeffrey M. 1997. “Quasi-Likelihood Methods for Count Data.” 
    Handbook of Applied Econometrics 2: 352–406. 
 
    and other articles and text book by Wooldridge 
 
    '''</span>

    <span class="s3">if </span><span class="s1">hasattr(result</span><span class="s3">, </span><span class="s4">'_result'</span><span class="s1">):</span>
        <span class="s1">res = result._result</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">res = result</span>

    <span class="s1">mod = result.model</span>
    <span class="s1">nobs = mod.endog.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s0">#mean_func = mod.family.link.inverse</span>
    <span class="s1">dlinkinv = mod.family.link.inverse_deriv</span>

    <span class="s0"># derivative of mean function w.r.t. beta (linear params)</span>
    <span class="s1">dm = </span><span class="s3">lambda </span><span class="s1">x</span><span class="s3">, </span><span class="s1">linpred: dlinkinv(linpred)[:</span><span class="s3">,None</span><span class="s1">] * x</span>

    <span class="s1">var_func = mod.family.variance</span>

    <span class="s1">x = result.model.exog</span>
    <span class="s1">x2 = exog_extra</span>

    <span class="s0"># test omitted</span>
    <span class="s3">try</span><span class="s1">:</span>
        <span class="s1">lin_pred = res.predict(which=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span>
    <span class="s3">except </span><span class="s1">TypeError:</span>
        <span class="s0"># TODO: Standardized to which=&quot;linear&quot; and remove linear kwarg</span>
        <span class="s1">lin_pred = res.predict(linear=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">dm_incl = dm(x</span><span class="s3">, </span><span class="s1">lin_pred)</span>
    <span class="s3">if </span><span class="s1">x2 </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">dm_excl = dm(x2</span><span class="s3">, </span><span class="s1">lin_pred)</span>
        <span class="s3">if </span><span class="s1">mean_deriv </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s0"># allow both and stack</span>
            <span class="s1">dm_excl = np.column_stack((dm_excl</span><span class="s3">, </span><span class="s1">mean_deriv))</span>
    <span class="s3">elif </span><span class="s1">mean_deriv </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">dm_excl = mean_deriv</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'either exog_extra or mean_deriv have to be provided'</span><span class="s1">)</span>

    <span class="s0"># TODO check for rank or redundant, note OLS calculates the rank</span>
    <span class="s1">k_constraint = dm_excl.shape[</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">fittedvalues = res.predict()  </span><span class="s0"># discrete has linpred instead of mean</span>
    <span class="s1">v = var_func(fittedvalues)</span>
    <span class="s1">std = np.sqrt(v)</span>
    <span class="s1">res_ols1 = OLS(res.resid_response / std</span><span class="s3">, </span><span class="s1">np.column_stack((dm_incl</span><span class="s3">, </span><span class="s1">dm_excl)) / std[:</span><span class="s3">, None</span><span class="s1">]).fit()</span>

    <span class="s0"># case: nonrobust assumes variance implied by distribution is correct</span>
    <span class="s1">c1 = res_ols1.ess</span>
    <span class="s1">pval1 = stats.chi2.sf(c1</span><span class="s3">, </span><span class="s1">k_constraint)</span>
    <span class="s0">#print c1, stats.chi2.sf(c1, 2)</span>

    <span class="s0"># case: robust to dispersion</span>
    <span class="s1">c2 = nobs * res_ols1.rsquared</span>
    <span class="s1">pval2 = stats.chi2.sf(c2</span><span class="s3">, </span><span class="s1">k_constraint)</span>
    <span class="s0">#print c2, stats.chi2.sf(c2, 2)</span>

    <span class="s0"># case: robust to heteroscedasticity</span>
    <span class="s3">from </span><span class="s1">statsmodels.stats.multivariate_tools </span><span class="s3">import </span><span class="s1">partial_project</span>
    <span class="s1">pp = partial_project(dm_excl / std[:</span><span class="s3">,None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dm_incl / std[:</span><span class="s3">,None</span><span class="s1">])</span>
    <span class="s1">resid_p = res.resid_response / std</span>
    <span class="s1">res_ols3 = OLS(np.ones(nobs)</span><span class="s3">, </span><span class="s1">pp.resid * resid_p[:</span><span class="s3">,None</span><span class="s1">]).fit()</span>
    <span class="s0">#c3 = nobs * res_ols3.rsquared   # this is Wooldridge</span>
    <span class="s1">c3b = res_ols3.ess  </span><span class="s0"># simpler if endog is ones</span>
    <span class="s1">pval3 = stats.chi2.sf(c3b</span><span class="s3">, </span><span class="s1">k_constraint)</span>

    <span class="s1">tres = TestResults(c1=c1</span><span class="s3">, </span><span class="s1">pval1=pval1</span><span class="s3">,</span>
                       <span class="s1">c2=c2</span><span class="s3">, </span><span class="s1">pval2=pval2</span><span class="s3">,</span>
                       <span class="s1">c3=c3b</span><span class="s3">, </span><span class="s1">pval3=pval3)</span>

    <span class="s3">return </span><span class="s1">tres</span>


<span class="s3">def </span><span class="s1">cm_test_robust(resid</span><span class="s3">, </span><span class="s1">resid_deriv</span><span class="s3">, </span><span class="s1">instruments</span><span class="s3">, </span><span class="s1">weights=</span><span class="s5">1</span><span class="s1">):</span>
    <span class="s2">'''score/lagrange multiplier of Wooldridge 
 
    generic version of Wooldridge procedure for test of conditional moments 
 
    Limitation: This version allows only for one unconditional moment 
    restriction, i.e. resid is scalar for each observation. 
    Another limitation is that it assumes independent observations, no 
    correlation in residuals and weights cannot be replaced by cross-observation 
    whitening. 
 
    Parameters 
    ---------- 
    resid : ndarray, (nobs, ) 
        conditional moment restriction, E(r | x, params) = 0 
    resid_deriv : ndarray, (nobs, k_params) 
        derivative of conditional moment restriction with respect to parameters 
    instruments : ndarray, (nobs, k_instruments) 
        indicator variables of Wooldridge, multiplies the conditional momen 
        restriction 
    weights : ndarray 
        This is a weights function as used in WLS. The moment 
        restrictions are multiplied by weights. This corresponds to the 
        inverse of the variance in a heteroskedastic model. 
 
    Returns 
    ------- 
    test_results : Results instance 
        ???  TODO 
 
    Notes 
    ----- 
    This implements the auxiliary regression procedure of Wooldridge, 
    implemented based on procedure 2.1 in Wooldridge 1990. 
 
    Wooldridge allows for multivariate conditional moments (`resid`) 
    TODO: check dimensions for multivariate case for extension 
 
    References 
    ---------- 
    Wooldridge 
    Wooldridge 
    and more Wooldridge 
 
    '''</span>
    <span class="s0"># notation: Wooldridge uses too mamny Greek letters</span>
    <span class="s0"># instruments is capital lambda</span>
    <span class="s0"># resid is small phi</span>
    <span class="s0"># resid_deriv is capital phi</span>
    <span class="s0"># weights is C</span>


    <span class="s1">nobs = resid.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s3">from </span><span class="s1">statsmodels.stats.multivariate_tools </span><span class="s3">import </span><span class="s1">partial_project</span>

    <span class="s1">w_sqrt = np.sqrt(weights)</span>
    <span class="s3">if </span><span class="s1">np.size(weights) &gt; </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">w_sqrt = w_sqrt[:</span><span class="s3">,None</span><span class="s1">]</span>
    <span class="s1">pp = partial_project(instruments * w_sqrt</span><span class="s3">, </span><span class="s1">resid_deriv * w_sqrt)</span>
    <span class="s1">mom_resid = pp.resid</span>

    <span class="s1">moms_test = mom_resid * resid[:</span><span class="s3">, None</span><span class="s1">] * w_sqrt</span>

    <span class="s0"># we get this here in case we extend resid to be more than 1-D</span>
    <span class="s1">k_constraint = moms_test.shape[</span><span class="s5">1</span><span class="s1">]</span>

    <span class="s0"># use OPG variance as in Wooldridge 1990. This might generalize</span>
    <span class="s1">cov = moms_test.T.dot(moms_test)</span>
    <span class="s1">diff = moms_test.sum(</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s0"># see Wooldridge last page in appendix</span>
    <span class="s1">stat = diff.dot(np.linalg.solve(cov</span><span class="s3">, </span><span class="s1">diff))</span>

    <span class="s0"># for checking, this corresponds to nobs * rsquared of auxiliary regression</span>
    <span class="s1">stat2 = OLS(np.ones(nobs)</span><span class="s3">, </span><span class="s1">moms_test).fit().ess</span>
    <span class="s1">pval = stats.chi2.sf(stat</span><span class="s3">, </span><span class="s1">k_constraint)</span>

    <span class="s3">return </span><span class="s1">stat</span><span class="s3">, </span><span class="s1">pval</span><span class="s3">, </span><span class="s1">stat2</span>


<span class="s3">def </span><span class="s1">lm_robust(score</span><span class="s3">, </span><span class="s1">constraint_matrix</span><span class="s3">, </span><span class="s1">score_deriv_inv</span><span class="s3">, </span><span class="s1">cov_score</span><span class="s3">,</span>
              <span class="s1">cov_params=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s2">'''general formula for score/LM test 
 
    generalized score or lagrange multiplier test for implicit constraints 
 
    `r(params) = 0`, with gradient `R = d r / d params` 
 
    linear constraints are given by `R params - q = 0` 
 
    It is assumed that all arrays are evaluated at the constrained estimates. 
 
    Parameters 
    ---------- 
    score : ndarray, 1-D 
        derivative of objective function at estimated parameters 
        of constrained model 
    constraint_matrix R : ndarray 
        Linear restriction matrix or Jacobian of nonlinear constraints 
    hessian_inv, Ainv : ndarray, symmetric, square 
        inverse of second derivative of objective function 
        TODO: could be OPG or any other estimator if information matrix 
        equality holds 
    cov_score B :  ndarray, symmetric, square 
        covariance matrix of the score. This is the inner part of a sandwich 
        estimator. 
    cov_params V :  ndarray, symmetric, square 
        covariance of full parameter vector evaluated at constrained parameter 
        estimate. This can be specified instead of cov_score B. 
 
    Returns 
    ------- 
    lm_stat : float 
        score/lagrange multiplier statistic 
 
    Notes 
    ----- 
 
    '''</span>
    <span class="s0"># shorthand alias</span>
    <span class="s1">R</span><span class="s3">, </span><span class="s1">Ainv</span><span class="s3">, </span><span class="s1">B</span><span class="s3">, </span><span class="s1">V = constraint_matrix</span><span class="s3">, </span><span class="s1">score_deriv_inv</span><span class="s3">, </span><span class="s1">cov_score</span><span class="s3">, </span><span class="s1">cov_params</span>

    <span class="s1">tmp = R.dot(Ainv)</span>
    <span class="s1">wscore = tmp.dot(score)  </span><span class="s0"># C Ainv score</span>

    <span class="s3">if </span><span class="s1">B </span><span class="s3">is None and </span><span class="s1">V </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s0"># only Ainv is given, so we assume information matrix identity holds</span>
        <span class="s0"># computational short cut, should be same if Ainv == inv(B)</span>
        <span class="s1">lm_stat = score.dot(Ainv.dot(score))</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s0"># information matrix identity does not hold</span>
        <span class="s3">if </span><span class="s1">V </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">inner = tmp.dot(B).dot(tmp.T)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">inner = R.dot(V).dot(R.T)</span>

        <span class="s0">#lm_stat2 = wscore.dot(np.linalg.pinv(inner).dot(wscore))</span>
        <span class="s0"># Let's assume inner is invertible, TODO: check if usecase for pinv exists</span>
        <span class="s1">lm_stat = wscore.dot(np.linalg.solve(inner</span><span class="s3">, </span><span class="s1">wscore))</span>

    <span class="s3">return </span><span class="s1">lm_stat</span><span class="s0">#, lm_stat2</span>


<span class="s3">def </span><span class="s1">lm_robust_subset(score</span><span class="s3">, </span><span class="s1">k_constraints</span><span class="s3">, </span><span class="s1">score_deriv_inv</span><span class="s3">, </span><span class="s1">cov_score):</span>
    <span class="s2">'''general formula for score/LM test 
 
    generalized score or lagrange multiplier test for constraints on a subset 
    of parameters 
 
    `params_1 = value`, where params_1 is a subset of the unconstrained 
    parameter vector. 
 
    It is assumed that all arrays are evaluated at the constrained estimates. 
 
    Parameters 
    ---------- 
    score : ndarray, 1-D 
        derivative of objective function at estimated parameters 
        of constrained model 
    k_constraint : int 
        number of constraints 
    score_deriv_inv : ndarray, symmetric, square 
        inverse of second derivative of objective function 
        TODO: could be OPG or any other estimator if information matrix 
        equality holds 
    cov_score B :  ndarray, symmetric, square 
        covariance matrix of the score. This is the inner part of a sandwich 
        estimator. 
    not cov_params V :  ndarray, symmetric, square 
        covariance of full parameter vector evaluated at constrained parameter 
        estimate. This can be specified instead of cov_score B. 
 
    Returns 
    ------- 
    lm_stat : float 
        score/lagrange multiplier statistic 
    p-value : float 
        p-value of the LM test based on chisquare distribution 
 
    Notes 
    ----- 
    The implementation is based on Boos 1992 section 4.1. The same derivation 
    is also in other articles and in text books. 
 
    '''</span>

    <span class="s0"># Notation in Boos</span>
    <span class="s0"># score `S = sum (s_i)</span>
    <span class="s0"># score_obs `s_i`</span>
    <span class="s0"># score_deriv `I` is derivative of score (hessian)</span>
    <span class="s0"># `D` is covariance matrix of score, OPG product given independent observations</span>

    <span class="s0">#k_params = len(score)</span>

    <span class="s0"># Note: I reverse order between constraint and unconstrained compared to Boos</span>

    <span class="s0"># submatrices of score_deriv/hessian</span>
    <span class="s0"># these are I22 and I12 in Boos</span>
    <span class="s0">#h_uu = score_deriv[-k_constraints:, -k_constraints:]</span>
    <span class="s1">h_uu = score_deriv_inv[:-k_constraints</span><span class="s3">, </span><span class="s1">:-k_constraints]</span>
    <span class="s1">h_cu = score_deriv_inv[-k_constraints:</span><span class="s3">, </span><span class="s1">:-k_constraints]</span>

    <span class="s0"># TODO: pinv or solve ?</span>
    <span class="s1">tmp_proj = h_cu.dot(np.linalg.inv(h_uu))</span>
    <span class="s1">tmp = np.column_stack((-tmp_proj</span><span class="s3">, </span><span class="s1">np.eye(k_constraints))) </span><span class="s0">#, tmp_proj))</span>

    <span class="s1">cov_score_constraints = tmp.dot(cov_score.dot(tmp.T))</span>

    <span class="s0">#lm_stat2 = wscore.dot(np.linalg.pinv(inner).dot(wscore))</span>
    <span class="s0"># Let's assume inner is invertible, TODO: check if usecase for pinv exists</span>
    <span class="s1">lm_stat = score.dot(np.linalg.solve(cov_score_constraints</span><span class="s3">, </span><span class="s1">score))</span>
    <span class="s1">pval = stats.chi2.sf(lm_stat</span><span class="s3">, </span><span class="s1">k_constraints)</span>

<span class="s0">#     # check second calculation Boos referencing Kent 1982 and Engle 1984</span>
<span class="s0">#     # we can use this when robust_cov_params of full model is available</span>
<span class="s0">#     #h_inv = np.linalg.inv(score_deriv)</span>
<span class="s0">#     hinv = score_deriv_inv</span>
<span class="s0">#     v = h_inv.dot(cov_score.dot(h_inv)) # this is robust cov_params</span>
<span class="s0">#     v_cc = v[:k_constraints, :k_constraints]</span>
<span class="s0">#     h_cc = score_deriv[:k_constraints, :k_constraints]</span>
<span class="s0">#     # brute force calculation:</span>
<span class="s0">#     h_resid_cu = h_cc - h_cu.dot(np.linalg.solve(h_uu, h_cu))</span>
<span class="s0">#     cov_s_c = h_resid_cu.dot(v_cc.dot(h_resid_cu))</span>
<span class="s0">#     diff = np.max(np.abs(cov_s_c - cov_score_constraints))</span>
    <span class="s3">return </span><span class="s1">lm_stat</span><span class="s3">, </span><span class="s1">pval  </span><span class="s0">#, lm_stat2</span>


<span class="s3">def </span><span class="s1">lm_robust_subset_parts(score</span><span class="s3">, </span><span class="s1">k_constraints</span><span class="s3">,</span>
                           <span class="s1">score_deriv_uu</span><span class="s3">, </span><span class="s1">score_deriv_cu</span><span class="s3">,</span>
                           <span class="s1">cov_score_cc</span><span class="s3">, </span><span class="s1">cov_score_cu</span><span class="s3">, </span><span class="s1">cov_score_uu):</span>
    <span class="s2">&quot;&quot;&quot;robust generalized score tests on subset of parameters 
 
    This is the same as lm_robust_subset with arguments in parts of 
    partitioned matrices. 
    This can be useful, when we have the parts based on different estimation 
    procedures, i.e. when we do not have the full unconstrained model. 
 
    Calculates mainly the covariance of the constraint part of the score. 
 
    Parameters 
    ---------- 
    score : ndarray, 1-D 
        derivative of objective function at estimated parameters 
        of constrained model. These is the score component for the restricted 
        part under hypothesis. The unconstrained part of the score is assumed 
        to be zero. 
    k_constraint : int 
        number of constraints 
    score_deriv_uu : ndarray, symmetric, square 
        first derivative of moment equation or second derivative of objective 
        function for the unconstrained part 
        TODO: could be OPG or any other estimator if information matrix 
        equality holds 
    score_deriv_cu : ndarray 
        first cross derivative of moment equation or second cross 
        derivative of objective function between. 
    cov_score_cc :  ndarray 
        covariance matrix of the score for the unconstrained part. 
        This is the inner part of a sandwich estimator. 
    cov_score_cu :  ndarray 
        covariance matrix of the score for the off-diagonal block, i.e. 
        covariance between constrained and unconstrained part. 
    cov_score_uu :  ndarray 
        covariance matrix of the score for the unconstrained part. 
 
    Returns 
    ------- 
    lm_stat : float 
        score/lagrange multiplier statistic 
    p-value : float 
        p-value of the LM test based on chisquare distribution 
 
    Notes 
    ----- 
    TODO: these function should just return the covariance of the score 
    instead of calculating the score/lm test. 
 
    Implementation similar to lm_robust_subset and is based on Boos 1992, 
    section 4.1 in the form attributed to Breslow (1990). It does not use the 
    computation attributed to Kent (1982) and Engle (1984). 
    &quot;&quot;&quot;</span>

    <span class="s1">tmp_proj = np.linalg.solve(score_deriv_uu</span><span class="s3">, </span><span class="s1">score_deriv_cu.T).T</span>
    <span class="s1">tmp = tmp_proj.dot(cov_score_cu.T)</span>

    <span class="s0"># this needs to make a copy of cov_score_cc for further inplace modification</span>
    <span class="s1">cov = cov_score_cc - tmp</span>
    <span class="s1">cov -= tmp.T</span>
    <span class="s1">cov += tmp_proj.dot(cov_score_uu).dot(tmp_proj.T)</span>

    <span class="s1">lm_stat = score.dot(np.linalg.solve(cov</span><span class="s3">, </span><span class="s1">score))</span>
    <span class="s1">pval = stats.chi2.sf(lm_stat</span><span class="s3">, </span><span class="s1">k_constraints)</span>
    <span class="s3">return </span><span class="s1">lm_stat</span><span class="s3">, </span><span class="s1">pval</span>


<span class="s3">def </span><span class="s1">lm_robust_reparameterized(score</span><span class="s3">, </span><span class="s1">params_deriv</span><span class="s3">, </span><span class="s1">score_deriv</span><span class="s3">, </span><span class="s1">cov_score):</span>
    <span class="s2">&quot;&quot;&quot;robust generalized score test for transformed parameters 
 
    The parameters are given by a nonlinear transformation of the estimated 
    reduced parameters 
 
    `params = g(params_reduced)`  with jacobian `G = d g / d params_reduced` 
 
    score and other arrays are for full parameter space `params` 
 
    Parameters 
    ---------- 
    score : ndarray, 1-D 
        derivative of objective function at estimated parameters 
        of constrained model 
    params_deriv : ndarray 
        Jacobian G of the parameter trasnformation 
    score_deriv : ndarray, symmetric, square 
        second derivative of objective function 
        TODO: could be OPG or any other estimator if information matrix 
        equality holds 
    cov_score B :  ndarray, symmetric, square 
        covariance matrix of the score. This is the inner part of a sandwich 
        estimator. 
 
    Returns 
    ------- 
    lm_stat : float 
        score/lagrange multiplier statistic 
    p-value : float 
        p-value of the LM test based on chisquare distribution 
 
    Notes 
    ----- 
    Boos 1992, section 4.3, expression for T_{GS} just before example 6 
    &quot;&quot;&quot;</span>
    <span class="s0"># Boos notation</span>
    <span class="s0"># params_deriv G</span>

    <span class="s1">k_params</span><span class="s3">, </span><span class="s1">k_reduced = params_deriv.shape</span>
    <span class="s1">k_constraints = k_params - k_reduced</span>

    <span class="s1">G = params_deriv  </span><span class="s0"># shortcut alias</span>

    <span class="s1">tmp_c0 = np.linalg.pinv(G.T.dot(score_deriv.dot(G)))</span>
    <span class="s1">tmp_c1 = score_deriv.dot(G.dot(tmp_c0.dot(G.T)))</span>
    <span class="s1">tmp_c = np.eye(k_params) - tmp_c1</span>

    <span class="s1">cov = tmp_c.dot(cov_score.dot(tmp_c.T))  </span><span class="s0"># warning: reduced rank</span>

    <span class="s1">lm_stat = score.dot(np.linalg.pinv(cov).dot(score))</span>
    <span class="s1">pval = stats.chi2.sf(lm_stat</span><span class="s3">, </span><span class="s1">k_constraints)</span>
    <span class="s3">return </span><span class="s1">lm_stat</span><span class="s3">, </span><span class="s1">pval</span>


<span class="s3">def </span><span class="s1">conditional_moment_test_generic(mom_test</span><span class="s3">, </span><span class="s1">mom_test_deriv</span><span class="s3">,</span>
                                    <span class="s1">mom_incl</span><span class="s3">, </span><span class="s1">mom_incl_deriv</span><span class="s3">,</span>
                                    <span class="s1">var_mom_all=</span><span class="s3">None,</span>
                                    <span class="s1">cov_type=</span><span class="s4">'OPG'</span><span class="s3">, </span><span class="s1">cov_kwds=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;generic conditional moment test 
 
    This is mainly intended as internal function in support of diagnostic 
    and specification tests. It has no conversion and checking of correct 
    arguments. 
 
    Parameters 
    ---------- 
    mom_test : ndarray, 2-D (nobs, k_constraints) 
        moment conditions that will be tested to be zero 
    mom_test_deriv : ndarray, 2-D, square (k_constraints, k_constraints) 
        derivative of moment conditions under test with respect to the 
        parameters of the model summed over observations. 
    mom_incl : ndarray, 2-D (nobs, k_params) 
        moment conditions that where use in estimation, assumed to be zero 
        This is score_obs in the case of (Q)MLE 
    mom_incl_deriv : ndarray, 2-D, square (k_params, k_params) 
        derivative of moment conditions of estimator summed over observations 
        This is the information matrix or Hessian in the case of (Q)MLE. 
    var_mom_all : None, or ndarray, 2-D, (k, k) with k = k_constraints + k_params 
        Expected product or variance of the joint (column_stacked) moment 
        conditions. The stacking should have the variance of the moment 
        conditions under test in the first k_constraint rows and columns. 
        If it is not None, then it will be estimated based on cov_type. 
        I think: This is the Hessian of the extended or alternative model 
        under full MLE and score test assuming information matrix identity 
        holds. 
 
    Returns 
    ------- 
    results 
 
    Notes 
    ----- 
    TODO: cov_type other than OPG is missing 
    initial implementation based on Cameron Trived countbook 1998 p.48, p.56 
 
    also included: mom_incl can be None if expected mom_test_deriv is zero. 
 
    References 
    ---------- 
    Cameron and Trivedi 1998 count book 
    Wooldridge ??? 
    Pagan and Vella 1989 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">cov_type != </span><span class="s4">'OPG'</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s1">k_constraints = mom_test.shape[</span><span class="s5">1</span><span class="s1">]</span>

    <span class="s3">if </span><span class="s1">mom_incl </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s0"># assume mom_test_deriv is zero, do not include effect of mom_incl</span>
        <span class="s3">if </span><span class="s1">var_mom_all </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">var_cm = mom_test.T.dot(mom_test)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">var_cm = var_mom_all</span>

    <span class="s3">else</span><span class="s1">:</span>
        <span class="s0"># take into account he effect of parameter estimates on mom_test</span>
        <span class="s3">if </span><span class="s1">var_mom_all </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">mom_all = np.column_stack((mom_test</span><span class="s3">, </span><span class="s1">mom_incl))</span>
            <span class="s0"># TODO: replace with inner sandwich covariance estimator</span>
            <span class="s1">var_mom_all = mom_all.T.dot(mom_all)</span>

        <span class="s1">tmp = mom_test_deriv.dot(np.linalg.pinv(mom_incl_deriv))</span>
        <span class="s1">h = np.column_stack((np.eye(k_constraints)</span><span class="s3">, </span><span class="s1">-tmp))</span>

        <span class="s1">var_cm = h.dot(var_mom_all.dot(h.T))</span>

    <span class="s0"># calculate test results with chisquare</span>
    <span class="s1">var_cm_inv = np.linalg.pinv(var_cm)</span>
    <span class="s1">mom_test_sum = mom_test.sum(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">statistic = mom_test_sum.dot(var_cm_inv.dot(mom_test_sum))</span>
    <span class="s1">pval = stats.chi2.sf(statistic</span><span class="s3">, </span><span class="s1">k_constraints)</span>

    <span class="s0"># normal test of individual components</span>
    <span class="s1">se = np.sqrt(np.diag(var_cm))</span>
    <span class="s1">tvalues = mom_test_sum / se</span>
    <span class="s1">pvalues = stats.norm.sf(np.abs(tvalues))</span>

    <span class="s1">res = ResultsGeneric(var_cm=var_cm</span><span class="s3">,</span>
                         <span class="s1">stat_cmt=statistic</span><span class="s3">,</span>
                         <span class="s1">pval_cmt=pval</span><span class="s3">,</span>
                         <span class="s1">tvalues=tvalues</span><span class="s3">,</span>
                         <span class="s1">pvalues=pvalues)</span>

    <span class="s3">return </span><span class="s1">res</span>


<span class="s3">def </span><span class="s1">conditional_moment_test_regression(mom_test</span><span class="s3">, </span><span class="s1">mom_test_deriv=</span><span class="s3">None,</span>
                                    <span class="s1">mom_incl=</span><span class="s3">None, </span><span class="s1">mom_incl_deriv=</span><span class="s3">None,</span>
                                    <span class="s1">var_mom_all=</span><span class="s3">None, </span><span class="s1">demean=</span><span class="s3">False,</span>
                                    <span class="s1">cov_type=</span><span class="s4">'OPG'</span><span class="s3">, </span><span class="s1">cov_kwds=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot;generic conditional moment test based artificial regression 
 
    this is very experimental, no options implemented yet 
 
    so far 
    OPG regression, or 
    artificial regression with Robust Wald test 
 
    The latter is (as far as I can see) the same as an overidentifying test 
    in GMM where the test statistic is the value of the GMM objective function 
    and it is assumed that parameters were estimated with optimial GMM, i.e. 
    the weight matrix equal to the expectation of the score variance. 
    &quot;&quot;&quot;</span>
    <span class="s0"># so far coded from memory</span>
    <span class="s1">nobs</span><span class="s3">, </span><span class="s1">k_constraints = mom_test.shape</span>

    <span class="s1">endog = np.ones(nobs)</span>
    <span class="s3">if </span><span class="s1">mom_incl </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">ex = np.column_stack((mom_test</span><span class="s3">, </span><span class="s1">mom_incl))</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">ex = mom_test</span>
    <span class="s3">if </span><span class="s1">demean:</span>
        <span class="s1">ex -= ex.mean(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">cov_type == </span><span class="s4">'OPG'</span><span class="s1">:</span>
        <span class="s1">res = OLS(endog</span><span class="s3">, </span><span class="s1">ex).fit()</span>

        <span class="s1">statistic = nobs * res.rsquared</span>
        <span class="s1">pval = stats.chi2.sf(statistic</span><span class="s3">, </span><span class="s1">k_constraints)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">res = OLS(endog</span><span class="s3">, </span><span class="s1">ex).fit(cov_type=cov_type</span><span class="s3">, </span><span class="s1">cov_kwds=cov_kwds)</span>
        <span class="s1">tres = res.wald_test(np.eye(ex.shape[</span><span class="s5">1</span><span class="s1">]))</span>
        <span class="s1">statistic = tres.statistic</span>
        <span class="s1">pval = tres.pvalue</span>

    <span class="s3">return </span><span class="s1">statistic</span><span class="s3">, </span><span class="s1">pval</span>


<span class="s3">class </span><span class="s1">CMTNewey:</span>
    <span class="s2">&quot;&quot;&quot;generic moment test for GMM 
 
    This is a class to calculate and hold the various results 
 
    This is based on Newey 1985 on GMM. 
    Lemma 1: 
    Theorem 1 
 
    The main method is `chisquare` which returns the result of the 
    conditional moment test. 
 
    Warning: name of class and methods will likely be changed 
 
    Parameters 
    ---------- 
    moments : ndarray, 1-D 
        moments that are tested to be zero. They do not need to be derived 
        from a likelihood function. 
    moments_deriv : ndarray 
        derivative of the moment function with respect to the parameters that 
        are estimated 
    cov_moments : ndarray 
        An estimate for the joint (expected) covariance of all moments. This 
        can be a heteroscedasticity or correlation robust covariance estimate, 
        i.e. the inner part of a sandwich covariance. 
    weights : ndarray 
        Weights used in the GMM estimation. 
    transf_mt : ndarray 
        This defines the test moments where `transf_mt` is the matrix that 
        defines a Linear combination of moments that have expected value equal 
        to zero under the Null hypothesis. 
 
    Notes 
    ----- 
    The one letter names in Newey 1985 are 
 
    moments, g : 
    cov_moments, V : 
    moments_deriv, H : 
    weights, W : 
    transf_mt, L : 
        linear transformation to get the test condition from the moments 
 
    not used, add as argument to methods or __init__? 
    K cov for misspecification 
    or mispecification_deriv 
 
    This follows the GMM version in Newey 1985a, not the MLE version in 
    Newey 1985b. Newey uses the generalized information matrix equality in the 
    MLE version Newey (1985b). 
 
    Newey 1985b Lemma 1 does not impose correctly specified likelihood, but 
    assumes it in the following. Lemma 1 in both articles are essentially the 
    same assuming D = H' W. 
 
    References 
    ---------- 
    - Newey 1985a, Generalized Method of Moment specification testing, 
      Journal of Econometrics 
    - Newey 1985b, Maximum Likelihood Specification Testing and Conditional 
      Moment Tests, Econometrica 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">moments</span><span class="s3">, </span><span class="s1">cov_moments</span><span class="s3">, </span><span class="s1">moments_deriv</span><span class="s3">,</span>
                 <span class="s1">weights</span><span class="s3">, </span><span class="s1">transf_mt):</span>

        <span class="s1">self.moments = moments</span>
        <span class="s1">self.cov_moments = cov_moments</span>
        <span class="s1">self.moments_deriv = moments_deriv</span>
        <span class="s1">self.weights = weights</span>
        <span class="s1">self.transf_mt = transf_mt</span>

        <span class="s0"># derived quantities</span>
        <span class="s1">self.moments_constraint = transf_mt.dot(moments)</span>
        <span class="s1">self.htw = moments_deriv.T.dot(weights)   </span><span class="s0"># H'W</span>

        <span class="s0"># TODO check these</span>
        <span class="s1">self.k_moments = self.moments.shape[-</span><span class="s5">1</span><span class="s1">]  </span><span class="s0"># in this case only 1-D</span>
        <span class="s0"># assuming full rank of L'</span>
        <span class="s1">self.k_constraints = self.transf_mt.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">asy_transf_params(self):</span>

        <span class="s1">moments_deriv = self.moments_deriv  </span><span class="s0"># H</span>
        <span class="s0">#weights = self.weights  # W</span>

        <span class="s1">htw = self.htw  </span><span class="s0"># moments_deriv.T.dot(weights)   # H'W</span>
        <span class="s1">res = np.linalg.solve(htw.dot(moments_deriv)</span><span class="s3">, </span><span class="s1">htw)</span>
        <span class="s0">#res = np.linalg.pinv(htw.dot(moments_deriv)).dot(htw)</span>
        <span class="s3">return </span><span class="s1">-res</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">project_w(self):</span>
        <span class="s0"># P_w = I - H (H' W H)^{-1} H' W</span>
        <span class="s1">moments_deriv = self.moments_deriv  </span><span class="s0"># H</span>

        <span class="s1">res = moments_deriv.dot(self.asy_transf_params)</span>
        <span class="s1">res += np.eye(res.shape[</span><span class="s5">0</span><span class="s1">])</span>
        <span class="s3">return </span><span class="s1">res</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">asy_transform_mom_constraints(self):</span>
        <span class="s0"># L P_w</span>
        <span class="s1">res = self.transf_mt.dot(self.project_w)</span>
        <span class="s3">return </span><span class="s1">res</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">asy_cov_moments(self):</span>
        <span class="s2">&quot;&quot;&quot; 
 
        `sqrt(T) * g_T(b_0) asy N(K delta, V)` 
 
        mean is not implemented, 
        V is the same as cov_moments in __init__ argument 
        &quot;&quot;&quot;</span>

        <span class="s3">return </span><span class="s1">self.cov_moments</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">cov_mom_constraints(self):</span>

        <span class="s0"># linear transformation</span>
        <span class="s1">transf = self.asy_transform_mom_constraints</span>

        <span class="s3">return </span><span class="s1">transf.dot(self.asy_cov_moments).dot(transf.T)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">rank_cov_mom_constraints(self):</span>
        <span class="s3">return </span><span class="s1">np.linalg.matrix_rank(self.cov_mom_constraints)</span>

    <span class="s3">def </span><span class="s1">ztest(self):</span>
        <span class="s2">&quot;&quot;&quot;statistic, p-value and degrees of freedom of separate moment test 
 
        currently two sided test only 
 
        TODO: This can use generic ztest/ttest features and return 
        ContrastResults 
        &quot;&quot;&quot;</span>
        <span class="s1">diff = self.moments_constraint</span>
        <span class="s1">bse = np.sqrt(np.diag(self.cov_mom_constraints))</span>

        <span class="s0"># Newey uses a generalized inverse</span>
        <span class="s1">stat = diff / bse</span>
        <span class="s1">pval = stats.norm.sf(np.abs(stat))*</span><span class="s5">2</span>
        <span class="s3">return </span><span class="s1">stat</span><span class="s3">, </span><span class="s1">pval</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">chisquare(self):</span>
        <span class="s2">&quot;&quot;&quot;statistic, p-value and degrees of freedom of joint moment test 
        &quot;&quot;&quot;</span>
        <span class="s1">diff = self.moments_constraint</span>
        <span class="s1">cov = self.cov_mom_constraints</span>

        <span class="s0"># Newey uses a generalized inverse</span>
        <span class="s1">stat = diff.T.dot(np.linalg.pinv(cov).dot(diff))</span>
        <span class="s1">df = self.rank_cov_mom_constraints</span>
        <span class="s1">pval = stats.chi2.sf(stat</span><span class="s3">, </span><span class="s1">df)  </span><span class="s0"># Theorem 1</span>
        <span class="s3">return </span><span class="s1">stat</span><span class="s3">, </span><span class="s1">pval</span><span class="s3">, </span><span class="s1">df</span>


<span class="s3">class </span><span class="s1">CMTTauchen:</span>
    <span class="s2">&quot;&quot;&quot;generic moment tests or conditional moment tests for Quasi-MLE 
 
    This is a generic class based on Tauchen 1985 
 
    The main method is `chisquare` which returns the result of the 
    conditional moment test. 
 
    Warning: name of class and of methods will likely be changed 
 
    Parameters 
    ---------- 
    score : ndarray, 1-D 
        moment condition used in estimation, score of log-likelihood function 
    score_deriv : ndarray 
        derivative of score function with respect to the parameters that are 
        estimated. This is the Hessian in quasi-maximum likelihood 
    moments : ndarray, 1-D 
        moments that are tested to be zero. They do not need to be derived 
        from a likelihood function. 
    moments_deriv : ndarray 
        derivative of the moment function with respect to the parameters that 
        are estimated 
    cov_moments : ndarray 
        An estimate for the joint (expected) covariance of score and test 
        moments. This can be a heteroscedasticity or correlation robust 
        covariance estimate, i.e. the inner part of a sandwich covariance. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">score</span><span class="s3">, </span><span class="s1">score_deriv</span><span class="s3">, </span><span class="s1">moments</span><span class="s3">, </span><span class="s1">moments_deriv</span><span class="s3">, </span><span class="s1">cov_moments):</span>
        <span class="s1">self.score = score</span>
        <span class="s1">self.score_deriv = score_deriv</span>
        <span class="s1">self.moments = moments</span>
        <span class="s1">self.moments_deriv = moments_deriv</span>
        <span class="s1">self.cov_moments_all = cov_moments</span>

        <span class="s1">self.k_moments_test = moments.shape[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">self.k_params = score.shape[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">self.k_moments_all = self.k_params + self.k_moments_test</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">cov_params_all(self):</span>
        <span class="s1">m_deriv = np.zeros((self.k_moments_all</span><span class="s3">, </span><span class="s1">self.k_moments_all))</span>
        <span class="s1">m_deriv[:self.k_params</span><span class="s3">, </span><span class="s1">:self.k_params] = self.score_deriv</span>
        <span class="s1">m_deriv[self.k_params:</span><span class="s3">, </span><span class="s1">:self.k_params] = self.moments_deriv</span>
        <span class="s1">m_deriv[self.k_params:</span><span class="s3">, </span><span class="s1">self.k_params:] = np.eye(self.k_moments_test)</span>

        <span class="s1">m_deriv_inv = np.linalg.inv(m_deriv)</span>
        <span class="s1">cov = m_deriv_inv.dot(self.cov_moments_all.dot(m_deriv_inv.T)) </span><span class="s0"># K_inv J K_inv</span>
        <span class="s3">return </span><span class="s1">cov</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">cov_mom_constraints(self):</span>
        <span class="s3">return </span><span class="s1">self.cov_params_all[self.k_params:</span><span class="s3">, </span><span class="s1">self.k_params:]</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">rank_cov_mom_constraints(self):</span>
        <span class="s3">return </span><span class="s1">np.linalg.matrix_rank(self.cov_mom_constraints)</span>

    <span class="s0"># TODO: not DRY, just copied from CMTNewey</span>
    <span class="s3">def </span><span class="s1">ztest(self):</span>
        <span class="s2">&quot;&quot;&quot;statistic, p-value and degrees of freedom of separate moment test 
 
        currently two sided test only 
 
        TODO: This can use generic ztest/ttest features and return 
        ContrastResults 
        &quot;&quot;&quot;</span>
        <span class="s1">diff = self.moments_constraint</span>
        <span class="s1">bse = np.sqrt(np.diag(self.cov_mom_constraints))</span>

        <span class="s0"># Newey uses a generalized inverse</span>
        <span class="s1">stat = diff / bse</span>
        <span class="s1">pval = stats.norm.sf(np.abs(stat))*</span><span class="s5">2</span>
        <span class="s3">return </span><span class="s1">stat</span><span class="s3">, </span><span class="s1">pval</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">chisquare(self):</span>
        <span class="s2">&quot;&quot;&quot;statistic, p-value and degrees of freedom of joint moment test 
        &quot;&quot;&quot;</span>
        <span class="s1">diff = self.moments </span><span class="s0">#_constraints</span>
        <span class="s1">cov = self.cov_mom_constraints</span>

        <span class="s0"># Newey uses a generalized inverse, we use it also here</span>
        <span class="s1">stat = diff.T.dot(np.linalg.pinv(cov).dot(diff))</span>
        <span class="s0">#df = self.k_moments_test</span>
        <span class="s0"># We allow for redundant mom_constraints:</span>
        <span class="s1">df = self.rank_cov_mom_constraints</span>
        <span class="s1">pval = stats.chi2.sf(stat</span><span class="s3">, </span><span class="s1">df)</span>
        <span class="s3">return </span><span class="s1">stat</span><span class="s3">, </span><span class="s1">pval</span><span class="s3">, </span><span class="s1">df</span>
</pre>
</body>
</html>