<html>
<head>
<title>truncated_model.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
truncated_model.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">__future__ </span><span class="s0">import </span><span class="s1">division</span>

<span class="s1">__all__ = [</span><span class="s2">&quot;TruncatedLFPoisson&quot;</span><span class="s0">, </span><span class="s2">&quot;TruncatedLFNegativeBinomialP&quot;</span><span class="s0">,</span>
           <span class="s2">&quot;HurdleCountModel&quot;</span><span class="s1">]</span>

<span class="s0">import </span><span class="s1">warnings</span>
<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">statsmodels.base.model </span><span class="s0">as </span><span class="s1">base</span>
<span class="s0">import </span><span class="s1">statsmodels.base.wrapper </span><span class="s0">as </span><span class="s1">wrap</span>
<span class="s0">import </span><span class="s1">statsmodels.regression.linear_model </span><span class="s0">as </span><span class="s1">lm</span>
<span class="s0">from </span><span class="s1">statsmodels.distributions.discrete </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">truncatedpoisson</span><span class="s0">,</span>
    <span class="s1">truncatednegbin</span><span class="s0">,</span>
    <span class="s1">)</span>
<span class="s0">from </span><span class="s1">statsmodels.discrete.discrete_model </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">DiscreteModel</span><span class="s0">,</span>
    <span class="s1">CountModel</span><span class="s0">,</span>
    <span class="s1">CountResults</span><span class="s0">,</span>
    <span class="s1">L1CountResults</span><span class="s0">,</span>
    <span class="s1">Poisson</span><span class="s0">,</span>
    <span class="s1">NegativeBinomialP</span><span class="s0">,</span>
    <span class="s1">GeneralizedPoisson</span><span class="s0">,</span>
    <span class="s1">_discrete_results_docs</span><span class="s0">,</span>
    <span class="s1">)</span>
<span class="s0">from </span><span class="s1">statsmodels.tools.numdiff </span><span class="s0">import </span><span class="s1">approx_hess</span>
<span class="s0">from </span><span class="s1">statsmodels.tools.decorators </span><span class="s0">import </span><span class="s1">cache_readonly</span>
<span class="s0">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s0">import </span><span class="s1">ConvergenceWarning</span>
<span class="s0">from </span><span class="s1">copy </span><span class="s0">import </span><span class="s1">deepcopy</span>


<span class="s0">class </span><span class="s1">TruncatedLFGeneric(CountModel):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Generic Truncated model for count data 
 
    .. versionadded:: 0.14.0 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : array 
        A reference to the endogenous response variable 
    exog : array 
        A reference to the exogenous design. 
    truncation : int, optional 
        Truncation parameter specify truncation point out of the support 
        of the distribution. pmf(k) = 0 for k &lt;= truncation 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s0">,</span>
           <span class="s2">'extra_params'</span><span class="s1">:</span>
           <span class="s2">&quot;&quot;&quot;offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    exposure : array_like 
        Log(exposure) is added to the linear prediction with coefficient 
        equal to 1. 
 
    &quot;&quot;&quot; </span><span class="s1">+ base._missing_param_doc}</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">truncation=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">offset=</span><span class="s0">None,</span>
                 <span class="s1">exposure=</span><span class="s0">None, </span><span class="s1">missing=</span><span class="s2">'none'</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s1">super(TruncatedLFGeneric</span><span class="s0">, </span><span class="s1">self).__init__(</span>
            <span class="s1">endog</span><span class="s0">,</span>
            <span class="s1">exog</span><span class="s0">,</span>
            <span class="s1">offset=offset</span><span class="s0">,</span>
            <span class="s1">exposure=exposure</span><span class="s0">,</span>
            <span class="s1">missing=missing</span><span class="s0">,</span>
            <span class="s1">**kwargs</span>
            <span class="s1">)</span>
        <span class="s1">mask = self.endog &gt; truncation</span>
        <span class="s1">self.exog = self.exog[mask]</span>
        <span class="s1">self.endog = self.endog[mask]</span>
        <span class="s0">if </span><span class="s1">offset </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">self.offset = self.offset[mask]</span>
        <span class="s0">if </span><span class="s1">exposure </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">self.exposure = self.exposure[mask]</span>

        <span class="s1">self.trunc = truncation</span>
        <span class="s1">self.truncation = truncation  </span><span class="s4"># needed for recreating model</span>
        <span class="s4"># We cannot set the correct df_resid here, not enough information</span>
        <span class="s1">self._init_keys.extend([</span><span class="s2">'truncation'</span><span class="s1">])</span>
        <span class="s1">self._null_drop_keys = []</span>

    <span class="s0">def </span><span class="s1">loglike(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Loglikelihood of Generic Truncated model 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike : float 
            The log-likelihood function of the model evaluated at `params`. 
            See notes. 
 
        Notes 
        ----- 
 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">np.sum(self.loglikeobs(params))</span>

    <span class="s0">def </span><span class="s1">loglikeobs(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Loglikelihood for observations of Generic Truncated model 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike : ndarray (nobs,) 
            The log likelihood for each observation of the model evaluated 
            at `params`. See Notes 
 
        Notes 
        ----- 
 
        &quot;&quot;&quot;</span>
        <span class="s1">llf_main = self.model_main.loglikeobs(params)</span>

        <span class="s1">yt = self.trunc + </span><span class="s3">1</span>

        <span class="s4"># equivalent ways to compute truncation probability</span>
        <span class="s4"># pmf0 = np.zeros_like(self.endog, dtype=np.float64)</span>
        <span class="s4"># for i in range(self.trunc + 1):</span>
        <span class="s4">#     model = self.model_main.__class__(np.ones_like(self.endog) * i,</span>
        <span class="s4">#                                       self.exog)</span>
        <span class="s4">#     pmf0 += np.exp(model.loglikeobs(params))</span>
        <span class="s4">#</span>
        <span class="s4"># pmf1 = self.model_main.predict(</span>
        <span class="s4">#     params, which=&quot;prob&quot;, y_values=np.arange(yt)).sum(-1)</span>

        <span class="s1">pmf = self.predict(</span>
            <span class="s1">params</span><span class="s0">, </span><span class="s1">which=</span><span class="s2">&quot;prob-base&quot;</span><span class="s0">, </span><span class="s1">y_values=np.arange(yt)).sum(-</span><span class="s3">1</span><span class="s1">)</span>

        <span class="s1">llf = llf_main - np.log(</span><span class="s3">1 </span><span class="s1">- pmf)</span>
        <span class="s4"># assert np.allclose(pmf0, pmf)</span>
        <span class="s4"># assert np.allclose(pmf1, pmf)</span>

        <span class="s0">return </span><span class="s1">llf</span>

    <span class="s0">def </span><span class="s1">score_obs(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Generic Truncated model score (gradient) vector of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        score : ndarray, 1-D 
            The score vector of the model, i.e. the first derivative of the 
            loglikelihood function, evaluated at `params` 
        &quot;&quot;&quot;</span>
        <span class="s1">score_main = self.model_main.score_obs(params)</span>

        <span class="s1">pmf = np.zeros_like(self.endog</span><span class="s0">, </span><span class="s1">dtype=np.float64)</span>
        <span class="s4"># TODO: can we rewrite to following without creating new models</span>
        <span class="s1">score_trunc = np.zeros_like(score_main</span><span class="s0">, </span><span class="s1">dtype=np.float64)</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(self.trunc + </span><span class="s3">1</span><span class="s1">):</span>
            <span class="s1">model = self.model_main.__class__(</span>
                <span class="s1">np.ones_like(self.endog) * i</span><span class="s0">,</span>
                <span class="s1">self.exog</span><span class="s0">,</span>
                <span class="s1">offset=getattr(self</span><span class="s0">, </span><span class="s2">&quot;offset&quot;</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">exposure=getattr(self</span><span class="s0">, </span><span class="s2">&quot;exposure&quot;</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
                <span class="s1">)</span>
            <span class="s1">pmf_i = np.exp(model.loglikeobs(params))</span>
            <span class="s1">score_trunc += (model.score_obs(params).T * pmf_i).T</span>
            <span class="s1">pmf += pmf_i</span>

        <span class="s1">dparams = score_main + (score_trunc.T / (</span><span class="s3">1 </span><span class="s1">- pmf)).T</span>

        <span class="s0">return </span><span class="s1">dparams</span>

    <span class="s0">def </span><span class="s1">score(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Generic Truncated model score (gradient) vector of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        score : ndarray, 1-D 
            The score vector of the model, i.e. the first derivative of the 
            loglikelihood function, evaluated at `params` 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">self.score_obs(params).sum(</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">start_params=</span><span class="s0">None, </span><span class="s1">method=</span><span class="s2">'bfgs'</span><span class="s0">, </span><span class="s1">maxiter=</span><span class="s3">35</span><span class="s0">,</span>
            <span class="s1">full_output=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">disp=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">callback=</span><span class="s0">None,</span>
            <span class="s1">cov_type=</span><span class="s2">'nonrobust'</span><span class="s0">, </span><span class="s1">cov_kwds=</span><span class="s0">None, </span><span class="s1">use_t=</span><span class="s0">None, </span><span class="s1">**kwargs):</span>
        <span class="s0">if </span><span class="s1">start_params </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">offset = getattr(self</span><span class="s0">, </span><span class="s2">&quot;offset&quot;</span><span class="s0">, </span><span class="s3">0</span><span class="s1">) + getattr(self</span><span class="s0">, </span><span class="s2">&quot;exposure&quot;</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span>
            <span class="s0">if </span><span class="s1">np.size(offset) == </span><span class="s3">1 </span><span class="s0">and </span><span class="s1">offset == </span><span class="s3">0</span><span class="s1">:</span>
                <span class="s1">offset = </span><span class="s0">None</span>
            <span class="s1">model = self.model_main.__class__(self.endog</span><span class="s0">, </span><span class="s1">self.exog</span><span class="s0">,</span>
                                              <span class="s1">offset=offset)</span>
            <span class="s0">with </span><span class="s1">warnings.catch_warnings():</span>
                <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;ignore&quot;</span><span class="s0">, </span><span class="s1">category=ConvergenceWarning)</span>
                <span class="s1">start_params = model.fit(disp=</span><span class="s3">0</span><span class="s1">).params</span>

        <span class="s4"># Todo: check how we can to this in __init__</span>
        <span class="s1">k_params = self.df_model + </span><span class="s3">1 </span><span class="s1">+ self.k_extra</span>
        <span class="s1">self.df_resid = self.endog.shape[</span><span class="s3">0</span><span class="s1">] - k_params</span>

        <span class="s1">mlefit = super(TruncatedLFGeneric</span><span class="s0">, </span><span class="s1">self).fit(</span>
            <span class="s1">start_params=start_params</span><span class="s0">,</span>
            <span class="s1">method=method</span><span class="s0">,</span>
            <span class="s1">maxiter=maxiter</span><span class="s0">,</span>
            <span class="s1">disp=disp</span><span class="s0">,</span>
            <span class="s1">full_output=full_output</span><span class="s0">,</span>
            <span class="s1">callback=</span><span class="s0">lambda </span><span class="s1">x: x</span><span class="s0">,</span>
            <span class="s1">**kwargs</span>
            <span class="s1">)</span>

        <span class="s1">zipfit = self.result_class(self</span><span class="s0">, </span><span class="s1">mlefit._results)</span>
        <span class="s1">result = self.result_class_wrapper(zipfit)</span>

        <span class="s0">if </span><span class="s1">cov_kwds </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">cov_kwds = {}</span>

        <span class="s1">result._get_robustcov_results(cov_type=cov_type</span><span class="s0">,</span>
                                      <span class="s1">use_self=</span><span class="s0">True, </span><span class="s1">use_t=use_t</span><span class="s0">, </span><span class="s1">**cov_kwds)</span>
        <span class="s0">return </span><span class="s1">result</span>

    <span class="s1">fit.__doc__ = DiscreteModel.fit.__doc__</span>

    <span class="s0">def </span><span class="s1">fit_regularized(</span>
            <span class="s1">self</span><span class="s0">, </span><span class="s1">start_params=</span><span class="s0">None, </span><span class="s1">method=</span><span class="s2">'l1'</span><span class="s0">,</span>
            <span class="s1">maxiter=</span><span class="s2">'defined_by_method'</span><span class="s0">, </span><span class="s1">full_output=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">disp=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">callback=</span><span class="s0">None,</span>
            <span class="s1">alpha=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">trim_mode=</span><span class="s2">'auto'</span><span class="s0">, </span><span class="s1">auto_trim_tol=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">size_trim_tol=</span><span class="s3">1e-4</span><span class="s0">,</span>
            <span class="s1">qc_tol=</span><span class="s3">0.03</span><span class="s0">, </span><span class="s1">**kwargs):</span>

        <span class="s0">if </span><span class="s1">np.size(alpha) == </span><span class="s3">1 </span><span class="s0">and </span><span class="s1">alpha != </span><span class="s3">0</span><span class="s1">:</span>
            <span class="s1">k_params = self.exog.shape[</span><span class="s3">1</span><span class="s1">]</span>
            <span class="s1">alpha = alpha * np.ones(k_params)</span>

        <span class="s1">alpha_p = alpha</span>
        <span class="s0">if </span><span class="s1">start_params </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">offset = getattr(self</span><span class="s0">, </span><span class="s2">&quot;offset&quot;</span><span class="s0">, </span><span class="s3">0</span><span class="s1">) + getattr(self</span><span class="s0">, </span><span class="s2">&quot;exposure&quot;</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span>
            <span class="s0">if </span><span class="s1">np.size(offset) == </span><span class="s3">1 </span><span class="s0">and </span><span class="s1">offset == </span><span class="s3">0</span><span class="s1">:</span>
                <span class="s1">offset = </span><span class="s0">None</span>
            <span class="s1">model = self.model_main.__class__(self.endog</span><span class="s0">, </span><span class="s1">self.exog</span><span class="s0">,</span>
                                              <span class="s1">offset=offset)</span>
            <span class="s1">start_params = model.fit_regularized(</span>
                <span class="s1">start_params=start_params</span><span class="s0">, </span><span class="s1">method=method</span><span class="s0">, </span><span class="s1">maxiter=maxiter</span><span class="s0">,</span>
                <span class="s1">full_output=full_output</span><span class="s0">, </span><span class="s1">disp=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">callback=callback</span><span class="s0">,</span>
                <span class="s1">alpha=alpha_p</span><span class="s0">, </span><span class="s1">trim_mode=trim_mode</span><span class="s0">,</span>
                <span class="s1">auto_trim_tol=auto_trim_tol</span><span class="s0">,</span>
                <span class="s1">size_trim_tol=size_trim_tol</span><span class="s0">, </span><span class="s1">qc_tol=qc_tol</span><span class="s0">, </span><span class="s1">**kwargs).params</span>
        <span class="s1">cntfit = super(CountModel</span><span class="s0">, </span><span class="s1">self).fit_regularized(</span>
                <span class="s1">start_params=start_params</span><span class="s0">, </span><span class="s1">method=method</span><span class="s0">, </span><span class="s1">maxiter=maxiter</span><span class="s0">,</span>
                <span class="s1">full_output=full_output</span><span class="s0">, </span><span class="s1">disp=disp</span><span class="s0">, </span><span class="s1">callback=callback</span><span class="s0">,</span>
                <span class="s1">alpha=alpha</span><span class="s0">, </span><span class="s1">trim_mode=trim_mode</span><span class="s0">, </span><span class="s1">auto_trim_tol=auto_trim_tol</span><span class="s0">,</span>
                <span class="s1">size_trim_tol=size_trim_tol</span><span class="s0">, </span><span class="s1">qc_tol=qc_tol</span><span class="s0">, </span><span class="s1">**kwargs)</span>

        <span class="s0">if </span><span class="s1">method </span><span class="s0">in </span><span class="s1">[</span><span class="s2">'l1'</span><span class="s0">, </span><span class="s2">'l1_cvxopt_cp'</span><span class="s1">]:</span>
            <span class="s1">discretefit = self.result_class_reg(self</span><span class="s0">, </span><span class="s1">cntfit)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">raise </span><span class="s1">TypeError(</span>
                    <span class="s2">&quot;argument method == %s, which is not handled&quot; </span><span class="s1">% method)</span>

        <span class="s0">return </span><span class="s1">self.result_class_reg_wrapper(discretefit)</span>

    <span class="s1">fit_regularized.__doc__ = DiscreteModel.fit_regularized.__doc__</span>

    <span class="s0">def </span><span class="s1">hessian(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Generic Truncated model Hessian matrix of the loglikelihood 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        hess : ndarray, (k_vars, k_vars) 
            The Hessian, second derivative of loglikelihood function, 
            evaluated at `params` 
 
        Notes 
        ----- 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">approx_hess(params</span><span class="s0">, </span><span class="s1">self.loglike)</span>

    <span class="s0">def </span><span class="s1">predict(self</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">exog=</span><span class="s0">None, </span><span class="s1">exposure=</span><span class="s0">None, </span><span class="s1">offset=</span><span class="s0">None,</span>
                <span class="s1">which=</span><span class="s2">'mean'</span><span class="s0">, </span><span class="s1">y_values=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Predict response variable or other statistic given exogenous variables. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
        exog : ndarray, optional 
            Explanatory variables for the main count model. 
            If ``exog`` is None, then the data from the model will be used. 
        offset : ndarray, optional 
            Offset is added to the linear predictor of the mean function with 
            coefficient equal to 1. 
            Default is zero if exog is not None, and the model offset if exog 
            is None. 
        exposure : ndarray, optional 
            Log(exposure) is added to the linear predictor with coefficient 
            equal to 1. If exposure is specified, then it will be logged by 
            the method. The user does not need to log it first. 
            Default is one if exog is is not None, and it is the model exposure 
            if exog is None. 
        which : str (optional) 
            Statitistic to predict. Default is 'mean'. 
 
            - 'mean' : the conditional expectation of endog E(y | x) 
            - 'mean-main' : mean parameter of truncated count model. 
              Note, this is not the mean of the truncated distribution. 
            - 'linear' : the linear predictor of the truncated count model. 
            - 'var' : returns the estimated variance of endog implied by the 
              model. 
            - 'prob-trunc' : probability of truncation. This is the probability 
              of observing a zero count implied 
              by the truncation model. 
            - 'prob' : probabilities of each count from 0 to max(endog), or 
              for y_values if those are provided. This is a multivariate 
              return (2-dim when predicting for several observations). 
              The probabilities in the truncated region are zero. 
            - 'prob-base' : probabilities for untruncated base distribution. 
              The probabilities are for each count from 0 to max(endog), or 
              for y_values if those are provided. This is a multivariate 
              return (2-dim when predicting for several observations). 
 
 
        y_values : array_like 
            Values of the random variable endog at which pmf is evaluated. 
            Only used if ``which=&quot;prob&quot;`` 
 
        Returns 
        ------- 
        predicted values 
 
        Notes 
        ----- 
        If exposure is specified, then it will be logged by the method. 
        The user does not need to log it first. 
        &quot;&quot;&quot;</span>
        <span class="s1">exog</span><span class="s0">, </span><span class="s1">offset</span><span class="s0">, </span><span class="s1">exposure = self._get_predict_arrays(</span>
            <span class="s1">exog=exog</span><span class="s0">,</span>
            <span class="s1">offset=offset</span><span class="s0">,</span>
            <span class="s1">exposure=exposure</span>
            <span class="s1">)</span>

        <span class="s1">fitted = np.dot(exog</span><span class="s0">, </span><span class="s1">params[:exog.shape[</span><span class="s3">1</span><span class="s1">]])</span>
        <span class="s1">linpred = fitted + exposure + offset</span>

        <span class="s0">if </span><span class="s1">which == </span><span class="s2">'mean'</span><span class="s1">:</span>
            <span class="s1">mu = np.exp(linpred)</span>
            <span class="s0">if </span><span class="s1">self.truncation == </span><span class="s3">0</span><span class="s1">:</span>
                <span class="s1">prob_main = self.model_main._prob_nonzero(mu</span><span class="s0">, </span><span class="s1">params)</span>
                <span class="s0">return </span><span class="s1">mu / prob_main</span>
            <span class="s0">elif </span><span class="s1">self.truncation == -</span><span class="s3">1</span><span class="s1">:</span>
                <span class="s0">return </span><span class="s1">mu</span>
            <span class="s0">elif </span><span class="s1">self.truncation &gt; </span><span class="s3">0</span><span class="s1">:</span>
                <span class="s1">counts = np.atleast_2d(np.arange(</span><span class="s3">0</span><span class="s0">, </span><span class="s1">self.truncation + </span><span class="s3">1</span><span class="s1">))</span>
                <span class="s4"># next is same as in prob-main below</span>
                <span class="s1">probs = self.model_main.predict(</span>
                    <span class="s1">params</span><span class="s0">, </span><span class="s1">exog=exog</span><span class="s0">, </span><span class="s1">exposure=np.exp(exposure)</span><span class="s0">,</span>
                    <span class="s1">offset=offset</span><span class="s0">, </span><span class="s1">which=</span><span class="s2">&quot;prob&quot;</span><span class="s0">, </span><span class="s1">y_values=counts)</span>
                <span class="s1">prob_tregion = probs.sum(</span><span class="s3">1</span><span class="s1">)</span>
                <span class="s1">mean_tregion = (np.arange(self.truncation + </span><span class="s3">1</span><span class="s1">) * probs).sum(</span><span class="s3">1</span><span class="s1">)</span>
                <span class="s1">mean = (mu - mean_tregion) / (</span><span class="s3">1 </span><span class="s1">- prob_tregion)</span>
                <span class="s0">return </span><span class="s1">mean</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;unsupported self.truncation&quot;</span><span class="s1">)</span>
        <span class="s0">elif </span><span class="s1">which == </span><span class="s2">'linear'</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">linpred</span>
        <span class="s0">elif </span><span class="s1">which == </span><span class="s2">'mean-main'</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">np.exp(linpred)</span>
        <span class="s0">elif </span><span class="s1">which == </span><span class="s2">'prob'</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">y_values </span><span class="s0">is not None</span><span class="s1">:</span>
                <span class="s1">counts = np.atleast_2d(y_values)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">counts = np.atleast_2d(np.arange(</span><span class="s3">0</span><span class="s0">, </span><span class="s1">np.max(self.endog)+</span><span class="s3">1</span><span class="s1">))</span>
            <span class="s1">mu = np.exp(linpred)[:</span><span class="s0">, None</span><span class="s1">]</span>
            <span class="s0">if </span><span class="s1">self.k_extra == </span><span class="s3">0</span><span class="s1">:</span>
                <span class="s4"># poisson, no extra params</span>
                <span class="s1">probs = self.model_dist.pmf(counts</span><span class="s0">, </span><span class="s1">mu</span><span class="s0">, </span><span class="s1">self.trunc)</span>
            <span class="s0">elif </span><span class="s1">self.k_extra == </span><span class="s3">1</span><span class="s1">:</span>
                <span class="s1">p = self.model_main.parameterization</span>
                <span class="s1">probs = self.model_dist.pmf(counts</span><span class="s0">, </span><span class="s1">mu</span><span class="s0">, </span><span class="s1">params[-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
                                            <span class="s1">p</span><span class="s0">, </span><span class="s1">self.trunc)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;k_extra is not 0 or 1&quot;</span><span class="s1">)</span>
            <span class="s0">return </span><span class="s1">probs</span>
        <span class="s0">elif </span><span class="s1">which == </span><span class="s2">'prob-base'</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">y_values </span><span class="s0">is not None</span><span class="s1">:</span>
                <span class="s1">counts = np.asarray(y_values)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">counts = np.arange(</span><span class="s3">0</span><span class="s0">, </span><span class="s1">np.max(self.endog)+</span><span class="s3">1</span><span class="s1">)</span>

            <span class="s1">probs = self.model_main.predict(</span>
                <span class="s1">params</span><span class="s0">, </span><span class="s1">exog=exog</span><span class="s0">, </span><span class="s1">exposure=np.exp(exposure)</span><span class="s0">,</span>
                <span class="s1">offset=offset</span><span class="s0">, </span><span class="s1">which=</span><span class="s2">&quot;prob&quot;</span><span class="s0">, </span><span class="s1">y_values=counts)</span>
            <span class="s0">return </span><span class="s1">probs</span>
        <span class="s0">elif </span><span class="s1">which == </span><span class="s2">'var'</span><span class="s1">:</span>
            <span class="s1">mu = np.exp(linpred)</span>
            <span class="s1">counts = np.atleast_2d(np.arange(</span><span class="s3">0</span><span class="s0">, </span><span class="s1">self.truncation + </span><span class="s3">1</span><span class="s1">))</span>
            <span class="s4"># next is same as in prob-main below</span>
            <span class="s1">probs = self.model_main.predict(</span>
                <span class="s1">params</span><span class="s0">, </span><span class="s1">exog=exog</span><span class="s0">, </span><span class="s1">exposure=np.exp(exposure)</span><span class="s0">,</span>
                <span class="s1">offset=offset</span><span class="s0">, </span><span class="s1">which=</span><span class="s2">&quot;prob&quot;</span><span class="s0">, </span><span class="s1">y_values=counts)</span>
            <span class="s1">prob_tregion = probs.sum(</span><span class="s3">1</span><span class="s1">)</span>
            <span class="s1">mean_tregion = (np.arange(self.truncation + </span><span class="s3">1</span><span class="s1">) * probs).sum(</span><span class="s3">1</span><span class="s1">)</span>
            <span class="s1">mean = (mu - mean_tregion) / (</span><span class="s3">1 </span><span class="s1">- prob_tregion)</span>
            <span class="s1">mnc2_tregion = (np.arange(self.truncation + </span><span class="s3">1</span><span class="s1">)**</span><span class="s3">2 </span><span class="s1">*</span>
                            <span class="s1">probs).sum(</span><span class="s3">1</span><span class="s1">)</span>
            <span class="s1">vm = self.model_main._var(mu</span><span class="s0">, </span><span class="s1">params)</span>
            <span class="s4"># uncentered 2nd moment</span>
            <span class="s1">mnc2 = (mu**</span><span class="s3">2 </span><span class="s1">+ vm - mnc2_tregion) / (</span><span class="s3">1 </span><span class="s1">- prob_tregion)</span>
            <span class="s1">v = mnc2 - mean**</span><span class="s3">2</span>
            <span class="s0">return </span><span class="s1">v</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span>
                <span class="s2">&quot;argument which == %s not handled&quot; </span><span class="s1">% which)</span>


<span class="s0">class </span><span class="s1">TruncatedLFPoisson(TruncatedLFGeneric):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Truncated Poisson model for count data 
 
    .. versionadded:: 0.14.0 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : array 
        A reference to the endogenous response variable 
    exog : array 
        A reference to the exogenous design. 
    truncation : int, optional 
        Truncation parameter specify truncation point out of the support 
        of the distribution. pmf(k) = 0 for k &lt;= truncation 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s0">,</span>
           <span class="s2">'extra_params'</span><span class="s1">:</span>
           <span class="s2">&quot;&quot;&quot;offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    exposure : array_like 
        Log(exposure) is added to the linear prediction with coefficient 
        equal to 1. 
 
    &quot;&quot;&quot; </span><span class="s1">+ base._missing_param_doc}</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">offset=</span><span class="s0">None, </span><span class="s1">exposure=</span><span class="s0">None,</span>
                 <span class="s1">truncation=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">missing=</span><span class="s2">'none'</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s1">super(TruncatedLFPoisson</span><span class="s0">, </span><span class="s1">self).__init__(</span>
            <span class="s1">endog</span><span class="s0">,</span>
            <span class="s1">exog</span><span class="s0">,</span>
            <span class="s1">offset=offset</span><span class="s0">,</span>
            <span class="s1">exposure=exposure</span><span class="s0">,</span>
            <span class="s1">truncation=truncation</span><span class="s0">,</span>
            <span class="s1">missing=missing</span><span class="s0">,</span>
            <span class="s1">**kwargs</span>
            <span class="s1">)</span>
        <span class="s1">self.model_main = Poisson(self.endog</span><span class="s0">, </span><span class="s1">self.exog</span><span class="s0">,</span>
                                  <span class="s1">exposure=getattr(self</span><span class="s0">, </span><span class="s2">&quot;exposure&quot;</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
                                  <span class="s1">offset=getattr(self</span><span class="s0">, </span><span class="s2">&quot;offset&quot;</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
                                  <span class="s1">)</span>
        <span class="s1">self.model_dist = truncatedpoisson</span>

        <span class="s1">self.result_class = TruncatedLFPoissonResults</span>
        <span class="s1">self.result_class_wrapper = TruncatedLFGenericResultsWrapper</span>
        <span class="s1">self.result_class_reg = L1TruncatedLFGenericResults</span>
        <span class="s1">self.result_class_reg_wrapper = L1TruncatedLFGenericResultsWrapper</span>

    <span class="s0">def </span><span class="s1">_predict_mom_trunc0(self</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">mu):</span>
        <span class="s5">&quot;&quot;&quot;Predict mean and variance of zero-truncated distribution. 
 
        experimental api, will likely be replaced by other methods 
 
        Parameters 
        ---------- 
        params : array_like 
            The model parameters. This is only used to extract extra params 
            like dispersion parameter. 
        mu : array_like 
            Array of mean predictions for main model. 
 
        Returns 
        ------- 
        Predicted conditional variance. 
        &quot;&quot;&quot;</span>
        <span class="s1">w = (</span><span class="s3">1 </span><span class="s1">- np.exp(-mu))  </span><span class="s4"># prob of no truncation, 1 - P(y=0)</span>
        <span class="s1">m = mu / w</span>
        <span class="s1">var_ = m - (</span><span class="s3">1 </span><span class="s1">- w) * m**</span><span class="s3">2</span>
        <span class="s0">return </span><span class="s1">m</span><span class="s0">, </span><span class="s1">var_</span>


<span class="s0">class </span><span class="s1">TruncatedLFNegativeBinomialP(TruncatedLFGeneric):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Truncated Generalized Negative Binomial model for count data 
 
    .. versionadded:: 0.14.0 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : array 
        A reference to the endogenous response variable 
    exog : array 
        A reference to the exogenous design. 
    truncation : int, optional 
        Truncation parameter specify truncation point out of the support 
        of the distribution. pmf(k) = 0 for k &lt;= truncation 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s0">,</span>
           <span class="s2">'extra_params'</span><span class="s1">:</span>
           <span class="s2">&quot;&quot;&quot;offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    exposure : array_like 
        Log(exposure) is added to the linear prediction with coefficient 
        equal to 1. 
 
    &quot;&quot;&quot; </span><span class="s1">+ base._missing_param_doc}</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">offset=</span><span class="s0">None, </span><span class="s1">exposure=</span><span class="s0">None,</span>
                 <span class="s1">truncation=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">p=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">missing=</span><span class="s2">'none'</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s1">super(TruncatedLFNegativeBinomialP</span><span class="s0">, </span><span class="s1">self).__init__(</span>
            <span class="s1">endog</span><span class="s0">,</span>
            <span class="s1">exog</span><span class="s0">,</span>
            <span class="s1">offset=offset</span><span class="s0">,</span>
            <span class="s1">exposure=exposure</span><span class="s0">,</span>
            <span class="s1">truncation=truncation</span><span class="s0">,</span>
            <span class="s1">missing=missing</span><span class="s0">,</span>
            <span class="s1">**kwargs</span>
            <span class="s1">)</span>
        <span class="s1">self.model_main = NegativeBinomialP(</span>
            <span class="s1">self.endog</span><span class="s0">,</span>
            <span class="s1">self.exog</span><span class="s0">,</span>
            <span class="s1">exposure=getattr(self</span><span class="s0">, </span><span class="s2">&quot;exposure&quot;</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">offset=getattr(self</span><span class="s0">, </span><span class="s2">&quot;offset&quot;</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">p=p</span>
            <span class="s1">)</span>
        <span class="s1">self.k_extra = self.model_main.k_extra</span>
        <span class="s1">self.exog_names.extend(self.model_main.exog_names[-self.k_extra:])</span>
        <span class="s1">self.model_dist = truncatednegbin</span>

        <span class="s1">self.result_class = TruncatedNegativeBinomialResults</span>
        <span class="s1">self.result_class_wrapper = TruncatedLFGenericResultsWrapper</span>
        <span class="s1">self.result_class_reg = L1TruncatedLFGenericResults</span>
        <span class="s1">self.result_class_reg_wrapper = L1TruncatedLFGenericResultsWrapper</span>

    <span class="s0">def </span><span class="s1">_predict_mom_trunc0(self</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">mu):</span>
        <span class="s5">&quot;&quot;&quot;Predict mean and variance of zero-truncated distribution. 
 
        experimental api, will likely be replaced by other methods 
 
        Parameters 
        ---------- 
        params : array_like 
            The model parameters. This is only used to extract extra params 
            like dispersion parameter. 
        mu : array_like 
            Array of mean predictions for main model. 
 
        Returns 
        ------- 
        Predicted conditional variance. 
        &quot;&quot;&quot;</span>
        <span class="s4"># note: prob_zero and vm are distribution specific, rest is generic</span>
        <span class="s4"># when mean of base model is mu</span>
        <span class="s1">alpha = params[-</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">p = self.model_main.parameterization</span>
        <span class="s1">prob_zero = (</span><span class="s3">1 </span><span class="s1">+ alpha * mu**(p-</span><span class="s3">1</span><span class="s1">))**(- </span><span class="s3">1 </span><span class="s1">/ alpha)</span>
        <span class="s1">w = </span><span class="s3">1 </span><span class="s1">- prob_zero  </span><span class="s4"># prob of no truncation, 1 - P(y=0)</span>
        <span class="s1">m = mu / w</span>
        <span class="s1">vm = mu * (</span><span class="s3">1 </span><span class="s1">+ alpha * mu**(p-</span><span class="s3">1</span><span class="s1">))  </span><span class="s4"># variance of NBP</span>
        <span class="s4"># uncentered 2nd moment is vm + mu**2</span>
        <span class="s1">mnc2 = (mu**</span><span class="s3">2 </span><span class="s1">+ vm) / w  </span><span class="s4"># uses mnc2_tregion = 0</span>
        <span class="s1">var_ = mnc2 - m**</span><span class="s3">2</span>
        <span class="s0">return </span><span class="s1">m</span><span class="s0">, </span><span class="s1">var_</span>


<span class="s0">class </span><span class="s1">TruncatedLFGeneralizedPoisson(TruncatedLFGeneric):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Truncated Generalized Poisson model for count data 
 
    .. versionadded:: 0.14.0 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : array 
        A reference to the endogenous response variable 
    exog : array 
        A reference to the exogenous design. 
    truncation : int, optional 
        Truncation parameter specify truncation point out of the support 
        of the distribution. pmf(k) = 0 for k &lt;= truncation 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s0">,</span>
           <span class="s2">'extra_params'</span><span class="s1">:</span>
           <span class="s2">&quot;&quot;&quot;offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    exposure : array_like 
        Log(exposure) is added to the linear prediction with coefficient 
        equal to 1. 
 
    &quot;&quot;&quot; </span><span class="s1">+ base._missing_param_doc}</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">offset=</span><span class="s0">None, </span><span class="s1">exposure=</span><span class="s0">None,</span>
                 <span class="s1">truncation=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">p=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">missing=</span><span class="s2">'none'</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s1">super(TruncatedLFGeneralizedPoisson</span><span class="s0">, </span><span class="s1">self).__init__(</span>
            <span class="s1">endog</span><span class="s0">,</span>
            <span class="s1">exog</span><span class="s0">,</span>
            <span class="s1">offset=offset</span><span class="s0">,</span>
            <span class="s1">exposure=exposure</span><span class="s0">,</span>
            <span class="s1">truncation=truncation</span><span class="s0">,</span>
            <span class="s1">missing=missing</span><span class="s0">,</span>
            <span class="s1">**kwargs</span>
            <span class="s1">)</span>
        <span class="s1">self.model_main = GeneralizedPoisson(</span>
            <span class="s1">self.endog</span><span class="s0">,</span>
            <span class="s1">self.exog</span><span class="s0">,</span>
            <span class="s1">exposure=getattr(self</span><span class="s0">, </span><span class="s2">&quot;exposure&quot;</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">offset=getattr(self</span><span class="s0">, </span><span class="s2">&quot;offset&quot;</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">p=p</span>
            <span class="s1">)</span>
        <span class="s1">self.k_extra = self.model_main.k_extra</span>
        <span class="s1">self.exog_names.extend(self.model_main.exog_names[-self.k_extra:])</span>
        <span class="s1">self.model_dist = </span><span class="s0">None</span>
        <span class="s1">self.result_class = TruncatedNegativeBinomialResults</span>

        <span class="s1">self.result_class_wrapper = TruncatedLFGenericResultsWrapper</span>
        <span class="s1">self.result_class_reg = L1TruncatedLFGenericResults</span>
        <span class="s1">self.result_class_reg_wrapper = L1TruncatedLFGenericResultsWrapper</span>


<span class="s0">class </span><span class="s1">_RCensoredGeneric(CountModel):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Generic right Censored model for count data 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : array 
        A reference to the endogenous response variable 
    exog : array 
        A reference to the exogenous design. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s0">,</span>
           <span class="s2">'extra_params'</span><span class="s1">:</span>
           <span class="s2">&quot;&quot;&quot;offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    exposure : array_like 
        Log(exposure) is added to the linear prediction with coefficient 
        equal to 1. 
 
    &quot;&quot;&quot; </span><span class="s1">+ base._missing_param_doc}</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">offset=</span><span class="s0">None, </span><span class="s1">exposure=</span><span class="s0">None,</span>
                 <span class="s1">missing=</span><span class="s2">'none'</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s1">self.zero_idx = np.nonzero(endog == </span><span class="s3">0</span><span class="s1">)[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">self.nonzero_idx = np.nonzero(endog)[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">super(_RCensoredGeneric</span><span class="s0">, </span><span class="s1">self).__init__(</span>
            <span class="s1">endog</span><span class="s0">,</span>
            <span class="s1">exog</span><span class="s0">,</span>
            <span class="s1">offset=offset</span><span class="s0">,</span>
            <span class="s1">exposure=exposure</span><span class="s0">,</span>
            <span class="s1">missing=missing</span><span class="s0">,</span>
            <span class="s1">**kwargs</span>
            <span class="s1">)</span>

    <span class="s0">def </span><span class="s1">loglike(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Loglikelihood of Generic Censored model 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike : float 
            The log-likelihood function of the model evaluated at `params`. 
            See notes. 
 
        Notes 
        ----- 
 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">np.sum(self.loglikeobs(params))</span>

    <span class="s0">def </span><span class="s1">loglikeobs(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Loglikelihood for observations of Generic Censored model 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike : ndarray (nobs,) 
            The log likelihood for each observation of the model evaluated 
            at `params`. See Notes 
 
        Notes 
        ----- 
 
        &quot;&quot;&quot;</span>
        <span class="s1">llf_main = self.model_main.loglikeobs(params)</span>

        <span class="s1">llf = np.concatenate(</span>
            <span class="s1">(llf_main[self.zero_idx]</span><span class="s0">,</span>
             <span class="s1">np.log(</span><span class="s3">1 </span><span class="s1">- np.exp(llf_main[self.nonzero_idx])))</span>
            <span class="s1">)</span>

        <span class="s0">return </span><span class="s1">llf</span>

    <span class="s0">def </span><span class="s1">score_obs(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Generic Censored model score (gradient) vector of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        score : ndarray, 1-D 
            The score vector of the model, i.e. the first derivative of the 
            loglikelihood function, evaluated at `params` 
        &quot;&quot;&quot;</span>
        <span class="s1">score_main = self.model_main.score_obs(params)</span>
        <span class="s1">llf_main = self.model_main.loglikeobs(params)</span>

        <span class="s1">score = np.concatenate((</span>
            <span class="s1">score_main[self.zero_idx]</span><span class="s0">,</span>
            <span class="s1">(score_main[self.nonzero_idx].T *</span>
             <span class="s1">-np.exp(llf_main[self.nonzero_idx]) /</span>
             <span class="s1">(</span><span class="s3">1 </span><span class="s1">- np.exp(llf_main[self.nonzero_idx]))).T</span>
            <span class="s1">))</span>

        <span class="s0">return </span><span class="s1">score</span>

    <span class="s0">def </span><span class="s1">score(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Generic Censored model score (gradient) vector of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        score : ndarray, 1-D 
            The score vector of the model, i.e. the first derivative of the 
            loglikelihood function, evaluated at `params` 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">self.score_obs(params).sum(</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">start_params=</span><span class="s0">None, </span><span class="s1">method=</span><span class="s2">'bfgs'</span><span class="s0">, </span><span class="s1">maxiter=</span><span class="s3">35</span><span class="s0">,</span>
            <span class="s1">full_output=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">disp=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">callback=</span><span class="s0">None,</span>
            <span class="s1">cov_type=</span><span class="s2">'nonrobust'</span><span class="s0">, </span><span class="s1">cov_kwds=</span><span class="s0">None, </span><span class="s1">use_t=</span><span class="s0">None, </span><span class="s1">**kwargs):</span>
        <span class="s0">if </span><span class="s1">start_params </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">offset = getattr(self</span><span class="s0">, </span><span class="s2">&quot;offset&quot;</span><span class="s0">, </span><span class="s3">0</span><span class="s1">) + getattr(self</span><span class="s0">, </span><span class="s2">&quot;exposure&quot;</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span>
            <span class="s0">if </span><span class="s1">np.size(offset) == </span><span class="s3">1 </span><span class="s0">and </span><span class="s1">offset == </span><span class="s3">0</span><span class="s1">:</span>
                <span class="s1">offset = </span><span class="s0">None</span>
            <span class="s1">model = self.model_main.__class__(self.endog</span><span class="s0">, </span><span class="s1">self.exog</span><span class="s0">,</span>
                                              <span class="s1">offset=offset)</span>
            <span class="s0">with </span><span class="s1">warnings.catch_warnings():</span>
                <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;ignore&quot;</span><span class="s0">, </span><span class="s1">category=ConvergenceWarning)</span>
                <span class="s1">start_params = model.fit(disp=</span><span class="s3">0</span><span class="s1">).params</span>
        <span class="s1">mlefit = super(_RCensoredGeneric</span><span class="s0">, </span><span class="s1">self).fit(</span>
            <span class="s1">start_params=start_params</span><span class="s0">,</span>
            <span class="s1">method=method</span><span class="s0">,</span>
            <span class="s1">maxiter=maxiter</span><span class="s0">,</span>
            <span class="s1">disp=disp</span><span class="s0">,</span>
            <span class="s1">full_output=full_output</span><span class="s0">,</span>
            <span class="s1">callback=</span><span class="s0">lambda </span><span class="s1">x: x</span><span class="s0">,</span>
            <span class="s1">**kwargs</span>
            <span class="s1">)</span>

        <span class="s1">zipfit = self.result_class(self</span><span class="s0">, </span><span class="s1">mlefit._results)</span>
        <span class="s1">result = self.result_class_wrapper(zipfit)</span>

        <span class="s0">if </span><span class="s1">cov_kwds </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">cov_kwds = {}</span>

        <span class="s1">result._get_robustcov_results(cov_type=cov_type</span><span class="s0">,</span>
                                      <span class="s1">use_self=</span><span class="s0">True, </span><span class="s1">use_t=use_t</span><span class="s0">, </span><span class="s1">**cov_kwds)</span>
        <span class="s0">return </span><span class="s1">result</span>

    <span class="s1">fit.__doc__ = DiscreteModel.fit.__doc__</span>

    <span class="s0">def </span><span class="s1">fit_regularized(</span>
            <span class="s1">self</span><span class="s0">, </span><span class="s1">start_params=</span><span class="s0">None, </span><span class="s1">method=</span><span class="s2">'l1'</span><span class="s0">,</span>
            <span class="s1">maxiter=</span><span class="s2">'defined_by_method'</span><span class="s0">, </span><span class="s1">full_output=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">disp=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">callback=</span><span class="s0">None,</span>
            <span class="s1">alpha=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">trim_mode=</span><span class="s2">'auto'</span><span class="s0">, </span><span class="s1">auto_trim_tol=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">size_trim_tol=</span><span class="s3">1e-4</span><span class="s0">,</span>
            <span class="s1">qc_tol=</span><span class="s3">0.03</span><span class="s0">, </span><span class="s1">**kwargs):</span>

        <span class="s0">if </span><span class="s1">np.size(alpha) == </span><span class="s3">1 </span><span class="s0">and </span><span class="s1">alpha != </span><span class="s3">0</span><span class="s1">:</span>
            <span class="s1">k_params = self.exog.shape[</span><span class="s3">1</span><span class="s1">]</span>
            <span class="s1">alpha = alpha * np.ones(k_params)</span>

        <span class="s1">alpha_p = alpha</span>
        <span class="s0">if </span><span class="s1">start_params </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">offset = getattr(self</span><span class="s0">, </span><span class="s2">&quot;offset&quot;</span><span class="s0">, </span><span class="s3">0</span><span class="s1">) + getattr(self</span><span class="s0">, </span><span class="s2">&quot;exposure&quot;</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span>
            <span class="s0">if </span><span class="s1">np.size(offset) == </span><span class="s3">1 </span><span class="s0">and </span><span class="s1">offset == </span><span class="s3">0</span><span class="s1">:</span>
                <span class="s1">offset = </span><span class="s0">None</span>
            <span class="s1">model = self.model_main.__class__(self.endog</span><span class="s0">, </span><span class="s1">self.exog</span><span class="s0">,</span>
                                              <span class="s1">offset=offset)</span>
            <span class="s1">start_params = model.fit_regularized(</span>
                <span class="s1">start_params=start_params</span><span class="s0">, </span><span class="s1">method=method</span><span class="s0">, </span><span class="s1">maxiter=maxiter</span><span class="s0">,</span>
                <span class="s1">full_output=full_output</span><span class="s0">, </span><span class="s1">disp=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">callback=callback</span><span class="s0">,</span>
                <span class="s1">alpha=alpha_p</span><span class="s0">, </span><span class="s1">trim_mode=trim_mode</span><span class="s0">,</span>
                <span class="s1">auto_trim_tol=auto_trim_tol</span><span class="s0">,</span>
                <span class="s1">size_trim_tol=size_trim_tol</span><span class="s0">, </span><span class="s1">qc_tol=qc_tol</span><span class="s0">, </span><span class="s1">**kwargs).params</span>
        <span class="s1">cntfit = super(CountModel</span><span class="s0">, </span><span class="s1">self).fit_regularized(</span>
                <span class="s1">start_params=start_params</span><span class="s0">, </span><span class="s1">method=method</span><span class="s0">, </span><span class="s1">maxiter=maxiter</span><span class="s0">,</span>
                <span class="s1">full_output=full_output</span><span class="s0">, </span><span class="s1">disp=disp</span><span class="s0">, </span><span class="s1">callback=callback</span><span class="s0">,</span>
                <span class="s1">alpha=alpha</span><span class="s0">, </span><span class="s1">trim_mode=trim_mode</span><span class="s0">, </span><span class="s1">auto_trim_tol=auto_trim_tol</span><span class="s0">,</span>
                <span class="s1">size_trim_tol=size_trim_tol</span><span class="s0">, </span><span class="s1">qc_tol=qc_tol</span><span class="s0">, </span><span class="s1">**kwargs)</span>

        <span class="s0">if </span><span class="s1">method </span><span class="s0">in </span><span class="s1">[</span><span class="s2">'l1'</span><span class="s0">, </span><span class="s2">'l1_cvxopt_cp'</span><span class="s1">]:</span>
            <span class="s1">discretefit = self.result_class_reg(self</span><span class="s0">, </span><span class="s1">cntfit)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">raise </span><span class="s1">TypeError(</span>
                    <span class="s2">&quot;argument method == %s, which is not handled&quot; </span><span class="s1">% method)</span>

        <span class="s0">return </span><span class="s1">self.result_class_reg_wrapper(discretefit)</span>

    <span class="s1">fit_regularized.__doc__ = DiscreteModel.fit_regularized.__doc__</span>

    <span class="s0">def </span><span class="s1">hessian(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Generic Censored model Hessian matrix of the loglikelihood 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        hess : ndarray, (k_vars, k_vars) 
            The Hessian, second derivative of loglikelihood function, 
            evaluated at `params` 
 
        Notes 
        ----- 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">approx_hess(params</span><span class="s0">, </span><span class="s1">self.loglike)</span>


<span class="s0">class </span><span class="s1">_RCensoredPoisson(_RCensoredGeneric):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Censored Poisson model for count data 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : array 
        A reference to the endogenous response variable 
    exog : array 
        A reference to the exogenous design. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s0">,</span>
           <span class="s2">'extra_params'</span><span class="s1">:</span>
           <span class="s2">&quot;&quot;&quot;offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    exposure : array_like 
        Log(exposure) is added to the linear prediction with coefficient 
        equal to 1. 
 
    &quot;&quot;&quot; </span><span class="s1">+ base._missing_param_doc}</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">offset=</span><span class="s0">None,</span>
                 <span class="s1">exposure=</span><span class="s0">None, </span><span class="s1">missing=</span><span class="s2">'none'</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s1">super(_RCensoredPoisson</span><span class="s0">, </span><span class="s1">self).__init__(endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">offset=offset</span><span class="s0">,</span>
                                                <span class="s1">exposure=exposure</span><span class="s0">,</span>
                                                <span class="s1">missing=missing</span><span class="s0">, </span><span class="s1">**kwargs)</span>
        <span class="s1">self.model_main = Poisson(np.zeros_like(self.endog)</span><span class="s0">, </span><span class="s1">self.exog)</span>
        <span class="s1">self.model_dist = </span><span class="s0">None</span>
        <span class="s1">self.result_class = TruncatedLFGenericResults</span>
        <span class="s1">self.result_class_wrapper = TruncatedLFGenericResultsWrapper</span>
        <span class="s1">self.result_class_reg = L1TruncatedLFGenericResults</span>
        <span class="s1">self.result_class_reg_wrapper = L1TruncatedLFGenericResultsWrapper</span>


<span class="s0">class </span><span class="s1">_RCensoredGeneralizedPoisson(_RCensoredGeneric):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Censored Generalized Poisson model for count data 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : array 
        A reference to the endogenous response variable 
    exog : array 
        A reference to the exogenous design. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s0">,</span>
           <span class="s2">'extra_params'</span><span class="s1">:</span>
           <span class="s2">&quot;&quot;&quot;offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    exposure : array_like 
        Log(exposure) is added to the linear prediction with coefficient 
        equal to 1. 
 
    &quot;&quot;&quot; </span><span class="s1">+ base._missing_param_doc}</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">offset=</span><span class="s0">None, </span><span class="s1">p=</span><span class="s3">2</span><span class="s0">,</span>
                 <span class="s1">exposure=</span><span class="s0">None, </span><span class="s1">missing=</span><span class="s2">'none'</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s1">super(_RCensoredGeneralizedPoisson</span><span class="s0">, </span><span class="s1">self).__init__(</span>
            <span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">offset=offset</span><span class="s0">, </span><span class="s1">exposure=exposure</span><span class="s0">,</span>
            <span class="s1">missing=missing</span><span class="s0">, </span><span class="s1">**kwargs)</span>

        <span class="s1">self.model_main = GeneralizedPoisson(</span>
            <span class="s1">np.zeros_like(self.endog)</span><span class="s0">, </span><span class="s1">self.exog)</span>
        <span class="s1">self.model_dist = </span><span class="s0">None</span>
        <span class="s1">self.result_class = TruncatedLFGenericResults</span>
        <span class="s1">self.result_class_wrapper = TruncatedLFGenericResultsWrapper</span>
        <span class="s1">self.result_class_reg = L1TruncatedLFGenericResults</span>
        <span class="s1">self.result_class_reg_wrapper = L1TruncatedLFGenericResultsWrapper</span>


<span class="s0">class </span><span class="s1">_RCensoredNegativeBinomialP(_RCensoredGeneric):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Censored Negative Binomial model for count data 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : array 
        A reference to the endogenous response variable 
    exog : array 
        A reference to the exogenous design. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s0">,</span>
           <span class="s2">'extra_params'</span><span class="s1">:</span>
           <span class="s2">&quot;&quot;&quot;offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    exposure : array_like 
        Log(exposure) is added to the linear prediction with coefficient 
        equal to 1. 
 
    &quot;&quot;&quot; </span><span class="s1">+ base._missing_param_doc}</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">offset=</span><span class="s0">None, </span><span class="s1">p=</span><span class="s3">2</span><span class="s0">,</span>
                 <span class="s1">exposure=</span><span class="s0">None, </span><span class="s1">missing=</span><span class="s2">'none'</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s1">super(_RCensoredNegativeBinomialP</span><span class="s0">, </span><span class="s1">self).__init__(</span>
            <span class="s1">endog</span><span class="s0">,</span>
            <span class="s1">exog</span><span class="s0">,</span>
            <span class="s1">offset=offset</span><span class="s0">,</span>
            <span class="s1">exposure=exposure</span><span class="s0">,</span>
            <span class="s1">missing=missing</span><span class="s0">,</span>
            <span class="s1">**kwargs</span>
            <span class="s1">)</span>
        <span class="s1">self.model_main = NegativeBinomialP(np.zeros_like(self.endog)</span><span class="s0">,</span>
                                            <span class="s1">self.exog</span><span class="s0">,</span>
                                            <span class="s1">p=p</span>
                                            <span class="s1">)</span>
        <span class="s1">self.model_dist = </span><span class="s0">None</span>
        <span class="s1">self.result_class = TruncatedLFGenericResults</span>
        <span class="s1">self.result_class_wrapper = TruncatedLFGenericResultsWrapper</span>
        <span class="s1">self.result_class_reg = L1TruncatedLFGenericResults</span>
        <span class="s1">self.result_class_reg_wrapper = L1TruncatedLFGenericResultsWrapper</span>


<span class="s0">class </span><span class="s1">_RCensored(_RCensoredGeneric):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Censored model for count data 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : array 
        A reference to the endogenous response variable 
    exog : array 
        A reference to the exogenous design. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s0">,</span>
           <span class="s2">'extra_params'</span><span class="s1">:</span>
           <span class="s2">&quot;&quot;&quot;offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    exposure : array_like 
        Log(exposure) is added to the linear prediction with coefficient 
        equal to 1. 
 
    &quot;&quot;&quot; </span><span class="s1">+ base._missing_param_doc}</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">model=Poisson</span><span class="s0">,</span>
                 <span class="s1">distribution=truncatedpoisson</span><span class="s0">, </span><span class="s1">offset=</span><span class="s0">None,</span>
                 <span class="s1">exposure=</span><span class="s0">None, </span><span class="s1">missing=</span><span class="s2">'none'</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s1">super(_RCensored</span><span class="s0">, </span><span class="s1">self).__init__(</span>
            <span class="s1">endog</span><span class="s0">,</span>
            <span class="s1">exog</span><span class="s0">,</span>
            <span class="s1">offset=offset</span><span class="s0">,</span>
            <span class="s1">exposure=exposure</span><span class="s0">,</span>
            <span class="s1">missing=missing</span><span class="s0">,</span>
            <span class="s1">**kwargs</span>
            <span class="s1">)</span>
        <span class="s1">self.model_main = model(np.zeros_like(self.endog)</span><span class="s0">, </span><span class="s1">self.exog)</span>
        <span class="s1">self.model_dist = distribution</span>
        <span class="s4"># fix k_extra and exog_names</span>
        <span class="s1">self.k_extra = k_extra = self.model_main.k_extra</span>
        <span class="s0">if </span><span class="s1">k_extra &gt; </span><span class="s3">0</span><span class="s1">:</span>
            <span class="s1">self.exog_names.extend(self.model_main.exog_names[-k_extra:])</span>

        <span class="s1">self.result_class = TruncatedLFGenericResults</span>
        <span class="s1">self.result_class_wrapper = TruncatedLFGenericResultsWrapper</span>
        <span class="s1">self.result_class_reg = L1TruncatedLFGenericResults</span>
        <span class="s1">self.result_class_reg_wrapper = L1TruncatedLFGenericResultsWrapper</span>

    <span class="s0">def </span><span class="s1">_prob_nonzero(self</span><span class="s0">, </span><span class="s1">mu</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot;Probability that count is not zero 
 
        internal use in Censored model, will be refactored or removed 
        &quot;&quot;&quot;</span>
        <span class="s1">prob_nz = self.model_main._prob_nonzero(mu</span><span class="s0">, </span><span class="s1">params)</span>
        <span class="s0">return </span><span class="s1">prob_nz</span>


<span class="s0">class </span><span class="s1">HurdleCountModel(CountModel):</span>
    <span class="s1">__doc__ = </span><span class="s2">&quot;&quot;&quot; 
    Hurdle model for count data 
 
    .. versionadded:: 0.14.0 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    endog : array 
        A reference to the endogenous response variable 
    exog : array 
        A reference to the exogenous design. 
    dist : string 
        Log-likelihood type of count model family. 'poisson' or 'negbin' 
    zerodist : string 
        Log-likelihood type of zero hurdle model family. 'poisson', 'negbin' 
    p : scalar 
        Define parameterization for count model. 
        Used when dist='negbin'. 
    pzero : scalar 
        Define parameterization parameter zero hurdle model family. 
        Used when zerodist='negbin'. 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s2">'params'</span><span class="s1">: base._model_params_doc</span><span class="s0">,</span>
           <span class="s2">'extra_params'</span><span class="s1">:</span>
           <span class="s2">&quot;&quot;&quot;offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    exposure : array_like 
        Log(exposure) is added to the linear prediction with coefficient 
        equal to 1. 
 
    Notes 
    ----- 
    The parameters in the NegativeBinomial zero model are not identified if 
    the predicted mean is constant. If there is no or only little variation in 
    the predicted mean, then convergence might fail, hessian might not be 
    invertible or parameter estimates will have large standard errors. 
 
    References 
    ---------- 
    not yet 
 
    &quot;&quot;&quot; </span><span class="s1">+ base._missing_param_doc}</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">offset=</span><span class="s0">None,</span>
                 <span class="s1">dist=</span><span class="s2">&quot;poisson&quot;</span><span class="s0">, </span><span class="s1">zerodist=</span><span class="s2">&quot;poisson&quot;</span><span class="s0">,</span>
                 <span class="s1">p=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">pzero=</span><span class="s3">2</span><span class="s0">,</span>
                 <span class="s1">exposure=</span><span class="s0">None, </span><span class="s1">missing=</span><span class="s2">'none'</span><span class="s0">, </span><span class="s1">**kwargs):</span>

        <span class="s0">if </span><span class="s1">(offset </span><span class="s0">is not None</span><span class="s1">) </span><span class="s0">or </span><span class="s1">(exposure </span><span class="s0">is not None</span><span class="s1">):</span>
            <span class="s1">msg = </span><span class="s2">&quot;Offset and exposure are not yet implemented&quot;</span>
            <span class="s0">raise </span><span class="s1">NotImplementedError(msg)</span>
        <span class="s1">super(HurdleCountModel</span><span class="s0">, </span><span class="s1">self).__init__(</span>
            <span class="s1">endog</span><span class="s0">,</span>
            <span class="s1">exog</span><span class="s0">,</span>
            <span class="s1">offset=offset</span><span class="s0">,</span>
            <span class="s1">exposure=exposure</span><span class="s0">,</span>
            <span class="s1">missing=missing</span><span class="s0">,</span>
            <span class="s1">**kwargs</span>
            <span class="s1">)</span>
        <span class="s1">self.k_extra1 = </span><span class="s3">0</span>
        <span class="s1">self.k_extra2 = </span><span class="s3">0</span>

        <span class="s1">self._initialize(dist</span><span class="s0">, </span><span class="s1">zerodist</span><span class="s0">, </span><span class="s1">p</span><span class="s0">, </span><span class="s1">pzero)</span>
        <span class="s1">self.result_class = HurdleCountResults</span>
        <span class="s1">self.result_class_wrapper = HurdleCountResultsWrapper</span>
        <span class="s1">self.result_class_reg = L1HurdleCountResults</span>
        <span class="s1">self.result_class_reg_wrapper = L1HurdleCountResultsWrapper</span>

    <span class="s0">def </span><span class="s1">_initialize(self</span><span class="s0">, </span><span class="s1">dist</span><span class="s0">, </span><span class="s1">zerodist</span><span class="s0">, </span><span class="s1">p</span><span class="s0">, </span><span class="s1">pzero):</span>
        <span class="s0">if </span><span class="s1">(dist </span><span class="s0">not in </span><span class="s1">[</span><span class="s2">&quot;poisson&quot;</span><span class="s0">, </span><span class="s2">&quot;negbin&quot;</span><span class="s1">] </span><span class="s0">or</span>
                <span class="s1">zerodist </span><span class="s0">not in </span><span class="s1">[</span><span class="s2">&quot;poisson&quot;</span><span class="s0">, </span><span class="s2">&quot;negbin&quot;</span><span class="s1">]):</span>
            <span class="s0">raise </span><span class="s1">NotImplementedError(</span><span class="s2">'dist and zerodist must be &quot;poisson&quot;,'</span>
                                      <span class="s2">'&quot;negbin&quot;'</span><span class="s1">)</span>

        <span class="s0">if </span><span class="s1">zerodist == </span><span class="s2">&quot;poisson&quot;</span><span class="s1">:</span>
            <span class="s1">self.model1 = _RCensored(self.endog</span><span class="s0">, </span><span class="s1">self.exog</span><span class="s0">, </span><span class="s1">model=Poisson)</span>
        <span class="s0">elif </span><span class="s1">zerodist == </span><span class="s2">&quot;negbin&quot;</span><span class="s1">:</span>
            <span class="s1">self.model1 = _RCensored(self.endog</span><span class="s0">, </span><span class="s1">self.exog</span><span class="s0">,</span>
                                     <span class="s1">model=NegativeBinomialP)</span>
            <span class="s1">self.k_extra1 += </span><span class="s3">1</span>

        <span class="s0">if </span><span class="s1">dist == </span><span class="s2">&quot;poisson&quot;</span><span class="s1">:</span>
            <span class="s1">self.model2 = TruncatedLFPoisson(self.endog</span><span class="s0">, </span><span class="s1">self.exog)</span>
        <span class="s0">elif </span><span class="s1">dist == </span><span class="s2">&quot;negbin&quot;</span><span class="s1">:</span>
            <span class="s1">self.model2 = TruncatedLFNegativeBinomialP(self.endog</span><span class="s0">, </span><span class="s1">self.exog</span><span class="s0">,</span>
                                                       <span class="s1">p=p)</span>
            <span class="s1">self.k_extra2 += </span><span class="s3">1</span>

    <span class="s0">def </span><span class="s1">loglike(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s5">&quot;&quot;&quot; 
        Loglikelihood of Generic Hurdle model 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike : float 
            The log-likelihood function of the model evaluated at `params`. 
            See notes. 
 
        Notes 
        ----- 
 
        &quot;&quot;&quot;</span>
        <span class="s1">k = int((len(params) - self.k_extra1 - self.k_extra2) / </span><span class="s3">2</span>
                <span class="s1">) + self.k_extra1</span>
        <span class="s0">return </span><span class="s1">(self.model1.loglike(params[:k]) +</span>
                <span class="s1">self.model2.loglike(params[k:]))</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">start_params=</span><span class="s0">None, </span><span class="s1">method=</span><span class="s2">'bfgs'</span><span class="s0">, </span><span class="s1">maxiter=</span><span class="s3">35</span><span class="s0">,</span>
            <span class="s1">full_output=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">disp=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">callback=</span><span class="s0">None,</span>
            <span class="s1">cov_type=</span><span class="s2">'nonrobust'</span><span class="s0">, </span><span class="s1">cov_kwds=</span><span class="s0">None, </span><span class="s1">use_t=</span><span class="s0">None, </span><span class="s1">**kwargs):</span>

        <span class="s0">if </span><span class="s1">cov_type != </span><span class="s2">&quot;nonrobust&quot;</span><span class="s1">:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;robust cov_type currently not supported&quot;</span><span class="s1">)</span>

        <span class="s1">results1 = self.model1.fit(</span>
            <span class="s1">start_params=start_params</span><span class="s0">,</span>
            <span class="s1">method=method</span><span class="s0">, </span><span class="s1">maxiter=maxiter</span><span class="s0">, </span><span class="s1">disp=disp</span><span class="s0">,</span>
            <span class="s1">full_output=full_output</span><span class="s0">, </span><span class="s1">callback=</span><span class="s0">lambda </span><span class="s1">x: x</span><span class="s0">,</span>
            <span class="s1">**kwargs</span>
            <span class="s1">)</span>

        <span class="s1">results2 = self.model2.fit(</span>
            <span class="s1">start_params=start_params</span><span class="s0">,</span>
            <span class="s1">method=method</span><span class="s0">, </span><span class="s1">maxiter=maxiter</span><span class="s0">, </span><span class="s1">disp=disp</span><span class="s0">,</span>
            <span class="s1">full_output=full_output</span><span class="s0">, </span><span class="s1">callback=</span><span class="s0">lambda </span><span class="s1">x: x</span><span class="s0">,</span>
            <span class="s1">**kwargs</span>
            <span class="s1">)</span>

        <span class="s1">result = deepcopy(results1)</span>
        <span class="s1">result._results.model = self</span>
        <span class="s1">result.mle_retvals[</span><span class="s2">'converged'</span><span class="s1">] = [results1.mle_retvals[</span><span class="s2">'converged'</span><span class="s1">]</span><span class="s0">,</span>
                                           <span class="s1">results2.mle_retvals[</span><span class="s2">'converged'</span><span class="s1">]]</span>
        <span class="s1">result._results.params = np.append(results1._results.params</span><span class="s0">,</span>
                                           <span class="s1">results2._results.params)</span>
        <span class="s4"># TODO: the following should be in __init__ or initialize</span>
        <span class="s1">result._results.df_model += results2._results.df_model</span>
        <span class="s4"># this looks wrong attr does not exist, always 0</span>
        <span class="s1">self.k_extra1 += getattr(results1._results</span><span class="s0">, </span><span class="s2">&quot;k_extra&quot;</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">self.k_extra2 += getattr(results2._results</span><span class="s0">, </span><span class="s2">&quot;k_extra&quot;</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">self.k_extra = (self.k_extra1 + self.k_extra2 + </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">xnames1 = [</span><span class="s2">&quot;zm_&quot; </span><span class="s1">+ name </span><span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">self.model1.exog_names]</span>
        <span class="s1">self.exog_names[:] = xnames1 + self.model2.exog_names</span>

        <span class="s4"># fix up cov_params,</span>
        <span class="s4"># we could use normalized cov_params directly, unless it's not used</span>
        <span class="s0">from </span><span class="s1">scipy.linalg </span><span class="s0">import </span><span class="s1">block_diag</span>
        <span class="s1">result._results.normalized_cov_params = </span><span class="s0">None</span>
        <span class="s0">try</span><span class="s1">:</span>
            <span class="s1">cov1 = results1._results.cov_params()</span>
            <span class="s1">cov2 = results2._results.cov_params()</span>
            <span class="s1">result._results.normalized_cov_params = block_diag(cov1</span><span class="s0">, </span><span class="s1">cov2)</span>
        <span class="s0">except </span><span class="s1">ValueError </span><span class="s0">as </span><span class="s1">e:</span>
            <span class="s0">if </span><span class="s2">&quot;need covariance&quot; </span><span class="s0">not in </span><span class="s1">str(e):</span>
                <span class="s4"># could be some other problem</span>
                <span class="s0">raise</span>

        <span class="s1">modelfit = self.result_class(self</span><span class="s0">, </span><span class="s1">result._results</span><span class="s0">, </span><span class="s1">results1</span><span class="s0">, </span><span class="s1">results2)</span>
        <span class="s1">result = self.result_class_wrapper(modelfit)</span>

        <span class="s0">return </span><span class="s1">result</span>

    <span class="s1">fit.__doc__ = DiscreteModel.fit.__doc__</span>

    <span class="s0">def </span><span class="s1">predict(self</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">exog=</span><span class="s0">None, </span><span class="s1">exposure=</span><span class="s0">None,</span>
                <span class="s1">offset=</span><span class="s0">None, </span><span class="s1">which=</span><span class="s2">'mean'</span><span class="s0">, </span><span class="s1">y_values=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Predict response variable or other statistic given exogenous variables. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
        exog : ndarray, optional 
            Explanatory variables for the main count model. 
            If ``exog`` is None, then the data from the model will be used. 
        exog_infl : ndarray, optional 
            Explanatory variables for the zero-inflation model. 
            ``exog_infl`` has to be provided if ``exog`` was provided unless 
            ``exog_infl`` in the model is only a constant. 
        offset : ndarray, optional 
            Offset is added to the linear predictor of the mean function with 
            coefficient equal to 1. 
            Default is zero if exog is not None, and the model offset if exog 
            is None. 
        exposure : ndarray, optional 
            Log(exposure) is added to the linear predictor with coefficient 
            equal to 1. If exposure is specified, then it will be logged by 
            the method. The user does not need to log it first. 
            Default is one if exog is is not None, and it is the model exposure 
            if exog is None. 
        which : str (optional) 
            Statitistic to predict. Default is 'mean'. 
 
            - 'mean' : the conditional expectation of endog E(y | x) 
            - 'mean-main' : mean parameter of truncated count model. 
              Note, this is not the mean of the truncated distribution. 
            - 'linear' : the linear predictor of the truncated count model. 
            - 'var' : returns the estimated variance of endog implied by the 
              model. 
            - 'prob-main' : probability of selecting the main model which is 
              the probability of observing a nonzero count P(y &gt; 0 | x). 
            - 'prob-zero' : probability of observing a zero count. P(y=0 | x). 
              This is equal to is ``1 - prob-main`` 
            - 'prob-trunc' : probability of truncation of the truncated count 
              model. This is the probability of observing a zero count implied 
              by the truncation model. 
            - 'mean-nonzero' : expected value conditional on having observation 
              larger than zero, E(y | X, y&gt;0) 
            - 'prob' : probabilities of each count from 0 to max(endog), or 
              for y_values if those are provided. This is a multivariate 
              return (2-dim when predicting for several observations). 
 
        y_values : array_like 
            Values of the random variable endog at which pmf is evaluated. 
            Only used if ``which=&quot;prob&quot;`` 
 
        Returns 
        ------- 
        predicted values 
 
        Notes 
        ----- 
        'prob-zero' / 'prob-trunc' is the ratio of probabilities of observing 
        a zero count between hurdle model and the truncated count model. 
        If this ratio is larger than one, then the hurdle model has an inflated 
        number of zeros compared to the count model. If it is smaller than one, 
        then the number of zeros is deflated. 
        &quot;&quot;&quot;</span>
        <span class="s1">which = which.lower()  </span><span class="s4"># make it case insensitive</span>
        <span class="s1">no_exog = </span><span class="s0">True if </span><span class="s1">exog </span><span class="s0">is None else False</span>
        <span class="s1">exog</span><span class="s0">, </span><span class="s1">offset</span><span class="s0">, </span><span class="s1">exposure = self._get_predict_arrays(</span>
            <span class="s1">exog=exog</span><span class="s0">,</span>
            <span class="s1">offset=offset</span><span class="s0">,</span>
            <span class="s1">exposure=exposure</span>
            <span class="s1">)</span>

        <span class="s1">exog_zero = </span><span class="s0">None  </span><span class="s4"># not yet</span>
        <span class="s0">if </span><span class="s1">exog_zero </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">no_exog:</span>
                <span class="s1">exog_zero = self.exog</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">exog_zero = exog</span>

        <span class="s1">k_zeros = int((len(params) - self.k_extra1 - self.k_extra2) / </span><span class="s3">2</span>
                      <span class="s1">) + self.k_extra1</span>
        <span class="s1">params_zero = params[:k_zeros]</span>
        <span class="s1">params_main = params[k_zeros:]</span>

        <span class="s1">lin_pred = (np.dot(exog</span><span class="s0">, </span><span class="s1">params_main[:self.exog.shape[</span><span class="s3">1</span><span class="s1">]]) +</span>
                    <span class="s1">exposure + offset)</span>

        <span class="s4"># this currently is mean_main, offset, exposure for zero part ?</span>
        <span class="s1">mu1 = self.model1.predict(params_zero</span><span class="s0">, </span><span class="s1">exog=exog)</span>
        <span class="s4"># prob that count model applies y&gt;0 from zero model predict</span>
        <span class="s1">prob_main = self.model1.model_main._prob_nonzero(mu1</span><span class="s0">, </span><span class="s1">params_zero)</span>
        <span class="s1">prob_zero = (</span><span class="s3">1 </span><span class="s1">- prob_main)</span>

        <span class="s1">mu2 = np.exp(lin_pred)</span>
        <span class="s1">prob_ntrunc = self.model2.model_main._prob_nonzero(mu2</span><span class="s0">, </span><span class="s1">params_main)</span>

        <span class="s0">if </span><span class="s1">which == </span><span class="s2">'mean'</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">prob_main * np.exp(lin_pred) / prob_ntrunc</span>
        <span class="s0">elif </span><span class="s1">which == </span><span class="s2">'mean-main'</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">np.exp(lin_pred)</span>
        <span class="s0">elif </span><span class="s1">which == </span><span class="s2">'linear'</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">lin_pred</span>
        <span class="s0">elif </span><span class="s1">which == </span><span class="s2">'mean-nonzero'</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">np.exp(lin_pred) / prob_ntrunc</span>
        <span class="s0">elif </span><span class="s1">which == </span><span class="s2">'prob-zero'</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">prob_zero</span>
        <span class="s0">elif </span><span class="s1">which == </span><span class="s2">'prob-main'</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">prob_main</span>
        <span class="s0">elif </span><span class="s1">which == </span><span class="s2">'prob-trunc'</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s3">1 </span><span class="s1">- prob_ntrunc</span>
        <span class="s4"># not yet supported</span>
        <span class="s0">elif </span><span class="s1">which == </span><span class="s2">'var'</span><span class="s1">:</span>
            <span class="s4"># generic computation using results from submodels</span>
            <span class="s1">mu = np.exp(lin_pred)</span>
            <span class="s1">mt</span><span class="s0">, </span><span class="s1">vt = self.model2._predict_mom_trunc0(params_main</span><span class="s0">, </span><span class="s1">mu)</span>
            <span class="s1">var_ = prob_main * vt + prob_main * (</span><span class="s3">1 </span><span class="s1">- prob_main) * mt**</span><span class="s3">2</span>
            <span class="s0">return </span><span class="s1">var_</span>
        <span class="s0">elif </span><span class="s1">which == </span><span class="s2">'prob'</span><span class="s1">:</span>
            <span class="s1">probs_main = self.model2.predict(</span>
                <span class="s1">params_main</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">np.exp(exposure)</span><span class="s0">, </span><span class="s1">offset</span><span class="s0">, </span><span class="s1">which=</span><span class="s2">&quot;prob&quot;</span><span class="s0">,</span>
                <span class="s1">y_values=y_values)</span>
            <span class="s1">probs_main *= prob_main[:</span><span class="s0">, None</span><span class="s1">]</span>
            <span class="s1">probs_main[:</span><span class="s0">, </span><span class="s3">0</span><span class="s1">] = prob_zero</span>
            <span class="s0">return </span><span class="s1">probs_main</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">'which = %s is not available' </span><span class="s1">% which)</span>


<span class="s0">class </span><span class="s1">TruncatedLFGenericResults(CountResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span>
        <span class="s2">&quot;one_line_description&quot;</span><span class="s1">: </span><span class="s2">&quot;A results class for Generic Truncated&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;extra_attr&quot;</span><span class="s1">: </span><span class="s2">&quot;&quot;</span><span class="s1">}</span>


<span class="s0">class </span><span class="s1">TruncatedLFPoissonResults(TruncatedLFGenericResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span>
        <span class="s2">&quot;one_line_description&quot;</span><span class="s1">: </span><span class="s2">&quot;A results class for Truncated Poisson&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;extra_attr&quot;</span><span class="s1">: </span><span class="s2">&quot;&quot;</span><span class="s1">}</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">_dispersion_factor(self):</span>
        <span class="s0">if </span><span class="s1">self.model.trunc != </span><span class="s3">0</span><span class="s1">:</span>
            <span class="s1">msg = </span><span class="s2">&quot;dispersion is only available for zero-truncation&quot;</span>
            <span class="s0">raise </span><span class="s1">NotImplementedError(msg)</span>

        <span class="s1">mu = np.exp(self.predict(which=</span><span class="s2">'linear'</span><span class="s1">))</span>

        <span class="s0">return </span><span class="s1">(</span><span class="s3">1 </span><span class="s1">- mu / (np.exp(mu) - </span><span class="s3">1</span><span class="s1">))</span>


<span class="s0">class </span><span class="s1">TruncatedNegativeBinomialResults(TruncatedLFGenericResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span>
        <span class="s2">&quot;one_line_description&quot;</span><span class="s1">:</span>
            <span class="s2">&quot;A results class for Truncated Negative Binomial&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;extra_attr&quot;</span><span class="s1">: </span><span class="s2">&quot;&quot;</span><span class="s1">}</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">_dispersion_factor(self):</span>
        <span class="s0">if </span><span class="s1">self.model.trunc != </span><span class="s3">0</span><span class="s1">:</span>
            <span class="s1">msg = </span><span class="s2">&quot;dispersion is only available for zero-truncation&quot;</span>
            <span class="s0">raise </span><span class="s1">NotImplementedError(msg)</span>

        <span class="s1">alpha = self.params[-</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">p = self.model.model_main.parameterization</span>
        <span class="s1">mu = np.exp(self.predict(which=</span><span class="s2">'linear'</span><span class="s1">))</span>

        <span class="s0">return </span><span class="s1">(</span><span class="s3">1 </span><span class="s1">- alpha * mu**(p-</span><span class="s3">1</span><span class="s1">) / (np.exp(mu**(p-</span><span class="s3">1</span><span class="s1">)) - </span><span class="s3">1</span><span class="s1">))</span>


<span class="s0">class </span><span class="s1">L1TruncatedLFGenericResults(L1CountResults</span><span class="s0">, </span><span class="s1">TruncatedLFGenericResults):</span>
    <span class="s0">pass</span>


<span class="s0">class </span><span class="s1">TruncatedLFGenericResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s0">pass</span>


<span class="s1">wrap.populate_wrapper(TruncatedLFGenericResultsWrapper</span><span class="s0">,</span>
                      <span class="s1">TruncatedLFGenericResults)</span>


<span class="s0">class </span><span class="s1">L1TruncatedLFGenericResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s0">pass</span>


<span class="s1">wrap.populate_wrapper(L1TruncatedLFGenericResultsWrapper</span><span class="s0">,</span>
                      <span class="s1">L1TruncatedLFGenericResults)</span>


<span class="s0">class </span><span class="s1">HurdleCountResults(CountResults):</span>
    <span class="s1">__doc__ = _discrete_results_docs % {</span>
        <span class="s2">&quot;one_line_description&quot;</span><span class="s1">: </span><span class="s2">&quot;A results class for Hurdle model&quot;</span><span class="s0">,</span>
        <span class="s2">&quot;extra_attr&quot;</span><span class="s1">: </span><span class="s2">&quot;&quot;</span><span class="s1">}</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">model</span><span class="s0">, </span><span class="s1">mlefit</span><span class="s0">, </span><span class="s1">results_zero</span><span class="s0">, </span><span class="s1">results_count</span><span class="s0">,</span>
                 <span class="s1">cov_type=</span><span class="s2">'nonrobust'</span><span class="s0">, </span><span class="s1">cov_kwds=</span><span class="s0">None, </span><span class="s1">use_t=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s1">super(HurdleCountResults</span><span class="s0">, </span><span class="s1">self).__init__(</span>
            <span class="s1">model</span><span class="s0">,</span>
            <span class="s1">mlefit</span><span class="s0">,</span>
            <span class="s1">cov_type=cov_type</span><span class="s0">,</span>
            <span class="s1">cov_kwds=cov_kwds</span><span class="s0">,</span>
            <span class="s1">use_t=use_t</span><span class="s0">,</span>
            <span class="s1">)</span>
        <span class="s1">self.results_zero = results_zero</span>
        <span class="s1">self.results_count = results_count</span>
        <span class="s4"># TODO: this is to fix df_resid, should be automatic but is not</span>
        <span class="s1">self.df_resid = self.model.endog.shape[</span><span class="s3">0</span><span class="s1">] - len(self.params)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">llnull(self):</span>
        <span class="s0">return </span><span class="s1">(self.results_zero._results.llnull +</span>
                <span class="s1">self.results_count._results.llnull)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s0">def </span><span class="s1">bse(self):</span>
        <span class="s0">return </span><span class="s1">np.append(self.results_zero.bse</span><span class="s0">, </span><span class="s1">self.results_count.bse)</span>


<span class="s0">class </span><span class="s1">L1HurdleCountResults(L1CountResults</span><span class="s0">, </span><span class="s1">HurdleCountResults):</span>
    <span class="s0">pass</span>


<span class="s0">class </span><span class="s1">HurdleCountResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s0">pass</span>


<span class="s1">wrap.populate_wrapper(HurdleCountResultsWrapper</span><span class="s0">,</span>
                      <span class="s1">HurdleCountResults)</span>


<span class="s0">class </span><span class="s1">L1HurdleCountResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s0">pass</span>


<span class="s1">wrap.populate_wrapper(L1HurdleCountResultsWrapper</span><span class="s0">,</span>
                      <span class="s1">L1HurdleCountResults)</span>
</pre>
</body>
</html>