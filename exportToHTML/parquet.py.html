<html>
<head>
<title>parquet.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #808080;}
.s5 { color: #a5c261;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
parquet.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; parquet compat &quot;&quot;&quot;</span>
<span class="s2">from </span><span class="s1">__future__ </span><span class="s2">import </span><span class="s1">annotations</span>

<span class="s2">import </span><span class="s1">io</span>
<span class="s2">import </span><span class="s1">json</span>
<span class="s2">import </span><span class="s1">os</span>
<span class="s2">from </span><span class="s1">typing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">TYPE_CHECKING</span><span class="s2">,</span>
    <span class="s1">Any</span><span class="s2">,</span>
    <span class="s1">Literal</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">warnings </span><span class="s2">import </span><span class="s1">catch_warnings</span>

<span class="s2">from </span><span class="s1">pandas._config </span><span class="s2">import </span><span class="s1">using_pyarrow_string_dtype</span>

<span class="s2">from </span><span class="s1">pandas._libs </span><span class="s2">import </span><span class="s1">lib</span>
<span class="s2">from </span><span class="s1">pandas.compat._optional </span><span class="s2">import </span><span class="s1">import_optional_dependency</span>
<span class="s2">from </span><span class="s1">pandas.errors </span><span class="s2">import </span><span class="s1">AbstractMethodError</span>
<span class="s2">from </span><span class="s1">pandas.util._decorators </span><span class="s2">import </span><span class="s1">doc</span>
<span class="s2">from </span><span class="s1">pandas.util._exceptions </span><span class="s2">import </span><span class="s1">find_stack_level</span>
<span class="s2">from </span><span class="s1">pandas.util._validators </span><span class="s2">import </span><span class="s1">check_dtype_backend</span>

<span class="s2">import </span><span class="s1">pandas </span><span class="s2">as </span><span class="s1">pd</span>
<span class="s2">from </span><span class="s1">pandas </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">DataFrame</span><span class="s2">,</span>
    <span class="s1">get_option</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">pandas.core.shared_docs </span><span class="s2">import </span><span class="s1">_shared_docs</span>

<span class="s2">from </span><span class="s1">pandas.io._util </span><span class="s2">import </span><span class="s1">arrow_string_types_mapper</span>
<span class="s2">from </span><span class="s1">pandas.io.common </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">IOHandles</span><span class="s2">,</span>
    <span class="s1">get_handle</span><span class="s2">,</span>
    <span class="s1">is_fsspec_url</span><span class="s2">,</span>
    <span class="s1">is_url</span><span class="s2">,</span>
    <span class="s1">stringify_path</span><span class="s2">,</span>
<span class="s1">)</span>

<span class="s2">if </span><span class="s1">TYPE_CHECKING:</span>
    <span class="s2">from </span><span class="s1">pandas._typing </span><span class="s2">import </span><span class="s1">(</span>
        <span class="s1">DtypeBackend</span><span class="s2">,</span>
        <span class="s1">FilePath</span><span class="s2">,</span>
        <span class="s1">ReadBuffer</span><span class="s2">,</span>
        <span class="s1">StorageOptions</span><span class="s2">,</span>
        <span class="s1">WriteBuffer</span><span class="s2">,</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">get_engine(engine: str) -&gt; BaseImpl:</span>
    <span class="s0">&quot;&quot;&quot;return our implementation&quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">engine == </span><span class="s3">&quot;auto&quot;</span><span class="s1">:</span>
        <span class="s1">engine = get_option(</span><span class="s3">&quot;io.parquet.engine&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">engine == </span><span class="s3">&quot;auto&quot;</span><span class="s1">:</span>
        <span class="s4"># try engines in this order</span>
        <span class="s1">engine_classes = [PyArrowImpl</span><span class="s2">, </span><span class="s1">FastParquetImpl]</span>

        <span class="s1">error_msgs = </span><span class="s3">&quot;&quot;</span>
        <span class="s2">for </span><span class="s1">engine_class </span><span class="s2">in </span><span class="s1">engine_classes:</span>
            <span class="s2">try</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">engine_class()</span>
            <span class="s2">except </span><span class="s1">ImportError </span><span class="s2">as </span><span class="s1">err:</span>
                <span class="s1">error_msgs += </span><span class="s3">&quot;</span><span class="s2">\n </span><span class="s3">- &quot; </span><span class="s1">+ str(err)</span>

        <span class="s2">raise </span><span class="s1">ImportError(</span>
            <span class="s3">&quot;Unable to find a usable engine; &quot;</span>
            <span class="s3">&quot;tried using: 'pyarrow', 'fastparquet'.</span><span class="s2">\n</span><span class="s3">&quot;</span>
            <span class="s3">&quot;A suitable version of &quot;</span>
            <span class="s3">&quot;pyarrow or fastparquet is required for parquet &quot;</span>
            <span class="s3">&quot;support.</span><span class="s2">\n</span><span class="s3">&quot;</span>
            <span class="s3">&quot;Trying to import the above resulted in these errors:&quot;</span>
            <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">error_msgs</span><span class="s2">}</span><span class="s3">&quot;</span>
        <span class="s1">)</span>

    <span class="s2">if </span><span class="s1">engine == </span><span class="s3">&quot;pyarrow&quot;</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">PyArrowImpl()</span>
    <span class="s2">elif </span><span class="s1">engine == </span><span class="s3">&quot;fastparquet&quot;</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">FastParquetImpl()</span>

    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;engine must be one of 'pyarrow', 'fastparquet'&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">_get_path_or_handle(</span>
    <span class="s1">path: FilePath | ReadBuffer[bytes] | WriteBuffer[bytes]</span><span class="s2">,</span>
    <span class="s1">fs: Any</span><span class="s2">,</span>
    <span class="s1">storage_options: StorageOptions | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
    <span class="s1">mode: str = </span><span class="s3">&quot;rb&quot;</span><span class="s2">,</span>
    <span class="s1">is_dir: bool = </span><span class="s2">False,</span>
<span class="s1">) -&gt; tuple[</span>
    <span class="s1">FilePath | ReadBuffer[bytes] | WriteBuffer[bytes]</span><span class="s2">, </span><span class="s1">IOHandles[bytes] | </span><span class="s2">None, </span><span class="s1">Any</span>
<span class="s1">]:</span>
    <span class="s0">&quot;&quot;&quot;File handling for PyArrow.&quot;&quot;&quot;</span>
    <span class="s1">path_or_handle = stringify_path(path)</span>
    <span class="s2">if </span><span class="s1">fs </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">pa_fs = import_optional_dependency(</span><span class="s3">&quot;pyarrow.fs&quot;</span><span class="s2">, </span><span class="s1">errors=</span><span class="s3">&quot;ignore&quot;</span><span class="s1">)</span>
        <span class="s1">fsspec = import_optional_dependency(</span><span class="s3">&quot;fsspec&quot;</span><span class="s2">, </span><span class="s1">errors=</span><span class="s3">&quot;ignore&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">pa_fs </span><span class="s2">is not None and </span><span class="s1">isinstance(fs</span><span class="s2">, </span><span class="s1">pa_fs.FileSystem):</span>
            <span class="s2">if </span><span class="s1">storage_options:</span>
                <span class="s2">raise </span><span class="s1">NotImplementedError(</span>
                    <span class="s3">&quot;storage_options not supported with a pyarrow FileSystem.&quot;</span>
                <span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">fsspec </span><span class="s2">is not None and </span><span class="s1">isinstance(fs</span><span class="s2">, </span><span class="s1">fsspec.spec.AbstractFileSystem):</span>
            <span class="s2">pass</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s3">f&quot;filesystem must be a pyarrow or fsspec FileSystem, &quot;</span>
                <span class="s3">f&quot;not a </span><span class="s2">{</span><span class="s1">type(fs).__name__</span><span class="s2">}</span><span class="s3">&quot;</span>
            <span class="s1">)</span>
    <span class="s2">if </span><span class="s1">is_fsspec_url(path_or_handle) </span><span class="s2">and </span><span class="s1">fs </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s2">if </span><span class="s1">storage_options </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">pa = import_optional_dependency(</span><span class="s3">&quot;pyarrow&quot;</span><span class="s1">)</span>
            <span class="s1">pa_fs = import_optional_dependency(</span><span class="s3">&quot;pyarrow.fs&quot;</span><span class="s1">)</span>

            <span class="s2">try</span><span class="s1">:</span>
                <span class="s1">fs</span><span class="s2">, </span><span class="s1">path_or_handle = pa_fs.FileSystem.from_uri(path)</span>
            <span class="s2">except </span><span class="s1">(TypeError</span><span class="s2">, </span><span class="s1">pa.ArrowInvalid):</span>
                <span class="s2">pass</span>
        <span class="s2">if </span><span class="s1">fs </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">fsspec = import_optional_dependency(</span><span class="s3">&quot;fsspec&quot;</span><span class="s1">)</span>
            <span class="s1">fs</span><span class="s2">, </span><span class="s1">path_or_handle = fsspec.core.url_to_fs(</span>
                <span class="s1">path_or_handle</span><span class="s2">, </span><span class="s1">**(storage_options </span><span class="s2">or </span><span class="s1">{})</span>
            <span class="s1">)</span>
    <span class="s2">elif </span><span class="s1">storage_options </span><span class="s2">and </span><span class="s1">(</span><span class="s2">not </span><span class="s1">is_url(path_or_handle) </span><span class="s2">or </span><span class="s1">mode != </span><span class="s3">&quot;rb&quot;</span><span class="s1">):</span>
        <span class="s4"># can't write to a remote url</span>
        <span class="s4"># without making use of fsspec at the moment</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;storage_options passed with buffer, or non-supported URL&quot;</span><span class="s1">)</span>

    <span class="s1">handles = </span><span class="s2">None</span>
    <span class="s2">if </span><span class="s1">(</span>
        <span class="s2">not </span><span class="s1">fs</span>
        <span class="s2">and not </span><span class="s1">is_dir</span>
        <span class="s2">and </span><span class="s1">isinstance(path_or_handle</span><span class="s2">, </span><span class="s1">str)</span>
        <span class="s2">and not </span><span class="s1">os.path.isdir(path_or_handle)</span>
    <span class="s1">):</span>
        <span class="s4"># use get_handle only when we are very certain that it is not a directory</span>
        <span class="s4"># fsspec resources can also point to directories</span>
        <span class="s4"># this branch is used for example when reading from non-fsspec URLs</span>
        <span class="s1">handles = get_handle(</span>
            <span class="s1">path_or_handle</span><span class="s2">, </span><span class="s1">mode</span><span class="s2">, </span><span class="s1">is_text=</span><span class="s2">False, </span><span class="s1">storage_options=storage_options</span>
        <span class="s1">)</span>
        <span class="s1">fs = </span><span class="s2">None</span>
        <span class="s1">path_or_handle = handles.handle</span>
    <span class="s2">return </span><span class="s1">path_or_handle</span><span class="s2">, </span><span class="s1">handles</span><span class="s2">, </span><span class="s1">fs</span>


<span class="s2">class </span><span class="s1">BaseImpl:</span>
    <span class="s1">@staticmethod</span>
    <span class="s2">def </span><span class="s1">validate_dataframe(df: DataFrame) -&gt; </span><span class="s2">None</span><span class="s1">:</span>
        <span class="s2">if not </span><span class="s1">isinstance(df</span><span class="s2">, </span><span class="s1">DataFrame):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;to_parquet only supports IO with DataFrames&quot;</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">write(self</span><span class="s2">, </span><span class="s1">df: DataFrame</span><span class="s2">, </span><span class="s1">path</span><span class="s2">, </span><span class="s1">compression</span><span class="s2">, </span><span class="s1">**kwargs):</span>
        <span class="s2">raise </span><span class="s1">AbstractMethodError(self)</span>

    <span class="s2">def </span><span class="s1">read(self</span><span class="s2">, </span><span class="s1">path</span><span class="s2">, </span><span class="s1">columns=</span><span class="s2">None, </span><span class="s1">**kwargs) -&gt; DataFrame:</span>
        <span class="s2">raise </span><span class="s1">AbstractMethodError(self)</span>


<span class="s2">class </span><span class="s1">PyArrowImpl(BaseImpl):</span>
    <span class="s2">def </span><span class="s1">__init__(self) -&gt; </span><span class="s2">None</span><span class="s1">:</span>
        <span class="s1">import_optional_dependency(</span>
            <span class="s3">&quot;pyarrow&quot;</span><span class="s2">, </span><span class="s1">extra=</span><span class="s3">&quot;pyarrow is required for parquet support.&quot;</span>
        <span class="s1">)</span>
        <span class="s2">import </span><span class="s1">pyarrow.parquet</span>

        <span class="s4"># import utils to register the pyarrow extension types</span>
        <span class="s2">import </span><span class="s1">pandas.core.arrays.arrow.extension_types  </span><span class="s4"># pyright: ignore[reportUnusedImport] # noqa: F401,E501</span>

        <span class="s1">self.api = pyarrow</span>

    <span class="s2">def </span><span class="s1">write(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">df: DataFrame</span><span class="s2">,</span>
        <span class="s1">path: FilePath | WriteBuffer[bytes]</span><span class="s2">,</span>
        <span class="s1">compression: str | </span><span class="s2">None </span><span class="s1">= </span><span class="s3">&quot;snappy&quot;</span><span class="s2">,</span>
        <span class="s1">index: bool | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
        <span class="s1">storage_options: StorageOptions | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
        <span class="s1">partition_cols: list[str] | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
        <span class="s1">filesystem=</span><span class="s2">None,</span>
        <span class="s1">**kwargs</span><span class="s2">,</span>
    <span class="s1">) -&gt; </span><span class="s2">None</span><span class="s1">:</span>
        <span class="s1">self.validate_dataframe(df)</span>

        <span class="s1">from_pandas_kwargs: dict[str</span><span class="s2">, </span><span class="s1">Any] = {</span><span class="s3">&quot;schema&quot;</span><span class="s1">: kwargs.pop(</span><span class="s3">&quot;schema&quot;</span><span class="s2">, None</span><span class="s1">)}</span>
        <span class="s2">if </span><span class="s1">index </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">from_pandas_kwargs[</span><span class="s3">&quot;preserve_index&quot;</span><span class="s1">] = index</span>

        <span class="s1">table = self.api.Table.from_pandas(df</span><span class="s2">, </span><span class="s1">**from_pandas_kwargs)</span>

        <span class="s2">if </span><span class="s1">df.attrs:</span>
            <span class="s1">df_metadata = {</span><span class="s3">&quot;PANDAS_ATTRS&quot;</span><span class="s1">: json.dumps(df.attrs)}</span>
            <span class="s1">existing_metadata = table.schema.metadata</span>
            <span class="s1">merged_metadata = {**existing_metadata</span><span class="s2">, </span><span class="s1">**df_metadata}</span>
            <span class="s1">table = table.replace_schema_metadata(merged_metadata)</span>

        <span class="s1">path_or_handle</span><span class="s2">, </span><span class="s1">handles</span><span class="s2">, </span><span class="s1">filesystem = _get_path_or_handle(</span>
            <span class="s1">path</span><span class="s2">,</span>
            <span class="s1">filesystem</span><span class="s2">,</span>
            <span class="s1">storage_options=storage_options</span><span class="s2">,</span>
            <span class="s1">mode=</span><span class="s3">&quot;wb&quot;</span><span class="s2">,</span>
            <span class="s1">is_dir=partition_cols </span><span class="s2">is not None,</span>
        <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">(</span>
            <span class="s1">isinstance(path_or_handle</span><span class="s2">, </span><span class="s1">io.BufferedWriter)</span>
            <span class="s2">and </span><span class="s1">hasattr(path_or_handle</span><span class="s2">, </span><span class="s3">&quot;name&quot;</span><span class="s1">)</span>
            <span class="s2">and </span><span class="s1">isinstance(path_or_handle.name</span><span class="s2">, </span><span class="s1">(str</span><span class="s2">, </span><span class="s1">bytes))</span>
        <span class="s1">):</span>
            <span class="s1">path_or_handle = path_or_handle.name</span>
            <span class="s2">if </span><span class="s1">isinstance(path_or_handle</span><span class="s2">, </span><span class="s1">bytes):</span>
                <span class="s1">path_or_handle = path_or_handle.decode()</span>

        <span class="s2">try</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">partition_cols </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s4"># writes to multiple files under the given path</span>
                <span class="s1">self.api.parquet.write_to_dataset(</span>
                    <span class="s1">table</span><span class="s2">,</span>
                    <span class="s1">path_or_handle</span><span class="s2">,</span>
                    <span class="s1">compression=compression</span><span class="s2">,</span>
                    <span class="s1">partition_cols=partition_cols</span><span class="s2">,</span>
                    <span class="s1">filesystem=filesystem</span><span class="s2">,</span>
                    <span class="s1">**kwargs</span><span class="s2">,</span>
                <span class="s1">)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s4"># write to single output file</span>
                <span class="s1">self.api.parquet.write_table(</span>
                    <span class="s1">table</span><span class="s2">,</span>
                    <span class="s1">path_or_handle</span><span class="s2">,</span>
                    <span class="s1">compression=compression</span><span class="s2">,</span>
                    <span class="s1">filesystem=filesystem</span><span class="s2">,</span>
                    <span class="s1">**kwargs</span><span class="s2">,</span>
                <span class="s1">)</span>
        <span class="s2">finally</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">handles </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">handles.close()</span>

    <span class="s2">def </span><span class="s1">read(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">path</span><span class="s2">,</span>
        <span class="s1">columns=</span><span class="s2">None,</span>
        <span class="s1">filters=</span><span class="s2">None,</span>
        <span class="s1">use_nullable_dtypes: bool = </span><span class="s2">False,</span>
        <span class="s1">dtype_backend: DtypeBackend | lib.NoDefault = lib.no_default</span><span class="s2">,</span>
        <span class="s1">storage_options: StorageOptions | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
        <span class="s1">filesystem=</span><span class="s2">None,</span>
        <span class="s1">**kwargs</span><span class="s2">,</span>
    <span class="s1">) -&gt; DataFrame:</span>
        <span class="s1">kwargs[</span><span class="s3">&quot;use_pandas_metadata&quot;</span><span class="s1">] = </span><span class="s2">True</span>

        <span class="s1">to_pandas_kwargs = {}</span>
        <span class="s2">if </span><span class="s1">dtype_backend == </span><span class="s3">&quot;numpy_nullable&quot;</span><span class="s1">:</span>
            <span class="s2">from </span><span class="s1">pandas.io._util </span><span class="s2">import </span><span class="s1">_arrow_dtype_mapping</span>

            <span class="s1">mapping = _arrow_dtype_mapping()</span>
            <span class="s1">to_pandas_kwargs[</span><span class="s3">&quot;types_mapper&quot;</span><span class="s1">] = mapping.get</span>
        <span class="s2">elif </span><span class="s1">dtype_backend == </span><span class="s3">&quot;pyarrow&quot;</span><span class="s1">:</span>
            <span class="s1">to_pandas_kwargs[</span><span class="s3">&quot;types_mapper&quot;</span><span class="s1">] = pd.ArrowDtype  </span><span class="s4"># type: ignore[assignment]  # noqa: E501</span>
        <span class="s2">elif </span><span class="s1">using_pyarrow_string_dtype():</span>
            <span class="s1">to_pandas_kwargs[</span><span class="s3">&quot;types_mapper&quot;</span><span class="s1">] = arrow_string_types_mapper()</span>

        <span class="s1">manager = get_option(</span><span class="s3">&quot;mode.data_manager&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">manager == </span><span class="s3">&quot;array&quot;</span><span class="s1">:</span>
            <span class="s1">to_pandas_kwargs[</span><span class="s3">&quot;split_blocks&quot;</span><span class="s1">] = </span><span class="s2">True  </span><span class="s4"># type: ignore[assignment]</span>

        <span class="s1">path_or_handle</span><span class="s2">, </span><span class="s1">handles</span><span class="s2">, </span><span class="s1">filesystem = _get_path_or_handle(</span>
            <span class="s1">path</span><span class="s2">,</span>
            <span class="s1">filesystem</span><span class="s2">,</span>
            <span class="s1">storage_options=storage_options</span><span class="s2">,</span>
            <span class="s1">mode=</span><span class="s3">&quot;rb&quot;</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s2">try</span><span class="s1">:</span>
            <span class="s1">pa_table = self.api.parquet.read_table(</span>
                <span class="s1">path_or_handle</span><span class="s2">,</span>
                <span class="s1">columns=columns</span><span class="s2">,</span>
                <span class="s1">filesystem=filesystem</span><span class="s2">,</span>
                <span class="s1">filters=filters</span><span class="s2">,</span>
                <span class="s1">**kwargs</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">result = pa_table.to_pandas(**to_pandas_kwargs)</span>

            <span class="s2">if </span><span class="s1">manager == </span><span class="s3">&quot;array&quot;</span><span class="s1">:</span>
                <span class="s1">result = result._as_manager(</span><span class="s3">&quot;array&quot;</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>

            <span class="s2">if </span><span class="s1">pa_table.schema.metadata:</span>
                <span class="s2">if </span><span class="s5">b&quot;PANDAS_ATTRS&quot; </span><span class="s2">in </span><span class="s1">pa_table.schema.metadata:</span>
                    <span class="s1">df_metadata = pa_table.schema.metadata[</span><span class="s5">b&quot;PANDAS_ATTRS&quot;</span><span class="s1">]</span>
                    <span class="s1">result.attrs = json.loads(df_metadata)</span>
            <span class="s2">return </span><span class="s1">result</span>
        <span class="s2">finally</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">handles </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">handles.close()</span>


<span class="s2">class </span><span class="s1">FastParquetImpl(BaseImpl):</span>
    <span class="s2">def </span><span class="s1">__init__(self) -&gt; </span><span class="s2">None</span><span class="s1">:</span>
        <span class="s4"># since pandas is a dependency of fastparquet</span>
        <span class="s4"># we need to import on first use</span>
        <span class="s1">fastparquet = import_optional_dependency(</span>
            <span class="s3">&quot;fastparquet&quot;</span><span class="s2">, </span><span class="s1">extra=</span><span class="s3">&quot;fastparquet is required for parquet support.&quot;</span>
        <span class="s1">)</span>
        <span class="s1">self.api = fastparquet</span>

    <span class="s2">def </span><span class="s1">write(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">df: DataFrame</span><span class="s2">,</span>
        <span class="s1">path</span><span class="s2">,</span>
        <span class="s1">compression: Literal[</span><span class="s3">&quot;snappy&quot;</span><span class="s2">, </span><span class="s3">&quot;gzip&quot;</span><span class="s2">, </span><span class="s3">&quot;brotli&quot;</span><span class="s1">] | </span><span class="s2">None </span><span class="s1">= </span><span class="s3">&quot;snappy&quot;</span><span class="s2">,</span>
        <span class="s1">index=</span><span class="s2">None,</span>
        <span class="s1">partition_cols=</span><span class="s2">None,</span>
        <span class="s1">storage_options: StorageOptions | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
        <span class="s1">filesystem=</span><span class="s2">None,</span>
        <span class="s1">**kwargs</span><span class="s2">,</span>
    <span class="s1">) -&gt; </span><span class="s2">None</span><span class="s1">:</span>
        <span class="s1">self.validate_dataframe(df)</span>

        <span class="s2">if </span><span class="s3">&quot;partition_on&quot; </span><span class="s2">in </span><span class="s1">kwargs </span><span class="s2">and </span><span class="s1">partition_cols </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s3">&quot;Cannot use both partition_on and &quot;</span>
                <span class="s3">&quot;partition_cols. Use partition_cols for partitioning data&quot;</span>
            <span class="s1">)</span>
        <span class="s2">if </span><span class="s3">&quot;partition_on&quot; </span><span class="s2">in </span><span class="s1">kwargs:</span>
            <span class="s1">partition_cols = kwargs.pop(</span><span class="s3">&quot;partition_on&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">partition_cols </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">kwargs[</span><span class="s3">&quot;file_scheme&quot;</span><span class="s1">] = </span><span class="s3">&quot;hive&quot;</span>

        <span class="s2">if </span><span class="s1">filesystem </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">NotImplementedError(</span>
                <span class="s3">&quot;filesystem is not implemented for the fastparquet engine.&quot;</span>
            <span class="s1">)</span>

        <span class="s4"># cannot use get_handle as write() does not accept file buffers</span>
        <span class="s1">path = stringify_path(path)</span>
        <span class="s2">if </span><span class="s1">is_fsspec_url(path):</span>
            <span class="s1">fsspec = import_optional_dependency(</span><span class="s3">&quot;fsspec&quot;</span><span class="s1">)</span>

            <span class="s4"># if filesystem is provided by fsspec, file must be opened in 'wb' mode.</span>
            <span class="s1">kwargs[</span><span class="s3">&quot;open_with&quot;</span><span class="s1">] = </span><span class="s2">lambda </span><span class="s1">path</span><span class="s2">, </span><span class="s1">_: fsspec.open(</span>
                <span class="s1">path</span><span class="s2">, </span><span class="s3">&quot;wb&quot;</span><span class="s2">, </span><span class="s1">**(storage_options </span><span class="s2">or </span><span class="s1">{})</span>
            <span class="s1">).open()</span>
        <span class="s2">elif </span><span class="s1">storage_options:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s3">&quot;storage_options passed with file object or non-fsspec file path&quot;</span>
            <span class="s1">)</span>

        <span class="s2">with </span><span class="s1">catch_warnings(record=</span><span class="s2">True</span><span class="s1">):</span>
            <span class="s1">self.api.write(</span>
                <span class="s1">path</span><span class="s2">,</span>
                <span class="s1">df</span><span class="s2">,</span>
                <span class="s1">compression=compression</span><span class="s2">,</span>
                <span class="s1">write_index=index</span><span class="s2">,</span>
                <span class="s1">partition_on=partition_cols</span><span class="s2">,</span>
                <span class="s1">**kwargs</span><span class="s2">,</span>
            <span class="s1">)</span>

    <span class="s2">def </span><span class="s1">read(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">path</span><span class="s2">,</span>
        <span class="s1">columns=</span><span class="s2">None,</span>
        <span class="s1">filters=</span><span class="s2">None,</span>
        <span class="s1">storage_options: StorageOptions | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
        <span class="s1">filesystem=</span><span class="s2">None,</span>
        <span class="s1">**kwargs</span><span class="s2">,</span>
    <span class="s1">) -&gt; DataFrame:</span>
        <span class="s1">parquet_kwargs: dict[str</span><span class="s2">, </span><span class="s1">Any] = {}</span>
        <span class="s1">use_nullable_dtypes = kwargs.pop(</span><span class="s3">&quot;use_nullable_dtypes&quot;</span><span class="s2">, False</span><span class="s1">)</span>
        <span class="s1">dtype_backend = kwargs.pop(</span><span class="s3">&quot;dtype_backend&quot;</span><span class="s2">, </span><span class="s1">lib.no_default)</span>
        <span class="s4"># We are disabling nullable dtypes for fastparquet pending discussion</span>
        <span class="s1">parquet_kwargs[</span><span class="s3">&quot;pandas_nulls&quot;</span><span class="s1">] = </span><span class="s2">False</span>
        <span class="s2">if </span><span class="s1">use_nullable_dtypes:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s3">&quot;The 'use_nullable_dtypes' argument is not supported for the &quot;</span>
                <span class="s3">&quot;fastparquet engine&quot;</span>
            <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">dtype_backend </span><span class="s2">is not </span><span class="s1">lib.no_default:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s3">&quot;The 'dtype_backend' argument is not supported for the &quot;</span>
                <span class="s3">&quot;fastparquet engine&quot;</span>
            <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">filesystem </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">NotImplementedError(</span>
                <span class="s3">&quot;filesystem is not implemented for the fastparquet engine.&quot;</span>
            <span class="s1">)</span>
        <span class="s1">path = stringify_path(path)</span>
        <span class="s1">handles = </span><span class="s2">None</span>
        <span class="s2">if </span><span class="s1">is_fsspec_url(path):</span>
            <span class="s1">fsspec = import_optional_dependency(</span><span class="s3">&quot;fsspec&quot;</span><span class="s1">)</span>

            <span class="s1">parquet_kwargs[</span><span class="s3">&quot;fs&quot;</span><span class="s1">] = fsspec.open(path</span><span class="s2">, </span><span class="s3">&quot;rb&quot;</span><span class="s2">, </span><span class="s1">**(storage_options </span><span class="s2">or </span><span class="s1">{})).fs</span>
        <span class="s2">elif </span><span class="s1">isinstance(path</span><span class="s2">, </span><span class="s1">str) </span><span class="s2">and not </span><span class="s1">os.path.isdir(path):</span>
            <span class="s4"># use get_handle only when we are very certain that it is not a directory</span>
            <span class="s4"># fsspec resources can also point to directories</span>
            <span class="s4"># this branch is used for example when reading from non-fsspec URLs</span>
            <span class="s1">handles = get_handle(</span>
                <span class="s1">path</span><span class="s2">, </span><span class="s3">&quot;rb&quot;</span><span class="s2">, </span><span class="s1">is_text=</span><span class="s2">False, </span><span class="s1">storage_options=storage_options</span>
            <span class="s1">)</span>
            <span class="s1">path = handles.handle</span>

        <span class="s2">try</span><span class="s1">:</span>
            <span class="s1">parquet_file = self.api.ParquetFile(path</span><span class="s2">, </span><span class="s1">**parquet_kwargs)</span>
            <span class="s2">return </span><span class="s1">parquet_file.to_pandas(columns=columns</span><span class="s2">, </span><span class="s1">filters=filters</span><span class="s2">, </span><span class="s1">**kwargs)</span>
        <span class="s2">finally</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">handles </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">handles.close()</span>


<span class="s1">@doc(storage_options=_shared_docs[</span><span class="s3">&quot;storage_options&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">to_parquet(</span>
    <span class="s1">df: DataFrame</span><span class="s2">,</span>
    <span class="s1">path: FilePath | WriteBuffer[bytes] | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
    <span class="s1">engine: str = </span><span class="s3">&quot;auto&quot;</span><span class="s2">,</span>
    <span class="s1">compression: str | </span><span class="s2">None </span><span class="s1">= </span><span class="s3">&quot;snappy&quot;</span><span class="s2">,</span>
    <span class="s1">index: bool | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
    <span class="s1">storage_options: StorageOptions | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
    <span class="s1">partition_cols: list[str] | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
    <span class="s1">filesystem: Any = </span><span class="s2">None,</span>
    <span class="s1">**kwargs</span><span class="s2">,</span>
<span class="s1">) -&gt; bytes | </span><span class="s2">None</span><span class="s1">:</span>
    <span class="s0">&quot;&quot;&quot; 
    Write a DataFrame to the parquet format. 
 
    Parameters 
    ---------- 
    df : DataFrame 
    path : str, path object, file-like object, or None, default None 
        String, path object (implementing ``os.PathLike[str]``), or file-like 
        object implementing a binary ``write()`` function. If None, the result is 
        returned as bytes. If a string, it will be used as Root Directory path 
        when writing a partitioned dataset. The engine fastparquet does not 
        accept file-like objects. 
 
        .. versionchanged:: 1.2.0 
 
    engine : {{'auto', 'pyarrow', 'fastparquet'}}, default 'auto' 
        Parquet library to use. If 'auto', then the option 
        ``io.parquet.engine`` is used. The default ``io.parquet.engine`` 
        behavior is to try 'pyarrow', falling back to 'fastparquet' if 
        'pyarrow' is unavailable. 
 
        When using the ``'pyarrow'`` engine and no storage options are provided 
        and a filesystem is implemented by both ``pyarrow.fs`` and ``fsspec`` 
        (e.g. &quot;s3://&quot;), then the ``pyarrow.fs`` filesystem is attempted first. 
        Use the filesystem keyword with an instantiated fsspec filesystem 
        if you wish to use its implementation. 
    compression : {{'snappy', 'gzip', 'brotli', 'lz4', 'zstd', None}}, 
        default 'snappy'. Name of the compression to use. Use ``None`` 
        for no compression. 
    index : bool, default None 
        If ``True``, include the dataframe's index(es) in the file output. If 
        ``False``, they will not be written to the file. 
        If ``None``, similar to ``True`` the dataframe's index(es) 
        will be saved. However, instead of being saved as values, 
        the RangeIndex will be stored as a range in the metadata so it 
        doesn't require much space and is faster. Other indexes will 
        be included as columns in the file output. 
    partition_cols : str or list, optional, default None 
        Column names by which to partition the dataset. 
        Columns are partitioned in the order they are given. 
        Must be None if path is not a string. 
    {storage_options} 
 
        .. versionadded:: 1.2.0 
 
    filesystem : fsspec or pyarrow filesystem, default None 
        Filesystem object to use when reading the parquet file. Only implemented 
        for ``engine=&quot;pyarrow&quot;``. 
 
        .. versionadded:: 2.1.0 
 
    kwargs 
        Additional keyword arguments passed to the engine 
 
    Returns 
    ------- 
    bytes if no path argument is provided else None 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">isinstance(partition_cols</span><span class="s2">, </span><span class="s1">str):</span>
        <span class="s1">partition_cols = [partition_cols]</span>
    <span class="s1">impl = get_engine(engine)</span>

    <span class="s1">path_or_buf: FilePath | WriteBuffer[bytes] = io.BytesIO() </span><span class="s2">if </span><span class="s1">path </span><span class="s2">is None else </span><span class="s1">path</span>

    <span class="s1">impl.write(</span>
        <span class="s1">df</span><span class="s2">,</span>
        <span class="s1">path_or_buf</span><span class="s2">,</span>
        <span class="s1">compression=compression</span><span class="s2">,</span>
        <span class="s1">index=index</span><span class="s2">,</span>
        <span class="s1">partition_cols=partition_cols</span><span class="s2">,</span>
        <span class="s1">storage_options=storage_options</span><span class="s2">,</span>
        <span class="s1">filesystem=filesystem</span><span class="s2">,</span>
        <span class="s1">**kwargs</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s2">if </span><span class="s1">path </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s1">isinstance(path_or_buf</span><span class="s2">, </span><span class="s1">io.BytesIO)</span>
        <span class="s2">return </span><span class="s1">path_or_buf.getvalue()</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">return None</span>


<span class="s1">@doc(storage_options=_shared_docs[</span><span class="s3">&quot;storage_options&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">read_parquet(</span>
    <span class="s1">path: FilePath | ReadBuffer[bytes]</span><span class="s2">,</span>
    <span class="s1">engine: str = </span><span class="s3">&quot;auto&quot;</span><span class="s2">,</span>
    <span class="s1">columns: list[str] | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
    <span class="s1">storage_options: StorageOptions | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
    <span class="s1">use_nullable_dtypes: bool | lib.NoDefault = lib.no_default</span><span class="s2">,</span>
    <span class="s1">dtype_backend: DtypeBackend | lib.NoDefault = lib.no_default</span><span class="s2">,</span>
    <span class="s1">filesystem: Any = </span><span class="s2">None,</span>
    <span class="s1">filters: list[tuple] | list[list[tuple]] | </span><span class="s2">None </span><span class="s1">= </span><span class="s2">None,</span>
    <span class="s1">**kwargs</span><span class="s2">,</span>
<span class="s1">) -&gt; DataFrame:</span>
    <span class="s0">&quot;&quot;&quot; 
    Load a parquet object from the file path, returning a DataFrame. 
 
    Parameters 
    ---------- 
    path : str, path object or file-like object 
        String, path object (implementing ``os.PathLike[str]``), or file-like 
        object implementing a binary ``read()`` function. 
        The string could be a URL. Valid URL schemes include http, ftp, s3, 
        gs, and file. For file URLs, a host is expected. A local file could be: 
        ``file://localhost/path/to/table.parquet``. 
        A file URL can also be a path to a directory that contains multiple 
        partitioned parquet files. Both pyarrow and fastparquet support 
        paths to directories as well as file URLs. A directory path could be: 
        ``file://localhost/path/to/tables`` or ``s3://bucket/partition_dir``. 
    engine : {{'auto', 'pyarrow', 'fastparquet'}}, default 'auto' 
        Parquet library to use. If 'auto', then the option 
        ``io.parquet.engine`` is used. The default ``io.parquet.engine`` 
        behavior is to try 'pyarrow', falling back to 'fastparquet' if 
        'pyarrow' is unavailable. 
 
        When using the ``'pyarrow'`` engine and no storage options are provided 
        and a filesystem is implemented by both ``pyarrow.fs`` and ``fsspec`` 
        (e.g. &quot;s3://&quot;), then the ``pyarrow.fs`` filesystem is attempted first. 
        Use the filesystem keyword with an instantiated fsspec filesystem 
        if you wish to use its implementation. 
    columns : list, default=None 
        If not None, only these columns will be read from the file. 
    {storage_options} 
 
        .. versionadded:: 1.3.0 
 
    use_nullable_dtypes : bool, default False 
        If True, use dtypes that use ``pd.NA`` as missing value indicator 
        for the resulting DataFrame. (only applicable for the ``pyarrow`` 
        engine) 
        As new dtypes are added that support ``pd.NA`` in the future, the 
        output with this option will change to use those dtypes. 
        Note: this is an experimental option, and behaviour (e.g. additional 
        support dtypes) may change without notice. 
 
        .. deprecated:: 2.0 
 
    dtype_backend : {{'numpy_nullable', 'pyarrow'}}, default 'numpy_nullable' 
        Back-end data type applied to the resultant :class:`DataFrame` 
        (still experimental). Behaviour is as follows: 
 
        * ``&quot;numpy_nullable&quot;``: returns nullable-dtype-backed :class:`DataFrame` 
          (default). 
        * ``&quot;pyarrow&quot;``: returns pyarrow-backed nullable :class:`ArrowDtype` 
          DataFrame. 
 
        .. versionadded:: 2.0 
 
    filesystem : fsspec or pyarrow filesystem, default None 
        Filesystem object to use when reading the parquet file. Only implemented 
        for ``engine=&quot;pyarrow&quot;``. 
 
        .. versionadded:: 2.1.0 
 
    filters : List[Tuple] or List[List[Tuple]], default None 
        To filter out data. 
        Filter syntax: [[(column, op, val), ...],...] 
        where op is [==, =, &gt;, &gt;=, &lt;, &lt;=, !=, in, not in] 
        The innermost tuples are transposed into a set of filters applied 
        through an `AND` operation. 
        The outer list combines these sets of filters through an `OR` 
        operation. 
        A single list of tuples can also be used, meaning that no `OR` 
        operation between set of filters is to be conducted. 
 
        Using this argument will NOT result in row-wise filtering of the final 
        partitions unless ``engine=&quot;pyarrow&quot;`` is also specified.  For 
        other engines, filtering is only performed at the partition level, that is, 
        to prevent the loading of some row-groups and/or files. 
 
        .. versionadded:: 2.1.0 
 
    **kwargs 
        Any additional kwargs are passed to the engine. 
 
    Returns 
    ------- 
    DataFrame 
 
    See Also 
    -------- 
    DataFrame.to_parquet : Create a parquet object that serializes a DataFrame. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; original_df = pd.DataFrame( 
    ...     {{&quot;foo&quot;: range(5), &quot;bar&quot;: range(5, 10)}} 
    ...    ) 
    &gt;&gt;&gt; original_df 
       foo  bar 
    0    0    5 
    1    1    6 
    2    2    7 
    3    3    8 
    4    4    9 
    &gt;&gt;&gt; df_parquet_bytes = original_df.to_parquet() 
    &gt;&gt;&gt; from io import BytesIO 
    &gt;&gt;&gt; restored_df = pd.read_parquet(BytesIO(df_parquet_bytes)) 
    &gt;&gt;&gt; restored_df 
       foo  bar 
    0    0    5 
    1    1    6 
    2    2    7 
    3    3    8 
    4    4    9 
    &gt;&gt;&gt; restored_df.equals(original_df) 
    True 
    &gt;&gt;&gt; restored_bar = pd.read_parquet(BytesIO(df_parquet_bytes), columns=[&quot;bar&quot;]) 
    &gt;&gt;&gt; restored_bar 
        bar 
    0    5 
    1    6 
    2    7 
    3    8 
    4    9 
    &gt;&gt;&gt; restored_bar.equals(original_df[['bar']]) 
    True 
 
    The function uses `kwargs` that are passed directly to the engine. 
    In the following example, we use the `filters` argument of the pyarrow 
    engine to filter the rows of the DataFrame. 
 
    Since `pyarrow` is the default engine, we can omit the `engine` argument. 
    Note that the `filters` argument is implemented by the `pyarrow` engine, 
    which can benefit from multithreading and also potentially be more 
    economical in terms of memory. 
 
    &gt;&gt;&gt; sel = [(&quot;foo&quot;, &quot;&gt;&quot;, 2)] 
    &gt;&gt;&gt; restored_part = pd.read_parquet(BytesIO(df_parquet_bytes), filters=sel) 
    &gt;&gt;&gt; restored_part 
        foo  bar 
    0    3    8 
    1    4    9 
    &quot;&quot;&quot;</span>

    <span class="s1">impl = get_engine(engine)</span>

    <span class="s2">if </span><span class="s1">use_nullable_dtypes </span><span class="s2">is not </span><span class="s1">lib.no_default:</span>
        <span class="s1">msg = (</span>
            <span class="s3">&quot;The argument 'use_nullable_dtypes' is deprecated and will be removed &quot;</span>
            <span class="s3">&quot;in a future version.&quot;</span>
        <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">use_nullable_dtypes </span><span class="s2">is True</span><span class="s1">:</span>
            <span class="s1">msg += (</span>
                <span class="s3">&quot;Use dtype_backend='numpy_nullable' instead of use_nullable_dtype=True.&quot;</span>
            <span class="s1">)</span>
        <span class="s1">warnings.warn(msg</span><span class="s2">, </span><span class="s1">FutureWarning</span><span class="s2">, </span><span class="s1">stacklevel=find_stack_level())</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">use_nullable_dtypes = </span><span class="s2">False</span>
    <span class="s1">check_dtype_backend(dtype_backend)</span>

    <span class="s2">return </span><span class="s1">impl.read(</span>
        <span class="s1">path</span><span class="s2">,</span>
        <span class="s1">columns=columns</span><span class="s2">,</span>
        <span class="s1">filters=filters</span><span class="s2">,</span>
        <span class="s1">storage_options=storage_options</span><span class="s2">,</span>
        <span class="s1">use_nullable_dtypes=use_nullable_dtypes</span><span class="s2">,</span>
        <span class="s1">dtype_backend=dtype_backend</span><span class="s2">,</span>
        <span class="s1">filesystem=filesystem</span><span class="s2">,</span>
        <span class="s1">**kwargs</span><span class="s2">,</span>
    <span class="s1">)</span>
</pre>
</body>
</html>