<html>
<head>
<title>_logsumexp.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6897bb;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_logsumexp.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">from </span><span class="s1">scipy._lib._util </span><span class="s0">import </span><span class="s1">_asarray_validated</span>

<span class="s1">__all__ = [</span><span class="s2">&quot;logsumexp&quot;</span><span class="s0">, </span><span class="s2">&quot;softmax&quot;</span><span class="s0">, </span><span class="s2">&quot;log_softmax&quot;</span><span class="s1">]</span>


<span class="s0">def </span><span class="s1">logsumexp(a</span><span class="s0">, </span><span class="s1">axis=</span><span class="s0">None, </span><span class="s1">b=</span><span class="s0">None, </span><span class="s1">keepdims=</span><span class="s0">False, </span><span class="s1">return_sign=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s3">&quot;&quot;&quot;Compute the log of the sum of exponentials of input elements. 
 
    Parameters 
    ---------- 
    a : array_like 
        Input array. 
    axis : None or int or tuple of ints, optional 
        Axis or axes over which the sum is taken. By default `axis` is None, 
        and all elements are summed. 
 
        .. versionadded:: 0.11.0 
    b : array-like, optional 
        Scaling factor for exp(`a`) must be of the same shape as `a` or 
        broadcastable to `a`. These values may be negative in order to 
        implement subtraction. 
 
        .. versionadded:: 0.12.0 
    keepdims : bool, optional 
        If this is set to True, the axes which are reduced are left in the 
        result as dimensions with size one. With this option, the result 
        will broadcast correctly against the original array. 
 
        .. versionadded:: 0.15.0 
    return_sign : bool, optional 
        If this is set to True, the result will be a pair containing sign 
        information; if False, results that are negative will be returned 
        as NaN. Default is False (no sign information). 
 
        .. versionadded:: 0.16.0 
 
    Returns 
    ------- 
    res : ndarray 
        The result, ``np.log(np.sum(np.exp(a)))`` calculated in a numerically 
        more stable way. If `b` is given then ``np.log(np.sum(b*np.exp(a)))`` 
        is returned. 
    sgn : ndarray 
        If return_sign is True, this will be an array of floating-point 
        numbers matching res and +1, 0, or -1 depending on the sign 
        of the result. If False, only one result is returned. 
 
    See Also 
    -------- 
    numpy.logaddexp, numpy.logaddexp2 
 
    Notes 
    ----- 
    NumPy has a logaddexp function which is very similar to `logsumexp`, but 
    only handles two arguments. `logaddexp.reduce` is similar to this 
    function, but may be less stable. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.special import logsumexp 
    &gt;&gt;&gt; a = np.arange(10) 
    &gt;&gt;&gt; logsumexp(a) 
    9.4586297444267107 
    &gt;&gt;&gt; np.log(np.sum(np.exp(a))) 
    9.4586297444267107 
 
    With weights 
 
    &gt;&gt;&gt; a = np.arange(10) 
    &gt;&gt;&gt; b = np.arange(10, 0, -1) 
    &gt;&gt;&gt; logsumexp(a, b=b) 
    9.9170178533034665 
    &gt;&gt;&gt; np.log(np.sum(b*np.exp(a))) 
    9.9170178533034647 
 
    Returning a sign flag 
 
    &gt;&gt;&gt; logsumexp([1,2],b=[1,-1],return_sign=True) 
    (1.5413248546129181, -1.0) 
 
    Notice that `logsumexp` does not directly support masked arrays. To use it 
    on a masked array, convert the mask into zero weights: 
 
    &gt;&gt;&gt; a = np.ma.array([np.log(2), 2, np.log(3)], 
    ...                  mask=[False, True, False]) 
    &gt;&gt;&gt; b = (~a.mask).astype(int) 
    &gt;&gt;&gt; logsumexp(a.data, b=b), np.log(5) 
    1.6094379124341005, 1.6094379124341005 
 
    &quot;&quot;&quot;</span>
    <span class="s1">a = _asarray_validated(a</span><span class="s0">, </span><span class="s1">check_finite=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">b </span><span class="s0">is not None</span><span class="s1">:</span>
        <span class="s1">a</span><span class="s0">, </span><span class="s1">b = np.broadcast_arrays(a</span><span class="s0">, </span><span class="s1">b)</span>
        <span class="s0">if </span><span class="s1">np.any(b == </span><span class="s4">0</span><span class="s1">):</span>
            <span class="s1">a = a + </span><span class="s4">0.  </span><span class="s5"># promote to at least float</span>
            <span class="s1">a[b == </span><span class="s4">0</span><span class="s1">] = -np.inf</span>

    <span class="s1">a_max = np.amax(a</span><span class="s0">, </span><span class="s1">axis=axis</span><span class="s0">, </span><span class="s1">keepdims=</span><span class="s0">True</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">a_max.ndim &gt; </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s1">a_max[~np.isfinite(a_max)] = </span><span class="s4">0</span>
    <span class="s0">elif not </span><span class="s1">np.isfinite(a_max):</span>
        <span class="s1">a_max = </span><span class="s4">0</span>

    <span class="s0">if </span><span class="s1">b </span><span class="s0">is not None</span><span class="s1">:</span>
        <span class="s1">b = np.asarray(b)</span>
        <span class="s1">tmp = b * np.exp(a - a_max)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">tmp = np.exp(a - a_max)</span>

    <span class="s5"># suppress warnings about log of zero</span>
    <span class="s0">with </span><span class="s1">np.errstate(divide=</span><span class="s2">'ignore'</span><span class="s1">):</span>
        <span class="s1">s = np.sum(tmp</span><span class="s0">, </span><span class="s1">axis=axis</span><span class="s0">, </span><span class="s1">keepdims=keepdims)</span>
        <span class="s0">if </span><span class="s1">return_sign:</span>
            <span class="s1">sgn = np.sign(s)</span>
            <span class="s1">s *= sgn  </span><span class="s5"># /= makes more sense but we need zero -&gt; zero</span>
        <span class="s1">out = np.log(s)</span>

    <span class="s0">if not </span><span class="s1">keepdims:</span>
        <span class="s1">a_max = np.squeeze(a_max</span><span class="s0">, </span><span class="s1">axis=axis)</span>
    <span class="s1">out += a_max</span>

    <span class="s0">if </span><span class="s1">return_sign:</span>
        <span class="s0">return </span><span class="s1">out</span><span class="s0">, </span><span class="s1">sgn</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">out</span>


<span class="s0">def </span><span class="s1">softmax(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s3">r&quot;&quot;&quot;Compute the softmax function. 
 
    The softmax function transforms each element of a collection by 
    computing the exponential of each element divided by the sum of the 
    exponentials of all the elements. That is, if `x` is a one-dimensional 
    numpy array:: 
 
        softmax(x) = np.exp(x)/sum(np.exp(x)) 
 
    Parameters 
    ---------- 
    x : array_like 
        Input array. 
    axis : int or tuple of ints, optional 
        Axis to compute values along. Default is None and softmax will be 
        computed over the entire array `x`. 
 
    Returns 
    ------- 
    s : ndarray 
        An array the same shape as `x`. The result will sum to 1 along the 
        specified axis. 
 
    Notes 
    ----- 
    The formula for the softmax function :math:`\sigma(x)` for a vector 
    :math:`x = \{x_0, x_1, ..., x_{n-1}\}` is 
 
    .. math:: \sigma(x)_j = \frac{e^{x_j}}{\sum_k e^{x_k}} 
 
    The `softmax` function is the gradient of `logsumexp`. 
 
    The implementation uses shifting to avoid overflow. See [1]_ for more 
    details. 
 
    .. versionadded:: 1.2.0 
 
    References 
    ---------- 
    .. [1] P. Blanchard, D.J. Higham, N.J. Higham, &quot;Accurately computing the 
       log-sum-exp and softmax functions&quot;, IMA Journal of Numerical Analysis, 
       Vol.41(4), :doi:`10.1093/imanum/draa038`. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.special import softmax 
    &gt;&gt;&gt; np.set_printoptions(precision=5) 
 
    &gt;&gt;&gt; x = np.array([[1, 0.5, 0.2, 3], 
    ...               [1,  -1,   7, 3], 
    ...               [2,  12,  13, 3]]) 
    ... 
 
    Compute the softmax transformation over the entire array. 
 
    &gt;&gt;&gt; m = softmax(x) 
    &gt;&gt;&gt; m 
    array([[  4.48309e-06,   2.71913e-06,   2.01438e-06,   3.31258e-05], 
           [  4.48309e-06,   6.06720e-07,   1.80861e-03,   3.31258e-05], 
           [  1.21863e-05,   2.68421e-01,   7.29644e-01,   3.31258e-05]]) 
 
    &gt;&gt;&gt; m.sum() 
    1.0 
 
    Compute the softmax transformation along the first axis (i.e., the 
    columns). 
 
    &gt;&gt;&gt; m = softmax(x, axis=0) 
 
    &gt;&gt;&gt; m 
    array([[  2.11942e-01,   1.01300e-05,   2.75394e-06,   3.33333e-01], 
           [  2.11942e-01,   2.26030e-06,   2.47262e-03,   3.33333e-01], 
           [  5.76117e-01,   9.99988e-01,   9.97525e-01,   3.33333e-01]]) 
 
    &gt;&gt;&gt; m.sum(axis=0) 
    array([ 1.,  1.,  1.,  1.]) 
 
    Compute the softmax transformation along the second axis (i.e., the rows). 
 
    &gt;&gt;&gt; m = softmax(x, axis=1) 
    &gt;&gt;&gt; m 
    array([[  1.05877e-01,   6.42177e-02,   4.75736e-02,   7.82332e-01], 
           [  2.42746e-03,   3.28521e-04,   9.79307e-01,   1.79366e-02], 
           [  1.22094e-05,   2.68929e-01,   7.31025e-01,   3.31885e-05]]) 
 
    &gt;&gt;&gt; m.sum(axis=1) 
    array([ 1.,  1.,  1.]) 
 
    &quot;&quot;&quot;</span>
    <span class="s1">x = _asarray_validated(x</span><span class="s0">, </span><span class="s1">check_finite=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">x_max = np.amax(x</span><span class="s0">, </span><span class="s1">axis=axis</span><span class="s0">, </span><span class="s1">keepdims=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">exp_x_shifted = np.exp(x - x_max)</span>
    <span class="s0">return </span><span class="s1">exp_x_shifted / np.sum(exp_x_shifted</span><span class="s0">, </span><span class="s1">axis=axis</span><span class="s0">, </span><span class="s1">keepdims=</span><span class="s0">True</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">log_softmax(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s3">r&quot;&quot;&quot;Compute the logarithm of the softmax function. 
 
    In principle:: 
 
        log_softmax(x) = log(softmax(x)) 
 
    but using a more accurate implementation. 
 
    Parameters 
    ---------- 
    x : array_like 
        Input array. 
    axis : int or tuple of ints, optional 
        Axis to compute values along. Default is None and softmax will be 
        computed over the entire array `x`. 
 
    Returns 
    ------- 
    s : ndarray or scalar 
        An array with the same shape as `x`. Exponential of the result will 
        sum to 1 along the specified axis. If `x` is a scalar, a scalar is 
        returned. 
 
    Notes 
    ----- 
    `log_softmax` is more accurate than ``np.log(softmax(x))`` with inputs that 
    make `softmax` saturate (see examples below). 
 
    .. versionadded:: 1.5.0 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.special import log_softmax 
    &gt;&gt;&gt; from scipy.special import softmax 
    &gt;&gt;&gt; np.set_printoptions(precision=5) 
 
    &gt;&gt;&gt; x = np.array([1000.0, 1.0]) 
 
    &gt;&gt;&gt; y = log_softmax(x) 
    &gt;&gt;&gt; y 
    array([   0., -999.]) 
 
    &gt;&gt;&gt; with np.errstate(divide='ignore'): 
    ...   y = np.log(softmax(x)) 
    ... 
    &gt;&gt;&gt; y 
    array([  0., -inf]) 
 
    &quot;&quot;&quot;</span>

    <span class="s1">x = _asarray_validated(x</span><span class="s0">, </span><span class="s1">check_finite=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s1">x_max = np.amax(x</span><span class="s0">, </span><span class="s1">axis=axis</span><span class="s0">, </span><span class="s1">keepdims=</span><span class="s0">True</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">x_max.ndim &gt; </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s1">x_max[~np.isfinite(x_max)] = </span><span class="s4">0</span>
    <span class="s0">elif not </span><span class="s1">np.isfinite(x_max):</span>
        <span class="s1">x_max = </span><span class="s4">0</span>

    <span class="s1">tmp = x - x_max</span>
    <span class="s1">exp_tmp = np.exp(tmp)</span>

    <span class="s5"># suppress warnings about log of zero</span>
    <span class="s0">with </span><span class="s1">np.errstate(divide=</span><span class="s2">'ignore'</span><span class="s1">):</span>
        <span class="s1">s = np.sum(exp_tmp</span><span class="s0">, </span><span class="s1">axis=axis</span><span class="s0">, </span><span class="s1">keepdims=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s1">out = np.log(s)</span>

    <span class="s1">out = tmp - out</span>
    <span class="s0">return </span><span class="s1">out</span>
</pre>
</body>
</html>