<html>
<head>
<title>test_vq.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6897bb;}
.s3 { color: #808080;}
.s4 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_vq.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">warnings</span>
<span class="s0">import </span><span class="s1">sys</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">(assert_array_equal</span><span class="s0">, </span><span class="s1">assert_array_almost_equal</span><span class="s0">,</span>
                           <span class="s1">assert_allclose</span><span class="s0">, </span><span class="s1">assert_equal</span><span class="s0">, </span><span class="s1">assert_</span><span class="s0">,</span>
                           <span class="s1">suppress_warnings)</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">pytest </span><span class="s0">import </span><span class="s1">raises </span><span class="s0">as </span><span class="s1">assert_raises</span>

<span class="s0">from </span><span class="s1">scipy.cluster.vq </span><span class="s0">import </span><span class="s1">(kmeans</span><span class="s0">, </span><span class="s1">kmeans2</span><span class="s0">, </span><span class="s1">py_vq</span><span class="s0">, </span><span class="s1">vq</span><span class="s0">, </span><span class="s1">whiten</span><span class="s0">,</span>
                              <span class="s1">ClusterError</span><span class="s0">, </span><span class="s1">_krandinit)</span>
<span class="s0">from </span><span class="s1">scipy.cluster </span><span class="s0">import </span><span class="s1">_vq</span>
<span class="s0">from </span><span class="s1">scipy.sparse._sputils </span><span class="s0">import </span><span class="s1">matrix</span>


<span class="s1">TESTDATA_2D = np.array([</span>
    <span class="s1">-</span><span class="s2">2.2</span><span class="s0">, </span><span class="s2">1.17</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.63</span><span class="s0">, </span><span class="s2">1.69</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.04</span><span class="s0">, </span><span class="s2">4.38</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.09</span><span class="s0">, </span><span class="s2">0.95</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.7</span><span class="s0">, </span><span class="s2">4.79</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.68</span><span class="s0">, </span><span class="s2">0.68</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">2.26</span><span class="s0">, </span><span class="s2">3.34</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.29</span><span class="s0">, </span><span class="s2">2.55</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.72</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.72</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.99</span><span class="s0">, </span><span class="s2">2.34</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.75</span><span class="s0">, </span><span class="s2">3.43</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.45</span><span class="s0">,</span>
    <span class="s2">2.41</span><span class="s0">, </span><span class="s1">-</span><span class="s2">4.26</span><span class="s0">, </span><span class="s2">3.65</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.57</span><span class="s0">, </span><span class="s2">1.87</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.96</span><span class="s0">, </span><span class="s2">4.03</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.01</span><span class="s0">, </span><span class="s2">3.86</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.53</span><span class="s0">, </span><span class="s2">1.28</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">4.0</span><span class="s0">, </span><span class="s2">3.95</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.62</span><span class="s0">, </span><span class="s2">1.25</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.42</span><span class="s0">, </span><span class="s2">3.17</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.17</span><span class="s0">, </span><span class="s2">0.12</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.03</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.27</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.07</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">0.55</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.17</span><span class="s0">, </span><span class="s2">1.34</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.82</span><span class="s0">, </span><span class="s2">3.08</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.44</span><span class="s0">, </span><span class="s2">0.24</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.71</span><span class="s0">, </span><span class="s2">2.48</span><span class="s0">, </span><span class="s1">-</span><span class="s2">5.23</span><span class="s0">, </span><span class="s2">4.29</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">2.08</span><span class="s0">, </span><span class="s2">3.69</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.89</span><span class="s0">, </span><span class="s2">3.62</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.09</span><span class="s0">, </span><span class="s2">0.26</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.92</span><span class="s0">, </span><span class="s2">1.07</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.25</span><span class="s0">, </span><span class="s2">0.88</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.25</span><span class="s0">,</span>
    <span class="s2">2.02</span><span class="s0">, </span><span class="s1">-</span><span class="s2">4.31</span><span class="s0">, </span><span class="s2">3.86</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.03</span><span class="s0">, </span><span class="s2">3.42</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.76</span><span class="s0">, </span><span class="s2">0.3</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.48</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.29</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.42</span><span class="s0">, </span><span class="s2">3.21</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">2.3</span><span class="s0">, </span><span class="s2">1.73</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.84</span><span class="s0">, </span><span class="s2">0.69</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.81</span><span class="s0">, </span><span class="s2">2.48</span><span class="s0">, </span><span class="s1">-</span><span class="s2">5.24</span><span class="s0">, </span><span class="s2">4.52</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.8</span><span class="s0">, </span><span class="s2">1.31</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.67</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">2.34</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.18</span><span class="s0">, </span><span class="s2">2.17</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.17</span><span class="s0">, </span><span class="s2">2.82</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.85</span><span class="s0">, </span><span class="s2">2.25</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.45</span><span class="s0">, </span><span class="s2">1.86</span><span class="s0">, </span><span class="s1">-</span><span class="s2">6.79</span><span class="s0">, </span><span class="s2">3.94</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">2.33</span><span class="s0">, </span><span class="s2">1.89</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.55</span><span class="s0">, </span><span class="s2">2.08</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.36</span><span class="s0">, </span><span class="s2">0.93</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.51</span><span class="s0">, </span><span class="s2">2.74</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.39</span><span class="s0">, </span><span class="s2">3.92</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.33</span><span class="s0">,</span>
    <span class="s2">2.99</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.06</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.9</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.83</span><span class="s0">, </span><span class="s2">3.35</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.59</span><span class="s0">, </span><span class="s2">3.05</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.36</span><span class="s0">, </span><span class="s2">1.85</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.69</span><span class="s0">, </span><span class="s2">1.8</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">1.39</span><span class="s0">, </span><span class="s2">0.66</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.06</span><span class="s0">, </span><span class="s2">0.38</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.47</span><span class="s0">, </span><span class="s2">0.44</span><span class="s0">, </span><span class="s1">-</span><span class="s2">4.68</span><span class="s0">, </span><span class="s2">3.77</span><span class="s0">, </span><span class="s1">-</span><span class="s2">5.58</span><span class="s0">, </span><span class="s2">3.44</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.29</span><span class="s0">,</span>
    <span class="s2">2.24</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.04</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.38</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.85</span><span class="s0">, </span><span class="s2">4.23</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.88</span><span class="s0">, </span><span class="s2">0.73</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.59</span><span class="s0">, </span><span class="s2">1.39</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.34</span><span class="s0">, </span><span class="s2">1.75</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">1.95</span><span class="s0">, </span><span class="s2">1.3</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.45</span><span class="s0">, </span><span class="s2">3.09</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.99</span><span class="s0">, </span><span class="s2">3.41</span><span class="s0">, </span><span class="s1">-</span><span class="s2">5.55</span><span class="s0">, </span><span class="s2">5.21</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.73</span><span class="s0">, </span><span class="s2">2.52</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.17</span><span class="s0">,</span>
    <span class="s2">0.85</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.06</span><span class="s0">, </span><span class="s2">0.49</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.54</span><span class="s0">, </span><span class="s2">2.07</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.03</span><span class="s0">, </span><span class="s2">1.3</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.23</span><span class="s0">, </span><span class="s2">3.09</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.55</span><span class="s0">, </span><span class="s2">1.44</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">0.81</span><span class="s0">, </span><span class="s2">1.1</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.99</span><span class="s0">, </span><span class="s2">2.92</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.59</span><span class="s0">, </span><span class="s2">2.18</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.45</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.73</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.12</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.3</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.83</span><span class="s0">,</span>
    <span class="s2">0.2</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.77</span><span class="s0">, </span><span class="s2">3.24</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.98</span><span class="s0">, </span><span class="s2">1.6</span><span class="s0">, </span><span class="s1">-</span><span class="s2">4.59</span><span class="s0">, </span><span class="s2">3.39</span><span class="s0">, </span><span class="s1">-</span><span class="s2">4.85</span><span class="s0">, </span><span class="s2">3.75</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.25</span><span class="s0">, </span><span class="s2">1.71</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.28</span><span class="s0">,</span>
    <span class="s2">3.38</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.74</span><span class="s0">, </span><span class="s2">0.88</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.41</span><span class="s0">, </span><span class="s2">1.92</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.24</span><span class="s0">, </span><span class="s2">1.19</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.48</span><span class="s0">, </span><span class="s2">1.06</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.68</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.62</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">1.3</span><span class="s0">, </span><span class="s2">0.39</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.78</span><span class="s0">, </span><span class="s2">2.35</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.54</span><span class="s0">, </span><span class="s2">2.44</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.32</span><span class="s0">, </span><span class="s2">0.66</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.38</span><span class="s0">, </span><span class="s2">2.76</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.35</span><span class="s0">,</span>
    <span class="s2">3.95</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.86</span><span class="s0">, </span><span class="s2">4.32</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.01</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.23</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.79</span><span class="s0">, </span><span class="s2">2.76</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.13</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.13</span><span class="s0">, </span><span class="s1">-</span><span class="s2">5.25</span><span class="s0">, </span><span class="s2">3.84</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">2.24</span><span class="s0">, </span><span class="s2">1.59</span><span class="s0">, </span><span class="s1">-</span><span class="s2">4.85</span><span class="s0">, </span><span class="s2">2.96</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.41</span><span class="s0">, </span><span class="s2">0.01</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.43</span><span class="s0">, </span><span class="s2">0.13</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.92</span><span class="s0">, </span><span class="s2">2.91</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.75</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">0.53</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.69</span><span class="s0">, </span><span class="s2">1.69</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.09</span><span class="s0">, </span><span class="s2">0.15</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.11</span><span class="s0">, </span><span class="s2">2.17</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.53</span><span class="s0">, </span><span class="s2">1.22</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.1</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.86</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">2.56</span><span class="s0">, </span><span class="s2">2.28</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.02</span><span class="s0">, </span><span class="s2">3.33</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.12</span><span class="s0">, </span><span class="s2">3.86</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.18</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.19</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.03</span><span class="s0">, </span><span class="s2">0.79</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.83</span><span class="s0">,</span>
    <span class="s2">0.97</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.19</span><span class="s0">, </span><span class="s2">1.45</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.34</span><span class="s0">, </span><span class="s2">1.28</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.52</span><span class="s0">, </span><span class="s2">4.22</span><span class="s0">, </span><span class="s1">-</span><span class="s2">4.53</span><span class="s0">, </span><span class="s2">3.22</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.97</span><span class="s0">, </span><span class="s2">1.75</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">2.36</span><span class="s0">, </span><span class="s2">3.19</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.83</span><span class="s0">, </span><span class="s2">1.53</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.59</span><span class="s0">, </span><span class="s2">1.86</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.17</span><span class="s0">, </span><span class="s2">2.3</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.63</span><span class="s0">, </span><span class="s2">2.71</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.03</span><span class="s0">,</span>
    <span class="s2">3.75</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.57</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.6</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.47</span><span class="s0">, </span><span class="s2">1.33</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.95</span><span class="s0">, </span><span class="s2">0.7</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.65</span><span class="s0">, </span><span class="s2">1.27</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.42</span><span class="s0">, </span><span class="s2">1.09</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.0</span><span class="s0">,</span>
    <span class="s2">3.87</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.51</span><span class="s0">, </span><span class="s2">3.06</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.6</span><span class="s0">, </span><span class="s2">0.74</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.08</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.03</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.44</span><span class="s0">, </span><span class="s2">1.31</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.65</span><span class="s0">, </span><span class="s2">2.99</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">1.84</span><span class="s0">, </span><span class="s2">1.65</span><span class="s0">, </span><span class="s1">-</span><span class="s2">4.76</span><span class="s0">, </span><span class="s2">3.75</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.07</span><span class="s0">, </span><span class="s2">3.98</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.4</span><span class="s0">, </span><span class="s2">2.67</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.21</span><span class="s0">, </span><span class="s2">1.49</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.21</span><span class="s0">,</span>
    <span class="s2">1.22</span><span class="s0">, </span><span class="s1">-</span><span class="s2">5.29</span><span class="s0">, </span><span class="s2">2.38</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.85</span><span class="s0">, </span><span class="s2">2.28</span><span class="s0">, </span><span class="s1">-</span><span class="s2">5.6</span><span class="s0">, </span><span class="s2">3.78</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.7</span><span class="s0">, </span><span class="s2">0.8</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.81</span><span class="s0">, </span><span class="s2">3.5</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3.75</span><span class="s0">,</span>
    <span class="s2">4.17</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.29</span><span class="s0">, </span><span class="s2">2.99</span><span class="s0">, </span><span class="s1">-</span><span class="s2">5.92</span><span class="s0">, </span><span class="s2">3.43</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.83</span><span class="s0">, </span><span class="s2">1.23</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.24</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.04</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.56</span><span class="s0">, </span><span class="s2">2.37</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">3.26</span><span class="s0">, </span><span class="s2">0.39</span><span class="s0">, </span><span class="s1">-</span><span class="s2">4.63</span><span class="s0">, </span><span class="s2">2.51</span><span class="s0">, </span><span class="s1">-</span><span class="s2">4.52</span><span class="s0">, </span><span class="s2">3.04</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.7</span><span class="s0">, </span><span class="s2">0.36</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.41</span><span class="s0">, </span><span class="s2">0.04</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.1</span><span class="s0">, </span><span class="s2">1.0</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">1.87</span><span class="s0">, </span><span class="s2">3.78</span><span class="s0">, </span><span class="s1">-</span><span class="s2">4.32</span><span class="s0">, </span><span class="s2">3.59</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.24</span><span class="s0">, </span><span class="s2">1.38</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.99</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.22</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.87</span><span class="s0">, </span><span class="s2">1.95</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.84</span><span class="s0">,</span>
    <span class="s2">2.17</span><span class="s0">, </span><span class="s1">-</span><span class="s2">5.38</span><span class="s0">, </span><span class="s2">3.56</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.27</span><span class="s0">, </span><span class="s2">2.9</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.79</span><span class="s0">, </span><span class="s2">3.31</span><span class="s0">, </span><span class="s1">-</span><span class="s2">5.47</span><span class="s0">, </span><span class="s2">3.85</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.44</span><span class="s0">, </span><span class="s2">3.69</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">2.02</span><span class="s0">, </span><span class="s2">0.37</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.29</span><span class="s0">, </span><span class="s2">0.33</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.34</span><span class="s0">, </span><span class="s2">2.56</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.74</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.27</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.97</span><span class="s0">, </span><span class="s2">1.22</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.51</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">0.16</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.64</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.96</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.99</span><span class="s0">, </span><span class="s2">1.4</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.53</span><span class="s0">, </span><span class="s2">3.31</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.24</span><span class="s0">, </span><span class="s2">0.45</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2.46</span><span class="s0">, </span><span class="s2">1.71</span><span class="s0">,</span>
    <span class="s1">-</span><span class="s2">2.88</span><span class="s0">, </span><span class="s2">1.56</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.63</span><span class="s0">, </span><span class="s2">1.46</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.41</span><span class="s0">, </span><span class="s2">0.68</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.96</span><span class="s0">, </span><span class="s2">2.76</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1.61</span><span class="s0">,</span>
    <span class="s2">2.11</span><span class="s1">]).reshape((</span><span class="s2">200</span><span class="s0">, </span><span class="s2">2</span><span class="s1">))</span>


<span class="s3"># Global data</span>
<span class="s1">X = np.array([[</span><span class="s2">3.0</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">4</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">4</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">,</span>
              <span class="s1">[</span><span class="s2">9</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">5</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">6</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">9</span><span class="s0">, </span><span class="s2">4</span><span class="s1">]</span><span class="s0">,</span>
              <span class="s1">[</span><span class="s2">5</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">5</span><span class="s0">, </span><span class="s2">4</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">7</span><span class="s0">, </span><span class="s2">4</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">6</span><span class="s0">, </span><span class="s2">5</span><span class="s1">]])</span>

<span class="s1">CODET1 = np.array([[</span><span class="s2">3.0000</span><span class="s0">, </span><span class="s2">3.0000</span><span class="s1">]</span><span class="s0">,</span>
                   <span class="s1">[</span><span class="s2">6.2000</span><span class="s0">, </span><span class="s2">4.0000</span><span class="s1">]</span><span class="s0">,</span>
                   <span class="s1">[</span><span class="s2">5.8000</span><span class="s0">, </span><span class="s2">1.8000</span><span class="s1">]])</span>

<span class="s1">CODET2 = np.array([[</span><span class="s2">11.0</span><span class="s1">/</span><span class="s2">3</span><span class="s0">, </span><span class="s2">8.0</span><span class="s1">/</span><span class="s2">3</span><span class="s1">]</span><span class="s0">,</span>
                   <span class="s1">[</span><span class="s2">6.7500</span><span class="s0">, </span><span class="s2">4.2500</span><span class="s1">]</span><span class="s0">,</span>
                   <span class="s1">[</span><span class="s2">6.2500</span><span class="s0">, </span><span class="s2">1.7500</span><span class="s1">]])</span>

<span class="s1">LABEL1 = np.array([</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">])</span>


<span class="s0">class </span><span class="s1">TestWhiten:</span>
    <span class="s0">def </span><span class="s1">test_whiten(self):</span>
        <span class="s1">desired = np.array([[</span><span class="s2">5.08738849</span><span class="s0">, </span><span class="s2">2.97091878</span><span class="s1">]</span><span class="s0">,</span>
                            <span class="s1">[</span><span class="s2">3.19909255</span><span class="s0">, </span><span class="s2">0.69660580</span><span class="s1">]</span><span class="s0">,</span>
                            <span class="s1">[</span><span class="s2">4.51041982</span><span class="s0">, </span><span class="s2">0.02640918</span><span class="s1">]</span><span class="s0">,</span>
                            <span class="s1">[</span><span class="s2">4.38567074</span><span class="s0">, </span><span class="s2">0.95120889</span><span class="s1">]</span><span class="s0">,</span>
                            <span class="s1">[</span><span class="s2">2.32191480</span><span class="s0">, </span><span class="s2">1.63195503</span><span class="s1">]])</span>
        <span class="s0">for </span><span class="s1">tp </span><span class="s0">in </span><span class="s1">np.array</span><span class="s0">, </span><span class="s1">matrix:</span>
            <span class="s1">obs = tp([[</span><span class="s2">0.98744510</span><span class="s0">, </span><span class="s2">0.82766775</span><span class="s1">]</span><span class="s0">,</span>
                      <span class="s1">[</span><span class="s2">0.62093317</span><span class="s0">, </span><span class="s2">0.19406729</span><span class="s1">]</span><span class="s0">,</span>
                      <span class="s1">[</span><span class="s2">0.87545741</span><span class="s0">, </span><span class="s2">0.00735733</span><span class="s1">]</span><span class="s0">,</span>
                      <span class="s1">[</span><span class="s2">0.85124403</span><span class="s0">, </span><span class="s2">0.26499712</span><span class="s1">]</span><span class="s0">,</span>
                      <span class="s1">[</span><span class="s2">0.45067590</span><span class="s0">, </span><span class="s2">0.45464607</span><span class="s1">]])</span>
            <span class="s1">assert_allclose(whiten(obs)</span><span class="s0">, </span><span class="s1">desired</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s2">1e-5</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">test_whiten_zero_std(self):</span>
        <span class="s1">desired = np.array([[</span><span class="s2">0.</span><span class="s0">, </span><span class="s2">1.0</span><span class="s0">, </span><span class="s2">2.86666544</span><span class="s1">]</span><span class="s0">,</span>
                            <span class="s1">[</span><span class="s2">0.</span><span class="s0">, </span><span class="s2">1.0</span><span class="s0">, </span><span class="s2">1.32460034</span><span class="s1">]</span><span class="s0">,</span>
                            <span class="s1">[</span><span class="s2">0.</span><span class="s0">, </span><span class="s2">1.0</span><span class="s0">, </span><span class="s2">3.74382172</span><span class="s1">]])</span>
        <span class="s0">for </span><span class="s1">tp </span><span class="s0">in </span><span class="s1">np.array</span><span class="s0">, </span><span class="s1">matrix:</span>
            <span class="s1">obs = tp([[</span><span class="s2">0.</span><span class="s0">, </span><span class="s2">1.</span><span class="s0">, </span><span class="s2">0.74109533</span><span class="s1">]</span><span class="s0">,</span>
                      <span class="s1">[</span><span class="s2">0.</span><span class="s0">, </span><span class="s2">1.</span><span class="s0">, </span><span class="s2">0.34243798</span><span class="s1">]</span><span class="s0">,</span>
                      <span class="s1">[</span><span class="s2">0.</span><span class="s0">, </span><span class="s2">1.</span><span class="s0">, </span><span class="s2">0.96785929</span><span class="s1">]])</span>
            <span class="s0">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s0">True</span><span class="s1">) </span><span class="s0">as </span><span class="s1">w:</span>
                <span class="s1">warnings.simplefilter(</span><span class="s4">'always'</span><span class="s1">)</span>
                <span class="s1">assert_allclose(whiten(obs)</span><span class="s0">, </span><span class="s1">desired</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s2">1e-5</span><span class="s1">)</span>
                <span class="s1">assert_equal(len(w)</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)</span>
                <span class="s1">assert_(issubclass(w[-</span><span class="s2">1</span><span class="s1">].category</span><span class="s0">, </span><span class="s1">RuntimeWarning))</span>

    <span class="s0">def </span><span class="s1">test_whiten_not_finite(self):</span>
        <span class="s0">for </span><span class="s1">tp </span><span class="s0">in </span><span class="s1">np.array</span><span class="s0">, </span><span class="s1">matrix:</span>
            <span class="s0">for </span><span class="s1">bad_value </span><span class="s0">in </span><span class="s1">np.nan</span><span class="s0">, </span><span class="s1">np.inf</span><span class="s0">, </span><span class="s1">-np.inf:</span>
                <span class="s1">obs = tp([[</span><span class="s2">0.98744510</span><span class="s0">, </span><span class="s1">bad_value]</span><span class="s0">,</span>
                          <span class="s1">[</span><span class="s2">0.62093317</span><span class="s0">, </span><span class="s2">0.19406729</span><span class="s1">]</span><span class="s0">,</span>
                          <span class="s1">[</span><span class="s2">0.87545741</span><span class="s0">, </span><span class="s2">0.00735733</span><span class="s1">]</span><span class="s0">,</span>
                          <span class="s1">[</span><span class="s2">0.85124403</span><span class="s0">, </span><span class="s2">0.26499712</span><span class="s1">]</span><span class="s0">,</span>
                          <span class="s1">[</span><span class="s2">0.45067590</span><span class="s0">, </span><span class="s2">0.45464607</span><span class="s1">]])</span>
                <span class="s1">assert_raises(ValueError</span><span class="s0">, </span><span class="s1">whiten</span><span class="s0">, </span><span class="s1">obs)</span>


<span class="s0">class </span><span class="s1">TestVq:</span>
    <span class="s0">def </span><span class="s1">test_py_vq(self):</span>
        <span class="s1">initc = np.concatenate([[X[</span><span class="s2">0</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[X[</span><span class="s2">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[X[</span><span class="s2">2</span><span class="s1">]]])</span>
        <span class="s0">for </span><span class="s1">tp </span><span class="s0">in </span><span class="s1">np.array</span><span class="s0">, </span><span class="s1">matrix:</span>
            <span class="s1">label1 = py_vq(tp(X)</span><span class="s0">, </span><span class="s1">tp(initc))[</span><span class="s2">0</span><span class="s1">]</span>
            <span class="s1">assert_array_equal(label1</span><span class="s0">, </span><span class="s1">LABEL1)</span>

    <span class="s0">def </span><span class="s1">test_vq(self):</span>
        <span class="s1">initc = np.concatenate([[X[</span><span class="s2">0</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[X[</span><span class="s2">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[X[</span><span class="s2">2</span><span class="s1">]]])</span>
        <span class="s0">for </span><span class="s1">tp </span><span class="s0">in </span><span class="s1">np.array</span><span class="s0">, </span><span class="s1">matrix:</span>
            <span class="s1">label1</span><span class="s0">, </span><span class="s1">dist = _vq.vq(tp(X)</span><span class="s0">, </span><span class="s1">tp(initc))</span>
            <span class="s1">assert_array_equal(label1</span><span class="s0">, </span><span class="s1">LABEL1)</span>
            <span class="s1">tlabel1</span><span class="s0">, </span><span class="s1">tdist = vq(tp(X)</span><span class="s0">, </span><span class="s1">tp(initc))</span>

    <span class="s0">def </span><span class="s1">test_vq_1d(self):</span>
        <span class="s3"># Test special rank 1 vq algo, python implementation.</span>
        <span class="s1">data = X[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span>
        <span class="s1">initc = data[:</span><span class="s2">3</span><span class="s1">]</span>
        <span class="s1">a</span><span class="s0">, </span><span class="s1">b = _vq.vq(data</span><span class="s0">, </span><span class="s1">initc)</span>
        <span class="s1">ta</span><span class="s0">, </span><span class="s1">tb = py_vq(data[:</span><span class="s0">, </span><span class="s1">np.newaxis]</span><span class="s0">, </span><span class="s1">initc[:</span><span class="s0">, </span><span class="s1">np.newaxis])</span>
        <span class="s1">assert_array_equal(a</span><span class="s0">, </span><span class="s1">ta)</span>
        <span class="s1">assert_array_equal(b</span><span class="s0">, </span><span class="s1">tb)</span>

    <span class="s0">def </span><span class="s1">test__vq_sametype(self):</span>
        <span class="s1">a = np.array([</span><span class="s2">1.0</span><span class="s0">, </span><span class="s2">2.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=np.float64)</span>
        <span class="s1">b = a.astype(np.float32)</span>
        <span class="s1">assert_raises(TypeError</span><span class="s0">, </span><span class="s1">_vq.vq</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">b)</span>

    <span class="s0">def </span><span class="s1">test__vq_invalid_type(self):</span>
        <span class="s1">a = np.array([</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=int)</span>
        <span class="s1">assert_raises(TypeError</span><span class="s0">, </span><span class="s1">_vq.vq</span><span class="s0">, </span><span class="s1">a</span><span class="s0">, </span><span class="s1">a)</span>

    <span class="s0">def </span><span class="s1">test_vq_large_nfeat(self):</span>
        <span class="s1">X = np.random.rand(</span><span class="s2">20</span><span class="s0">, </span><span class="s2">20</span><span class="s1">)</span>
        <span class="s1">code_book = np.random.rand(</span><span class="s2">3</span><span class="s0">, </span><span class="s2">20</span><span class="s1">)</span>

        <span class="s1">codes0</span><span class="s0">, </span><span class="s1">dis0 = _vq.vq(X</span><span class="s0">, </span><span class="s1">code_book)</span>
        <span class="s1">codes1</span><span class="s0">, </span><span class="s1">dis1 = py_vq(X</span><span class="s0">, </span><span class="s1">code_book)</span>
        <span class="s1">assert_allclose(dis0</span><span class="s0">, </span><span class="s1">dis1</span><span class="s0">, </span><span class="s2">1e-5</span><span class="s1">)</span>
        <span class="s1">assert_array_equal(codes0</span><span class="s0">, </span><span class="s1">codes1)</span>

        <span class="s1">X = X.astype(np.float32)</span>
        <span class="s1">code_book = code_book.astype(np.float32)</span>

        <span class="s1">codes0</span><span class="s0">, </span><span class="s1">dis0 = _vq.vq(X</span><span class="s0">, </span><span class="s1">code_book)</span>
        <span class="s1">codes1</span><span class="s0">, </span><span class="s1">dis1 = py_vq(X</span><span class="s0">, </span><span class="s1">code_book)</span>
        <span class="s1">assert_allclose(dis0</span><span class="s0">, </span><span class="s1">dis1</span><span class="s0">, </span><span class="s2">1e-5</span><span class="s1">)</span>
        <span class="s1">assert_array_equal(codes0</span><span class="s0">, </span><span class="s1">codes1)</span>

    <span class="s0">def </span><span class="s1">test_vq_large_features(self):</span>
        <span class="s1">X = np.random.rand(</span><span class="s2">10</span><span class="s0">, </span><span class="s2">5</span><span class="s1">) * </span><span class="s2">1000000</span>
        <span class="s1">code_book = np.random.rand(</span><span class="s2">2</span><span class="s0">, </span><span class="s2">5</span><span class="s1">) * </span><span class="s2">1000000</span>

        <span class="s1">codes0</span><span class="s0">, </span><span class="s1">dis0 = _vq.vq(X</span><span class="s0">, </span><span class="s1">code_book)</span>
        <span class="s1">codes1</span><span class="s0">, </span><span class="s1">dis1 = py_vq(X</span><span class="s0">, </span><span class="s1">code_book)</span>
        <span class="s1">assert_allclose(dis0</span><span class="s0">, </span><span class="s1">dis1</span><span class="s0">, </span><span class="s2">1e-5</span><span class="s1">)</span>
        <span class="s1">assert_array_equal(codes0</span><span class="s0">, </span><span class="s1">codes1)</span>


<span class="s0">class </span><span class="s1">TestKMean:</span>
    <span class="s0">def </span><span class="s1">test_large_features(self):</span>
        <span class="s3"># Generate a data set with large values, and run kmeans on it to</span>
        <span class="s3"># (regression for 1077).</span>
        <span class="s1">d = </span><span class="s2">300</span>
        <span class="s1">n = </span><span class="s2">100</span>

        <span class="s1">m1 = np.random.randn(d)</span>
        <span class="s1">m2 = np.random.randn(d)</span>
        <span class="s1">x = </span><span class="s2">10000 </span><span class="s1">* np.random.randn(n</span><span class="s0">, </span><span class="s1">d) - </span><span class="s2">20000 </span><span class="s1">* m1</span>
        <span class="s1">y = </span><span class="s2">10000 </span><span class="s1">* np.random.randn(n</span><span class="s0">, </span><span class="s1">d) + </span><span class="s2">20000 </span><span class="s1">* m2</span>

        <span class="s1">data = np.empty((x.shape[</span><span class="s2">0</span><span class="s1">] + y.shape[</span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">d)</span><span class="s0">, </span><span class="s1">np.double)</span>
        <span class="s1">data[:x.shape[</span><span class="s2">0</span><span class="s1">]] = x</span>
        <span class="s1">data[x.shape[</span><span class="s2">0</span><span class="s1">]:] = y</span>

        <span class="s1">kmeans(data</span><span class="s0">, </span><span class="s2">2</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">test_kmeans_simple(self):</span>
        <span class="s1">np.random.seed(</span><span class="s2">54321</span><span class="s1">)</span>
        <span class="s1">initc = np.concatenate([[X[</span><span class="s2">0</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[X[</span><span class="s2">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[X[</span><span class="s2">2</span><span class="s1">]]])</span>
        <span class="s0">for </span><span class="s1">tp </span><span class="s0">in </span><span class="s1">np.array</span><span class="s0">, </span><span class="s1">matrix:</span>
            <span class="s1">code1 = kmeans(tp(X)</span><span class="s0">, </span><span class="s1">tp(initc)</span><span class="s0">, </span><span class="s1">iter=</span><span class="s2">1</span><span class="s1">)[</span><span class="s2">0</span><span class="s1">]</span>
            <span class="s1">assert_array_almost_equal(code1</span><span class="s0">, </span><span class="s1">CODET2)</span>

    <span class="s0">def </span><span class="s1">test_kmeans_lost_cluster(self):</span>
        <span class="s3"># This will cause kmeans to have a cluster with no points.</span>
        <span class="s1">data = TESTDATA_2D</span>
        <span class="s1">initk = np.array([[-</span><span class="s2">1.8127404</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.67128041</span><span class="s1">]</span><span class="s0">,</span>
                         <span class="s1">[</span><span class="s2">2.04621601</span><span class="s0">, </span><span class="s2">0.07401111</span><span class="s1">]</span><span class="s0">,</span>
                         <span class="s1">[-</span><span class="s2">2.31149087</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.05160469</span><span class="s1">]])</span>

        <span class="s1">kmeans(data</span><span class="s0">, </span><span class="s1">initk)</span>
        <span class="s0">with </span><span class="s1">suppress_warnings() </span><span class="s0">as </span><span class="s1">sup:</span>
            <span class="s1">sup.filter(UserWarning</span><span class="s0">,</span>
                       <span class="s4">&quot;One of the clusters is empty. Re-run kmeans with a &quot;</span>
                       <span class="s4">&quot;different initialization&quot;</span><span class="s1">)</span>
            <span class="s1">kmeans2(data</span><span class="s0">, </span><span class="s1">initk</span><span class="s0">, </span><span class="s1">missing=</span><span class="s4">'warn'</span><span class="s1">)</span>

        <span class="s1">assert_raises(ClusterError</span><span class="s0">, </span><span class="s1">kmeans2</span><span class="s0">, </span><span class="s1">data</span><span class="s0">, </span><span class="s1">initk</span><span class="s0">, </span><span class="s1">missing=</span><span class="s4">'raise'</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">test_kmeans2_simple(self):</span>
        <span class="s1">np.random.seed(</span><span class="s2">12345678</span><span class="s1">)</span>
        <span class="s1">initc = np.concatenate([[X[</span><span class="s2">0</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[X[</span><span class="s2">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[X[</span><span class="s2">2</span><span class="s1">]]])</span>
        <span class="s0">for </span><span class="s1">tp </span><span class="s0">in </span><span class="s1">np.array</span><span class="s0">, </span><span class="s1">matrix:</span>
            <span class="s1">code1 = kmeans2(tp(X)</span><span class="s0">, </span><span class="s1">tp(initc)</span><span class="s0">, </span><span class="s1">iter=</span><span class="s2">1</span><span class="s1">)[</span><span class="s2">0</span><span class="s1">]</span>
            <span class="s1">code2 = kmeans2(tp(X)</span><span class="s0">, </span><span class="s1">tp(initc)</span><span class="s0">, </span><span class="s1">iter=</span><span class="s2">2</span><span class="s1">)[</span><span class="s2">0</span><span class="s1">]</span>

            <span class="s1">assert_array_almost_equal(code1</span><span class="s0">, </span><span class="s1">CODET1)</span>
            <span class="s1">assert_array_almost_equal(code2</span><span class="s0">, </span><span class="s1">CODET2)</span>

    <span class="s0">def </span><span class="s1">test_kmeans2_rank1(self):</span>
        <span class="s1">data = TESTDATA_2D</span>
        <span class="s1">data1 = data[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span>

        <span class="s1">initc = data1[:</span><span class="s2">3</span><span class="s1">]</span>
        <span class="s1">code = initc.copy()</span>
        <span class="s1">kmeans2(data1</span><span class="s0">, </span><span class="s1">code</span><span class="s0">, </span><span class="s1">iter=</span><span class="s2">1</span><span class="s1">)[</span><span class="s2">0</span><span class="s1">]</span>
        <span class="s1">kmeans2(data1</span><span class="s0">, </span><span class="s1">code</span><span class="s0">, </span><span class="s1">iter=</span><span class="s2">2</span><span class="s1">)[</span><span class="s2">0</span><span class="s1">]</span>

    <span class="s0">def </span><span class="s1">test_kmeans2_rank1_2(self):</span>
        <span class="s1">data = TESTDATA_2D</span>
        <span class="s1">data1 = data[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span>
        <span class="s1">kmeans2(data1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s1">iter=</span><span class="s2">1</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">test_kmeans2_high_dim(self):</span>
        <span class="s3"># test kmeans2 when the number of dimensions exceeds the number</span>
        <span class="s3"># of input points</span>
        <span class="s1">data = TESTDATA_2D</span>
        <span class="s1">data = data.reshape((</span><span class="s2">20</span><span class="s0">, </span><span class="s2">20</span><span class="s1">))[:</span><span class="s2">10</span><span class="s1">]</span>
        <span class="s1">kmeans2(data</span><span class="s0">, </span><span class="s2">2</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">test_kmeans2_init(self):</span>
        <span class="s1">np.random.seed(</span><span class="s2">12345</span><span class="s1">)</span>
        <span class="s1">data = TESTDATA_2D</span>

        <span class="s1">kmeans2(data</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s1">minit=</span><span class="s4">'points'</span><span class="s1">)</span>
        <span class="s1">kmeans2(data[:</span><span class="s0">, </span><span class="s1">:</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s1">minit=</span><span class="s4">'points'</span><span class="s1">)  </span><span class="s3"># special case (1-D)</span>

        <span class="s1">kmeans2(data</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s1">minit=</span><span class="s4">'++'</span><span class="s1">)</span>
        <span class="s1">kmeans2(data[:</span><span class="s0">, </span><span class="s1">:</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s1">minit=</span><span class="s4">'++'</span><span class="s1">)  </span><span class="s3"># special case (1-D)</span>

        <span class="s3"># minit='random' can give warnings, filter those</span>
        <span class="s0">with </span><span class="s1">suppress_warnings() </span><span class="s0">as </span><span class="s1">sup:</span>
            <span class="s1">sup.filter(message=</span><span class="s4">&quot;One of the clusters is empty. Re-run.&quot;</span><span class="s1">)</span>
            <span class="s1">kmeans2(data</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s1">minit=</span><span class="s4">'random'</span><span class="s1">)</span>
            <span class="s1">kmeans2(data[:</span><span class="s0">, </span><span class="s1">:</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s1">minit=</span><span class="s4">'random'</span><span class="s1">)  </span><span class="s3"># special case (1-D)</span>

    <span class="s1">@pytest.mark.skipif(sys.platform == </span><span class="s4">'win32'</span><span class="s0">,</span>
                        <span class="s1">reason=</span><span class="s4">'Fails with MemoryError in Wine.'</span><span class="s1">)</span>
    <span class="s0">def </span><span class="s1">test_krandinit(self):</span>
        <span class="s1">data = TESTDATA_2D</span>
        <span class="s1">datas = [data.reshape((</span><span class="s2">200</span><span class="s0">, </span><span class="s2">2</span><span class="s1">))</span><span class="s0">, </span><span class="s1">data.reshape((</span><span class="s2">20</span><span class="s0">, </span><span class="s2">20</span><span class="s1">))[:</span><span class="s2">10</span><span class="s1">]]</span>
        <span class="s1">k = int(</span><span class="s2">1e6</span><span class="s1">)</span>
        <span class="s0">for </span><span class="s1">data </span><span class="s0">in </span><span class="s1">datas:</span>
            <span class="s3"># check that np.random.Generator can be used (numpy &gt;= 1.17)</span>
            <span class="s0">if </span><span class="s1">hasattr(np.random</span><span class="s0">, </span><span class="s4">'default_rng'</span><span class="s1">):</span>
                <span class="s1">rng = np.random.default_rng(</span><span class="s2">1234</span><span class="s1">)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">rng = np.random.RandomState(</span><span class="s2">1234</span><span class="s1">)</span>

            <span class="s1">init = _krandinit(data</span><span class="s0">, </span><span class="s1">k</span><span class="s0">, </span><span class="s1">rng)</span>
            <span class="s1">orig_cov = np.cov(data</span><span class="s0">, </span><span class="s1">rowvar=</span><span class="s2">0</span><span class="s1">)</span>
            <span class="s1">init_cov = np.cov(init</span><span class="s0">, </span><span class="s1">rowvar=</span><span class="s2">0</span><span class="s1">)</span>
            <span class="s1">assert_allclose(orig_cov</span><span class="s0">, </span><span class="s1">init_cov</span><span class="s0">, </span><span class="s1">atol=</span><span class="s2">1e-2</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">test_kmeans2_empty(self):</span>
        <span class="s3"># Regression test for gh-1032.</span>
        <span class="s1">assert_raises(ValueError</span><span class="s0">, </span><span class="s1">kmeans2</span><span class="s0">, </span><span class="s1">[]</span><span class="s0">, </span><span class="s2">2</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">test_kmeans_0k(self):</span>
        <span class="s3"># Regression test for gh-1073: fail when k arg is 0.</span>
        <span class="s1">assert_raises(ValueError</span><span class="s0">, </span><span class="s1">kmeans</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s2">0</span><span class="s1">)</span>
        <span class="s1">assert_raises(ValueError</span><span class="s0">, </span><span class="s1">kmeans2</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s2">0</span><span class="s1">)</span>
        <span class="s1">assert_raises(ValueError</span><span class="s0">, </span><span class="s1">kmeans2</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">np.array([]))</span>

    <span class="s0">def </span><span class="s1">test_kmeans_large_thres(self):</span>
        <span class="s3"># Regression test for gh-1774</span>
        <span class="s1">x = np.array([</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s2">4</span><span class="s0">, </span><span class="s2">10</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=float)</span>
        <span class="s1">res = kmeans(x</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s1">thresh=</span><span class="s2">1e16</span><span class="s1">)</span>
        <span class="s1">assert_allclose(res[</span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s2">4.</span><span class="s1">]))</span>
        <span class="s1">assert_allclose(res[</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s2">2.3999999999999999</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">test_kmeans2_kpp_low_dim(self):</span>
        <span class="s3"># Regression test for gh-11462</span>
        <span class="s1">prev_res = np.array([[-</span><span class="s2">1.95266667</span><span class="s0">, </span><span class="s2">0.898</span><span class="s1">]</span><span class="s0">,</span>
                             <span class="s1">[-</span><span class="s2">3.153375</span><span class="s0">, </span><span class="s2">3.3945</span><span class="s1">]])</span>
        <span class="s1">np.random.seed(</span><span class="s2">42</span><span class="s1">)</span>
        <span class="s1">res</span><span class="s0">, </span><span class="s1">_ = kmeans2(TESTDATA_2D</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s1">minit=</span><span class="s4">'++'</span><span class="s1">)</span>
        <span class="s1">assert_allclose(res</span><span class="s0">, </span><span class="s1">prev_res)</span>

    <span class="s0">def </span><span class="s1">test_kmeans2_kpp_high_dim(self):</span>
        <span class="s3"># Regression test for gh-11462</span>
        <span class="s1">n_dim = </span><span class="s2">100</span>
        <span class="s1">size = </span><span class="s2">10</span>
        <span class="s1">centers = np.vstack([</span><span class="s2">5 </span><span class="s1">* np.ones(n_dim)</span><span class="s0">,</span>
                             <span class="s1">-</span><span class="s2">5 </span><span class="s1">* np.ones(n_dim)])</span>
        <span class="s1">np.random.seed(</span><span class="s2">42</span><span class="s1">)</span>
        <span class="s1">data = np.vstack([</span>
            <span class="s1">np.random.multivariate_normal(centers[</span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.eye(n_dim)</span><span class="s0">, </span><span class="s1">size=size)</span><span class="s0">,</span>
            <span class="s1">np.random.multivariate_normal(centers[</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.eye(n_dim)</span><span class="s0">, </span><span class="s1">size=size)</span>
        <span class="s1">])</span>
        <span class="s1">res</span><span class="s0">, </span><span class="s1">_ = kmeans2(data</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s1">minit=</span><span class="s4">'++'</span><span class="s1">)</span>
        <span class="s1">assert_array_almost_equal(res</span><span class="s0">, </span><span class="s1">centers</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">test_kmeans_diff_convergence(self):</span>
        <span class="s3"># Regression test for gh-8727</span>
        <span class="s1">obs = np.array([-</span><span class="s2">3</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">8</span><span class="s1">]</span><span class="s0">, </span><span class="s1">float)</span>
        <span class="s1">res = kmeans(obs</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s2">3.</span><span class="s0">, </span><span class="s2">0.99</span><span class="s1">]))</span>
        <span class="s1">assert_allclose(res[</span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s2">0.4</span><span class="s0">,  </span><span class="s2">8.</span><span class="s1">]))</span>
        <span class="s1">assert_allclose(res[</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s2">1.0666666666666667</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">test_kmeans_and_kmeans2_random_seed(self):</span>

        <span class="s1">seed_list = [</span><span class="s2">1234</span><span class="s0">, </span><span class="s1">np.random.RandomState(</span><span class="s2">1234</span><span class="s1">)]</span>

        <span class="s3"># check that np.random.Generator can be used (numpy &gt;= 1.17)</span>
        <span class="s0">if </span><span class="s1">hasattr(np.random</span><span class="s0">, </span><span class="s4">'default_rng'</span><span class="s1">):</span>
            <span class="s1">seed_list.append(np.random.default_rng(</span><span class="s2">1234</span><span class="s1">))</span>

        <span class="s0">for </span><span class="s1">seed </span><span class="s0">in </span><span class="s1">seed_list:</span>
            <span class="s3"># test for kmeans</span>
            <span class="s1">res1</span><span class="s0">, </span><span class="s1">_ = kmeans(TESTDATA_2D</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s1">seed=seed)</span>
            <span class="s1">res2</span><span class="s0">, </span><span class="s1">_ = kmeans(TESTDATA_2D</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s1">seed=seed)</span>
            <span class="s1">assert_allclose(res1</span><span class="s0">, </span><span class="s1">res1)  </span><span class="s3"># should be same results</span>

            <span class="s3"># test for kmeans2</span>
            <span class="s0">for </span><span class="s1">minit </span><span class="s0">in </span><span class="s1">[</span><span class="s4">&quot;random&quot;</span><span class="s0">, </span><span class="s4">&quot;points&quot;</span><span class="s0">, </span><span class="s4">&quot;++&quot;</span><span class="s1">]:</span>
                <span class="s1">res1</span><span class="s0">, </span><span class="s1">_ = kmeans2(TESTDATA_2D</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s1">minit=minit</span><span class="s0">, </span><span class="s1">seed=seed)</span>
                <span class="s1">res2</span><span class="s0">, </span><span class="s1">_ = kmeans2(TESTDATA_2D</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s1">minit=minit</span><span class="s0">, </span><span class="s1">seed=seed)</span>
                <span class="s1">assert_allclose(res1</span><span class="s0">, </span><span class="s1">res1)  </span><span class="s3"># should be same results</span>
</pre>
</body>
</html>