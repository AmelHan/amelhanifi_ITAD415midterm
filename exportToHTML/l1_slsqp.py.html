<html>
<head>
<title>l1_slsqp.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
l1_slsqp.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Holds files for l1 regularization of LikelihoodModel, using 
scipy.optimize.slsqp 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy.optimize </span><span class="s2">import </span><span class="s1">fmin_slsqp</span>
<span class="s2">import </span><span class="s1">statsmodels.base.l1_solvers_common </span><span class="s2">as </span><span class="s1">l1_solvers_common</span>


<span class="s2">def </span><span class="s1">fit_l1_slsqp(</span>
        <span class="s1">f</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">start_params</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">kwargs</span><span class="s2">, </span><span class="s1">disp=</span><span class="s2">False, </span><span class="s1">maxiter=</span><span class="s3">1000</span><span class="s2">,</span>
        <span class="s1">callback=</span><span class="s2">None, </span><span class="s1">retall=</span><span class="s2">False, </span><span class="s1">full_output=</span><span class="s2">False, </span><span class="s1">hess=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Solve the l1 regularized problem using scipy.optimize.fmin_slsqp(). 
 
    Specifically:  We convert the convex but non-smooth problem 
 
    .. math:: \\min_\\beta f(\\beta) + \\sum_k\\alpha_k |\\beta_k| 
 
    via the transformation to the smooth, convex, constrained problem in twice 
    as many variables (adding the &quot;added variables&quot; :math:`u_k`) 
 
    .. math:: \\min_{\\beta,u} f(\\beta) + \\sum_k\\alpha_k u_k, 
 
    subject to 
 
    .. math:: -u_k \\leq \\beta_k \\leq u_k. 
 
    Parameters 
    ---------- 
    All the usual parameters from LikelhoodModel.fit 
    alpha : non-negative scalar or numpy array (same size as parameters) 
        The weight multiplying the l1 penalty term 
    trim_mode : 'auto, 'size', or 'off' 
        If not 'off', trim (set to zero) parameters that would have been zero 
            if the solver reached the theoretical minimum. 
        If 'auto', trim params using the Theory above. 
        If 'size', trim params if they have very small absolute value 
    size_trim_tol : float or 'auto' (default = 'auto') 
        For use when trim_mode === 'size' 
    auto_trim_tol : float 
        For sue when trim_mode == 'auto'.  Use 
    qc_tol : float 
        Print warning and do not allow auto trim when (ii) in &quot;Theory&quot; (above) 
        is violated by this much. 
    qc_verbose : bool 
        If true, print out a full QC report upon failure 
    acc : float (default 1e-6) 
        Requested accuracy as used by slsqp 
    &quot;&quot;&quot;</span>
    <span class="s1">start_params = np.array(start_params).ravel(</span><span class="s4">'F'</span><span class="s1">)</span>

    <span class="s5">### Extract values</span>
    <span class="s5"># k_params is total number of covariates,</span>
    <span class="s5"># possibly including a leading constant.</span>
    <span class="s1">k_params = len(start_params)</span>
    <span class="s5"># The start point</span>
    <span class="s1">x0 = np.append(start_params</span><span class="s2">, </span><span class="s1">np.fabs(start_params))</span>
    <span class="s5"># alpha is the regularization parameter</span>
    <span class="s1">alpha = np.array(kwargs[</span><span class="s4">'alpha_rescaled'</span><span class="s1">]).ravel(</span><span class="s4">'F'</span><span class="s1">)</span>
    <span class="s5"># Make sure it's a vector</span>
    <span class="s1">alpha = alpha * np.ones(k_params)</span>
    <span class="s2">assert </span><span class="s1">alpha.min() &gt;= </span><span class="s3">0</span>
    <span class="s5"># Convert display parameters to scipy.optimize form</span>
    <span class="s1">disp_slsqp = _get_disp_slsqp(disp</span><span class="s2">, </span><span class="s1">retall)</span>
    <span class="s5"># Set/retrieve the desired accuracy</span>
    <span class="s1">acc = kwargs.setdefault(</span><span class="s4">'acc'</span><span class="s2">, </span><span class="s3">1e-10</span><span class="s1">)</span>

    <span class="s5">### Wrap up for use in fmin_slsqp</span>
    <span class="s1">func = </span><span class="s2">lambda </span><span class="s1">x_full: _objective_func(f</span><span class="s2">, </span><span class="s1">x_full</span><span class="s2">, </span><span class="s1">k_params</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">*args)</span>
    <span class="s1">f_ieqcons_wrap = </span><span class="s2">lambda </span><span class="s1">x_full: _f_ieqcons(x_full</span><span class="s2">, </span><span class="s1">k_params)</span>
    <span class="s1">fprime_wrap = </span><span class="s2">lambda </span><span class="s1">x_full: _fprime(score</span><span class="s2">, </span><span class="s1">x_full</span><span class="s2">, </span><span class="s1">k_params</span><span class="s2">, </span><span class="s1">alpha)</span>
    <span class="s1">fprime_ieqcons_wrap = </span><span class="s2">lambda </span><span class="s1">x_full: _fprime_ieqcons(x_full</span><span class="s2">, </span><span class="s1">k_params)</span>

    <span class="s5">### Call the solver</span>
    <span class="s1">results = fmin_slsqp(</span>
        <span class="s1">func</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">f_ieqcons=f_ieqcons_wrap</span><span class="s2">, </span><span class="s1">fprime=fprime_wrap</span><span class="s2">, </span><span class="s1">acc=acc</span><span class="s2">,</span>
        <span class="s1">iter=maxiter</span><span class="s2">, </span><span class="s1">disp=disp_slsqp</span><span class="s2">, </span><span class="s1">full_output=full_output</span><span class="s2">,</span>
        <span class="s1">fprime_ieqcons=fprime_ieqcons_wrap)</span>
    <span class="s1">params = np.asarray(results[</span><span class="s3">0</span><span class="s1">][:k_params])</span>

    <span class="s5">### Post-process</span>
    <span class="s5"># QC</span>
    <span class="s1">qc_tol = kwargs[</span><span class="s4">'qc_tol'</span><span class="s1">]</span>
    <span class="s1">qc_verbose = kwargs[</span><span class="s4">'qc_verbose'</span><span class="s1">]</span>
    <span class="s1">passed = l1_solvers_common.qc_results(</span>
        <span class="s1">params</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">qc_tol</span><span class="s2">, </span><span class="s1">qc_verbose)</span>
    <span class="s5"># Possibly trim</span>
    <span class="s1">trim_mode = kwargs[</span><span class="s4">'trim_mode'</span><span class="s1">]</span>
    <span class="s1">size_trim_tol = kwargs[</span><span class="s4">'size_trim_tol'</span><span class="s1">]</span>
    <span class="s1">auto_trim_tol = kwargs[</span><span class="s4">'auto_trim_tol'</span><span class="s1">]</span>
    <span class="s1">params</span><span class="s2">, </span><span class="s1">trimmed = l1_solvers_common.do_trim_params(</span>
        <span class="s1">params</span><span class="s2">, </span><span class="s1">k_params</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">score</span><span class="s2">, </span><span class="s1">passed</span><span class="s2">, </span><span class="s1">trim_mode</span><span class="s2">, </span><span class="s1">size_trim_tol</span><span class="s2">,</span>
        <span class="s1">auto_trim_tol)</span>

    <span class="s5">### Pack up return values for statsmodels optimizers</span>
    <span class="s5"># TODO These retvals are returned as mle_retvals...but the fit was not ML.</span>
    <span class="s5"># This could be confusing someday.</span>
    <span class="s2">if </span><span class="s1">full_output:</span>
        <span class="s1">x_full</span><span class="s2">, </span><span class="s1">fx</span><span class="s2">, </span><span class="s1">its</span><span class="s2">, </span><span class="s1">imode</span><span class="s2">, </span><span class="s1">smode = results</span>
        <span class="s1">fopt = func(np.asarray(x_full))</span>
        <span class="s1">converged = (imode == </span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">warnflag = str(imode) + </span><span class="s4">' ' </span><span class="s1">+ smode</span>
        <span class="s1">iterations = its</span>
        <span class="s1">gopt = float(</span><span class="s4">'nan'</span><span class="s1">)     </span><span class="s5"># Objective is non-differentiable</span>
        <span class="s1">hopt = float(</span><span class="s4">'nan'</span><span class="s1">)</span>
        <span class="s1">retvals = {</span>
            <span class="s4">'fopt'</span><span class="s1">: fopt</span><span class="s2">, </span><span class="s4">'converged'</span><span class="s1">: converged</span><span class="s2">, </span><span class="s4">'iterations'</span><span class="s1">: iterations</span><span class="s2">,</span>
            <span class="s4">'gopt'</span><span class="s1">: gopt</span><span class="s2">, </span><span class="s4">'hopt'</span><span class="s1">: hopt</span><span class="s2">, </span><span class="s4">'trimmed'</span><span class="s1">: trimmed</span><span class="s2">,</span>
            <span class="s4">'warnflag'</span><span class="s1">: warnflag}</span>

    <span class="s5">### Return</span>
    <span class="s2">if </span><span class="s1">full_output:</span>
        <span class="s2">return </span><span class="s1">params</span><span class="s2">, </span><span class="s1">retvals</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">params</span>


<span class="s2">def </span><span class="s1">_get_disp_slsqp(disp</span><span class="s2">, </span><span class="s1">retall):</span>
    <span class="s2">if </span><span class="s1">disp </span><span class="s2">or </span><span class="s1">retall:</span>
        <span class="s2">if </span><span class="s1">disp:</span>
            <span class="s1">disp_slsqp = </span><span class="s3">1</span>
        <span class="s2">if </span><span class="s1">retall:</span>
            <span class="s1">disp_slsqp = </span><span class="s3">2</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">disp_slsqp = </span><span class="s3">0</span>
    <span class="s2">return </span><span class="s1">disp_slsqp</span>


<span class="s2">def </span><span class="s1">_objective_func(f</span><span class="s2">, </span><span class="s1">x_full</span><span class="s2">, </span><span class="s1">k_params</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">*args):</span>
    <span class="s0">&quot;&quot;&quot; 
    The regularized objective function 
    &quot;&quot;&quot;</span>
    <span class="s1">x_params = x_full[:k_params]</span>
    <span class="s1">x_added = x_full[k_params:]</span>
    <span class="s5">## Return</span>
    <span class="s2">return </span><span class="s1">f(x_params</span><span class="s2">, </span><span class="s1">*args) + (alpha * x_added).sum()</span>


<span class="s2">def </span><span class="s1">_fprime(score</span><span class="s2">, </span><span class="s1">x_full</span><span class="s2">, </span><span class="s1">k_params</span><span class="s2">, </span><span class="s1">alpha):</span>
    <span class="s0">&quot;&quot;&quot; 
    The regularized derivative 
    &quot;&quot;&quot;</span>
    <span class="s1">x_params = x_full[:k_params]</span>
    <span class="s5"># The derivative just appends a vector of constants</span>
    <span class="s2">return </span><span class="s1">np.append(score(x_params)</span><span class="s2">, </span><span class="s1">alpha)</span>


<span class="s2">def </span><span class="s1">_f_ieqcons(x_full</span><span class="s2">, </span><span class="s1">k_params):</span>
    <span class="s0">&quot;&quot;&quot; 
    The inequality constraints. 
    &quot;&quot;&quot;</span>
    <span class="s1">x_params = x_full[:k_params]</span>
    <span class="s1">x_added = x_full[k_params:]</span>
    <span class="s5"># All entries in this vector must be \geq 0 in a feasible solution</span>
    <span class="s2">return </span><span class="s1">np.append(x_params + x_added</span><span class="s2">, </span><span class="s1">x_added - x_params)</span>


<span class="s2">def </span><span class="s1">_fprime_ieqcons(x_full</span><span class="s2">, </span><span class="s1">k_params):</span>
    <span class="s0">&quot;&quot;&quot; 
    Derivative of the inequality constraints 
    &quot;&quot;&quot;</span>
    <span class="s1">I = np.eye(k_params)  </span><span class="s5"># noqa:E741</span>
    <span class="s1">A = np.concatenate((I</span><span class="s2">, </span><span class="s1">I)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">B = np.concatenate((-I</span><span class="s2">, </span><span class="s1">I)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">C = np.concatenate((A</span><span class="s2">, </span><span class="s1">B)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s5">## Return</span>
    <span class="s2">return </span><span class="s1">C</span>
</pre>
</body>
</html>