<html>
<head>
<title>_qmvnt.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_qmvnt.py</font>
</center></td></tr></table>
<pre><span class="s0"># Integration of multivariate normal and t distributions.</span>

<span class="s0"># Adapted from the MATLAB original implementations by Dr. Alan Genz.</span>

<span class="s0">#     http://www.math.wsu.edu/faculty/genz/software/software.html</span>

<span class="s0"># Copyright (C) 2013, Alan Genz,  All rights reserved.</span>
<span class="s0"># Python implementation is copyright (C) 2022, Robert Kern,  All rights</span>
<span class="s0"># reserved.</span>

<span class="s0"># Redistribution and use in source and binary forms, with or without</span>
<span class="s0"># modification, are permitted provided the following conditions are met:</span>
<span class="s0">#   1. Redistributions of source code must retain the above copyright</span>
<span class="s0">#      notice, this list of conditions and the following disclaimer.</span>
<span class="s0">#   2. Redistributions in binary form must reproduce the above copyright</span>
<span class="s0">#      notice, this list of conditions and the following disclaimer in</span>
<span class="s0">#      the documentation and/or other materials provided with the</span>
<span class="s0">#      distribution.</span>
<span class="s0">#   3. The contributor name(s) may not be used to endorse or promote</span>
<span class="s0">#      products derived from this software without specific prior</span>
<span class="s0">#      written permission.</span>
<span class="s0"># THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS</span>
<span class="s0"># &quot;AS IS&quot; AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT</span>
<span class="s0"># LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS</span>
<span class="s0"># FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE</span>
<span class="s0"># COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,</span>
<span class="s0"># INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,</span>
<span class="s0"># BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS</span>
<span class="s0"># OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND</span>
<span class="s0"># ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR</span>
<span class="s0"># TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF USE</span>
<span class="s0"># OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</span>


<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>

<span class="s2">from </span><span class="s1">scipy.fft </span><span class="s2">import </span><span class="s1">fft</span><span class="s2">, </span><span class="s1">ifft</span>
<span class="s2">from </span><span class="s1">scipy.special </span><span class="s2">import </span><span class="s1">gammaincinv</span><span class="s2">, </span><span class="s1">ndtr</span><span class="s2">, </span><span class="s1">ndtri</span>
<span class="s2">from </span><span class="s1">scipy.stats._qmc </span><span class="s2">import </span><span class="s1">primes_from_2_to</span>


<span class="s1">phi = ndtr</span>
<span class="s1">phinv = ndtri</span>


<span class="s2">def </span><span class="s1">_factorize_int(n):</span>
    <span class="s3">&quot;&quot;&quot;Return a sorted list of the unique prime factors of a positive integer. 
    &quot;&quot;&quot;</span>
    <span class="s0"># NOTE: There are lots faster ways to do this, but this isn't terrible.</span>
    <span class="s1">factors = set()</span>
    <span class="s2">for </span><span class="s1">p </span><span class="s2">in </span><span class="s1">primes_from_2_to(int(np.sqrt(n)) + </span><span class="s4">1</span><span class="s1">):</span>
        <span class="s2">while not </span><span class="s1">(n % p):</span>
            <span class="s1">factors.add(p)</span>
            <span class="s1">n //= p</span>
        <span class="s2">if </span><span class="s1">n == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">break</span>
    <span class="s2">if </span><span class="s1">n != </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">factors.add(n)</span>
    <span class="s2">return </span><span class="s1">sorted(factors)</span>


<span class="s2">def </span><span class="s1">_primitive_root(p):</span>
    <span class="s3">&quot;&quot;&quot;Compute a primitive root of the prime number `p`. 
 
    Used in the CBC lattice construction. 
 
    References 
    ---------- 
    .. [1] https://en.wikipedia.org/wiki/Primitive_root_modulo_n 
    &quot;&quot;&quot;</span>
    <span class="s0"># p is prime</span>
    <span class="s1">pm = p - </span><span class="s4">1</span>
    <span class="s1">factors = _factorize_int(pm)</span>
    <span class="s1">n = len(factors)</span>
    <span class="s1">r = </span><span class="s4">2</span>
    <span class="s1">k = </span><span class="s4">0</span>
    <span class="s2">while </span><span class="s1">k &lt; n:</span>
        <span class="s1">d = pm // factors[k]</span>
        <span class="s0"># pow() doesn't like numpy scalar types.</span>
        <span class="s1">rd = pow(int(r)</span><span class="s2">, </span><span class="s1">int(d)</span><span class="s2">, </span><span class="s1">int(p))</span>
        <span class="s2">if </span><span class="s1">rd == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">r += </span><span class="s4">1</span>
            <span class="s1">k = </span><span class="s4">0</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">k += </span><span class="s4">1</span>
    <span class="s2">return </span><span class="s1">r</span>


<span class="s2">def </span><span class="s1">_cbc_lattice(n_dim</span><span class="s2">, </span><span class="s1">n_qmc_samples):</span>
    <span class="s3">&quot;&quot;&quot;Compute a QMC lattice generator using a Fast CBC construction. 
 
    Parameters 
    ---------- 
    n_dim : int &gt; 0 
        The number of dimensions for the lattice. 
    n_qmc_samples : int &gt; 0 
        The desired number of QMC samples. This will be rounded down to the 
        nearest prime to enable the CBC construction. 
 
    Returns 
    ------- 
    q : float array : shape=(n_dim,) 
        The lattice generator vector. All values are in the open interval 
        `(0, 1)`. 
    actual_n_qmc_samples : int 
        The prime number of QMC samples that must be used with this lattice, 
        no more, no less. 
 
    References 
    ---------- 
    .. [1] Nuyens, D. and Cools, R. &quot;Fast Component-by-Component Construction, 
           a Reprise for Different Kernels&quot;, In H. Niederreiter and D. Talay, 
           editors, Monte-Carlo and Quasi-Monte Carlo Methods 2004, 
           Springer-Verlag, 2006, 371-385. 
    &quot;&quot;&quot;</span>
    <span class="s0"># Round down to the nearest prime number.</span>
    <span class="s1">primes = primes_from_2_to(n_qmc_samples + </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">n_qmc_samples = primes[-</span><span class="s4">1</span><span class="s1">]</span>

    <span class="s1">bt = np.ones(n_dim)</span>
    <span class="s1">gm = np.hstack([</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.8 </span><span class="s1">** np.arange(n_dim - </span><span class="s4">1</span><span class="s1">)])</span>
    <span class="s1">q = </span><span class="s4">1</span>
    <span class="s1">w = </span><span class="s4">0</span>
    <span class="s1">z = np.arange(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">n_dim + </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">m = (n_qmc_samples - </span><span class="s4">1</span><span class="s1">) // </span><span class="s4">2</span>
    <span class="s1">g = _primitive_root(n_qmc_samples)</span>
    <span class="s0"># Slightly faster way to compute perm[j] = pow(g, j, n_qmc_samples)</span>
    <span class="s0"># Shame that we don't have modulo pow() implemented as a ufunc.</span>
    <span class="s1">perm = np.ones(m</span><span class="s2">, </span><span class="s1">dtype=int)</span>
    <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(m - </span><span class="s4">1</span><span class="s1">):</span>
        <span class="s1">perm[j + </span><span class="s4">1</span><span class="s1">] = (g * perm[j]) % n_qmc_samples</span>
    <span class="s1">perm = np.minimum(n_qmc_samples - perm</span><span class="s2">, </span><span class="s1">perm)</span>
    <span class="s1">pn = perm / n_qmc_samples</span>
    <span class="s1">c = pn * pn - pn + </span><span class="s4">1.0 </span><span class="s1">/ </span><span class="s4">6</span>
    <span class="s1">fc = fft(c)</span>
    <span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">n_dim):</span>
        <span class="s1">reordered = np.hstack([</span>
            <span class="s1">c[:w+</span><span class="s4">1</span><span class="s1">][::-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">c[w+</span><span class="s4">1</span><span class="s1">:m][::-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">])</span>
        <span class="s1">q = q * (bt[s-</span><span class="s4">1</span><span class="s1">] + gm[s-</span><span class="s4">1</span><span class="s1">] * reordered)</span>
        <span class="s1">w = ifft(fc * fft(q)).real.argmin()</span>
        <span class="s1">z[s] = perm[w]</span>
    <span class="s1">q = z / n_qmc_samples</span>
    <span class="s2">return </span><span class="s1">q</span><span class="s2">, </span><span class="s1">n_qmc_samples</span>


<span class="s0"># Note: this function is not currently used or tested by any SciPy code. It is</span>
<span class="s0"># included in this file to facilitate the development of a parameter for users</span>
<span class="s0"># to set the desired CDF accuracy, but must be reviewed and tested before use.</span>
<span class="s2">def </span><span class="s1">_qauto(func</span><span class="s2">, </span><span class="s1">covar</span><span class="s2">, </span><span class="s1">low</span><span class="s2">, </span><span class="s1">high</span><span class="s2">, </span><span class="s1">rng</span><span class="s2">, </span><span class="s1">error=</span><span class="s4">1e-3</span><span class="s2">, </span><span class="s1">limit=</span><span class="s4">10_000</span><span class="s2">, </span><span class="s1">**kwds):</span>
    <span class="s3">&quot;&quot;&quot;Automatically rerun the integration to get the required error bound. 
 
    Parameters 
    ---------- 
    func : callable 
        Either :func:`_qmvn` or :func:`_qmvt`. 
    covar, low, high : array 
        As specified in :func:`_qmvn` and :func:`_qmvt`. 
    rng : Generator, optional 
        default_rng(), yada, yada 
    error : float &gt; 0 
        The desired error bound. 
    limit : int &gt; 0: 
        The rough limit of the number of integration points to consider. The 
        integration will stop looping once this limit has been *exceeded*. 
    **kwds : 
        Other keyword arguments to pass to `func`. When using :func:`_qmvt`, be 
        sure to include ``nu=`` as one of these. 
 
    Returns 
    ------- 
    prob : float 
        The estimated probability mass within the bounds. 
    est_error : float 
        3 times the standard error of the batch estimates. 
    n_samples : int 
        The number of integration points actually used. 
    &quot;&quot;&quot;</span>
    <span class="s1">n = len(covar)</span>
    <span class="s1">n_samples = </span><span class="s4">0</span>
    <span class="s2">if </span><span class="s1">n == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">prob = phi(high) - phi(low)</span>
        <span class="s0"># More or less</span>
        <span class="s1">est_error = </span><span class="s4">1e-15</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">mi = min(limit</span><span class="s2">, </span><span class="s1">n * </span><span class="s4">1000</span><span class="s1">)</span>
        <span class="s1">prob = </span><span class="s4">0.0</span>
        <span class="s1">est_error = </span><span class="s4">1.0</span>
        <span class="s1">ei = </span><span class="s4">0.0</span>
        <span class="s2">while </span><span class="s1">est_error &gt; error </span><span class="s2">and </span><span class="s1">n_samples &lt; limit:</span>
            <span class="s1">mi = round(np.sqrt(</span><span class="s4">2</span><span class="s1">) * mi)</span>
            <span class="s1">pi</span><span class="s2">, </span><span class="s1">ei</span><span class="s2">, </span><span class="s1">ni = func(mi</span><span class="s2">, </span><span class="s1">covar</span><span class="s2">, </span><span class="s1">low</span><span class="s2">, </span><span class="s1">high</span><span class="s2">, </span><span class="s1">rng=rng</span><span class="s2">, </span><span class="s1">**kwds)</span>
            <span class="s1">n_samples += ni</span>
            <span class="s1">wt = </span><span class="s4">1.0 </span><span class="s1">/ (</span><span class="s4">1 </span><span class="s1">+ (ei / est_error)**</span><span class="s4">2</span><span class="s1">)</span>
            <span class="s1">prob += wt * (pi - prob)</span>
            <span class="s1">est_error = np.sqrt(wt) * ei</span>
    <span class="s2">return </span><span class="s1">prob</span><span class="s2">, </span><span class="s1">est_error</span><span class="s2">, </span><span class="s1">n_samples</span>


<span class="s0"># Note: this function is not currently used or tested by any SciPy code. It is</span>
<span class="s0"># included in this file to facilitate the resolution of gh-8367, gh-16142, and</span>
<span class="s0"># possibly gh-14286, but must be reviewed and tested before use.</span>
<span class="s2">def </span><span class="s1">_qmvn(m</span><span class="s2">, </span><span class="s1">covar</span><span class="s2">, </span><span class="s1">low</span><span class="s2">, </span><span class="s1">high</span><span class="s2">, </span><span class="s1">rng</span><span class="s2">, </span><span class="s1">lattice=</span><span class="s5">'cbc'</span><span class="s2">, </span><span class="s1">n_batches=</span><span class="s4">10</span><span class="s1">):</span>
    <span class="s3">&quot;&quot;&quot;Multivariate normal integration over box bounds. 
 
    Parameters 
    ---------- 
    m : int &gt; n_batches 
        The number of points to sample. This number will be divided into 
        `n_batches` batches that apply random offsets of the sampling lattice 
        for each batch in order to estimate the error. 
    covar : (n, n) float array 
        Possibly singular, positive semidefinite symmetric covariance matrix. 
    low, high : (n,) float array 
        The low and high integration bounds. 
    rng : Generator, optional 
        default_rng(), yada, yada 
    lattice : 'cbc' or callable 
        The type of lattice rule to use to construct the integration points. 
    n_batches : int &gt; 0, optional 
        The number of QMC batches to apply. 
 
    Returns 
    ------- 
    prob : float 
        The estimated probability mass within the bounds. 
    est_error : float 
        3 times the standard error of the batch estimates. 
    &quot;&quot;&quot;</span>
    <span class="s1">cho</span><span class="s2">, </span><span class="s1">lo</span><span class="s2">, </span><span class="s1">hi = _permuted_cholesky(covar</span><span class="s2">, </span><span class="s1">low</span><span class="s2">, </span><span class="s1">high)</span>
    <span class="s1">n = cho.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">ct = cho[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">c = phi(lo[</span><span class="s4">0</span><span class="s1">] / ct)</span>
    <span class="s1">d = phi(hi[</span><span class="s4">0</span><span class="s1">] / ct)</span>
    <span class="s1">ci = c</span>
    <span class="s1">dci = d - ci</span>
    <span class="s1">prob = </span><span class="s4">0.0</span>
    <span class="s1">error_var = </span><span class="s4">0.0</span>
    <span class="s1">q</span><span class="s2">, </span><span class="s1">n_qmc_samples = _cbc_lattice(n - </span><span class="s4">1</span><span class="s2">, </span><span class="s1">max(m // n_batches</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">y = np.zeros((n - </span><span class="s4">1</span><span class="s2">, </span><span class="s1">n_qmc_samples))</span>
    <span class="s1">i_samples = np.arange(n_qmc_samples) + </span><span class="s4">1</span>
    <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(n_batches):</span>
        <span class="s1">c = np.full(n_qmc_samples</span><span class="s2">, </span><span class="s1">ci)</span>
        <span class="s1">dc = np.full(n_qmc_samples</span><span class="s2">, </span><span class="s1">dci)</span>
        <span class="s1">pv = dc.copy()</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">n):</span>
            <span class="s0"># Pseudorandomly-shifted lattice coordinate.</span>
            <span class="s1">z = q[i - </span><span class="s4">1</span><span class="s1">] * i_samples + rng.random()</span>
            <span class="s0"># Fast remainder(z, 1.0)</span>
            <span class="s1">z -= z.astype(int)</span>
            <span class="s0"># Tent periodization transform.</span>
            <span class="s1">x = abs(</span><span class="s4">2 </span><span class="s1">* z - </span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">y[i - </span><span class="s4">1</span><span class="s2">, </span><span class="s1">:] = phinv(c + x * dc)</span>
            <span class="s1">s = cho[i</span><span class="s2">, </span><span class="s1">:i] @ y[:i</span><span class="s2">, </span><span class="s1">:]</span>
            <span class="s1">ct = cho[i</span><span class="s2">, </span><span class="s1">i]</span>
            <span class="s1">c = phi((lo[i] - s) / ct)</span>
            <span class="s1">d = phi((hi[i] - s) / ct)</span>
            <span class="s1">dc = d - c</span>
            <span class="s1">pv = pv * dc</span>
        <span class="s0"># Accumulate the mean and error variances with online formulations.</span>
        <span class="s1">d = (pv.mean() - prob) / (j + </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">prob += d</span>
        <span class="s1">error_var = (j - </span><span class="s4">1</span><span class="s1">) * error_var / (j + </span><span class="s4">1</span><span class="s1">) + d * d</span>
    <span class="s0"># Error bounds are 3 times the standard error of the estimates.</span>
    <span class="s1">est_error = </span><span class="s4">3 </span><span class="s1">* np.sqrt(error_var)</span>
    <span class="s1">n_samples = n_qmc_samples * n_batches</span>
    <span class="s2">return </span><span class="s1">prob</span><span class="s2">, </span><span class="s1">est_error</span><span class="s2">, </span><span class="s1">n_samples</span>


<span class="s0"># Note: this function is not currently used or tested by any SciPy code. It is</span>
<span class="s0"># included in this file to facilitate the resolution of gh-8367, gh-16142, and</span>
<span class="s0"># possibly gh-14286, but must be reviewed and tested before use.</span>
<span class="s2">def </span><span class="s1">_mvn_qmc_integrand(covar</span><span class="s2">, </span><span class="s1">low</span><span class="s2">, </span><span class="s1">high</span><span class="s2">, </span><span class="s1">use_tent=</span><span class="s2">False</span><span class="s1">):</span>
    <span class="s3">&quot;&quot;&quot;Transform the multivariate normal integration into a QMC integrand over 
    a unit hypercube. 
 
    The dimensionality of the resulting hypercube integration domain is one 
    less than the dimensionality of the original integrand. Note that this 
    transformation subsumes the integration bounds in order to account for 
    infinite bounds. The QMC integration one does with the returned integrand 
    should be on the unit hypercube. 
 
    Parameters 
    ---------- 
    covar : (n, n) float array 
        Possibly singular, positive semidefinite symmetric covariance matrix. 
    low, high : (n,) float array 
        The low and high integration bounds. 
    use_tent : bool, optional 
        If True, then use tent periodization. Only helpful for lattice rules. 
 
    Returns 
    ------- 
    integrand : Callable[[NDArray], NDArray] 
        The QMC-integrable integrand. It takes an 
        ``(n_qmc_samples, ndim_integrand)`` array of QMC samples in the unit 
        hypercube and returns the ``(n_qmc_samples,)`` evaluations of at these 
        QMC points. 
    ndim_integrand : int 
        The dimensionality of the integrand. Equal to ``n-1``. 
    &quot;&quot;&quot;</span>
    <span class="s1">cho</span><span class="s2">, </span><span class="s1">lo</span><span class="s2">, </span><span class="s1">hi = _permuted_cholesky(covar</span><span class="s2">, </span><span class="s1">low</span><span class="s2">, </span><span class="s1">high)</span>
    <span class="s1">n = cho.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">ndim_integrand = n - </span><span class="s4">1</span>
    <span class="s1">ct = cho[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">c = phi(lo[</span><span class="s4">0</span><span class="s1">] / ct)</span>
    <span class="s1">d = phi(hi[</span><span class="s4">0</span><span class="s1">] / ct)</span>
    <span class="s1">ci = c</span>
    <span class="s1">dci = d - ci</span>

    <span class="s2">def </span><span class="s1">integrand(*zs):</span>
        <span class="s1">ndim_qmc = len(zs)</span>
        <span class="s1">n_qmc_samples = len(np.atleast_1d(zs[</span><span class="s4">0</span><span class="s1">]))</span>
        <span class="s2">assert </span><span class="s1">ndim_qmc == ndim_integrand</span>
        <span class="s1">y = np.zeros((ndim_qmc</span><span class="s2">, </span><span class="s1">n_qmc_samples))</span>
        <span class="s1">c = np.full(n_qmc_samples</span><span class="s2">, </span><span class="s1">ci)</span>
        <span class="s1">dc = np.full(n_qmc_samples</span><span class="s2">, </span><span class="s1">dci)</span>
        <span class="s1">pv = dc.copy()</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">n):</span>
            <span class="s2">if </span><span class="s1">use_tent:</span>
                <span class="s0"># Tent periodization transform.</span>
                <span class="s1">x = abs(</span><span class="s4">2 </span><span class="s1">* zs[i-</span><span class="s4">1</span><span class="s1">] - </span><span class="s4">1</span><span class="s1">)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">x = zs[i-</span><span class="s4">1</span><span class="s1">]</span>
            <span class="s1">y[i - </span><span class="s4">1</span><span class="s2">, </span><span class="s1">:] = phinv(c + x * dc)</span>
            <span class="s1">s = cho[i</span><span class="s2">, </span><span class="s1">:i] @ y[:i</span><span class="s2">, </span><span class="s1">:]</span>
            <span class="s1">ct = cho[i</span><span class="s2">, </span><span class="s1">i]</span>
            <span class="s1">c = phi((lo[i] - s) / ct)</span>
            <span class="s1">d = phi((hi[i] - s) / ct)</span>
            <span class="s1">dc = d - c</span>
            <span class="s1">pv = pv * dc</span>
        <span class="s2">return </span><span class="s1">pv</span>

    <span class="s2">return </span><span class="s1">integrand</span><span class="s2">, </span><span class="s1">ndim_integrand</span>


<span class="s2">def </span><span class="s1">_qmvt(m</span><span class="s2">, </span><span class="s1">nu</span><span class="s2">, </span><span class="s1">covar</span><span class="s2">, </span><span class="s1">low</span><span class="s2">, </span><span class="s1">high</span><span class="s2">, </span><span class="s1">rng</span><span class="s2">, </span><span class="s1">lattice=</span><span class="s5">'cbc'</span><span class="s2">, </span><span class="s1">n_batches=</span><span class="s4">10</span><span class="s1">):</span>
    <span class="s3">&quot;&quot;&quot;Multivariate t integration over box bounds. 
 
    Parameters 
    ---------- 
    m : int &gt; n_batches 
        The number of points to sample. This number will be divided into 
        `n_batches` batches that apply random offsets of the sampling lattice 
        for each batch in order to estimate the error. 
    nu : float &gt;= 0 
        The shape parameter of the multivariate t distribution. 
    covar : (n, n) float array 
        Possibly singular, positive semidefinite symmetric covariance matrix. 
    low, high : (n,) float array 
        The low and high integration bounds. 
    rng : Generator, optional 
        default_rng(), yada, yada 
    lattice : 'cbc' or callable 
        The type of lattice rule to use to construct the integration points. 
    n_batches : int &gt; 0, optional 
        The number of QMC batches to apply. 
 
    Returns 
    ------- 
    prob : float 
        The estimated probability mass within the bounds. 
    est_error : float 
        3 times the standard error of the batch estimates. 
    n_samples : int 
        The number of samples actually used. 
    &quot;&quot;&quot;</span>
    <span class="s1">sn = max(</span><span class="s4">1.0</span><span class="s2">, </span><span class="s1">np.sqrt(nu))</span>
    <span class="s1">low = np.asarray(low</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s1">high = np.asarray(high</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s1">cho</span><span class="s2">, </span><span class="s1">lo</span><span class="s2">, </span><span class="s1">hi = _permuted_cholesky(covar</span><span class="s2">, </span><span class="s1">low / sn</span><span class="s2">, </span><span class="s1">high / sn)</span>
    <span class="s1">n = cho.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">prob = </span><span class="s4">0.0</span>
    <span class="s1">error_var = </span><span class="s4">0.0</span>
    <span class="s1">q</span><span class="s2">, </span><span class="s1">n_qmc_samples = _cbc_lattice(n</span><span class="s2">, </span><span class="s1">max(m // n_batches</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">i_samples = np.arange(n_qmc_samples) + </span><span class="s4">1</span>
    <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(n_batches):</span>
        <span class="s1">pv = np.ones(n_qmc_samples)</span>
        <span class="s1">s = np.zeros((n</span><span class="s2">, </span><span class="s1">n_qmc_samples))</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n):</span>
            <span class="s0"># Pseudorandomly-shifted lattice coordinate.</span>
            <span class="s1">z = q[i] * i_samples + rng.random()</span>
            <span class="s0"># Fast remainder(z, 1.0)</span>
            <span class="s1">z -= z.astype(int)</span>
            <span class="s0"># Tent periodization transform.</span>
            <span class="s1">x = abs(</span><span class="s4">2 </span><span class="s1">* z - </span><span class="s4">1</span><span class="s1">)</span>
            <span class="s0"># FIXME: Lift the i==0 case out of the loop to make the logic</span>
            <span class="s0"># easier to follow.</span>
            <span class="s2">if </span><span class="s1">i == </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s0"># We'll use one of the QR variates to pull out the</span>
                <span class="s0"># t-distribution scaling.</span>
                <span class="s2">if </span><span class="s1">nu &gt; </span><span class="s4">0</span><span class="s1">:</span>
                    <span class="s1">r = np.sqrt(</span><span class="s4">2 </span><span class="s1">* gammaincinv(nu / </span><span class="s4">2</span><span class="s2">, </span><span class="s1">x))</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">r = np.ones_like(x)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">y = phinv(c + x * dc)  </span><span class="s0"># noqa: F821</span>
                <span class="s2">with </span><span class="s1">np.errstate(invalid=</span><span class="s5">'ignore'</span><span class="s1">):</span>
                    <span class="s1">s[i:</span><span class="s2">, </span><span class="s1">:] += cho[i:</span><span class="s2">, </span><span class="s1">i - </span><span class="s4">1</span><span class="s1">][:</span><span class="s2">, </span><span class="s1">np.newaxis] * y</span>
            <span class="s1">si = s[i</span><span class="s2">, </span><span class="s1">:]</span>

            <span class="s1">c = np.ones(n_qmc_samples)</span>
            <span class="s1">d = np.ones(n_qmc_samples)</span>
            <span class="s2">with </span><span class="s1">np.errstate(invalid=</span><span class="s5">'ignore'</span><span class="s1">):</span>
                <span class="s1">lois = lo[i] * r - si</span>
                <span class="s1">hiis = hi[i] * r - si</span>
            <span class="s1">c[lois &lt; -</span><span class="s4">9</span><span class="s1">] = </span><span class="s4">0.0</span>
            <span class="s1">d[hiis &lt; -</span><span class="s4">9</span><span class="s1">] = </span><span class="s4">0.0</span>
            <span class="s1">lo_mask = abs(lois) &lt; </span><span class="s4">9</span>
            <span class="s1">hi_mask = abs(hiis) &lt; </span><span class="s4">9</span>
            <span class="s1">c[lo_mask] = phi(lois[lo_mask])</span>
            <span class="s1">d[hi_mask] = phi(hiis[hi_mask])</span>

            <span class="s1">dc = d - c</span>
            <span class="s1">pv *= dc</span>

        <span class="s0"># Accumulate the mean and error variances with online formulations.</span>
        <span class="s1">d = (pv.mean() - prob) / (j + </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">prob += d</span>
        <span class="s1">error_var = (j - </span><span class="s4">1</span><span class="s1">) * error_var / (j + </span><span class="s4">1</span><span class="s1">) + d * d</span>
    <span class="s0"># Error bounds are 3 times the standard error of the estimates.</span>
    <span class="s1">est_error = </span><span class="s4">3 </span><span class="s1">* np.sqrt(error_var)</span>
    <span class="s1">n_samples = n_qmc_samples * n_batches</span>
    <span class="s2">return </span><span class="s1">prob</span><span class="s2">, </span><span class="s1">est_error</span><span class="s2">, </span><span class="s1">n_samples</span>


<span class="s2">def </span><span class="s1">_permuted_cholesky(covar</span><span class="s2">, </span><span class="s1">low</span><span class="s2">, </span><span class="s1">high</span><span class="s2">, </span><span class="s1">tol=</span><span class="s4">1e-10</span><span class="s1">):</span>
    <span class="s3">&quot;&quot;&quot;Compute a scaled, permuted Cholesky factor, with integration bounds. 
 
    The scaling and permuting of the dimensions accomplishes part of the 
    transformation of the original integration problem into a more numerically 
    tractable form. The lower-triangular Cholesky factor will then be used in 
    the subsequent integration. The integration bounds will be scaled and 
    permuted as well. 
 
    Parameters 
    ---------- 
    covar : (n, n) float array 
        Possibly singular, positive semidefinite symmetric covariance matrix. 
    low, high : (n,) float array 
        The low and high integration bounds. 
    tol : float, optional 
        The singularity tolerance. 
 
    Returns 
    ------- 
    cho : (n, n) float array 
        Lower Cholesky factor, scaled and permuted. 
    new_low, new_high : (n,) float array 
        The scaled and permuted low and high integration bounds. 
    &quot;&quot;&quot;</span>
    <span class="s0"># Make copies for outputting.</span>
    <span class="s1">cho = np.array(covar</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s1">new_lo = np.array(low</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s1">new_hi = np.array(high</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s1">n = cho.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s2">if </span><span class="s1">cho.shape != (n</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;expected a square symmetric array&quot;</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">new_lo.shape != (n</span><span class="s2">,</span><span class="s1">) </span><span class="s2">or </span><span class="s1">new_hi.shape != (n</span><span class="s2">,</span><span class="s1">):</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span>
            <span class="s5">&quot;expected integration boundaries the same dimensions &quot;</span>
            <span class="s5">&quot;as the covariance matrix&quot;</span>
        <span class="s1">)</span>
    <span class="s0"># Scale by the sqrt of the diagonal.</span>
    <span class="s1">dc = np.sqrt(np.maximum(np.diag(cho)</span><span class="s2">, </span><span class="s4">0.0</span><span class="s1">))</span>
    <span class="s0"># But don't divide by 0.</span>
    <span class="s1">dc[dc == </span><span class="s4">0.0</span><span class="s1">] = </span><span class="s4">1.0</span>
    <span class="s1">new_lo /= dc</span>
    <span class="s1">new_hi /= dc</span>
    <span class="s1">cho /= dc</span>
    <span class="s1">cho /= dc[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>

    <span class="s1">y = np.zeros(n)</span>
    <span class="s1">sqtp = np.sqrt(</span><span class="s4">2 </span><span class="s1">* np.pi)</span>
    <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(n):</span>
        <span class="s1">epk = (k + </span><span class="s4">1</span><span class="s1">) * tol</span>
        <span class="s1">im = k</span>
        <span class="s1">ck = </span><span class="s4">0.0</span>
        <span class="s1">dem = </span><span class="s4">1.0</span>
        <span class="s1">s = </span><span class="s4">0.0</span>
        <span class="s1">lo_m = </span><span class="s4">0.0</span>
        <span class="s1">hi_m = </span><span class="s4">0.0</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(k</span><span class="s2">, </span><span class="s1">n):</span>
            <span class="s2">if </span><span class="s1">cho[i</span><span class="s2">, </span><span class="s1">i] &gt; tol:</span>
                <span class="s1">ci = np.sqrt(cho[i</span><span class="s2">, </span><span class="s1">i])</span>
                <span class="s2">if </span><span class="s1">i &gt; </span><span class="s4">0</span><span class="s1">:</span>
                    <span class="s1">s = cho[i</span><span class="s2">, </span><span class="s1">:k] @ y[:k]</span>
                <span class="s1">lo_i = (new_lo[i] - s) / ci</span>
                <span class="s1">hi_i = (new_hi[i] - s) / ci</span>
                <span class="s1">de = phi(hi_i) - phi(lo_i)</span>
                <span class="s2">if </span><span class="s1">de &lt;= dem:</span>
                    <span class="s1">ck = ci</span>
                    <span class="s1">dem = de</span>
                    <span class="s1">lo_m = lo_i</span>
                    <span class="s1">hi_m = hi_i</span>
                    <span class="s1">im = i</span>
        <span class="s2">if </span><span class="s1">im &gt; k:</span>
            <span class="s0"># Swap im and k</span>
            <span class="s1">cho[im</span><span class="s2">, </span><span class="s1">im] = cho[k</span><span class="s2">, </span><span class="s1">k]</span>
            <span class="s1">_swap_slices(cho</span><span class="s2">, </span><span class="s1">np.s_[im</span><span class="s2">, </span><span class="s1">:k]</span><span class="s2">, </span><span class="s1">np.s_[k</span><span class="s2">, </span><span class="s1">:k])</span>
            <span class="s1">_swap_slices(cho</span><span class="s2">, </span><span class="s1">np.s_[im + </span><span class="s4">1</span><span class="s1">:</span><span class="s2">, </span><span class="s1">im]</span><span class="s2">, </span><span class="s1">np.s_[im + </span><span class="s4">1</span><span class="s1">:</span><span class="s2">, </span><span class="s1">k])</span>
            <span class="s1">_swap_slices(cho</span><span class="s2">, </span><span class="s1">np.s_[k + </span><span class="s4">1</span><span class="s1">:im</span><span class="s2">, </span><span class="s1">k]</span><span class="s2">, </span><span class="s1">np.s_[im</span><span class="s2">, </span><span class="s1">k + </span><span class="s4">1</span><span class="s1">:im])</span>
            <span class="s1">_swap_slices(new_lo</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">im)</span>
            <span class="s1">_swap_slices(new_hi</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">im)</span>
        <span class="s2">if </span><span class="s1">ck &gt; epk:</span>
            <span class="s1">cho[k</span><span class="s2">, </span><span class="s1">k] = ck</span>
            <span class="s1">cho[k</span><span class="s2">, </span><span class="s1">k + </span><span class="s4">1</span><span class="s1">:] = </span><span class="s4">0.0</span>
            <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(k + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">n):</span>
                <span class="s1">cho[i</span><span class="s2">, </span><span class="s1">k] /= ck</span>
                <span class="s1">cho[i</span><span class="s2">, </span><span class="s1">k + </span><span class="s4">1</span><span class="s1">:i + </span><span class="s4">1</span><span class="s1">] -= cho[i</span><span class="s2">, </span><span class="s1">k] * cho[k + </span><span class="s4">1</span><span class="s1">:i + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">k]</span>
            <span class="s2">if </span><span class="s1">abs(dem) &gt; tol:</span>
                <span class="s1">y[k] = ((np.exp(-lo_m * lo_m / </span><span class="s4">2</span><span class="s1">) - np.exp(-hi_m * hi_m / </span><span class="s4">2</span><span class="s1">)) /</span>
                        <span class="s1">(sqtp * dem))</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">y[k] = (lo_m + hi_m) / </span><span class="s4">2</span>
                <span class="s2">if </span><span class="s1">lo_m &lt; -</span><span class="s4">10</span><span class="s1">:</span>
                    <span class="s1">y[k] = hi_m</span>
                <span class="s2">elif </span><span class="s1">hi_m &gt; </span><span class="s4">10</span><span class="s1">:</span>
                    <span class="s1">y[k] = lo_m</span>
            <span class="s1">cho[k</span><span class="s2">, </span><span class="s1">:k + </span><span class="s4">1</span><span class="s1">] /= ck</span>
            <span class="s1">new_lo[k] /= ck</span>
            <span class="s1">new_hi[k] /= ck</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">cho[k:</span><span class="s2">, </span><span class="s1">k] = </span><span class="s4">0.0</span>
            <span class="s1">y[k] = (new_lo[k] + new_hi[k]) / </span><span class="s4">2</span>
    <span class="s2">return </span><span class="s1">cho</span><span class="s2">, </span><span class="s1">new_lo</span><span class="s2">, </span><span class="s1">new_hi</span>


<span class="s2">def </span><span class="s1">_swap_slices(x</span><span class="s2">, </span><span class="s1">slc1</span><span class="s2">, </span><span class="s1">slc2):</span>
    <span class="s1">t = x[slc1].copy()</span>
    <span class="s1">x[slc1] = x[slc2].copy()</span>
    <span class="s1">x[slc2] = t</span>
</pre>
</body>
</html>