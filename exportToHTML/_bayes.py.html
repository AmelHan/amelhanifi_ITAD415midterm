<html>
<head>
<title>_bayes.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_bayes.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Various bayesian regression 
&quot;&quot;&quot;</span>

<span class="s2"># Authors: V. Michel, F. Pedregosa, A. Gramfort</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">math </span><span class="s3">import </span><span class="s1">log</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">linalg</span>
<span class="s3">from </span><span class="s1">scipy.linalg </span><span class="s3">import </span><span class="s1">pinvh</span>

<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">RegressorMixin</span><span class="s3">, </span><span class="s1">_fit_context</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">Hidden</span><span class="s3">, </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">StrOptions</span>
<span class="s3">from </span><span class="s1">..utils.extmath </span><span class="s3">import </span><span class="s1">fast_logdet</span>
<span class="s3">from </span><span class="s1">..utils.validation </span><span class="s3">import </span><span class="s1">_check_sample_weight</span>
<span class="s3">from </span><span class="s1">._base </span><span class="s3">import </span><span class="s1">LinearModel</span><span class="s3">, </span><span class="s1">_preprocess_data</span><span class="s3">, </span><span class="s1">_rescale_data</span>


<span class="s2"># TODO(1.5) Remove</span>
<span class="s3">def </span><span class="s1">_deprecate_n_iter(n_iter</span><span class="s3">, </span><span class="s1">max_iter):</span>
    <span class="s0">&quot;&quot;&quot;Deprecates n_iter in favour of max_iter. Checks if the n_iter has been 
    used instead of max_iter and generates a deprecation warning if True. 
 
    Parameters 
    ---------- 
    n_iter : int, 
        Value of n_iter attribute passed by the estimator. 
 
    max_iter : int, default=None 
        Value of max_iter attribute passed by the estimator. 
        If `None`, it corresponds to `max_iter=300`. 
 
    Returns 
    ------- 
    max_iter : int, 
        Value of max_iter which shall further be used by the estimator. 
 
    Notes 
    ----- 
    This function should be completely removed in 1.5. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">n_iter != </span><span class="s4">&quot;deprecated&quot;</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">max_iter </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;Both `n_iter` and `max_iter` attributes were set. Attribute&quot;</span>
                <span class="s4">&quot; `n_iter` was deprecated in version 1.3 and will be removed in&quot;</span>
                <span class="s4">&quot; 1.5. To avoid this error, only set the `max_iter` attribute.&quot;</span>
            <span class="s1">)</span>
        <span class="s1">warnings.warn(</span>
            <span class="s1">(</span>
                <span class="s4">&quot;'n_iter' was renamed to 'max_iter' in version 1.3 and &quot;</span>
                <span class="s4">&quot;will be removed in 1.5&quot;</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">FutureWarning</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">max_iter = n_iter</span>
    <span class="s3">elif </span><span class="s1">max_iter </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">max_iter = </span><span class="s5">300</span>
    <span class="s3">return </span><span class="s1">max_iter</span>


<span class="s2">###############################################################################</span>
<span class="s2"># BayesianRidge regression</span>


<span class="s3">class </span><span class="s1">BayesianRidge(RegressorMixin</span><span class="s3">, </span><span class="s1">LinearModel):</span>
    <span class="s0">&quot;&quot;&quot;Bayesian ridge regression. 
 
    Fit a Bayesian ridge model. See the Notes section for details on this 
    implementation and the optimization of the regularization parameters 
    lambda (precision of the weights) and alpha (precision of the noise). 
 
    Read more in the :ref:`User Guide &lt;bayesian_regression&gt;`. 
 
    Parameters 
    ---------- 
    max_iter : int, default=None 
        Maximum number of iterations over the complete dataset before 
        stopping independently of any early stopping criterion. If `None`, it 
        corresponds to `max_iter=300`. 
 
        .. versionchanged:: 1.3 
 
    tol : float, default=1e-3 
        Stop the algorithm if w has converged. 
 
    alpha_1 : float, default=1e-6 
        Hyper-parameter : shape parameter for the Gamma distribution prior 
        over the alpha parameter. 
 
    alpha_2 : float, default=1e-6 
        Hyper-parameter : inverse scale parameter (rate parameter) for the 
        Gamma distribution prior over the alpha parameter. 
 
    lambda_1 : float, default=1e-6 
        Hyper-parameter : shape parameter for the Gamma distribution prior 
        over the lambda parameter. 
 
    lambda_2 : float, default=1e-6 
        Hyper-parameter : inverse scale parameter (rate parameter) for the 
        Gamma distribution prior over the lambda parameter. 
 
    alpha_init : float, default=None 
        Initial value for alpha (precision of the noise). 
        If not set, alpha_init is 1/Var(y). 
 
            .. versionadded:: 0.22 
 
    lambda_init : float, default=None 
        Initial value for lambda (precision of the weights). 
        If not set, lambda_init is 1. 
 
            .. versionadded:: 0.22 
 
    compute_score : bool, default=False 
        If True, compute the log marginal likelihood at each iteration of the 
        optimization. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. 
        The intercept is not treated as a probabilistic parameter 
        and thus has no associated variance. If set 
        to False, no intercept will be used in calculations 
        (i.e. data is expected to be centered). 
 
    copy_X : bool, default=True 
        If True, X will be copied; else, it may be overwritten. 
 
    verbose : bool, default=False 
        Verbose mode when fitting the model. 
 
    n_iter : int 
        Maximum number of iterations. Should be greater than or equal to 1. 
 
        .. deprecated:: 1.3 
           `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use 
           `max_iter` instead. 
 
    Attributes 
    ---------- 
    coef_ : array-like of shape (n_features,) 
        Coefficients of the regression model (mean of distribution) 
 
    intercept_ : float 
        Independent term in decision function. Set to 0.0 if 
        `fit_intercept = False`. 
 
    alpha_ : float 
       Estimated precision of the noise. 
 
    lambda_ : float 
       Estimated precision of the weights. 
 
    sigma_ : array-like of shape (n_features, n_features) 
        Estimated variance-covariance matrix of the weights 
 
    scores_ : array-like of shape (n_iter_+1,) 
        If computed_score is True, value of the log marginal likelihood (to be 
        maximized) at each iteration of the optimization. The array starts 
        with the value of the log marginal likelihood obtained for the initial 
        values of alpha and lambda and ends with the value obtained for the 
        estimated alpha and lambda. 
 
    n_iter_ : int 
        The actual number of iterations to reach the stopping criterion. 
 
    X_offset_ : ndarray of shape (n_features,) 
        If `fit_intercept=True`, offset subtracted for centering data to a 
        zero mean. Set to np.zeros(n_features) otherwise. 
 
    X_scale_ : ndarray of shape (n_features,) 
        Set to np.ones(n_features). 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    ARDRegression : Bayesian ARD regression. 
 
    Notes 
    ----- 
    There exist several strategies to perform Bayesian ridge regression. This 
    implementation is based on the algorithm described in Appendix A of 
    (Tipping, 2001) where updates of the regularization parameters are done as 
    suggested in (MacKay, 1992). Note that according to A New 
    View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these 
    update rules do not guarantee that the marginal likelihood is increasing 
    between two consecutive iterations of the optimization. 
 
    References 
    ---------- 
    D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems, 
    Vol. 4, No. 3, 1992. 
 
    M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine, 
    Journal of Machine Learning Research, Vol. 1, 2001. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn import linear_model 
    &gt;&gt;&gt; clf = linear_model.BayesianRidge() 
    &gt;&gt;&gt; clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2]) 
    BayesianRidge() 
    &gt;&gt;&gt; clf.predict([[1, 1]]) 
    array([1.]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;alpha_1&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;alpha_2&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;lambda_1&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;lambda_2&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;alpha_init&quot;</span><span class="s1">: [</span><span class="s3">None, </span><span class="s1">Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;lambda_init&quot;</span><span class="s1">: [</span><span class="s3">None, </span><span class="s1">Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;compute_score&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;fit_intercept&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;copy_X&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s4">&quot;verbose&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_iter&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">Hidden(StrOptions({</span><span class="s4">&quot;deprecated&quot;</span><span class="s1">}))</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s3">None,  </span><span class="s2"># TODO(1.5): Set to 300</span>
        <span class="s1">tol=</span><span class="s5">1.0e-3</span><span class="s3">,</span>
        <span class="s1">alpha_1=</span><span class="s5">1.0e-6</span><span class="s3">,</span>
        <span class="s1">alpha_2=</span><span class="s5">1.0e-6</span><span class="s3">,</span>
        <span class="s1">lambda_1=</span><span class="s5">1.0e-6</span><span class="s3">,</span>
        <span class="s1">lambda_2=</span><span class="s5">1.0e-6</span><span class="s3">,</span>
        <span class="s1">alpha_init=</span><span class="s3">None,</span>
        <span class="s1">lambda_init=</span><span class="s3">None,</span>
        <span class="s1">compute_score=</span><span class="s3">False,</span>
        <span class="s1">fit_intercept=</span><span class="s3">True,</span>
        <span class="s1">copy_X=</span><span class="s3">True,</span>
        <span class="s1">verbose=</span><span class="s3">False,</span>
        <span class="s1">n_iter=</span><span class="s4">&quot;deprecated&quot;</span><span class="s3">,  </span><span class="s2"># TODO(1.5): Remove</span>
    <span class="s1">):</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.alpha_1 = alpha_1</span>
        <span class="s1">self.alpha_2 = alpha_2</span>
        <span class="s1">self.lambda_1 = lambda_1</span>
        <span class="s1">self.lambda_2 = lambda_2</span>
        <span class="s1">self.alpha_init = alpha_init</span>
        <span class="s1">self.lambda_init = lambda_init</span>
        <span class="s1">self.compute_score = compute_score</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.copy_X = copy_X</span>
        <span class="s1">self.verbose = verbose</span>
        <span class="s1">self.n_iter = n_iter</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples, n_features) 
            Training data. 
        y : ndarray of shape (n_samples,) 
            Target values. Will be cast to X's dtype if necessary. 
 
        sample_weight : ndarray of shape (n_samples,), default=None 
            Individual weights for each sample. 
 
            .. versionadded:: 0.20 
               parameter *sample_weight* support to BayesianRidge. 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">max_iter = _deprecate_n_iter(self.n_iter</span><span class="s3">, </span><span class="s1">self.max_iter)</span>

        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">, </span><span class="s1">y_numeric=</span><span class="s3">True</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">X_offset_</span><span class="s3">, </span><span class="s1">y_offset_</span><span class="s3">, </span><span class="s1">X_scale_ = _preprocess_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">self.fit_intercept</span><span class="s3">,</span>
            <span class="s1">copy=self.copy_X</span><span class="s3">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s2"># Sample weight can be implemented via a simple rescaling.</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">_ = _rescale_data(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight)</span>

        <span class="s1">self.X_offset_ = X_offset_</span>
        <span class="s1">self.X_scale_ = X_scale_</span>
        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>

        <span class="s2"># Initialization of the values of the parameters</span>
        <span class="s1">eps = np.finfo(np.float64).eps</span>
        <span class="s2"># Add `eps` in the denominator to omit division by zero if `np.var(y)`</span>
        <span class="s2"># is zero</span>
        <span class="s1">alpha_ = self.alpha_init</span>
        <span class="s1">lambda_ = self.lambda_init</span>
        <span class="s3">if </span><span class="s1">alpha_ </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">alpha_ = </span><span class="s5">1.0 </span><span class="s1">/ (np.var(y) + eps)</span>
        <span class="s3">if </span><span class="s1">lambda_ </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">lambda_ = </span><span class="s5">1.0</span>

        <span class="s1">verbose = self.verbose</span>
        <span class="s1">lambda_1 = self.lambda_1</span>
        <span class="s1">lambda_2 = self.lambda_2</span>
        <span class="s1">alpha_1 = self.alpha_1</span>
        <span class="s1">alpha_2 = self.alpha_2</span>

        <span class="s1">self.scores_ = list()</span>
        <span class="s1">coef_old_ = </span><span class="s3">None</span>

        <span class="s1">XT_y = np.dot(X.T</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s1">U</span><span class="s3">, </span><span class="s1">S</span><span class="s3">, </span><span class="s1">Vh = linalg.svd(X</span><span class="s3">, </span><span class="s1">full_matrices=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">eigen_vals_ = S**</span><span class="s5">2</span>

        <span class="s2"># Convergence loop of the bayesian ridge regression</span>
        <span class="s3">for </span><span class="s1">iter_ </span><span class="s3">in </span><span class="s1">range(max_iter):</span>
            <span class="s2"># update posterior mean coef_ based on alpha_ and lambda_ and</span>
            <span class="s2"># compute corresponding rmse</span>
            <span class="s1">coef_</span><span class="s3">, </span><span class="s1">rmse_ = self._update_coef_(</span>
                <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">, </span><span class="s1">XT_y</span><span class="s3">, </span><span class="s1">U</span><span class="s3">, </span><span class="s1">Vh</span><span class="s3">, </span><span class="s1">eigen_vals_</span><span class="s3">, </span><span class="s1">alpha_</span><span class="s3">, </span><span class="s1">lambda_</span>
            <span class="s1">)</span>
            <span class="s3">if </span><span class="s1">self.compute_score:</span>
                <span class="s2"># compute the log marginal likelihood</span>
                <span class="s1">s = self._log_marginal_likelihood(</span>
                    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">, </span><span class="s1">eigen_vals_</span><span class="s3">, </span><span class="s1">alpha_</span><span class="s3">, </span><span class="s1">lambda_</span><span class="s3">, </span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">rmse_</span>
                <span class="s1">)</span>
                <span class="s1">self.scores_.append(s)</span>

            <span class="s2"># Update alpha and lambda according to (MacKay, 1992)</span>
            <span class="s1">gamma_ = np.sum((alpha_ * eigen_vals_) / (lambda_ + alpha_ * eigen_vals_))</span>
            <span class="s1">lambda_ = (gamma_ + </span><span class="s5">2 </span><span class="s1">* lambda_1) / (np.sum(coef_**</span><span class="s5">2</span><span class="s1">) + </span><span class="s5">2 </span><span class="s1">* lambda_2)</span>
            <span class="s1">alpha_ = (n_samples - gamma_ + </span><span class="s5">2 </span><span class="s1">* alpha_1) / (rmse_ + </span><span class="s5">2 </span><span class="s1">* alpha_2)</span>

            <span class="s2"># Check for convergence</span>
            <span class="s3">if </span><span class="s1">iter_ != </span><span class="s5">0 </span><span class="s3">and </span><span class="s1">np.sum(np.abs(coef_old_ - coef_)) &lt; self.tol:</span>
                <span class="s3">if </span><span class="s1">verbose:</span>
                    <span class="s1">print(</span><span class="s4">&quot;Convergence after &quot;</span><span class="s3">, </span><span class="s1">str(iter_)</span><span class="s3">, </span><span class="s4">&quot; iterations&quot;</span><span class="s1">)</span>
                <span class="s3">break</span>
            <span class="s1">coef_old_ = np.copy(coef_)</span>

        <span class="s1">self.n_iter_ = iter_ + </span><span class="s5">1</span>

        <span class="s2"># return regularization parameters and corresponding posterior mean,</span>
        <span class="s2"># log marginal likelihood and posterior covariance</span>
        <span class="s1">self.alpha_ = alpha_</span>
        <span class="s1">self.lambda_ = lambda_</span>
        <span class="s1">self.coef_</span><span class="s3">, </span><span class="s1">rmse_ = self._update_coef_(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">, </span><span class="s1">XT_y</span><span class="s3">, </span><span class="s1">U</span><span class="s3">, </span><span class="s1">Vh</span><span class="s3">, </span><span class="s1">eigen_vals_</span><span class="s3">, </span><span class="s1">alpha_</span><span class="s3">, </span><span class="s1">lambda_</span>
        <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">self.compute_score:</span>
            <span class="s2"># compute the log marginal likelihood</span>
            <span class="s1">s = self._log_marginal_likelihood(</span>
                <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">, </span><span class="s1">eigen_vals_</span><span class="s3">, </span><span class="s1">alpha_</span><span class="s3">, </span><span class="s1">lambda_</span><span class="s3">, </span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">rmse_</span>
            <span class="s1">)</span>
            <span class="s1">self.scores_.append(s)</span>
            <span class="s1">self.scores_ = np.array(self.scores_)</span>

        <span class="s2"># posterior covariance is given by 1/alpha_ * scaled_sigma_</span>
        <span class="s1">scaled_sigma_ = np.dot(</span>
            <span class="s1">Vh.T</span><span class="s3">, </span><span class="s1">Vh / (eigen_vals_ + lambda_ / alpha_)[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">)</span>
        <span class="s1">self.sigma_ = (</span><span class="s5">1.0 </span><span class="s1">/ alpha_) * scaled_sigma_</span>

        <span class="s1">self._set_intercept(X_offset_</span><span class="s3">, </span><span class="s1">y_offset_</span><span class="s3">, </span><span class="s1">X_scale_)</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Predict using the linear model. 
 
        In addition to the mean of the predictive distribution, also its 
        standard deviation can be returned. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Samples. 
 
        return_std : bool, default=False 
            Whether to return the standard deviation of posterior prediction. 
 
        Returns 
        ------- 
        y_mean : array-like of shape (n_samples,) 
            Mean of predictive distribution of query points. 
 
        y_std : array-like of shape (n_samples,) 
            Standard deviation of predictive distribution of query points. 
        &quot;&quot;&quot;</span>
        <span class="s1">y_mean = self._decision_function(X)</span>
        <span class="s3">if not </span><span class="s1">return_std:</span>
            <span class="s3">return </span><span class="s1">y_mean</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">sigmas_squared_data = (np.dot(X</span><span class="s3">, </span><span class="s1">self.sigma_) * X).sum(axis=</span><span class="s5">1</span><span class="s1">)</span>
            <span class="s1">y_std = np.sqrt(sigmas_squared_data + (</span><span class="s5">1.0 </span><span class="s1">/ self.alpha_))</span>
            <span class="s3">return </span><span class="s1">y_mean</span><span class="s3">, </span><span class="s1">y_std</span>

    <span class="s3">def </span><span class="s1">_update_coef_(</span>
        <span class="s1">self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">, </span><span class="s1">XT_y</span><span class="s3">, </span><span class="s1">U</span><span class="s3">, </span><span class="s1">Vh</span><span class="s3">, </span><span class="s1">eigen_vals_</span><span class="s3">, </span><span class="s1">alpha_</span><span class="s3">, </span><span class="s1">lambda_</span>
    <span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Update posterior mean and compute corresponding rmse. 
 
        Posterior mean is given by coef_ = scaled_sigma_ * X.T * y where 
        scaled_sigma_ = (lambda_/alpha_ * np.eye(n_features) 
                         + np.dot(X.T, X))^-1 
        &quot;&quot;&quot;</span>

        <span class="s3">if </span><span class="s1">n_samples &gt; n_features:</span>
            <span class="s1">coef_ = np.linalg.multi_dot(</span>
                <span class="s1">[Vh.T</span><span class="s3">, </span><span class="s1">Vh / (eigen_vals_ + lambda_ / alpha_)[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span><span class="s3">, </span><span class="s1">XT_y]</span>
            <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">coef_ = np.linalg.multi_dot(</span>
                <span class="s1">[X.T</span><span class="s3">, </span><span class="s1">U / (eigen_vals_ + lambda_ / alpha_)[</span><span class="s3">None, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">U.T</span><span class="s3">, </span><span class="s1">y]</span>
            <span class="s1">)</span>

        <span class="s1">rmse_ = np.sum((y - np.dot(X</span><span class="s3">, </span><span class="s1">coef_)) ** </span><span class="s5">2</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">rmse_</span>

    <span class="s3">def </span><span class="s1">_log_marginal_likelihood(</span>
        <span class="s1">self</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">, </span><span class="s1">eigen_vals</span><span class="s3">, </span><span class="s1">alpha_</span><span class="s3">, </span><span class="s1">lambda_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">rmse</span>
    <span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Log marginal likelihood.&quot;&quot;&quot;</span>
        <span class="s1">alpha_1 = self.alpha_1</span>
        <span class="s1">alpha_2 = self.alpha_2</span>
        <span class="s1">lambda_1 = self.lambda_1</span>
        <span class="s1">lambda_2 = self.lambda_2</span>

        <span class="s2"># compute the log of the determinant of the posterior covariance.</span>
        <span class="s2"># posterior covariance is given by</span>
        <span class="s2"># sigma = (lambda_ * np.eye(n_features) + alpha_ * np.dot(X.T, X))^-1</span>
        <span class="s3">if </span><span class="s1">n_samples &gt; n_features:</span>
            <span class="s1">logdet_sigma = -np.sum(np.log(lambda_ + alpha_ * eigen_vals))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">logdet_sigma = np.full(n_features</span><span class="s3">, </span><span class="s1">lambda_</span><span class="s3">, </span><span class="s1">dtype=np.array(lambda_).dtype)</span>
            <span class="s1">logdet_sigma[:n_samples] += alpha_ * eigen_vals</span>
            <span class="s1">logdet_sigma = -np.sum(np.log(logdet_sigma))</span>

        <span class="s1">score = lambda_1 * log(lambda_) - lambda_2 * lambda_</span>
        <span class="s1">score += alpha_1 * log(alpha_) - alpha_2 * alpha_</span>
        <span class="s1">score += </span><span class="s5">0.5 </span><span class="s1">* (</span>
            <span class="s1">n_features * log(lambda_)</span>
            <span class="s1">+ n_samples * log(alpha_)</span>
            <span class="s1">- alpha_ * rmse</span>
            <span class="s1">- lambda_ * np.sum(coef**</span><span class="s5">2</span><span class="s1">)</span>
            <span class="s1">+ logdet_sigma</span>
            <span class="s1">- n_samples * log(</span><span class="s5">2 </span><span class="s1">* np.pi)</span>
        <span class="s1">)</span>

        <span class="s3">return </span><span class="s1">score</span>


<span class="s2">###############################################################################</span>
<span class="s2"># ARD (Automatic Relevance Determination) regression</span>


<span class="s3">class </span><span class="s1">ARDRegression(RegressorMixin</span><span class="s3">, </span><span class="s1">LinearModel):</span>
    <span class="s0">&quot;&quot;&quot;Bayesian ARD regression. 
 
    Fit the weights of a regression model, using an ARD prior. The weights of 
    the regression model are assumed to be in Gaussian distributions. 
    Also estimate the parameters lambda (precisions of the distributions of the 
    weights) and alpha (precision of the distribution of the noise). 
    The estimation is done by an iterative procedures (Evidence Maximization) 
 
    Read more in the :ref:`User Guide &lt;bayesian_regression&gt;`. 
 
    Parameters 
    ---------- 
    max_iter : int, default=None 
        Maximum number of iterations. If `None`, it corresponds to `max_iter=300`. 
 
        .. versionchanged:: 1.3 
 
    tol : float, default=1e-3 
        Stop the algorithm if w has converged. 
 
    alpha_1 : float, default=1e-6 
        Hyper-parameter : shape parameter for the Gamma distribution prior 
        over the alpha parameter. 
 
    alpha_2 : float, default=1e-6 
        Hyper-parameter : inverse scale parameter (rate parameter) for the 
        Gamma distribution prior over the alpha parameter. 
 
    lambda_1 : float, default=1e-6 
        Hyper-parameter : shape parameter for the Gamma distribution prior 
        over the lambda parameter. 
 
    lambda_2 : float, default=1e-6 
        Hyper-parameter : inverse scale parameter (rate parameter) for the 
        Gamma distribution prior over the lambda parameter. 
 
    compute_score : bool, default=False 
        If True, compute the objective function at each step of the model. 
 
    threshold_lambda : float, default=10 000 
        Threshold for removing (pruning) weights with high precision from 
        the computation. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. If set 
        to false, no intercept will be used in calculations 
        (i.e. data is expected to be centered). 
 
    copy_X : bool, default=True 
        If True, X will be copied; else, it may be overwritten. 
 
    verbose : bool, default=False 
        Verbose mode when fitting the model. 
 
    n_iter : int 
        Maximum number of iterations. 
 
        .. deprecated:: 1.3 
           `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use 
           `max_iter` instead. 
 
    Attributes 
    ---------- 
    coef_ : array-like of shape (n_features,) 
        Coefficients of the regression model (mean of distribution) 
 
    alpha_ : float 
       estimated precision of the noise. 
 
    lambda_ : array-like of shape (n_features,) 
       estimated precisions of the weights. 
 
    sigma_ : array-like of shape (n_features, n_features) 
        estimated variance-covariance matrix of the weights 
 
    scores_ : float 
        if computed, value of the objective function (to be maximized) 
 
    n_iter_ : int 
        The actual number of iterations to reach the stopping criterion. 
 
        .. versionadded:: 1.3 
 
    intercept_ : float 
        Independent term in decision function. Set to 0.0 if 
        ``fit_intercept = False``. 
 
    X_offset_ : float 
        If `fit_intercept=True`, offset subtracted for centering data to a 
        zero mean. Set to np.zeros(n_features) otherwise. 
 
    X_scale_ : float 
        Set to np.ones(n_features). 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    BayesianRidge : Bayesian ridge regression. 
 
    Notes 
    ----- 
    For an example, see :ref:`examples/linear_model/plot_ard.py 
    &lt;sphx_glr_auto_examples_linear_model_plot_ard.py&gt;`. 
 
    References 
    ---------- 
    D. J. C. MacKay, Bayesian nonlinear modeling for the prediction 
    competition, ASHRAE Transactions, 1994. 
 
    R. Salakhutdinov, Lecture notes on Statistical Machine Learning, 
    http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15 
    Their beta is our ``self.alpha_`` 
    Their alpha is our ``self.lambda_`` 
    ARD is a little different than the slide: only dimensions/features for 
    which ``self.lambda_ &lt; self.threshold_lambda`` are kept and the rest are 
    discarded. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn import linear_model 
    &gt;&gt;&gt; clf = linear_model.ARDRegression() 
    &gt;&gt;&gt; clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2]) 
    ARDRegression() 
    &gt;&gt;&gt; clf.predict([[1, 1]]) 
    array([1.]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;alpha_1&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;alpha_2&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;lambda_1&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;lambda_2&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;compute_score&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;threshold_lambda&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;fit_intercept&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;copy_X&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s4">&quot;verbose&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_iter&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">Hidden(StrOptions({</span><span class="s4">&quot;deprecated&quot;</span><span class="s1">}))</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s3">None,  </span><span class="s2"># TODO(1.5): Set to 300</span>
        <span class="s1">tol=</span><span class="s5">1.0e-3</span><span class="s3">,</span>
        <span class="s1">alpha_1=</span><span class="s5">1.0e-6</span><span class="s3">,</span>
        <span class="s1">alpha_2=</span><span class="s5">1.0e-6</span><span class="s3">,</span>
        <span class="s1">lambda_1=</span><span class="s5">1.0e-6</span><span class="s3">,</span>
        <span class="s1">lambda_2=</span><span class="s5">1.0e-6</span><span class="s3">,</span>
        <span class="s1">compute_score=</span><span class="s3">False,</span>
        <span class="s1">threshold_lambda=</span><span class="s5">1.0e4</span><span class="s3">,</span>
        <span class="s1">fit_intercept=</span><span class="s3">True,</span>
        <span class="s1">copy_X=</span><span class="s3">True,</span>
        <span class="s1">verbose=</span><span class="s3">False,</span>
        <span class="s1">n_iter=</span><span class="s4">&quot;deprecated&quot;</span><span class="s3">,  </span><span class="s2"># TODO(1.5): Remove</span>
    <span class="s1">):</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.alpha_1 = alpha_1</span>
        <span class="s1">self.alpha_2 = alpha_2</span>
        <span class="s1">self.lambda_1 = lambda_1</span>
        <span class="s1">self.lambda_2 = lambda_2</span>
        <span class="s1">self.compute_score = compute_score</span>
        <span class="s1">self.threshold_lambda = threshold_lambda</span>
        <span class="s1">self.copy_X = copy_X</span>
        <span class="s1">self.verbose = verbose</span>
        <span class="s1">self.n_iter = n_iter</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model according to the given training data and parameters. 
 
        Iterative procedure to maximize the evidence 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training vector, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
        y : array-like of shape (n_samples,) 
            Target values (integers). Will be cast to X's dtype if necessary. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">max_iter = _deprecate_n_iter(self.n_iter</span><span class="s3">, </span><span class="s1">self.max_iter)</span>

        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">, </span><span class="s1">y_numeric=</span><span class="s3">True, </span><span class="s1">ensure_min_samples=</span><span class="s5">2</span>
        <span class="s1">)</span>

        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>
        <span class="s1">coef_ = np.zeros(n_features</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">X_offset_</span><span class="s3">, </span><span class="s1">y_offset_</span><span class="s3">, </span><span class="s1">X_scale_ = _preprocess_data(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">self.fit_intercept</span><span class="s3">, </span><span class="s1">copy=self.copy_X</span>
        <span class="s1">)</span>

        <span class="s1">self.X_offset_ = X_offset_</span>
        <span class="s1">self.X_scale_ = X_scale_</span>

        <span class="s2"># Launch the convergence loop</span>
        <span class="s1">keep_lambda = np.ones(n_features</span><span class="s3">, </span><span class="s1">dtype=bool)</span>

        <span class="s1">lambda_1 = self.lambda_1</span>
        <span class="s1">lambda_2 = self.lambda_2</span>
        <span class="s1">alpha_1 = self.alpha_1</span>
        <span class="s1">alpha_2 = self.alpha_2</span>
        <span class="s1">verbose = self.verbose</span>

        <span class="s2"># Initialization of the values of the parameters</span>
        <span class="s1">eps = np.finfo(np.float64).eps</span>
        <span class="s2"># Add `eps` in the denominator to omit division by zero if `np.var(y)`</span>
        <span class="s2"># is zero</span>
        <span class="s1">alpha_ = </span><span class="s5">1.0 </span><span class="s1">/ (np.var(y) + eps)</span>
        <span class="s1">lambda_ = np.ones(n_features</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s1">self.scores_ = list()</span>
        <span class="s1">coef_old_ = </span><span class="s3">None</span>

        <span class="s3">def </span><span class="s1">update_coeff(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">alpha_</span><span class="s3">, </span><span class="s1">keep_lambda</span><span class="s3">, </span><span class="s1">sigma_):</span>
            <span class="s1">coef_[keep_lambda] = alpha_ * np.linalg.multi_dot(</span>
                <span class="s1">[sigma_</span><span class="s3">, </span><span class="s1">X[:</span><span class="s3">, </span><span class="s1">keep_lambda].T</span><span class="s3">, </span><span class="s1">y]</span>
            <span class="s1">)</span>
            <span class="s3">return </span><span class="s1">coef_</span>

        <span class="s1">update_sigma = (</span>
            <span class="s1">self._update_sigma</span>
            <span class="s3">if </span><span class="s1">n_samples &gt;= n_features</span>
            <span class="s3">else </span><span class="s1">self._update_sigma_woodbury</span>
        <span class="s1">)</span>
        <span class="s2"># Iterative procedure of ARDRegression</span>
        <span class="s3">for </span><span class="s1">iter_ </span><span class="s3">in </span><span class="s1">range(max_iter):</span>
            <span class="s1">sigma_ = update_sigma(X</span><span class="s3">, </span><span class="s1">alpha_</span><span class="s3">, </span><span class="s1">lambda_</span><span class="s3">, </span><span class="s1">keep_lambda)</span>
            <span class="s1">coef_ = update_coeff(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">alpha_</span><span class="s3">, </span><span class="s1">keep_lambda</span><span class="s3">, </span><span class="s1">sigma_)</span>

            <span class="s2"># Update alpha and lambda</span>
            <span class="s1">rmse_ = np.sum((y - np.dot(X</span><span class="s3">, </span><span class="s1">coef_)) ** </span><span class="s5">2</span><span class="s1">)</span>
            <span class="s1">gamma_ = </span><span class="s5">1.0 </span><span class="s1">- lambda_[keep_lambda] * np.diag(sigma_)</span>
            <span class="s1">lambda_[keep_lambda] = (gamma_ + </span><span class="s5">2.0 </span><span class="s1">* lambda_1) / (</span>
                <span class="s1">(coef_[keep_lambda]) ** </span><span class="s5">2 </span><span class="s1">+ </span><span class="s5">2.0 </span><span class="s1">* lambda_2</span>
            <span class="s1">)</span>
            <span class="s1">alpha_ = (n_samples - gamma_.sum() + </span><span class="s5">2.0 </span><span class="s1">* alpha_1) / (</span>
                <span class="s1">rmse_ + </span><span class="s5">2.0 </span><span class="s1">* alpha_2</span>
            <span class="s1">)</span>

            <span class="s2"># Prune the weights with a precision over a threshold</span>
            <span class="s1">keep_lambda = lambda_ &lt; self.threshold_lambda</span>
            <span class="s1">coef_[~keep_lambda] = </span><span class="s5">0</span>

            <span class="s2"># Compute the objective function</span>
            <span class="s3">if </span><span class="s1">self.compute_score:</span>
                <span class="s1">s = (lambda_1 * np.log(lambda_) - lambda_2 * lambda_).sum()</span>
                <span class="s1">s += alpha_1 * log(alpha_) - alpha_2 * alpha_</span>
                <span class="s1">s += </span><span class="s5">0.5 </span><span class="s1">* (</span>
                    <span class="s1">fast_logdet(sigma_)</span>
                    <span class="s1">+ n_samples * log(alpha_)</span>
                    <span class="s1">+ np.sum(np.log(lambda_))</span>
                <span class="s1">)</span>
                <span class="s1">s -= </span><span class="s5">0.5 </span><span class="s1">* (alpha_ * rmse_ + (lambda_ * coef_**</span><span class="s5">2</span><span class="s1">).sum())</span>
                <span class="s1">self.scores_.append(s)</span>

            <span class="s2"># Check for convergence</span>
            <span class="s3">if </span><span class="s1">iter_ &gt; </span><span class="s5">0 </span><span class="s3">and </span><span class="s1">np.sum(np.abs(coef_old_ - coef_)) &lt; self.tol:</span>
                <span class="s3">if </span><span class="s1">verbose:</span>
                    <span class="s1">print(</span><span class="s4">&quot;Converged after %s iterations&quot; </span><span class="s1">% iter_)</span>
                <span class="s3">break</span>
            <span class="s1">coef_old_ = np.copy(coef_)</span>

            <span class="s3">if not </span><span class="s1">keep_lambda.any():</span>
                <span class="s3">break</span>

        <span class="s1">self.n_iter_ = iter_ + </span><span class="s5">1</span>

        <span class="s3">if </span><span class="s1">keep_lambda.any():</span>
            <span class="s2"># update sigma and mu using updated params from the last iteration</span>
            <span class="s1">sigma_ = update_sigma(X</span><span class="s3">, </span><span class="s1">alpha_</span><span class="s3">, </span><span class="s1">lambda_</span><span class="s3">, </span><span class="s1">keep_lambda)</span>
            <span class="s1">coef_ = update_coeff(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">alpha_</span><span class="s3">, </span><span class="s1">keep_lambda</span><span class="s3">, </span><span class="s1">sigma_)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">sigma_ = np.array([]).reshape(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>

        <span class="s1">self.coef_ = coef_</span>
        <span class="s1">self.alpha_ = alpha_</span>
        <span class="s1">self.sigma_ = sigma_</span>
        <span class="s1">self.lambda_ = lambda_</span>
        <span class="s1">self._set_intercept(X_offset_</span><span class="s3">, </span><span class="s1">y_offset_</span><span class="s3">, </span><span class="s1">X_scale_)</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">_update_sigma_woodbury(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">alpha_</span><span class="s3">, </span><span class="s1">lambda_</span><span class="s3">, </span><span class="s1">keep_lambda):</span>
        <span class="s2"># See slides as referenced in the docstring note</span>
        <span class="s2"># this function is used when n_samples &lt; n_features and will invert</span>
        <span class="s2"># a matrix of shape (n_samples, n_samples) making use of the</span>
        <span class="s2"># woodbury formula:</span>
        <span class="s2"># https://en.wikipedia.org/wiki/Woodbury_matrix_identity</span>
        <span class="s1">n_samples = X.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">X_keep = X[:</span><span class="s3">, </span><span class="s1">keep_lambda]</span>
        <span class="s1">inv_lambda = </span><span class="s5">1 </span><span class="s1">/ lambda_[keep_lambda].reshape(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">sigma_ = pinvh(</span>
            <span class="s1">np.eye(n_samples</span><span class="s3">, </span><span class="s1">dtype=X.dtype) / alpha_</span>
            <span class="s1">+ np.dot(X_keep * inv_lambda</span><span class="s3">, </span><span class="s1">X_keep.T)</span>
        <span class="s1">)</span>
        <span class="s1">sigma_ = np.dot(sigma_</span><span class="s3">, </span><span class="s1">X_keep * inv_lambda)</span>
        <span class="s1">sigma_ = -np.dot(inv_lambda.reshape(-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">) * X_keep.T</span><span class="s3">, </span><span class="s1">sigma_)</span>
        <span class="s1">sigma_[np.diag_indices(sigma_.shape[</span><span class="s5">1</span><span class="s1">])] += </span><span class="s5">1.0 </span><span class="s1">/ lambda_[keep_lambda]</span>
        <span class="s3">return </span><span class="s1">sigma_</span>

    <span class="s3">def </span><span class="s1">_update_sigma(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">alpha_</span><span class="s3">, </span><span class="s1">lambda_</span><span class="s3">, </span><span class="s1">keep_lambda):</span>
        <span class="s2"># See slides as referenced in the docstring note</span>
        <span class="s2"># this function is used when n_samples &gt;= n_features and will</span>
        <span class="s2"># invert a matrix of shape (n_features, n_features)</span>
        <span class="s1">X_keep = X[:</span><span class="s3">, </span><span class="s1">keep_lambda]</span>
        <span class="s1">gram = np.dot(X_keep.T</span><span class="s3">, </span><span class="s1">X_keep)</span>
        <span class="s1">eye = np.eye(gram.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s1">sigma_inv = lambda_[keep_lambda] * eye + alpha_ * gram</span>
        <span class="s1">sigma_ = pinvh(sigma_inv)</span>
        <span class="s3">return </span><span class="s1">sigma_</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Predict using the linear model. 
 
        In addition to the mean of the predictive distribution, also its 
        standard deviation can be returned. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Samples. 
 
        return_std : bool, default=False 
            Whether to return the standard deviation of posterior prediction. 
 
        Returns 
        ------- 
        y_mean : array-like of shape (n_samples,) 
            Mean of predictive distribution of query points. 
 
        y_std : array-like of shape (n_samples,) 
            Standard deviation of predictive distribution of query points. 
        &quot;&quot;&quot;</span>
        <span class="s1">y_mean = self._decision_function(X)</span>
        <span class="s3">if </span><span class="s1">return_std </span><span class="s3">is False</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">y_mean</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">X = X[:</span><span class="s3">, </span><span class="s1">self.lambda_ &lt; self.threshold_lambda]</span>
            <span class="s1">sigmas_squared_data = (np.dot(X</span><span class="s3">, </span><span class="s1">self.sigma_) * X).sum(axis=</span><span class="s5">1</span><span class="s1">)</span>
            <span class="s1">y_std = np.sqrt(sigmas_squared_data + (</span><span class="s5">1.0 </span><span class="s1">/ self.alpha_))</span>
            <span class="s3">return </span><span class="s1">y_mean</span><span class="s3">, </span><span class="s1">y_std</span>
</pre>
</body>
</html>