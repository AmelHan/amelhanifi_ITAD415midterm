<html>
<head>
<title>test_nmf.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_nmf.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">re</span>
<span class="s0">import </span><span class="s1">sys</span>
<span class="s0">import </span><span class="s1">warnings</span>
<span class="s0">from </span><span class="s1">io </span><span class="s0">import </span><span class="s1">StringIO</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">import </span><span class="s1">scipy.sparse </span><span class="s0">as </span><span class="s1">sp</span>
<span class="s0">from </span><span class="s1">scipy </span><span class="s0">import </span><span class="s1">linalg</span>
<span class="s0">from </span><span class="s1">scipy.sparse </span><span class="s0">import </span><span class="s1">csc_matrix</span>

<span class="s0">from </span><span class="s1">sklearn.base </span><span class="s0">import </span><span class="s1">clone</span>
<span class="s0">from </span><span class="s1">sklearn.decomposition </span><span class="s0">import </span><span class="s1">NMF</span><span class="s0">, </span><span class="s1">MiniBatchNMF</span><span class="s0">, </span><span class="s1">non_negative_factorization</span>
<span class="s0">from </span><span class="s1">sklearn.decomposition </span><span class="s0">import </span><span class="s1">_nmf </span><span class="s0">as </span><span class="s1">nmf  </span><span class="s2"># For testing internals</span>
<span class="s0">from </span><span class="s1">sklearn.exceptions </span><span class="s0">import </span><span class="s1">ConvergenceWarning</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s0">,</span>
    <span class="s1">assert_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_equal</span><span class="s0">,</span>
    <span class="s1">ignore_warnings</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.utils.extmath </span><span class="s0">import </span><span class="s1">squared_norm</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s1">[</span><span class="s3">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s3">&quot;solver&quot;</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">[[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;cd&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;mu&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[MiniBatchNMF</span><span class="s0">, </span><span class="s1">{}]]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_convergence_warning(Estimator</span><span class="s0">, </span><span class="s1">solver):</span>
    <span class="s1">convergence_warning = (</span>
        <span class="s3">&quot;Maximum number of iterations 1 reached. Increase it to improve convergence.&quot;</span>
    <span class="s1">)</span>
    <span class="s1">A = np.ones((</span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s0">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s0">, </span><span class="s1">match=convergence_warning):</span>
        <span class="s1">Estimator(max_iter=</span><span class="s4">1</span><span class="s0">, </span><span class="s1">**solver).fit(A)</span>


<span class="s0">def </span><span class="s1">test_initialize_nn_output():</span>
    <span class="s2"># Test that initialization does not return negative values</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">data = np.abs(rng.randn(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s0">for </span><span class="s1">init </span><span class="s0">in </span><span class="s1">(</span><span class="s3">&quot;random&quot;</span><span class="s0">, </span><span class="s3">&quot;nndsvd&quot;</span><span class="s0">, </span><span class="s3">&quot;nndsvda&quot;</span><span class="s0">, </span><span class="s3">&quot;nndsvdar&quot;</span><span class="s1">):</span>
        <span class="s1">W</span><span class="s0">, </span><span class="s1">H = nmf._initialize_nmf(data</span><span class="s0">, </span><span class="s4">10</span><span class="s0">, </span><span class="s1">init=init</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s0">assert not </span><span class="s1">((W &lt; </span><span class="s4">0</span><span class="s1">).any() </span><span class="s0">or </span><span class="s1">(H &lt; </span><span class="s4">0</span><span class="s1">).any())</span>


<span class="s1">@pytest.mark.filterwarnings(</span>
    <span class="s3">r&quot;ignore:The multiplicative update \('mu'\) solver cannot update zeros present in&quot;</span>
    <span class="s3">r&quot; the initialization&quot;</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_parameter_checking():</span>
    <span class="s2"># Here we only check for invalid parameter values that are not already</span>
    <span class="s2"># automatically tested in the common tests.</span>

    <span class="s1">A = np.ones((</span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">))</span>

    <span class="s1">msg = </span><span class="s3">&quot;Invalid beta_loss parameter: solver 'cd' does not handle beta_loss = 1.0&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">NMF(solver=</span><span class="s3">&quot;cd&quot;</span><span class="s0">, </span><span class="s1">beta_loss=</span><span class="s4">1.0</span><span class="s1">).fit(A)</span>
    <span class="s1">msg = </span><span class="s3">&quot;Negative values in data passed to&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">NMF().fit(-A)</span>
    <span class="s1">clf = NMF(</span><span class="s4">2</span><span class="s0">, </span><span class="s1">tol=</span><span class="s4">0.1</span><span class="s1">).fit(A)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.transform(-A)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">nmf._initialize_nmf(-A</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s3">&quot;nndsvd&quot;</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">init </span><span class="s0">in </span><span class="s1">[</span><span class="s3">&quot;nndsvd&quot;</span><span class="s0">, </span><span class="s3">&quot;nndsvda&quot;</span><span class="s0">, </span><span class="s3">&quot;nndsvdar&quot;</span><span class="s1">]:</span>
        <span class="s1">msg = re.escape(</span>
            <span class="s3">&quot;init = '{}' can only be used when &quot;</span>
            <span class="s3">&quot;n_components &lt;= min(n_samples, n_features)&quot;</span><span class="s1">.format(init)</span>
        <span class="s1">)</span>
        <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
            <span class="s1">NMF(</span><span class="s4">3</span><span class="s0">, </span><span class="s1">init=init).fit(A)</span>
        <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
            <span class="s1">MiniBatchNMF(</span><span class="s4">3</span><span class="s0">, </span><span class="s1">init=init).fit(A)</span>
        <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
            <span class="s1">nmf._initialize_nmf(A</span><span class="s0">, </span><span class="s4">3</span><span class="s0">, </span><span class="s1">init)</span>


<span class="s0">def </span><span class="s1">test_initialize_close():</span>
    <span class="s2"># Test NNDSVD error</span>
    <span class="s2"># Test that _initialize_nmf error is less than the standard deviation of</span>
    <span class="s2"># the entries in the matrix.</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">A = np.abs(rng.randn(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">W</span><span class="s0">, </span><span class="s1">H = nmf._initialize_nmf(A</span><span class="s0">, </span><span class="s4">10</span><span class="s0">, </span><span class="s1">init=</span><span class="s3">&quot;nndsvd&quot;</span><span class="s1">)</span>
    <span class="s1">error = linalg.norm(np.dot(W</span><span class="s0">, </span><span class="s1">H) - A)</span>
    <span class="s1">sdev = linalg.norm(A - A.mean())</span>
    <span class="s0">assert </span><span class="s1">error &lt;= sdev</span>


<span class="s0">def </span><span class="s1">test_initialize_variants():</span>
    <span class="s2"># Test NNDSVD variants correctness</span>
    <span class="s2"># Test that the variants 'nndsvda' and 'nndsvdar' differ from basic</span>
    <span class="s2"># 'nndsvd' only where the basic version has zeros.</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">data = np.abs(rng.randn(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">W0</span><span class="s0">, </span><span class="s1">H0 = nmf._initialize_nmf(data</span><span class="s0">, </span><span class="s4">10</span><span class="s0">, </span><span class="s1">init=</span><span class="s3">&quot;nndsvd&quot;</span><span class="s1">)</span>
    <span class="s1">Wa</span><span class="s0">, </span><span class="s1">Ha = nmf._initialize_nmf(data</span><span class="s0">, </span><span class="s4">10</span><span class="s0">, </span><span class="s1">init=</span><span class="s3">&quot;nndsvda&quot;</span><span class="s1">)</span>
    <span class="s1">War</span><span class="s0">, </span><span class="s1">Har = nmf._initialize_nmf(data</span><span class="s0">, </span><span class="s4">10</span><span class="s0">, </span><span class="s1">init=</span><span class="s3">&quot;nndsvdar&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">ref</span><span class="s0">, </span><span class="s1">evl </span><span class="s0">in </span><span class="s1">((W0</span><span class="s0">, </span><span class="s1">Wa)</span><span class="s0">, </span><span class="s1">(W0</span><span class="s0">, </span><span class="s1">War)</span><span class="s0">, </span><span class="s1">(H0</span><span class="s0">, </span><span class="s1">Ha)</span><span class="s0">, </span><span class="s1">(H0</span><span class="s0">, </span><span class="s1">Har)):</span>
        <span class="s1">assert_almost_equal(evl[ref != </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">ref[ref != </span><span class="s4">0</span><span class="s1">])</span>


<span class="s2"># ignore UserWarning raised when both solver='mu' and init='nndsvd'</span>
<span class="s1">@ignore_warnings(category=UserWarning)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s1">[</span><span class="s3">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s3">&quot;solver&quot;</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">[[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;cd&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;mu&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[MiniBatchNMF</span><span class="s0">, </span><span class="s1">{}]]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;init&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s0">None, </span><span class="s3">&quot;nndsvd&quot;</span><span class="s0">, </span><span class="s3">&quot;nndsvda&quot;</span><span class="s0">, </span><span class="s3">&quot;nndsvdar&quot;</span><span class="s0">, </span><span class="s3">&quot;random&quot;</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;alpha_W&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">0.0</span><span class="s0">, </span><span class="s4">1.0</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;alpha_H&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">0.0</span><span class="s0">, </span><span class="s4">1.0</span><span class="s0">, </span><span class="s3">&quot;same&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_nmf_fit_nn_output(Estimator</span><span class="s0">, </span><span class="s1">solver</span><span class="s0">, </span><span class="s1">init</span><span class="s0">, </span><span class="s1">alpha_W</span><span class="s0">, </span><span class="s1">alpha_H):</span>
    <span class="s2"># Test that the decomposition does not contain negative values</span>
    <span class="s1">A = np.c_[</span><span class="s4">5.0 </span><span class="s1">- np.arange(</span><span class="s4">1</span><span class="s0">, </span><span class="s4">6</span><span class="s1">)</span><span class="s0">, </span><span class="s4">5.0 </span><span class="s1">+ np.arange(</span><span class="s4">1</span><span class="s0">, </span><span class="s4">6</span><span class="s1">)]</span>
    <span class="s1">model = Estimator(</span>
        <span class="s1">n_components=</span><span class="s4">2</span><span class="s0">,</span>
        <span class="s1">init=init</span><span class="s0">,</span>
        <span class="s1">alpha_W=alpha_W</span><span class="s0">,</span>
        <span class="s1">alpha_H=alpha_H</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">**solver</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">transf = model.fit_transform(A)</span>
    <span class="s0">assert not </span><span class="s1">((model.components_ &lt; </span><span class="s4">0</span><span class="s1">).any() </span><span class="s0">or </span><span class="s1">(transf &lt; </span><span class="s4">0</span><span class="s1">).any())</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s1">[</span><span class="s3">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s3">&quot;solver&quot;</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">[[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;cd&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;mu&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[MiniBatchNMF</span><span class="s0">, </span><span class="s1">{}]]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_nmf_fit_close(Estimator</span><span class="s0">, </span><span class="s1">solver):</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s2"># Test that the fit is not too far away</span>
    <span class="s1">pnmf = Estimator(</span>
        <span class="s4">5</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s3">&quot;nndsvdar&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s4">600</span><span class="s0">,</span>
        <span class="s1">**solver</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X = np.abs(rng.randn(</span><span class="s4">6</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s0">assert </span><span class="s1">pnmf.fit(X).reconstruction_err_ &lt; </span><span class="s4">0.1</span>


<span class="s0">def </span><span class="s1">test_nmf_true_reconstruction():</span>
    <span class="s2"># Test that the fit is not too far away from an exact solution</span>
    <span class="s2"># (by construction)</span>
    <span class="s1">n_samples = </span><span class="s4">15</span>
    <span class="s1">n_features = </span><span class="s4">10</span>
    <span class="s1">n_components = </span><span class="s4">5</span>
    <span class="s1">beta_loss = </span><span class="s4">1</span>
    <span class="s1">batch_size = </span><span class="s4">3</span>
    <span class="s1">max_iter = </span><span class="s4">1000</span>

    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">W_true = np.zeros([n_samples</span><span class="s0">, </span><span class="s1">n_components])</span>
    <span class="s1">W_array = np.abs(rng.randn(n_samples))</span>
    <span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range(n_components):</span>
        <span class="s1">W_true[j % n_samples</span><span class="s0">, </span><span class="s1">j] = W_array[j % n_samples]</span>
    <span class="s1">H_true = np.zeros([n_components</span><span class="s0">, </span><span class="s1">n_features])</span>
    <span class="s1">H_array = np.abs(rng.randn(n_components))</span>
    <span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range(n_features):</span>
        <span class="s1">H_true[j % n_components</span><span class="s0">, </span><span class="s1">j] = H_array[j % n_components]</span>
    <span class="s1">X = np.dot(W_true</span><span class="s0">, </span><span class="s1">H_true)</span>

    <span class="s1">model = NMF(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">solver=</span><span class="s3">&quot;mu&quot;</span><span class="s0">,</span>
        <span class="s1">beta_loss=beta_loss</span><span class="s0">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">transf = model.fit_transform(X)</span>
    <span class="s1">X_calc = np.dot(transf</span><span class="s0">, </span><span class="s1">model.components_)</span>

    <span class="s0">assert </span><span class="s1">model.reconstruction_err_ &lt; </span><span class="s4">0.1</span>
    <span class="s1">assert_allclose(X</span><span class="s0">, </span><span class="s1">X_calc)</span>

    <span class="s1">mbmodel = MiniBatchNMF(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">beta_loss=beta_loss</span><span class="s0">,</span>
        <span class="s1">batch_size=batch_size</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">transf = mbmodel.fit_transform(X)</span>
    <span class="s1">X_calc = np.dot(transf</span><span class="s0">, </span><span class="s1">mbmodel.components_)</span>

    <span class="s0">assert </span><span class="s1">mbmodel.reconstruction_err_ &lt; </span><span class="s4">0.1</span>
    <span class="s1">assert_allclose(X</span><span class="s0">, </span><span class="s1">X_calc</span><span class="s0">, </span><span class="s1">atol=</span><span class="s4">1</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">&quot;cd&quot;</span><span class="s0">, </span><span class="s3">&quot;mu&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_nmf_transform(solver):</span>
    <span class="s2"># Test that fit_transform is equivalent to fit.transform for NMF</span>
    <span class="s2"># Test that NMF.transform returns close values</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">A = np.abs(rng.randn(</span><span class="s4">6</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">m = NMF(</span>
        <span class="s1">solver=solver</span><span class="s0">,</span>
        <span class="s1">n_components=</span><span class="s4">3</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s3">&quot;random&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s4">1e-6</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">ft = m.fit_transform(A)</span>
    <span class="s1">t = m.transform(A)</span>
    <span class="s1">assert_allclose(ft</span><span class="s0">, </span><span class="s1">t</span><span class="s0">, </span><span class="s1">atol=</span><span class="s4">1e-1</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_minibatch_nmf_transform():</span>
    <span class="s2"># Test that fit_transform is equivalent to fit.transform for MiniBatchNMF</span>
    <span class="s2"># Only guaranteed with fresh restarts</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">A = np.abs(rng.randn(</span><span class="s4">6</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">m = MiniBatchNMF(</span>
        <span class="s1">n_components=</span><span class="s4">3</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s4">1e-3</span><span class="s0">,</span>
        <span class="s1">fresh_restarts=</span><span class="s0">True,</span>
    <span class="s1">)</span>
    <span class="s1">ft = m.fit_transform(A)</span>
    <span class="s1">t = m.transform(A)</span>
    <span class="s1">assert_allclose(ft</span><span class="s0">, </span><span class="s1">t)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s1">[</span><span class="s3">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s3">&quot;solver&quot;</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">[[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;cd&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;mu&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[MiniBatchNMF</span><span class="s0">, </span><span class="s1">{}]]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_nmf_transform_custom_init(Estimator</span><span class="s0">, </span><span class="s1">solver):</span>
    <span class="s2"># Smoke test that checks if NMF.transform works with custom initialization</span>
    <span class="s1">random_state = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">A = np.abs(random_state.randn(</span><span class="s4">6</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">n_components = </span><span class="s4">4</span>
    <span class="s1">avg = np.sqrt(A.mean() / n_components)</span>
    <span class="s1">H_init = np.abs(avg * random_state.randn(n_components</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">W_init = np.abs(avg * random_state.randn(</span><span class="s4">6</span><span class="s0">, </span><span class="s1">n_components))</span>

    <span class="s1">m = Estimator(</span>
        <span class="s1">n_components=n_components</span><span class="s0">, </span><span class="s1">init=</span><span class="s3">&quot;custom&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">tol=</span><span class="s4">1e-3</span><span class="s0">, </span><span class="s1">**solver</span>
    <span class="s1">)</span>
    <span class="s1">m.fit_transform(A</span><span class="s0">, </span><span class="s1">W=W_init</span><span class="s0">, </span><span class="s1">H=H_init)</span>
    <span class="s1">m.transform(A)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s3">&quot;cd&quot;</span><span class="s0">, </span><span class="s3">&quot;mu&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_nmf_inverse_transform(solver):</span>
    <span class="s2"># Test that NMF.inverse_transform returns close values</span>
    <span class="s1">random_state = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">A = np.abs(random_state.randn(</span><span class="s4">6</span><span class="s0">, </span><span class="s4">4</span><span class="s1">))</span>
    <span class="s1">m = NMF(</span>
        <span class="s1">solver=solver</span><span class="s0">,</span>
        <span class="s1">n_components=</span><span class="s4">4</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s3">&quot;random&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s4">1000</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">ft = m.fit_transform(A)</span>
    <span class="s1">A_new = m.inverse_transform(ft)</span>
    <span class="s1">assert_array_almost_equal(A</span><span class="s0">, </span><span class="s1">A_new</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s4">2</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_mbnmf_inverse_transform():</span>
    <span class="s2"># Test that MiniBatchNMF.transform followed by MiniBatchNMF.inverse_transform</span>
    <span class="s2"># is close to the identity</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">A = np.abs(rng.randn(</span><span class="s4">6</span><span class="s0">, </span><span class="s4">4</span><span class="s1">))</span>
    <span class="s1">nmf = MiniBatchNMF(</span>
        <span class="s1">random_state=rng</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s4">500</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s3">&quot;nndsvdar&quot;</span><span class="s0">,</span>
        <span class="s1">fresh_restarts=</span><span class="s0">True,</span>
    <span class="s1">)</span>
    <span class="s1">ft = nmf.fit_transform(A)</span>
    <span class="s1">A_new = nmf.inverse_transform(ft)</span>
    <span class="s1">assert_allclose(A</span><span class="s0">, </span><span class="s1">A_new</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s4">1e-3</span><span class="s0">, </span><span class="s1">atol=</span><span class="s4">1e-2</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s1">[NMF</span><span class="s0">, </span><span class="s1">MiniBatchNMF])</span>
<span class="s0">def </span><span class="s1">test_n_components_greater_n_features(Estimator):</span>
    <span class="s2"># Smoke test for the case of more components than features.</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">A = np.abs(rng.randn(</span><span class="s4">30</span><span class="s0">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">Estimator(n_components=</span><span class="s4">15</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">tol=</span><span class="s4">1e-2</span><span class="s1">).fit(A)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s1">[</span><span class="s3">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s3">&quot;solver&quot;</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">[[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;cd&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;mu&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[MiniBatchNMF</span><span class="s0">, </span><span class="s1">{}]]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;alpha_W&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">0.0</span><span class="s0">, </span><span class="s4">1.0</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;alpha_H&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">0.0</span><span class="s0">, </span><span class="s4">1.0</span><span class="s0">, </span><span class="s3">&quot;same&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_nmf_sparse_input(Estimator</span><span class="s0">, </span><span class="s1">solver</span><span class="s0">, </span><span class="s1">alpha_W</span><span class="s0">, </span><span class="s1">alpha_H):</span>
    <span class="s2"># Test that sparse matrices are accepted as input</span>
    <span class="s0">from </span><span class="s1">scipy.sparse </span><span class="s0">import </span><span class="s1">csc_matrix</span>

    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">A = np.abs(rng.randn(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">A[:</span><span class="s0">, </span><span class="s4">2 </span><span class="s1">* np.arange(</span><span class="s4">5</span><span class="s1">)] = </span><span class="s4">0</span>
    <span class="s1">A_sparse = csc_matrix(A)</span>

    <span class="s1">est1 = Estimator(</span>
        <span class="s1">n_components=</span><span class="s4">5</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s3">&quot;random&quot;</span><span class="s0">,</span>
        <span class="s1">alpha_W=alpha_W</span><span class="s0">,</span>
        <span class="s1">alpha_H=alpha_H</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s4">100</span><span class="s0">,</span>
        <span class="s1">**solver</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">est2 = clone(est1)</span>

    <span class="s1">W1 = est1.fit_transform(A)</span>
    <span class="s1">W2 = est2.fit_transform(A_sparse)</span>
    <span class="s1">H1 = est1.components_</span>
    <span class="s1">H2 = est2.components_</span>

    <span class="s1">assert_allclose(W1</span><span class="s0">, </span><span class="s1">W2)</span>
    <span class="s1">assert_allclose(H1</span><span class="s0">, </span><span class="s1">H2)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s1">[</span><span class="s3">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s3">&quot;solver&quot;</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">[[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;cd&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;mu&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[MiniBatchNMF</span><span class="s0">, </span><span class="s1">{}]]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_nmf_sparse_transform(Estimator</span><span class="s0">, </span><span class="s1">solver):</span>
    <span class="s2"># Test that transform works on sparse data.  Issue #2124</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">A = np.abs(rng.randn(</span><span class="s4">3</span><span class="s0">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">A[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">] = </span><span class="s4">0</span>
    <span class="s1">A = csc_matrix(A)</span>

    <span class="s1">model = Estimator(random_state=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">n_components=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s4">400</span><span class="s0">, </span><span class="s1">**solver)</span>
    <span class="s1">A_fit_tr = model.fit_transform(A)</span>
    <span class="s1">A_tr = model.transform(A)</span>
    <span class="s1">assert_allclose(A_fit_tr</span><span class="s0">, </span><span class="s1">A_tr</span><span class="s0">, </span><span class="s1">atol=</span><span class="s4">1e-1</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;init&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">&quot;random&quot;</span><span class="s0">, </span><span class="s3">&quot;nndsvd&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s3">&quot;cd&quot;</span><span class="s0">, </span><span class="s3">&quot;mu&quot;</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;alpha_W&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">0.0</span><span class="s0">, </span><span class="s4">1.0</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;alpha_H&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">0.0</span><span class="s0">, </span><span class="s4">1.0</span><span class="s0">, </span><span class="s3">&quot;same&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_non_negative_factorization_consistency(init</span><span class="s0">, </span><span class="s1">solver</span><span class="s0">, </span><span class="s1">alpha_W</span><span class="s0">, </span><span class="s1">alpha_H):</span>
    <span class="s2"># Test that the function is called in the same way, either directly</span>
    <span class="s2"># or through the NMF class</span>
    <span class="s1">max_iter = </span><span class="s4">500</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">A = np.abs(rng.randn(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">A[:</span><span class="s0">, </span><span class="s4">2 </span><span class="s1">* np.arange(</span><span class="s4">5</span><span class="s1">)] = </span><span class="s4">0</span>

    <span class="s1">W_nmf</span><span class="s0">, </span><span class="s1">H</span><span class="s0">, </span><span class="s1">_ = non_negative_factorization(</span>
        <span class="s1">A</span><span class="s0">,</span>
        <span class="s1">init=init</span><span class="s0">,</span>
        <span class="s1">solver=solver</span><span class="s0">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s0">,</span>
        <span class="s1">alpha_W=alpha_W</span><span class="s0">,</span>
        <span class="s1">alpha_H=alpha_H</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">1</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s4">1e-2</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">W_nmf_2</span><span class="s0">, </span><span class="s1">H</span><span class="s0">, </span><span class="s1">_ = non_negative_factorization(</span>
        <span class="s1">A</span><span class="s0">,</span>
        <span class="s1">H=H</span><span class="s0">,</span>
        <span class="s1">update_H=</span><span class="s0">False,</span>
        <span class="s1">init=init</span><span class="s0">,</span>
        <span class="s1">solver=solver</span><span class="s0">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s0">,</span>
        <span class="s1">alpha_W=alpha_W</span><span class="s0">,</span>
        <span class="s1">alpha_H=alpha_H</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">1</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s4">1e-2</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">model_class = NMF(</span>
        <span class="s1">init=init</span><span class="s0">,</span>
        <span class="s1">solver=solver</span><span class="s0">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s0">,</span>
        <span class="s1">alpha_W=alpha_W</span><span class="s0">,</span>
        <span class="s1">alpha_H=alpha_H</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">1</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s4">1e-2</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">W_cls = model_class.fit_transform(A)</span>
    <span class="s1">W_cls_2 = model_class.transform(A)</span>

    <span class="s1">assert_allclose(W_nmf</span><span class="s0">, </span><span class="s1">W_cls)</span>
    <span class="s1">assert_allclose(W_nmf_2</span><span class="s0">, </span><span class="s1">W_cls_2)</span>


<span class="s0">def </span><span class="s1">test_non_negative_factorization_checking():</span>
    <span class="s2"># Note that the validity of parameter types and range of possible values</span>
    <span class="s2"># for scalar numerical or str parameters is already checked in the common</span>
    <span class="s2"># tests. Here we only check for problems that cannot be captured by simple</span>
    <span class="s2"># declarative constraints on the valid parameter values.</span>

    <span class="s1">A = np.ones((</span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s2"># Test parameters checking in public function</span>
    <span class="s1">nnmf = non_negative_factorization</span>
    <span class="s1">msg = re.escape(</span><span class="s3">&quot;Negative values in data passed to NMF (input H)&quot;</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">nnmf(A</span><span class="s0">, </span><span class="s1">A</span><span class="s0">, </span><span class="s1">-A</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s1">init=</span><span class="s3">&quot;custom&quot;</span><span class="s1">)</span>
    <span class="s1">msg = re.escape(</span><span class="s3">&quot;Negative values in data passed to NMF (input W)&quot;</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">nnmf(A</span><span class="s0">, </span><span class="s1">-A</span><span class="s0">, </span><span class="s1">A</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s1">init=</span><span class="s3">&quot;custom&quot;</span><span class="s1">)</span>
    <span class="s1">msg = re.escape(</span><span class="s3">&quot;Array passed to NMF (input H) is full of zeros&quot;</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">nnmf(A</span><span class="s0">, </span><span class="s1">A</span><span class="s0">, </span><span class="s4">0 </span><span class="s1">* A</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s1">init=</span><span class="s3">&quot;custom&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">_beta_divergence_dense(X</span><span class="s0">, </span><span class="s1">W</span><span class="s0">, </span><span class="s1">H</span><span class="s0">, </span><span class="s1">beta):</span>
    <span class="s5">&quot;&quot;&quot;Compute the beta-divergence of X and W.H for dense array only. 
 
    Used as a reference for testing nmf._beta_divergence. 
    &quot;&quot;&quot;</span>
    <span class="s1">WH = np.dot(W</span><span class="s0">, </span><span class="s1">H)</span>

    <span class="s0">if </span><span class="s1">beta == </span><span class="s4">2</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">squared_norm(X - WH) / </span><span class="s4">2</span>

    <span class="s1">WH_Xnonzero = WH[X != </span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">X_nonzero = X[X != </span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">np.maximum(WH_Xnonzero</span><span class="s0">, </span><span class="s4">1e-9</span><span class="s0">, </span><span class="s1">out=WH_Xnonzero)</span>

    <span class="s0">if </span><span class="s1">beta == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">res = np.sum(X_nonzero * np.log(X_nonzero / WH_Xnonzero))</span>
        <span class="s1">res += WH.sum() - X.sum()</span>

    <span class="s0">elif </span><span class="s1">beta == </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s1">div = X_nonzero / WH_Xnonzero</span>
        <span class="s1">res = np.sum(div) - X.size - np.sum(np.log(div))</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">res = (X_nonzero**beta).sum()</span>
        <span class="s1">res += (beta - </span><span class="s4">1</span><span class="s1">) * (WH**beta).sum()</span>
        <span class="s1">res -= beta * (X_nonzero * (WH_Xnonzero ** (beta - </span><span class="s4">1</span><span class="s1">))).sum()</span>
        <span class="s1">res /= beta * (beta - </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s0">return </span><span class="s1">res</span>


<span class="s0">def </span><span class="s1">test_beta_divergence():</span>
    <span class="s2"># Compare _beta_divergence with the reference _beta_divergence_dense</span>
    <span class="s1">n_samples = </span><span class="s4">20</span>
    <span class="s1">n_features = </span><span class="s4">10</span>
    <span class="s1">n_components = </span><span class="s4">5</span>
    <span class="s1">beta_losses = [</span><span class="s4">0.0</span><span class="s0">, </span><span class="s4">0.5</span><span class="s0">, </span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">1.5</span><span class="s0">, </span><span class="s4">2.0</span><span class="s0">, </span><span class="s4">3.0</span><span class="s1">]</span>

    <span class="s2"># initialization</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">np.clip(X</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, None, </span><span class="s1">out=X)</span>
    <span class="s1">X_csr = sp.csr_matrix(X)</span>
    <span class="s1">W</span><span class="s0">, </span><span class="s1">H = nmf._initialize_nmf(X</span><span class="s0">, </span><span class="s1">n_components</span><span class="s0">, </span><span class="s1">init=</span><span class="s3">&quot;random&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">beta </span><span class="s0">in </span><span class="s1">beta_losses:</span>
        <span class="s1">ref = _beta_divergence_dense(X</span><span class="s0">, </span><span class="s1">W</span><span class="s0">, </span><span class="s1">H</span><span class="s0">, </span><span class="s1">beta)</span>
        <span class="s1">loss = nmf._beta_divergence(X</span><span class="s0">, </span><span class="s1">W</span><span class="s0">, </span><span class="s1">H</span><span class="s0">, </span><span class="s1">beta)</span>
        <span class="s1">loss_csr = nmf._beta_divergence(X_csr</span><span class="s0">, </span><span class="s1">W</span><span class="s0">, </span><span class="s1">H</span><span class="s0">, </span><span class="s1">beta)</span>

        <span class="s1">assert_almost_equal(ref</span><span class="s0">, </span><span class="s1">loss</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s4">7</span><span class="s1">)</span>
        <span class="s1">assert_almost_equal(ref</span><span class="s0">, </span><span class="s1">loss_csr</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s4">7</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_special_sparse_dot():</span>
    <span class="s2"># Test the function that computes np.dot(W, H), only where X is non zero.</span>
    <span class="s1">n_samples = </span><span class="s4">10</span>
    <span class="s1">n_features = </span><span class="s4">5</span>
    <span class="s1">n_components = </span><span class="s4">3</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">np.clip(X</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, None, </span><span class="s1">out=X)</span>
    <span class="s1">X_csr = sp.csr_matrix(X)</span>

    <span class="s1">W = np.abs(rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_components))</span>
    <span class="s1">H = np.abs(rng.randn(n_components</span><span class="s0">, </span><span class="s1">n_features))</span>

    <span class="s1">WH_safe = nmf._special_sparse_dot(W</span><span class="s0">, </span><span class="s1">H</span><span class="s0">, </span><span class="s1">X_csr)</span>
    <span class="s1">WH = nmf._special_sparse_dot(W</span><span class="s0">, </span><span class="s1">H</span><span class="s0">, </span><span class="s1">X)</span>

    <span class="s2"># test that both results have same values, in X_csr nonzero elements</span>
    <span class="s1">ii</span><span class="s0">, </span><span class="s1">jj = X_csr.nonzero()</span>
    <span class="s1">WH_safe_data = np.asarray(WH_safe[ii</span><span class="s0">, </span><span class="s1">jj]).ravel()</span>
    <span class="s1">assert_array_almost_equal(WH_safe_data</span><span class="s0">, </span><span class="s1">WH[ii</span><span class="s0">, </span><span class="s1">jj]</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s4">10</span><span class="s1">)</span>

    <span class="s2"># test that WH_safe and X_csr have the same sparse structure</span>
    <span class="s1">assert_array_equal(WH_safe.indices</span><span class="s0">, </span><span class="s1">X_csr.indices)</span>
    <span class="s1">assert_array_equal(WH_safe.indptr</span><span class="s0">, </span><span class="s1">X_csr.indptr)</span>
    <span class="s1">assert_array_equal(WH_safe.shape</span><span class="s0">, </span><span class="s1">X_csr.shape)</span>


<span class="s1">@ignore_warnings(category=ConvergenceWarning)</span>
<span class="s0">def </span><span class="s1">test_nmf_multiplicative_update_sparse():</span>
    <span class="s2"># Compare sparse and dense input in multiplicative update NMF</span>
    <span class="s2"># Also test continuity of the results with respect to beta_loss parameter</span>
    <span class="s1">n_samples = </span><span class="s4">20</span>
    <span class="s1">n_features = </span><span class="s4">10</span>
    <span class="s1">n_components = </span><span class="s4">5</span>
    <span class="s1">alpha = </span><span class="s4">0.1</span>
    <span class="s1">l1_ratio = </span><span class="s4">0.5</span>
    <span class="s1">n_iter = </span><span class="s4">20</span>

    <span class="s2"># initialization</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">1337</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">X = np.abs(X)</span>
    <span class="s1">X_csr = sp.csr_matrix(X)</span>
    <span class="s1">W0</span><span class="s0">, </span><span class="s1">H0 = nmf._initialize_nmf(X</span><span class="s0">, </span><span class="s1">n_components</span><span class="s0">, </span><span class="s1">init=</span><span class="s3">&quot;random&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">beta_loss </span><span class="s0">in </span><span class="s1">(-</span><span class="s4">1.2</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0.2</span><span class="s0">, </span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">2.0</span><span class="s0">, </span><span class="s4">2.5</span><span class="s1">):</span>
        <span class="s2"># Reference with dense array X</span>
        <span class="s1">W</span><span class="s0">, </span><span class="s1">H = W0.copy()</span><span class="s0">, </span><span class="s1">H0.copy()</span>
        <span class="s1">W1</span><span class="s0">, </span><span class="s1">H1</span><span class="s0">, </span><span class="s1">_ = non_negative_factorization(</span>
            <span class="s1">X</span><span class="s0">,</span>
            <span class="s1">W</span><span class="s0">,</span>
            <span class="s1">H</span><span class="s0">,</span>
            <span class="s1">n_components</span><span class="s0">,</span>
            <span class="s1">init=</span><span class="s3">&quot;custom&quot;</span><span class="s0">,</span>
            <span class="s1">update_H=</span><span class="s0">True,</span>
            <span class="s1">solver=</span><span class="s3">&quot;mu&quot;</span><span class="s0">,</span>
            <span class="s1">beta_loss=beta_loss</span><span class="s0">,</span>
            <span class="s1">max_iter=n_iter</span><span class="s0">,</span>
            <span class="s1">alpha_W=alpha</span><span class="s0">,</span>
            <span class="s1">l1_ratio=l1_ratio</span><span class="s0">,</span>
            <span class="s1">random_state=</span><span class="s4">42</span><span class="s0">,</span>
        <span class="s1">)</span>

        <span class="s2"># Compare with sparse X</span>
        <span class="s1">W</span><span class="s0">, </span><span class="s1">H = W0.copy()</span><span class="s0">, </span><span class="s1">H0.copy()</span>
        <span class="s1">W2</span><span class="s0">, </span><span class="s1">H2</span><span class="s0">, </span><span class="s1">_ = non_negative_factorization(</span>
            <span class="s1">X_csr</span><span class="s0">,</span>
            <span class="s1">W</span><span class="s0">,</span>
            <span class="s1">H</span><span class="s0">,</span>
            <span class="s1">n_components</span><span class="s0">,</span>
            <span class="s1">init=</span><span class="s3">&quot;custom&quot;</span><span class="s0">,</span>
            <span class="s1">update_H=</span><span class="s0">True,</span>
            <span class="s1">solver=</span><span class="s3">&quot;mu&quot;</span><span class="s0">,</span>
            <span class="s1">beta_loss=beta_loss</span><span class="s0">,</span>
            <span class="s1">max_iter=n_iter</span><span class="s0">,</span>
            <span class="s1">alpha_W=alpha</span><span class="s0">,</span>
            <span class="s1">l1_ratio=l1_ratio</span><span class="s0">,</span>
            <span class="s1">random_state=</span><span class="s4">42</span><span class="s0">,</span>
        <span class="s1">)</span>

        <span class="s1">assert_allclose(W1</span><span class="s0">, </span><span class="s1">W2</span><span class="s0">, </span><span class="s1">atol=</span><span class="s4">1e-7</span><span class="s1">)</span>
        <span class="s1">assert_allclose(H1</span><span class="s0">, </span><span class="s1">H2</span><span class="s0">, </span><span class="s1">atol=</span><span class="s4">1e-7</span><span class="s1">)</span>

        <span class="s2"># Compare with almost same beta_loss, since some values have a specific</span>
        <span class="s2"># behavior, but the results should be continuous w.r.t beta_loss</span>
        <span class="s1">beta_loss -= </span><span class="s4">1.0e-5</span>
        <span class="s1">W</span><span class="s0">, </span><span class="s1">H = W0.copy()</span><span class="s0">, </span><span class="s1">H0.copy()</span>
        <span class="s1">W3</span><span class="s0">, </span><span class="s1">H3</span><span class="s0">, </span><span class="s1">_ = non_negative_factorization(</span>
            <span class="s1">X_csr</span><span class="s0">,</span>
            <span class="s1">W</span><span class="s0">,</span>
            <span class="s1">H</span><span class="s0">,</span>
            <span class="s1">n_components</span><span class="s0">,</span>
            <span class="s1">init=</span><span class="s3">&quot;custom&quot;</span><span class="s0">,</span>
            <span class="s1">update_H=</span><span class="s0">True,</span>
            <span class="s1">solver=</span><span class="s3">&quot;mu&quot;</span><span class="s0">,</span>
            <span class="s1">beta_loss=beta_loss</span><span class="s0">,</span>
            <span class="s1">max_iter=n_iter</span><span class="s0">,</span>
            <span class="s1">alpha_W=alpha</span><span class="s0">,</span>
            <span class="s1">l1_ratio=l1_ratio</span><span class="s0">,</span>
            <span class="s1">random_state=</span><span class="s4">42</span><span class="s0">,</span>
        <span class="s1">)</span>

        <span class="s1">assert_allclose(W1</span><span class="s0">, </span><span class="s1">W3</span><span class="s0">, </span><span class="s1">atol=</span><span class="s4">1e-4</span><span class="s1">)</span>
        <span class="s1">assert_allclose(H1</span><span class="s0">, </span><span class="s1">H3</span><span class="s0">, </span><span class="s1">atol=</span><span class="s4">1e-4</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_nmf_negative_beta_loss():</span>
    <span class="s2"># Test that an error is raised if beta_loss &lt; 0 and X contains zeros.</span>
    <span class="s2"># Test that the output has not NaN values when the input contains zeros.</span>
    <span class="s1">n_samples = </span><span class="s4">6</span>
    <span class="s1">n_features = </span><span class="s4">5</span>
    <span class="s1">n_components = </span><span class="s4">3</span>

    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">np.clip(X</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, None, </span><span class="s1">out=X)</span>
    <span class="s1">X_csr = sp.csr_matrix(X)</span>

    <span class="s0">def </span><span class="s1">_assert_nmf_no_nan(X</span><span class="s0">, </span><span class="s1">beta_loss):</span>
        <span class="s1">W</span><span class="s0">, </span><span class="s1">H</span><span class="s0">, </span><span class="s1">_ = non_negative_factorization(</span>
            <span class="s1">X</span><span class="s0">,</span>
            <span class="s1">init=</span><span class="s3">&quot;random&quot;</span><span class="s0">,</span>
            <span class="s1">n_components=n_components</span><span class="s0">,</span>
            <span class="s1">solver=</span><span class="s3">&quot;mu&quot;</span><span class="s0">,</span>
            <span class="s1">beta_loss=beta_loss</span><span class="s0">,</span>
            <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
            <span class="s1">max_iter=</span><span class="s4">1000</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s0">assert not </span><span class="s1">np.any(np.isnan(W))</span>
        <span class="s0">assert not </span><span class="s1">np.any(np.isnan(H))</span>

    <span class="s1">msg = </span><span class="s3">&quot;When beta_loss &lt;= 0 and X contains zeros, the solver may diverge.&quot;</span>
    <span class="s0">for </span><span class="s1">beta_loss </span><span class="s0">in </span><span class="s1">(-</span><span class="s4">0.6</span><span class="s0">, </span><span class="s4">0.0</span><span class="s1">):</span>
        <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
            <span class="s1">_assert_nmf_no_nan(X</span><span class="s0">, </span><span class="s1">beta_loss)</span>
        <span class="s1">_assert_nmf_no_nan(X + </span><span class="s4">1e-9</span><span class="s0">, </span><span class="s1">beta_loss)</span>

    <span class="s0">for </span><span class="s1">beta_loss </span><span class="s0">in </span><span class="s1">(</span><span class="s4">0.2</span><span class="s0">, </span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">1.2</span><span class="s0">, </span><span class="s4">2.0</span><span class="s0">, </span><span class="s4">2.5</span><span class="s1">):</span>
        <span class="s1">_assert_nmf_no_nan(X</span><span class="s0">, </span><span class="s1">beta_loss)</span>
        <span class="s1">_assert_nmf_no_nan(X_csr</span><span class="s0">, </span><span class="s1">beta_loss)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;beta_loss&quot;</span><span class="s0">, </span><span class="s1">[-</span><span class="s4">0.5</span><span class="s0">, </span><span class="s4">0.0</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_minibatch_nmf_negative_beta_loss(beta_loss):</span>
    <span class="s5">&quot;&quot;&quot;Check that an error is raised if beta_loss &lt; 0 and X contains zeros.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = rng.normal(size=(</span><span class="s4">6</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">X[X &lt; </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">0</span>

    <span class="s1">nmf = MiniBatchNMF(beta_loss=beta_loss</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">msg = </span><span class="s3">&quot;When beta_loss &lt;= 0 and X contains zeros, the solver may diverge.&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">nmf.fit(X)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s1">[</span><span class="s3">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s3">&quot;solver&quot;</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">[[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;cd&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;mu&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[MiniBatchNMF</span><span class="s0">, </span><span class="s1">{}]]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_nmf_regularization(Estimator</span><span class="s0">, </span><span class="s1">solver):</span>
    <span class="s2"># Test the effect of L1 and L2 regularizations</span>
    <span class="s1">n_samples = </span><span class="s4">6</span>
    <span class="s1">n_features = </span><span class="s4">5</span>
    <span class="s1">n_components = </span><span class="s4">3</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">X = np.abs(rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features))</span>

    <span class="s2"># L1 regularization should increase the number of zeros</span>
    <span class="s1">l1_ratio = </span><span class="s4">1.0</span>
    <span class="s1">regul = Estimator(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">alpha_W=</span><span class="s4">0.5</span><span class="s0">,</span>
        <span class="s1">l1_ratio=l1_ratio</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">42</span><span class="s0">,</span>
        <span class="s1">**solver</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">model = Estimator(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">alpha_W=</span><span class="s4">0.0</span><span class="s0">,</span>
        <span class="s1">l1_ratio=l1_ratio</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">42</span><span class="s0">,</span>
        <span class="s1">**solver</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">W_regul = regul.fit_transform(X)</span>
    <span class="s1">W_model = model.fit_transform(X)</span>

    <span class="s1">H_regul = regul.components_</span>
    <span class="s1">H_model = model.components_</span>

    <span class="s1">eps = np.finfo(np.float64).eps</span>
    <span class="s1">W_regul_n_zeros = W_regul[W_regul &lt;= eps].size</span>
    <span class="s1">W_model_n_zeros = W_model[W_model &lt;= eps].size</span>
    <span class="s1">H_regul_n_zeros = H_regul[H_regul &lt;= eps].size</span>
    <span class="s1">H_model_n_zeros = H_model[H_model &lt;= eps].size</span>

    <span class="s0">assert </span><span class="s1">W_regul_n_zeros &gt; W_model_n_zeros</span>
    <span class="s0">assert </span><span class="s1">H_regul_n_zeros &gt; H_model_n_zeros</span>

    <span class="s2"># L2 regularization should decrease the sum of the squared norm</span>
    <span class="s2"># of the matrices W and H</span>
    <span class="s1">l1_ratio = </span><span class="s4">0.0</span>
    <span class="s1">regul = Estimator(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">alpha_W=</span><span class="s4">0.5</span><span class="s0">,</span>
        <span class="s1">l1_ratio=l1_ratio</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">42</span><span class="s0">,</span>
        <span class="s1">**solver</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">model = Estimator(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">alpha_W=</span><span class="s4">0.0</span><span class="s0">,</span>
        <span class="s1">l1_ratio=l1_ratio</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">42</span><span class="s0">,</span>
        <span class="s1">**solver</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">W_regul = regul.fit_transform(X)</span>
    <span class="s1">W_model = model.fit_transform(X)</span>

    <span class="s1">H_regul = regul.components_</span>
    <span class="s1">H_model = model.components_</span>

    <span class="s0">assert </span><span class="s1">(linalg.norm(W_model)) ** </span><span class="s4">2.0 </span><span class="s1">+ (linalg.norm(H_model)) ** </span><span class="s4">2.0 </span><span class="s1">&gt; (</span>
        <span class="s1">linalg.norm(W_regul)</span>
    <span class="s1">) ** </span><span class="s4">2.0 </span><span class="s1">+ (linalg.norm(H_regul)) ** </span><span class="s4">2.0</span>


<span class="s1">@ignore_warnings(category=ConvergenceWarning)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s3">&quot;cd&quot;</span><span class="s0">, </span><span class="s3">&quot;mu&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_nmf_decreasing(solver):</span>
    <span class="s2"># test that the objective function is decreasing at each iteration</span>
    <span class="s1">n_samples = </span><span class="s4">20</span>
    <span class="s1">n_features = </span><span class="s4">15</span>
    <span class="s1">n_components = </span><span class="s4">10</span>
    <span class="s1">alpha = </span><span class="s4">0.1</span>
    <span class="s1">l1_ratio = </span><span class="s4">0.5</span>
    <span class="s1">tol = </span><span class="s4">0.0</span>

    <span class="s2"># initialization</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">np.abs(X</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">W0</span><span class="s0">, </span><span class="s1">H0 = nmf._initialize_nmf(X</span><span class="s0">, </span><span class="s1">n_components</span><span class="s0">, </span><span class="s1">init=</span><span class="s3">&quot;random&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">beta_loss </span><span class="s0">in </span><span class="s1">(-</span><span class="s4">1.2</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0.2</span><span class="s0">, </span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">2.0</span><span class="s0">, </span><span class="s4">2.5</span><span class="s1">):</span>
        <span class="s0">if </span><span class="s1">solver != </span><span class="s3">&quot;mu&quot; </span><span class="s0">and </span><span class="s1">beta_loss != </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s2"># not implemented</span>
            <span class="s0">continue</span>
        <span class="s1">W</span><span class="s0">, </span><span class="s1">H = W0.copy()</span><span class="s0">, </span><span class="s1">H0.copy()</span>
        <span class="s1">previous_loss = </span><span class="s0">None</span>
        <span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">30</span><span class="s1">):</span>
            <span class="s2"># one more iteration starting from the previous results</span>
            <span class="s1">W</span><span class="s0">, </span><span class="s1">H</span><span class="s0">, </span><span class="s1">_ = non_negative_factorization(</span>
                <span class="s1">X</span><span class="s0">,</span>
                <span class="s1">W</span><span class="s0">,</span>
                <span class="s1">H</span><span class="s0">,</span>
                <span class="s1">beta_loss=beta_loss</span><span class="s0">,</span>
                <span class="s1">init=</span><span class="s3">&quot;custom&quot;</span><span class="s0">,</span>
                <span class="s1">n_components=n_components</span><span class="s0">,</span>
                <span class="s1">max_iter=</span><span class="s4">1</span><span class="s0">,</span>
                <span class="s1">alpha_W=alpha</span><span class="s0">,</span>
                <span class="s1">solver=solver</span><span class="s0">,</span>
                <span class="s1">tol=tol</span><span class="s0">,</span>
                <span class="s1">l1_ratio=l1_ratio</span><span class="s0">,</span>
                <span class="s1">verbose=</span><span class="s4">0</span><span class="s0">,</span>
                <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
                <span class="s1">update_H=</span><span class="s0">True,</span>
            <span class="s1">)</span>

            <span class="s1">loss = (</span>
                <span class="s1">nmf._beta_divergence(X</span><span class="s0">, </span><span class="s1">W</span><span class="s0">, </span><span class="s1">H</span><span class="s0">, </span><span class="s1">beta_loss)</span>
                <span class="s1">+ alpha * l1_ratio * n_features * W.sum()</span>
                <span class="s1">+ alpha * l1_ratio * n_samples * H.sum()</span>
                <span class="s1">+ alpha * (</span><span class="s4">1 </span><span class="s1">- l1_ratio) * n_features * (W**</span><span class="s4">2</span><span class="s1">).sum()</span>
                <span class="s1">+ alpha * (</span><span class="s4">1 </span><span class="s1">- l1_ratio) * n_samples * (H**</span><span class="s4">2</span><span class="s1">).sum()</span>
            <span class="s1">)</span>
            <span class="s0">if </span><span class="s1">previous_loss </span><span class="s0">is not None</span><span class="s1">:</span>
                <span class="s0">assert </span><span class="s1">previous_loss &gt; loss</span>
            <span class="s1">previous_loss = loss</span>


<span class="s0">def </span><span class="s1">test_nmf_underflow():</span>
    <span class="s2"># Regression test for an underflow issue in _beta_divergence</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features</span><span class="s0">, </span><span class="s1">n_components = </span><span class="s4">10</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2</span>
    <span class="s1">X = np.abs(rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)) * </span><span class="s4">10</span>
    <span class="s1">W = np.abs(rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_components)) * </span><span class="s4">10</span>
    <span class="s1">H = np.abs(rng.randn(n_components</span><span class="s0">, </span><span class="s1">n_features))</span>

    <span class="s1">X[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">0</span>
    <span class="s1">ref = nmf._beta_divergence(X</span><span class="s0">, </span><span class="s1">W</span><span class="s0">, </span><span class="s1">H</span><span class="s0">, </span><span class="s1">beta=</span><span class="s4">1.0</span><span class="s1">)</span>
    <span class="s1">X[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">1e-323</span>
    <span class="s1">res = nmf._beta_divergence(X</span><span class="s0">, </span><span class="s1">W</span><span class="s0">, </span><span class="s1">H</span><span class="s0">, </span><span class="s1">beta=</span><span class="s4">1.0</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(res</span><span class="s0">, </span><span class="s1">ref)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;dtype_in, dtype_out&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(np.float32</span><span class="s0">, </span><span class="s1">np.float32)</span><span class="s0">,</span>
        <span class="s1">(np.float64</span><span class="s0">, </span><span class="s1">np.float64)</span><span class="s0">,</span>
        <span class="s1">(np.int32</span><span class="s0">, </span><span class="s1">np.float64)</span><span class="s0">,</span>
        <span class="s1">(np.int64</span><span class="s0">, </span><span class="s1">np.float64)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s1">[</span><span class="s3">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s3">&quot;solver&quot;</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">[[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;cd&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;mu&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[MiniBatchNMF</span><span class="s0">, </span><span class="s1">{}]]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_nmf_dtype_match(Estimator</span><span class="s0">, </span><span class="s1">solver</span><span class="s0">, </span><span class="s1">dtype_in</span><span class="s0">, </span><span class="s1">dtype_out):</span>
    <span class="s2"># Check that NMF preserves dtype (float32 and float64)</span>
    <span class="s1">X = np.random.RandomState(</span><span class="s4">0</span><span class="s1">).randn(</span><span class="s4">20</span><span class="s0">, </span><span class="s4">15</span><span class="s1">).astype(dtype_in</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">np.abs(X</span><span class="s0">, </span><span class="s1">out=X)</span>

    <span class="s1">nmf = Estimator(alpha_W=</span><span class="s4">1.0</span><span class="s0">, </span><span class="s1">alpha_H=</span><span class="s4">1.0</span><span class="s0">, </span><span class="s1">tol=</span><span class="s4">1e-2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">**solver)</span>

    <span class="s0">assert </span><span class="s1">nmf.fit(X).transform(X).dtype == dtype_out</span>
    <span class="s0">assert </span><span class="s1">nmf.fit_transform(X).dtype == dtype_out</span>
    <span class="s0">assert </span><span class="s1">nmf.components_.dtype == dtype_out</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s1">[</span><span class="s3">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s3">&quot;solver&quot;</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">[[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;cd&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[NMF</span><span class="s0">, </span><span class="s1">{</span><span class="s3">&quot;solver&quot;</span><span class="s1">: </span><span class="s3">&quot;mu&quot;</span><span class="s1">}]</span><span class="s0">, </span><span class="s1">[MiniBatchNMF</span><span class="s0">, </span><span class="s1">{}]]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_nmf_float32_float64_consistency(Estimator</span><span class="s0">, </span><span class="s1">solver):</span>
    <span class="s2"># Check that the result of NMF is the same between float32 and float64</span>
    <span class="s1">X = np.random.RandomState(</span><span class="s4">0</span><span class="s1">).randn(</span><span class="s4">50</span><span class="s0">, </span><span class="s4">7</span><span class="s1">)</span>
    <span class="s1">np.abs(X</span><span class="s0">, </span><span class="s1">out=X)</span>
    <span class="s1">nmf32 = Estimator(random_state=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">tol=</span><span class="s4">1e-3</span><span class="s0">, </span><span class="s1">**solver)</span>
    <span class="s1">W32 = nmf32.fit_transform(X.astype(np.float32))</span>
    <span class="s1">nmf64 = Estimator(random_state=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">tol=</span><span class="s4">1e-3</span><span class="s0">, </span><span class="s1">**solver)</span>
    <span class="s1">W64 = nmf64.fit_transform(X)</span>

    <span class="s1">assert_allclose(W32</span><span class="s0">, </span><span class="s1">W64</span><span class="s0">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s1">[NMF</span><span class="s0">, </span><span class="s1">MiniBatchNMF])</span>
<span class="s0">def </span><span class="s1">test_nmf_custom_init_dtype_error(Estimator):</span>
    <span class="s2"># Check that an error is raise if custom H and/or W don't have the same</span>
    <span class="s2"># dtype as X.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = rng.random_sample((</span><span class="s4">20</span><span class="s0">, </span><span class="s4">15</span><span class="s1">))</span>
    <span class="s1">H = rng.random_sample((</span><span class="s4">15</span><span class="s0">, </span><span class="s4">15</span><span class="s1">)).astype(np.float32)</span>
    <span class="s1">W = rng.random_sample((</span><span class="s4">20</span><span class="s0">, </span><span class="s4">15</span><span class="s1">))</span>

    <span class="s0">with </span><span class="s1">pytest.raises(TypeError</span><span class="s0">, </span><span class="s1">match=</span><span class="s3">&quot;should have the same dtype as X&quot;</span><span class="s1">):</span>
        <span class="s1">Estimator(init=</span><span class="s3">&quot;custom&quot;</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">H=H</span><span class="s0">, </span><span class="s1">W=W)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(TypeError</span><span class="s0">, </span><span class="s1">match=</span><span class="s3">&quot;should have the same dtype as X&quot;</span><span class="s1">):</span>
        <span class="s1">non_negative_factorization(X</span><span class="s0">, </span><span class="s1">H=H</span><span class="s0">, </span><span class="s1">update_H=</span><span class="s0">False</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;beta_loss&quot;</span><span class="s0">, </span><span class="s1">[-</span><span class="s4">0.5</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0.5</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1.5</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">2.5</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_nmf_minibatchnmf_equivalence(beta_loss):</span>
    <span class="s2"># Test that MiniBatchNMF is equivalent to NMF when batch_size = n_samples and</span>
    <span class="s2"># forget_factor 0.0 (stopping criterion put aside)</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">X = np.abs(rng.randn(</span><span class="s4">48</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>

    <span class="s1">nmf = NMF(</span>
        <span class="s1">n_components=</span><span class="s4">5</span><span class="s0">,</span>
        <span class="s1">beta_loss=beta_loss</span><span class="s0">,</span>
        <span class="s1">solver=</span><span class="s3">&quot;mu&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s4">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">mbnmf = MiniBatchNMF(</span>
        <span class="s1">n_components=</span><span class="s4">5</span><span class="s0">,</span>
        <span class="s1">beta_loss=beta_loss</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">max_no_improvement=</span><span class="s0">None,</span>
        <span class="s1">batch_size=X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">forget_factor=</span><span class="s4">0.0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">W = nmf.fit_transform(X)</span>
    <span class="s1">mbW = mbnmf.fit_transform(X)</span>
    <span class="s1">assert_allclose(W</span><span class="s0">, </span><span class="s1">mbW)</span>


<span class="s0">def </span><span class="s1">test_minibatch_nmf_partial_fit():</span>
    <span class="s2"># Check fit / partial_fit equivalence. Applicable only with fresh restarts.</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">X = np.abs(rng.randn(</span><span class="s4">100</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>

    <span class="s1">n_components = </span><span class="s4">5</span>
    <span class="s1">batch_size = </span><span class="s4">10</span>
    <span class="s1">max_iter = </span><span class="s4">2</span>

    <span class="s1">mbnmf1 = MiniBatchNMF(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s3">&quot;custom&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s0">,</span>
        <span class="s1">batch_size=batch_size</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">max_no_improvement=</span><span class="s0">None,</span>
        <span class="s1">fresh_restarts=</span><span class="s0">False,</span>
    <span class="s1">)</span>
    <span class="s1">mbnmf2 = MiniBatchNMF(n_components=n_components</span><span class="s0">, </span><span class="s1">init=</span><span class="s3">&quot;custom&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s2"># Force the same init of H (W is recomputed anyway) to be able to compare results.</span>
    <span class="s1">W</span><span class="s0">, </span><span class="s1">H = nmf._initialize_nmf(</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">n_components=n_components</span><span class="s0">, </span><span class="s1">init=</span><span class="s3">&quot;random&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>

    <span class="s1">mbnmf1.fit(X</span><span class="s0">, </span><span class="s1">W=W</span><span class="s0">, </span><span class="s1">H=H)</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(max_iter):</span>
        <span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range(batch_size):</span>
            <span class="s1">mbnmf2.partial_fit(X[j : j + batch_size]</span><span class="s0">, </span><span class="s1">W=W[:batch_size]</span><span class="s0">, </span><span class="s1">H=H)</span>

    <span class="s0">assert </span><span class="s1">mbnmf1.n_steps_ == mbnmf2.n_steps_</span>
    <span class="s1">assert_allclose(mbnmf1.components_</span><span class="s0">, </span><span class="s1">mbnmf2.components_)</span>


<span class="s0">def </span><span class="s1">test_feature_names_out():</span>
    <span class="s5">&quot;&quot;&quot;Check feature names out for NMF.&quot;&quot;&quot;</span>
    <span class="s1">random_state = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = np.abs(random_state.randn(</span><span class="s4">10</span><span class="s0">, </span><span class="s4">4</span><span class="s1">))</span>
    <span class="s1">nmf = NMF(n_components=</span><span class="s4">3</span><span class="s1">).fit(X)</span>

    <span class="s1">names = nmf.get_feature_names_out()</span>
    <span class="s1">assert_array_equal([</span><span class="s3">f&quot;nmf</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s3">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">3</span><span class="s1">)]</span><span class="s0">, </span><span class="s1">names)</span>


<span class="s0">def </span><span class="s1">test_minibatch_nmf_verbose():</span>
    <span class="s2"># Check verbose mode of MiniBatchNMF for better coverage.</span>
    <span class="s1">A = np.random.RandomState(</span><span class="s4">0</span><span class="s1">).random_sample((</span><span class="s4">100</span><span class="s0">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">nmf = MiniBatchNMF(tol=</span><span class="s4">1e-2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">old_stdout = sys.stdout</span>
    <span class="s1">sys.stdout = StringIO()</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">nmf.fit(A)</span>
    <span class="s0">finally</span><span class="s1">:</span>
        <span class="s1">sys.stdout = old_stdout</span>


<span class="s2"># TODO(1.5): remove this test</span>
<span class="s0">def </span><span class="s1">test_NMF_inverse_transform_W_deprecation():</span>
    <span class="s1">rng = np.random.mtrand.RandomState(</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">A = np.abs(rng.randn(</span><span class="s4">6</span><span class="s0">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">est = NMF(</span>
        <span class="s1">n_components=</span><span class="s4">3</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s3">&quot;random&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s4">1e-6</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">Xt = est.fit_transform(A)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(TypeError</span><span class="s0">, </span><span class="s1">match=</span><span class="s3">&quot;Missing required positional argument&quot;</span><span class="s1">):</span>
        <span class="s1">est.inverse_transform()</span>

    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s3">&quot;Please provide only&quot;</span><span class="s1">):</span>
        <span class="s1">est.inverse_transform(Xt=Xt</span><span class="s0">, </span><span class="s1">W=Xt)</span>

    <span class="s0">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s1">warnings.simplefilter(</span><span class="s3">&quot;error&quot;</span><span class="s1">)</span>
        <span class="s1">est.inverse_transform(Xt)</span>

    <span class="s0">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s0">, </span><span class="s1">match=</span><span class="s3">&quot;Input argument `W` was renamed to `Xt`&quot;</span><span class="s1">):</span>
        <span class="s1">est.inverse_transform(W=Xt)</span>
</pre>
</body>
</html>