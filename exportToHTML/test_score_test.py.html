<html>
<head>
<title>test_score_test.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_score_test.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot; 
Created on Thu May 31 15:39:15 2018 
 
Author: Josef Perktold 
&quot;&quot;&quot;</span>


<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">numpy.testing </span><span class="s3">import </span><span class="s1">assert_allclose</span>

<span class="s3">from </span><span class="s1">statsmodels.genmod.generalized_linear_model </span><span class="s3">import </span><span class="s1">GLM</span>
<span class="s3">from </span><span class="s1">statsmodels.genmod </span><span class="s3">import </span><span class="s1">families</span>
<span class="s3">from </span><span class="s1">statsmodels.discrete.discrete_model </span><span class="s3">import </span><span class="s1">Poisson</span>
<span class="s3">import </span><span class="s1">statsmodels.stats._diagnostic_other </span><span class="s3">as </span><span class="s1">diao</span>
<span class="s3">import </span><span class="s1">statsmodels.discrete._diagnostics_count </span><span class="s3">as </span><span class="s1">diac</span>
<span class="s3">from </span><span class="s1">statsmodels.base._parameter_inference </span><span class="s3">import </span><span class="s1">score_test</span>


<span class="s3">class </span><span class="s1">CheckScoreTest():</span>

    <span class="s3">def </span><span class="s1">test_wald_score(self):</span>
        <span class="s1">mod_full = self.model_full</span>
        <span class="s1">mod_drop = self.model_drop</span>
        <span class="s1">restriction = </span><span class="s4">'x5=0, x6=0'</span>
        <span class="s1">res_full = mod_full.fit()</span>
        <span class="s1">res_constr = mod_full.fit_constrained(</span><span class="s4">'x5=0, x6=0'</span><span class="s1">)</span>
        <span class="s1">res_drop = mod_drop.fit()</span>

        <span class="s1">wald = res_full.wald_test(restriction</span><span class="s3">, </span><span class="s1">scalar=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s0"># note: need to use method for res_constr for correct df_resid</span>
        <span class="s1">lm_constr = np.hstack(res_constr.score_test())</span>
        <span class="s1">lm_extra = np.hstack(score_test(res_drop</span><span class="s3">, </span><span class="s1">exog_extra=self.exog_extra))</span>
        <span class="s1">lm_full = np.hstack(res_full.score_test(</span>
            <span class="s1">params_constrained=res_constr.params</span><span class="s3">,</span>
            <span class="s1">k_constraints=res_constr.k_constr))</span>

        <span class="s1">res_wald = np.hstack([wald.statistic</span><span class="s3">, </span><span class="s1">wald.pvalue</span><span class="s3">, </span><span class="s1">[wald.df_denom]])</span>
        <span class="s1">assert_allclose(lm_constr</span><span class="s3">, </span><span class="s1">res_wald</span><span class="s3">, </span><span class="s1">rtol=self.rtol_ws</span><span class="s3">, </span><span class="s1">atol=self.atol_ws)</span>
        <span class="s1">assert_allclose(lm_extra</span><span class="s3">, </span><span class="s1">res_wald</span><span class="s3">, </span><span class="s1">rtol=self.rtol_ws</span><span class="s3">, </span><span class="s1">atol=self.atol_ws)</span>
        <span class="s1">assert_allclose(lm_constr</span><span class="s3">, </span><span class="s1">lm_extra</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-12</span><span class="s3">, </span><span class="s1">atol=</span><span class="s5">1e-14</span><span class="s1">)</span>
        <span class="s1">assert_allclose(lm_full</span><span class="s3">, </span><span class="s1">lm_extra</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-12</span><span class="s3">, </span><span class="s1">atol=</span><span class="s5">1e-14</span><span class="s1">)</span>
        <span class="s0"># regression number</span>
        <span class="s1">assert_allclose(lm_constr[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">self.res_pvalue[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-12</span><span class="s3">, </span><span class="s1">atol=</span><span class="s5">1e-14</span><span class="s1">)</span>

        <span class="s1">cov_type=</span><span class="s4">'HC0'</span>
        <span class="s1">res_full_hc = mod_full.fit(cov_type=cov_type</span><span class="s3">, </span><span class="s1">start_params=res_full.params)</span>
        <span class="s1">wald = res_full_hc.wald_test(restriction</span><span class="s3">, </span><span class="s1">scalar=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">lm_constr = np.hstack(score_test(res_constr</span><span class="s3">, </span><span class="s1">cov_type=cov_type))</span>
        <span class="s1">lm_extra = np.hstack(score_test(res_drop</span><span class="s3">, </span><span class="s1">exog_extra=self.exog_extra</span><span class="s3">,</span>
                                        <span class="s1">cov_type=cov_type))</span>

        <span class="s1">res_wald = np.hstack([wald.statistic</span><span class="s3">, </span><span class="s1">wald.pvalue</span><span class="s3">, </span><span class="s1">[wald.df_denom]])</span>
        <span class="s1">assert_allclose(lm_constr</span><span class="s3">, </span><span class="s1">res_wald</span><span class="s3">, </span><span class="s1">rtol=self.rtol_ws</span><span class="s3">, </span><span class="s1">atol=self.atol_ws)</span>
        <span class="s1">assert_allclose(lm_extra</span><span class="s3">, </span><span class="s1">res_wald</span><span class="s3">, </span><span class="s1">rtol=self.rtol_ws</span><span class="s3">, </span><span class="s1">atol=self.atol_ws)</span>
        <span class="s1">assert_allclose(lm_constr</span><span class="s3">, </span><span class="s1">lm_extra</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-13</span><span class="s1">)</span>
        <span class="s0"># regression number</span>
        <span class="s1">assert_allclose(lm_constr[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">self.res_pvalue[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-12</span><span class="s3">, </span><span class="s1">atol=</span><span class="s5">1e-14</span><span class="s1">)</span>

        <span class="s3">if not </span><span class="s1">self.skip_wooldridge:</span>
            <span class="s0"># compare with Wooldridge auxiliary regression</span>
            <span class="s0"># does not work for Poisson, even with family attribute</span>
            <span class="s0"># diao.lm_test_glm assumes fittedvalues is mean (not linear pred)</span>
            <span class="s1">lm_wooldridge = diao.lm_test_glm(res_drop</span><span class="s3">, </span><span class="s1">self.exog_extra)</span>
            <span class="s1">assert_allclose(lm_wooldridge.pval1</span><span class="s3">, </span><span class="s1">self.res_pvalue[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                            <span class="s1">rtol=</span><span class="s5">1e-12</span><span class="s3">, </span><span class="s1">atol=</span><span class="s5">1e-14</span><span class="s1">)</span>
            <span class="s1">assert_allclose(lm_wooldridge.pval3</span><span class="s3">, </span><span class="s1">self.res_pvalue[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">,</span>
                            <span class="s1">rtol=self.rtol_wooldridge)</span>
            <span class="s0"># smoke test</span>
            <span class="s1">lm_wooldridge.summary()</span>


<span class="s3">class </span><span class="s1">TestScoreTest(CheckScoreTest):</span>
    <span class="s0"># compares score to wald, and regression test for pvalues</span>
    <span class="s1">rtol_ws = </span><span class="s5">5e-3</span>
    <span class="s1">atol_ws = </span><span class="s5">0</span>
    <span class="s1">rtol_wooldridge = </span><span class="s5">0.004</span>
    <span class="s1">dispersed = </span><span class="s3">False  </span><span class="s0"># Poisson correctly specified</span>
    <span class="s0"># regression numbers</span>
    <span class="s1">res_pvalue = [</span><span class="s5">0.31786373532550893</span><span class="s3">, </span><span class="s5">0.32654081685271297</span><span class="s1">]</span>
    <span class="s1">skip_wooldridge = </span><span class="s3">False</span>
    <span class="s1">res_disptest = np.array([</span>
        <span class="s1">[</span><span class="s5">0.1392791916012637</span><span class="s3">, </span><span class="s5">0.8892295323009857</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">0.1392791916012645</span><span class="s3">, </span><span class="s5">0.8892295323009850</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">0.2129554490802097</span><span class="s3">, </span><span class="s5">0.8313617120611572</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">0.1493501809372359</span><span class="s3">, </span><span class="s5">0.8812773205886350</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">0.1493501809372359</span><span class="s3">, </span><span class="s5">0.8812773205886350</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">0.1454862255574059</span><span class="s3">, </span><span class="s5">0.8843269904545624</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">0.2281321688124869</span><span class="s3">, </span><span class="s5">0.8195434922982738</span><span class="s1">]</span>
        <span class="s1">])</span>
    <span class="s1">res_disptest_g = [</span><span class="s5">0.052247629593715761</span><span class="s3">, </span><span class="s5">0.81919738867722225</span><span class="s1">]</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">nobs</span><span class="s3">, </span><span class="s1">k_vars = </span><span class="s5">500</span><span class="s3">, </span><span class="s5">5</span>

        <span class="s1">np.random.seed(</span><span class="s5">786452</span><span class="s1">)</span>
        <span class="s1">x = np.random.randn(nobs</span><span class="s3">, </span><span class="s1">k_vars)</span>
        <span class="s1">x[:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = </span><span class="s5">1</span>
        <span class="s1">x2 = np.random.randn(nobs</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span>
        <span class="s1">xx = np.column_stack((x</span><span class="s3">, </span><span class="s1">x2))</span>

        <span class="s3">if </span><span class="s1">cls.dispersed:</span>
            <span class="s1">het = np.random.randn(nobs)</span>
            <span class="s1">y = np.random.poisson(np.exp(x.sum(</span><span class="s5">1</span><span class="s1">) * </span><span class="s5">0.5 </span><span class="s1">+ het))</span>
            <span class="s0">#y_mc = np.random.negative_binomial(np.exp(x.sum(1) * 0.5), 2)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">y = np.random.poisson(np.exp(x.sum(</span><span class="s5">1</span><span class="s1">) * </span><span class="s5">0.5</span><span class="s1">))</span>

        <span class="s1">cls.exog_extra = x2</span>
        <span class="s1">cls.model_full = GLM(y</span><span class="s3">, </span><span class="s1">xx</span><span class="s3">, </span><span class="s1">family=families.Poisson())</span>
        <span class="s1">cls.model_drop = GLM(y</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">family=families.Poisson())</span>

    <span class="s3">def </span><span class="s1">test_dispersion(self):</span>
        <span class="s1">res_drop = self.model_drop.fit()</span>
        <span class="s1">res_test = diac.test_poisson_dispersion(res_drop)</span>
        <span class="s1">res_test_ = np.column_stack((res_test.statistic</span><span class="s3">, </span><span class="s1">res_test.pvalue))</span>
        <span class="s1">assert_allclose(res_test_</span><span class="s3">, </span><span class="s1">self.res_disptest</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-6</span><span class="s3">, </span><span class="s1">atol=</span><span class="s5">1e-14</span><span class="s1">)</span>
        <span class="s0"># constant only dispersion</span>
        <span class="s1">ex = np.ones((res_drop.model.endog.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s0"># ex = np.column_stack((np.ones(res_drop.model.endog.shape[0]),</span>
        <span class="s0">#                      res_drop.predict()))  # or **2</span>
        <span class="s0"># dispersion_poisson_generic might not be correct</span>
        <span class="s0"># or not clear what the alternative hypothesis is</span>
        <span class="s0"># choosing different `ex` implies different alternative hypotheses</span>
        <span class="s1">res_test = diac._test_poisson_dispersion_generic(res_drop</span><span class="s3">, </span><span class="s1">ex)</span>
        <span class="s1">assert_allclose(res_test</span><span class="s3">, </span><span class="s1">self.res_disptest_g</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-6</span><span class="s3">, </span><span class="s1">atol=</span><span class="s5">1e-14</span><span class="s1">)</span>


<span class="s3">class </span><span class="s1">TestScoreTestDispersed(TestScoreTest):</span>
    <span class="s1">rtol_ws = </span><span class="s5">0.11</span>
    <span class="s1">atol_ws = </span><span class="s5">0.015</span>
    <span class="s1">rtol_wooldridge = </span><span class="s5">0.03</span>
    <span class="s1">dispersed = </span><span class="s3">True  </span><span class="s0"># Poisson is mis-specified</span>
    <span class="s1">res_pvalue = [</span><span class="s5">5.412978775609189e-14</span><span class="s3">, </span><span class="s5">0.05027602575743518</span><span class="s1">]</span>
    <span class="s1">res_disptest = np.array([</span>
        <span class="s1">[</span><span class="s5">1.2647363371056005e+02</span><span class="s3">, </span><span class="s5">0.0000000000000000e+00</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">1.2647363371056124e+02</span><span class="s3">, </span><span class="s5">0.0000000000000000e+00</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">1.1939362149777617e+02</span><span class="s3">, </span><span class="s5">0.0000000000000000e+00</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">4.5394051864300318e+00</span><span class="s3">, </span><span class="s5">5.6413139746586543e-06</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">4.5394051864300318e+00</span><span class="s3">, </span><span class="s5">5.6413139746586543e-06</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">2.9164548934767525e+00</span><span class="s3">, </span><span class="s5">3.5403391013549782e-03</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">4.2714141112771529e+00</span><span class="s3">, </span><span class="s5">1.9423733575592056e-05</span><span class="s1">]</span>
        <span class="s1">])</span>
    <span class="s1">res_disptest_g = [</span><span class="s5">17.670784788586968</span><span class="s3">, </span><span class="s5">2.6262956791721383e-05</span><span class="s1">]</span>


<span class="s3">class </span><span class="s1">TestScoreTestPoisson(TestScoreTest):</span>
    <span class="s0"># same as GLM above but for discrete Poisson</span>
    <span class="s0"># compares score to wald, and regression test for pvalues</span>
    <span class="s1">rtol_ws = </span><span class="s5">5e-3</span>
    <span class="s1">atol_ws = </span><span class="s5">0</span>
    <span class="s1">rtol_wooldridge = </span><span class="s5">0.004</span>
    <span class="s1">dispersed = </span><span class="s3">False  </span><span class="s0"># Poisson correctly specified</span>
    <span class="s0"># regression numbers</span>
    <span class="s1">res_pvalue = [</span><span class="s5">0.31786373532550893</span><span class="s3">, </span><span class="s5">0.32654081685271297</span><span class="s1">]</span>
    <span class="s1">skip_wooldridge = </span><span class="s3">False</span>
    <span class="s1">res_disptest = np.array([</span>
        <span class="s1">[</span><span class="s5">0.1392791916012637</span><span class="s3">, </span><span class="s5">0.8892295323009857</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">0.1392791916012645</span><span class="s3">, </span><span class="s5">0.8892295323009850</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">0.2129554490802097</span><span class="s3">, </span><span class="s5">0.8313617120611572</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">0.1493501809372359</span><span class="s3">, </span><span class="s5">0.8812773205886350</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">0.1493501809372359</span><span class="s3">, </span><span class="s5">0.8812773205886350</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">0.1454862255574059</span><span class="s3">, </span><span class="s5">0.8843269904545624</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">0.2281321688124869</span><span class="s3">, </span><span class="s5">0.8195434922982738</span><span class="s1">]</span>
        <span class="s1">])</span>
    <span class="s1">res_disptest_g = [</span><span class="s5">0.052247629593715761</span><span class="s3">, </span><span class="s5">0.81919738867722225</span><span class="s1">]</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0"># copy-paste except for model</span>
        <span class="s1">nobs</span><span class="s3">, </span><span class="s1">k_vars = </span><span class="s5">500</span><span class="s3">, </span><span class="s5">5</span>

        <span class="s1">np.random.seed(</span><span class="s5">786452</span><span class="s1">)</span>
        <span class="s1">x = np.random.randn(nobs</span><span class="s3">, </span><span class="s1">k_vars)</span>
        <span class="s1">x[:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = </span><span class="s5">1</span>
        <span class="s1">x2 = np.random.randn(nobs</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span>
        <span class="s1">xx = np.column_stack((x</span><span class="s3">, </span><span class="s1">x2))</span>

        <span class="s3">if </span><span class="s1">cls.dispersed:</span>
            <span class="s1">het = np.random.randn(nobs)</span>
            <span class="s1">y = np.random.poisson(np.exp(x.sum(</span><span class="s5">1</span><span class="s1">) * </span><span class="s5">0.5 </span><span class="s1">+ het))</span>
            <span class="s0">#y_mc = np.random.negative_binomial(np.exp(x.sum(1) * 0.5), 2)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">y = np.random.poisson(np.exp(x.sum(</span><span class="s5">1</span><span class="s1">) * </span><span class="s5">0.5</span><span class="s1">))</span>

        <span class="s1">cls.exog_extra = x2</span>
        <span class="s1">cls.model_full = Poisson(y</span><span class="s3">, </span><span class="s1">xx)</span>
        <span class="s1">cls.model_drop = Poisson(y</span><span class="s3">, </span><span class="s1">x)</span>

    <span class="s3">def </span><span class="s1">test_wald_score(self):</span>
        <span class="s1">super(TestScoreTestPoisson</span><span class="s3">, </span><span class="s1">self).test_wald_score()</span>


<span class="s3">class </span><span class="s1">TestScoreTestPoissonDispersed(TestScoreTestPoisson):</span>
    <span class="s0"># same as GLM above but for discrete Poisson</span>
    <span class="s1">rtol_ws = </span><span class="s5">0.11</span>
    <span class="s1">atol_ws = </span><span class="s5">0.015</span>
    <span class="s1">rtol_wooldridge = </span><span class="s5">0.03</span>
    <span class="s1">dispersed = </span><span class="s3">True  </span><span class="s0"># Poisson is mis-specified</span>
    <span class="s1">res_pvalue = [</span><span class="s5">5.412978775609189e-14</span><span class="s3">, </span><span class="s5">0.05027602575743518</span><span class="s1">]</span>
    <span class="s1">res_disptest = np.array([</span>
        <span class="s1">[</span><span class="s5">1.2647363371056005e+02</span><span class="s3">, </span><span class="s5">0.0000000000000000e+00</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">1.2647363371056124e+02</span><span class="s3">, </span><span class="s5">0.0000000000000000e+00</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">1.1939362149777617e+02</span><span class="s3">, </span><span class="s5">0.0000000000000000e+00</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">4.5394051864300318e+00</span><span class="s3">, </span><span class="s5">5.6413139746586543e-06</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">4.5394051864300318e+00</span><span class="s3">, </span><span class="s5">5.6413139746586543e-06</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">2.9164548934767525e+00</span><span class="s3">, </span><span class="s5">3.5403391013549782e-03</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">4.2714141112771529e+00</span><span class="s3">, </span><span class="s5">1.9423733575592056e-05</span><span class="s1">]</span>
        <span class="s1">])</span>
    <span class="s1">res_disptest_g = [</span><span class="s5">17.670784788586968</span><span class="s3">, </span><span class="s5">2.6262956791721383e-05</span><span class="s1">]</span>


<span class="s3">class </span><span class="s1">TestScoreTestGaussian(CheckScoreTest):</span>
    <span class="s0"># compares score to wald, and regression test for pvalues</span>
    <span class="s1">rtol_ws = </span><span class="s5">0.01</span>
    <span class="s1">atol_ws = </span><span class="s5">0</span>
    <span class="s1">rtol_wooldridge = </span><span class="s5">0.06</span>
    <span class="s1">dispersed = </span><span class="s3">False</span>
    <span class="s0"># regression numbers for nonrobust and HC0</span>
    <span class="s1">res_pvalue = [</span><span class="s5">0.44423875090566467</span><span class="s3">, </span><span class="s5">0.4370837418475849</span><span class="s1">]</span>
    <span class="s0"># TODO: check why wooldrige is not very close, which pval1, pval2, pval3 ?</span>
    <span class="s1">skip_wooldridge = </span><span class="s3">True</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">nobs</span><span class="s3">, </span><span class="s1">k_vars = </span><span class="s5">500</span><span class="s3">, </span><span class="s5">5</span>

        <span class="s1">np.random.seed(</span><span class="s5">786452</span><span class="s1">)</span>
        <span class="s1">x = np.random.randn(nobs</span><span class="s3">, </span><span class="s1">k_vars)</span>
        <span class="s1">x[:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = </span><span class="s5">1</span>
        <span class="s1">x2 = np.random.randn(nobs</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span>
        <span class="s1">xx = np.column_stack((x</span><span class="s3">, </span><span class="s1">x2))</span>

        <span class="s3">if </span><span class="s1">cls.dispersed:</span>
            <span class="s1">het = np.random.randn(nobs)</span>
            <span class="s1">y = np.random.randn(nobs) + x.sum(</span><span class="s5">1</span><span class="s1">) * </span><span class="s5">0.5 </span><span class="s1">+ het</span>
            <span class="s0">#y_mc = np.random.negative_binomial(np.exp(x.sum(1) * 0.5), 2)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">y = np.random.randn(nobs) + x.sum(</span><span class="s5">1</span><span class="s1">) * </span><span class="s5">0.5</span>

        <span class="s1">cls.exog_extra = x2</span>
        <span class="s1">cls.model_full = GLM(y</span><span class="s3">, </span><span class="s1">xx</span><span class="s3">, </span><span class="s1">family=families.Gaussian())</span>
        <span class="s1">cls.model_drop = GLM(y</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">family=families.Gaussian())</span>
</pre>
</body>
</html>