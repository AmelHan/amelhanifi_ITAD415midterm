<html>
<head>
<title>_bagging.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_bagging.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Bagging meta-estimator.&quot;&quot;&quot;</span>

<span class="s2"># Author: Gilles Louppe &lt;g.louppe@gmail.com&gt;</span>
<span class="s2"># License: BSD 3 clause</span>


<span class="s3">import </span><span class="s1">itertools</span>
<span class="s3">import </span><span class="s1">numbers</span>
<span class="s3">from </span><span class="s1">abc </span><span class="s3">import </span><span class="s1">ABCMeta</span><span class="s3">, </span><span class="s1">abstractmethod</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span>
<span class="s3">from </span><span class="s1">warnings </span><span class="s3">import </span><span class="s1">warn</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">ClassifierMixin</span><span class="s3">, </span><span class="s1">RegressorMixin</span><span class="s3">, </span><span class="s1">_fit_context</span>
<span class="s3">from </span><span class="s1">..metrics </span><span class="s3">import </span><span class="s1">accuracy_score</span><span class="s3">, </span><span class="s1">r2_score</span>
<span class="s3">from </span><span class="s1">..tree </span><span class="s3">import </span><span class="s1">DecisionTreeClassifier</span><span class="s3">, </span><span class="s1">DecisionTreeRegressor</span>
<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">check_random_state</span><span class="s3">, </span><span class="s1">column_or_1d</span><span class="s3">, </span><span class="s1">indices_to_mask</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">HasMethods</span><span class="s3">, </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">RealNotInt</span><span class="s3">, </span><span class="s1">StrOptions</span>
<span class="s3">from </span><span class="s1">..utils._tags </span><span class="s3">import </span><span class="s1">_safe_tags</span>
<span class="s3">from </span><span class="s1">..utils.metaestimators </span><span class="s3">import </span><span class="s1">available_if</span>
<span class="s3">from </span><span class="s1">..utils.multiclass </span><span class="s3">import </span><span class="s1">check_classification_targets</span>
<span class="s3">from </span><span class="s1">..utils.parallel </span><span class="s3">import </span><span class="s1">Parallel</span><span class="s3">, </span><span class="s1">delayed</span>
<span class="s3">from </span><span class="s1">..utils.random </span><span class="s3">import </span><span class="s1">sample_without_replacement</span>
<span class="s3">from </span><span class="s1">..utils.validation </span><span class="s3">import </span><span class="s1">_check_sample_weight</span><span class="s3">, </span><span class="s1">check_is_fitted</span><span class="s3">, </span><span class="s1">has_fit_parameter</span>
<span class="s3">from </span><span class="s1">._base </span><span class="s3">import </span><span class="s1">BaseEnsemble</span><span class="s3">, </span><span class="s1">_partition_estimators</span>

<span class="s1">__all__ = [</span><span class="s4">&quot;BaggingClassifier&quot;</span><span class="s3">, </span><span class="s4">&quot;BaggingRegressor&quot;</span><span class="s1">]</span>

<span class="s1">MAX_INT = np.iinfo(np.int32).max</span>


<span class="s3">def </span><span class="s1">_generate_indices(random_state</span><span class="s3">, </span><span class="s1">bootstrap</span><span class="s3">, </span><span class="s1">n_population</span><span class="s3">, </span><span class="s1">n_samples):</span>
    <span class="s0">&quot;&quot;&quot;Draw randomly sampled indices.&quot;&quot;&quot;</span>
    <span class="s2"># Draw sample indices</span>
    <span class="s3">if </span><span class="s1">bootstrap:</span>
        <span class="s1">indices = random_state.randint(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">n_population</span><span class="s3">, </span><span class="s1">n_samples)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">indices = sample_without_replacement(</span>
            <span class="s1">n_population</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">random_state=random_state</span>
        <span class="s1">)</span>

    <span class="s3">return </span><span class="s1">indices</span>


<span class="s3">def </span><span class="s1">_generate_bagging_indices(</span>
    <span class="s1">random_state</span><span class="s3">,</span>
    <span class="s1">bootstrap_features</span><span class="s3">,</span>
    <span class="s1">bootstrap_samples</span><span class="s3">,</span>
    <span class="s1">n_features</span><span class="s3">,</span>
    <span class="s1">n_samples</span><span class="s3">,</span>
    <span class="s1">max_features</span><span class="s3">,</span>
    <span class="s1">max_samples</span><span class="s3">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Randomly draw feature and sample indices.&quot;&quot;&quot;</span>
    <span class="s2"># Get valid random state</span>
    <span class="s1">random_state = check_random_state(random_state)</span>

    <span class="s2"># Draw indices</span>
    <span class="s1">feature_indices = _generate_indices(</span>
        <span class="s1">random_state</span><span class="s3">, </span><span class="s1">bootstrap_features</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">, </span><span class="s1">max_features</span>
    <span class="s1">)</span>
    <span class="s1">sample_indices = _generate_indices(</span>
        <span class="s1">random_state</span><span class="s3">, </span><span class="s1">bootstrap_samples</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">max_samples</span>
    <span class="s1">)</span>

    <span class="s3">return </span><span class="s1">feature_indices</span><span class="s3">, </span><span class="s1">sample_indices</span>


<span class="s3">def </span><span class="s1">_parallel_build_estimators(</span>
    <span class="s1">n_estimators</span><span class="s3">,</span>
    <span class="s1">ensemble</span><span class="s3">,</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">y</span><span class="s3">,</span>
    <span class="s1">sample_weight</span><span class="s3">,</span>
    <span class="s1">seeds</span><span class="s3">,</span>
    <span class="s1">total_n_estimators</span><span class="s3">,</span>
    <span class="s1">verbose</span><span class="s3">,</span>
    <span class="s1">check_input</span><span class="s3">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Private function used to build a batch of estimators within a job.&quot;&quot;&quot;</span>
    <span class="s2"># Retrieve settings</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">max_features = ensemble._max_features</span>
    <span class="s1">max_samples = ensemble._max_samples</span>
    <span class="s1">bootstrap = ensemble.bootstrap</span>
    <span class="s1">bootstrap_features = ensemble.bootstrap_features</span>
    <span class="s1">support_sample_weight = has_fit_parameter(ensemble.estimator_</span><span class="s3">, </span><span class="s4">&quot;sample_weight&quot;</span><span class="s1">)</span>
    <span class="s1">has_check_input = has_fit_parameter(ensemble.estimator_</span><span class="s3">, </span><span class="s4">&quot;check_input&quot;</span><span class="s1">)</span>
    <span class="s1">requires_feature_indexing = bootstrap_features </span><span class="s3">or </span><span class="s1">max_features != n_features</span>

    <span class="s3">if not </span><span class="s1">support_sample_weight </span><span class="s3">and </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;The base estimator doesn't support sample weight&quot;</span><span class="s1">)</span>

    <span class="s2"># Build estimators</span>
    <span class="s1">estimators = []</span>
    <span class="s1">estimators_features = []</span>

    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_estimators):</span>
        <span class="s3">if </span><span class="s1">verbose &gt; </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">print(</span>
                <span class="s4">&quot;Building estimator %d of %d for this parallel run (total %d)...&quot;</span>
                <span class="s1">% (i + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">n_estimators</span><span class="s3">, </span><span class="s1">total_n_estimators)</span>
            <span class="s1">)</span>

        <span class="s1">random_state = seeds[i]</span>
        <span class="s1">estimator = ensemble._make_estimator(append=</span><span class="s3">False, </span><span class="s1">random_state=random_state)</span>

        <span class="s3">if </span><span class="s1">has_check_input:</span>
            <span class="s1">estimator_fit = partial(estimator.fit</span><span class="s3">, </span><span class="s1">check_input=check_input)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">estimator_fit = estimator.fit</span>

        <span class="s2"># Draw random feature, sample indices</span>
        <span class="s1">features</span><span class="s3">, </span><span class="s1">indices = _generate_bagging_indices(</span>
            <span class="s1">random_state</span><span class="s3">,</span>
            <span class="s1">bootstrap_features</span><span class="s3">,</span>
            <span class="s1">bootstrap</span><span class="s3">,</span>
            <span class="s1">n_features</span><span class="s3">,</span>
            <span class="s1">n_samples</span><span class="s3">,</span>
            <span class="s1">max_features</span><span class="s3">,</span>
            <span class="s1">max_samples</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s2"># Draw samples, using sample weights, and then fit</span>
        <span class="s3">if </span><span class="s1">support_sample_weight:</span>
            <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">curr_sample_weight = np.ones((n_samples</span><span class="s3">,</span><span class="s1">))</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">curr_sample_weight = sample_weight.copy()</span>

            <span class="s3">if </span><span class="s1">bootstrap:</span>
                <span class="s1">sample_counts = np.bincount(indices</span><span class="s3">, </span><span class="s1">minlength=n_samples)</span>
                <span class="s1">curr_sample_weight *= sample_counts</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">not_indices_mask = ~indices_to_mask(indices</span><span class="s3">, </span><span class="s1">n_samples)</span>
                <span class="s1">curr_sample_weight[not_indices_mask] = </span><span class="s5">0</span>

            <span class="s1">X_ = X[:</span><span class="s3">, </span><span class="s1">features] </span><span class="s3">if </span><span class="s1">requires_feature_indexing </span><span class="s3">else </span><span class="s1">X</span>
            <span class="s1">estimator_fit(X_</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=curr_sample_weight)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">X_ = X[indices][:</span><span class="s3">, </span><span class="s1">features] </span><span class="s3">if </span><span class="s1">requires_feature_indexing </span><span class="s3">else </span><span class="s1">X[indices]</span>
            <span class="s1">estimator_fit(X_</span><span class="s3">, </span><span class="s1">y[indices])</span>

        <span class="s1">estimators.append(estimator)</span>
        <span class="s1">estimators_features.append(features)</span>

    <span class="s3">return </span><span class="s1">estimators</span><span class="s3">, </span><span class="s1">estimators_features</span>


<span class="s3">def </span><span class="s1">_parallel_predict_proba(estimators</span><span class="s3">, </span><span class="s1">estimators_features</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">n_classes):</span>
    <span class="s0">&quot;&quot;&quot;Private function used to compute (proba-)predictions within a job.&quot;&quot;&quot;</span>
    <span class="s1">n_samples = X.shape[</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">proba = np.zeros((n_samples</span><span class="s3">, </span><span class="s1">n_classes))</span>

    <span class="s3">for </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">features </span><span class="s3">in </span><span class="s1">zip(estimators</span><span class="s3">, </span><span class="s1">estimators_features):</span>
        <span class="s3">if </span><span class="s1">hasattr(estimator</span><span class="s3">, </span><span class="s4">&quot;predict_proba&quot;</span><span class="s1">):</span>
            <span class="s1">proba_estimator = estimator.predict_proba(X[:</span><span class="s3">, </span><span class="s1">features])</span>

            <span class="s3">if </span><span class="s1">n_classes == len(estimator.classes_):</span>
                <span class="s1">proba += proba_estimator</span>

            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">proba[:</span><span class="s3">, </span><span class="s1">estimator.classes_] += proba_estimator[</span>
                    <span class="s1">:</span><span class="s3">, </span><span class="s1">range(len(estimator.classes_))</span>
                <span class="s1">]</span>

        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># Resort to voting</span>
            <span class="s1">predictions = estimator.predict(X[:</span><span class="s3">, </span><span class="s1">features])</span>

            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_samples):</span>
                <span class="s1">proba[i</span><span class="s3">, </span><span class="s1">predictions[i]] += </span><span class="s5">1</span>

    <span class="s3">return </span><span class="s1">proba</span>


<span class="s3">def </span><span class="s1">_parallel_predict_log_proba(estimators</span><span class="s3">, </span><span class="s1">estimators_features</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">n_classes):</span>
    <span class="s0">&quot;&quot;&quot;Private function used to compute log probabilities within a job.&quot;&quot;&quot;</span>
    <span class="s1">n_samples = X.shape[</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">log_proba = np.empty((n_samples</span><span class="s3">, </span><span class="s1">n_classes))</span>
    <span class="s1">log_proba.fill(-np.inf)</span>
    <span class="s1">all_classes = np.arange(n_classes</span><span class="s3">, </span><span class="s1">dtype=int)</span>

    <span class="s3">for </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">features </span><span class="s3">in </span><span class="s1">zip(estimators</span><span class="s3">, </span><span class="s1">estimators_features):</span>
        <span class="s1">log_proba_estimator = estimator.predict_log_proba(X[:</span><span class="s3">, </span><span class="s1">features])</span>

        <span class="s3">if </span><span class="s1">n_classes == len(estimator.classes_):</span>
            <span class="s1">log_proba = np.logaddexp(log_proba</span><span class="s3">, </span><span class="s1">log_proba_estimator)</span>

        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">log_proba[:</span><span class="s3">, </span><span class="s1">estimator.classes_] = np.logaddexp(</span>
                <span class="s1">log_proba[:</span><span class="s3">, </span><span class="s1">estimator.classes_]</span><span class="s3">,</span>
                <span class="s1">log_proba_estimator[:</span><span class="s3">, </span><span class="s1">range(len(estimator.classes_))]</span><span class="s3">,</span>
            <span class="s1">)</span>

            <span class="s1">missing = np.setdiff1d(all_classes</span><span class="s3">, </span><span class="s1">estimator.classes_)</span>
            <span class="s1">log_proba[:</span><span class="s3">, </span><span class="s1">missing] = np.logaddexp(log_proba[:</span><span class="s3">, </span><span class="s1">missing]</span><span class="s3">, </span><span class="s1">-np.inf)</span>

    <span class="s3">return </span><span class="s1">log_proba</span>


<span class="s3">def </span><span class="s1">_parallel_decision_function(estimators</span><span class="s3">, </span><span class="s1">estimators_features</span><span class="s3">, </span><span class="s1">X):</span>
    <span class="s0">&quot;&quot;&quot;Private function used to compute decisions within a job.&quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">sum(</span>
        <span class="s1">estimator.decision_function(X[:</span><span class="s3">, </span><span class="s1">features])</span>
        <span class="s3">for </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">features </span><span class="s3">in </span><span class="s1">zip(estimators</span><span class="s3">, </span><span class="s1">estimators_features)</span>
    <span class="s1">)</span>


<span class="s3">def </span><span class="s1">_parallel_predict_regression(estimators</span><span class="s3">, </span><span class="s1">estimators_features</span><span class="s3">, </span><span class="s1">X):</span>
    <span class="s0">&quot;&quot;&quot;Private function used to compute predictions within a job.&quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">sum(</span>
        <span class="s1">estimator.predict(X[:</span><span class="s3">, </span><span class="s1">features])</span>
        <span class="s3">for </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">features </span><span class="s3">in </span><span class="s1">zip(estimators</span><span class="s3">, </span><span class="s1">estimators_features)</span>
    <span class="s1">)</span>


<span class="s3">def </span><span class="s1">_estimator_has(attr):</span>
    <span class="s0">&quot;&quot;&quot;Check if we can delegate a method to the underlying estimator. 
 
    First, we check the first fitted estimator if available, otherwise we 
    check the estimator attribute. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">check(self):</span>
        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;estimators_&quot;</span><span class="s1">):</span>
            <span class="s3">return </span><span class="s1">hasattr(self.estimators_[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">attr)</span>
        <span class="s3">elif </span><span class="s1">self.estimator </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">hasattr(self.estimator</span><span class="s3">, </span><span class="s1">attr)</span>
        <span class="s3">else</span><span class="s1">:  </span><span class="s2"># TODO(1.4): Remove when the base_estimator deprecation cycle ends</span>
            <span class="s3">return </span><span class="s1">hasattr(self.base_estimator</span><span class="s3">, </span><span class="s1">attr)</span>

    <span class="s3">return </span><span class="s1">check</span>


<span class="s3">class </span><span class="s1">BaseBagging(BaseEnsemble</span><span class="s3">, </span><span class="s1">metaclass=ABCMeta):</span>
    <span class="s0">&quot;&quot;&quot;Base class for Bagging meta-estimator. 
 
    Warning: This class should not be used directly. Use derived classes 
    instead. 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;estimator&quot;</span><span class="s1">: [HasMethods([</span><span class="s4">&quot;fit&quot;</span><span class="s3">, </span><span class="s4">&quot;predict&quot;</span><span class="s1">])</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_estimators&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;max_samples&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">Interval(RealNotInt</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">closed=</span><span class="s4">&quot;right&quot;</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;max_features&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">Interval(RealNotInt</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">closed=</span><span class="s4">&quot;right&quot;</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;bootstrap&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;bootstrap_features&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;oob_score&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;warm_start&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_jobs&quot;</span><span class="s1">: [</span><span class="s3">None, </span><span class="s1">Integral]</span><span class="s3">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s4">&quot;verbose&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;base_estimator&quot;</span><span class="s1">: [</span>
            <span class="s1">HasMethods([</span><span class="s4">&quot;fit&quot;</span><span class="s3">, </span><span class="s4">&quot;predict&quot;</span><span class="s1">])</span><span class="s3">,</span>
            <span class="s1">StrOptions({</span><span class="s4">&quot;deprecated&quot;</span><span class="s1">})</span><span class="s3">,</span>
            <span class="s3">None,</span>
        <span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s1">@abstractmethod</span>
    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">estimator=</span><span class="s3">None,</span>
        <span class="s1">n_estimators=</span><span class="s5">10</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">max_samples=</span><span class="s5">1.0</span><span class="s3">,</span>
        <span class="s1">max_features=</span><span class="s5">1.0</span><span class="s3">,</span>
        <span class="s1">bootstrap=</span><span class="s3">True,</span>
        <span class="s1">bootstrap_features=</span><span class="s3">False,</span>
        <span class="s1">oob_score=</span><span class="s3">False,</span>
        <span class="s1">warm_start=</span><span class="s3">False,</span>
        <span class="s1">n_jobs=</span><span class="s3">None,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">base_estimator=</span><span class="s4">&quot;deprecated&quot;</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">estimator=estimator</span><span class="s3">,</span>
            <span class="s1">n_estimators=n_estimators</span><span class="s3">,</span>
            <span class="s1">base_estimator=base_estimator</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">self.max_samples = max_samples</span>
        <span class="s1">self.max_features = max_features</span>
        <span class="s1">self.bootstrap = bootstrap</span>
        <span class="s1">self.bootstrap_features = bootstrap_features</span>
        <span class="s1">self.oob_score = oob_score</span>
        <span class="s1">self.warm_start = warm_start</span>
        <span class="s1">self.n_jobs = n_jobs</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.verbose = verbose</span>

    <span class="s1">@_fit_context(</span>
        <span class="s2"># BaseBagging.estimator is not validated yet</span>
        <span class="s1">prefer_skip_nested_validation=</span><span class="s3">False</span>
    <span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Build a Bagging ensemble of estimators from the training set (X, y). 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The training input samples. Sparse matrices are accepted only if 
            they are supported by the base estimator. 
 
        y : array-like of shape (n_samples,) 
            The target values (class labels in classification, real numbers in 
            regression). 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. If None, then samples are equally weighted. 
            Note that this is supported only if the base estimator supports 
            sample weighting. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s2"># Convert data (X is required to be 2d and indexable)</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">accept_sparse=[</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s4">&quot;csc&quot;</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">dtype=</span><span class="s3">None,</span>
            <span class="s1">force_all_finite=</span><span class="s3">False,</span>
            <span class="s1">multi_output=</span><span class="s3">True,</span>
        <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">self._fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">self.max_samples</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s3">def </span><span class="s1">_parallel_args(self):</span>
        <span class="s3">return </span><span class="s1">{}</span>

    <span class="s3">def </span><span class="s1">_fit(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">X</span><span class="s3">,</span>
        <span class="s1">y</span><span class="s3">,</span>
        <span class="s1">max_samples=</span><span class="s3">None,</span>
        <span class="s1">max_depth=</span><span class="s3">None,</span>
        <span class="s1">sample_weight=</span><span class="s3">None,</span>
        <span class="s1">check_input=</span><span class="s3">True,</span>
    <span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Build a Bagging ensemble of estimators from the training 
           set (X, y). 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The training input samples. Sparse matrices are accepted only if 
            they are supported by the base estimator. 
 
        y : array-like of shape (n_samples,) 
            The target values (class labels in classification, real numbers in 
            regression). 
 
        max_samples : int or float, default=None 
            Argument to use instead of self.max_samples. 
 
        max_depth : int, default=None 
            Override value used when constructing base estimator. Only 
            supported if the base estimator has a max_depth parameter. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. If None, then samples are equally weighted. 
            Note that this is supported only if the base estimator supports 
            sample weighting. 
 
        check_input : bool, default=True 
            Override value used when fitting base estimator. Only supported 
            if the base estimator has a check_input parameter for fit function. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">random_state = check_random_state(self.random_state)</span>

        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=</span><span class="s3">None</span><span class="s1">)</span>

        <span class="s2"># Remap output</span>
        <span class="s1">n_samples = X.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">self._n_samples = n_samples</span>
        <span class="s1">y = self._validate_y(y)</span>

        <span class="s2"># Check parameters</span>
        <span class="s1">self._validate_estimator()</span>

        <span class="s3">if </span><span class="s1">max_depth </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">self.estimator_.max_depth = max_depth</span>

        <span class="s2"># Validate max_samples</span>
        <span class="s3">if </span><span class="s1">max_samples </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">max_samples = self.max_samples</span>
        <span class="s3">elif not </span><span class="s1">isinstance(max_samples</span><span class="s3">, </span><span class="s1">numbers.Integral):</span>
            <span class="s1">max_samples = int(max_samples * X.shape[</span><span class="s5">0</span><span class="s1">])</span>

        <span class="s3">if </span><span class="s1">max_samples &gt; X.shape[</span><span class="s5">0</span><span class="s1">]:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;max_samples must be &lt;= n_samples&quot;</span><span class="s1">)</span>

        <span class="s2"># Store validated integer row sampling value</span>
        <span class="s1">self._max_samples = max_samples</span>

        <span class="s2"># Validate max_features</span>
        <span class="s3">if </span><span class="s1">isinstance(self.max_features</span><span class="s3">, </span><span class="s1">numbers.Integral):</span>
            <span class="s1">max_features = self.max_features</span>
        <span class="s3">elif </span><span class="s1">isinstance(self.max_features</span><span class="s3">, </span><span class="s1">float):</span>
            <span class="s1">max_features = int(self.max_features * self.n_features_in_)</span>

        <span class="s3">if </span><span class="s1">max_features &gt; self.n_features_in_:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;max_features must be &lt;= n_features&quot;</span><span class="s1">)</span>

        <span class="s1">max_features = max(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">int(max_features))</span>

        <span class="s2"># Store validated integer feature sampling value</span>
        <span class="s1">self._max_features = max_features</span>

        <span class="s2"># Other checks</span>
        <span class="s3">if not </span><span class="s1">self.bootstrap </span><span class="s3">and </span><span class="s1">self.oob_score:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Out of bag estimation only available if bootstrap=True&quot;</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.warm_start </span><span class="s3">and </span><span class="s1">self.oob_score:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Out of bag estimate only available if warm_start=False&quot;</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;oob_score_&quot;</span><span class="s1">) </span><span class="s3">and </span><span class="s1">self.warm_start:</span>
            <span class="s3">del </span><span class="s1">self.oob_score_</span>

        <span class="s3">if not </span><span class="s1">self.warm_start </span><span class="s3">or not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;estimators_&quot;</span><span class="s1">):</span>
            <span class="s2"># Free allocated memory, if any</span>
            <span class="s1">self.estimators_ = []</span>
            <span class="s1">self.estimators_features_ = []</span>

        <span class="s1">n_more_estimators = self.n_estimators - len(self.estimators_)</span>

        <span class="s3">if </span><span class="s1">n_more_estimators &lt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;n_estimators=%d must be larger or equal to &quot;</span>
                <span class="s4">&quot;len(estimators_)=%d when warm_start==True&quot;</span>
                <span class="s1">% (self.n_estimators</span><span class="s3">, </span><span class="s1">len(self.estimators_))</span>
            <span class="s1">)</span>

        <span class="s3">elif </span><span class="s1">n_more_estimators == </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">warn(</span>
                <span class="s4">&quot;Warm-start fitting without increasing n_estimators does not &quot;</span>
                <span class="s4">&quot;fit new trees.&quot;</span>
            <span class="s1">)</span>
            <span class="s3">return </span><span class="s1">self</span>

        <span class="s2"># Parallel loop</span>
        <span class="s1">n_jobs</span><span class="s3">, </span><span class="s1">n_estimators</span><span class="s3">, </span><span class="s1">starts = _partition_estimators(</span>
            <span class="s1">n_more_estimators</span><span class="s3">, </span><span class="s1">self.n_jobs</span>
        <span class="s1">)</span>
        <span class="s1">total_n_estimators = sum(n_estimators)</span>

        <span class="s2"># Advance random state to state after training</span>
        <span class="s2"># the first n_estimators</span>
        <span class="s3">if </span><span class="s1">self.warm_start </span><span class="s3">and </span><span class="s1">len(self.estimators_) &gt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">random_state.randint(MAX_INT</span><span class="s3">, </span><span class="s1">size=len(self.estimators_))</span>

        <span class="s1">seeds = random_state.randint(MAX_INT</span><span class="s3">, </span><span class="s1">size=n_more_estimators)</span>
        <span class="s1">self._seeds = seeds</span>

        <span class="s1">all_results = Parallel(</span>
            <span class="s1">n_jobs=n_jobs</span><span class="s3">, </span><span class="s1">verbose=self.verbose</span><span class="s3">, </span><span class="s1">**self._parallel_args()</span>
        <span class="s1">)(</span>
            <span class="s1">delayed(_parallel_build_estimators)(</span>
                <span class="s1">n_estimators[i]</span><span class="s3">,</span>
                <span class="s1">self</span><span class="s3">,</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">y</span><span class="s3">,</span>
                <span class="s1">sample_weight</span><span class="s3">,</span>
                <span class="s1">seeds[starts[i] : starts[i + </span><span class="s5">1</span><span class="s1">]]</span><span class="s3">,</span>
                <span class="s1">total_n_estimators</span><span class="s3">,</span>
                <span class="s1">verbose=self.verbose</span><span class="s3">,</span>
                <span class="s1">check_input=check_input</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_jobs)</span>
        <span class="s1">)</span>

        <span class="s2"># Reduce</span>
        <span class="s1">self.estimators_ += list(</span>
            <span class="s1">itertools.chain.from_iterable(t[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">all_results)</span>
        <span class="s1">)</span>
        <span class="s1">self.estimators_features_ += list(</span>
            <span class="s1">itertools.chain.from_iterable(t[</span><span class="s5">1</span><span class="s1">] </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">all_results)</span>
        <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.oob_score:</span>
            <span class="s1">self._set_oob_score(X</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s1">@abstractmethod</span>
    <span class="s3">def </span><span class="s1">_set_oob_score(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s0">&quot;&quot;&quot;Calculate out of bag predictions and score.&quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">_validate_y(self</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s3">if </span><span class="s1">len(y.shape) == </span><span class="s5">1 </span><span class="s3">or </span><span class="s1">y.shape[</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">column_or_1d(y</span><span class="s3">, </span><span class="s1">warn=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">y</span>

    <span class="s3">def </span><span class="s1">_get_estimators_indices(self):</span>
        <span class="s2"># Get drawn indices along both sample and feature axes</span>
        <span class="s3">for </span><span class="s1">seed </span><span class="s3">in </span><span class="s1">self._seeds:</span>
            <span class="s2"># Operations accessing random_state must be performed identically</span>
            <span class="s2"># to those in `_parallel_build_estimators()`</span>
            <span class="s1">feature_indices</span><span class="s3">, </span><span class="s1">sample_indices = _generate_bagging_indices(</span>
                <span class="s1">seed</span><span class="s3">,</span>
                <span class="s1">self.bootstrap_features</span><span class="s3">,</span>
                <span class="s1">self.bootstrap</span><span class="s3">,</span>
                <span class="s1">self.n_features_in_</span><span class="s3">,</span>
                <span class="s1">self._n_samples</span><span class="s3">,</span>
                <span class="s1">self._max_features</span><span class="s3">,</span>
                <span class="s1">self._max_samples</span><span class="s3">,</span>
            <span class="s1">)</span>

            <span class="s3">yield </span><span class="s1">feature_indices</span><span class="s3">, </span><span class="s1">sample_indices</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">estimators_samples_(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        The subset of drawn samples for each base estimator. 
 
        Returns a dynamically generated list of indices identifying 
        the samples used for fitting each member of the ensemble, i.e., 
        the in-bag samples. 
 
        Note: the list is re-created at each call to the property in order 
        to reduce the object memory footprint by not storing the sampling 
        data. Thus fetching the property may be slower than expected. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">[sample_indices </span><span class="s3">for </span><span class="s1">_</span><span class="s3">, </span><span class="s1">sample_indices </span><span class="s3">in </span><span class="s1">self._get_estimators_indices()]</span>


<span class="s3">class </span><span class="s1">BaggingClassifier(ClassifierMixin</span><span class="s3">, </span><span class="s1">BaseBagging):</span>
    <span class="s0">&quot;&quot;&quot;A Bagging classifier. 
 
    A Bagging classifier is an ensemble meta-estimator that fits base 
    classifiers each on random subsets of the original dataset and then 
    aggregate their individual predictions (either by voting or by averaging) 
    to form a final prediction. Such a meta-estimator can typically be used as 
    a way to reduce the variance of a black-box estimator (e.g., a decision 
    tree), by introducing randomization into its construction procedure and 
    then making an ensemble out of it. 
 
    This algorithm encompasses several works from the literature. When random 
    subsets of the dataset are drawn as random subsets of the samples, then 
    this algorithm is known as Pasting [1]_. If samples are drawn with 
    replacement, then the method is known as Bagging [2]_. When random subsets 
    of the dataset are drawn as random subsets of the features, then the method 
    is known as Random Subspaces [3]_. Finally, when base estimators are built 
    on subsets of both samples and features, then the method is known as 
    Random Patches [4]_. 
 
    Read more in the :ref:`User Guide &lt;bagging&gt;`. 
 
    .. versionadded:: 0.15 
 
    Parameters 
    ---------- 
    estimator : object, default=None 
        The base estimator to fit on random subsets of the dataset. 
        If None, then the base estimator is a 
        :class:`~sklearn.tree.DecisionTreeClassifier`. 
 
        .. versionadded:: 1.2 
           `base_estimator` was renamed to `estimator`. 
 
    n_estimators : int, default=10 
        The number of base estimators in the ensemble. 
 
    max_samples : int or float, default=1.0 
        The number of samples to draw from X to train each base estimator (with 
        replacement by default, see `bootstrap` for more details). 
 
        - If int, then draw `max_samples` samples. 
        - If float, then draw `max_samples * X.shape[0]` samples. 
 
    max_features : int or float, default=1.0 
        The number of features to draw from X to train each base estimator ( 
        without replacement by default, see `bootstrap_features` for more 
        details). 
 
        - If int, then draw `max_features` features. 
        - If float, then draw `max(1, int(max_features * n_features_in_))` features. 
 
    bootstrap : bool, default=True 
        Whether samples are drawn with replacement. If False, sampling 
        without replacement is performed. 
 
    bootstrap_features : bool, default=False 
        Whether features are drawn with replacement. 
 
    oob_score : bool, default=False 
        Whether to use out-of-bag samples to estimate 
        the generalization error. Only available if bootstrap=True. 
 
    warm_start : bool, default=False 
        When set to True, reuse the solution of the previous call to fit 
        and add more estimators to the ensemble, otherwise, just fit 
        a whole new ensemble. See :term:`the Glossary &lt;warm_start&gt;`. 
 
        .. versionadded:: 0.17 
           *warm_start* constructor parameter. 
 
    n_jobs : int, default=None 
        The number of jobs to run in parallel for both :meth:`fit` and 
        :meth:`predict`. ``None`` means 1 unless in a 
        :obj:`joblib.parallel_backend` context. ``-1`` means using all 
        processors. See :term:`Glossary &lt;n_jobs&gt;` for more details. 
 
    random_state : int, RandomState instance or None, default=None 
        Controls the random resampling of the original dataset 
        (sample wise and feature wise). 
        If the base estimator accepts a `random_state` attribute, a different 
        seed is generated for each instance in the ensemble. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    verbose : int, default=0 
        Controls the verbosity when fitting and predicting. 
 
    base_estimator : object, default=&quot;deprecated&quot; 
        Use `estimator` instead. 
 
        .. deprecated:: 1.2 
            `base_estimator` is deprecated and will be removed in 1.4. 
            Use `estimator` instead. 
 
    Attributes 
    ---------- 
    estimator_ : estimator 
        The base estimator from which the ensemble is grown. 
 
        .. versionadded:: 1.2 
           `base_estimator_` was renamed to `estimator_`. 
 
    base_estimator_ : estimator 
        The base estimator from which the ensemble is grown. 
 
        .. deprecated:: 1.2 
            `base_estimator_` is deprecated and will be removed in 1.4. 
            Use `estimator_` instead. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    estimators_ : list of estimators 
        The collection of fitted base estimators. 
 
    estimators_samples_ : list of arrays 
        The subset of drawn samples (i.e., the in-bag samples) for each base 
        estimator. Each subset is defined by an array of the indices selected. 
 
    estimators_features_ : list of arrays 
        The subset of drawn features for each base estimator. 
 
    classes_ : ndarray of shape (n_classes,) 
        The classes labels. 
 
    n_classes_ : int or list 
        The number of classes. 
 
    oob_score_ : float 
        Score of the training dataset obtained using an out-of-bag estimate. 
        This attribute exists only when ``oob_score`` is True. 
 
    oob_decision_function_ : ndarray of shape (n_samples, n_classes) 
        Decision function computed with out-of-bag estimate on the training 
        set. If n_estimators is small it might be possible that a data point 
        was never left out during the bootstrap. In this case, 
        `oob_decision_function_` might contain NaN. This attribute exists 
        only when ``oob_score`` is True. 
 
    See Also 
    -------- 
    BaggingRegressor : A Bagging regressor. 
 
    References 
    ---------- 
 
    .. [1] L. Breiman, &quot;Pasting small votes for classification in large 
           databases and on-line&quot;, Machine Learning, 36(1), 85-103, 1999. 
 
    .. [2] L. Breiman, &quot;Bagging predictors&quot;, Machine Learning, 24(2), 123-140, 
           1996. 
 
    .. [3] T. Ho, &quot;The random subspace method for constructing decision 
           forests&quot;, Pattern Analysis and Machine Intelligence, 20(8), 832-844, 
           1998. 
 
    .. [4] G. Louppe and P. Geurts, &quot;Ensembles on Random Patches&quot;, Machine 
           Learning and Knowledge Discovery in Databases, 346-361, 2012. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.svm import SVC 
    &gt;&gt;&gt; from sklearn.ensemble import BaggingClassifier 
    &gt;&gt;&gt; from sklearn.datasets import make_classification 
    &gt;&gt;&gt; X, y = make_classification(n_samples=100, n_features=4, 
    ...                            n_informative=2, n_redundant=0, 
    ...                            random_state=0, shuffle=False) 
    &gt;&gt;&gt; clf = BaggingClassifier(estimator=SVC(), 
    ...                         n_estimators=10, random_state=0).fit(X, y) 
    &gt;&gt;&gt; clf.predict([[0, 0, 0, 0]]) 
    array([1]) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">estimator=</span><span class="s3">None,</span>
        <span class="s1">n_estimators=</span><span class="s5">10</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">max_samples=</span><span class="s5">1.0</span><span class="s3">,</span>
        <span class="s1">max_features=</span><span class="s5">1.0</span><span class="s3">,</span>
        <span class="s1">bootstrap=</span><span class="s3">True,</span>
        <span class="s1">bootstrap_features=</span><span class="s3">False,</span>
        <span class="s1">oob_score=</span><span class="s3">False,</span>
        <span class="s1">warm_start=</span><span class="s3">False,</span>
        <span class="s1">n_jobs=</span><span class="s3">None,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">base_estimator=</span><span class="s4">&quot;deprecated&quot;</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">estimator=estimator</span><span class="s3">,</span>
            <span class="s1">n_estimators=n_estimators</span><span class="s3">,</span>
            <span class="s1">max_samples=max_samples</span><span class="s3">,</span>
            <span class="s1">max_features=max_features</span><span class="s3">,</span>
            <span class="s1">bootstrap=bootstrap</span><span class="s3">,</span>
            <span class="s1">bootstrap_features=bootstrap_features</span><span class="s3">,</span>
            <span class="s1">oob_score=oob_score</span><span class="s3">,</span>
            <span class="s1">warm_start=warm_start</span><span class="s3">,</span>
            <span class="s1">n_jobs=n_jobs</span><span class="s3">,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
            <span class="s1">base_estimator=base_estimator</span><span class="s3">,</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_validate_estimator(self):</span>
        <span class="s0">&quot;&quot;&quot;Check the estimator and set the estimator_ attribute.&quot;&quot;&quot;</span>
        <span class="s1">super()._validate_estimator(default=DecisionTreeClassifier())</span>

    <span class="s3">def </span><span class="s1">_set_oob_score(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s1">n_samples = y.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">n_classes_ = self.n_classes_</span>

        <span class="s1">predictions = np.zeros((n_samples</span><span class="s3">, </span><span class="s1">n_classes_))</span>

        <span class="s3">for </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">samples</span><span class="s3">, </span><span class="s1">features </span><span class="s3">in </span><span class="s1">zip(</span>
            <span class="s1">self.estimators_</span><span class="s3">, </span><span class="s1">self.estimators_samples_</span><span class="s3">, </span><span class="s1">self.estimators_features_</span>
        <span class="s1">):</span>
            <span class="s2"># Create mask for OOB samples</span>
            <span class="s1">mask = ~indices_to_mask(samples</span><span class="s3">, </span><span class="s1">n_samples)</span>

            <span class="s3">if </span><span class="s1">hasattr(estimator</span><span class="s3">, </span><span class="s4">&quot;predict_proba&quot;</span><span class="s1">):</span>
                <span class="s1">predictions[mask</span><span class="s3">, </span><span class="s1">:] += estimator.predict_proba(</span>
                    <span class="s1">(X[mask</span><span class="s3">, </span><span class="s1">:])[:</span><span class="s3">, </span><span class="s1">features]</span>
                <span class="s1">)</span>

            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">p = estimator.predict((X[mask</span><span class="s3">, </span><span class="s1">:])[:</span><span class="s3">, </span><span class="s1">features])</span>
                <span class="s1">j = </span><span class="s5">0</span>

                <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_samples):</span>
                    <span class="s3">if </span><span class="s1">mask[i]:</span>
                        <span class="s1">predictions[i</span><span class="s3">, </span><span class="s1">p[j]] += </span><span class="s5">1</span>
                        <span class="s1">j += </span><span class="s5">1</span>

        <span class="s3">if </span><span class="s1">(predictions.sum(axis=</span><span class="s5">1</span><span class="s1">) == </span><span class="s5">0</span><span class="s1">).any():</span>
            <span class="s1">warn(</span>
                <span class="s4">&quot;Some inputs do not have OOB scores. &quot;</span>
                <span class="s4">&quot;This probably means too few estimators were used &quot;</span>
                <span class="s4">&quot;to compute any reliable oob estimates.&quot;</span>
            <span class="s1">)</span>

        <span class="s1">oob_decision_function = predictions / predictions.sum(axis=</span><span class="s5">1</span><span class="s1">)[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">oob_score = accuracy_score(y</span><span class="s3">, </span><span class="s1">np.argmax(predictions</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">))</span>

        <span class="s1">self.oob_decision_function_ = oob_decision_function</span>
        <span class="s1">self.oob_score_ = oob_score</span>

    <span class="s3">def </span><span class="s1">_validate_y(self</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s1">y = column_or_1d(y</span><span class="s3">, </span><span class="s1">warn=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">check_classification_targets(y)</span>
        <span class="s1">self.classes_</span><span class="s3">, </span><span class="s1">y = np.unique(y</span><span class="s3">, </span><span class="s1">return_inverse=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">self.n_classes_ = len(self.classes_)</span>

        <span class="s3">return </span><span class="s1">y</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict class for X. 
 
        The predicted class of an input sample is computed as the class with 
        the highest mean predicted probability. If base estimators do not 
        implement a ``predict_proba`` method, then it resorts to voting. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The training input samples. Sparse matrices are accepted only if 
            they are supported by the base estimator. 
 
        Returns 
        ------- 
        y : ndarray of shape (n_samples,) 
            The predicted classes. 
        &quot;&quot;&quot;</span>
        <span class="s1">predicted_probabilitiy = self.predict_proba(X)</span>
        <span class="s3">return </span><span class="s1">self.classes_.take((np.argmax(predicted_probabilitiy</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">))</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">predict_proba(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict class probabilities for X. 
 
        The predicted class probabilities of an input sample is computed as 
        the mean predicted class probabilities of the base estimators in the 
        ensemble. If base estimators do not implement a ``predict_proba`` 
        method, then it resorts to voting and the predicted class probabilities 
        of an input sample represents the proportion of estimators predicting 
        each class. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The training input samples. Sparse matrices are accepted only if 
            they are supported by the base estimator. 
 
        Returns 
        ------- 
        p : ndarray of shape (n_samples, n_classes) 
            The class probabilities of the input samples. The order of the 
            classes corresponds to that in the attribute :term:`classes_`. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s2"># Check data</span>
        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">accept_sparse=[</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s4">&quot;csc&quot;</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">dtype=</span><span class="s3">None,</span>
            <span class="s1">force_all_finite=</span><span class="s3">False,</span>
            <span class="s1">reset=</span><span class="s3">False,</span>
        <span class="s1">)</span>

        <span class="s2"># Parallel loop</span>
        <span class="s1">n_jobs</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">starts = _partition_estimators(self.n_estimators</span><span class="s3">, </span><span class="s1">self.n_jobs)</span>

        <span class="s1">all_proba = Parallel(</span>
            <span class="s1">n_jobs=n_jobs</span><span class="s3">, </span><span class="s1">verbose=self.verbose</span><span class="s3">, </span><span class="s1">**self._parallel_args()</span>
        <span class="s1">)(</span>
            <span class="s1">delayed(_parallel_predict_proba)(</span>
                <span class="s1">self.estimators_[starts[i] : starts[i + </span><span class="s5">1</span><span class="s1">]]</span><span class="s3">,</span>
                <span class="s1">self.estimators_features_[starts[i] : starts[i + </span><span class="s5">1</span><span class="s1">]]</span><span class="s3">,</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">self.n_classes_</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_jobs)</span>
        <span class="s1">)</span>

        <span class="s2"># Reduce</span>
        <span class="s1">proba = sum(all_proba) / self.n_estimators</span>

        <span class="s3">return </span><span class="s1">proba</span>

    <span class="s3">def </span><span class="s1">predict_log_proba(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict class log-probabilities for X. 
 
        The predicted class log-probabilities of an input sample is computed as 
        the log of the mean predicted class probabilities of the base 
        estimators in the ensemble. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The training input samples. Sparse matrices are accepted only if 
            they are supported by the base estimator. 
 
        Returns 
        ------- 
        p : ndarray of shape (n_samples, n_classes) 
            The class log-probabilities of the input samples. The order of the 
            classes corresponds to that in the attribute :term:`classes_`. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">if </span><span class="s1">hasattr(self.estimator_</span><span class="s3">, </span><span class="s4">&quot;predict_log_proba&quot;</span><span class="s1">):</span>
            <span class="s2"># Check data</span>
            <span class="s1">X = self._validate_data(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">accept_sparse=[</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s4">&quot;csc&quot;</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">dtype=</span><span class="s3">None,</span>
                <span class="s1">force_all_finite=</span><span class="s3">False,</span>
                <span class="s1">reset=</span><span class="s3">False,</span>
            <span class="s1">)</span>

            <span class="s2"># Parallel loop</span>
            <span class="s1">n_jobs</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">starts = _partition_estimators(self.n_estimators</span><span class="s3">, </span><span class="s1">self.n_jobs)</span>

            <span class="s1">all_log_proba = Parallel(n_jobs=n_jobs</span><span class="s3">, </span><span class="s1">verbose=self.verbose)(</span>
                <span class="s1">delayed(_parallel_predict_log_proba)(</span>
                    <span class="s1">self.estimators_[starts[i] : starts[i + </span><span class="s5">1</span><span class="s1">]]</span><span class="s3">,</span>
                    <span class="s1">self.estimators_features_[starts[i] : starts[i + </span><span class="s5">1</span><span class="s1">]]</span><span class="s3">,</span>
                    <span class="s1">X</span><span class="s3">,</span>
                    <span class="s1">self.n_classes_</span><span class="s3">,</span>
                <span class="s1">)</span>
                <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_jobs)</span>
            <span class="s1">)</span>

            <span class="s2"># Reduce</span>
            <span class="s1">log_proba = all_log_proba[</span><span class="s5">0</span><span class="s1">]</span>

            <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">len(all_log_proba)):</span>
                <span class="s1">log_proba = np.logaddexp(log_proba</span><span class="s3">, </span><span class="s1">all_log_proba[j])</span>

            <span class="s1">log_proba -= np.log(self.n_estimators)</span>

        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">log_proba = np.log(self.predict_proba(X))</span>

        <span class="s3">return </span><span class="s1">log_proba</span>

    <span class="s1">@available_if(_estimator_has(</span><span class="s4">&quot;decision_function&quot;</span><span class="s1">))</span>
    <span class="s3">def </span><span class="s1">decision_function(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Average of the decision functions of the base classifiers. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The training input samples. Sparse matrices are accepted only if 
            they are supported by the base estimator. 
 
        Returns 
        ------- 
        score : ndarray of shape (n_samples, k) 
            The decision function of the input samples. The columns correspond 
            to the classes in sorted order, as they appear in the attribute 
            ``classes_``. Regression and binary classification are special 
            cases with ``k == 1``, otherwise ``k==n_classes``. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s2"># Check data</span>
        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">accept_sparse=[</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s4">&quot;csc&quot;</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">dtype=</span><span class="s3">None,</span>
            <span class="s1">force_all_finite=</span><span class="s3">False,</span>
            <span class="s1">reset=</span><span class="s3">False,</span>
        <span class="s1">)</span>

        <span class="s2"># Parallel loop</span>
        <span class="s1">n_jobs</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">starts = _partition_estimators(self.n_estimators</span><span class="s3">, </span><span class="s1">self.n_jobs)</span>

        <span class="s1">all_decisions = Parallel(n_jobs=n_jobs</span><span class="s3">, </span><span class="s1">verbose=self.verbose)(</span>
            <span class="s1">delayed(_parallel_decision_function)(</span>
                <span class="s1">self.estimators_[starts[i] : starts[i + </span><span class="s5">1</span><span class="s1">]]</span><span class="s3">,</span>
                <span class="s1">self.estimators_features_[starts[i] : starts[i + </span><span class="s5">1</span><span class="s1">]]</span><span class="s3">,</span>
                <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_jobs)</span>
        <span class="s1">)</span>

        <span class="s2"># Reduce</span>
        <span class="s1">decisions = sum(all_decisions) / self.n_estimators</span>

        <span class="s3">return </span><span class="s1">decisions</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">if </span><span class="s1">self.estimator </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">estimator = DecisionTreeClassifier()</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">estimator = self.estimator</span>

        <span class="s3">return </span><span class="s1">{</span><span class="s4">&quot;allow_nan&quot;</span><span class="s1">: _safe_tags(estimator</span><span class="s3">, </span><span class="s4">&quot;allow_nan&quot;</span><span class="s1">)}</span>


<span class="s3">class </span><span class="s1">BaggingRegressor(RegressorMixin</span><span class="s3">, </span><span class="s1">BaseBagging):</span>
    <span class="s0">&quot;&quot;&quot;A Bagging regressor. 
 
    A Bagging regressor is an ensemble meta-estimator that fits base 
    regressors each on random subsets of the original dataset and then 
    aggregate their individual predictions (either by voting or by averaging) 
    to form a final prediction. Such a meta-estimator can typically be used as 
    a way to reduce the variance of a black-box estimator (e.g., a decision 
    tree), by introducing randomization into its construction procedure and 
    then making an ensemble out of it. 
 
    This algorithm encompasses several works from the literature. When random 
    subsets of the dataset are drawn as random subsets of the samples, then 
    this algorithm is known as Pasting [1]_. If samples are drawn with 
    replacement, then the method is known as Bagging [2]_. When random subsets 
    of the dataset are drawn as random subsets of the features, then the method 
    is known as Random Subspaces [3]_. Finally, when base estimators are built 
    on subsets of both samples and features, then the method is known as 
    Random Patches [4]_. 
 
    Read more in the :ref:`User Guide &lt;bagging&gt;`. 
 
    .. versionadded:: 0.15 
 
    Parameters 
    ---------- 
    estimator : object, default=None 
        The base estimator to fit on random subsets of the dataset. 
        If None, then the base estimator is a 
        :class:`~sklearn.tree.DecisionTreeRegressor`. 
 
        .. versionadded:: 1.2 
           `base_estimator` was renamed to `estimator`. 
 
    n_estimators : int, default=10 
        The number of base estimators in the ensemble. 
 
    max_samples : int or float, default=1.0 
        The number of samples to draw from X to train each base estimator (with 
        replacement by default, see `bootstrap` for more details). 
 
        - If int, then draw `max_samples` samples. 
        - If float, then draw `max_samples * X.shape[0]` samples. 
 
    max_features : int or float, default=1.0 
        The number of features to draw from X to train each base estimator ( 
        without replacement by default, see `bootstrap_features` for more 
        details). 
 
        - If int, then draw `max_features` features. 
        - If float, then draw `max(1, int(max_features * n_features_in_))` features. 
 
    bootstrap : bool, default=True 
        Whether samples are drawn with replacement. If False, sampling 
        without replacement is performed. 
 
    bootstrap_features : bool, default=False 
        Whether features are drawn with replacement. 
 
    oob_score : bool, default=False 
        Whether to use out-of-bag samples to estimate 
        the generalization error. Only available if bootstrap=True. 
 
    warm_start : bool, default=False 
        When set to True, reuse the solution of the previous call to fit 
        and add more estimators to the ensemble, otherwise, just fit 
        a whole new ensemble. See :term:`the Glossary &lt;warm_start&gt;`. 
 
    n_jobs : int, default=None 
        The number of jobs to run in parallel for both :meth:`fit` and 
        :meth:`predict`. ``None`` means 1 unless in a 
        :obj:`joblib.parallel_backend` context. ``-1`` means using all 
        processors. See :term:`Glossary &lt;n_jobs&gt;` for more details. 
 
    random_state : int, RandomState instance or None, default=None 
        Controls the random resampling of the original dataset 
        (sample wise and feature wise). 
        If the base estimator accepts a `random_state` attribute, a different 
        seed is generated for each instance in the ensemble. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    verbose : int, default=0 
        Controls the verbosity when fitting and predicting. 
 
    base_estimator : object, default=&quot;deprecated&quot; 
        Use `estimator` instead. 
 
        .. deprecated:: 1.2 
            `base_estimator` is deprecated and will be removed in 1.4. 
            Use `estimator` instead. 
 
    Attributes 
    ---------- 
    estimator_ : estimator 
        The base estimator from which the ensemble is grown. 
 
        .. versionadded:: 1.2 
           `base_estimator_` was renamed to `estimator_`. 
 
    base_estimator_ : estimator 
        The base estimator from which the ensemble is grown. 
 
        .. deprecated:: 1.2 
            `base_estimator_` is deprecated and will be removed in 1.4. 
            Use `estimator_` instead. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    estimators_ : list of estimators 
        The collection of fitted sub-estimators. 
 
    estimators_samples_ : list of arrays 
        The subset of drawn samples (i.e., the in-bag samples) for each base 
        estimator. Each subset is defined by an array of the indices selected. 
 
    estimators_features_ : list of arrays 
        The subset of drawn features for each base estimator. 
 
    oob_score_ : float 
        Score of the training dataset obtained using an out-of-bag estimate. 
        This attribute exists only when ``oob_score`` is True. 
 
    oob_prediction_ : ndarray of shape (n_samples,) 
        Prediction computed with out-of-bag estimate on the training 
        set. If n_estimators is small it might be possible that a data point 
        was never left out during the bootstrap. In this case, 
        `oob_prediction_` might contain NaN. This attribute exists only 
        when ``oob_score`` is True. 
 
    See Also 
    -------- 
    BaggingClassifier : A Bagging classifier. 
 
    References 
    ---------- 
 
    .. [1] L. Breiman, &quot;Pasting small votes for classification in large 
           databases and on-line&quot;, Machine Learning, 36(1), 85-103, 1999. 
 
    .. [2] L. Breiman, &quot;Bagging predictors&quot;, Machine Learning, 24(2), 123-140, 
           1996. 
 
    .. [3] T. Ho, &quot;The random subspace method for constructing decision 
           forests&quot;, Pattern Analysis and Machine Intelligence, 20(8), 832-844, 
           1998. 
 
    .. [4] G. Louppe and P. Geurts, &quot;Ensembles on Random Patches&quot;, Machine 
           Learning and Knowledge Discovery in Databases, 346-361, 2012. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.svm import SVR 
    &gt;&gt;&gt; from sklearn.ensemble import BaggingRegressor 
    &gt;&gt;&gt; from sklearn.datasets import make_regression 
    &gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=4, 
    ...                        n_informative=2, n_targets=1, 
    ...                        random_state=0, shuffle=False) 
    &gt;&gt;&gt; regr = BaggingRegressor(estimator=SVR(), 
    ...                         n_estimators=10, random_state=0).fit(X, y) 
    &gt;&gt;&gt; regr.predict([[0, 0, 0, 0]]) 
    array([-2.8720...]) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">estimator=</span><span class="s3">None,</span>
        <span class="s1">n_estimators=</span><span class="s5">10</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">max_samples=</span><span class="s5">1.0</span><span class="s3">,</span>
        <span class="s1">max_features=</span><span class="s5">1.0</span><span class="s3">,</span>
        <span class="s1">bootstrap=</span><span class="s3">True,</span>
        <span class="s1">bootstrap_features=</span><span class="s3">False,</span>
        <span class="s1">oob_score=</span><span class="s3">False,</span>
        <span class="s1">warm_start=</span><span class="s3">False,</span>
        <span class="s1">n_jobs=</span><span class="s3">None,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">base_estimator=</span><span class="s4">&quot;deprecated&quot;</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">estimator=estimator</span><span class="s3">,</span>
            <span class="s1">n_estimators=n_estimators</span><span class="s3">,</span>
            <span class="s1">max_samples=max_samples</span><span class="s3">,</span>
            <span class="s1">max_features=max_features</span><span class="s3">,</span>
            <span class="s1">bootstrap=bootstrap</span><span class="s3">,</span>
            <span class="s1">bootstrap_features=bootstrap_features</span><span class="s3">,</span>
            <span class="s1">oob_score=oob_score</span><span class="s3">,</span>
            <span class="s1">warm_start=warm_start</span><span class="s3">,</span>
            <span class="s1">n_jobs=n_jobs</span><span class="s3">,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
            <span class="s1">base_estimator=base_estimator</span><span class="s3">,</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict regression target for X. 
 
        The predicted regression target of an input sample is computed as the 
        mean predicted regression targets of the estimators in the ensemble. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The training input samples. Sparse matrices are accepted only if 
            they are supported by the base estimator. 
 
        Returns 
        ------- 
        y : ndarray of shape (n_samples,) 
            The predicted values. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s2"># Check data</span>
        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">accept_sparse=[</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s4">&quot;csc&quot;</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">dtype=</span><span class="s3">None,</span>
            <span class="s1">force_all_finite=</span><span class="s3">False,</span>
            <span class="s1">reset=</span><span class="s3">False,</span>
        <span class="s1">)</span>

        <span class="s2"># Parallel loop</span>
        <span class="s1">n_jobs</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">starts = _partition_estimators(self.n_estimators</span><span class="s3">, </span><span class="s1">self.n_jobs)</span>

        <span class="s1">all_y_hat = Parallel(n_jobs=n_jobs</span><span class="s3">, </span><span class="s1">verbose=self.verbose)(</span>
            <span class="s1">delayed(_parallel_predict_regression)(</span>
                <span class="s1">self.estimators_[starts[i] : starts[i + </span><span class="s5">1</span><span class="s1">]]</span><span class="s3">,</span>
                <span class="s1">self.estimators_features_[starts[i] : starts[i + </span><span class="s5">1</span><span class="s1">]]</span><span class="s3">,</span>
                <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_jobs)</span>
        <span class="s1">)</span>

        <span class="s2"># Reduce</span>
        <span class="s1">y_hat = sum(all_y_hat) / self.n_estimators</span>

        <span class="s3">return </span><span class="s1">y_hat</span>

    <span class="s3">def </span><span class="s1">_validate_estimator(self):</span>
        <span class="s0">&quot;&quot;&quot;Check the estimator and set the estimator_ attribute.&quot;&quot;&quot;</span>
        <span class="s1">super()._validate_estimator(default=DecisionTreeRegressor())</span>

    <span class="s3">def </span><span class="s1">_set_oob_score(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s1">n_samples = y.shape[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s1">predictions = np.zeros((n_samples</span><span class="s3">,</span><span class="s1">))</span>
        <span class="s1">n_predictions = np.zeros((n_samples</span><span class="s3">,</span><span class="s1">))</span>

        <span class="s3">for </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">samples</span><span class="s3">, </span><span class="s1">features </span><span class="s3">in </span><span class="s1">zip(</span>
            <span class="s1">self.estimators_</span><span class="s3">, </span><span class="s1">self.estimators_samples_</span><span class="s3">, </span><span class="s1">self.estimators_features_</span>
        <span class="s1">):</span>
            <span class="s2"># Create mask for OOB samples</span>
            <span class="s1">mask = ~indices_to_mask(samples</span><span class="s3">, </span><span class="s1">n_samples)</span>

            <span class="s1">predictions[mask] += estimator.predict((X[mask</span><span class="s3">, </span><span class="s1">:])[:</span><span class="s3">, </span><span class="s1">features])</span>
            <span class="s1">n_predictions[mask] += </span><span class="s5">1</span>

        <span class="s3">if </span><span class="s1">(n_predictions == </span><span class="s5">0</span><span class="s1">).any():</span>
            <span class="s1">warn(</span>
                <span class="s4">&quot;Some inputs do not have OOB scores. &quot;</span>
                <span class="s4">&quot;This probably means too few estimators were used &quot;</span>
                <span class="s4">&quot;to compute any reliable oob estimates.&quot;</span>
            <span class="s1">)</span>
            <span class="s1">n_predictions[n_predictions == </span><span class="s5">0</span><span class="s1">] = </span><span class="s5">1</span>

        <span class="s1">predictions /= n_predictions</span>

        <span class="s1">self.oob_prediction_ = predictions</span>
        <span class="s1">self.oob_score_ = r2_score(y</span><span class="s3">, </span><span class="s1">predictions)</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">if </span><span class="s1">self.estimator </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">estimator = DecisionTreeRegressor()</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">estimator = self.estimator</span>
        <span class="s3">return </span><span class="s1">{</span><span class="s4">&quot;allow_nan&quot;</span><span class="s1">: _safe_tags(estimator</span><span class="s3">, </span><span class="s4">&quot;allow_nan&quot;</span><span class="s1">)}</span>
</pre>
</body>
</html>