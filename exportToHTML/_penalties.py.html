<html>
<head>
<title>_penalties.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_penalties.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot; 
A collection of smooth penalty functions. 
 
Penalties on vectors take a vector argument and return a scalar 
penalty.  The gradient of the penalty is a vector with the same shape 
as the input value. 
 
Penalties on covariance matrices take two arguments: the matrix and 
its inverse, both in unpacked (square) form.  The returned penalty is 
a scalar, and the gradient is returned as a vector that contains the 
gradient with respect to the free elements in the lower triangle of 
the covariance matrix. 
 
All penalties are subtracted from the log-likelihood, so greater 
penalty values correspond to a greater degree of penalization. 
 
The penaties should be smooth so that they can be subtracted from log 
likelihood functions and optimized using standard methods (i.e. L1 
penalties do not belong here). 
&quot;&quot;&quot;</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>


<span class="s3">class </span><span class="s1">Penalty:</span>
    <span class="s2">&quot;&quot;&quot; 
    A class for representing a scalar-value penalty. 
 
    Parameters 
    ---------- 
    weights : array_like 
        A vector of weights that determines the weight of the penalty 
        for each parameter. 
 
    Notes 
    ----- 
    The class has a member called `alpha` that scales the weights. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">weights=</span><span class="s4">1.</span><span class="s1">):</span>
        <span class="s1">self.weights = weights</span>
        <span class="s1">self.alpha = </span><span class="s4">1.</span>

    <span class="s3">def </span><span class="s1">func(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot; 
        A penalty function on a vector of parameters. 
 
        Parameters 
        ---------- 
        params : array_like 
            A vector of parameters. 
 
        Returns 
        ------- 
        A scalar penaty value; greater values imply greater 
        penalization. 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">def </span><span class="s1">deriv(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot; 
        The gradient of a penalty function. 
 
        Parameters 
        ---------- 
        params : array_like 
            A vector of parameters 
 
        Returns 
        ------- 
        The gradient of the penalty with respect to each element in 
        `params`. 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">def </span><span class="s1">_null_weights(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot;work around for Null model 
 
        This will not be needed anymore when we can use `self._null_drop_keys` 
        as in DiscreteModels. 
        TODO: check other models 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">np.size(self.weights) &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">len(params) == </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s3">raise  </span><span class="s0"># raise to identify models where this would be needed</span>
                <span class="s3">return </span><span class="s4">0.</span>

        <span class="s3">return </span><span class="s1">self.weights</span>


<span class="s3">class </span><span class="s1">NonePenalty(Penalty):</span>
    <span class="s2">&quot;&quot;&quot; 
    A penalty that does not penalize. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">**kwds):</span>
        <span class="s1">super().__init__()</span>
        <span class="s3">if </span><span class="s1">kwds:</span>
            <span class="s3">import </span><span class="s1">warnings</span>
            <span class="s1">warnings.warn(</span><span class="s5">'keyword arguments are be ignored'</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">func(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">if </span><span class="s1">params.ndim == </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">np.zeros(params.shape[</span><span class="s4">1</span><span class="s1">:])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s4">0</span>

    <span class="s3">def </span><span class="s1">deriv(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">return </span><span class="s1">np.zeros(params.shape)</span>

    <span class="s3">def </span><span class="s1">deriv2(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0"># returns diagonal of hessian</span>
        <span class="s3">return </span><span class="s1">np.zeros(params.shape[</span><span class="s4">0</span><span class="s1">])</span>


<span class="s3">class </span><span class="s1">L2(Penalty):</span>
    <span class="s2">&quot;&quot;&quot; 
    The L2 (ridge) penalty. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">weights=</span><span class="s4">1.</span><span class="s1">):</span>
        <span class="s1">super().__init__(weights)</span>

    <span class="s3">def </span><span class="s1">func(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">return </span><span class="s1">np.sum(self.weights * self.alpha * params**</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">deriv(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">return </span><span class="s4">2 </span><span class="s1">* self.weights * self.alpha * params</span>

    <span class="s3">def </span><span class="s1">deriv2(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">return </span><span class="s4">2 </span><span class="s1">* self.weights * self.alpha * np.ones(len(params))</span>


<span class="s3">class </span><span class="s1">L2Univariate(Penalty):</span>
    <span class="s2">&quot;&quot;&quot; 
    The L2 (ridge) penalty applied to each parameter. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">weights=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s3">if </span><span class="s1">weights </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self.weights = </span><span class="s4">1.</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.weights = weights</span>

    <span class="s3">def </span><span class="s1">func(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">return </span><span class="s1">self.weights * params**</span><span class="s4">2</span>

    <span class="s3">def </span><span class="s1">deriv(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">return </span><span class="s4">2 </span><span class="s1">* self.weights * params</span>

    <span class="s3">def </span><span class="s1">deriv2(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">return </span><span class="s4">2 </span><span class="s1">* self.weights * np.ones(len(params))</span>


<span class="s3">class </span><span class="s1">PseudoHuber(Penalty):</span>
    <span class="s2">&quot;&quot;&quot; 
    The pseudo-Huber penalty. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">dlt</span><span class="s3">, </span><span class="s1">weights=</span><span class="s4">1.</span><span class="s1">):</span>
        <span class="s1">super().__init__(weights)</span>
        <span class="s1">self.dlt = dlt</span>

    <span class="s3">def </span><span class="s1">func(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s1">v = np.sqrt(</span><span class="s4">1 </span><span class="s1">+ (params / self.dlt)**</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">v -= </span><span class="s4">1</span>
        <span class="s1">v *= self.dlt**</span><span class="s4">2</span>
        <span class="s3">return </span><span class="s1">np.sum(self.weights * self.alpha * v</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">deriv(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s1">v = np.sqrt(</span><span class="s4">1 </span><span class="s1">+ (params / self.dlt)**</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">params * self.weights * self.alpha / v</span>

    <span class="s3">def </span><span class="s1">deriv2(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s1">v = np.power(</span><span class="s4">1 </span><span class="s1">+ (params / self.dlt)**</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">3</span><span class="s1">/</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">self.weights * self.alpha * v</span>


<span class="s3">class </span><span class="s1">SCAD(Penalty):</span>
    <span class="s2">&quot;&quot;&quot; 
    The SCAD penalty of Fan and Li. 
 
    The SCAD penalty is linear around zero as a L1 penalty up to threshold tau. 
    The SCAD penalty is constant for values larger than c*tau. 
    The middle segment is quadratic and connect the two segments with a continuous 
    derivative. 
    The penalty is symmetric around zero. 
 
    Parameterization follows Boo, Johnson, Li and Tan 2011. 
    Fan and Li use lambda instead of tau, and a instead of c. Fan and Li 
    recommend setting c=3.7. 
 
    f(x) = { tau |x|                                        if 0 &lt;= |x| &lt; tau 
           { -(|x|^2 - 2 c tau |x| + tau^2) / (2 (c - 1))   if tau &lt;= |x| &lt; c tau 
           { (c + 1) tau^2 / 2                              if c tau &lt;= |x| 
 
    Parameters 
    ---------- 
    tau : float 
        slope and threshold for linear segment 
    c : float 
        factor for second threshold which is c * tau 
    weights : None or array 
        weights for penalty of each parameter. If an entry is zero, then the 
        corresponding parameter will not be penalized. 
 
    References 
    ---------- 
    Buu, Anne, Norman J. Johnson, Runze Li, and Xianming Tan. &quot;New variable 
    selection methods for zero‚Äêinflated count data with applications to the 
    substance abuse field.&quot; 
    Statistics in medicine 30, no. 18 (2011): 2326-2340. 
 
    Fan, Jianqing, and Runze Li. &quot;Variable selection via nonconcave penalized 
    likelihood and its oracle properties.&quot; 
    Journal of the American statistical Association 96, no. 456 (2001): 
    1348-1360. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">tau</span><span class="s3">, </span><span class="s1">c=</span><span class="s4">3.7</span><span class="s3">, </span><span class="s1">weights=</span><span class="s4">1.</span><span class="s1">):</span>
        <span class="s1">super().__init__(weights)</span>
        <span class="s1">self.tau = tau</span>
        <span class="s1">self.c = c</span>

    <span class="s3">def </span><span class="s1">func(self</span><span class="s3">, </span><span class="s1">params):</span>

        <span class="s0"># 3 segments in absolute value</span>
        <span class="s1">tau = self.tau</span>
        <span class="s1">p_abs = np.atleast_1d(np.abs(params))</span>
        <span class="s1">res = np.empty(p_abs.shape</span><span class="s3">, </span><span class="s1">p_abs.dtype)</span>
        <span class="s1">res.fill(np.nan)</span>
        <span class="s1">mask1 = p_abs &lt; tau</span>
        <span class="s1">mask3 = p_abs &gt;= self.c * tau</span>
        <span class="s1">res[mask1] = tau * p_abs[mask1]</span>
        <span class="s1">mask2 = ~mask1 &amp; ~mask3</span>
        <span class="s1">p_abs2 = p_abs[mask2]</span>
        <span class="s1">tmp = (p_abs2**</span><span class="s4">2 </span><span class="s1">- </span><span class="s4">2 </span><span class="s1">* self.c * tau * p_abs2 + tau**</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">res[mask2] = -tmp / (</span><span class="s4">2 </span><span class="s1">* (self.c - </span><span class="s4">1</span><span class="s1">))</span>
        <span class="s1">res[mask3] = (self.c + </span><span class="s4">1</span><span class="s1">) * tau**</span><span class="s4">2 </span><span class="s1">/ </span><span class="s4">2.</span>

        <span class="s3">return </span><span class="s1">(self.weights * res).sum(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">deriv(self</span><span class="s3">, </span><span class="s1">params):</span>

        <span class="s0"># 3 segments in absolute value</span>
        <span class="s1">tau = self.tau</span>
        <span class="s1">p = np.atleast_1d(params)</span>
        <span class="s1">p_abs = np.abs(p)</span>
        <span class="s1">p_sign = np.sign(p)</span>
        <span class="s1">res = np.empty(p_abs.shape)</span>
        <span class="s1">res.fill(np.nan)</span>

        <span class="s1">mask1 = p_abs &lt; tau</span>
        <span class="s1">mask3 = p_abs &gt;= self.c * tau</span>
        <span class="s1">mask2 = ~mask1 &amp; ~mask3</span>
        <span class="s1">res[mask1] = p_sign[mask1] * tau</span>
        <span class="s1">tmp = p_sign[mask2] * (p_abs[mask2] - self.c * tau)</span>
        <span class="s1">res[mask2] = -tmp / (self.c - </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">res[mask3] = </span><span class="s4">0</span>

        <span class="s3">return </span><span class="s1">self.weights * res</span>

    <span class="s3">def </span><span class="s1">deriv2(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot;Second derivative of function 
 
        This returns scalar or vector in same shape as params, not a square 
        Hessian. If the return is 1 dimensional, then it is the diagonal of 
        the Hessian. 
        &quot;&quot;&quot;</span>

        <span class="s0"># 3 segments in absolute value</span>
        <span class="s1">tau = self.tau</span>
        <span class="s1">p = np.atleast_1d(params)</span>
        <span class="s1">p_abs = np.abs(p)</span>
        <span class="s1">res = np.zeros(p_abs.shape)</span>

        <span class="s1">mask1 = p_abs &lt; tau</span>
        <span class="s1">mask3 = p_abs &gt;= self.c * tau</span>
        <span class="s1">mask2 = ~mask1 &amp; ~mask3</span>
        <span class="s1">res[mask2] = -</span><span class="s4">1 </span><span class="s1">/ (self.c - </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">self.weights * res</span>


<span class="s3">class </span><span class="s1">SCADSmoothed(SCAD):</span>
    <span class="s2">&quot;&quot;&quot; 
    The SCAD penalty of Fan and Li, quadratically smoothed around zero. 
 
    This follows Fan and Li 2001 equation (3.7). 
 
    Parameterization follows Boo, Johnson, Li and Tan 2011 
    see docstring of SCAD 
 
    Parameters 
    ---------- 
    tau : float 
        slope and threshold for linear segment 
    c : float 
        factor for second threshold 
    c0 : float 
        threshold for quadratically smoothed segment 
    restriction : None or array 
        linear constraints for 
 
    Notes 
    ----- 
    TODO: Use delegation instead of subclassing, so smoothing can be added to 
    all penalty classes. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">tau</span><span class="s3">, </span><span class="s1">c=</span><span class="s4">3.7</span><span class="s3">, </span><span class="s1">c0=</span><span class="s3">None, </span><span class="s1">weights=</span><span class="s4">1.</span><span class="s3">, </span><span class="s1">restriction=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(tau</span><span class="s3">, </span><span class="s1">c=c</span><span class="s3">, </span><span class="s1">weights=weights)</span>
        <span class="s1">self.tau = tau</span>
        <span class="s1">self.c = c</span>
        <span class="s1">self.c0 = c0 </span><span class="s3">if </span><span class="s1">c0 </span><span class="s3">is not None else </span><span class="s1">tau * </span><span class="s4">0.1</span>
        <span class="s3">if </span><span class="s1">self.c0 &gt; tau:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'c0 cannot be larger than tau'</span><span class="s1">)</span>

        <span class="s0"># get coefficients for quadratic approximation</span>
        <span class="s1">c0 = self.c0</span>
        <span class="s0"># need to temporarily override weights for call to super</span>
        <span class="s1">weights = self.weights</span>
        <span class="s1">self.weights = </span><span class="s4">1.</span>
        <span class="s1">deriv_c0 = super(SCADSmoothed</span><span class="s3">, </span><span class="s1">self).deriv(c0)</span>
        <span class="s1">value_c0 = super(SCADSmoothed</span><span class="s3">, </span><span class="s1">self).func(c0)</span>
        <span class="s1">self.weights = weights</span>

        <span class="s1">self.aq1 = value_c0 - </span><span class="s4">0.5 </span><span class="s1">* deriv_c0 * c0</span>
        <span class="s1">self.aq2 = </span><span class="s4">0.5 </span><span class="s1">* deriv_c0 / c0</span>
        <span class="s1">self.restriction = restriction</span>

    <span class="s3">def </span><span class="s1">func(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0"># workaround for Null model</span>
        <span class="s1">weights = self._null_weights(params)</span>
        <span class="s0"># TODO: `and np.size(params) &gt; 1` is hack for llnull, need better solution</span>
        <span class="s3">if </span><span class="s1">self.restriction </span><span class="s3">is not None and </span><span class="s1">np.size(params) &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">params = self.restriction.dot(params)</span>
        <span class="s0"># need to temporarily override weights for call to super</span>
        <span class="s0"># Note: we have the same problem with `restriction`</span>
        <span class="s1">self_weights = self.weights</span>
        <span class="s1">self.weights = </span><span class="s4">1.</span>
        <span class="s1">value = super(SCADSmoothed</span><span class="s3">, </span><span class="s1">self).func(params[</span><span class="s3">None, </span><span class="s1">...])</span>
        <span class="s1">self.weights = self_weights</span>

        <span class="s0"># shift down so func(0) == 0</span>
        <span class="s1">value -= self.aq1</span>
        <span class="s0"># change the segment corrsponding to quadratic approximation</span>
        <span class="s1">p_abs = np.atleast_1d(np.abs(params))</span>
        <span class="s1">mask = p_abs &lt; self.c0</span>
        <span class="s1">p_abs_masked = p_abs[mask]</span>
        <span class="s1">value[mask] = self.aq2 * p_abs_masked**</span><span class="s4">2</span>

        <span class="s3">return </span><span class="s1">(weights * value).sum(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">deriv(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0"># workaround for Null model</span>
        <span class="s1">weights = self._null_weights(params)</span>
        <span class="s3">if </span><span class="s1">self.restriction </span><span class="s3">is not None and </span><span class="s1">np.size(params) &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">params = self.restriction.dot(params)</span>
        <span class="s0"># need to temporarily override weights for call to super</span>
        <span class="s1">self_weights = self.weights</span>
        <span class="s1">self.weights = </span><span class="s4">1.</span>
        <span class="s1">value = super(SCADSmoothed</span><span class="s3">, </span><span class="s1">self).deriv(params)</span>
        <span class="s1">self.weights = self_weights</span>

        <span class="s0">#change the segment corrsponding to quadratic approximation</span>
        <span class="s1">p = np.atleast_1d(params)</span>
        <span class="s1">mask = np.abs(p) &lt; self.c0</span>
        <span class="s1">value[mask] = </span><span class="s4">2 </span><span class="s1">* self.aq2 * p[mask]</span>

        <span class="s3">if </span><span class="s1">self.restriction </span><span class="s3">is not None and </span><span class="s1">np.size(params) &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">weights * value.dot(self.restriction)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">weights * value</span>

    <span class="s3">def </span><span class="s1">deriv2(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0"># workaround for Null model</span>
        <span class="s1">weights = self._null_weights(params)</span>
        <span class="s3">if </span><span class="s1">self.restriction </span><span class="s3">is not None and </span><span class="s1">np.size(params) &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">params = self.restriction.dot(params)</span>
        <span class="s0"># need to temporarily override weights for call to super</span>
        <span class="s1">self_weights = self.weights</span>
        <span class="s1">self.weights = </span><span class="s4">1.</span>
        <span class="s1">value = super(SCADSmoothed</span><span class="s3">, </span><span class="s1">self).deriv2(params)</span>
        <span class="s1">self.weights = self_weights</span>

        <span class="s0"># change the segment corrsponding to quadratic approximation</span>
        <span class="s1">p = np.atleast_1d(params)</span>
        <span class="s1">mask = np.abs(p) &lt; self.c0</span>
        <span class="s1">value[mask] = </span><span class="s4">2 </span><span class="s1">* self.aq2</span>

        <span class="s3">if </span><span class="s1">self.restriction </span><span class="s3">is not None and </span><span class="s1">np.size(params) &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s0"># note: super returns 1d array for diag, i.e. hessian_diag</span>
            <span class="s0"># TODO: weights are missing</span>
            <span class="s3">return </span><span class="s1">(self.restriction.T * (weights * value)</span>
                    <span class="s1">).dot(self.restriction)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">weights * value</span>


<span class="s3">class </span><span class="s1">ConstraintsPenalty:</span>
    <span class="s2">&quot;&quot;&quot; 
    Penalty applied to linear transformation of parameters 
 
    Parameters 
    ---------- 
    penalty: instance of penalty function 
        currently this requires an instance of a univariate, vectorized 
        penalty class 
    weights : None or ndarray 
        weights for adding penalties of transformed params 
    restriction : None or ndarray 
        If it is not None, then restriction defines a linear transformation 
        of the parameters. The penalty function is applied to each transformed 
        parameter independently. 
 
    Notes 
    ----- 
    `restrictions` allows us to impose penalization on contrasts or stochastic 
    constraints of the original parameters. 
    Examples for these contrast are difference penalities or all pairs 
    penalties. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">penalty</span><span class="s3">, </span><span class="s1">weights=</span><span class="s3">None, </span><span class="s1">restriction=</span><span class="s3">None</span><span class="s1">):</span>

        <span class="s1">self.penalty = penalty</span>
        <span class="s3">if </span><span class="s1">weights </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self.weights = </span><span class="s4">1.</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.weights = weights</span>

        <span class="s3">if </span><span class="s1">restriction </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">restriction = np.asarray(restriction)</span>

        <span class="s1">self.restriction = restriction</span>

    <span class="s3">def </span><span class="s1">func(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot;evaluate penalty function at params 
 
        Parameter 
        --------- 
        params : ndarray 
            array of parameters at which derivative is evaluated 
 
        Returns 
        ------- 
        deriv2 : ndarray 
            value(s) of penalty function 
        &quot;&quot;&quot;</span>
        <span class="s0"># TODO: `and np.size(params) &gt; 1` is hack for llnull, need better solution</span>
        <span class="s0"># Is this still needed? it seems to work without</span>
        <span class="s3">if </span><span class="s1">self.restriction </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">params = self.restriction.dot(params)</span>

        <span class="s1">value = self.penalty.func(params)</span>

        <span class="s3">return </span><span class="s1">(self.weights * value.T).T.sum(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">deriv(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot;first derivative of penalty function w.r.t. params 
 
        Parameter 
        --------- 
        params : ndarray 
            array of parameters at which derivative is evaluated 
 
        Returns 
        ------- 
        deriv2 : ndarray 
            array of first partial derivatives 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self.restriction </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">params = self.restriction.dot(params)</span>

        <span class="s1">value = self.penalty.deriv(params)</span>

        <span class="s3">if </span><span class="s1">self.restriction </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">self.weights * value.T.dot(self.restriction)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">(self.weights * value.T)</span>

    <span class="s1">grad = deriv</span>

    <span class="s3">def </span><span class="s1">deriv2(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot;second derivative of penalty function w.r.t. params 
 
        Parameter 
        --------- 
        params : ndarray 
            array of parameters at which derivative is evaluated 
 
        Returns 
        ------- 
        deriv2 : ndarray, 2-D 
            second derivative matrix 
        &quot;&quot;&quot;</span>

        <span class="s3">if </span><span class="s1">self.restriction </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">params = self.restriction.dot(params)</span>

        <span class="s1">value = self.penalty.deriv2(params)</span>

        <span class="s3">if </span><span class="s1">self.restriction </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s0"># note: univariate penalty returns 1d array for diag,</span>
            <span class="s0"># i.e. hessian_diag</span>
            <span class="s1">v = (self.restriction.T * value * self.weights)</span>
            <span class="s1">value = v.dot(self.restriction)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">value = np.diag(self.weights * value)</span>

        <span class="s3">return </span><span class="s1">value</span>


<span class="s3">class </span><span class="s1">L2ConstraintsPenalty(ConstraintsPenalty):</span>
    <span class="s2">&quot;&quot;&quot;convenience class of ConstraintsPenalty with L2 penalization 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">weights=</span><span class="s3">None, </span><span class="s1">restriction=</span><span class="s3">None, </span><span class="s1">sigma_prior=</span><span class="s3">None</span><span class="s1">):</span>

        <span class="s3">if </span><span class="s1">sigma_prior </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">'sigma_prior is not implemented yet'</span><span class="s1">)</span>

        <span class="s1">penalty = L2Univariate()</span>

        <span class="s1">super(L2ConstraintsPenalty</span><span class="s3">, </span><span class="s1">self).__init__(penalty</span><span class="s3">, </span><span class="s1">weights=weights</span><span class="s3">,</span>
                                                  <span class="s1">restriction=restriction)</span>


<span class="s3">class </span><span class="s1">CovariancePenalty:</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">weight):</span>
        <span class="s0"># weight should be scalar</span>
        <span class="s1">self.weight = weight</span>

    <span class="s3">def </span><span class="s1">func(self</span><span class="s3">, </span><span class="s1">mat</span><span class="s3">, </span><span class="s1">mat_inv):</span>
        <span class="s2">&quot;&quot;&quot; 
        Parameters 
        ---------- 
        mat : square matrix 
            The matrix to be penalized. 
        mat_inv : square matrix 
            The inverse of `mat`. 
 
        Returns 
        ------- 
        A scalar penalty value 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">def </span><span class="s1">deriv(self</span><span class="s3">, </span><span class="s1">mat</span><span class="s3">, </span><span class="s1">mat_inv):</span>
        <span class="s2">&quot;&quot;&quot; 
        Parameters 
        ---------- 
        mat : square matrix 
            The matrix to be penalized. 
        mat_inv : square matrix 
            The inverse of `mat`. 
 
        Returns 
        ------- 
        A vector containing the gradient of the penalty 
        with respect to each element in the lower triangle 
        of `mat`. 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>


<span class="s3">class </span><span class="s1">PSD(CovariancePenalty):</span>
    <span class="s2">&quot;&quot;&quot; 
    A penalty that converges to +infinity as the argument matrix 
    approaches the boundary of the domain of symmetric, positive 
    definite matrices. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">func(self</span><span class="s3">, </span><span class="s1">mat</span><span class="s3">, </span><span class="s1">mat_inv):</span>
        <span class="s3">try</span><span class="s1">:</span>
            <span class="s1">cy = np.linalg.cholesky(mat)</span>
        <span class="s3">except </span><span class="s1">np.linalg.LinAlgError:</span>
            <span class="s3">return </span><span class="s1">np.inf</span>
        <span class="s3">return </span><span class="s1">-</span><span class="s4">2 </span><span class="s1">* self.weight * np.sum(np.log(np.diag(cy)))</span>

    <span class="s3">def </span><span class="s1">deriv(self</span><span class="s3">, </span><span class="s1">mat</span><span class="s3">, </span><span class="s1">mat_inv):</span>
        <span class="s1">cy = mat_inv.copy()</span>
        <span class="s1">cy = </span><span class="s4">2</span><span class="s1">*cy - np.diag(np.diag(cy))</span>
        <span class="s1">i</span><span class="s3">,</span><span class="s1">j = np.tril_indices(mat.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s3">return </span><span class="s1">-self.weight * cy[i</span><span class="s3">,</span><span class="s1">j]</span>
</pre>
</body>
</html>