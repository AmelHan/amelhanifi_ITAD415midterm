<html>
<head>
<title>discriminant_analysis.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
discriminant_analysis.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Linear Discriminant Analysis and Quadratic Discriminant Analysis 
&quot;&quot;&quot;</span>

<span class="s2"># Authors: Clemens Brunner</span>
<span class="s2">#          Martin Billinger</span>
<span class="s2">#          Matthieu Perrot</span>
<span class="s2">#          Mathieu Blondel</span>

<span class="s2"># License: BSD 3-Clause</span>

<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">scipy.linalg</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">linalg</span>

<span class="s3">from </span><span class="s1">.base </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">BaseEstimator</span><span class="s3">,</span>
    <span class="s1">ClassifierMixin</span><span class="s3">,</span>
    <span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s3">,</span>
    <span class="s1">TransformerMixin</span><span class="s3">,</span>
    <span class="s1">_fit_context</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">.covariance </span><span class="s3">import </span><span class="s1">empirical_covariance</span><span class="s3">, </span><span class="s1">ledoit_wolf</span><span class="s3">, </span><span class="s1">shrunk_covariance</span>
<span class="s3">from </span><span class="s1">.linear_model._base </span><span class="s3">import </span><span class="s1">LinearClassifierMixin</span>
<span class="s3">from </span><span class="s1">.preprocessing </span><span class="s3">import </span><span class="s1">StandardScaler</span>
<span class="s3">from </span><span class="s1">.utils._array_api </span><span class="s3">import </span><span class="s1">_expit</span><span class="s3">, </span><span class="s1">device</span><span class="s3">, </span><span class="s1">get_namespace</span><span class="s3">, </span><span class="s1">size</span>
<span class="s3">from </span><span class="s1">.utils._param_validation </span><span class="s3">import </span><span class="s1">HasMethods</span><span class="s3">, </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">StrOptions</span>
<span class="s3">from </span><span class="s1">.utils.extmath </span><span class="s3">import </span><span class="s1">softmax</span>
<span class="s3">from </span><span class="s1">.utils.multiclass </span><span class="s3">import </span><span class="s1">check_classification_targets</span><span class="s3">, </span><span class="s1">unique_labels</span>
<span class="s3">from </span><span class="s1">.utils.validation </span><span class="s3">import </span><span class="s1">check_is_fitted</span>

<span class="s1">__all__ = [</span><span class="s4">&quot;LinearDiscriminantAnalysis&quot;</span><span class="s3">, </span><span class="s4">&quot;QuadraticDiscriminantAnalysis&quot;</span><span class="s1">]</span>


<span class="s3">def </span><span class="s1">_cov(X</span><span class="s3">, </span><span class="s1">shrinkage=</span><span class="s3">None, </span><span class="s1">covariance_estimator=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Estimate covariance matrix (using optional covariance_estimator). 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
        Input data. 
 
    shrinkage : {'empirical', 'auto'} or float, default=None 
        Shrinkage parameter, possible values: 
          - None or 'empirical': no shrinkage (default). 
          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma. 
          - float between 0 and 1: fixed shrinkage parameter. 
 
        Shrinkage parameter is ignored if  `covariance_estimator` 
        is not None. 
 
    covariance_estimator : estimator, default=None 
        If not None, `covariance_estimator` is used to estimate 
        the covariance matrices instead of relying on the empirical 
        covariance estimator (with potential shrinkage). 
        The object should have a fit method and a ``covariance_`` attribute 
        like the estimators in :mod:`sklearn.covariance``. 
        if None the shrinkage parameter drives the estimate. 
 
        .. versionadded:: 0.24 
 
    Returns 
    ------- 
    s : ndarray of shape (n_features, n_features) 
        Estimated covariance matrix. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">covariance_estimator </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">shrinkage = </span><span class="s4">&quot;empirical&quot; </span><span class="s3">if </span><span class="s1">shrinkage </span><span class="s3">is None else </span><span class="s1">shrinkage</span>
        <span class="s3">if </span><span class="s1">isinstance(shrinkage</span><span class="s3">, </span><span class="s1">str):</span>
            <span class="s3">if </span><span class="s1">shrinkage == </span><span class="s4">&quot;auto&quot;</span><span class="s1">:</span>
                <span class="s1">sc = StandardScaler()  </span><span class="s2"># standardize features</span>
                <span class="s1">X = sc.fit_transform(X)</span>
                <span class="s1">s = ledoit_wolf(X)[</span><span class="s5">0</span><span class="s1">]</span>
                <span class="s2"># rescale</span>
                <span class="s1">s = sc.scale_[:</span><span class="s3">, </span><span class="s1">np.newaxis] * s * sc.scale_[np.newaxis</span><span class="s3">, </span><span class="s1">:]</span>
            <span class="s3">elif </span><span class="s1">shrinkage == </span><span class="s4">&quot;empirical&quot;</span><span class="s1">:</span>
                <span class="s1">s = empirical_covariance(X)</span>
        <span class="s3">elif </span><span class="s1">isinstance(shrinkage</span><span class="s3">, </span><span class="s1">Real):</span>
            <span class="s1">s = shrunk_covariance(empirical_covariance(X)</span><span class="s3">, </span><span class="s1">shrinkage)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">shrinkage </span><span class="s3">is not None and </span><span class="s1">shrinkage != </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;covariance_estimator and shrinkage parameters &quot;</span>
                <span class="s4">&quot;are not None. Only one of the two can be set.&quot;</span>
            <span class="s1">)</span>
        <span class="s1">covariance_estimator.fit(X)</span>
        <span class="s3">if not </span><span class="s1">hasattr(covariance_estimator</span><span class="s3">, </span><span class="s4">&quot;covariance_&quot;</span><span class="s1">):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;%s does not have a covariance_ attribute&quot;</span>
                <span class="s1">% covariance_estimator.__class__.__name__</span>
            <span class="s1">)</span>
        <span class="s1">s = covariance_estimator.covariance_</span>
    <span class="s3">return </span><span class="s1">s</span>


<span class="s3">def </span><span class="s1">_class_means(X</span><span class="s3">, </span><span class="s1">y):</span>
    <span class="s0">&quot;&quot;&quot;Compute class means. 
 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
        Input data. 
 
    y : array-like of shape (n_samples,) or (n_samples, n_targets) 
        Target values. 
 
    Returns 
    ------- 
    means : array-like of shape (n_classes, n_features) 
        Class means. 
    &quot;&quot;&quot;</span>
    <span class="s1">xp</span><span class="s3">, </span><span class="s1">is_array_api_compliant = get_namespace(X)</span>
    <span class="s1">classes</span><span class="s3">, </span><span class="s1">y = xp.unique_inverse(y)</span>
    <span class="s1">means = xp.zeros((classes.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">device=device(X)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s3">if </span><span class="s1">is_array_api_compliant:</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(classes.shape[</span><span class="s5">0</span><span class="s1">]):</span>
            <span class="s1">means[i</span><span class="s3">, </span><span class="s1">:] = xp.mean(X[y == i]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s2"># TODO: Explore the choice of using bincount + add.at as it seems sub optimal</span>
        <span class="s2"># from a performance-wise</span>
        <span class="s1">cnt = np.bincount(y)</span>
        <span class="s1">np.add.at(means</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">X)</span>
        <span class="s1">means /= cnt[:</span><span class="s3">, None</span><span class="s1">]</span>
    <span class="s3">return </span><span class="s1">means</span>


<span class="s3">def </span><span class="s1">_class_cov(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">priors</span><span class="s3">, </span><span class="s1">shrinkage=</span><span class="s3">None, </span><span class="s1">covariance_estimator=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Compute weighted within-class covariance matrix. 
 
    The per-class covariance are weighted by the class priors. 
 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
        Input data. 
 
    y : array-like of shape (n_samples,) or (n_samples, n_targets) 
        Target values. 
 
    priors : array-like of shape (n_classes,) 
        Class priors. 
 
    shrinkage : 'auto' or float, default=None 
        Shrinkage parameter, possible values: 
          - None: no shrinkage (default). 
          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma. 
          - float between 0 and 1: fixed shrinkage parameter. 
 
        Shrinkage parameter is ignored if `covariance_estimator` is not None. 
 
    covariance_estimator : estimator, default=None 
        If not None, `covariance_estimator` is used to estimate 
        the covariance matrices instead of relying the empirical 
        covariance estimator (with potential shrinkage). 
        The object should have a fit method and a ``covariance_`` attribute 
        like the estimators in sklearn.covariance. 
        If None, the shrinkage parameter drives the estimate. 
 
        .. versionadded:: 0.24 
 
    Returns 
    ------- 
    cov : array-like of shape (n_features, n_features) 
        Weighted within-class covariance matrix 
    &quot;&quot;&quot;</span>
    <span class="s1">classes = np.unique(y)</span>
    <span class="s1">cov = np.zeros(shape=(X.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">]))</span>
    <span class="s3">for </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">group </span><span class="s3">in </span><span class="s1">enumerate(classes):</span>
        <span class="s1">Xg = X[y == group</span><span class="s3">, </span><span class="s1">:]</span>
        <span class="s1">cov += priors[idx] * np.atleast_2d(_cov(Xg</span><span class="s3">, </span><span class="s1">shrinkage</span><span class="s3">, </span><span class="s1">covariance_estimator))</span>
    <span class="s3">return </span><span class="s1">cov</span>


<span class="s3">class </span><span class="s1">LinearDiscriminantAnalysis(</span>
    <span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s3">,</span>
    <span class="s1">LinearClassifierMixin</span><span class="s3">,</span>
    <span class="s1">TransformerMixin</span><span class="s3">,</span>
    <span class="s1">BaseEstimator</span><span class="s3">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Linear Discriminant Analysis. 
 
    A classifier with a linear decision boundary, generated by fitting class 
    conditional densities to the data and using Bayes' rule. 
 
    The model fits a Gaussian density to each class, assuming that all classes 
    share the same covariance matrix. 
 
    The fitted model can also be used to reduce the dimensionality of the input 
    by projecting it to the most discriminative directions, using the 
    `transform` method. 
 
    .. versionadded:: 0.17 
       *LinearDiscriminantAnalysis*. 
 
    Read more in the :ref:`User Guide &lt;lda_qda&gt;`. 
 
    Parameters 
    ---------- 
    solver : {'svd', 'lsqr', 'eigen'}, default='svd' 
        Solver to use, possible values: 
          - 'svd': Singular value decomposition (default). 
            Does not compute the covariance matrix, therefore this solver is 
            recommended for data with a large number of features. 
          - 'lsqr': Least squares solution. 
            Can be combined with shrinkage or custom covariance estimator. 
          - 'eigen': Eigenvalue decomposition. 
            Can be combined with shrinkage or custom covariance estimator. 
 
        .. versionchanged:: 1.2 
            `solver=&quot;svd&quot;` now has experimental Array API support. See the 
            :ref:`Array API User Guide &lt;array_api&gt;` for more details. 
 
    shrinkage : 'auto' or float, default=None 
        Shrinkage parameter, possible values: 
          - None: no shrinkage (default). 
          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma. 
          - float between 0 and 1: fixed shrinkage parameter. 
 
        This should be left to None if `covariance_estimator` is used. 
        Note that shrinkage works only with 'lsqr' and 'eigen' solvers. 
 
    priors : array-like of shape (n_classes,), default=None 
        The class prior probabilities. By default, the class proportions are 
        inferred from the training data. 
 
    n_components : int, default=None 
        Number of components (&lt;= min(n_classes - 1, n_features)) for 
        dimensionality reduction. If None, will be set to 
        min(n_classes - 1, n_features). This parameter only affects the 
        `transform` method. 
 
    store_covariance : bool, default=False 
        If True, explicitly compute the weighted within-class covariance 
        matrix when solver is 'svd'. The matrix is always computed 
        and stored for the other solvers. 
 
        .. versionadded:: 0.17 
 
    tol : float, default=1.0e-4 
        Absolute threshold for a singular value of X to be considered 
        significant, used to estimate the rank of X. Dimensions whose 
        singular values are non-significant are discarded. Only used if 
        solver is 'svd'. 
 
        .. versionadded:: 0.17 
 
    covariance_estimator : covariance estimator, default=None 
        If not None, `covariance_estimator` is used to estimate 
        the covariance matrices instead of relying on the empirical 
        covariance estimator (with potential shrinkage). 
        The object should have a fit method and a ``covariance_`` attribute 
        like the estimators in :mod:`sklearn.covariance`. 
        if None the shrinkage parameter drives the estimate. 
 
        This should be left to None if `shrinkage` is used. 
        Note that `covariance_estimator` works only with 'lsqr' and 'eigen' 
        solvers. 
 
        .. versionadded:: 0.24 
 
    Attributes 
    ---------- 
    coef_ : ndarray of shape (n_features,) or (n_classes, n_features) 
        Weight vector(s). 
 
    intercept_ : ndarray of shape (n_classes,) 
        Intercept term. 
 
    covariance_ : array-like of shape (n_features, n_features) 
        Weighted within-class covariance matrix. It corresponds to 
        `sum_k prior_k * C_k` where `C_k` is the covariance matrix of the 
        samples in class `k`. The `C_k` are estimated using the (potentially 
        shrunk) biased estimator of covariance. If solver is 'svd', only 
        exists when `store_covariance` is True. 
 
    explained_variance_ratio_ : ndarray of shape (n_components,) 
        Percentage of variance explained by each of the selected components. 
        If ``n_components`` is not set then all components are stored and the 
        sum of explained variances is equal to 1.0. Only available when eigen 
        or svd solver is used. 
 
    means_ : array-like of shape (n_classes, n_features) 
        Class-wise means. 
 
    priors_ : array-like of shape (n_classes,) 
        Class priors (sum to 1). 
 
    scalings_ : array-like of shape (rank, n_classes - 1) 
        Scaling of the features in the space spanned by the class centroids. 
        Only available for 'svd' and 'eigen' solvers. 
 
    xbar_ : array-like of shape (n_features,) 
        Overall mean. Only present if solver is 'svd'. 
 
    classes_ : array-like of shape (n_classes,) 
        Unique class labels. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    QuadraticDiscriminantAnalysis : Quadratic Discriminant Analysis. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.discriminant_analysis import LinearDiscriminantAnalysis 
    &gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) 
    &gt;&gt;&gt; y = np.array([1, 1, 1, 2, 2, 2]) 
    &gt;&gt;&gt; clf = LinearDiscriminantAnalysis() 
    &gt;&gt;&gt; clf.fit(X, y) 
    LinearDiscriminantAnalysis() 
    &gt;&gt;&gt; print(clf.predict([[-0.8, -1]])) 
    [1] 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;solver&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;svd&quot;</span><span class="s3">, </span><span class="s4">&quot;lsqr&quot;</span><span class="s3">, </span><span class="s4">&quot;eigen&quot;</span><span class="s1">})]</span><span class="s3">,</span>
        <span class="s4">&quot;shrinkage&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;auto&quot;</span><span class="s1">})</span><span class="s3">, </span><span class="s1">Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">closed=</span><span class="s4">&quot;both&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_components&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;priors&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;store_covariance&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;covariance_estimator&quot;</span><span class="s1">: [HasMethods(</span><span class="s4">&quot;fit&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">solver=</span><span class="s4">&quot;svd&quot;</span><span class="s3">,</span>
        <span class="s1">shrinkage=</span><span class="s3">None,</span>
        <span class="s1">priors=</span><span class="s3">None,</span>
        <span class="s1">n_components=</span><span class="s3">None,</span>
        <span class="s1">store_covariance=</span><span class="s3">False,</span>
        <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
        <span class="s1">covariance_estimator=</span><span class="s3">None,</span>
    <span class="s1">):</span>
        <span class="s1">self.solver = solver</span>
        <span class="s1">self.shrinkage = shrinkage</span>
        <span class="s1">self.priors = priors</span>
        <span class="s1">self.n_components = n_components</span>
        <span class="s1">self.store_covariance = store_covariance  </span><span class="s2"># used only in svd solver</span>
        <span class="s1">self.tol = tol  </span><span class="s2"># used only in svd solver</span>
        <span class="s1">self.covariance_estimator = covariance_estimator</span>

    <span class="s3">def </span><span class="s1">_solve_lstsq(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">shrinkage</span><span class="s3">, </span><span class="s1">covariance_estimator):</span>
        <span class="s0">&quot;&quot;&quot;Least squares solver. 
 
        The least squares solver computes a straightforward solution of the 
        optimal decision rule based directly on the discriminant functions. It 
        can only be used for classification (with any covariance estimator), 
        because 
        estimation of eigenvectors is not performed. Therefore, dimensionality 
        reduction with the transform is not supported. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training data. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_classes) 
            Target values. 
 
        shrinkage : 'auto', float or None 
            Shrinkage parameter, possible values: 
              - None: no shrinkage. 
              - 'auto': automatic shrinkage using the Ledoit-Wolf lemma. 
              - float between 0 and 1: fixed shrinkage parameter. 
 
            Shrinkage parameter is ignored if  `covariance_estimator` i 
            not None 
 
        covariance_estimator : estimator, default=None 
            If not None, `covariance_estimator` is used to estimate 
            the covariance matrices instead of relying the empirical 
            covariance estimator (with potential shrinkage). 
            The object should have a fit method and a ``covariance_`` attribute 
            like the estimators in sklearn.covariance. 
            if None the shrinkage parameter drives the estimate. 
 
            .. versionadded:: 0.24 
 
        Notes 
        ----- 
        This solver is based on [1]_, section 2.6.2, pp. 39-41. 
 
        References 
        ---------- 
        .. [1] R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification 
           (Second Edition). John Wiley &amp; Sons, Inc., New York, 2001. ISBN 
           0-471-05669-3. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.means_ = _class_means(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s1">self.covariance_ = _class_cov(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">self.priors_</span><span class="s3">, </span><span class="s1">shrinkage</span><span class="s3">, </span><span class="s1">covariance_estimator</span>
        <span class="s1">)</span>
        <span class="s1">self.coef_ = linalg.lstsq(self.covariance_</span><span class="s3">, </span><span class="s1">self.means_.T)[</span><span class="s5">0</span><span class="s1">].T</span>
        <span class="s1">self.intercept_ = -</span><span class="s5">0.5 </span><span class="s1">* np.diag(np.dot(self.means_</span><span class="s3">, </span><span class="s1">self.coef_.T)) + np.log(</span>
            <span class="s1">self.priors_</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_solve_eigen(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">shrinkage</span><span class="s3">, </span><span class="s1">covariance_estimator):</span>
        <span class="s0">&quot;&quot;&quot;Eigenvalue solver. 
 
        The eigenvalue solver computes the optimal solution of the Rayleigh 
        coefficient (basically the ratio of between class scatter to within 
        class scatter). This solver supports both classification and 
        dimensionality reduction (with any covariance estimator). 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training data. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_targets) 
            Target values. 
 
        shrinkage : 'auto', float or None 
            Shrinkage parameter, possible values: 
              - None: no shrinkage. 
              - 'auto': automatic shrinkage using the Ledoit-Wolf lemma. 
              - float between 0 and 1: fixed shrinkage constant. 
 
            Shrinkage parameter is ignored if  `covariance_estimator` i 
            not None 
 
        covariance_estimator : estimator, default=None 
            If not None, `covariance_estimator` is used to estimate 
            the covariance matrices instead of relying the empirical 
            covariance estimator (with potential shrinkage). 
            The object should have a fit method and a ``covariance_`` attribute 
            like the estimators in sklearn.covariance. 
            if None the shrinkage parameter drives the estimate. 
 
            .. versionadded:: 0.24 
 
        Notes 
        ----- 
        This solver is based on [1]_, section 3.8.3, pp. 121-124. 
 
        References 
        ---------- 
        .. [1] R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification 
           (Second Edition). John Wiley &amp; Sons, Inc., New York, 2001. ISBN 
           0-471-05669-3. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.means_ = _class_means(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s1">self.covariance_ = _class_cov(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">self.priors_</span><span class="s3">, </span><span class="s1">shrinkage</span><span class="s3">, </span><span class="s1">covariance_estimator</span>
        <span class="s1">)</span>

        <span class="s1">Sw = self.covariance_  </span><span class="s2"># within scatter</span>
        <span class="s1">St = _cov(X</span><span class="s3">, </span><span class="s1">shrinkage</span><span class="s3">, </span><span class="s1">covariance_estimator)  </span><span class="s2"># total scatter</span>
        <span class="s1">Sb = St - Sw  </span><span class="s2"># between scatter</span>

        <span class="s1">evals</span><span class="s3">, </span><span class="s1">evecs = linalg.eigh(Sb</span><span class="s3">, </span><span class="s1">Sw)</span>
        <span class="s1">self.explained_variance_ratio_ = np.sort(evals / np.sum(evals))[::-</span><span class="s5">1</span><span class="s1">][</span>
            <span class="s1">: self._max_components</span>
        <span class="s1">]</span>
        <span class="s1">evecs = evecs[:</span><span class="s3">, </span><span class="s1">np.argsort(evals)[::-</span><span class="s5">1</span><span class="s1">]]  </span><span class="s2"># sort eigenvectors</span>

        <span class="s1">self.scalings_ = evecs</span>
        <span class="s1">self.coef_ = np.dot(self.means_</span><span class="s3">, </span><span class="s1">evecs).dot(evecs.T)</span>
        <span class="s1">self.intercept_ = -</span><span class="s5">0.5 </span><span class="s1">* np.diag(np.dot(self.means_</span><span class="s3">, </span><span class="s1">self.coef_.T)) + np.log(</span>
            <span class="s1">self.priors_</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_solve_svd(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s0">&quot;&quot;&quot;SVD solver. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training data. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_targets) 
            Target values. 
        &quot;&quot;&quot;</span>
        <span class="s1">xp</span><span class="s3">, </span><span class="s1">is_array_api_compliant = get_namespace(X)</span>

        <span class="s3">if </span><span class="s1">is_array_api_compliant:</span>
            <span class="s1">svd = xp.linalg.svd</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">svd = scipy.linalg.svd</span>

        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>
        <span class="s1">n_classes = self.classes_.shape[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s1">self.means_ = _class_means(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s3">if </span><span class="s1">self.store_covariance:</span>
            <span class="s1">self.covariance_ = _class_cov(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">self.priors_)</span>

        <span class="s1">Xc = []</span>
        <span class="s3">for </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">group </span><span class="s3">in </span><span class="s1">enumerate(self.classes_):</span>
            <span class="s1">Xg = X[y == group]</span>
            <span class="s1">Xc.append(Xg - self.means_[idx</span><span class="s3">, </span><span class="s1">:])</span>

        <span class="s1">self.xbar_ = self.priors_ @ self.means_</span>

        <span class="s1">Xc = xp.concat(Xc</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>

        <span class="s2"># 1) within (univariate) scaling by with classes std-dev</span>
        <span class="s1">std = xp.std(Xc</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s2"># avoid division by zero in normalization</span>
        <span class="s1">std[std == </span><span class="s5">0</span><span class="s1">] = </span><span class="s5">1.0</span>
        <span class="s1">fac = xp.asarray(</span><span class="s5">1.0 </span><span class="s1">/ (n_samples - n_classes))</span>

        <span class="s2"># 2) Within variance scaling</span>
        <span class="s1">X = xp.sqrt(fac) * (Xc / std)</span>
        <span class="s2"># SVD of centered (within)scaled data</span>
        <span class="s1">U</span><span class="s3">, </span><span class="s1">S</span><span class="s3">, </span><span class="s1">Vt = svd(X</span><span class="s3">, </span><span class="s1">full_matrices=</span><span class="s3">False</span><span class="s1">)</span>

        <span class="s1">rank = xp.sum(xp.astype(S &gt; self.tol</span><span class="s3">, </span><span class="s1">xp.int32))</span>
        <span class="s2"># Scaling of within covariance is: V' 1/S</span>
        <span class="s1">scalings = (Vt[:rank</span><span class="s3">, </span><span class="s1">:] / std).T / S[:rank]</span>
        <span class="s1">fac = </span><span class="s5">1.0 </span><span class="s3">if </span><span class="s1">n_classes == </span><span class="s5">1 </span><span class="s3">else </span><span class="s5">1.0 </span><span class="s1">/ (n_classes - </span><span class="s5">1</span><span class="s1">)</span>

        <span class="s2"># 3) Between variance scaling</span>
        <span class="s2"># Scale weighted centers</span>
        <span class="s1">X = (</span>
            <span class="s1">(xp.sqrt((n_samples * self.priors_) * fac)) * (self.means_ - self.xbar_).T</span>
        <span class="s1">).T @ scalings</span>
        <span class="s2"># Centers are living in a space with n_classes-1 dim (maximum)</span>
        <span class="s2"># Use SVD to find projection in the space spanned by the</span>
        <span class="s2"># (n_classes) centers</span>
        <span class="s1">_</span><span class="s3">, </span><span class="s1">S</span><span class="s3">, </span><span class="s1">Vt = svd(X</span><span class="s3">, </span><span class="s1">full_matrices=</span><span class="s3">False</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self._max_components == </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">self.explained_variance_ratio_ = xp.empty((</span><span class="s5">0</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=S.dtype)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.explained_variance_ratio_ = (S**</span><span class="s5">2 </span><span class="s1">/ xp.sum(S**</span><span class="s5">2</span><span class="s1">))[</span>
                <span class="s1">: self._max_components</span>
            <span class="s1">]</span>

        <span class="s1">rank = xp.sum(xp.astype(S &gt; self.tol * S[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">xp.int32))</span>
        <span class="s1">self.scalings_ = scalings @ Vt.T[:</span><span class="s3">, </span><span class="s1">:rank]</span>
        <span class="s1">coef = (self.means_ - self.xbar_) @ self.scalings_</span>
        <span class="s1">self.intercept_ = -</span><span class="s5">0.5 </span><span class="s1">* xp.sum(coef**</span><span class="s5">2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">) + xp.log(self.priors_)</span>
        <span class="s1">self.coef_ = coef @ self.scalings_.T</span>
        <span class="s1">self.intercept_ -= self.xbar_ @ self.coef_.T</span>

    <span class="s1">@_fit_context(</span>
        <span class="s2"># LinearDiscriminantAnalysis.covariance_estimator is not validated yet</span>
        <span class="s1">prefer_skip_nested_validation=</span><span class="s3">False</span>
    <span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s0">&quot;&quot;&quot;Fit the Linear Discriminant Analysis model. 
 
           .. versionchanged:: 0.19 
              *store_covariance* has been moved to main constructor. 
 
           .. versionchanged:: 0.19 
              *tol* has been moved to main constructor. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training data. 
 
        y : array-like of shape (n_samples,) 
            Target values. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">xp</span><span class="s3">, </span><span class="s1">_ = get_namespace(X)</span>

        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">ensure_min_samples=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">dtype=[xp.float64</span><span class="s3">, </span><span class="s1">xp.float32]</span>
        <span class="s1">)</span>
        <span class="s1">self.classes_ = unique_labels(y)</span>
        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">_ = X.shape</span>
        <span class="s1">n_classes = self.classes_.shape[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s3">if </span><span class="s1">n_samples == n_classes:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;The number of samples must be more than the number of classes.&quot;</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.priors </span><span class="s3">is None</span><span class="s1">:  </span><span class="s2"># estimate priors from sample</span>
            <span class="s1">_</span><span class="s3">, </span><span class="s1">cnts = xp.unique_counts(y)  </span><span class="s2"># non-negative ints</span>
            <span class="s1">self.priors_ = xp.astype(cnts</span><span class="s3">, </span><span class="s1">X.dtype) / float(y.shape[</span><span class="s5">0</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.priors_ = xp.asarray(self.priors</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s3">if </span><span class="s1">xp.any(self.priors_ &lt; </span><span class="s5">0</span><span class="s1">):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;priors must be non-negative&quot;</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">xp.abs(xp.sum(self.priors_) - </span><span class="s5">1.0</span><span class="s1">) &gt; </span><span class="s5">1e-5</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span><span class="s4">&quot;The priors do not sum to 1. Renormalizing&quot;</span><span class="s3">, </span><span class="s1">UserWarning)</span>
            <span class="s1">self.priors_ = self.priors_ / self.priors_.sum()</span>

        <span class="s2"># Maximum number of components no matter what n_components is</span>
        <span class="s2"># specified:</span>
        <span class="s1">max_components = min(n_classes - </span><span class="s5">1</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">])</span>

        <span class="s3">if </span><span class="s1">self.n_components </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self._max_components = max_components</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self.n_components &gt; max_components:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;n_components cannot be larger than min(n_features, n_classes - 1).&quot;</span>
                <span class="s1">)</span>
            <span class="s1">self._max_components = self.n_components</span>

        <span class="s3">if </span><span class="s1">self.solver == </span><span class="s4">&quot;svd&quot;</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self.shrinkage </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;shrinkage not supported with 'svd' solver.&quot;</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">self.covariance_estimator </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;covariance estimator &quot;</span>
                    <span class="s4">&quot;is not supported &quot;</span>
                    <span class="s4">&quot;with svd solver. Try another solver&quot;</span>
                <span class="s1">)</span>
            <span class="s1">self._solve_svd(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s3">elif </span><span class="s1">self.solver == </span><span class="s4">&quot;lsqr&quot;</span><span class="s1">:</span>
            <span class="s1">self._solve_lstsq(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">y</span><span class="s3">,</span>
                <span class="s1">shrinkage=self.shrinkage</span><span class="s3">,</span>
                <span class="s1">covariance_estimator=self.covariance_estimator</span><span class="s3">,</span>
            <span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">self.solver == </span><span class="s4">&quot;eigen&quot;</span><span class="s1">:</span>
            <span class="s1">self._solve_eigen(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">y</span><span class="s3">,</span>
                <span class="s1">shrinkage=self.shrinkage</span><span class="s3">,</span>
                <span class="s1">covariance_estimator=self.covariance_estimator</span><span class="s3">,</span>
            <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">size(self.classes_) == </span><span class="s5">2</span><span class="s1">:  </span><span class="s2"># treat binary case as a special case</span>
            <span class="s1">coef_ = xp.asarray(self.coef_[</span><span class="s5">1</span><span class="s3">, </span><span class="s1">:] - self.coef_[</span><span class="s5">0</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
            <span class="s1">self.coef_ = xp.reshape(coef_</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">))</span>
            <span class="s1">intercept_ = xp.asarray(</span>
                <span class="s1">self.intercept_[</span><span class="s5">1</span><span class="s1">] - self.intercept_[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span>
            <span class="s1">)</span>
            <span class="s1">self.intercept_ = xp.reshape(intercept_</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">,</span><span class="s1">))</span>
        <span class="s1">self._n_features_out = self._max_components</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">transform(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Project data to maximize class separation. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Input data. 
 
        Returns 
        ------- 
        X_new : ndarray of shape (n_samples, n_components) or \ 
            (n_samples, min(rank, n_components)) 
            Transformed data. In the case of the 'svd' solver, the shape 
            is (n_samples, min(rank, n_components)). 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self.solver == </span><span class="s4">&quot;lsqr&quot;</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
                <span class="s4">&quot;transform not implemented for 'lsqr' solver (use 'svd' or 'eigen').&quot;</span>
            <span class="s1">)</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">xp</span><span class="s3">, </span><span class="s1">_ = get_namespace(X)</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.solver == </span><span class="s4">&quot;svd&quot;</span><span class="s1">:</span>
            <span class="s1">X_new = (X - self.xbar_) @ self.scalings_</span>
        <span class="s3">elif </span><span class="s1">self.solver == </span><span class="s4">&quot;eigen&quot;</span><span class="s1">:</span>
            <span class="s1">X_new = X @ self.scalings_</span>

        <span class="s3">return </span><span class="s1">X_new[:</span><span class="s3">, </span><span class="s1">: self._max_components]</span>

    <span class="s3">def </span><span class="s1">predict_proba(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Estimate probability. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Input data. 
 
        Returns 
        ------- 
        C : ndarray of shape (n_samples, n_classes) 
            Estimated probabilities. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">xp</span><span class="s3">, </span><span class="s1">is_array_api_compliant = get_namespace(X)</span>
        <span class="s1">decision = self.decision_function(X)</span>
        <span class="s3">if </span><span class="s1">size(self.classes_) == </span><span class="s5">2</span><span class="s1">:</span>
            <span class="s1">proba = _expit(decision)</span>
            <span class="s3">return </span><span class="s1">xp.stack([</span><span class="s5">1 </span><span class="s1">- proba</span><span class="s3">, </span><span class="s1">proba]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">softmax(decision)</span>

    <span class="s3">def </span><span class="s1">predict_log_proba(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Estimate log probability. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Input data. 
 
        Returns 
        ------- 
        C : ndarray of shape (n_samples, n_classes) 
            Estimated log probabilities. 
        &quot;&quot;&quot;</span>
        <span class="s1">xp</span><span class="s3">, </span><span class="s1">_ = get_namespace(X)</span>
        <span class="s1">prediction = self.predict_proba(X)</span>

        <span class="s1">info = xp.finfo(prediction.dtype)</span>
        <span class="s3">if </span><span class="s1">hasattr(info</span><span class="s3">, </span><span class="s4">&quot;smallest_normal&quot;</span><span class="s1">):</span>
            <span class="s1">smallest_normal = info.smallest_normal</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># smallest_normal was introduced in NumPy 1.22</span>
            <span class="s1">smallest_normal = info.tiny</span>

        <span class="s1">prediction[prediction == </span><span class="s5">0.0</span><span class="s1">] += smallest_normal</span>
        <span class="s3">return </span><span class="s1">xp.log(prediction)</span>

    <span class="s3">def </span><span class="s1">decision_function(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Apply decision function to an array of samples. 
 
        The decision function is equal (up to a constant factor) to the 
        log-posterior of the model, i.e. `log p(y = k | x)`. In a binary 
        classification setting this instead corresponds to the difference 
        `log p(y = 1 | x) - log p(y = 0 | x)`. See :ref:`lda_qda_math`. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Array of samples (test vectors). 
 
        Returns 
        ------- 
        C : ndarray of shape (n_samples,) or (n_samples, n_classes) 
            Decision function values related to each class, per sample. 
            In the two-class case, the shape is (n_samples,), giving the 
            log likelihood ratio of the positive class. 
        &quot;&quot;&quot;</span>
        <span class="s2"># Only override for the doc</span>
        <span class="s3">return </span><span class="s1">super().decision_function(X)</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span><span class="s4">&quot;array_api_support&quot;</span><span class="s1">: </span><span class="s3">True</span><span class="s1">}</span>


<span class="s3">class </span><span class="s1">QuadraticDiscriminantAnalysis(ClassifierMixin</span><span class="s3">, </span><span class="s1">BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Quadratic Discriminant Analysis. 
 
    A classifier with a quadratic decision boundary, generated 
    by fitting class conditional densities to the data 
    and using Bayes' rule. 
 
    The model fits a Gaussian density to each class. 
 
    .. versionadded:: 0.17 
       *QuadraticDiscriminantAnalysis* 
 
    Read more in the :ref:`User Guide &lt;lda_qda&gt;`. 
 
    Parameters 
    ---------- 
    priors : array-like of shape (n_classes,), default=None 
        Class priors. By default, the class proportions are inferred from the 
        training data. 
 
    reg_param : float, default=0.0 
        Regularizes the per-class covariance estimates by transforming S2 as 
        ``S2 = (1 - reg_param) * S2 + reg_param * np.eye(n_features)``, 
        where S2 corresponds to the `scaling_` attribute of a given class. 
 
    store_covariance : bool, default=False 
        If True, the class covariance matrices are explicitly computed and 
        stored in the `self.covariance_` attribute. 
 
        .. versionadded:: 0.17 
 
    tol : float, default=1.0e-4 
        Absolute threshold for a singular value to be considered significant, 
        used to estimate the rank of `Xk` where `Xk` is the centered matrix 
        of samples in class k. This parameter does not affect the 
        predictions. It only controls a warning that is raised when features 
        are considered to be colinear. 
 
        .. versionadded:: 0.17 
 
    Attributes 
    ---------- 
    covariance_ : list of len n_classes of ndarray \ 
            of shape (n_features, n_features) 
        For each class, gives the covariance matrix estimated using the 
        samples of that class. The estimations are unbiased. Only present if 
        `store_covariance` is True. 
 
    means_ : array-like of shape (n_classes, n_features) 
        Class-wise means. 
 
    priors_ : array-like of shape (n_classes,) 
        Class priors (sum to 1). 
 
    rotations_ : list of len n_classes of ndarray of shape (n_features, n_k) 
        For each class k an array of shape (n_features, n_k), where 
        ``n_k = min(n_features, number of elements in class k)`` 
        It is the rotation of the Gaussian distribution, i.e. its 
        principal axis. It corresponds to `V`, the matrix of eigenvectors 
        coming from the SVD of `Xk = U S Vt` where `Xk` is the centered 
        matrix of samples from class k. 
 
    scalings_ : list of len n_classes of ndarray of shape (n_k,) 
        For each class, contains the scaling of 
        the Gaussian distributions along its principal axes, i.e. the 
        variance in the rotated coordinate system. It corresponds to `S^2 / 
        (n_samples - 1)`, where `S` is the diagonal matrix of singular values 
        from the SVD of `Xk`, where `Xk` is the centered matrix of samples 
        from class k. 
 
    classes_ : ndarray of shape (n_classes,) 
        Unique class labels. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    LinearDiscriminantAnalysis : Linear Discriminant Analysis. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) 
    &gt;&gt;&gt; y = np.array([1, 1, 1, 2, 2, 2]) 
    &gt;&gt;&gt; clf = QuadraticDiscriminantAnalysis() 
    &gt;&gt;&gt; clf.fit(X, y) 
    QuadraticDiscriminantAnalysis() 
    &gt;&gt;&gt; print(clf.predict([[-0.8, -1]])) 
    [1] 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;priors&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;reg_param&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">closed=</span><span class="s4">&quot;both&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;store_covariance&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">priors=</span><span class="s3">None, </span><span class="s1">reg_param=</span><span class="s5">0.0</span><span class="s3">, </span><span class="s1">store_covariance=</span><span class="s3">False, </span><span class="s1">tol=</span><span class="s5">1.0e-4</span>
    <span class="s1">):</span>
        <span class="s1">self.priors = priors</span>
        <span class="s1">self.reg_param = reg_param</span>
        <span class="s1">self.store_covariance = store_covariance</span>
        <span class="s1">self.tol = tol</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model according to the given training data and parameters. 
 
            .. versionchanged:: 0.19 
               ``store_covariances`` has been moved to main constructor as 
               ``store_covariance`` 
 
            .. versionchanged:: 0.19 
               ``tol`` has been moved to main constructor. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training vector, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
 
        y : array-like of shape (n_samples,) 
            Target values (integers). 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s1">check_classification_targets(y)</span>
        <span class="s1">self.classes_</span><span class="s3">, </span><span class="s1">y = np.unique(y</span><span class="s3">, </span><span class="s1">return_inverse=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>
        <span class="s1">n_classes = len(self.classes_)</span>
        <span class="s3">if </span><span class="s1">n_classes &lt; </span><span class="s5">2</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;The number of classes has to be greater than one; got %d class&quot;</span>
                <span class="s1">% (n_classes)</span>
            <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">self.priors </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self.priors_ = np.bincount(y) / float(n_samples)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.priors_ = np.array(self.priors)</span>

        <span class="s1">cov = </span><span class="s3">None</span>
        <span class="s1">store_covariance = self.store_covariance</span>
        <span class="s3">if </span><span class="s1">store_covariance:</span>
            <span class="s1">cov = []</span>
        <span class="s1">means = []</span>
        <span class="s1">scalings = []</span>
        <span class="s1">rotations = []</span>
        <span class="s3">for </span><span class="s1">ind </span><span class="s3">in </span><span class="s1">range(n_classes):</span>
            <span class="s1">Xg = X[y == ind</span><span class="s3">, </span><span class="s1">:]</span>
            <span class="s1">meang = Xg.mean(</span><span class="s5">0</span><span class="s1">)</span>
            <span class="s1">means.append(meang)</span>
            <span class="s3">if </span><span class="s1">len(Xg) == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;y has only 1 sample in class %s, covariance is ill defined.&quot;</span>
                    <span class="s1">% str(self.classes_[ind])</span>
                <span class="s1">)</span>
            <span class="s1">Xgc = Xg - meang</span>
            <span class="s2"># Xgc = U * S * V.T</span>
            <span class="s1">_</span><span class="s3">, </span><span class="s1">S</span><span class="s3">, </span><span class="s1">Vt = np.linalg.svd(Xgc</span><span class="s3">, </span><span class="s1">full_matrices=</span><span class="s3">False</span><span class="s1">)</span>
            <span class="s1">rank = np.sum(S &gt; self.tol)</span>
            <span class="s3">if </span><span class="s1">rank &lt; n_features:</span>
                <span class="s1">warnings.warn(</span><span class="s4">&quot;Variables are collinear&quot;</span><span class="s1">)</span>
            <span class="s1">S2 = (S**</span><span class="s5">2</span><span class="s1">) / (len(Xg) - </span><span class="s5">1</span><span class="s1">)</span>
            <span class="s1">S2 = ((</span><span class="s5">1 </span><span class="s1">- self.reg_param) * S2) + self.reg_param</span>
            <span class="s3">if </span><span class="s1">self.store_covariance </span><span class="s3">or </span><span class="s1">store_covariance:</span>
                <span class="s2"># cov = V * (S^2 / (n-1)) * V.T</span>
                <span class="s1">cov.append(np.dot(S2 * Vt.T</span><span class="s3">, </span><span class="s1">Vt))</span>
            <span class="s1">scalings.append(S2)</span>
            <span class="s1">rotations.append(Vt.T)</span>
        <span class="s3">if </span><span class="s1">self.store_covariance </span><span class="s3">or </span><span class="s1">store_covariance:</span>
            <span class="s1">self.covariance_ = cov</span>
        <span class="s1">self.means_ = np.asarray(means)</span>
        <span class="s1">self.scalings_ = scalings</span>
        <span class="s1">self.rotations_ = rotations</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">_decision_function(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s2"># return log posterior, see eq (4.12) p. 110 of the ESL.</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">norm2 = []</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(self.classes_)):</span>
            <span class="s1">R = self.rotations_[i]</span>
            <span class="s1">S = self.scalings_[i]</span>
            <span class="s1">Xm = X - self.means_[i]</span>
            <span class="s1">X2 = np.dot(Xm</span><span class="s3">, </span><span class="s1">R * (S ** (-</span><span class="s5">0.5</span><span class="s1">)))</span>
            <span class="s1">norm2.append(np.sum(X2**</span><span class="s5">2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">norm2 = np.array(norm2).T  </span><span class="s2"># shape = [len(X), n_classes]</span>
        <span class="s1">u = np.asarray([np.sum(np.log(s)) </span><span class="s3">for </span><span class="s1">s </span><span class="s3">in </span><span class="s1">self.scalings_])</span>
        <span class="s3">return </span><span class="s1">-</span><span class="s5">0.5 </span><span class="s1">* (norm2 + u) + np.log(self.priors_)</span>

    <span class="s3">def </span><span class="s1">decision_function(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Apply decision function to an array of samples. 
 
        The decision function is equal (up to a constant factor) to the 
        log-posterior of the model, i.e. `log p(y = k | x)`. In a binary 
        classification setting this instead corresponds to the difference 
        `log p(y = 1 | x) - log p(y = 0 | x)`. See :ref:`lda_qda_math`. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Array of samples (test vectors). 
 
        Returns 
        ------- 
        C : ndarray of shape (n_samples,) or (n_samples, n_classes) 
            Decision function values related to each class, per sample. 
            In the two-class case, the shape is (n_samples,), giving the 
            log likelihood ratio of the positive class. 
        &quot;&quot;&quot;</span>
        <span class="s1">dec_func = self._decision_function(X)</span>
        <span class="s2"># handle special case of two classes</span>
        <span class="s3">if </span><span class="s1">len(self.classes_) == </span><span class="s5">2</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">dec_func[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">] - dec_func[:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>
        <span class="s3">return </span><span class="s1">dec_func</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Perform classification on an array of test vectors X. 
 
        The predicted class C for each sample in X is returned. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Vector to be scored, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
 
        Returns 
        ------- 
        C : ndarray of shape (n_samples,) 
            Estimated probabilities. 
        &quot;&quot;&quot;</span>
        <span class="s1">d = self._decision_function(X)</span>
        <span class="s1">y_pred = self.classes_.take(d.argmax(</span><span class="s5">1</span><span class="s1">))</span>
        <span class="s3">return </span><span class="s1">y_pred</span>

    <span class="s3">def </span><span class="s1">predict_proba(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Return posterior probabilities of classification. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Array of samples/test vectors. 
 
        Returns 
        ------- 
        C : ndarray of shape (n_samples, n_classes) 
            Posterior probabilities of classification per class. 
        &quot;&quot;&quot;</span>
        <span class="s1">values = self._decision_function(X)</span>
        <span class="s2"># compute the likelihood of the underlying gaussian models</span>
        <span class="s2"># up to a multiplicative constant.</span>
        <span class="s1">likelihood = np.exp(values - values.max(axis=</span><span class="s5">1</span><span class="s1">)[:</span><span class="s3">, </span><span class="s1">np.newaxis])</span>
        <span class="s2"># compute posterior probabilities</span>
        <span class="s3">return </span><span class="s1">likelihood / likelihood.sum(axis=</span><span class="s5">1</span><span class="s1">)[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>

    <span class="s3">def </span><span class="s1">predict_log_proba(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Return log of posterior probabilities of classification. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Array of samples/test vectors. 
 
        Returns 
        ------- 
        C : ndarray of shape (n_samples, n_classes) 
            Posterior log-probabilities of classification per class. 
        &quot;&quot;&quot;</span>
        <span class="s2"># XXX : can do better to avoid precision overflows</span>
        <span class="s1">probas_ = self.predict_proba(X)</span>
        <span class="s3">return </span><span class="s1">np.log(probas_)</span>
</pre>
</body>
</html>