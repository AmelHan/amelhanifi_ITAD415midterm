<html>
<head>
<title>_bounds.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_bounds.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Determination of parameter bounds&quot;&quot;&quot;</span>
<span class="s2"># Author: Paolo Losi</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">..preprocessing </span><span class="s3">import </span><span class="s1">LabelBinarizer</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">StrOptions</span><span class="s3">, </span><span class="s1">validate_params</span>
<span class="s3">from </span><span class="s1">..utils.extmath </span><span class="s3">import </span><span class="s1">safe_sparse_dot</span>
<span class="s3">from </span><span class="s1">..utils.validation </span><span class="s3">import </span><span class="s1">check_array</span><span class="s3">, </span><span class="s1">check_consistent_length</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s4">&quot;X&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, </span><span class="s4">&quot;sparse matrix&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;y&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;loss&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;squared_hinge&quot;</span><span class="s3">, </span><span class="s4">&quot;log&quot;</span><span class="s1">})]</span><span class="s3">,</span>
        <span class="s4">&quot;fit_intercept&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;intercept_scaling&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)]</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">True,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">l1_min_c(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">loss=</span><span class="s4">&quot;squared_hinge&quot;</span><span class="s3">, </span><span class="s1">fit_intercept=</span><span class="s3">True, </span><span class="s1">intercept_scaling=</span><span class="s5">1.0</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Return the lowest bound for C. 
 
    The lower bound for C is computed such that for C in (l1_min_C, infinity) 
    the model is guaranteed not to be empty. This applies to l1 penalized 
    classifiers, such as LinearSVC with penalty='l1' and 
    linear_model.LogisticRegression with penalty='l1'. 
 
    This value is valid if class_weight parameter in fit() is not set. 
 
    Parameters 
    ---------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) 
        Training vector, where `n_samples` is the number of samples and 
        `n_features` is the number of features. 
 
    y : array-like of shape (n_samples,) 
        Target vector relative to X. 
 
    loss : {'squared_hinge', 'log'}, default='squared_hinge' 
        Specifies the loss function. 
        With 'squared_hinge' it is the squared hinge loss (a.k.a. L2 loss). 
        With 'log' it is the loss of logistic regression models. 
 
    fit_intercept : bool, default=True 
        Specifies if the intercept should be fitted by the model. 
        It must match the fit() method parameter. 
 
    intercept_scaling : float, default=1.0 
        When fit_intercept is True, instance vector x becomes 
        [x, intercept_scaling], 
        i.e. a &quot;synthetic&quot; feature with constant value equals to 
        intercept_scaling is appended to the instance vector. 
        It must match the fit() method parameter. 
 
    Returns 
    ------- 
    l1_min_c : float 
        Minimum value for C. 
    &quot;&quot;&quot;</span>

    <span class="s1">X = check_array(X</span><span class="s3">, </span><span class="s1">accept_sparse=</span><span class="s4">&quot;csc&quot;</span><span class="s1">)</span>
    <span class="s1">check_consistent_length(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">Y = LabelBinarizer(neg_label=-</span><span class="s5">1</span><span class="s1">).fit_transform(y).T</span>
    <span class="s2"># maximum absolute value over classes and features</span>
    <span class="s1">den = np.max(np.abs(safe_sparse_dot(Y</span><span class="s3">, </span><span class="s1">X)))</span>
    <span class="s3">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">bias = np.full(</span>
            <span class="s1">(np.size(y)</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">intercept_scaling</span><span class="s3">, </span><span class="s1">dtype=np.array(intercept_scaling).dtype</span>
        <span class="s1">)</span>
        <span class="s1">den = max(den</span><span class="s3">, </span><span class="s1">abs(np.dot(Y</span><span class="s3">, </span><span class="s1">bias)).max())</span>

    <span class="s3">if </span><span class="s1">den == </span><span class="s5">0.0</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;Ill-posed l1_min_c calculation: l1 will always &quot;</span>
            <span class="s4">&quot;select zero coefficients for this data&quot;</span>
        <span class="s1">)</span>
    <span class="s3">if </span><span class="s1">loss == </span><span class="s4">&quot;squared_hinge&quot;</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s5">0.5 </span><span class="s1">/ den</span>
    <span class="s3">else</span><span class="s1">:  </span><span class="s2"># loss == 'log':</span>
        <span class="s3">return </span><span class="s5">2.0 </span><span class="s1">/ den</span>
</pre>
</body>
</html>