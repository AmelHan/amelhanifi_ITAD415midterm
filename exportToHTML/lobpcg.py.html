<html>
<head>
<title>lobpcg.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
lobpcg.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Locally Optimal Block Preconditioned Conjugate Gradient Method (LOBPCG). 
 
References 
---------- 
.. [1] A. V. Knyazev (2001), 
       Toward the Optimal Preconditioned Eigensolver: Locally Optimal 
       Block Preconditioned Conjugate Gradient Method. 
       SIAM Journal on Scientific Computing 23, no. 2, 
       pp. 517-541. :doi:`10.1137/S1064827500366124` 
 
.. [2] A. V. Knyazev, I. Lashuk, M. E. Argentati, and E. Ovchinnikov (2007), 
       Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) 
       in hypre and PETSc.  :arxiv:`0705.2626` 
 
.. [3] A. V. Knyazev's C and MATLAB implementations: 
       https://github.com/lobpcg/blopex 
&quot;&quot;&quot;</span>

<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy.linalg </span><span class="s2">import </span><span class="s1">(inv</span><span class="s2">, </span><span class="s1">eigh</span><span class="s2">, </span><span class="s1">cho_factor</span><span class="s2">, </span><span class="s1">cho_solve</span><span class="s2">,</span>
                          <span class="s1">cholesky</span><span class="s2">, </span><span class="s1">LinAlgError)</span>
<span class="s2">from </span><span class="s1">scipy.sparse.linalg </span><span class="s2">import </span><span class="s1">LinearOperator</span>
<span class="s2">from </span><span class="s1">scipy.sparse </span><span class="s2">import </span><span class="s1">issparse</span>

<span class="s1">__all__ = [</span><span class="s3">&quot;lobpcg&quot;</span><span class="s1">]</span>


<span class="s2">def </span><span class="s1">_report_nonhermitian(M</span><span class="s2">, </span><span class="s1">name):</span>
    <span class="s0">&quot;&quot;&quot; 
    Report if `M` is not a Hermitian matrix given its type. 
    &quot;&quot;&quot;</span>
    <span class="s2">from </span><span class="s1">scipy.linalg </span><span class="s2">import </span><span class="s1">norm</span>

    <span class="s1">md = M - M.T.conj()</span>
    <span class="s1">nmd = norm(md</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">tol = </span><span class="s4">10 </span><span class="s1">* np.finfo(M.dtype).eps</span>
    <span class="s1">tol = max(tol</span><span class="s2">, </span><span class="s1">tol * norm(M</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s2">if </span><span class="s1">nmd &gt; tol:</span>
        <span class="s1">warnings.warn(</span>
              <span class="s3">f&quot;Matrix </span><span class="s2">{</span><span class="s1">name</span><span class="s2">} </span><span class="s3">of the type </span><span class="s2">{</span><span class="s1">M.dtype</span><span class="s2">} </span><span class="s3">is not Hermitian: &quot;</span>
              <span class="s3">f&quot;condition: </span><span class="s2">{</span><span class="s1">nmd</span><span class="s2">} </span><span class="s3">&lt; </span><span class="s2">{</span><span class="s1">tol</span><span class="s2">} </span><span class="s3">fails.&quot;</span><span class="s2">,</span>
              <span class="s1">UserWarning</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">4</span>
         <span class="s1">)</span>

<span class="s2">def </span><span class="s1">_as2d(ar):</span>
    <span class="s0">&quot;&quot;&quot; 
    If the input array is 2D return it, if it is 1D, append a dimension, 
    making it a column vector. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">ar.ndim == </span><span class="s4">2</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">ar</span>
    <span class="s2">else</span><span class="s1">:  </span><span class="s5"># Assume 1!</span>
        <span class="s1">aux = np.array(ar</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">aux.shape = (ar.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">aux</span>


<span class="s2">def </span><span class="s1">_makeMatMat(m):</span>
    <span class="s2">if </span><span class="s1">m </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s2">return None</span>
    <span class="s2">elif </span><span class="s1">callable(m):</span>
        <span class="s2">return lambda </span><span class="s1">v: m(v)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">return lambda </span><span class="s1">v: m @ v</span>


<span class="s2">def </span><span class="s1">_matmul_inplace(x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">verbosityLevel=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Perform 'np.matmul' in-place if possible. 
 
    If some sufficient conditions for inplace matmul are met, do so. 
    Otherwise try inplace update and fall back to overwrite if that fails. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">x.flags[</span><span class="s3">&quot;CARRAY&quot;</span><span class="s1">] </span><span class="s2">and </span><span class="s1">x.shape[</span><span class="s4">1</span><span class="s1">] == y.shape[</span><span class="s4">1</span><span class="s1">] </span><span class="s2">and </span><span class="s1">x.dtype == y.dtype:</span>
        <span class="s5"># conditions where we can guarantee that inplace updates will work;</span>
        <span class="s5"># i.e. x is not a view/slice, x &amp; y have compatible dtypes, and the</span>
        <span class="s5"># shape of the result of x @ y matches the shape of x.</span>
        <span class="s1">np.matmul(x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">out=x)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s5"># ideally, we'd have an exhaustive list of conditions above when</span>
        <span class="s5"># inplace updates are possible; since we don't, we opportunistically</span>
        <span class="s5"># try if it works, and fall back to overwriting if necessary</span>
        <span class="s2">try</span><span class="s1">:</span>
            <span class="s1">np.matmul(x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">out=x)</span>
        <span class="s2">except </span><span class="s1">Exception:</span>
            <span class="s2">if </span><span class="s1">verbosityLevel:</span>
                <span class="s1">warnings.warn(</span>
                    <span class="s3">&quot;Inplace update of x = x @ y failed, &quot;</span>
                    <span class="s3">&quot;x needs to be overwritten.&quot;</span><span class="s2">,</span>
                    <span class="s1">UserWarning</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">3</span>
                <span class="s1">)</span>
            <span class="s1">x = x @ y</span>
    <span class="s2">return </span><span class="s1">x</span>


<span class="s2">def </span><span class="s1">_applyConstraints(blockVectorV</span><span class="s2">, </span><span class="s1">factYBY</span><span class="s2">, </span><span class="s1">blockVectorBY</span><span class="s2">, </span><span class="s1">blockVectorY):</span>
    <span class="s0">&quot;&quot;&quot;Changes blockVectorV in-place.&quot;&quot;&quot;</span>
    <span class="s1">YBV = blockVectorBY.T.conj() @ blockVectorV</span>
    <span class="s1">tmp = cho_solve(factYBY</span><span class="s2">, </span><span class="s1">YBV)</span>
    <span class="s1">blockVectorV -= blockVectorY @ tmp</span>


<span class="s2">def </span><span class="s1">_b_orthonormalize(B</span><span class="s2">, </span><span class="s1">blockVectorV</span><span class="s2">, </span><span class="s1">blockVectorBV=</span><span class="s2">None,</span>
                      <span class="s1">verbosityLevel=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;in-place B-orthonormalize the given block vector using Cholesky.&quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">blockVectorBV </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s2">if </span><span class="s1">B </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">blockVectorBV = blockVectorV</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">try</span><span class="s1">:</span>
                <span class="s1">blockVectorBV = B(blockVectorV)</span>
            <span class="s2">except </span><span class="s1">Exception </span><span class="s2">as </span><span class="s1">e:</span>
                <span class="s2">if </span><span class="s1">verbosityLevel:</span>
                    <span class="s1">warnings.warn(</span>
                        <span class="s3">f&quot;Secondary MatMul call failed with error</span><span class="s2">\n</span><span class="s3">&quot;</span>
                        <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">e</span><span class="s2">}\n</span><span class="s3">&quot;</span><span class="s2">,</span>
                        <span class="s1">UserWarning</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">3</span>
                    <span class="s1">)</span>
                    <span class="s2">return None, None, None</span>
            <span class="s2">if </span><span class="s1">blockVectorBV.shape != blockVectorV.shape:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s3">f&quot;The shape </span><span class="s2">{</span><span class="s1">blockVectorV.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
                    <span class="s3">f&quot;of the orthogonalized matrix not preserved</span><span class="s2">\n</span><span class="s3">&quot;</span>
                    <span class="s3">f&quot;and changed to </span><span class="s2">{</span><span class="s1">blockVectorBV.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
                    <span class="s3">f&quot;after multiplying by the secondary matrix.</span><span class="s2">\n</span><span class="s3">&quot;</span>
                <span class="s1">)</span>

    <span class="s1">VBV = blockVectorV.T.conj() @ blockVectorBV</span>
    <span class="s2">try</span><span class="s1">:</span>
        <span class="s5"># VBV is a Cholesky factor from now on...</span>
        <span class="s1">VBV = cholesky(VBV</span><span class="s2">, </span><span class="s1">overwrite_a=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">VBV = inv(VBV</span><span class="s2">, </span><span class="s1">overwrite_a=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">blockVectorV = _matmul_inplace(</span>
            <span class="s1">blockVectorV</span><span class="s2">, </span><span class="s1">VBV</span><span class="s2">,</span>
            <span class="s1">verbosityLevel=verbosityLevel</span>
        <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">B </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">blockVectorBV = _matmul_inplace(</span>
                <span class="s1">blockVectorBV</span><span class="s2">, </span><span class="s1">VBV</span><span class="s2">,</span>
                <span class="s1">verbosityLevel=verbosityLevel</span>
            <span class="s1">)</span>
        <span class="s2">return </span><span class="s1">blockVectorV</span><span class="s2">, </span><span class="s1">blockVectorBV</span><span class="s2">, </span><span class="s1">VBV</span>
    <span class="s2">except </span><span class="s1">LinAlgError:</span>
        <span class="s2">if </span><span class="s1">verbosityLevel:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s3">&quot;Cholesky has failed.&quot;</span><span class="s2">,</span>
                <span class="s1">UserWarning</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">3</span>
            <span class="s1">)</span>
        <span class="s2">return None, None, None</span>


<span class="s2">def </span><span class="s1">_get_indx(_lambda</span><span class="s2">, </span><span class="s1">num</span><span class="s2">, </span><span class="s1">largest):</span>
    <span class="s0">&quot;&quot;&quot;Get `num` indices into `_lambda` depending on `largest` option.&quot;&quot;&quot;</span>
    <span class="s1">ii = np.argsort(_lambda)</span>
    <span class="s2">if </span><span class="s1">largest:</span>
        <span class="s1">ii = ii[:-num - </span><span class="s4">1</span><span class="s1">:-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">ii = ii[:num]</span>

    <span class="s2">return </span><span class="s1">ii</span>


<span class="s2">def </span><span class="s1">_handle_gramA_gramB_verbosity(gramA</span><span class="s2">, </span><span class="s1">gramB</span><span class="s2">, </span><span class="s1">verbosityLevel):</span>
    <span class="s2">if </span><span class="s1">verbosityLevel:</span>
        <span class="s1">_report_nonhermitian(gramA</span><span class="s2">, </span><span class="s3">&quot;gramA&quot;</span><span class="s1">)</span>
        <span class="s1">_report_nonhermitian(gramB</span><span class="s2">, </span><span class="s3">&quot;gramB&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">lobpcg(</span>
    <span class="s1">A</span><span class="s2">,</span>
    <span class="s1">X</span><span class="s2">,</span>
    <span class="s1">B=</span><span class="s2">None,</span>
    <span class="s1">M=</span><span class="s2">None,</span>
    <span class="s1">Y=</span><span class="s2">None,</span>
    <span class="s1">tol=</span><span class="s2">None,</span>
    <span class="s1">maxiter=</span><span class="s2">None,</span>
    <span class="s1">largest=</span><span class="s2">True,</span>
    <span class="s1">verbosityLevel=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">retLambdaHistory=</span><span class="s2">False,</span>
    <span class="s1">retResidualNormsHistory=</span><span class="s2">False,</span>
    <span class="s1">restartControl=</span><span class="s4">20</span><span class="s2">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Locally Optimal Block Preconditioned Conjugate Gradient Method (LOBPCG). 
 
    LOBPCG is a preconditioned eigensolver for large real symmetric and complex 
    Hermitian definite generalized eigenproblems. 
 
    Parameters 
    ---------- 
    A : {sparse matrix, ndarray, LinearOperator, callable object} 
        The Hermitian linear operator of the problem, usually given by a 
        sparse matrix.  Often called the &quot;stiffness matrix&quot;. 
    X : ndarray, float32 or float64 
        Initial approximation to the ``k`` eigenvectors (non-sparse). 
        If `A` has ``shape=(n,n)`` then `X` must have ``shape=(n,k)``. 
    B : {sparse matrix, ndarray, LinearOperator, callable object} 
        Optional. By default ``B = None``, which is equivalent to identity. 
        The right hand side operator in a generalized eigenproblem if present. 
        Often called the &quot;mass matrix&quot;. Must be Hermitian positive definite. 
    M : {sparse matrix, ndarray, LinearOperator, callable object} 
        Optional. By default ``M = None``, which is equivalent to identity. 
        Preconditioner aiming to accelerate convergence. 
    Y : ndarray, float32 or float64, default: None 
        An ``n-by-sizeY`` ndarray of constraints with ``sizeY &lt; n``. 
        The iterations will be performed in the ``B``-orthogonal complement 
        of the column-space of `Y`. `Y` must be full rank if present. 
    tol : scalar, optional 
        The default is ``tol=n*sqrt(eps)``. 
        Solver tolerance for the stopping criterion. 
    maxiter : int, default: 20 
        Maximum number of iterations. 
    largest : bool, default: True 
        When True, solve for the largest eigenvalues, otherwise the smallest. 
    verbosityLevel : int, optional 
        By default ``verbosityLevel=0`` no output. 
        Controls the solver standard/screen output. 
    retLambdaHistory : bool, default: False 
        Whether to return iterative eigenvalue history. 
    retResidualNormsHistory : bool, default: False 
        Whether to return iterative history of residual norms. 
    restartControl : int, optional. 
        Iterations restart if the residuals jump ``2**restartControl`` times 
        compared to the smallest recorded in ``retResidualNormsHistory``. 
        The default is ``restartControl=20``, making the restarts rare for 
        backward compatibility. 
 
    Returns 
    ------- 
    lambda : ndarray of the shape ``(k, )``. 
        Array of ``k`` approximate eigenvalues. 
    v : ndarray of the same shape as ``X.shape``. 
        An array of ``k`` approximate eigenvectors. 
    lambdaHistory : ndarray, optional. 
        The eigenvalue history, if `retLambdaHistory` is ``True``. 
    ResidualNormsHistory : ndarray, optional. 
        The history of residual norms, if `retResidualNormsHistory` 
        is ``True``. 
 
    Notes 
    ----- 
    The iterative loop runs ``maxit=maxiter`` (20 if ``maxit=None``) 
    iterations at most and finishes earler if the tolerance is met. 
    Breaking backward compatibility with the previous version, LOBPCG 
    now returns the block of iterative vectors with the best accuracy rather 
    than the last one iterated, as a cure for possible divergence. 
 
    If ``X.dtype == np.float32`` and user-provided operations/multiplications 
    by `A`, `B`, and `M` all preserve the ``np.float32`` data type, 
    all the calculations and the output are in ``np.float32``. 
 
    The size of the iteration history output equals to the number of the best 
    (limited by `maxit`) iterations plus 3: initial, final, and postprocessing. 
 
    If both `retLambdaHistory` and `retResidualNormsHistory` are ``True``, 
    the return tuple has the following format 
    ``(lambda, V, lambda history, residual norms history)``. 
 
    In the following ``n`` denotes the matrix size and ``k`` the number 
    of required eigenvalues (smallest or largest). 
 
    The LOBPCG code internally solves eigenproblems of the size ``3k`` on every 
    iteration by calling the dense eigensolver `eigh`, so if ``k`` is not 
    small enough compared to ``n``, it makes no sense to call the LOBPCG code. 
    Moreover, if one calls the LOBPCG algorithm for ``5k &gt; n``, it would likely 
    break internally, so the code calls the standard function `eigh` instead. 
    It is not that ``n`` should be large for the LOBPCG to work, but rather the 
    ratio ``n / k`` should be large. It you call LOBPCG with ``k=1`` 
    and ``n=10``, it works though ``n`` is small. The method is intended 
    for extremely large ``n / k``. 
 
    The convergence speed depends basically on three factors: 
 
    1. Quality of the initial approximations `X` to the seeking eigenvectors. 
       Randomly distributed around the origin vectors work well if no better 
       choice is known. 
 
    2. Relative separation of the desired eigenvalues from the rest 
       of the eigenvalues. One can vary ``k`` to improve the separation. 
 
    3. Proper preconditioning to shrink the spectral spread. 
       For example, a rod vibration test problem (under tests 
       directory) is ill-conditioned for large ``n``, so convergence will be 
       slow, unless efficient preconditioning is used. For this specific 
       problem, a good simple preconditioner function would be a linear solve 
       for `A`, which is easy to code since `A` is tridiagonal. 
 
    References 
    ---------- 
    .. [1] A. V. Knyazev (2001), 
           Toward the Optimal Preconditioned Eigensolver: Locally Optimal 
           Block Preconditioned Conjugate Gradient Method. 
           SIAM Journal on Scientific Computing 23, no. 2, 
           pp. 517-541. :doi:`10.1137/S1064827500366124` 
 
    .. [2] A. V. Knyazev, I. Lashuk, M. E. Argentati, and E. Ovchinnikov 
           (2007), Block Locally Optimal Preconditioned Eigenvalue Xolvers 
           (BLOPEX) in hypre and PETSc. :arxiv:`0705.2626` 
 
    .. [3] A. V. Knyazev's C and MATLAB implementations: 
           https://github.com/lobpcg/blopex 
 
    Examples 
    -------- 
    Our first example is minimalistic - find the largest eigenvalue of 
    a diagonal matrix by solving the non-generalized eigenvalue problem 
    ``A x = lambda x`` without constraints or preconditioning. 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.sparse import spdiags 
    &gt;&gt;&gt; from scipy.sparse.linalg import LinearOperator, aslinearoperator 
    &gt;&gt;&gt; from scipy.sparse.linalg import lobpcg 
 
    The square matrix size is 
 
    &gt;&gt;&gt; n = 100 
 
    and its diagonal entries are 1, ..., 100 defined by 
 
    &gt;&gt;&gt; vals = np.arange(1, n + 1).astype(np.int16) 
 
    The first mandatory input parameter in this test is 
    the sparse diagonal matrix `A` 
    of the eigenvalue problem ``A x = lambda x`` to solve. 
 
    &gt;&gt;&gt; A = spdiags(vals, 0, n, n) 
    &gt;&gt;&gt; A = A.astype(np.int16) 
    &gt;&gt;&gt; A.toarray() 
    array([[  1,   0,   0, ...,   0,   0,   0], 
           [  0,   2,   0, ...,   0,   0,   0], 
           [  0,   0,   3, ...,   0,   0,   0], 
           ..., 
           [  0,   0,   0, ...,  98,   0,   0], 
           [  0,   0,   0, ...,   0,  99,   0], 
           [  0,   0,   0, ...,   0,   0, 100]], dtype=int16) 
 
    The second mandatory input parameter `X` is a 2D array with the 
    row dimension determining the number of requested eigenvalues. 
    `X` is an initial guess for targeted eigenvectors. 
    `X` must have linearly independent columns. 
    If no initial approximations available, randomly oriented vectors 
    commonly work best, e.g., with components normally distributed 
    around zero or uniformly distributed on the interval [-1 1]. 
    Setting the initial approximations to dtype ``np.float32`` 
    forces all iterative values to dtype ``np.float32`` speeding up 
    the run while still allowing accurate eigenvalue computations. 
 
    &gt;&gt;&gt; k = 1 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; X = rng.normal(size=(n, k)) 
    &gt;&gt;&gt; X = X.astype(np.float32) 
 
    &gt;&gt;&gt; eigenvalues, _ = lobpcg(A, X, maxiter=60) 
    &gt;&gt;&gt; eigenvalues 
    array([100.]) 
    &gt;&gt;&gt; eigenvalues.dtype 
    dtype('float32') 
 
    LOBPCG needs only access the matrix product with `A` rather 
    then the matrix itself. Since the matrix `A` is diagonal in 
    this example, one can write a function of the product 
    ``A @ X`` using the diagonal values ``vals`` only, e.g., by 
    element-wise multiplication with broadcasting 
 
    &gt;&gt;&gt; A_f = lambda X: vals[:, np.newaxis] * X 
 
    and use the handle ``A_f`` to this callable function as an input 
 
    &gt;&gt;&gt; eigenvalues, _ = lobpcg(A_f, X, maxiter=60) 
    &gt;&gt;&gt; eigenvalues 
    array([100.]) 
 
    The next example illustrates computing 3 smallest eigenvalues of 
    the same matrix given by the function handle ``A_f`` with 
    constraints and preconditioning. 
 
    &gt;&gt;&gt; k = 3 
    &gt;&gt;&gt; X = rng.normal(size=(n, k)) 
 
    Constraints - an optional input parameter is a 2D array comprising 
    of column vectors that the eigenvectors must be orthogonal to 
 
    &gt;&gt;&gt; Y = np.eye(n, 3) 
 
    The preconditioner acts as the inverse of `A` in this example, but 
    in the reduced precision ``np.float32`` even though the initial `X` 
    and thus all iterates and the output are in full ``np.float64``. 
 
    &gt;&gt;&gt; inv_vals = 1./vals 
    &gt;&gt;&gt; inv_vals = inv_vals.astype(np.float32) 
    &gt;&gt;&gt; M = lambda X: inv_vals[:, np.newaxis] * X 
 
    Let us now solve the eigenvalue problem for the matrix `A` first 
    without preconditioning requesting 80 iterations 
 
    &gt;&gt;&gt; eigenvalues, _ = lobpcg(A_f, X, Y=Y, largest=False, maxiter=80) 
    &gt;&gt;&gt; eigenvalues 
    array([4., 5., 6.]) 
    &gt;&gt;&gt; eigenvalues.dtype 
    dtype('float64') 
 
    With preconditioning we need only 20 iterations from the same `X` 
 
    &gt;&gt;&gt; eigenvalues, _ = lobpcg(A_f, X, Y=Y, M=M, largest=False, maxiter=20) 
    &gt;&gt;&gt; eigenvalues 
    array([4., 5., 6.]) 
 
    Note that the vectors passed in `Y` are the eigenvectors of the 3 
    smallest eigenvalues. The results returned above are orthogonal to those. 
 
    Finally, the primary matrix `A` may be indefinite, e.g., after shifting 
    ``vals`` by 50 from 1, ..., 100 to -49, ..., 50, we still can compute 
    the 3 smallest or largest eigenvalues. 
 
    &gt;&gt;&gt; vals = vals - 50 
    &gt;&gt;&gt; X = rng.normal(size=(n, k)) 
    &gt;&gt;&gt; eigenvalues, _ = lobpcg(A_f, X, largest=False, maxiter=99) 
    &gt;&gt;&gt; eigenvalues 
    array([-49., -48., -47.]) 
    &gt;&gt;&gt; eigenvalues, _ = lobpcg(A_f, X, largest=True, maxiter=99) 
    &gt;&gt;&gt; eigenvalues 
    array([50., 49., 48.]) 
 
    &quot;&quot;&quot;</span>
    <span class="s1">blockVectorX = X</span>
    <span class="s1">bestblockVectorX = blockVectorX</span>
    <span class="s1">blockVectorY = Y</span>
    <span class="s1">residualTolerance = tol</span>
    <span class="s2">if </span><span class="s1">maxiter </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">maxiter = </span><span class="s4">20</span>

    <span class="s1">bestIterationNumber = maxiter</span>

    <span class="s1">sizeY = </span><span class="s4">0</span>
    <span class="s2">if </span><span class="s1">blockVectorY </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s2">if </span><span class="s1">len(blockVectorY.shape) != </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s3">f&quot;Expected rank-2 array for argument Y, instead got &quot;</span>
                <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">len(blockVectorY.shape)</span><span class="s2">}</span><span class="s3">, &quot;</span>
                <span class="s3">f&quot;so ignore it and use no constraints.&quot;</span><span class="s2">,</span>
                <span class="s1">UserWarning</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">2</span>
            <span class="s1">)</span>
            <span class="s1">blockVectorY = </span><span class="s2">None</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">sizeY = blockVectorY.shape[</span><span class="s4">1</span><span class="s1">]</span>

    <span class="s5"># Block size.</span>
    <span class="s2">if </span><span class="s1">blockVectorX </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;The mandatory initial matrix X cannot be None&quot;</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">len(blockVectorX.shape) != </span><span class="s4">2</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;expected rank-2 array for argument X&quot;</span><span class="s1">)</span>

    <span class="s1">n</span><span class="s2">, </span><span class="s1">sizeX = blockVectorX.shape</span>

    <span class="s5"># Data type of iterates, determined by X, must be inexact</span>
    <span class="s2">if not </span><span class="s1">np.issubdtype(blockVectorX.dtype</span><span class="s2">, </span><span class="s1">np.inexact):</span>
        <span class="s1">warnings.warn(</span>
            <span class="s3">f&quot;Data type for argument X is </span><span class="s2">{</span><span class="s1">blockVectorX.dtype</span><span class="s2">}</span><span class="s3">, &quot;</span>
            <span class="s3">f&quot;which is not inexact, so casted to np.float32.&quot;</span><span class="s2">,</span>
            <span class="s1">UserWarning</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">2</span>
        <span class="s1">)</span>
        <span class="s1">blockVectorX = np.asarray(blockVectorX</span><span class="s2">, </span><span class="s1">dtype=np.float32)</span>

    <span class="s2">if </span><span class="s1">retLambdaHistory:</span>
        <span class="s1">lambdaHistory = np.zeros((maxiter + </span><span class="s4">3</span><span class="s2">, </span><span class="s1">sizeX)</span><span class="s2">,</span>
                                 <span class="s1">dtype=blockVectorX.dtype)</span>
    <span class="s2">if </span><span class="s1">retResidualNormsHistory:</span>
        <span class="s1">residualNormsHistory = np.zeros((maxiter + </span><span class="s4">3</span><span class="s2">, </span><span class="s1">sizeX)</span><span class="s2">,</span>
                                        <span class="s1">dtype=blockVectorX.dtype)</span>

    <span class="s2">if </span><span class="s1">verbosityLevel:</span>
        <span class="s1">aux = </span><span class="s3">&quot;Solving &quot;</span>
        <span class="s2">if </span><span class="s1">B </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">aux += </span><span class="s3">&quot;standard&quot;</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">aux += </span><span class="s3">&quot;generalized&quot;</span>
        <span class="s1">aux += </span><span class="s3">&quot; eigenvalue problem with&quot;</span>
        <span class="s2">if </span><span class="s1">M </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">aux += </span><span class="s3">&quot;out&quot;</span>
        <span class="s1">aux += </span><span class="s3">&quot; preconditioning</span><span class="s2">\n\n</span><span class="s3">&quot;</span>
        <span class="s1">aux += </span><span class="s3">&quot;matrix size %d</span><span class="s2">\n</span><span class="s3">&quot; </span><span class="s1">% n</span>
        <span class="s1">aux += </span><span class="s3">&quot;block size %d</span><span class="s2">\n\n</span><span class="s3">&quot; </span><span class="s1">% sizeX</span>
        <span class="s2">if </span><span class="s1">blockVectorY </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">aux += </span><span class="s3">&quot;No constraints</span><span class="s2">\n\n</span><span class="s3">&quot;</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">sizeY &gt; </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">aux += </span><span class="s3">&quot;%d constraints</span><span class="s2">\n\n</span><span class="s3">&quot; </span><span class="s1">% sizeY</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">aux += </span><span class="s3">&quot;%d constraint</span><span class="s2">\n\n</span><span class="s3">&quot; </span><span class="s1">% sizeY</span>
        <span class="s1">print(aux)</span>

    <span class="s2">if </span><span class="s1">(n - sizeY) &lt; (</span><span class="s4">5 </span><span class="s1">* sizeX):</span>
        <span class="s1">warnings.warn(</span>
            <span class="s3">f&quot;The problem size </span><span class="s2">{</span><span class="s1">n</span><span class="s2">} </span><span class="s3">minus the constraints size </span><span class="s2">{</span><span class="s1">sizeY</span><span class="s2">} </span><span class="s3">&quot;</span>
            <span class="s3">f&quot;is too small relative to the block size </span><span class="s2">{</span><span class="s1">sizeX</span><span class="s2">}</span><span class="s3">. &quot;</span>
            <span class="s3">f&quot;Using a dense eigensolver instead of LOBPCG iterations.&quot;</span>
            <span class="s3">f&quot;No output of the history of the iterations.&quot;</span><span class="s2">,</span>
            <span class="s1">UserWarning</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">2</span>
        <span class="s1">)</span>

        <span class="s1">sizeX = min(sizeX</span><span class="s2">, </span><span class="s1">n)</span>

        <span class="s2">if </span><span class="s1">blockVectorY </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">NotImplementedError(</span>
                <span class="s3">&quot;The dense eigensolver does not support constraints.&quot;</span>
            <span class="s1">)</span>

        <span class="s5"># Define the closed range of indices of eigenvalues to return.</span>
        <span class="s2">if </span><span class="s1">largest:</span>
            <span class="s1">eigvals = (n - sizeX</span><span class="s2">, </span><span class="s1">n - </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">eigvals = (</span><span class="s4">0</span><span class="s2">, </span><span class="s1">sizeX - </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s2">try</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">isinstance(A</span><span class="s2">, </span><span class="s1">LinearOperator):</span>
                <span class="s1">A = A(np.eye(n</span><span class="s2">, </span><span class="s1">dtype=int))</span>
            <span class="s2">elif </span><span class="s1">callable(A):</span>
                <span class="s1">A = A(np.eye(n</span><span class="s2">, </span><span class="s1">dtype=int))</span>
                <span class="s2">if </span><span class="s1">A.shape != (n</span><span class="s2">, </span><span class="s1">n):</span>
                    <span class="s2">raise </span><span class="s1">ValueError(</span>
                        <span class="s3">f&quot;The shape </span><span class="s2">{</span><span class="s1">A.shape</span><span class="s2">} </span><span class="s3">of the primary matrix</span><span class="s2">\n</span><span class="s3">&quot;</span>
                        <span class="s3">f&quot;defined by a callable object is wrong.</span><span class="s2">\n</span><span class="s3">&quot;</span>
                    <span class="s1">)</span>
            <span class="s2">elif </span><span class="s1">issparse(A):</span>
                <span class="s1">A = A.toarray()</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">A = np.asarray(A)</span>
        <span class="s2">except </span><span class="s1">Exception </span><span class="s2">as </span><span class="s1">e:</span>
            <span class="s2">raise </span><span class="s1">Exception(</span>
                <span class="s3">f&quot;Primary MatMul call failed with error</span><span class="s2">\n</span><span class="s3">&quot;</span>
                <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">e</span><span class="s2">}\n</span><span class="s3">&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">B </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">try</span><span class="s1">:</span>
                <span class="s2">if </span><span class="s1">isinstance(B</span><span class="s2">, </span><span class="s1">LinearOperator):</span>
                    <span class="s1">B = B(np.eye(n</span><span class="s2">, </span><span class="s1">dtype=int))</span>
                <span class="s2">elif </span><span class="s1">callable(B):</span>
                    <span class="s1">B = B(np.eye(n</span><span class="s2">, </span><span class="s1">dtype=int))</span>
                    <span class="s2">if </span><span class="s1">B.shape != (n</span><span class="s2">, </span><span class="s1">n):</span>
                        <span class="s2">raise </span><span class="s1">ValueError(</span>
                            <span class="s3">f&quot;The shape </span><span class="s2">{</span><span class="s1">B.shape</span><span class="s2">} </span><span class="s3">of the secondary matrix</span><span class="s2">\n</span><span class="s3">&quot;</span>
                            <span class="s3">f&quot;defined by a callable object is wrong.</span><span class="s2">\n</span><span class="s3">&quot;</span>
                        <span class="s1">)</span>
                <span class="s2">elif </span><span class="s1">issparse(B):</span>
                    <span class="s1">B = B.toarray()</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">B = np.asarray(B)</span>
            <span class="s2">except </span><span class="s1">Exception </span><span class="s2">as </span><span class="s1">e:</span>
                <span class="s2">raise </span><span class="s1">Exception(</span>
                    <span class="s3">f&quot;Secondary MatMul call failed with error</span><span class="s2">\n</span><span class="s3">&quot;</span>
                    <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">e</span><span class="s2">}\n</span><span class="s3">&quot;</span><span class="s1">)</span>

        <span class="s2">try</span><span class="s1">:</span>
            <span class="s1">vals</span><span class="s2">, </span><span class="s1">vecs = eigh(A</span><span class="s2">,</span>
                              <span class="s1">B</span><span class="s2">,</span>
                              <span class="s1">subset_by_index=eigvals</span><span class="s2">,</span>
                              <span class="s1">check_finite=</span><span class="s2">False</span><span class="s1">)</span>
            <span class="s2">if </span><span class="s1">largest:</span>
                <span class="s5"># Reverse order to be compatible with eigs() in 'LM' mode.</span>
                <span class="s1">vals = vals[::-</span><span class="s4">1</span><span class="s1">]</span>
                <span class="s1">vecs = vecs[:</span><span class="s2">, </span><span class="s1">::-</span><span class="s4">1</span><span class="s1">]</span>

            <span class="s2">return </span><span class="s1">vals</span><span class="s2">, </span><span class="s1">vecs</span>
        <span class="s2">except </span><span class="s1">Exception </span><span class="s2">as </span><span class="s1">e:</span>
            <span class="s2">raise </span><span class="s1">Exception(</span>
                <span class="s3">f&quot;Dense eigensolver failed with error</span><span class="s2">\n</span><span class="s3">&quot;</span>
                <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">e</span><span class="s2">}\n</span><span class="s3">&quot;</span>
            <span class="s1">)</span>

    <span class="s2">if </span><span class="s1">(residualTolerance </span><span class="s2">is None</span><span class="s1">) </span><span class="s2">or </span><span class="s1">(residualTolerance &lt;= </span><span class="s4">0.0</span><span class="s1">):</span>
        <span class="s1">residualTolerance = np.sqrt(np.finfo(blockVectorX.dtype).eps) * n</span>

    <span class="s1">A = _makeMatMat(A)</span>
    <span class="s1">B = _makeMatMat(B)</span>
    <span class="s1">M = _makeMatMat(M)</span>

    <span class="s5"># Apply constraints to X.</span>
    <span class="s2">if </span><span class="s1">blockVectorY </span><span class="s2">is not None</span><span class="s1">:</span>

        <span class="s2">if </span><span class="s1">B </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">blockVectorBY = B(blockVectorY)</span>
            <span class="s2">if </span><span class="s1">blockVectorBY.shape != blockVectorY.shape:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s3">f&quot;The shape </span><span class="s2">{</span><span class="s1">blockVectorY.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
                    <span class="s3">f&quot;of the constraint not preserved</span><span class="s2">\n</span><span class="s3">&quot;</span>
                    <span class="s3">f&quot;and changed to </span><span class="s2">{</span><span class="s1">blockVectorBY.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
                    <span class="s3">f&quot;after multiplying by the secondary matrix.</span><span class="s2">\n</span><span class="s3">&quot;</span>
                <span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">blockVectorBY = blockVectorY</span>

        <span class="s5"># gramYBY is a dense array.</span>
        <span class="s1">gramYBY = blockVectorY.T.conj() @ blockVectorBY</span>
        <span class="s2">try</span><span class="s1">:</span>
            <span class="s5"># gramYBY is a Cholesky factor from now on...</span>
            <span class="s1">gramYBY = cho_factor(gramYBY</span><span class="s2">, </span><span class="s1">overwrite_a=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s2">except </span><span class="s1">LinAlgError </span><span class="s2">as </span><span class="s1">e:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Linearly dependent constraints&quot;</span><span class="s1">) </span><span class="s2">from </span><span class="s1">e</span>

        <span class="s1">_applyConstraints(blockVectorX</span><span class="s2">, </span><span class="s1">gramYBY</span><span class="s2">, </span><span class="s1">blockVectorBY</span><span class="s2">, </span><span class="s1">blockVectorY)</span>

    <span class="s5">##</span>
    <span class="s5"># B-orthonormalize X.</span>
    <span class="s1">blockVectorX</span><span class="s2">, </span><span class="s1">blockVectorBX</span><span class="s2">, </span><span class="s1">_ = _b_orthonormalize(</span>
        <span class="s1">B</span><span class="s2">, </span><span class="s1">blockVectorX</span><span class="s2">, </span><span class="s1">verbosityLevel=verbosityLevel)</span>
    <span class="s2">if </span><span class="s1">blockVectorX </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Linearly dependent initial approximations&quot;</span><span class="s1">)</span>

    <span class="s5">##</span>
    <span class="s5"># Compute the initial Ritz vectors: solve the eigenproblem.</span>
    <span class="s1">blockVectorAX = A(blockVectorX)</span>
    <span class="s2">if </span><span class="s1">blockVectorAX.shape != blockVectorX.shape:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span>
            <span class="s3">f&quot;The shape </span><span class="s2">{</span><span class="s1">blockVectorX.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
            <span class="s3">f&quot;of the initial approximations not preserved</span><span class="s2">\n</span><span class="s3">&quot;</span>
            <span class="s3">f&quot;and changed to </span><span class="s2">{</span><span class="s1">blockVectorAX.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
            <span class="s3">f&quot;after multiplying by the primary matrix.</span><span class="s2">\n</span><span class="s3">&quot;</span>
        <span class="s1">)</span>

    <span class="s1">gramXAX = blockVectorX.T.conj() @ blockVectorAX</span>

    <span class="s1">_lambda</span><span class="s2">, </span><span class="s1">eigBlockVector = eigh(gramXAX</span><span class="s2">, </span><span class="s1">check_finite=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">ii = _get_indx(_lambda</span><span class="s2">, </span><span class="s1">sizeX</span><span class="s2">, </span><span class="s1">largest)</span>
    <span class="s1">_lambda = _lambda[ii]</span>
    <span class="s2">if </span><span class="s1">retLambdaHistory:</span>
        <span class="s1">lambdaHistory[</span><span class="s4">0</span><span class="s2">, </span><span class="s1">:] = _lambda</span>

    <span class="s1">eigBlockVector = np.asarray(eigBlockVector[:</span><span class="s2">, </span><span class="s1">ii])</span>
    <span class="s1">blockVectorX = _matmul_inplace(</span>
        <span class="s1">blockVectorX</span><span class="s2">, </span><span class="s1">eigBlockVector</span><span class="s2">,</span>
        <span class="s1">verbosityLevel=verbosityLevel</span>
    <span class="s1">)</span>
    <span class="s1">blockVectorAX = _matmul_inplace(</span>
        <span class="s1">blockVectorAX</span><span class="s2">, </span><span class="s1">eigBlockVector</span><span class="s2">,</span>
        <span class="s1">verbosityLevel=verbosityLevel</span>
    <span class="s1">)</span>
    <span class="s2">if </span><span class="s1">B </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">blockVectorBX = _matmul_inplace(</span>
            <span class="s1">blockVectorBX</span><span class="s2">, </span><span class="s1">eigBlockVector</span><span class="s2">,</span>
            <span class="s1">verbosityLevel=verbosityLevel</span>
        <span class="s1">)</span>

    <span class="s5">##</span>
    <span class="s5"># Active index set.</span>
    <span class="s1">activeMask = np.ones((sizeX</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype=bool)</span>

    <span class="s5">##</span>
    <span class="s5"># Main iteration loop.</span>

    <span class="s1">blockVectorP = </span><span class="s2">None  </span><span class="s5"># set during iteration</span>
    <span class="s1">blockVectorAP = </span><span class="s2">None</span>
    <span class="s1">blockVectorBP = </span><span class="s2">None</span>

    <span class="s1">smallestResidualNorm = np.abs(np.finfo(blockVectorX.dtype).max)</span>

    <span class="s1">iterationNumber = -</span><span class="s4">1</span>
    <span class="s1">restart = </span><span class="s2">True</span>
    <span class="s1">forcedRestart = </span><span class="s2">False</span>
    <span class="s1">explicitGramFlag = </span><span class="s2">False</span>
    <span class="s2">while </span><span class="s1">iterationNumber &lt; maxiter:</span>
        <span class="s1">iterationNumber += </span><span class="s4">1</span>

        <span class="s2">if </span><span class="s1">B </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">aux = blockVectorBX * _lambda[np.newaxis</span><span class="s2">, </span><span class="s1">:]</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">aux = blockVectorX * _lambda[np.newaxis</span><span class="s2">, </span><span class="s1">:]</span>

        <span class="s1">blockVectorR = blockVectorAX - aux</span>

        <span class="s1">aux = np.sum(blockVectorR.conj() * blockVectorR</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">residualNorms = np.sqrt(np.abs(aux))</span>
        <span class="s2">if </span><span class="s1">retResidualNormsHistory:</span>
            <span class="s1">residualNormsHistory[iterationNumber</span><span class="s2">, </span><span class="s1">:] = residualNorms</span>
        <span class="s1">residualNorm = np.sum(np.abs(residualNorms)) / sizeX</span>

        <span class="s2">if </span><span class="s1">residualNorm &lt; smallestResidualNorm:</span>
            <span class="s1">smallestResidualNorm = residualNorm</span>
            <span class="s1">bestIterationNumber = iterationNumber</span>
            <span class="s1">bestblockVectorX = blockVectorX</span>
        <span class="s2">elif </span><span class="s1">residualNorm &gt; </span><span class="s4">2</span><span class="s1">**restartControl * smallestResidualNorm:</span>
            <span class="s1">forcedRestart = </span><span class="s2">True</span>
            <span class="s1">blockVectorAX = A(blockVectorX)</span>
            <span class="s2">if </span><span class="s1">blockVectorAX.shape != blockVectorX.shape:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s3">f&quot;The shape </span><span class="s2">{</span><span class="s1">blockVectorX.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
                    <span class="s3">f&quot;of the restarted iterate not preserved</span><span class="s2">\n</span><span class="s3">&quot;</span>
                    <span class="s3">f&quot;and changed to </span><span class="s2">{</span><span class="s1">blockVectorAX.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
                    <span class="s3">f&quot;after multiplying by the primary matrix.</span><span class="s2">\n</span><span class="s3">&quot;</span>
                <span class="s1">)</span>
            <span class="s2">if </span><span class="s1">B </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">blockVectorBX = B(blockVectorX)</span>
                <span class="s2">if </span><span class="s1">blockVectorBX.shape != blockVectorX.shape:</span>
                    <span class="s2">raise </span><span class="s1">ValueError(</span>
                        <span class="s3">f&quot;The shape </span><span class="s2">{</span><span class="s1">blockVectorX.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
                        <span class="s3">f&quot;of the restarted iterate not preserved</span><span class="s2">\n</span><span class="s3">&quot;</span>
                        <span class="s3">f&quot;and changed to </span><span class="s2">{</span><span class="s1">blockVectorBX.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
                        <span class="s3">f&quot;after multiplying by the secondary matrix.</span><span class="s2">\n</span><span class="s3">&quot;</span>
                    <span class="s1">)</span>

        <span class="s1">ii = np.where(residualNorms &gt; residualTolerance</span><span class="s2">, True, False</span><span class="s1">)</span>
        <span class="s1">activeMask = activeMask &amp; ii</span>
        <span class="s1">currentBlockSize = activeMask.sum()</span>

        <span class="s2">if </span><span class="s1">verbosityLevel:</span>
            <span class="s1">print(</span><span class="s3">f&quot;iteration </span><span class="s2">{</span><span class="s1">iterationNumber</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
            <span class="s1">print(</span><span class="s3">f&quot;current block size: </span><span class="s2">{</span><span class="s1">currentBlockSize</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
            <span class="s1">print(</span><span class="s3">f&quot;eigenvalue(s):</span><span class="s2">\n{</span><span class="s1">_lambda</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
            <span class="s1">print(</span><span class="s3">f&quot;residual norm(s):</span><span class="s2">\n{</span><span class="s1">residualNorms</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">currentBlockSize == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">break</span>

        <span class="s1">activeBlockVectorR = _as2d(blockVectorR[:</span><span class="s2">, </span><span class="s1">activeMask])</span>

        <span class="s2">if </span><span class="s1">iterationNumber &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">activeBlockVectorP = _as2d(blockVectorP[:</span><span class="s2">, </span><span class="s1">activeMask])</span>
            <span class="s1">activeBlockVectorAP = _as2d(blockVectorAP[:</span><span class="s2">, </span><span class="s1">activeMask])</span>
            <span class="s2">if </span><span class="s1">B </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">activeBlockVectorBP = _as2d(blockVectorBP[:</span><span class="s2">, </span><span class="s1">activeMask])</span>

        <span class="s2">if </span><span class="s1">M </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s5"># Apply preconditioner T to the active residuals.</span>
            <span class="s1">activeBlockVectorR = M(activeBlockVectorR)</span>

        <span class="s5">##</span>
        <span class="s5"># Apply constraints to the preconditioned residuals.</span>
        <span class="s2">if </span><span class="s1">blockVectorY </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">_applyConstraints(activeBlockVectorR</span><span class="s2">,</span>
                              <span class="s1">gramYBY</span><span class="s2">,</span>
                              <span class="s1">blockVectorBY</span><span class="s2">,</span>
                              <span class="s1">blockVectorY)</span>

        <span class="s5">##</span>
        <span class="s5"># B-orthogonalize the preconditioned residuals to X.</span>
        <span class="s2">if </span><span class="s1">B </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">activeBlockVectorR = activeBlockVectorR - (</span>
                <span class="s1">blockVectorX @</span>
                <span class="s1">(blockVectorBX.T.conj() @ activeBlockVectorR)</span>
            <span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">activeBlockVectorR = activeBlockVectorR - (</span>
                <span class="s1">blockVectorX @</span>
                <span class="s1">(blockVectorX.T.conj() @ activeBlockVectorR)</span>
            <span class="s1">)</span>

        <span class="s5">##</span>
        <span class="s5"># B-orthonormalize the preconditioned residuals.</span>
        <span class="s1">aux = _b_orthonormalize(</span>
            <span class="s1">B</span><span class="s2">, </span><span class="s1">activeBlockVectorR</span><span class="s2">, </span><span class="s1">verbosityLevel=verbosityLevel)</span>
        <span class="s1">activeBlockVectorR</span><span class="s2">, </span><span class="s1">activeBlockVectorBR</span><span class="s2">, </span><span class="s1">_ = aux</span>

        <span class="s2">if </span><span class="s1">activeBlockVectorR </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s3">f&quot;Failed at iteration </span><span class="s2">{</span><span class="s1">iterationNumber</span><span class="s2">} </span><span class="s3">with accuracies &quot;</span>
                <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">residualNorms</span><span class="s2">}\n </span><span class="s3">not reaching the requested &quot;</span>
                <span class="s3">f&quot;tolerance </span><span class="s2">{</span><span class="s1">residualTolerance</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s2">,</span>
                <span class="s1">UserWarning</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">2</span>
            <span class="s1">)</span>
            <span class="s2">break</span>
        <span class="s1">activeBlockVectorAR = A(activeBlockVectorR)</span>

        <span class="s2">if </span><span class="s1">iterationNumber &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">B </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">aux = _b_orthonormalize(</span>
                    <span class="s1">B</span><span class="s2">, </span><span class="s1">activeBlockVectorP</span><span class="s2">, </span><span class="s1">activeBlockVectorBP</span><span class="s2">,</span>
                    <span class="s1">verbosityLevel=verbosityLevel</span>
                <span class="s1">)</span>
                <span class="s1">activeBlockVectorP</span><span class="s2">, </span><span class="s1">activeBlockVectorBP</span><span class="s2">, </span><span class="s1">invR = aux</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">aux = _b_orthonormalize(B</span><span class="s2">, </span><span class="s1">activeBlockVectorP</span><span class="s2">,</span>
                                        <span class="s1">verbosityLevel=verbosityLevel)</span>
                <span class="s1">activeBlockVectorP</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">invR = aux</span>
            <span class="s5"># Function _b_orthonormalize returns None if Cholesky fails</span>
            <span class="s2">if </span><span class="s1">activeBlockVectorP </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">activeBlockVectorAP = _matmul_inplace(</span>
                    <span class="s1">activeBlockVectorAP</span><span class="s2">, </span><span class="s1">invR</span><span class="s2">,</span>
                    <span class="s1">verbosityLevel=verbosityLevel</span>
                <span class="s1">)</span>
                <span class="s1">restart = forcedRestart</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">restart = </span><span class="s2">True</span>

        <span class="s5">##</span>
        <span class="s5"># Perform the Rayleigh Ritz Procedure:</span>
        <span class="s5"># Compute symmetric Gram matrices:</span>

        <span class="s2">if </span><span class="s1">activeBlockVectorAR.dtype == </span><span class="s3">&quot;float32&quot;</span><span class="s1">:</span>
            <span class="s1">myeps = </span><span class="s4">1</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">myeps = np.sqrt(np.finfo(activeBlockVectorR.dtype).eps)</span>

        <span class="s2">if </span><span class="s1">residualNorms.max() &gt; myeps </span><span class="s2">and not </span><span class="s1">explicitGramFlag:</span>
            <span class="s1">explicitGramFlag = </span><span class="s2">False</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s5"># Once explicitGramFlag, forever explicitGramFlag.</span>
            <span class="s1">explicitGramFlag = </span><span class="s2">True</span>

        <span class="s5"># Shared memory assingments to simplify the code</span>
        <span class="s2">if </span><span class="s1">B </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">blockVectorBX = blockVectorX</span>
            <span class="s1">activeBlockVectorBR = activeBlockVectorR</span>
            <span class="s2">if not </span><span class="s1">restart:</span>
                <span class="s1">activeBlockVectorBP = activeBlockVectorP</span>

        <span class="s5"># Common submatrices:</span>
        <span class="s1">gramXAR = np.dot(blockVectorX.T.conj()</span><span class="s2">, </span><span class="s1">activeBlockVectorAR)</span>
        <span class="s1">gramRAR = np.dot(activeBlockVectorR.T.conj()</span><span class="s2">, </span><span class="s1">activeBlockVectorAR)</span>

        <span class="s1">gramDtype = activeBlockVectorAR.dtype</span>
        <span class="s2">if </span><span class="s1">explicitGramFlag:</span>
            <span class="s1">gramRAR = (gramRAR + gramRAR.T.conj()) / </span><span class="s4">2</span>
            <span class="s1">gramXAX = np.dot(blockVectorX.T.conj()</span><span class="s2">, </span><span class="s1">blockVectorAX)</span>
            <span class="s1">gramXAX = (gramXAX + gramXAX.T.conj()) / </span><span class="s4">2</span>
            <span class="s1">gramXBX = np.dot(blockVectorX.T.conj()</span><span class="s2">, </span><span class="s1">blockVectorBX)</span>
            <span class="s1">gramRBR = np.dot(activeBlockVectorR.T.conj()</span><span class="s2">, </span><span class="s1">activeBlockVectorBR)</span>
            <span class="s1">gramXBR = np.dot(blockVectorX.T.conj()</span><span class="s2">, </span><span class="s1">activeBlockVectorBR)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">gramXAX = np.diag(_lambda).astype(gramDtype)</span>
            <span class="s1">gramXBX = np.eye(sizeX</span><span class="s2">, </span><span class="s1">dtype=gramDtype)</span>
            <span class="s1">gramRBR = np.eye(currentBlockSize</span><span class="s2">, </span><span class="s1">dtype=gramDtype)</span>
            <span class="s1">gramXBR = np.zeros((sizeX</span><span class="s2">, </span><span class="s1">currentBlockSize)</span><span class="s2">, </span><span class="s1">dtype=gramDtype)</span>

        <span class="s2">if not </span><span class="s1">restart:</span>
            <span class="s1">gramXAP = np.dot(blockVectorX.T.conj()</span><span class="s2">, </span><span class="s1">activeBlockVectorAP)</span>
            <span class="s1">gramRAP = np.dot(activeBlockVectorR.T.conj()</span><span class="s2">, </span><span class="s1">activeBlockVectorAP)</span>
            <span class="s1">gramPAP = np.dot(activeBlockVectorP.T.conj()</span><span class="s2">, </span><span class="s1">activeBlockVectorAP)</span>
            <span class="s1">gramXBP = np.dot(blockVectorX.T.conj()</span><span class="s2">, </span><span class="s1">activeBlockVectorBP)</span>
            <span class="s1">gramRBP = np.dot(activeBlockVectorR.T.conj()</span><span class="s2">, </span><span class="s1">activeBlockVectorBP)</span>
            <span class="s2">if </span><span class="s1">explicitGramFlag:</span>
                <span class="s1">gramPAP = (gramPAP + gramPAP.T.conj()) / </span><span class="s4">2</span>
                <span class="s1">gramPBP = np.dot(activeBlockVectorP.T.conj()</span><span class="s2">,</span>
                                 <span class="s1">activeBlockVectorBP)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">gramPBP = np.eye(currentBlockSize</span><span class="s2">, </span><span class="s1">dtype=gramDtype)</span>

            <span class="s1">gramA = np.block(</span>
                <span class="s1">[</span>
                    <span class="s1">[gramXAX</span><span class="s2">, </span><span class="s1">gramXAR</span><span class="s2">, </span><span class="s1">gramXAP]</span><span class="s2">,</span>
                    <span class="s1">[gramXAR.T.conj()</span><span class="s2">, </span><span class="s1">gramRAR</span><span class="s2">, </span><span class="s1">gramRAP]</span><span class="s2">,</span>
                    <span class="s1">[gramXAP.T.conj()</span><span class="s2">, </span><span class="s1">gramRAP.T.conj()</span><span class="s2">, </span><span class="s1">gramPAP]</span><span class="s2">,</span>
                <span class="s1">]</span>
            <span class="s1">)</span>
            <span class="s1">gramB = np.block(</span>
                <span class="s1">[</span>
                    <span class="s1">[gramXBX</span><span class="s2">, </span><span class="s1">gramXBR</span><span class="s2">, </span><span class="s1">gramXBP]</span><span class="s2">,</span>
                    <span class="s1">[gramXBR.T.conj()</span><span class="s2">, </span><span class="s1">gramRBR</span><span class="s2">, </span><span class="s1">gramRBP]</span><span class="s2">,</span>
                    <span class="s1">[gramXBP.T.conj()</span><span class="s2">, </span><span class="s1">gramRBP.T.conj()</span><span class="s2">, </span><span class="s1">gramPBP]</span><span class="s2">,</span>
                <span class="s1">]</span>
            <span class="s1">)</span>

            <span class="s1">_handle_gramA_gramB_verbosity(gramA</span><span class="s2">, </span><span class="s1">gramB</span><span class="s2">, </span><span class="s1">verbosityLevel)</span>

            <span class="s2">try</span><span class="s1">:</span>
                <span class="s1">_lambda</span><span class="s2">, </span><span class="s1">eigBlockVector = eigh(gramA</span><span class="s2">,</span>
                                               <span class="s1">gramB</span><span class="s2">,</span>
                                               <span class="s1">check_finite=</span><span class="s2">False</span><span class="s1">)</span>
            <span class="s2">except </span><span class="s1">LinAlgError </span><span class="s2">as </span><span class="s1">e:</span>
                <span class="s5"># raise ValueError(&quot;eigh failed in lobpcg iterations&quot;) from e</span>
                <span class="s2">if </span><span class="s1">verbosityLevel:</span>
                    <span class="s1">warnings.warn(</span>
                        <span class="s3">f&quot;eigh failed at iteration </span><span class="s2">{</span><span class="s1">iterationNumber</span><span class="s2">} \n</span><span class="s3">&quot;</span>
                        <span class="s3">f&quot;with error </span><span class="s2">{</span><span class="s1">e</span><span class="s2">} </span><span class="s3">causing a restart.</span><span class="s2">\n</span><span class="s3">&quot;</span><span class="s2">,</span>
                        <span class="s1">UserWarning</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">2</span>
                    <span class="s1">)</span>
                <span class="s5"># try again after dropping the direction vectors P from RR</span>
                <span class="s1">restart = </span><span class="s2">True</span>

        <span class="s2">if </span><span class="s1">restart:</span>
            <span class="s1">gramA = np.block([[gramXAX</span><span class="s2">, </span><span class="s1">gramXAR]</span><span class="s2">, </span><span class="s1">[gramXAR.T.conj()</span><span class="s2">, </span><span class="s1">gramRAR]])</span>
            <span class="s1">gramB = np.block([[gramXBX</span><span class="s2">, </span><span class="s1">gramXBR]</span><span class="s2">, </span><span class="s1">[gramXBR.T.conj()</span><span class="s2">, </span><span class="s1">gramRBR]])</span>

            <span class="s1">_handle_gramA_gramB_verbosity(gramA</span><span class="s2">, </span><span class="s1">gramB</span><span class="s2">, </span><span class="s1">verbosityLevel)</span>

            <span class="s2">try</span><span class="s1">:</span>
                <span class="s1">_lambda</span><span class="s2">, </span><span class="s1">eigBlockVector = eigh(gramA</span><span class="s2">,</span>
                                               <span class="s1">gramB</span><span class="s2">,</span>
                                               <span class="s1">check_finite=</span><span class="s2">False</span><span class="s1">)</span>
            <span class="s2">except </span><span class="s1">LinAlgError </span><span class="s2">as </span><span class="s1">e:</span>
                <span class="s5"># raise ValueError(&quot;eigh failed in lobpcg iterations&quot;) from e</span>
                <span class="s1">warnings.warn(</span>
                    <span class="s3">f&quot;eigh failed at iteration </span><span class="s2">{</span><span class="s1">iterationNumber</span><span class="s2">} </span><span class="s3">with error</span><span class="s2">\n</span><span class="s3">&quot;</span>
                    <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">e</span><span class="s2">}\n</span><span class="s3">&quot;</span><span class="s2">,</span>
                    <span class="s1">UserWarning</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">2</span>
                <span class="s1">)</span>
                <span class="s2">break</span>

        <span class="s1">ii = _get_indx(_lambda</span><span class="s2">, </span><span class="s1">sizeX</span><span class="s2">, </span><span class="s1">largest)</span>
        <span class="s1">_lambda = _lambda[ii]</span>
        <span class="s1">eigBlockVector = eigBlockVector[:</span><span class="s2">, </span><span class="s1">ii]</span>
        <span class="s2">if </span><span class="s1">retLambdaHistory:</span>
            <span class="s1">lambdaHistory[iterationNumber + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">:] = _lambda</span>

        <span class="s5"># Compute Ritz vectors.</span>
        <span class="s2">if </span><span class="s1">B </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">if not </span><span class="s1">restart:</span>
                <span class="s1">eigBlockVectorX = eigBlockVector[:sizeX]</span>
                <span class="s1">eigBlockVectorR = eigBlockVector[sizeX:</span>
                                                 <span class="s1">sizeX + currentBlockSize]</span>
                <span class="s1">eigBlockVectorP = eigBlockVector[sizeX + currentBlockSize:]</span>

                <span class="s1">pp = np.dot(activeBlockVectorR</span><span class="s2">, </span><span class="s1">eigBlockVectorR)</span>
                <span class="s1">pp += np.dot(activeBlockVectorP</span><span class="s2">, </span><span class="s1">eigBlockVectorP)</span>

                <span class="s1">app = np.dot(activeBlockVectorAR</span><span class="s2">, </span><span class="s1">eigBlockVectorR)</span>
                <span class="s1">app += np.dot(activeBlockVectorAP</span><span class="s2">, </span><span class="s1">eigBlockVectorP)</span>

                <span class="s1">bpp = np.dot(activeBlockVectorBR</span><span class="s2">, </span><span class="s1">eigBlockVectorR)</span>
                <span class="s1">bpp += np.dot(activeBlockVectorBP</span><span class="s2">, </span><span class="s1">eigBlockVectorP)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">eigBlockVectorX = eigBlockVector[:sizeX]</span>
                <span class="s1">eigBlockVectorR = eigBlockVector[sizeX:]</span>

                <span class="s1">pp = np.dot(activeBlockVectorR</span><span class="s2">, </span><span class="s1">eigBlockVectorR)</span>
                <span class="s1">app = np.dot(activeBlockVectorAR</span><span class="s2">, </span><span class="s1">eigBlockVectorR)</span>
                <span class="s1">bpp = np.dot(activeBlockVectorBR</span><span class="s2">, </span><span class="s1">eigBlockVectorR)</span>

            <span class="s1">blockVectorX = np.dot(blockVectorX</span><span class="s2">, </span><span class="s1">eigBlockVectorX) + pp</span>
            <span class="s1">blockVectorAX = np.dot(blockVectorAX</span><span class="s2">, </span><span class="s1">eigBlockVectorX) + app</span>
            <span class="s1">blockVectorBX = np.dot(blockVectorBX</span><span class="s2">, </span><span class="s1">eigBlockVectorX) + bpp</span>

            <span class="s1">blockVectorP</span><span class="s2">, </span><span class="s1">blockVectorAP</span><span class="s2">, </span><span class="s1">blockVectorBP = pp</span><span class="s2">, </span><span class="s1">app</span><span class="s2">, </span><span class="s1">bpp</span>

        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">if not </span><span class="s1">restart:</span>
                <span class="s1">eigBlockVectorX = eigBlockVector[:sizeX]</span>
                <span class="s1">eigBlockVectorR = eigBlockVector[sizeX:</span>
                                                 <span class="s1">sizeX + currentBlockSize]</span>
                <span class="s1">eigBlockVectorP = eigBlockVector[sizeX + currentBlockSize:]</span>

                <span class="s1">pp = np.dot(activeBlockVectorR</span><span class="s2">, </span><span class="s1">eigBlockVectorR)</span>
                <span class="s1">pp += np.dot(activeBlockVectorP</span><span class="s2">, </span><span class="s1">eigBlockVectorP)</span>

                <span class="s1">app = np.dot(activeBlockVectorAR</span><span class="s2">, </span><span class="s1">eigBlockVectorR)</span>
                <span class="s1">app += np.dot(activeBlockVectorAP</span><span class="s2">, </span><span class="s1">eigBlockVectorP)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">eigBlockVectorX = eigBlockVector[:sizeX]</span>
                <span class="s1">eigBlockVectorR = eigBlockVector[sizeX:]</span>

                <span class="s1">pp = np.dot(activeBlockVectorR</span><span class="s2">, </span><span class="s1">eigBlockVectorR)</span>
                <span class="s1">app = np.dot(activeBlockVectorAR</span><span class="s2">, </span><span class="s1">eigBlockVectorR)</span>

            <span class="s1">blockVectorX = np.dot(blockVectorX</span><span class="s2">, </span><span class="s1">eigBlockVectorX) + pp</span>
            <span class="s1">blockVectorAX = np.dot(blockVectorAX</span><span class="s2">, </span><span class="s1">eigBlockVectorX) + app</span>

            <span class="s1">blockVectorP</span><span class="s2">, </span><span class="s1">blockVectorAP = pp</span><span class="s2">, </span><span class="s1">app</span>

    <span class="s2">if </span><span class="s1">B </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">aux = blockVectorBX * _lambda[np.newaxis</span><span class="s2">, </span><span class="s1">:]</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">aux = blockVectorX * _lambda[np.newaxis</span><span class="s2">, </span><span class="s1">:]</span>

    <span class="s1">blockVectorR = blockVectorAX - aux</span>

    <span class="s1">aux = np.sum(blockVectorR.conj() * blockVectorR</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">residualNorms = np.sqrt(np.abs(aux))</span>
    <span class="s5"># Use old lambda in case of early loop exit.</span>
    <span class="s2">if </span><span class="s1">retLambdaHistory:</span>
        <span class="s1">lambdaHistory[iterationNumber + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">:] = _lambda</span>
    <span class="s2">if </span><span class="s1">retResidualNormsHistory:</span>
        <span class="s1">residualNormsHistory[iterationNumber + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">:] = residualNorms</span>
    <span class="s1">residualNorm = np.sum(np.abs(residualNorms)) / sizeX</span>
    <span class="s2">if </span><span class="s1">residualNorm &lt; smallestResidualNorm:</span>
        <span class="s1">smallestResidualNorm = residualNorm</span>
        <span class="s1">bestIterationNumber = iterationNumber + </span><span class="s4">1</span>
        <span class="s1">bestblockVectorX = blockVectorX</span>

    <span class="s2">if </span><span class="s1">np.max(np.abs(residualNorms)) &gt; residualTolerance:</span>
        <span class="s1">warnings.warn(</span>
            <span class="s3">f&quot;Exited at iteration </span><span class="s2">{</span><span class="s1">iterationNumber</span><span class="s2">} </span><span class="s3">with accuracies </span><span class="s2">\n</span><span class="s3">&quot;</span>
            <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">residualNorms</span><span class="s2">}\n</span><span class="s3">&quot;</span>
            <span class="s3">f&quot;not reaching the requested tolerance </span><span class="s2">{</span><span class="s1">residualTolerance</span><span class="s2">}</span><span class="s3">.</span><span class="s2">\n</span><span class="s3">&quot;</span>
            <span class="s3">f&quot;Use iteration </span><span class="s2">{</span><span class="s1">bestIterationNumber</span><span class="s2">} </span><span class="s3">instead with accuracy </span><span class="s2">\n</span><span class="s3">&quot;</span>
            <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">smallestResidualNorm</span><span class="s2">}</span><span class="s3">.</span><span class="s2">\n</span><span class="s3">&quot;</span><span class="s2">,</span>
            <span class="s1">UserWarning</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">2</span>
        <span class="s1">)</span>

    <span class="s2">if </span><span class="s1">verbosityLevel:</span>
        <span class="s1">print(</span><span class="s3">f&quot;Final iterative eigenvalue(s):</span><span class="s2">\n{</span><span class="s1">_lambda</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
        <span class="s1">print(</span><span class="s3">f&quot;Final iterative residual norm(s):</span><span class="s2">\n{</span><span class="s1">residualNorms</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>

    <span class="s1">blockVectorX = bestblockVectorX</span>
    <span class="s5"># Making eigenvectors &quot;exactly&quot; satisfy the blockVectorY constrains</span>
    <span class="s2">if </span><span class="s1">blockVectorY </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">_applyConstraints(blockVectorX</span><span class="s2">,</span>
                          <span class="s1">gramYBY</span><span class="s2">,</span>
                          <span class="s1">blockVectorBY</span><span class="s2">,</span>
                          <span class="s1">blockVectorY)</span>

    <span class="s5"># Making eigenvectors &quot;exactly&quot; othonormalized by final &quot;exact&quot; RR</span>
    <span class="s1">blockVectorAX = A(blockVectorX)</span>
    <span class="s2">if </span><span class="s1">blockVectorAX.shape != blockVectorX.shape:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span>
            <span class="s3">f&quot;The shape </span><span class="s2">{</span><span class="s1">blockVectorX.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
            <span class="s3">f&quot;of the postprocessing iterate not preserved</span><span class="s2">\n</span><span class="s3">&quot;</span>
            <span class="s3">f&quot;and changed to </span><span class="s2">{</span><span class="s1">blockVectorAX.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
            <span class="s3">f&quot;after multiplying by the primary matrix.</span><span class="s2">\n</span><span class="s3">&quot;</span>
        <span class="s1">)</span>
    <span class="s1">gramXAX = np.dot(blockVectorX.T.conj()</span><span class="s2">, </span><span class="s1">blockVectorAX)</span>

    <span class="s1">blockVectorBX = blockVectorX</span>
    <span class="s2">if </span><span class="s1">B </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">blockVectorBX = B(blockVectorX)</span>
        <span class="s2">if </span><span class="s1">blockVectorBX.shape != blockVectorX.shape:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s3">f&quot;The shape </span><span class="s2">{</span><span class="s1">blockVectorX.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
                <span class="s3">f&quot;of the postprocessing iterate not preserved</span><span class="s2">\n</span><span class="s3">&quot;</span>
                <span class="s3">f&quot;and changed to </span><span class="s2">{</span><span class="s1">blockVectorBX.shape</span><span class="s2">} </span><span class="s3">&quot;</span>
                <span class="s3">f&quot;after multiplying by the secondary matrix.</span><span class="s2">\n</span><span class="s3">&quot;</span>
            <span class="s1">)</span>

    <span class="s1">gramXBX = np.dot(blockVectorX.T.conj()</span><span class="s2">, </span><span class="s1">blockVectorBX)</span>
    <span class="s1">_handle_gramA_gramB_verbosity(gramXAX</span><span class="s2">, </span><span class="s1">gramXBX</span><span class="s2">, </span><span class="s1">verbosityLevel)</span>
    <span class="s1">gramXAX = (gramXAX + gramXAX.T.conj()) / </span><span class="s4">2</span>
    <span class="s1">gramXBX = (gramXBX + gramXBX.T.conj()) / </span><span class="s4">2</span>
    <span class="s2">try</span><span class="s1">:</span>
        <span class="s1">_lambda</span><span class="s2">, </span><span class="s1">eigBlockVector = eigh(gramXAX</span><span class="s2">,</span>
                                       <span class="s1">gramXBX</span><span class="s2">,</span>
                                       <span class="s1">check_finite=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s2">except </span><span class="s1">LinAlgError </span><span class="s2">as </span><span class="s1">e:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;eigh has failed in lobpcg postprocessing&quot;</span><span class="s1">) </span><span class="s2">from </span><span class="s1">e</span>

    <span class="s1">ii = _get_indx(_lambda</span><span class="s2">, </span><span class="s1">sizeX</span><span class="s2">, </span><span class="s1">largest)</span>
    <span class="s1">_lambda = _lambda[ii]</span>
    <span class="s1">eigBlockVector = np.asarray(eigBlockVector[:</span><span class="s2">, </span><span class="s1">ii])</span>

    <span class="s1">blockVectorX = np.dot(blockVectorX</span><span class="s2">, </span><span class="s1">eigBlockVector)</span>
    <span class="s1">blockVectorAX = np.dot(blockVectorAX</span><span class="s2">, </span><span class="s1">eigBlockVector)</span>

    <span class="s2">if </span><span class="s1">B </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">blockVectorBX = np.dot(blockVectorBX</span><span class="s2">, </span><span class="s1">eigBlockVector)</span>
        <span class="s1">aux = blockVectorBX * _lambda[np.newaxis</span><span class="s2">, </span><span class="s1">:]</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">aux = blockVectorX * _lambda[np.newaxis</span><span class="s2">, </span><span class="s1">:]</span>

    <span class="s1">blockVectorR = blockVectorAX - aux</span>

    <span class="s1">aux = np.sum(blockVectorR.conj() * blockVectorR</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">residualNorms = np.sqrt(np.abs(aux))</span>

    <span class="s2">if </span><span class="s1">retLambdaHistory:</span>
        <span class="s1">lambdaHistory[bestIterationNumber + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">:] = _lambda</span>
    <span class="s2">if </span><span class="s1">retResidualNormsHistory:</span>
        <span class="s1">residualNormsHistory[bestIterationNumber + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">:] = residualNorms</span>

    <span class="s2">if </span><span class="s1">retLambdaHistory:</span>
        <span class="s1">lambdaHistory = lambdaHistory[</span>
            <span class="s1">: bestIterationNumber + </span><span class="s4">2</span><span class="s2">, </span><span class="s1">:]</span>
    <span class="s2">if </span><span class="s1">retResidualNormsHistory:</span>
        <span class="s1">residualNormsHistory = residualNormsHistory[</span>
            <span class="s1">: bestIterationNumber + </span><span class="s4">2</span><span class="s2">, </span><span class="s1">:]</span>

    <span class="s2">if </span><span class="s1">np.max(np.abs(residualNorms)) &gt; residualTolerance:</span>
        <span class="s1">warnings.warn(</span>
            <span class="s3">f&quot;Exited postprocessing with accuracies </span><span class="s2">\n</span><span class="s3">&quot;</span>
            <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">residualNorms</span><span class="s2">}\n</span><span class="s3">&quot;</span>
            <span class="s3">f&quot;not reaching the requested tolerance </span><span class="s2">{</span><span class="s1">residualTolerance</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s2">,</span>
            <span class="s1">UserWarning</span><span class="s2">, </span><span class="s1">stacklevel=</span><span class="s4">2</span>
        <span class="s1">)</span>

    <span class="s2">if </span><span class="s1">verbosityLevel:</span>
        <span class="s1">print(</span><span class="s3">f&quot;Final postprocessing eigenvalue(s):</span><span class="s2">\n{</span><span class="s1">_lambda</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
        <span class="s1">print(</span><span class="s3">f&quot;Final residual norm(s):</span><span class="s2">\n{</span><span class="s1">residualNorms</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">retLambdaHistory:</span>
        <span class="s1">lambdaHistory = np.vsplit(lambdaHistory</span><span class="s2">, </span><span class="s1">np.shape(lambdaHistory)[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">lambdaHistory = [np.squeeze(i) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">lambdaHistory]</span>
    <span class="s2">if </span><span class="s1">retResidualNormsHistory:</span>
        <span class="s1">residualNormsHistory = np.vsplit(residualNormsHistory</span><span class="s2">,</span>
                                         <span class="s1">np.shape(residualNormsHistory)[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">residualNormsHistory = [np.squeeze(i) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">residualNormsHistory]</span>

    <span class="s2">if </span><span class="s1">retLambdaHistory:</span>
        <span class="s2">if </span><span class="s1">retResidualNormsHistory:</span>
            <span class="s2">return </span><span class="s1">_lambda</span><span class="s2">, </span><span class="s1">blockVectorX</span><span class="s2">, </span><span class="s1">lambdaHistory</span><span class="s2">, </span><span class="s1">residualNormsHistory</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">_lambda</span><span class="s2">, </span><span class="s1">blockVectorX</span><span class="s2">, </span><span class="s1">lambdaHistory</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">if </span><span class="s1">retResidualNormsHistory:</span>
            <span class="s2">return </span><span class="s1">_lambda</span><span class="s2">, </span><span class="s1">blockVectorX</span><span class="s2">, </span><span class="s1">residualNormsHistory</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">_lambda</span><span class="s2">, </span><span class="s1">blockVectorX</span>
</pre>
</body>
</html>