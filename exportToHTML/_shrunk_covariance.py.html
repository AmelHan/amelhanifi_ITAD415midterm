<html>
<head>
<title>_shrunk_covariance.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_shrunk_covariance.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Covariance estimators using shrinkage. 
 
Shrinkage corresponds to regularising `cov` using a convex combination: 
shrunk_cov = (1-shrinkage)*cov + shrinkage*structured_estimate. 
 
&quot;&quot;&quot;</span>

<span class="s2"># Author: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr&gt;</span>
<span class="s2">#         Gael Varoquaux &lt;gael.varoquaux@normalesup.org&gt;</span>
<span class="s2">#         Virgile Fritsch &lt;virgile.fritsch@inria.fr&gt;</span>
<span class="s2">#</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s2"># avoid division truncation</span>
<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">_fit_context</span>
<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">check_array</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">validate_params</span>
<span class="s3">from </span><span class="s1">. </span><span class="s3">import </span><span class="s1">EmpiricalCovariance</span><span class="s3">, </span><span class="s1">empirical_covariance</span>


<span class="s3">def </span><span class="s1">_ledoit_wolf(X</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">assume_centered</span><span class="s3">, </span><span class="s1">block_size):</span>
    <span class="s0">&quot;&quot;&quot;Estimate the shrunk Ledoit-Wolf covariance matrix.&quot;&quot;&quot;</span>
    <span class="s2"># for only one feature, the result is the same whatever the shrinkage</span>
    <span class="s3">if </span><span class="s1">len(X.shape) == </span><span class="s4">2 </span><span class="s3">and </span><span class="s1">X.shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s3">if not </span><span class="s1">assume_centered:</span>
            <span class="s1">X = X - X.mean()</span>
        <span class="s3">return </span><span class="s1">np.atleast_2d((X**</span><span class="s4">2</span><span class="s1">).mean())</span><span class="s3">, </span><span class="s4">0.0</span>
    <span class="s1">n_features = X.shape[</span><span class="s4">1</span><span class="s1">]</span>

    <span class="s2"># get Ledoit-Wolf shrinkage</span>
    <span class="s1">shrinkage = ledoit_wolf_shrinkage(</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">assume_centered=assume_centered</span><span class="s3">, </span><span class="s1">block_size=block_size</span>
    <span class="s1">)</span>
    <span class="s1">emp_cov = empirical_covariance(X</span><span class="s3">, </span><span class="s1">assume_centered=assume_centered)</span>
    <span class="s1">mu = np.sum(np.trace(emp_cov)) / n_features</span>
    <span class="s1">shrunk_cov = (</span><span class="s4">1.0 </span><span class="s1">- shrinkage) * emp_cov</span>
    <span class="s1">shrunk_cov.flat[:: n_features + </span><span class="s4">1</span><span class="s1">] += shrinkage * mu</span>

    <span class="s3">return </span><span class="s1">shrunk_cov</span><span class="s3">, </span><span class="s1">shrinkage</span>


<span class="s3">def </span><span class="s1">_oas(X</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">assume_centered=</span><span class="s3">False</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Estimate covariance with the Oracle Approximating Shrinkage algorithm. 
 
    The formulation is based on [1]_. 
    [1] &quot;Shrinkage algorithms for MMSE covariance estimation.&quot;, 
        Chen, Y., Wiesel, A., Eldar, Y. C., &amp; Hero, A. O. 
        IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010. 
        https://arxiv.org/pdf/0907.4698.pdf 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">len(X.shape) == </span><span class="s4">2 </span><span class="s3">and </span><span class="s1">X.shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s2"># for only one feature, the result is the same whatever the shrinkage</span>
        <span class="s3">if not </span><span class="s1">assume_centered:</span>
            <span class="s1">X = X - X.mean()</span>
        <span class="s3">return </span><span class="s1">np.atleast_2d((X**</span><span class="s4">2</span><span class="s1">).mean())</span><span class="s3">, </span><span class="s4">0.0</span>

    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>

    <span class="s1">emp_cov = empirical_covariance(X</span><span class="s3">, </span><span class="s1">assume_centered=assume_centered)</span>

    <span class="s2"># The shrinkage is defined as:</span>
    <span class="s2"># shrinkage = min(</span>
    <span class="s2"># trace(S @ S.T) + trace(S)**2) / ((n + 1) (trace(S @ S.T) - trace(S)**2 / p), 1</span>
    <span class="s2"># )</span>
    <span class="s2"># where n and p are n_samples and n_features, respectively (cf. Eq. 23 in [1]).</span>
    <span class="s2"># The factor 2 / p is omitted since it does not impact the value of the estimator</span>
    <span class="s2"># for large p.</span>

    <span class="s2"># Instead of computing trace(S)**2, we can compute the average of the squared</span>
    <span class="s2"># elements of S that is equal to trace(S)**2 / p**2.</span>
    <span class="s2"># See the definition of the Frobenius norm:</span>
    <span class="s2"># https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm</span>
    <span class="s1">alpha = np.mean(emp_cov**</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">mu = np.trace(emp_cov) / n_features</span>
    <span class="s1">mu_squared = mu**</span><span class="s4">2</span>

    <span class="s2"># The factor 1 / p**2 will cancel out since it is in both the numerator and</span>
    <span class="s2"># denominator</span>
    <span class="s1">num = alpha + mu_squared</span>
    <span class="s1">den = (n_samples + </span><span class="s4">1</span><span class="s1">) * (alpha - mu_squared / n_features)</span>
    <span class="s1">shrinkage = </span><span class="s4">1.0 </span><span class="s3">if </span><span class="s1">den == </span><span class="s4">0 </span><span class="s3">else </span><span class="s1">min(num / den</span><span class="s3">, </span><span class="s4">1.0</span><span class="s1">)</span>

    <span class="s2"># The shrunk covariance is defined as:</span>
    <span class="s2"># (1 - shrinkage) * S + shrinkage * F (cf. Eq. 4 in [1])</span>
    <span class="s2"># where S is the empirical covariance and F is the shrinkage target defined as</span>
    <span class="s2"># F = trace(S) / n_features * np.identity(n_features) (cf. Eq. 3 in [1])</span>
    <span class="s1">shrunk_cov = (</span><span class="s4">1.0 </span><span class="s1">- shrinkage) * emp_cov</span>
    <span class="s1">shrunk_cov.flat[:: n_features + </span><span class="s4">1</span><span class="s1">] += shrinkage * mu</span>

    <span class="s3">return </span><span class="s1">shrunk_cov</span><span class="s3">, </span><span class="s1">shrinkage</span>


<span class="s2">###############################################################################</span>
<span class="s2"># Public API</span>
<span class="s2"># ShrunkCovariance estimator</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s5">&quot;emp_cov&quot;</span><span class="s1">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;shrinkage&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s1">closed=</span><span class="s5">&quot;both&quot;</span><span class="s1">)]</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">True,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">shrunk_covariance(emp_cov</span><span class="s3">, </span><span class="s1">shrinkage=</span><span class="s4">0.1</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Calculate a covariance matrix shrunk on the diagonal. 
 
    Read more in the :ref:`User Guide &lt;shrunk_covariance&gt;`. 
 
    Parameters 
    ---------- 
    emp_cov : array-like of shape (n_features, n_features) 
        Covariance matrix to be shrunk. 
 
    shrinkage : float, default=0.1 
        Coefficient in the convex combination used for the computation 
        of the shrunk estimate. Range is [0, 1]. 
 
    Returns 
    ------- 
    shrunk_cov : ndarray of shape (n_features, n_features) 
        Shrunk covariance. 
 
    Notes 
    ----- 
    The regularized (shrunk) covariance is given by:: 
 
        (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features) 
 
    where `mu = trace(cov) / n_features`. 
    &quot;&quot;&quot;</span>
    <span class="s1">emp_cov = check_array(emp_cov)</span>
    <span class="s1">n_features = emp_cov.shape[</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s1">mu = np.trace(emp_cov) / n_features</span>
    <span class="s1">shrunk_cov = (</span><span class="s4">1.0 </span><span class="s1">- shrinkage) * emp_cov</span>
    <span class="s1">shrunk_cov.flat[:: n_features + </span><span class="s4">1</span><span class="s1">] += shrinkage * mu</span>

    <span class="s3">return </span><span class="s1">shrunk_cov</span>


<span class="s3">class </span><span class="s1">ShrunkCovariance(EmpiricalCovariance):</span>
    <span class="s0">&quot;&quot;&quot;Covariance estimator with shrinkage. 
 
    Read more in the :ref:`User Guide &lt;shrunk_covariance&gt;`. 
 
    Parameters 
    ---------- 
    store_precision : bool, default=True 
        Specify if the estimated precision is stored. 
 
    assume_centered : bool, default=False 
        If True, data will not be centered before computation. 
        Useful when working with data whose mean is almost, but not exactly 
        zero. 
        If False, data will be centered before computation. 
 
    shrinkage : float, default=0.1 
        Coefficient in the convex combination used for the computation 
        of the shrunk estimate. Range is [0, 1]. 
 
    Attributes 
    ---------- 
    covariance_ : ndarray of shape (n_features, n_features) 
        Estimated covariance matrix 
 
    location_ : ndarray of shape (n_features,) 
        Estimated location, i.e. the estimated mean. 
 
    precision_ : ndarray of shape (n_features, n_features) 
        Estimated pseudo inverse matrix. 
        (stored only if store_precision is True) 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    EllipticEnvelope : An object for detecting outliers in 
        a Gaussian distributed dataset. 
    EmpiricalCovariance : Maximum likelihood covariance estimator. 
    GraphicalLasso : Sparse inverse covariance estimation 
        with an l1-penalized estimator. 
    GraphicalLassoCV : Sparse inverse covariance with cross-validated 
        choice of the l1 penalty. 
    LedoitWolf : LedoitWolf Estimator. 
    MinCovDet : Minimum Covariance Determinant 
        (robust estimator of covariance). 
    OAS : Oracle Approximating Shrinkage Estimator. 
 
    Notes 
    ----- 
    The regularized covariance is given by: 
 
    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features) 
 
    where mu = trace(cov) / n_features 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.covariance import ShrunkCovariance 
    &gt;&gt;&gt; from sklearn.datasets import make_gaussian_quantiles 
    &gt;&gt;&gt; real_cov = np.array([[.8, .3], 
    ...                      [.3, .4]]) 
    &gt;&gt;&gt; rng = np.random.RandomState(0) 
    &gt;&gt;&gt; X = rng.multivariate_normal(mean=[0, 0], 
    ...                                   cov=real_cov, 
    ...                                   size=500) 
    &gt;&gt;&gt; cov = ShrunkCovariance().fit(X) 
    &gt;&gt;&gt; cov.covariance_ 
    array([[0.7387..., 0.2536...], 
           [0.2536..., 0.4110...]]) 
    &gt;&gt;&gt; cov.location_ 
    array([0.0622..., 0.0193...]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**EmpiricalCovariance._parameter_constraints</span><span class="s3">,</span>
        <span class="s5">&quot;shrinkage&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s1">closed=</span><span class="s5">&quot;both&quot;</span><span class="s1">)]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">store_precision=</span><span class="s3">True, </span><span class="s1">assume_centered=</span><span class="s3">False, </span><span class="s1">shrinkage=</span><span class="s4">0.1</span><span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">store_precision=store_precision</span><span class="s3">, </span><span class="s1">assume_centered=assume_centered</span>
        <span class="s1">)</span>
        <span class="s1">self.shrinkage = shrinkage</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the shrunk covariance model to X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training data, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        y : Ignored 
            Not used, present for API consistency by convention. 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X)</span>
        <span class="s2"># Not calling the parent object to fit, to avoid a potential</span>
        <span class="s2"># matrix inversion when setting the precision</span>
        <span class="s3">if </span><span class="s1">self.assume_centered:</span>
            <span class="s1">self.location_ = np.zeros(X.shape[</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.location_ = X.mean(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">covariance = empirical_covariance(X</span><span class="s3">, </span><span class="s1">assume_centered=self.assume_centered)</span>
        <span class="s1">covariance = shrunk_covariance(covariance</span><span class="s3">, </span><span class="s1">self.shrinkage)</span>
        <span class="s1">self._set_covariance(covariance)</span>

        <span class="s3">return </span><span class="s1">self</span>


<span class="s2"># Ledoit-Wolf estimator</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s5">&quot;X&quot;</span><span class="s1">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;assume_centered&quot;</span><span class="s1">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;block_size&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">True,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">ledoit_wolf_shrinkage(X</span><span class="s3">, </span><span class="s1">assume_centered=</span><span class="s3">False, </span><span class="s1">block_size=</span><span class="s4">1000</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Estimate the shrunk Ledoit-Wolf covariance matrix. 
 
    Read more in the :ref:`User Guide &lt;shrunk_covariance&gt;`. 
 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
        Data from which to compute the Ledoit-Wolf shrunk covariance shrinkage. 
 
    assume_centered : bool, default=False 
        If True, data will not be centered before computation. 
        Useful to work with data whose mean is significantly equal to 
        zero but is not exactly zero. 
        If False, data will be centered before computation. 
 
    block_size : int, default=1000 
        Size of blocks into which the covariance matrix will be split. 
 
    Returns 
    ------- 
    shrinkage : float 
        Coefficient in the convex combination used for the computation 
        of the shrunk estimate. 
 
    Notes 
    ----- 
    The regularized (shrunk) covariance is: 
 
    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features) 
 
    where mu = trace(cov) / n_features 
    &quot;&quot;&quot;</span>
    <span class="s1">X = check_array(X)</span>
    <span class="s2"># for only one feature, the result is the same whatever the shrinkage</span>
    <span class="s3">if </span><span class="s1">len(X.shape) == </span><span class="s4">2 </span><span class="s3">and </span><span class="s1">X.shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s4">0.0</span>
    <span class="s3">if </span><span class="s1">X.ndim == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">X = np.reshape(X</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">))</span>

    <span class="s3">if </span><span class="s1">X.shape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">warnings.warn(</span>
            <span class="s5">&quot;Only one sample available. You may want to reshape your data array&quot;</span>
        <span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>

    <span class="s2"># optionally center data</span>
    <span class="s3">if not </span><span class="s1">assume_centered:</span>
        <span class="s1">X = X - X.mean(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s2"># A non-blocked version of the computation is present in the tests</span>
    <span class="s2"># in tests/test_covariance.py</span>

    <span class="s2"># number of blocks to split the covariance matrix into</span>
    <span class="s1">n_splits = int(n_features / block_size)</span>
    <span class="s1">X2 = X**</span><span class="s4">2</span>
    <span class="s1">emp_cov_trace = np.sum(X2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">) / n_samples</span>
    <span class="s1">mu = np.sum(emp_cov_trace) / n_features</span>
    <span class="s1">beta_ = </span><span class="s4">0.0  </span><span class="s2"># sum of the coefficients of &lt;X2.T, X2&gt;</span>
    <span class="s1">delta_ = </span><span class="s4">0.0  </span><span class="s2"># sum of the *squared* coefficients of &lt;X.T, X&gt;</span>
    <span class="s2"># starting block computation</span>
    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_splits):</span>
        <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(n_splits):</span>
            <span class="s1">rows = slice(block_size * i</span><span class="s3">, </span><span class="s1">block_size * (i + </span><span class="s4">1</span><span class="s1">))</span>
            <span class="s1">cols = slice(block_size * j</span><span class="s3">, </span><span class="s1">block_size * (j + </span><span class="s4">1</span><span class="s1">))</span>
            <span class="s1">beta_ += np.sum(np.dot(X2.T[rows]</span><span class="s3">, </span><span class="s1">X2[:</span><span class="s3">, </span><span class="s1">cols]))</span>
            <span class="s1">delta_ += np.sum(np.dot(X.T[rows]</span><span class="s3">, </span><span class="s1">X[:</span><span class="s3">, </span><span class="s1">cols]) ** </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">rows = slice(block_size * i</span><span class="s3">, </span><span class="s1">block_size * (i + </span><span class="s4">1</span><span class="s1">))</span>
        <span class="s1">beta_ += np.sum(np.dot(X2.T[rows]</span><span class="s3">, </span><span class="s1">X2[:</span><span class="s3">, </span><span class="s1">block_size * n_splits :]))</span>
        <span class="s1">delta_ += np.sum(np.dot(X.T[rows]</span><span class="s3">, </span><span class="s1">X[:</span><span class="s3">, </span><span class="s1">block_size * n_splits :]) ** </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(n_splits):</span>
        <span class="s1">cols = slice(block_size * j</span><span class="s3">, </span><span class="s1">block_size * (j + </span><span class="s4">1</span><span class="s1">))</span>
        <span class="s1">beta_ += np.sum(np.dot(X2.T[block_size * n_splits :]</span><span class="s3">, </span><span class="s1">X2[:</span><span class="s3">, </span><span class="s1">cols]))</span>
        <span class="s1">delta_ += np.sum(np.dot(X.T[block_size * n_splits :]</span><span class="s3">, </span><span class="s1">X[:</span><span class="s3">, </span><span class="s1">cols]) ** </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">delta_ += np.sum(</span>
        <span class="s1">np.dot(X.T[block_size * n_splits :]</span><span class="s3">, </span><span class="s1">X[:</span><span class="s3">, </span><span class="s1">block_size * n_splits :]) ** </span><span class="s4">2</span>
    <span class="s1">)</span>
    <span class="s1">delta_ /= n_samples**</span><span class="s4">2</span>
    <span class="s1">beta_ += np.sum(</span>
        <span class="s1">np.dot(X2.T[block_size * n_splits :]</span><span class="s3">, </span><span class="s1">X2[:</span><span class="s3">, </span><span class="s1">block_size * n_splits :])</span>
    <span class="s1">)</span>
    <span class="s2"># use delta_ to compute beta</span>
    <span class="s1">beta = </span><span class="s4">1.0 </span><span class="s1">/ (n_features * n_samples) * (beta_ / n_samples - delta_)</span>
    <span class="s2"># delta is the sum of the squared coefficients of (&lt;X.T,X&gt; - mu*Id) / p</span>
    <span class="s1">delta = delta_ - </span><span class="s4">2.0 </span><span class="s1">* mu * emp_cov_trace.sum() + n_features * mu**</span><span class="s4">2</span>
    <span class="s1">delta /= n_features</span>
    <span class="s2"># get final beta as the min between beta and delta</span>
    <span class="s2"># We do this to prevent shrinking more than &quot;1&quot;, which would invert</span>
    <span class="s2"># the value of covariances</span>
    <span class="s1">beta = min(beta</span><span class="s3">, </span><span class="s1">delta)</span>
    <span class="s2"># finally get shrinkage</span>
    <span class="s1">shrinkage = </span><span class="s4">0 </span><span class="s3">if </span><span class="s1">beta == </span><span class="s4">0 </span><span class="s3">else </span><span class="s1">beta / delta</span>
    <span class="s3">return </span><span class="s1">shrinkage</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span><span class="s5">&quot;X&quot;</span><span class="s1">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s1">]}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">False,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">ledoit_wolf(X</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">assume_centered=</span><span class="s3">False, </span><span class="s1">block_size=</span><span class="s4">1000</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Estimate the shrunk Ledoit-Wolf covariance matrix. 
 
    Read more in the :ref:`User Guide &lt;shrunk_covariance&gt;`. 
 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
        Data from which to compute the covariance estimate. 
 
    assume_centered : bool, default=False 
        If True, data will not be centered before computation. 
        Useful to work with data whose mean is significantly equal to 
        zero but is not exactly zero. 
        If False, data will be centered before computation. 
 
    block_size : int, default=1000 
        Size of blocks into which the covariance matrix will be split. 
        This is purely a memory optimization and does not affect results. 
 
    Returns 
    ------- 
    shrunk_cov : ndarray of shape (n_features, n_features) 
        Shrunk covariance. 
 
    shrinkage : float 
        Coefficient in the convex combination used for the computation 
        of the shrunk estimate. 
 
    Notes 
    ----- 
    The regularized (shrunk) covariance is: 
 
    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features) 
 
    where mu = trace(cov) / n_features 
    &quot;&quot;&quot;</span>
    <span class="s1">estimator = LedoitWolf(</span>
        <span class="s1">assume_centered=assume_centered</span><span class="s3">,</span>
        <span class="s1">block_size=block_size</span><span class="s3">,</span>
        <span class="s1">store_precision=</span><span class="s3">False,</span>
    <span class="s1">).fit(X)</span>

    <span class="s3">return </span><span class="s1">estimator.covariance_</span><span class="s3">, </span><span class="s1">estimator.shrinkage_</span>


<span class="s3">class </span><span class="s1">LedoitWolf(EmpiricalCovariance):</span>
    <span class="s0">&quot;&quot;&quot;LedoitWolf Estimator. 
 
    Ledoit-Wolf is a particular form of shrinkage, where the shrinkage 
    coefficient is computed using O. Ledoit and M. Wolf's formula as 
    described in &quot;A Well-Conditioned Estimator for Large-Dimensional 
    Covariance Matrices&quot;, Ledoit and Wolf, Journal of Multivariate 
    Analysis, Volume 88, Issue 2, February 2004, pages 365-411. 
 
    Read more in the :ref:`User Guide &lt;shrunk_covariance&gt;`. 
 
    Parameters 
    ---------- 
    store_precision : bool, default=True 
        Specify if the estimated precision is stored. 
 
    assume_centered : bool, default=False 
        If True, data will not be centered before computation. 
        Useful when working with data whose mean is almost, but not exactly 
        zero. 
        If False (default), data will be centered before computation. 
 
    block_size : int, default=1000 
        Size of blocks into which the covariance matrix will be split 
        during its Ledoit-Wolf estimation. This is purely a memory 
        optimization and does not affect results. 
 
    Attributes 
    ---------- 
    covariance_ : ndarray of shape (n_features, n_features) 
        Estimated covariance matrix. 
 
    location_ : ndarray of shape (n_features,) 
        Estimated location, i.e. the estimated mean. 
 
    precision_ : ndarray of shape (n_features, n_features) 
        Estimated pseudo inverse matrix. 
        (stored only if store_precision is True) 
 
    shrinkage_ : float 
        Coefficient in the convex combination used for the computation 
        of the shrunk estimate. Range is [0, 1]. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    EllipticEnvelope : An object for detecting outliers in 
        a Gaussian distributed dataset. 
    EmpiricalCovariance : Maximum likelihood covariance estimator. 
    GraphicalLasso : Sparse inverse covariance estimation 
        with an l1-penalized estimator. 
    GraphicalLassoCV : Sparse inverse covariance with cross-validated 
        choice of the l1 penalty. 
    MinCovDet : Minimum Covariance Determinant 
        (robust estimator of covariance). 
    OAS : Oracle Approximating Shrinkage Estimator. 
    ShrunkCovariance : Covariance estimator with shrinkage. 
 
    Notes 
    ----- 
    The regularised covariance is: 
 
    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features) 
 
    where mu = trace(cov) / n_features 
    and shrinkage is given by the Ledoit and Wolf formula (see References) 
 
    References 
    ---------- 
    &quot;A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices&quot;, 
    Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2, 
    February 2004, pages 365-411. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.covariance import LedoitWolf 
    &gt;&gt;&gt; real_cov = np.array([[.4, .2], 
    ...                      [.2, .8]]) 
    &gt;&gt;&gt; np.random.seed(0) 
    &gt;&gt;&gt; X = np.random.multivariate_normal(mean=[0, 0], 
    ...                                   cov=real_cov, 
    ...                                   size=50) 
    &gt;&gt;&gt; cov = LedoitWolf().fit(X) 
    &gt;&gt;&gt; cov.covariance_ 
    array([[0.4406..., 0.1616...], 
           [0.1616..., 0.8022...]]) 
    &gt;&gt;&gt; cov.location_ 
    array([ 0.0595... , -0.0075...]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**EmpiricalCovariance._parameter_constraints</span><span class="s3">,</span>
        <span class="s5">&quot;block_size&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">store_precision=</span><span class="s3">True, </span><span class="s1">assume_centered=</span><span class="s3">False, </span><span class="s1">block_size=</span><span class="s4">1000</span><span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">store_precision=store_precision</span><span class="s3">, </span><span class="s1">assume_centered=assume_centered</span>
        <span class="s1">)</span>
        <span class="s1">self.block_size = block_size</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the Ledoit-Wolf shrunk covariance model to X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training data, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
        y : Ignored 
            Not used, present for API consistency by convention. 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s2"># Not calling the parent object to fit, to avoid computing the</span>
        <span class="s2"># covariance matrix (and potentially the precision)</span>
        <span class="s1">X = self._validate_data(X)</span>
        <span class="s3">if </span><span class="s1">self.assume_centered:</span>
            <span class="s1">self.location_ = np.zeros(X.shape[</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.location_ = X.mean(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">covariance</span><span class="s3">, </span><span class="s1">shrinkage = _ledoit_wolf(</span>
            <span class="s1">X - self.location_</span><span class="s3">, </span><span class="s1">assume_centered=</span><span class="s3">True, </span><span class="s1">block_size=self.block_size</span>
        <span class="s1">)</span>
        <span class="s1">self.shrinkage_ = shrinkage</span>
        <span class="s1">self._set_covariance(covariance)</span>

        <span class="s3">return </span><span class="s1">self</span>


<span class="s2"># OAS estimator</span>
<span class="s1">@validate_params(</span>
    <span class="s1">{</span><span class="s5">&quot;X&quot;</span><span class="s1">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s1">]}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">False,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">oas(X</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">assume_centered=</span><span class="s3">False</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Estimate covariance with the Oracle Approximating Shrinkage as proposed in [1]_. 
 
    Read more in the :ref:`User Guide &lt;shrunk_covariance&gt;`. 
 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
        Data from which to compute the covariance estimate. 
 
    assume_centered : bool, default=False 
      If True, data will not be centered before computation. 
      Useful to work with data whose mean is significantly equal to 
      zero but is not exactly zero. 
      If False, data will be centered before computation. 
 
    Returns 
    ------- 
    shrunk_cov : array-like of shape (n_features, n_features) 
        Shrunk covariance. 
 
    shrinkage : float 
        Coefficient in the convex combination used for the computation 
        of the shrunk estimate. 
 
    Notes 
    ----- 
    The regularised covariance is: 
 
    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features), 
 
    where mu = trace(cov) / n_features and shrinkage is given by the OAS formula 
    (see [1]_). 
 
    The shrinkage formulation implemented here differs from Eq. 23 in [1]_. In 
    the original article, formula (23) states that 2/p (p being the number of 
    features) is multiplied by Trace(cov*cov) in both the numerator and 
    denominator, but this operation is omitted because for a large p, the value 
    of 2/p is so small that it doesn't affect the value of the estimator. 
 
    References 
    ---------- 
    .. [1] :arxiv:`&quot;Shrinkage algorithms for MMSE covariance estimation.&quot;, 
           Chen, Y., Wiesel, A., Eldar, Y. C., &amp; Hero, A. O. 
           IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010. 
           &lt;0907.4698&gt;` 
    &quot;&quot;&quot;</span>
    <span class="s1">estimator = OAS(</span>
        <span class="s1">assume_centered=assume_centered</span><span class="s3">,</span>
    <span class="s1">).fit(X)</span>
    <span class="s3">return </span><span class="s1">estimator.covariance_</span><span class="s3">, </span><span class="s1">estimator.shrinkage_</span>


<span class="s3">class </span><span class="s1">OAS(EmpiricalCovariance):</span>
    <span class="s0">&quot;&quot;&quot;Oracle Approximating Shrinkage Estimator as proposed in [1]_. 
 
    Read more in the :ref:`User Guide &lt;shrunk_covariance&gt;`. 
 
    Parameters 
    ---------- 
    store_precision : bool, default=True 
        Specify if the estimated precision is stored. 
 
    assume_centered : bool, default=False 
        If True, data will not be centered before computation. 
        Useful when working with data whose mean is almost, but not exactly 
        zero. 
        If False (default), data will be centered before computation. 
 
    Attributes 
    ---------- 
    covariance_ : ndarray of shape (n_features, n_features) 
        Estimated covariance matrix. 
 
    location_ : ndarray of shape (n_features,) 
        Estimated location, i.e. the estimated mean. 
 
    precision_ : ndarray of shape (n_features, n_features) 
        Estimated pseudo inverse matrix. 
        (stored only if store_precision is True) 
 
    shrinkage_ : float 
      coefficient in the convex combination used for the computation 
      of the shrunk estimate. Range is [0, 1]. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    EllipticEnvelope : An object for detecting outliers in 
        a Gaussian distributed dataset. 
    EmpiricalCovariance : Maximum likelihood covariance estimator. 
    GraphicalLasso : Sparse inverse covariance estimation 
        with an l1-penalized estimator. 
    GraphicalLassoCV : Sparse inverse covariance with cross-validated 
        choice of the l1 penalty. 
    LedoitWolf : LedoitWolf Estimator. 
    MinCovDet : Minimum Covariance Determinant 
        (robust estimator of covariance). 
    ShrunkCovariance : Covariance estimator with shrinkage. 
 
    Notes 
    ----- 
    The regularised covariance is: 
 
    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features), 
 
    where mu = trace(cov) / n_features and shrinkage is given by the OAS formula 
    (see [1]_). 
 
    The shrinkage formulation implemented here differs from Eq. 23 in [1]_. In 
    the original article, formula (23) states that 2/p (p being the number of 
    features) is multiplied by Trace(cov*cov) in both the numerator and 
    denominator, but this operation is omitted because for a large p, the value 
    of 2/p is so small that it doesn't affect the value of the estimator. 
 
    References 
    ---------- 
    .. [1] :arxiv:`&quot;Shrinkage algorithms for MMSE covariance estimation.&quot;, 
           Chen, Y., Wiesel, A., Eldar, Y. C., &amp; Hero, A. O. 
           IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010. 
           &lt;0907.4698&gt;` 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.covariance import OAS 
    &gt;&gt;&gt; from sklearn.datasets import make_gaussian_quantiles 
    &gt;&gt;&gt; real_cov = np.array([[.8, .3], 
    ...                      [.3, .4]]) 
    &gt;&gt;&gt; rng = np.random.RandomState(0) 
    &gt;&gt;&gt; X = rng.multivariate_normal(mean=[0, 0], 
    ...                             cov=real_cov, 
    ...                             size=500) 
    &gt;&gt;&gt; oas = OAS().fit(X) 
    &gt;&gt;&gt; oas.covariance_ 
    array([[0.7533..., 0.2763...], 
           [0.2763..., 0.3964...]]) 
    &gt;&gt;&gt; oas.precision_ 
    array([[ 1.7833..., -1.2431... ], 
           [-1.2431...,  3.3889...]]) 
    &gt;&gt;&gt; oas.shrinkage_ 
    0.0195... 
    &quot;&quot;&quot;</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the Oracle Approximating Shrinkage covariance model to X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training data, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
        y : Ignored 
            Not used, present for API consistency by convention. 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X)</span>
        <span class="s2"># Not calling the parent object to fit, to avoid computing the</span>
        <span class="s2"># covariance matrix (and potentially the precision)</span>
        <span class="s3">if </span><span class="s1">self.assume_centered:</span>
            <span class="s1">self.location_ = np.zeros(X.shape[</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.location_ = X.mean(</span><span class="s4">0</span><span class="s1">)</span>

        <span class="s1">covariance</span><span class="s3">, </span><span class="s1">shrinkage = _oas(X - self.location_</span><span class="s3">, </span><span class="s1">assume_centered=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">self.shrinkage_ = shrinkage</span>
        <span class="s1">self._set_covariance(covariance)</span>

        <span class="s3">return </span><span class="s1">self</span>
</pre>
</body>
</html>