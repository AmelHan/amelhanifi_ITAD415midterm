<html>
<head>
<title>dogbox.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
dogbox.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Dogleg algorithm with rectangular trust regions for least-squares minimization. 
 
The description of the algorithm can be found in [Voglis]_. The algorithm does 
trust-region iterations, but the shape of trust regions is rectangular as 
opposed to conventional elliptical. The intersection of a trust region and 
an initial feasible region is again some rectangle. Thus, on each iteration a 
bound-constrained quadratic optimization problem is solved. 
 
A quadratic problem is solved by well-known dogleg approach, where the 
function is minimized along piecewise-linear &quot;dogleg&quot; path [NumOpt]_, 
Chapter 4. If Jacobian is not rank-deficient then the function is decreasing 
along this path, and optimization amounts to simply following along this 
path as long as a point stays within the bounds. A constrained Cauchy step 
(along the anti-gradient) is considered for safety in rank deficient cases, 
in this situations the convergence might be slow. 
 
If during iterations some variable hit the initial bound and the component 
of anti-gradient points outside the feasible region, then a next dogleg step 
won't make any progress. At this state such variables satisfy first-order 
optimality conditions and they are excluded before computing a next dogleg 
step. 
 
Gauss-Newton step can be computed exactly by `numpy.linalg.lstsq` (for dense 
Jacobian matrices) or by iterative procedure `scipy.sparse.linalg.lsmr` (for 
dense and sparse matrices, or Jacobian being LinearOperator). The second 
option allows to solve very large problems (up to couple of millions of 
residuals on a regular PC), provided the Jacobian matrix is sufficiently 
sparse. But note that dogbox is not very good for solving problems with 
large number of constraints, because of variables exclusion-inclusion on each 
iteration (a required number of function evaluations might be high or accuracy 
of a solution will be poor), thus its large-scale usage is probably limited 
to unconstrained problems. 
 
References 
---------- 
.. [Voglis] C. Voglis and I. E. Lagaris, &quot;A Rectangular Trust Region Dogleg 
            Approach for Unconstrained and Bound Constrained Nonlinear 
            Optimization&quot;, WSEAS International Conference on Applied 
            Mathematics, Corfu, Greece, 2004. 
.. [NumOpt] J. Nocedal and S. J. Wright, &quot;Numerical optimization, 2nd edition&quot;. 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">numpy.linalg </span><span class="s2">import </span><span class="s1">lstsq</span><span class="s2">, </span><span class="s1">norm</span>

<span class="s2">from </span><span class="s1">scipy.sparse.linalg </span><span class="s2">import </span><span class="s1">LinearOperator</span><span class="s2">, </span><span class="s1">aslinearoperator</span><span class="s2">, </span><span class="s1">lsmr</span>
<span class="s2">from </span><span class="s1">scipy.optimize </span><span class="s2">import </span><span class="s1">OptimizeResult</span>

<span class="s2">from </span><span class="s1">.common </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">step_size_to_bound</span><span class="s2">, </span><span class="s1">in_bounds</span><span class="s2">, </span><span class="s1">update_tr_radius</span><span class="s2">, </span><span class="s1">evaluate_quadratic</span><span class="s2">,</span>
    <span class="s1">build_quadratic_1d</span><span class="s2">, </span><span class="s1">minimize_quadratic_1d</span><span class="s2">, </span><span class="s1">compute_grad</span><span class="s2">,</span>
    <span class="s1">compute_jac_scale</span><span class="s2">, </span><span class="s1">check_termination</span><span class="s2">, </span><span class="s1">scale_for_robust_loss_function</span><span class="s2">,</span>
    <span class="s1">print_header_nonlinear</span><span class="s2">, </span><span class="s1">print_iteration_nonlinear)</span>


<span class="s2">def </span><span class="s1">lsmr_operator(Jop</span><span class="s2">, </span><span class="s1">d</span><span class="s2">, </span><span class="s1">active_set):</span>
    <span class="s0">&quot;&quot;&quot;Compute LinearOperator to use in LSMR by dogbox algorithm. 
 
    `active_set` mask is used to excluded active variables from computations 
    of matrix-vector products. 
    &quot;&quot;&quot;</span>
    <span class="s1">m</span><span class="s2">, </span><span class="s1">n = Jop.shape</span>

    <span class="s2">def </span><span class="s1">matvec(x):</span>
        <span class="s1">x_free = x.ravel().copy()</span>
        <span class="s1">x_free[active_set] = </span><span class="s3">0</span>
        <span class="s2">return </span><span class="s1">Jop.matvec(x * d)</span>

    <span class="s2">def </span><span class="s1">rmatvec(x):</span>
        <span class="s1">r = d * Jop.rmatvec(x)</span>
        <span class="s1">r[active_set] = </span><span class="s3">0</span>
        <span class="s2">return </span><span class="s1">r</span>

    <span class="s2">return </span><span class="s1">LinearOperator((m</span><span class="s2">, </span><span class="s1">n)</span><span class="s2">, </span><span class="s1">matvec=matvec</span><span class="s2">, </span><span class="s1">rmatvec=rmatvec</span><span class="s2">, </span><span class="s1">dtype=float)</span>


<span class="s2">def </span><span class="s1">find_intersection(x</span><span class="s2">, </span><span class="s1">tr_bounds</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">ub):</span>
    <span class="s0">&quot;&quot;&quot;Find intersection of trust-region bounds and initial bounds. 
 
    Returns 
    ------- 
    lb_total, ub_total : ndarray with shape of x 
        Lower and upper bounds of the intersection region. 
    orig_l, orig_u : ndarray of bool with shape of x 
        True means that an original bound is taken as a corresponding bound 
        in the intersection region. 
    tr_l, tr_u : ndarray of bool with shape of x 
        True means that a trust-region bound is taken as a corresponding bound 
        in the intersection region. 
    &quot;&quot;&quot;</span>
    <span class="s1">lb_centered = lb - x</span>
    <span class="s1">ub_centered = ub - x</span>

    <span class="s1">lb_total = np.maximum(lb_centered</span><span class="s2">, </span><span class="s1">-tr_bounds)</span>
    <span class="s1">ub_total = np.minimum(ub_centered</span><span class="s2">, </span><span class="s1">tr_bounds)</span>

    <span class="s1">orig_l = np.equal(lb_total</span><span class="s2">, </span><span class="s1">lb_centered)</span>
    <span class="s1">orig_u = np.equal(ub_total</span><span class="s2">, </span><span class="s1">ub_centered)</span>

    <span class="s1">tr_l = np.equal(lb_total</span><span class="s2">, </span><span class="s1">-tr_bounds)</span>
    <span class="s1">tr_u = np.equal(ub_total</span><span class="s2">, </span><span class="s1">tr_bounds)</span>

    <span class="s2">return </span><span class="s1">lb_total</span><span class="s2">, </span><span class="s1">ub_total</span><span class="s2">, </span><span class="s1">orig_l</span><span class="s2">, </span><span class="s1">orig_u</span><span class="s2">, </span><span class="s1">tr_l</span><span class="s2">, </span><span class="s1">tr_u</span>


<span class="s2">def </span><span class="s1">dogleg_step(x</span><span class="s2">, </span><span class="s1">newton_step</span><span class="s2">, </span><span class="s1">g</span><span class="s2">, </span><span class="s1">a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">tr_bounds</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">ub):</span>
    <span class="s0">&quot;&quot;&quot;Find dogleg step in a rectangular region. 
 
    Returns 
    ------- 
    step : ndarray, shape (n,) 
        Computed dogleg step. 
    bound_hits : ndarray of int, shape (n,) 
        Each component shows whether a corresponding variable hits the 
        initial bound after the step is taken: 
            *  0 - a variable doesn't hit the bound. 
            * -1 - lower bound is hit. 
            *  1 - upper bound is hit. 
    tr_hit : bool 
        Whether the step hit the boundary of the trust-region. 
    &quot;&quot;&quot;</span>
    <span class="s1">lb_total</span><span class="s2">, </span><span class="s1">ub_total</span><span class="s2">, </span><span class="s1">orig_l</span><span class="s2">, </span><span class="s1">orig_u</span><span class="s2">, </span><span class="s1">tr_l</span><span class="s2">, </span><span class="s1">tr_u = find_intersection(</span>
        <span class="s1">x</span><span class="s2">, </span><span class="s1">tr_bounds</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">ub</span>
    <span class="s1">)</span>
    <span class="s1">bound_hits = np.zeros_like(x</span><span class="s2">, </span><span class="s1">dtype=int)</span>

    <span class="s2">if </span><span class="s1">in_bounds(newton_step</span><span class="s2">, </span><span class="s1">lb_total</span><span class="s2">, </span><span class="s1">ub_total):</span>
        <span class="s2">return </span><span class="s1">newton_step</span><span class="s2">, </span><span class="s1">bound_hits</span><span class="s2">, False</span>

    <span class="s1">to_bounds</span><span class="s2">, </span><span class="s1">_ = step_size_to_bound(np.zeros_like(x)</span><span class="s2">, </span><span class="s1">-g</span><span class="s2">, </span><span class="s1">lb_total</span><span class="s2">, </span><span class="s1">ub_total)</span>

    <span class="s4"># The classical dogleg algorithm would check if Cauchy step fits into</span>
    <span class="s4"># the bounds, and just return it constrained version if not. But in a</span>
    <span class="s4"># rectangular trust region it makes sense to try to improve constrained</span>
    <span class="s4"># Cauchy step too. Thus, we don't distinguish these two cases.</span>

    <span class="s1">cauchy_step = -minimize_quadratic_1d(a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s1">to_bounds)[</span><span class="s3">0</span><span class="s1">] * g</span>

    <span class="s1">step_diff = newton_step - cauchy_step</span>
    <span class="s1">step_size</span><span class="s2">, </span><span class="s1">hits = step_size_to_bound(cauchy_step</span><span class="s2">, </span><span class="s1">step_diff</span><span class="s2">,</span>
                                         <span class="s1">lb_total</span><span class="s2">, </span><span class="s1">ub_total)</span>
    <span class="s1">bound_hits[(hits &lt; </span><span class="s3">0</span><span class="s1">) &amp; orig_l] = -</span><span class="s3">1</span>
    <span class="s1">bound_hits[(hits &gt; </span><span class="s3">0</span><span class="s1">) &amp; orig_u] = </span><span class="s3">1</span>
    <span class="s1">tr_hit = np.any((hits &lt; </span><span class="s3">0</span><span class="s1">) &amp; tr_l | (hits &gt; </span><span class="s3">0</span><span class="s1">) &amp; tr_u)</span>

    <span class="s2">return </span><span class="s1">cauchy_step + step_size * step_diff</span><span class="s2">, </span><span class="s1">bound_hits</span><span class="s2">, </span><span class="s1">tr_hit</span>


<span class="s2">def </span><span class="s1">dogbox(fun</span><span class="s2">, </span><span class="s1">jac</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">f0</span><span class="s2">, </span><span class="s1">J0</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">ub</span><span class="s2">, </span><span class="s1">ftol</span><span class="s2">, </span><span class="s1">xtol</span><span class="s2">, </span><span class="s1">gtol</span><span class="s2">, </span><span class="s1">max_nfev</span><span class="s2">, </span><span class="s1">x_scale</span><span class="s2">,</span>
           <span class="s1">loss_function</span><span class="s2">, </span><span class="s1">tr_solver</span><span class="s2">, </span><span class="s1">tr_options</span><span class="s2">, </span><span class="s1">verbose):</span>
    <span class="s1">f = f0</span>
    <span class="s1">f_true = f.copy()</span>
    <span class="s1">nfev = </span><span class="s3">1</span>

    <span class="s1">J = J0</span>
    <span class="s1">njev = </span><span class="s3">1</span>

    <span class="s2">if </span><span class="s1">loss_function </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">rho = loss_function(f)</span>
        <span class="s1">cost = </span><span class="s3">0.5 </span><span class="s1">* np.sum(rho[</span><span class="s3">0</span><span class="s1">])</span>
        <span class="s1">J</span><span class="s2">, </span><span class="s1">f = scale_for_robust_loss_function(J</span><span class="s2">, </span><span class="s1">f</span><span class="s2">, </span><span class="s1">rho)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">cost = </span><span class="s3">0.5 </span><span class="s1">* np.dot(f</span><span class="s2">, </span><span class="s1">f)</span>

    <span class="s1">g = compute_grad(J</span><span class="s2">, </span><span class="s1">f)</span>

    <span class="s1">jac_scale = isinstance(x_scale</span><span class="s2">, </span><span class="s1">str) </span><span class="s2">and </span><span class="s1">x_scale == </span><span class="s5">'jac'</span>
    <span class="s2">if </span><span class="s1">jac_scale:</span>
        <span class="s1">scale</span><span class="s2">, </span><span class="s1">scale_inv = compute_jac_scale(J)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">scale</span><span class="s2">, </span><span class="s1">scale_inv = x_scale</span><span class="s2">, </span><span class="s3">1 </span><span class="s1">/ x_scale</span>

    <span class="s1">Delta = norm(x0 * scale_inv</span><span class="s2">, </span><span class="s1">ord=np.inf)</span>
    <span class="s2">if </span><span class="s1">Delta == </span><span class="s3">0</span><span class="s1">:</span>
        <span class="s1">Delta = </span><span class="s3">1.0</span>

    <span class="s1">on_bound = np.zeros_like(x0</span><span class="s2">, </span><span class="s1">dtype=int)</span>
    <span class="s1">on_bound[np.equal(x0</span><span class="s2">, </span><span class="s1">lb)] = -</span><span class="s3">1</span>
    <span class="s1">on_bound[np.equal(x0</span><span class="s2">, </span><span class="s1">ub)] = </span><span class="s3">1</span>

    <span class="s1">x = x0</span>
    <span class="s1">step = np.empty_like(x0)</span>

    <span class="s2">if </span><span class="s1">max_nfev </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">max_nfev = x0.size * </span><span class="s3">100</span>

    <span class="s1">termination_status = </span><span class="s2">None</span>
    <span class="s1">iteration = </span><span class="s3">0</span>
    <span class="s1">step_norm = </span><span class="s2">None</span>
    <span class="s1">actual_reduction = </span><span class="s2">None</span>

    <span class="s2">if </span><span class="s1">verbose == </span><span class="s3">2</span><span class="s1">:</span>
        <span class="s1">print_header_nonlinear()</span>

    <span class="s2">while True</span><span class="s1">:</span>
        <span class="s1">active_set = on_bound * g &lt; </span><span class="s3">0</span>
        <span class="s1">free_set = ~active_set</span>

        <span class="s1">g_free = g[free_set]</span>
        <span class="s1">g_full = g.copy()</span>
        <span class="s1">g[active_set] = </span><span class="s3">0</span>

        <span class="s1">g_norm = norm(g</span><span class="s2">, </span><span class="s1">ord=np.inf)</span>
        <span class="s2">if </span><span class="s1">g_norm &lt; gtol:</span>
            <span class="s1">termination_status = </span><span class="s3">1</span>

        <span class="s2">if </span><span class="s1">verbose == </span><span class="s3">2</span><span class="s1">:</span>
            <span class="s1">print_iteration_nonlinear(iteration</span><span class="s2">, </span><span class="s1">nfev</span><span class="s2">, </span><span class="s1">cost</span><span class="s2">, </span><span class="s1">actual_reduction</span><span class="s2">,</span>
                                      <span class="s1">step_norm</span><span class="s2">, </span><span class="s1">g_norm)</span>

        <span class="s2">if </span><span class="s1">termination_status </span><span class="s2">is not None or </span><span class="s1">nfev == max_nfev:</span>
            <span class="s2">break</span>

        <span class="s1">x_free = x[free_set]</span>
        <span class="s1">lb_free = lb[free_set]</span>
        <span class="s1">ub_free = ub[free_set]</span>
        <span class="s1">scale_free = scale[free_set]</span>

        <span class="s4"># Compute (Gauss-)Newton and build quadratic model for Cauchy step.</span>
        <span class="s2">if </span><span class="s1">tr_solver == </span><span class="s5">'exact'</span><span class="s1">:</span>
            <span class="s1">J_free = J[:</span><span class="s2">, </span><span class="s1">free_set]</span>
            <span class="s1">newton_step = lstsq(J_free</span><span class="s2">, </span><span class="s1">-f</span><span class="s2">, </span><span class="s1">rcond=-</span><span class="s3">1</span><span class="s1">)[</span><span class="s3">0</span><span class="s1">]</span>

            <span class="s4"># Coefficients for the quadratic model along the anti-gradient.</span>
            <span class="s1">a</span><span class="s2">, </span><span class="s1">b = build_quadratic_1d(J_free</span><span class="s2">, </span><span class="s1">g_free</span><span class="s2">, </span><span class="s1">-g_free)</span>
        <span class="s2">elif </span><span class="s1">tr_solver == </span><span class="s5">'lsmr'</span><span class="s1">:</span>
            <span class="s1">Jop = aslinearoperator(J)</span>

            <span class="s4"># We compute lsmr step in scaled variables and then</span>
            <span class="s4"># transform back to normal variables, if lsmr would give exact lsq</span>
            <span class="s4"># solution, this would be equivalent to not doing any</span>
            <span class="s4"># transformations, but from experience it's better this way.</span>

            <span class="s4"># We pass active_set to make computations as if we selected</span>
            <span class="s4"># the free subset of J columns, but without actually doing any</span>
            <span class="s4"># slicing, which is expensive for sparse matrices and impossible</span>
            <span class="s4"># for LinearOperator.</span>

            <span class="s1">lsmr_op = lsmr_operator(Jop</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">active_set)</span>
            <span class="s1">newton_step = -lsmr(lsmr_op</span><span class="s2">, </span><span class="s1">f</span><span class="s2">, </span><span class="s1">**tr_options)[</span><span class="s3">0</span><span class="s1">][free_set]</span>
            <span class="s1">newton_step *= scale_free</span>

            <span class="s4"># Components of g for active variables were zeroed, so this call</span>
            <span class="s4"># is correct and equivalent to using J_free and g_free.</span>
            <span class="s1">a</span><span class="s2">, </span><span class="s1">b = build_quadratic_1d(Jop</span><span class="s2">, </span><span class="s1">g</span><span class="s2">, </span><span class="s1">-g)</span>

        <span class="s1">actual_reduction = -</span><span class="s3">1.0</span>
        <span class="s2">while </span><span class="s1">actual_reduction &lt;= </span><span class="s3">0 </span><span class="s2">and </span><span class="s1">nfev &lt; max_nfev:</span>
            <span class="s1">tr_bounds = Delta * scale_free</span>

            <span class="s1">step_free</span><span class="s2">, </span><span class="s1">on_bound_free</span><span class="s2">, </span><span class="s1">tr_hit = dogleg_step(</span>
                <span class="s1">x_free</span><span class="s2">, </span><span class="s1">newton_step</span><span class="s2">, </span><span class="s1">g_free</span><span class="s2">, </span><span class="s1">a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">tr_bounds</span><span class="s2">, </span><span class="s1">lb_free</span><span class="s2">, </span><span class="s1">ub_free)</span>

            <span class="s1">step.fill(</span><span class="s3">0.0</span><span class="s1">)</span>
            <span class="s1">step[free_set] = step_free</span>

            <span class="s2">if </span><span class="s1">tr_solver == </span><span class="s5">'exact'</span><span class="s1">:</span>
                <span class="s1">predicted_reduction = -evaluate_quadratic(J_free</span><span class="s2">, </span><span class="s1">g_free</span><span class="s2">,</span>
                                                          <span class="s1">step_free)</span>
            <span class="s2">elif </span><span class="s1">tr_solver == </span><span class="s5">'lsmr'</span><span class="s1">:</span>
                <span class="s1">predicted_reduction = -evaluate_quadratic(Jop</span><span class="s2">, </span><span class="s1">g</span><span class="s2">, </span><span class="s1">step)</span>

            <span class="s4"># gh11403 ensure that solution is fully within bounds.</span>
            <span class="s1">x_new = np.clip(x + step</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">ub)</span>

            <span class="s1">f_new = fun(x_new)</span>
            <span class="s1">nfev += </span><span class="s3">1</span>

            <span class="s1">step_h_norm = norm(step * scale_inv</span><span class="s2">, </span><span class="s1">ord=np.inf)</span>

            <span class="s2">if not </span><span class="s1">np.all(np.isfinite(f_new)):</span>
                <span class="s1">Delta = </span><span class="s3">0.25 </span><span class="s1">* step_h_norm</span>
                <span class="s2">continue</span>

            <span class="s4"># Usual trust-region step quality estimation.</span>
            <span class="s2">if </span><span class="s1">loss_function </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">cost_new = loss_function(f_new</span><span class="s2">, </span><span class="s1">cost_only=</span><span class="s2">True</span><span class="s1">)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">cost_new = </span><span class="s3">0.5 </span><span class="s1">* np.dot(f_new</span><span class="s2">, </span><span class="s1">f_new)</span>
            <span class="s1">actual_reduction = cost - cost_new</span>

            <span class="s1">Delta</span><span class="s2">, </span><span class="s1">ratio = update_tr_radius(</span>
                <span class="s1">Delta</span><span class="s2">, </span><span class="s1">actual_reduction</span><span class="s2">, </span><span class="s1">predicted_reduction</span><span class="s2">,</span>
                <span class="s1">step_h_norm</span><span class="s2">, </span><span class="s1">tr_hit</span>
            <span class="s1">)</span>

            <span class="s1">step_norm = norm(step)</span>
            <span class="s1">termination_status = check_termination(</span>
                <span class="s1">actual_reduction</span><span class="s2">, </span><span class="s1">cost</span><span class="s2">, </span><span class="s1">step_norm</span><span class="s2">, </span><span class="s1">norm(x)</span><span class="s2">, </span><span class="s1">ratio</span><span class="s2">, </span><span class="s1">ftol</span><span class="s2">, </span><span class="s1">xtol)</span>

            <span class="s2">if </span><span class="s1">termination_status </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s2">break</span>

        <span class="s2">if </span><span class="s1">actual_reduction &gt; </span><span class="s3">0</span><span class="s1">:</span>
            <span class="s1">on_bound[free_set] = on_bound_free</span>

            <span class="s1">x = x_new</span>
            <span class="s4"># Set variables exactly at the boundary.</span>
            <span class="s1">mask = on_bound == -</span><span class="s3">1</span>
            <span class="s1">x[mask] = lb[mask]</span>
            <span class="s1">mask = on_bound == </span><span class="s3">1</span>
            <span class="s1">x[mask] = ub[mask]</span>

            <span class="s1">f = f_new</span>
            <span class="s1">f_true = f.copy()</span>

            <span class="s1">cost = cost_new</span>

            <span class="s1">J = jac(x</span><span class="s2">, </span><span class="s1">f)</span>
            <span class="s1">njev += </span><span class="s3">1</span>

            <span class="s2">if </span><span class="s1">loss_function </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">rho = loss_function(f)</span>
                <span class="s1">J</span><span class="s2">, </span><span class="s1">f = scale_for_robust_loss_function(J</span><span class="s2">, </span><span class="s1">f</span><span class="s2">, </span><span class="s1">rho)</span>

            <span class="s1">g = compute_grad(J</span><span class="s2">, </span><span class="s1">f)</span>

            <span class="s2">if </span><span class="s1">jac_scale:</span>
                <span class="s1">scale</span><span class="s2">, </span><span class="s1">scale_inv = compute_jac_scale(J</span><span class="s2">, </span><span class="s1">scale_inv)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">step_norm = </span><span class="s3">0</span>
            <span class="s1">actual_reduction = </span><span class="s3">0</span>

        <span class="s1">iteration += </span><span class="s3">1</span>

    <span class="s2">if </span><span class="s1">termination_status </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">termination_status = </span><span class="s3">0</span>

    <span class="s2">return </span><span class="s1">OptimizeResult(</span>
        <span class="s1">x=x</span><span class="s2">, </span><span class="s1">cost=cost</span><span class="s2">, </span><span class="s1">fun=f_true</span><span class="s2">, </span><span class="s1">jac=J</span><span class="s2">, </span><span class="s1">grad=g_full</span><span class="s2">, </span><span class="s1">optimality=g_norm</span><span class="s2">,</span>
        <span class="s1">active_mask=on_bound</span><span class="s2">, </span><span class="s1">nfev=nfev</span><span class="s2">, </span><span class="s1">njev=njev</span><span class="s2">, </span><span class="s1">status=termination_status)</span>
</pre>
</body>
</html>