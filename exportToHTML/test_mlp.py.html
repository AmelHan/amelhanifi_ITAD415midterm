<html>
<head>
<title>test_mlp.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_mlp.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Testing for Multi-layer Perceptron module (sklearn.neural_network) 
&quot;&quot;&quot;</span>

<span class="s2"># Author: Issam H. Laradji</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">re</span>
<span class="s3">import </span><span class="s1">sys</span>
<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">io </span><span class="s3">import </span><span class="s1">StringIO</span>

<span class="s3">import </span><span class="s1">joblib</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">pytest</span>
<span class="s3">from </span><span class="s1">numpy.testing </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s3">,</span>
    <span class="s1">assert_almost_equal</span><span class="s3">,</span>
    <span class="s1">assert_array_equal</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">scipy.sparse </span><span class="s3">import </span><span class="s1">csr_matrix</span>

<span class="s3">from </span><span class="s1">sklearn.datasets </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">load_digits</span><span class="s3">,</span>
    <span class="s1">load_iris</span><span class="s3">,</span>
    <span class="s1">make_multilabel_classification</span><span class="s3">,</span>
    <span class="s1">make_regression</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">sklearn.exceptions </span><span class="s3">import </span><span class="s1">ConvergenceWarning</span>
<span class="s3">from </span><span class="s1">sklearn.metrics </span><span class="s3">import </span><span class="s1">roc_auc_score</span>
<span class="s3">from </span><span class="s1">sklearn.neural_network </span><span class="s3">import </span><span class="s1">MLPClassifier</span><span class="s3">, </span><span class="s1">MLPRegressor</span>
<span class="s3">from </span><span class="s1">sklearn.preprocessing </span><span class="s3">import </span><span class="s1">LabelBinarizer</span><span class="s3">, </span><span class="s1">MinMaxScaler</span><span class="s3">, </span><span class="s1">scale</span>
<span class="s3">from </span><span class="s1">sklearn.utils._testing </span><span class="s3">import </span><span class="s1">ignore_warnings</span>

<span class="s1">ACTIVATION_TYPES = [</span><span class="s4">&quot;identity&quot;</span><span class="s3">, </span><span class="s4">&quot;logistic&quot;</span><span class="s3">, </span><span class="s4">&quot;tanh&quot;</span><span class="s3">, </span><span class="s4">&quot;relu&quot;</span><span class="s1">]</span>

<span class="s1">X_digits</span><span class="s3">, </span><span class="s1">y_digits = load_digits(n_class=</span><span class="s5">3</span><span class="s3">, </span><span class="s1">return_X_y=</span><span class="s3">True</span><span class="s1">)</span>

<span class="s1">X_digits_multi = MinMaxScaler().fit_transform(X_digits[:</span><span class="s5">200</span><span class="s1">])</span>
<span class="s1">y_digits_multi = y_digits[:</span><span class="s5">200</span><span class="s1">]</span>

<span class="s1">X_digits</span><span class="s3">, </span><span class="s1">y_digits = load_digits(n_class=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">return_X_y=</span><span class="s3">True</span><span class="s1">)</span>

<span class="s1">X_digits_binary = MinMaxScaler().fit_transform(X_digits[:</span><span class="s5">200</span><span class="s1">])</span>
<span class="s1">y_digits_binary = y_digits[:</span><span class="s5">200</span><span class="s1">]</span>

<span class="s1">classification_datasets = [</span>
    <span class="s1">(X_digits_multi</span><span class="s3">, </span><span class="s1">y_digits_multi)</span><span class="s3">,</span>
    <span class="s1">(X_digits_binary</span><span class="s3">, </span><span class="s1">y_digits_binary)</span><span class="s3">,</span>
<span class="s1">]</span>

<span class="s1">X_reg</span><span class="s3">, </span><span class="s1">y_reg = make_regression(</span>
    <span class="s1">n_samples=</span><span class="s5">200</span><span class="s3">, </span><span class="s1">n_features=</span><span class="s5">10</span><span class="s3">, </span><span class="s1">bias=</span><span class="s5">20.0</span><span class="s3">, </span><span class="s1">noise=</span><span class="s5">100.0</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">7</span>
<span class="s1">)</span>
<span class="s1">y_reg = scale(y_reg)</span>
<span class="s1">regression_datasets = [(X_reg</span><span class="s3">, </span><span class="s1">y_reg)]</span>

<span class="s1">iris = load_iris()</span>

<span class="s1">X_iris = iris.data</span>
<span class="s1">y_iris = iris.target</span>


<span class="s3">def </span><span class="s1">test_alpha():</span>
    <span class="s2"># Test that larger alpha yields weights closer to zero</span>
    <span class="s1">X = X_digits_binary[:</span><span class="s5">100</span><span class="s1">]</span>
    <span class="s1">y = y_digits_binary[:</span><span class="s5">100</span><span class="s1">]</span>

    <span class="s1">alpha_vectors = []</span>
    <span class="s1">alpha_values = np.arange(</span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">absolute_sum = </span><span class="s3">lambda </span><span class="s1">x: np.sum(np.abs(x))</span>

    <span class="s3">for </span><span class="s1">alpha </span><span class="s3">in </span><span class="s1">alpha_values:</span>
        <span class="s1">mlp = MLPClassifier(hidden_layer_sizes=</span><span class="s5">10</span><span class="s3">, </span><span class="s1">alpha=alpha</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s3">with </span><span class="s1">ignore_warnings(category=ConvergenceWarning):</span>
            <span class="s1">mlp.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s1">alpha_vectors.append(</span>
            <span class="s1">np.array([absolute_sum(mlp.coefs_[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">, </span><span class="s1">absolute_sum(mlp.coefs_[</span><span class="s5">1</span><span class="s1">])])</span>
        <span class="s1">)</span>

    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(alpha_values) - </span><span class="s5">1</span><span class="s1">):</span>
        <span class="s3">assert </span><span class="s1">(alpha_vectors[i] &gt; alpha_vectors[i + </span><span class="s5">1</span><span class="s1">]).all()</span>


<span class="s3">def </span><span class="s1">test_fit():</span>
    <span class="s2"># Test that the algorithm solution is equal to a worked out example.</span>
    <span class="s1">X = np.array([[</span><span class="s5">0.6</span><span class="s3">, </span><span class="s5">0.8</span><span class="s3">, </span><span class="s5">0.7</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s1">mlp = MLPClassifier(</span>
        <span class="s1">solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s3">,</span>
        <span class="s1">learning_rate_init=</span><span class="s5">0.1</span><span class="s3">,</span>
        <span class="s1">alpha=</span><span class="s5">0.1</span><span class="s3">,</span>
        <span class="s1">activation=</span><span class="s4">&quot;logistic&quot;</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s5">1</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s5">1</span><span class="s3">,</span>
        <span class="s1">hidden_layer_sizes=</span><span class="s5">2</span><span class="s3">,</span>
        <span class="s1">momentum=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s2"># set weights</span>
    <span class="s1">mlp.coefs_ = [</span><span class="s5">0</span><span class="s1">] * </span><span class="s5">2</span>
    <span class="s1">mlp.intercepts_ = [</span><span class="s5">0</span><span class="s1">] * </span><span class="s5">2</span>
    <span class="s1">mlp.n_outputs_ = </span><span class="s5">1</span>
    <span class="s1">mlp.coefs_[</span><span class="s5">0</span><span class="s1">] = np.array([[</span><span class="s5">0.1</span><span class="s3">, </span><span class="s5">0.2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0.3</span><span class="s3">, </span><span class="s5">0.1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0.5</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]])</span>
    <span class="s1">mlp.coefs_[</span><span class="s5">1</span><span class="s1">] = np.array([[</span><span class="s5">0.1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0.2</span><span class="s1">]])</span>
    <span class="s1">mlp.intercepts_[</span><span class="s5">0</span><span class="s1">] = np.array([</span><span class="s5">0.1</span><span class="s3">, </span><span class="s5">0.1</span><span class="s1">])</span>
    <span class="s1">mlp.intercepts_[</span><span class="s5">1</span><span class="s1">] = np.array([</span><span class="s5">1.0</span><span class="s1">])</span>
    <span class="s1">mlp._coef_grads = [] * </span><span class="s5">2</span>
    <span class="s1">mlp._intercept_grads = [] * </span><span class="s5">2</span>
    <span class="s1">mlp.n_features_in_ = </span><span class="s5">3</span>

    <span class="s2"># Initialize parameters</span>
    <span class="s1">mlp.n_iter_ = </span><span class="s5">0</span>
    <span class="s1">mlp.learning_rate_ = </span><span class="s5">0.1</span>

    <span class="s2"># Compute the number of layers</span>
    <span class="s1">mlp.n_layers_ = </span><span class="s5">3</span>

    <span class="s2"># Pre-allocate gradient matrices</span>
    <span class="s1">mlp._coef_grads = [</span><span class="s5">0</span><span class="s1">] * (mlp.n_layers_ - </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">mlp._intercept_grads = [</span><span class="s5">0</span><span class="s1">] * (mlp.n_layers_ - </span><span class="s5">1</span><span class="s1">)</span>

    <span class="s1">mlp.out_activation_ = </span><span class="s4">&quot;logistic&quot;</span>
    <span class="s1">mlp.t_ = </span><span class="s5">0</span>
    <span class="s1">mlp.best_loss_ = np.inf</span>
    <span class="s1">mlp.loss_curve_ = []</span>
    <span class="s1">mlp._no_improvement_count = </span><span class="s5">0</span>
    <span class="s1">mlp._intercept_velocity = [</span>
        <span class="s1">np.zeros_like(intercepts) </span><span class="s3">for </span><span class="s1">intercepts </span><span class="s3">in </span><span class="s1">mlp.intercepts_</span>
    <span class="s1">]</span>
    <span class="s1">mlp._coef_velocity = [np.zeros_like(coefs) </span><span class="s3">for </span><span class="s1">coefs </span><span class="s3">in </span><span class="s1">mlp.coefs_]</span>

    <span class="s1">mlp.partial_fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">classes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">])</span>
    <span class="s2"># Manually worked out example</span>
    <span class="s2"># h1 = g(X1 * W_i1 + b11) = g(0.6 * 0.1 + 0.8 * 0.3 + 0.7 * 0.5 + 0.1)</span>
    <span class="s2">#       =  0.679178699175393</span>
    <span class="s2"># h2 = g(X2 * W_i2 + b12) = g(0.6 * 0.2 + 0.8 * 0.1 + 0.7 * 0 + 0.1)</span>
    <span class="s2">#         = 0.574442516811659</span>
    <span class="s2"># o1 = g(h * W2 + b21) = g(0.679 * 0.1 + 0.574 * 0.2 + 1)</span>
    <span class="s2">#       = 0.7654329236196236</span>
    <span class="s2"># d21 = -(0 - 0.765) = 0.765</span>
    <span class="s2"># d11 = (1 - 0.679) * 0.679 * 0.765 * 0.1 = 0.01667</span>
    <span class="s2"># d12 = (1 - 0.574) * 0.574 * 0.765 * 0.2 = 0.0374</span>
    <span class="s2"># W1grad11 = X1 * d11 + alpha * W11 = 0.6 * 0.01667 + 0.1 * 0.1 = 0.0200</span>
    <span class="s2"># W1grad11 = X1 * d12 + alpha * W12 = 0.6 * 0.0374 + 0.1 * 0.2 = 0.04244</span>
    <span class="s2"># W1grad21 = X2 * d11 + alpha * W13 = 0.8 * 0.01667 + 0.1 * 0.3 = 0.043336</span>
    <span class="s2"># W1grad22 = X2 * d12 + alpha * W14 = 0.8 * 0.0374 + 0.1 * 0.1 = 0.03992</span>
    <span class="s2"># W1grad31 = X3 * d11 + alpha * W15 = 0.6 * 0.01667 + 0.1 * 0.5 = 0.060002</span>
    <span class="s2"># W1grad32 = X3 * d12 + alpha * W16 = 0.6 * 0.0374 + 0.1 * 0 = 0.02244</span>
    <span class="s2"># W2grad1 = h1 * d21 + alpha * W21 = 0.679 * 0.765 + 0.1 * 0.1 = 0.5294</span>
    <span class="s2"># W2grad2 = h2 * d21 + alpha * W22 = 0.574 * 0.765 + 0.1 * 0.2 = 0.45911</span>
    <span class="s2"># b1grad1 = d11 = 0.01667</span>
    <span class="s2"># b1grad2 = d12 = 0.0374</span>
    <span class="s2"># b2grad = d21 = 0.765</span>
    <span class="s2"># W1 = W1 - eta * [W1grad11, .., W1grad32] = [[0.1, 0.2], [0.3, 0.1],</span>
    <span class="s2">#          [0.5, 0]] - 0.1 * [[0.0200, 0.04244], [0.043336, 0.03992],</span>
    <span class="s2">#          [0.060002, 0.02244]] = [[0.098, 0.195756], [0.2956664,</span>
    <span class="s2">#          0.096008], [0.4939998, -0.002244]]</span>
    <span class="s2"># W2 = W2 - eta * [W2grad1, W2grad2] = [[0.1], [0.2]] - 0.1 *</span>
    <span class="s2">#        [[0.5294], [0.45911]] = [[0.04706], [0.154089]]</span>
    <span class="s2"># b1 = b1 - eta * [b1grad1, b1grad2] = 0.1 - 0.1 * [0.01667, 0.0374]</span>
    <span class="s2">#         = [0.098333, 0.09626]</span>
    <span class="s2"># b2 = b2 - eta * b2grad = 1.0 - 0.1 * 0.765 = 0.9235</span>
    <span class="s1">assert_almost_equal(</span>
        <span class="s1">mlp.coefs_[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">np.array([[</span><span class="s5">0.098</span><span class="s3">, </span><span class="s5">0.195756</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0.2956664</span><span class="s3">, </span><span class="s5">0.096008</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0.4939998</span><span class="s3">, </span><span class="s1">-</span><span class="s5">0.002244</span><span class="s1">]])</span><span class="s3">,</span>
        <span class="s1">decimal=</span><span class="s5">3</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_almost_equal(mlp.coefs_[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">np.array([[</span><span class="s5">0.04706</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">0.154089</span><span class="s1">]])</span><span class="s3">, </span><span class="s1">decimal=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(mlp.intercepts_[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">np.array([</span><span class="s5">0.098333</span><span class="s3">, </span><span class="s5">0.09626</span><span class="s1">])</span><span class="s3">, </span><span class="s1">decimal=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(mlp.intercepts_[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">np.array(</span><span class="s5">0.9235</span><span class="s1">)</span><span class="s3">, </span><span class="s1">decimal=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s2"># Testing output</span>
    <span class="s2">#  h1 = g(X1 * W_i1 + b11) = g(0.6 * 0.098 + 0.8 * 0.2956664 +</span>
    <span class="s2">#               0.7 * 0.4939998 + 0.098333) = 0.677</span>
    <span class="s2">#  h2 = g(X2 * W_i2 + b12) = g(0.6 * 0.195756 + 0.8 * 0.096008 +</span>
    <span class="s2">#            0.7 * -0.002244 + 0.09626) = 0.572</span>
    <span class="s2">#  o1 = h * W2 + b21 = 0.677 * 0.04706 +</span>
    <span class="s2">#             0.572 * 0.154089 + 0.9235 = 1.043</span>
    <span class="s2">#  prob = sigmoid(o1) = 0.739</span>
    <span class="s1">assert_almost_equal(mlp.predict_proba(X)[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s5">0.739</span><span class="s3">, </span><span class="s1">decimal=</span><span class="s5">3</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_gradient():</span>
    <span class="s2"># Test gradient.</span>

    <span class="s2"># This makes sure that the activation functions and their derivatives</span>
    <span class="s2"># are correct. The numerical and analytical computation of the gradient</span>
    <span class="s2"># should be close.</span>
    <span class="s3">for </span><span class="s1">n_labels </span><span class="s3">in </span><span class="s1">[</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">]:</span>
        <span class="s1">n_samples = </span><span class="s5">5</span>
        <span class="s1">n_features = </span><span class="s5">10</span>
        <span class="s1">random_state = np.random.RandomState(seed=</span><span class="s5">42</span><span class="s1">)</span>
        <span class="s1">X = random_state.rand(n_samples</span><span class="s3">, </span><span class="s1">n_features)</span>
        <span class="s1">y = </span><span class="s5">1 </span><span class="s1">+ np.mod(np.arange(n_samples) + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">n_labels)</span>
        <span class="s1">Y = LabelBinarizer().fit_transform(y)</span>

        <span class="s3">for </span><span class="s1">activation </span><span class="s3">in </span><span class="s1">ACTIVATION_TYPES:</span>
            <span class="s1">mlp = MLPClassifier(</span>
                <span class="s1">activation=activation</span><span class="s3">,</span>
                <span class="s1">hidden_layer_sizes=</span><span class="s5">10</span><span class="s3">,</span>
                <span class="s1">solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">,</span>
                <span class="s1">alpha=</span><span class="s5">1e-5</span><span class="s3">,</span>
                <span class="s1">learning_rate_init=</span><span class="s5">0.2</span><span class="s3">,</span>
                <span class="s1">max_iter=</span><span class="s5">1</span><span class="s3">,</span>
                <span class="s1">random_state=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">mlp.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

            <span class="s1">theta = np.hstack([l.ravel() </span><span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">mlp.coefs_ + mlp.intercepts_])</span>

            <span class="s1">layer_units = [X.shape[</span><span class="s5">1</span><span class="s1">]] + [mlp.hidden_layer_sizes] + [mlp.n_outputs_]</span>

            <span class="s1">activations = []</span>
            <span class="s1">deltas = []</span>
            <span class="s1">coef_grads = []</span>
            <span class="s1">intercept_grads = []</span>

            <span class="s1">activations.append(X)</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(mlp.n_layers_ - </span><span class="s5">1</span><span class="s1">):</span>
                <span class="s1">activations.append(np.empty((X.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">layer_units[i + </span><span class="s5">1</span><span class="s1">])))</span>
                <span class="s1">deltas.append(np.empty((X.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">layer_units[i + </span><span class="s5">1</span><span class="s1">])))</span>

                <span class="s1">fan_in = layer_units[i]</span>
                <span class="s1">fan_out = layer_units[i + </span><span class="s5">1</span><span class="s1">]</span>
                <span class="s1">coef_grads.append(np.empty((fan_in</span><span class="s3">, </span><span class="s1">fan_out)))</span>
                <span class="s1">intercept_grads.append(np.empty(fan_out))</span>

            <span class="s2"># analytically compute the gradients</span>
            <span class="s3">def </span><span class="s1">loss_grad_fun(t):</span>
                <span class="s3">return </span><span class="s1">mlp._loss_grad_lbfgs(</span>
                    <span class="s1">t</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">Y</span><span class="s3">, </span><span class="s1">activations</span><span class="s3">, </span><span class="s1">deltas</span><span class="s3">, </span><span class="s1">coef_grads</span><span class="s3">, </span><span class="s1">intercept_grads</span>
                <span class="s1">)</span>

            <span class="s1">[value</span><span class="s3">, </span><span class="s1">grad] = loss_grad_fun(theta)</span>
            <span class="s1">numgrad = np.zeros(np.size(theta))</span>
            <span class="s1">n = np.size(theta</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
            <span class="s1">E = np.eye(n)</span>
            <span class="s1">epsilon = </span><span class="s5">1e-5</span>
            <span class="s2"># numerically compute the gradients</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n):</span>
                <span class="s1">dtheta = E[:</span><span class="s3">, </span><span class="s1">i] * epsilon</span>
                <span class="s1">numgrad[i] = (</span>
                    <span class="s1">loss_grad_fun(theta + dtheta)[</span><span class="s5">0</span><span class="s1">] - loss_grad_fun(theta - dtheta)[</span><span class="s5">0</span><span class="s1">]</span>
                <span class="s1">) / (epsilon * </span><span class="s5">2.0</span><span class="s1">)</span>
            <span class="s1">assert_almost_equal(numgrad</span><span class="s3">, </span><span class="s1">grad)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;X,y&quot;</span><span class="s3">, </span><span class="s1">classification_datasets)</span>
<span class="s3">def </span><span class="s1">test_lbfgs_classification(X</span><span class="s3">, </span><span class="s1">y):</span>
    <span class="s2"># Test lbfgs on classification.</span>
    <span class="s2"># It should achieve a score higher than 0.95 for the binary and multi-class</span>
    <span class="s2"># versions of the digits dataset.</span>
    <span class="s1">X_train = X[:</span><span class="s5">150</span><span class="s1">]</span>
    <span class="s1">y_train = y[:</span><span class="s5">150</span><span class="s1">]</span>
    <span class="s1">X_test = X[</span><span class="s5">150</span><span class="s1">:]</span>
    <span class="s1">expected_shape_dtype = (X_test.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">y_train.dtype.kind)</span>

    <span class="s3">for </span><span class="s1">activation </span><span class="s3">in </span><span class="s1">ACTIVATION_TYPES:</span>
        <span class="s1">mlp = MLPClassifier(</span>
            <span class="s1">solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">,</span>
            <span class="s1">hidden_layer_sizes=</span><span class="s5">50</span><span class="s3">,</span>
            <span class="s1">max_iter=</span><span class="s5">150</span><span class="s3">,</span>
            <span class="s1">shuffle=</span><span class="s3">True,</span>
            <span class="s1">random_state=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">activation=activation</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">mlp.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
        <span class="s1">y_predict = mlp.predict(X_test)</span>
        <span class="s3">assert </span><span class="s1">mlp.score(X_train</span><span class="s3">, </span><span class="s1">y_train) &gt; </span><span class="s5">0.95</span>
        <span class="s3">assert </span><span class="s1">(y_predict.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">y_predict.dtype.kind) == expected_shape_dtype</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;X,y&quot;</span><span class="s3">, </span><span class="s1">regression_datasets)</span>
<span class="s3">def </span><span class="s1">test_lbfgs_regression(X</span><span class="s3">, </span><span class="s1">y):</span>
    <span class="s2"># Test lbfgs on the regression dataset.</span>
    <span class="s3">for </span><span class="s1">activation </span><span class="s3">in </span><span class="s1">ACTIVATION_TYPES:</span>
        <span class="s1">mlp = MLPRegressor(</span>
            <span class="s1">solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">,</span>
            <span class="s1">hidden_layer_sizes=</span><span class="s5">50</span><span class="s3">,</span>
            <span class="s1">max_iter=</span><span class="s5">150</span><span class="s3">,</span>
            <span class="s1">shuffle=</span><span class="s3">True,</span>
            <span class="s1">random_state=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">activation=activation</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">mlp.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s3">if </span><span class="s1">activation == </span><span class="s4">&quot;identity&quot;</span><span class="s1">:</span>
            <span class="s3">assert </span><span class="s1">mlp.score(X</span><span class="s3">, </span><span class="s1">y) &gt; </span><span class="s5">0.80</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># Non linear models perform much better than linear bottleneck:</span>
            <span class="s3">assert </span><span class="s1">mlp.score(X</span><span class="s3">, </span><span class="s1">y) &gt; </span><span class="s5">0.98</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;X,y&quot;</span><span class="s3">, </span><span class="s1">classification_datasets)</span>
<span class="s3">def </span><span class="s1">test_lbfgs_classification_maxfun(X</span><span class="s3">, </span><span class="s1">y):</span>
    <span class="s2"># Test lbfgs parameter max_fun.</span>
    <span class="s2"># It should independently limit the number of iterations for lbfgs.</span>
    <span class="s1">max_fun = </span><span class="s5">10</span>
    <span class="s2"># classification tests</span>
    <span class="s3">for </span><span class="s1">activation </span><span class="s3">in </span><span class="s1">ACTIVATION_TYPES:</span>
        <span class="s1">mlp = MLPClassifier(</span>
            <span class="s1">solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">,</span>
            <span class="s1">hidden_layer_sizes=</span><span class="s5">50</span><span class="s3">,</span>
            <span class="s1">max_iter=</span><span class="s5">150</span><span class="s3">,</span>
            <span class="s1">max_fun=max_fun</span><span class="s3">,</span>
            <span class="s1">shuffle=</span><span class="s3">True,</span>
            <span class="s1">random_state=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">activation=activation</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s3">with </span><span class="s1">pytest.warns(ConvergenceWarning):</span>
            <span class="s1">mlp.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
            <span class="s3">assert </span><span class="s1">max_fun &gt;= mlp.n_iter_</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;X,y&quot;</span><span class="s3">, </span><span class="s1">regression_datasets)</span>
<span class="s3">def </span><span class="s1">test_lbfgs_regression_maxfun(X</span><span class="s3">, </span><span class="s1">y):</span>
    <span class="s2"># Test lbfgs parameter max_fun.</span>
    <span class="s2"># It should independently limit the number of iterations for lbfgs.</span>
    <span class="s1">max_fun = </span><span class="s5">10</span>
    <span class="s2"># regression tests</span>
    <span class="s3">for </span><span class="s1">activation </span><span class="s3">in </span><span class="s1">ACTIVATION_TYPES:</span>
        <span class="s1">mlp = MLPRegressor(</span>
            <span class="s1">solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">,</span>
            <span class="s1">hidden_layer_sizes=</span><span class="s5">50</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s5">0.0</span><span class="s3">,</span>
            <span class="s1">max_iter=</span><span class="s5">150</span><span class="s3">,</span>
            <span class="s1">max_fun=max_fun</span><span class="s3">,</span>
            <span class="s1">shuffle=</span><span class="s3">True,</span>
            <span class="s1">random_state=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">activation=activation</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s3">with </span><span class="s1">pytest.warns(ConvergenceWarning):</span>
            <span class="s1">mlp.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
            <span class="s3">assert </span><span class="s1">max_fun &gt;= mlp.n_iter_</span>


<span class="s3">def </span><span class="s1">test_learning_rate_warmstart():</span>
    <span class="s2"># Tests that warm_start reuse past solutions.</span>
    <span class="s1">X = [[</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">6</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">5</span><span class="s3">, </span><span class="s5">6</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[-</span><span class="s5">2</span><span class="s3">, </span><span class="s1">-</span><span class="s5">4</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>
    <span class="s3">for </span><span class="s1">learning_rate </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;invscaling&quot;</span><span class="s3">, </span><span class="s4">&quot;constant&quot;</span><span class="s1">]:</span>
        <span class="s1">mlp = MLPClassifier(</span>
            <span class="s1">solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s3">,</span>
            <span class="s1">hidden_layer_sizes=</span><span class="s5">4</span><span class="s3">,</span>
            <span class="s1">learning_rate=learning_rate</span><span class="s3">,</span>
            <span class="s1">max_iter=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">power_t=</span><span class="s5">0.25</span><span class="s3">,</span>
            <span class="s1">warm_start=</span><span class="s3">True,</span>
        <span class="s1">)</span>
        <span class="s3">with </span><span class="s1">ignore_warnings(category=ConvergenceWarning):</span>
            <span class="s1">mlp.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
            <span class="s1">prev_eta = mlp._optimizer.learning_rate</span>
            <span class="s1">mlp.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
            <span class="s1">post_eta = mlp._optimizer.learning_rate</span>

        <span class="s3">if </span><span class="s1">learning_rate == </span><span class="s4">&quot;constant&quot;</span><span class="s1">:</span>
            <span class="s3">assert </span><span class="s1">prev_eta == post_eta</span>
        <span class="s3">elif </span><span class="s1">learning_rate == </span><span class="s4">&quot;invscaling&quot;</span><span class="s1">:</span>
            <span class="s3">assert </span><span class="s1">mlp.learning_rate_init / pow(</span><span class="s5">8 </span><span class="s1">+ </span><span class="s5">1</span><span class="s3">, </span><span class="s1">mlp.power_t) == post_eta</span>


<span class="s3">def </span><span class="s1">test_multilabel_classification():</span>
    <span class="s2"># Test that multi-label classification works as expected.</span>
    <span class="s2"># test fit method</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = make_multilabel_classification(</span>
        <span class="s1">n_samples=</span><span class="s5">50</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">return_indicator=</span><span class="s3">True</span>
    <span class="s1">)</span>
    <span class="s1">mlp = MLPClassifier(</span>
        <span class="s1">solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">,</span>
        <span class="s1">hidden_layer_sizes=</span><span class="s5">50</span><span class="s3">,</span>
        <span class="s1">alpha=</span><span class="s5">1e-5</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s5">150</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">activation=</span><span class="s4">&quot;logistic&quot;</span><span class="s3">,</span>
        <span class="s1">learning_rate_init=</span><span class="s5">0.2</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s1">mlp.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">mlp.score(X</span><span class="s3">, </span><span class="s1">y) &gt; </span><span class="s5">0.97</span>

    <span class="s2"># test partial fit method</span>
    <span class="s1">mlp = MLPClassifier(</span>
        <span class="s1">solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s3">,</span>
        <span class="s1">hidden_layer_sizes=</span><span class="s5">50</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s5">150</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">activation=</span><span class="s4">&quot;logistic&quot;</span><span class="s3">,</span>
        <span class="s1">alpha=</span><span class="s5">1e-5</span><span class="s3">,</span>
        <span class="s1">learning_rate_init=</span><span class="s5">0.2</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">100</span><span class="s1">):</span>
        <span class="s1">mlp.partial_fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">classes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">])</span>
    <span class="s3">assert </span><span class="s1">mlp.score(X</span><span class="s3">, </span><span class="s1">y) &gt; </span><span class="s5">0.9</span>

    <span class="s2"># Make sure early stopping still work now that splitting is stratified by</span>
    <span class="s2"># default (it is disabled for multilabel classification)</span>
    <span class="s1">mlp = MLPClassifier(early_stopping=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">mlp.fit(X</span><span class="s3">, </span><span class="s1">y).predict(X)</span>


<span class="s3">def </span><span class="s1">test_multioutput_regression():</span>
    <span class="s2"># Test that multi-output regression works as expected</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = make_regression(n_samples=</span><span class="s5">200</span><span class="s3">, </span><span class="s1">n_targets=</span><span class="s5">5</span><span class="s1">)</span>
    <span class="s1">mlp = MLPRegressor(</span>
        <span class="s1">solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s1">hidden_layer_sizes=</span><span class="s5">50</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s5">200</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">1</span>
    <span class="s1">)</span>
    <span class="s1">mlp.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">mlp.score(X</span><span class="s3">, </span><span class="s1">y) &gt; </span><span class="s5">0.9</span>


<span class="s3">def </span><span class="s1">test_partial_fit_classes_error():</span>
    <span class="s2"># Tests that passing different classes to partial_fit raises an error</span>
    <span class="s1">X = [[</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">clf = MLPClassifier(solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s1">)</span>
    <span class="s1">clf.partial_fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">classes=[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">])</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.partial_fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">classes=[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">])</span>


<span class="s3">def </span><span class="s1">test_partial_fit_classification():</span>
    <span class="s2"># Test partial_fit on classification.</span>
    <span class="s2"># `partial_fit` should yield the same results as 'fit' for binary and</span>
    <span class="s2"># multi-class classification.</span>
    <span class="s3">for </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y </span><span class="s3">in </span><span class="s1">classification_datasets:</span>
        <span class="s1">mlp = MLPClassifier(</span>
            <span class="s1">solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s3">,</span>
            <span class="s1">max_iter=</span><span class="s5">100</span><span class="s3">,</span>
            <span class="s1">random_state=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s5">0</span><span class="s3">,</span>
            <span class="s1">alpha=</span><span class="s5">1e-5</span><span class="s3">,</span>
            <span class="s1">learning_rate_init=</span><span class="s5">0.2</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s3">with </span><span class="s1">ignore_warnings(category=ConvergenceWarning):</span>
            <span class="s1">mlp.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s1">pred1 = mlp.predict(X)</span>
        <span class="s1">mlp = MLPClassifier(</span>
            <span class="s1">solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s5">1e-5</span><span class="s3">, </span><span class="s1">learning_rate_init=</span><span class="s5">0.2</span>
        <span class="s1">)</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">100</span><span class="s1">):</span>
            <span class="s1">mlp.partial_fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">classes=np.unique(y))</span>
        <span class="s1">pred2 = mlp.predict(X)</span>
        <span class="s1">assert_array_equal(pred1</span><span class="s3">, </span><span class="s1">pred2)</span>
        <span class="s3">assert </span><span class="s1">mlp.score(X</span><span class="s3">, </span><span class="s1">y) &gt; </span><span class="s5">0.95</span>


<span class="s3">def </span><span class="s1">test_partial_fit_unseen_classes():</span>
    <span class="s2"># Non regression test for bug 6994</span>
    <span class="s2"># Tests for labeling errors in partial fit</span>

    <span class="s1">clf = MLPClassifier(random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">clf.partial_fit([[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">3</span><span class="s1">]]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;a&quot;</span><span class="s3">, </span><span class="s4">&quot;b&quot;</span><span class="s3">, </span><span class="s4">&quot;c&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">classes=[</span><span class="s4">&quot;a&quot;</span><span class="s3">, </span><span class="s4">&quot;b&quot;</span><span class="s3">, </span><span class="s4">&quot;c&quot;</span><span class="s3">, </span><span class="s4">&quot;d&quot;</span><span class="s1">])</span>
    <span class="s1">clf.partial_fit([[</span><span class="s5">4</span><span class="s1">]]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;d&quot;</span><span class="s1">])</span>
    <span class="s3">assert </span><span class="s1">clf.score([[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">4</span><span class="s1">]]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;a&quot;</span><span class="s3">, </span><span class="s4">&quot;b&quot;</span><span class="s3">, </span><span class="s4">&quot;c&quot;</span><span class="s3">, </span><span class="s4">&quot;d&quot;</span><span class="s1">]) &gt; </span><span class="s5">0</span>


<span class="s3">def </span><span class="s1">test_partial_fit_regression():</span>
    <span class="s2"># Test partial_fit on regression.</span>
    <span class="s2"># `partial_fit` should yield the same results as 'fit' for regression.</span>
    <span class="s1">X = X_reg</span>
    <span class="s1">y = y_reg</span>

    <span class="s3">for </span><span class="s1">momentum </span><span class="s3">in </span><span class="s1">[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0.9</span><span class="s1">]:</span>
        <span class="s1">mlp = MLPRegressor(</span>
            <span class="s1">solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s3">,</span>
            <span class="s1">max_iter=</span><span class="s5">100</span><span class="s3">,</span>
            <span class="s1">activation=</span><span class="s4">&quot;relu&quot;</span><span class="s3">,</span>
            <span class="s1">random_state=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">learning_rate_init=</span><span class="s5">0.01</span><span class="s3">,</span>
            <span class="s1">batch_size=X.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">momentum=momentum</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s3">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s3">True</span><span class="s1">):</span>
            <span class="s2"># catch convergence warning</span>
            <span class="s1">mlp.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s1">pred1 = mlp.predict(X)</span>
        <span class="s1">mlp = MLPRegressor(</span>
            <span class="s1">solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s3">,</span>
            <span class="s1">activation=</span><span class="s4">&quot;relu&quot;</span><span class="s3">,</span>
            <span class="s1">learning_rate_init=</span><span class="s5">0.01</span><span class="s3">,</span>
            <span class="s1">random_state=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">batch_size=X.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">momentum=momentum</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">100</span><span class="s1">):</span>
            <span class="s1">mlp.partial_fit(X</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s1">pred2 = mlp.predict(X)</span>
        <span class="s1">assert_allclose(pred1</span><span class="s3">, </span><span class="s1">pred2)</span>
        <span class="s1">score = mlp.score(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s3">assert </span><span class="s1">score &gt; </span><span class="s5">0.65</span>


<span class="s3">def </span><span class="s1">test_partial_fit_errors():</span>
    <span class="s2"># Test partial_fit error handling.</span>
    <span class="s1">X = [[</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">6</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>

    <span class="s2"># no classes passed</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">MLPClassifier(solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s1">).partial_fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">classes=[</span><span class="s5">2</span><span class="s1">])</span>

    <span class="s2"># lbfgs doesn't support partial_fit</span>
    <span class="s3">assert not </span><span class="s1">hasattr(MLPClassifier(solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s4">&quot;partial_fit&quot;</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_nonfinite_params():</span>
    <span class="s2"># Check that MLPRegressor throws ValueError when dealing with non-finite</span>
    <span class="s2"># parameter values</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s5">10</span>
    <span class="s1">fmax = np.finfo(np.float64).max</span>
    <span class="s1">X = fmax * rng.uniform(size=(n_samples</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span>
    <span class="s1">y = rng.standard_normal(size=n_samples)</span>

    <span class="s1">clf = MLPRegressor()</span>
    <span class="s1">msg = (</span>
        <span class="s4">&quot;Solver produced non-finite parameter weights. The input data may contain large&quot;</span>
        <span class="s4">&quot; values and need to be preprocessed.&quot;</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError</span><span class="s3">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">test_predict_proba_binary():</span>
    <span class="s2"># Test that predict_proba works as expected for binary class.</span>
    <span class="s1">X = X_digits_binary[:</span><span class="s5">50</span><span class="s1">]</span>
    <span class="s1">y = y_digits_binary[:</span><span class="s5">50</span><span class="s1">]</span>

    <span class="s1">clf = MLPClassifier(hidden_layer_sizes=</span><span class="s5">5</span><span class="s3">, </span><span class="s1">activation=</span><span class="s4">&quot;logistic&quot;</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s3">with </span><span class="s1">ignore_warnings(category=ConvergenceWarning):</span>
        <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">y_proba = clf.predict_proba(X)</span>
    <span class="s1">y_log_proba = clf.predict_log_proba(X)</span>

    <span class="s1">(n_samples</span><span class="s3">, </span><span class="s1">n_classes) = y.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">2</span>

    <span class="s1">proba_max = y_proba.argmax(axis=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">proba_log_max = y_log_proba.argmax(axis=</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">assert </span><span class="s1">y_proba.shape == (n_samples</span><span class="s3">, </span><span class="s1">n_classes)</span>
    <span class="s1">assert_array_equal(proba_max</span><span class="s3">, </span><span class="s1">proba_log_max)</span>
    <span class="s1">assert_allclose(y_log_proba</span><span class="s3">, </span><span class="s1">np.log(y_proba))</span>

    <span class="s3">assert </span><span class="s1">roc_auc_score(y</span><span class="s3">, </span><span class="s1">y_proba[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]) == </span><span class="s5">1.0</span>


<span class="s3">def </span><span class="s1">test_predict_proba_multiclass():</span>
    <span class="s2"># Test that predict_proba works as expected for multi class.</span>
    <span class="s1">X = X_digits_multi[:</span><span class="s5">10</span><span class="s1">]</span>
    <span class="s1">y = y_digits_multi[:</span><span class="s5">10</span><span class="s1">]</span>

    <span class="s1">clf = MLPClassifier(hidden_layer_sizes=</span><span class="s5">5</span><span class="s1">)</span>
    <span class="s3">with </span><span class="s1">ignore_warnings(category=ConvergenceWarning):</span>
        <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">y_proba = clf.predict_proba(X)</span>
    <span class="s1">y_log_proba = clf.predict_log_proba(X)</span>

    <span class="s1">(n_samples</span><span class="s3">, </span><span class="s1">n_classes) = y.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">np.unique(y).size</span>

    <span class="s1">proba_max = y_proba.argmax(axis=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">proba_log_max = y_log_proba.argmax(axis=</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">assert </span><span class="s1">y_proba.shape == (n_samples</span><span class="s3">, </span><span class="s1">n_classes)</span>
    <span class="s1">assert_array_equal(proba_max</span><span class="s3">, </span><span class="s1">proba_log_max)</span>
    <span class="s1">assert_allclose(y_log_proba</span><span class="s3">, </span><span class="s1">np.log(y_proba))</span>


<span class="s3">def </span><span class="s1">test_predict_proba_multilabel():</span>
    <span class="s2"># Test that predict_proba works as expected for multilabel.</span>
    <span class="s2"># Multilabel should not use softmax which makes probabilities sum to 1</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">Y = make_multilabel_classification(</span>
        <span class="s1">n_samples=</span><span class="s5">50</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">return_indicator=</span><span class="s3">True</span>
    <span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_classes = Y.shape</span>

    <span class="s1">clf = MLPClassifier(solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s1">hidden_layer_sizes=</span><span class="s5">30</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">Y)</span>
    <span class="s1">y_proba = clf.predict_proba(X)</span>

    <span class="s3">assert </span><span class="s1">y_proba.shape == (n_samples</span><span class="s3">, </span><span class="s1">n_classes)</span>
    <span class="s1">assert_array_equal(y_proba &gt; </span><span class="s5">0.5</span><span class="s3">, </span><span class="s1">Y)</span>

    <span class="s1">y_log_proba = clf.predict_log_proba(X)</span>
    <span class="s1">proba_max = y_proba.argmax(axis=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">proba_log_max = y_log_proba.argmax(axis=</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">assert </span><span class="s1">(y_proba.sum(</span><span class="s5">1</span><span class="s1">) - </span><span class="s5">1</span><span class="s1">).dot(y_proba.sum(</span><span class="s5">1</span><span class="s1">) - </span><span class="s5">1</span><span class="s1">) &gt; </span><span class="s5">1e-10</span>
    <span class="s1">assert_array_equal(proba_max</span><span class="s3">, </span><span class="s1">proba_log_max)</span>
    <span class="s1">assert_allclose(y_log_proba</span><span class="s3">, </span><span class="s1">np.log(y_proba))</span>


<span class="s3">def </span><span class="s1">test_shuffle():</span>
    <span class="s2"># Test that the shuffle parameter affects the training process (it should)</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = make_regression(n_samples=</span><span class="s5">50</span><span class="s3">, </span><span class="s1">n_features=</span><span class="s5">5</span><span class="s3">, </span><span class="s1">n_targets=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s2"># The coefficients will be identical if both do or do not shuffle</span>
    <span class="s3">for </span><span class="s1">shuffle </span><span class="s3">in </span><span class="s1">[</span><span class="s3">True, False</span><span class="s1">]:</span>
        <span class="s1">mlp1 = MLPRegressor(</span>
            <span class="s1">hidden_layer_sizes=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">max_iter=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">batch_size=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">random_state=</span><span class="s5">0</span><span class="s3">,</span>
            <span class="s1">shuffle=shuffle</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">mlp2 = MLPRegressor(</span>
            <span class="s1">hidden_layer_sizes=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">max_iter=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">batch_size=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">random_state=</span><span class="s5">0</span><span class="s3">,</span>
            <span class="s1">shuffle=shuffle</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">mlp1.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s1">mlp2.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s3">assert </span><span class="s1">np.array_equal(mlp1.coefs_[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">mlp2.coefs_[</span><span class="s5">0</span><span class="s1">])</span>

    <span class="s2"># The coefficients will be slightly different if shuffle=True</span>
    <span class="s1">mlp1 = MLPRegressor(</span>
        <span class="s1">hidden_layer_sizes=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">batch_size=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">shuffle=</span><span class="s3">True</span>
    <span class="s1">)</span>
    <span class="s1">mlp2 = MLPRegressor(</span>
        <span class="s1">hidden_layer_sizes=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">batch_size=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">shuffle=</span><span class="s3">False</span>
    <span class="s1">)</span>
    <span class="s1">mlp1.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">mlp2.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s3">assert not </span><span class="s1">np.array_equal(mlp1.coefs_[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">mlp2.coefs_[</span><span class="s5">0</span><span class="s1">])</span>


<span class="s3">def </span><span class="s1">test_sparse_matrices():</span>
    <span class="s2"># Test that sparse and dense input matrices output the same results.</span>
    <span class="s1">X = X_digits_binary[:</span><span class="s5">50</span><span class="s1">]</span>
    <span class="s1">y = y_digits_binary[:</span><span class="s5">50</span><span class="s1">]</span>
    <span class="s1">X_sparse = csr_matrix(X)</span>
    <span class="s1">mlp = MLPClassifier(solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s1">hidden_layer_sizes=</span><span class="s5">15</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">mlp.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">pred1 = mlp.predict(X)</span>
    <span class="s1">mlp.fit(X_sparse</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">pred2 = mlp.predict(X_sparse)</span>
    <span class="s1">assert_almost_equal(pred1</span><span class="s3">, </span><span class="s1">pred2)</span>
    <span class="s1">pred1 = mlp.predict(X)</span>
    <span class="s1">pred2 = mlp.predict(X_sparse)</span>
    <span class="s1">assert_array_equal(pred1</span><span class="s3">, </span><span class="s1">pred2)</span>


<span class="s3">def </span><span class="s1">test_tolerance():</span>
    <span class="s2"># Test tolerance.</span>
    <span class="s2"># It should force the solver to exit the loop when it converges.</span>
    <span class="s1">X = [[</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">6</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">clf = MLPClassifier(tol=</span><span class="s5">0.5</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s5">3000</span><span class="s3">, </span><span class="s1">solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">clf.max_iter &gt; clf.n_iter_</span>


<span class="s3">def </span><span class="s1">test_verbose_sgd():</span>
    <span class="s2"># Test verbose.</span>
    <span class="s1">X = [[</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">6</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">clf = MLPClassifier(solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">verbose=</span><span class="s5">10</span><span class="s3">, </span><span class="s1">hidden_layer_sizes=</span><span class="s5">2</span><span class="s1">)</span>
    <span class="s1">old_stdout = sys.stdout</span>
    <span class="s1">sys.stdout = output = StringIO()</span>

    <span class="s3">with </span><span class="s1">ignore_warnings(category=ConvergenceWarning):</span>
        <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">clf.partial_fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">sys.stdout = old_stdout</span>
    <span class="s3">assert </span><span class="s4">&quot;Iteration&quot; </span><span class="s3">in </span><span class="s1">output.getvalue()</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;MLPEstimator&quot;</span><span class="s3">, </span><span class="s1">[MLPClassifier</span><span class="s3">, </span><span class="s1">MLPRegressor])</span>
<span class="s3">def </span><span class="s1">test_early_stopping(MLPEstimator):</span>
    <span class="s1">X = X_digits_binary[:</span><span class="s5">100</span><span class="s1">]</span>
    <span class="s1">y = y_digits_binary[:</span><span class="s5">100</span><span class="s1">]</span>
    <span class="s1">tol = </span><span class="s5">0.2</span>
    <span class="s1">mlp_estimator = MLPEstimator(</span>
        <span class="s1">tol=tol</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s5">3000</span><span class="s3">, </span><span class="s1">solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s3">, </span><span class="s1">early_stopping=</span><span class="s3">True</span>
    <span class="s1">)</span>
    <span class="s1">mlp_estimator.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">mlp_estimator.max_iter &gt; mlp_estimator.n_iter_</span>

    <span class="s3">assert </span><span class="s1">mlp_estimator.best_loss_ </span><span class="s3">is None</span>
    <span class="s3">assert </span><span class="s1">isinstance(mlp_estimator.validation_scores_</span><span class="s3">, </span><span class="s1">list)</span>

    <span class="s1">valid_scores = mlp_estimator.validation_scores_</span>
    <span class="s1">best_valid_score = mlp_estimator.best_validation_score_</span>
    <span class="s3">assert </span><span class="s1">max(valid_scores) == best_valid_score</span>
    <span class="s3">assert </span><span class="s1">best_valid_score + tol &gt; valid_scores[-</span><span class="s5">2</span><span class="s1">]</span>
    <span class="s3">assert </span><span class="s1">best_valid_score + tol &gt; valid_scores[-</span><span class="s5">1</span><span class="s1">]</span>

    <span class="s2"># check that the attributes `validation_scores_` and `best_validation_score_`</span>
    <span class="s2"># are set to None when `early_stopping=False`</span>
    <span class="s1">mlp_estimator = MLPEstimator(</span>
        <span class="s1">tol=tol</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s5">3000</span><span class="s3">, </span><span class="s1">solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s3">, </span><span class="s1">early_stopping=</span><span class="s3">False</span>
    <span class="s1">)</span>
    <span class="s1">mlp_estimator.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">mlp_estimator.validation_scores_ </span><span class="s3">is None</span>
    <span class="s3">assert </span><span class="s1">mlp_estimator.best_validation_score_ </span><span class="s3">is None</span>
    <span class="s3">assert </span><span class="s1">mlp_estimator.best_loss_ </span><span class="s3">is not None</span>


<span class="s3">def </span><span class="s1">test_adaptive_learning_rate():</span>
    <span class="s1">X = [[</span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">6</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">clf = MLPClassifier(tol=</span><span class="s5">0.5</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s5">3000</span><span class="s3">, </span><span class="s1">solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s3">, </span><span class="s1">learning_rate=</span><span class="s4">&quot;adaptive&quot;</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">clf.max_iter &gt; clf.n_iter_</span>
    <span class="s3">assert </span><span class="s5">1e-6 </span><span class="s1">&gt; clf._optimizer.learning_rate</span>


<span class="s1">@ignore_warnings(category=RuntimeWarning)</span>
<span class="s3">def </span><span class="s1">test_warm_start():</span>
    <span class="s1">X = X_iris</span>
    <span class="s1">y = y_iris</span>

    <span class="s1">y_2classes = np.array([</span><span class="s5">0</span><span class="s1">] * </span><span class="s5">75 </span><span class="s1">+ [</span><span class="s5">1</span><span class="s1">] * </span><span class="s5">75</span><span class="s1">)</span>
    <span class="s1">y_3classes = np.array([</span><span class="s5">0</span><span class="s1">] * </span><span class="s5">40 </span><span class="s1">+ [</span><span class="s5">1</span><span class="s1">] * </span><span class="s5">40 </span><span class="s1">+ [</span><span class="s5">2</span><span class="s1">] * </span><span class="s5">70</span><span class="s1">)</span>
    <span class="s1">y_3classes_alt = np.array([</span><span class="s5">0</span><span class="s1">] * </span><span class="s5">50 </span><span class="s1">+ [</span><span class="s5">1</span><span class="s1">] * </span><span class="s5">50 </span><span class="s1">+ [</span><span class="s5">3</span><span class="s1">] * </span><span class="s5">50</span><span class="s1">)</span>
    <span class="s1">y_4classes = np.array([</span><span class="s5">0</span><span class="s1">] * </span><span class="s5">37 </span><span class="s1">+ [</span><span class="s5">1</span><span class="s1">] * </span><span class="s5">37 </span><span class="s1">+ [</span><span class="s5">2</span><span class="s1">] * </span><span class="s5">38 </span><span class="s1">+ [</span><span class="s5">3</span><span class="s1">] * </span><span class="s5">38</span><span class="s1">)</span>
    <span class="s1">y_5classes = np.array([</span><span class="s5">0</span><span class="s1">] * </span><span class="s5">30 </span><span class="s1">+ [</span><span class="s5">1</span><span class="s1">] * </span><span class="s5">30 </span><span class="s1">+ [</span><span class="s5">2</span><span class="s1">] * </span><span class="s5">30 </span><span class="s1">+ [</span><span class="s5">3</span><span class="s1">] * </span><span class="s5">30 </span><span class="s1">+ [</span><span class="s5">4</span><span class="s1">] * </span><span class="s5">30</span><span class="s1">)</span>

    <span class="s2"># No error raised</span>
    <span class="s1">clf = MLPClassifier(hidden_layer_sizes=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True</span><span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y_3classes)</span>

    <span class="s3">for </span><span class="s1">y_i </span><span class="s3">in </span><span class="s1">(y_2classes</span><span class="s3">, </span><span class="s1">y_3classes_alt</span><span class="s3">, </span><span class="s1">y_4classes</span><span class="s3">, </span><span class="s1">y_5classes):</span>
        <span class="s1">clf = MLPClassifier(hidden_layer_sizes=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True</span><span class="s1">).fit(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span>
        <span class="s1">)</span>
        <span class="s1">message = (</span>
            <span class="s4">&quot;warm_start can only be used where `y` has the same &quot;</span>
            <span class="s4">&quot;classes as in the previous call to fit.&quot;</span>
            <span class="s4">&quot; Previously got [0 1 2], `y` has %s&quot;</span>
            <span class="s1">% np.unique(y_i)</span>
        <span class="s1">)</span>
        <span class="s3">with </span><span class="s1">pytest.raises(ValueError</span><span class="s3">, </span><span class="s1">match=re.escape(message)):</span>
            <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y_i)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;MLPEstimator&quot;</span><span class="s3">, </span><span class="s1">[MLPClassifier</span><span class="s3">, </span><span class="s1">MLPRegressor])</span>
<span class="s3">def </span><span class="s1">test_warm_start_full_iteration(MLPEstimator):</span>
    <span class="s2"># Non-regression test for:</span>
    <span class="s2"># https://github.com/scikit-learn/scikit-learn/issues/16812</span>
    <span class="s2"># Check that the MLP estimator accomplish `max_iter` with a</span>
    <span class="s2"># warm started estimator.</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = X_iris</span><span class="s3">, </span><span class="s1">y_iris</span>
    <span class="s1">max_iter = </span><span class="s5">3</span>
    <span class="s1">clf = MLPEstimator(</span>
        <span class="s1">hidden_layer_sizes=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True, </span><span class="s1">max_iter=max_iter</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">max_iter == clf.n_iter_</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">max_iter == clf.n_iter_</span>


<span class="s3">def </span><span class="s1">test_n_iter_no_change():</span>
    <span class="s2"># test n_iter_no_change using binary data set</span>
    <span class="s2"># the classifying fitting process is not prone to loss curve fluctuations</span>
    <span class="s1">X = X_digits_binary[:</span><span class="s5">100</span><span class="s1">]</span>
    <span class="s1">y = y_digits_binary[:</span><span class="s5">100</span><span class="s1">]</span>
    <span class="s1">tol = </span><span class="s5">0.01</span>
    <span class="s1">max_iter = </span><span class="s5">3000</span>

    <span class="s2"># test multiple n_iter_no_change</span>
    <span class="s3">for </span><span class="s1">n_iter_no_change </span><span class="s3">in </span><span class="s1">[</span><span class="s5">2</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">10</span><span class="s3">, </span><span class="s5">50</span><span class="s3">, </span><span class="s5">100</span><span class="s1">]:</span>
        <span class="s1">clf = MLPClassifier(</span>
            <span class="s1">tol=tol</span><span class="s3">, </span><span class="s1">max_iter=max_iter</span><span class="s3">, </span><span class="s1">solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s3">, </span><span class="s1">n_iter_no_change=n_iter_no_change</span>
        <span class="s1">)</span>
        <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s2"># validate n_iter_no_change</span>
        <span class="s3">assert </span><span class="s1">clf._no_improvement_count == n_iter_no_change + </span><span class="s5">1</span>
        <span class="s3">assert </span><span class="s1">max_iter &gt; clf.n_iter_</span>


<span class="s1">@ignore_warnings(category=ConvergenceWarning)</span>
<span class="s3">def </span><span class="s1">test_n_iter_no_change_inf():</span>
    <span class="s2"># test n_iter_no_change using binary data set</span>
    <span class="s2"># the fitting process should go to max_iter iterations</span>
    <span class="s1">X = X_digits_binary[:</span><span class="s5">100</span><span class="s1">]</span>
    <span class="s1">y = y_digits_binary[:</span><span class="s5">100</span><span class="s1">]</span>

    <span class="s2"># set a ridiculous tolerance</span>
    <span class="s2"># this should always trigger _update_no_improvement_count()</span>
    <span class="s1">tol = </span><span class="s5">1e9</span>

    <span class="s2"># fit</span>
    <span class="s1">n_iter_no_change = np.inf</span>
    <span class="s1">max_iter = </span><span class="s5">3000</span>
    <span class="s1">clf = MLPClassifier(</span>
        <span class="s1">tol=tol</span><span class="s3">, </span><span class="s1">max_iter=max_iter</span><span class="s3">, </span><span class="s1">solver=</span><span class="s4">&quot;sgd&quot;</span><span class="s3">, </span><span class="s1">n_iter_no_change=n_iter_no_change</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s2"># validate n_iter_no_change doesn't cause early stopping</span>
    <span class="s3">assert </span><span class="s1">clf.n_iter_ == max_iter</span>

    <span class="s2"># validate _update_no_improvement_count() was always triggered</span>
    <span class="s3">assert </span><span class="s1">clf._no_improvement_count == clf.n_iter_ - </span><span class="s5">1</span>


<span class="s3">def </span><span class="s1">test_early_stopping_stratified():</span>
    <span class="s2"># Make sure data splitting for early stopping is stratified</span>
    <span class="s1">X = [[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">4</span><span class="s3">, </span><span class="s5">5</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]</span>

    <span class="s1">mlp = MLPClassifier(early_stopping=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s3">, </span><span class="s1">match=</span><span class="s4">&quot;The least populated class in y has only 1 member&quot;</span>
    <span class="s1">):</span>
        <span class="s1">mlp.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">test_mlp_classifier_dtypes_casting():</span>
    <span class="s2"># Compare predictions for different dtypes</span>
    <span class="s1">mlp_64 = MLPClassifier(</span>
        <span class="s1">alpha=</span><span class="s5">1e-5</span><span class="s3">, </span><span class="s1">hidden_layer_sizes=(</span><span class="s5">5</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s5">50</span>
    <span class="s1">)</span>
    <span class="s1">mlp_64.fit(X_digits[:</span><span class="s5">300</span><span class="s1">]</span><span class="s3">, </span><span class="s1">y_digits[:</span><span class="s5">300</span><span class="s1">])</span>
    <span class="s1">pred_64 = mlp_64.predict(X_digits[</span><span class="s5">300</span><span class="s1">:])</span>
    <span class="s1">proba_64 = mlp_64.predict_proba(X_digits[</span><span class="s5">300</span><span class="s1">:])</span>

    <span class="s1">mlp_32 = MLPClassifier(</span>
        <span class="s1">alpha=</span><span class="s5">1e-5</span><span class="s3">, </span><span class="s1">hidden_layer_sizes=(</span><span class="s5">5</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s5">50</span>
    <span class="s1">)</span>
    <span class="s1">mlp_32.fit(X_digits[:</span><span class="s5">300</span><span class="s1">].astype(np.float32)</span><span class="s3">, </span><span class="s1">y_digits[:</span><span class="s5">300</span><span class="s1">])</span>
    <span class="s1">pred_32 = mlp_32.predict(X_digits[</span><span class="s5">300</span><span class="s1">:].astype(np.float32))</span>
    <span class="s1">proba_32 = mlp_32.predict_proba(X_digits[</span><span class="s5">300</span><span class="s1">:].astype(np.float32))</span>

    <span class="s1">assert_array_equal(pred_64</span><span class="s3">, </span><span class="s1">pred_32)</span>
    <span class="s1">assert_allclose(proba_64</span><span class="s3">, </span><span class="s1">proba_32</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-02</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_mlp_regressor_dtypes_casting():</span>
    <span class="s1">mlp_64 = MLPRegressor(</span>
        <span class="s1">alpha=</span><span class="s5">1e-5</span><span class="s3">, </span><span class="s1">hidden_layer_sizes=(</span><span class="s5">5</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s5">50</span>
    <span class="s1">)</span>
    <span class="s1">mlp_64.fit(X_digits[:</span><span class="s5">300</span><span class="s1">]</span><span class="s3">, </span><span class="s1">y_digits[:</span><span class="s5">300</span><span class="s1">])</span>
    <span class="s1">pred_64 = mlp_64.predict(X_digits[</span><span class="s5">300</span><span class="s1">:])</span>

    <span class="s1">mlp_32 = MLPRegressor(</span>
        <span class="s1">alpha=</span><span class="s5">1e-5</span><span class="s3">, </span><span class="s1">hidden_layer_sizes=(</span><span class="s5">5</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s5">50</span>
    <span class="s1">)</span>
    <span class="s1">mlp_32.fit(X_digits[:</span><span class="s5">300</span><span class="s1">].astype(np.float32)</span><span class="s3">, </span><span class="s1">y_digits[:</span><span class="s5">300</span><span class="s1">])</span>
    <span class="s1">pred_32 = mlp_32.predict(X_digits[</span><span class="s5">300</span><span class="s1">:].astype(np.float32))</span>

    <span class="s1">assert_allclose(pred_64</span><span class="s3">, </span><span class="s1">pred_32</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-04</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;dtype&quot;</span><span class="s3">, </span><span class="s1">[np.float32</span><span class="s3">, </span><span class="s1">np.float64])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s3">, </span><span class="s1">[MLPClassifier</span><span class="s3">, </span><span class="s1">MLPRegressor])</span>
<span class="s3">def </span><span class="s1">test_mlp_param_dtypes(dtype</span><span class="s3">, </span><span class="s1">Estimator):</span>
    <span class="s2"># Checks if input dtype is used for network parameters</span>
    <span class="s2"># and predictions</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = X_digits.astype(dtype)</span><span class="s3">, </span><span class="s1">y_digits</span>
    <span class="s1">mlp = Estimator(alpha=</span><span class="s5">1e-5</span><span class="s3">, </span><span class="s1">hidden_layer_sizes=(</span><span class="s5">5</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s5">50</span><span class="s1">)</span>
    <span class="s1">mlp.fit(X[:</span><span class="s5">300</span><span class="s1">]</span><span class="s3">, </span><span class="s1">y[:</span><span class="s5">300</span><span class="s1">])</span>
    <span class="s1">pred = mlp.predict(X[</span><span class="s5">300</span><span class="s1">:])</span>

    <span class="s3">assert </span><span class="s1">all([intercept.dtype == dtype </span><span class="s3">for </span><span class="s1">intercept </span><span class="s3">in </span><span class="s1">mlp.intercepts_])</span>

    <span class="s3">assert </span><span class="s1">all([coef.dtype == dtype </span><span class="s3">for </span><span class="s1">coef </span><span class="s3">in </span><span class="s1">mlp.coefs_])</span>

    <span class="s3">if </span><span class="s1">Estimator == MLPRegressor:</span>
        <span class="s3">assert </span><span class="s1">pred.dtype == dtype</span>


<span class="s3">def </span><span class="s1">test_mlp_loading_from_joblib_partial_fit(tmp_path):</span>
    <span class="s0">&quot;&quot;&quot;Loading from MLP and partial fitting updates weights. Non-regression 
    test for #19626.&quot;&quot;&quot;</span>
    <span class="s1">pre_trained_estimator = MLPRegressor(</span>
        <span class="s1">hidden_layer_sizes=(</span><span class="s5">42</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">42</span><span class="s3">, </span><span class="s1">learning_rate_init=</span><span class="s5">0.01</span><span class="s3">, </span><span class="s1">max_iter=</span><span class="s5">200</span>
    <span class="s1">)</span>
    <span class="s1">features</span><span class="s3">, </span><span class="s1">target = [[</span><span class="s5">2</span><span class="s1">]]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">4</span><span class="s1">]</span>

    <span class="s2"># Fit on x=2, y=4</span>
    <span class="s1">pre_trained_estimator.fit(features</span><span class="s3">, </span><span class="s1">target)</span>

    <span class="s2"># dump and load model</span>
    <span class="s1">pickled_file = tmp_path / </span><span class="s4">&quot;mlp.pkl&quot;</span>
    <span class="s1">joblib.dump(pre_trained_estimator</span><span class="s3">, </span><span class="s1">pickled_file)</span>
    <span class="s1">load_estimator = joblib.load(pickled_file)</span>

    <span class="s2"># Train for a more epochs on point x=2, y=1</span>
    <span class="s1">fine_tune_features</span><span class="s3">, </span><span class="s1">fine_tune_target = [[</span><span class="s5">2</span><span class="s1">]]</span><span class="s3">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">]</span>

    <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">200</span><span class="s1">):</span>
        <span class="s1">load_estimator.partial_fit(fine_tune_features</span><span class="s3">, </span><span class="s1">fine_tune_target)</span>

    <span class="s2"># finetuned model learned the new target</span>
    <span class="s1">predicted_value = load_estimator.predict(fine_tune_features)</span>
    <span class="s1">assert_allclose(predicted_value</span><span class="s3">, </span><span class="s1">fine_tune_target</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-4</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s3">, </span><span class="s1">[MLPClassifier</span><span class="s3">, </span><span class="s1">MLPRegressor])</span>
<span class="s3">def </span><span class="s1">test_preserve_feature_names(Estimator):</span>
    <span class="s0">&quot;&quot;&quot;Check that feature names are preserved when early stopping is enabled. 
 
    Feature names are required for consistency checks during scoring. 
 
    Non-regression test for gh-24846 
    &quot;&quot;&quot;</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s4">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">X = pd.DataFrame(data=rng.randn(</span><span class="s5">10</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">columns=[</span><span class="s4">&quot;colname_a&quot;</span><span class="s3">, </span><span class="s4">&quot;colname_b&quot;</span><span class="s1">])</span>
    <span class="s1">y = pd.Series(data=np.full(</span><span class="s5">10</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">name=</span><span class="s4">&quot;colname_y&quot;</span><span class="s1">)</span>

    <span class="s1">model = Estimator(early_stopping=</span><span class="s3">True, </span><span class="s1">validation_fraction=</span><span class="s5">0.2</span><span class="s1">)</span>

    <span class="s3">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s4">&quot;error&quot;</span><span class="s3">, </span><span class="s1">UserWarning)</span>
        <span class="s1">model.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;MLPEstimator&quot;</span><span class="s3">, </span><span class="s1">[MLPClassifier</span><span class="s3">, </span><span class="s1">MLPRegressor])</span>
<span class="s3">def </span><span class="s1">test_mlp_warm_start_with_early_stopping(MLPEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Check that early stopping works with warm start.&quot;&quot;&quot;</span>
    <span class="s1">mlp = MLPEstimator(</span>
        <span class="s1">max_iter=</span><span class="s5">10</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True, </span><span class="s1">early_stopping=</span><span class="s3">True</span>
    <span class="s1">)</span>
    <span class="s1">mlp.fit(X_iris</span><span class="s3">, </span><span class="s1">y_iris)</span>
    <span class="s1">n_validation_scores = len(mlp.validation_scores_)</span>
    <span class="s1">mlp.set_params(max_iter=</span><span class="s5">20</span><span class="s1">)</span>
    <span class="s1">mlp.fit(X_iris</span><span class="s3">, </span><span class="s1">y_iris)</span>
    <span class="s3">assert </span><span class="s1">len(mlp.validation_scores_) &gt; n_validation_scores</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;MLPEstimator&quot;</span><span class="s3">, </span><span class="s1">[MLPClassifier</span><span class="s3">, </span><span class="s1">MLPRegressor])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;solver&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;sgd&quot;</span><span class="s3">, </span><span class="s4">&quot;adam&quot;</span><span class="s3">, </span><span class="s4">&quot;lbfgs&quot;</span><span class="s1">])</span>
<span class="s3">def </span><span class="s1">test_mlp_warm_start_no_convergence(MLPEstimator</span><span class="s3">, </span><span class="s1">solver):</span>
    <span class="s0">&quot;&quot;&quot;Check that we stop the number of iteration at `max_iter` when warm starting. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/24764 
    &quot;&quot;&quot;</span>
    <span class="s1">model = MLPEstimator(</span>
        <span class="s1">solver=solver</span><span class="s3">,</span>
        <span class="s1">warm_start=</span><span class="s3">True,</span>
        <span class="s1">early_stopping=</span><span class="s3">False,</span>
        <span class="s1">max_iter=</span><span class="s5">10</span><span class="s3">,</span>
        <span class="s1">n_iter_no_change=np.inf</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s3">with </span><span class="s1">pytest.warns(ConvergenceWarning):</span>
        <span class="s1">model.fit(X_iris</span><span class="s3">, </span><span class="s1">y_iris)</span>
    <span class="s3">assert </span><span class="s1">model.n_iter_ == </span><span class="s5">10</span>

    <span class="s1">model.set_params(max_iter=</span><span class="s5">20</span><span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.warns(ConvergenceWarning):</span>
        <span class="s1">model.fit(X_iris</span><span class="s3">, </span><span class="s1">y_iris)</span>
    <span class="s3">assert </span><span class="s1">model.n_iter_ == </span><span class="s5">20</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;MLPEstimator&quot;</span><span class="s3">, </span><span class="s1">[MLPClassifier</span><span class="s3">, </span><span class="s1">MLPRegressor])</span>
<span class="s3">def </span><span class="s1">test_mlp_partial_fit_after_fit(MLPEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Check partial fit does not fail after fit when early_stopping=True. 
 
    Non-regression test for gh-25693. 
    &quot;&quot;&quot;</span>
    <span class="s1">mlp = MLPEstimator(early_stopping=</span><span class="s3">True, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">).fit(X_iris</span><span class="s3">, </span><span class="s1">y_iris)</span>

    <span class="s1">msg = </span><span class="s4">&quot;partial_fit does not support early_stopping=True&quot;</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError</span><span class="s3">, </span><span class="s1">match=msg):</span>
        <span class="s1">mlp.partial_fit(X_iris</span><span class="s3">, </span><span class="s1">y_iris)</span>
</pre>
</body>
</html>