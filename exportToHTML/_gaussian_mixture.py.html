<html>
<head>
<title>_gaussian_mixture.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_gaussian_mixture.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Gaussian Mixture Model.&quot;&quot;&quot;</span>

<span class="s2"># Author: Wei Xue &lt;xuewei4d@gmail.com&gt;</span>
<span class="s2"># Modified by Thierry Guillemot &lt;thierry.guillemot.work@gmail.com&gt;</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">linalg</span>

<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">check_array</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">StrOptions</span>
<span class="s3">from </span><span class="s1">..utils.extmath </span><span class="s3">import </span><span class="s1">row_norms</span>
<span class="s3">from </span><span class="s1">._base </span><span class="s3">import </span><span class="s1">BaseMixture</span><span class="s3">, </span><span class="s1">_check_shape</span>

<span class="s2">###############################################################################</span>
<span class="s2"># Gaussian mixture shape checkers used by the GaussianMixture class</span>


<span class="s3">def </span><span class="s1">_check_weights(weights</span><span class="s3">, </span><span class="s1">n_components):</span>
    <span class="s0">&quot;&quot;&quot;Check the user provided 'weights'. 
 
    Parameters 
    ---------- 
    weights : array-like of shape (n_components,) 
        The proportions of components of each mixture. 
 
    n_components : int 
        Number of components. 
 
    Returns 
    ------- 
    weights : array, shape (n_components,) 
    &quot;&quot;&quot;</span>
    <span class="s1">weights = check_array(weights</span><span class="s3">, </span><span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">, </span><span class="s1">ensure_2d=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">_check_shape(weights</span><span class="s3">, </span><span class="s1">(n_components</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s4">&quot;weights&quot;</span><span class="s1">)</span>

    <span class="s2"># check range</span>
    <span class="s3">if </span><span class="s1">any(np.less(weights</span><span class="s3">, </span><span class="s5">0.0</span><span class="s1">)) </span><span class="s3">or </span><span class="s1">any(np.greater(weights</span><span class="s3">, </span><span class="s5">1.0</span><span class="s1">)):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;The parameter 'weights' should be in the range &quot;</span>
            <span class="s4">&quot;[0, 1], but got max value %.5f, min value %.5f&quot;</span>
            <span class="s1">% (np.min(weights)</span><span class="s3">, </span><span class="s1">np.max(weights))</span>
        <span class="s1">)</span>

    <span class="s2"># check normalization</span>
    <span class="s3">if not </span><span class="s1">np.allclose(np.abs(</span><span class="s5">1.0 </span><span class="s1">- np.sum(weights))</span><span class="s3">, </span><span class="s5">0.0</span><span class="s1">):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;The parameter 'weights' should be normalized, but got sum(weights) = %.5f&quot;</span>
            <span class="s1">% np.sum(weights)</span>
        <span class="s1">)</span>
    <span class="s3">return </span><span class="s1">weights</span>


<span class="s3">def </span><span class="s1">_check_means(means</span><span class="s3">, </span><span class="s1">n_components</span><span class="s3">, </span><span class="s1">n_features):</span>
    <span class="s0">&quot;&quot;&quot;Validate the provided 'means'. 
 
    Parameters 
    ---------- 
    means : array-like of shape (n_components, n_features) 
        The centers of the current components. 
 
    n_components : int 
        Number of components. 
 
    n_features : int 
        Number of features. 
 
    Returns 
    ------- 
    means : array, (n_components, n_features) 
    &quot;&quot;&quot;</span>
    <span class="s1">means = check_array(means</span><span class="s3">, </span><span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">, </span><span class="s1">ensure_2d=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">_check_shape(means</span><span class="s3">, </span><span class="s1">(n_components</span><span class="s3">, </span><span class="s1">n_features)</span><span class="s3">, </span><span class="s4">&quot;means&quot;</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">means</span>


<span class="s3">def </span><span class="s1">_check_precision_positivity(precision</span><span class="s3">, </span><span class="s1">covariance_type):</span>
    <span class="s0">&quot;&quot;&quot;Check a precision vector is positive-definite.&quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">np.any(np.less_equal(precision</span><span class="s3">, </span><span class="s5">0.0</span><span class="s1">)):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;'%s precision' should be positive&quot; </span><span class="s1">% covariance_type)</span>


<span class="s3">def </span><span class="s1">_check_precision_matrix(precision</span><span class="s3">, </span><span class="s1">covariance_type):</span>
    <span class="s0">&quot;&quot;&quot;Check a precision matrix is symmetric and positive-definite.&quot;&quot;&quot;</span>
    <span class="s3">if not </span><span class="s1">(</span>
        <span class="s1">np.allclose(precision</span><span class="s3">, </span><span class="s1">precision.T) </span><span class="s3">and </span><span class="s1">np.all(linalg.eigvalsh(precision) &gt; </span><span class="s5">0.0</span><span class="s1">)</span>
    <span class="s1">):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;'%s precision' should be symmetric, positive-definite&quot; </span><span class="s1">% covariance_type</span>
        <span class="s1">)</span>


<span class="s3">def </span><span class="s1">_check_precisions_full(precisions</span><span class="s3">, </span><span class="s1">covariance_type):</span>
    <span class="s0">&quot;&quot;&quot;Check the precision matrices are symmetric and positive-definite.&quot;&quot;&quot;</span>
    <span class="s3">for </span><span class="s1">prec </span><span class="s3">in </span><span class="s1">precisions:</span>
        <span class="s1">_check_precision_matrix(prec</span><span class="s3">, </span><span class="s1">covariance_type)</span>


<span class="s3">def </span><span class="s1">_check_precisions(precisions</span><span class="s3">, </span><span class="s1">covariance_type</span><span class="s3">, </span><span class="s1">n_components</span><span class="s3">, </span><span class="s1">n_features):</span>
    <span class="s0">&quot;&quot;&quot;Validate user provided precisions. 
 
    Parameters 
    ---------- 
    precisions : array-like 
        'full' : shape of (n_components, n_features, n_features) 
        'tied' : shape of (n_features, n_features) 
        'diag' : shape of (n_components, n_features) 
        'spherical' : shape of (n_components,) 
 
    covariance_type : str 
 
    n_components : int 
        Number of components. 
 
    n_features : int 
        Number of features. 
 
    Returns 
    ------- 
    precisions : array 
    &quot;&quot;&quot;</span>
    <span class="s1">precisions = check_array(</span>
        <span class="s1">precisions</span><span class="s3">,</span>
        <span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">,</span>
        <span class="s1">ensure_2d=</span><span class="s3">False,</span>
        <span class="s1">allow_nd=covariance_type == </span><span class="s4">&quot;full&quot;</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s1">precisions_shape = {</span>
        <span class="s4">&quot;full&quot;</span><span class="s1">: (n_components</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">, </span><span class="s1">n_features)</span><span class="s3">,</span>
        <span class="s4">&quot;tied&quot;</span><span class="s1">: (n_features</span><span class="s3">, </span><span class="s1">n_features)</span><span class="s3">,</span>
        <span class="s4">&quot;diag&quot;</span><span class="s1">: (n_components</span><span class="s3">, </span><span class="s1">n_features)</span><span class="s3">,</span>
        <span class="s4">&quot;spherical&quot;</span><span class="s1">: (n_components</span><span class="s3">,</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">}</span>
    <span class="s1">_check_shape(</span>
        <span class="s1">precisions</span><span class="s3">, </span><span class="s1">precisions_shape[covariance_type]</span><span class="s3">, </span><span class="s4">&quot;%s precision&quot; </span><span class="s1">% covariance_type</span>
    <span class="s1">)</span>

    <span class="s1">_check_precisions = {</span>
        <span class="s4">&quot;full&quot;</span><span class="s1">: _check_precisions_full</span><span class="s3">,</span>
        <span class="s4">&quot;tied&quot;</span><span class="s1">: _check_precision_matrix</span><span class="s3">,</span>
        <span class="s4">&quot;diag&quot;</span><span class="s1">: _check_precision_positivity</span><span class="s3">,</span>
        <span class="s4">&quot;spherical&quot;</span><span class="s1">: _check_precision_positivity</span><span class="s3">,</span>
    <span class="s1">}</span>
    <span class="s1">_check_precisions[covariance_type](precisions</span><span class="s3">, </span><span class="s1">covariance_type)</span>
    <span class="s3">return </span><span class="s1">precisions</span>


<span class="s2">###############################################################################</span>
<span class="s2"># Gaussian mixture parameters estimators (used by the M-Step)</span>


<span class="s3">def </span><span class="s1">_estimate_gaussian_covariances_full(resp</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">nk</span><span class="s3">, </span><span class="s1">means</span><span class="s3">, </span><span class="s1">reg_covar):</span>
    <span class="s0">&quot;&quot;&quot;Estimate the full covariance matrices. 
 
    Parameters 
    ---------- 
    resp : array-like of shape (n_samples, n_components) 
 
    X : array-like of shape (n_samples, n_features) 
 
    nk : array-like of shape (n_components,) 
 
    means : array-like of shape (n_components, n_features) 
 
    reg_covar : float 
 
    Returns 
    ------- 
    covariances : array, shape (n_components, n_features, n_features) 
        The covariance matrix of the current components. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_components</span><span class="s3">, </span><span class="s1">n_features = means.shape</span>
    <span class="s1">covariances = np.empty((n_components</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">, </span><span class="s1">n_features))</span>
    <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range(n_components):</span>
        <span class="s1">diff = X - means[k]</span>
        <span class="s1">covariances[k] = np.dot(resp[:</span><span class="s3">, </span><span class="s1">k] * diff.T</span><span class="s3">, </span><span class="s1">diff) / nk[k]</span>
        <span class="s1">covariances[k].flat[:: n_features + </span><span class="s5">1</span><span class="s1">] += reg_covar</span>
    <span class="s3">return </span><span class="s1">covariances</span>


<span class="s3">def </span><span class="s1">_estimate_gaussian_covariances_tied(resp</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">nk</span><span class="s3">, </span><span class="s1">means</span><span class="s3">, </span><span class="s1">reg_covar):</span>
    <span class="s0">&quot;&quot;&quot;Estimate the tied covariance matrix. 
 
    Parameters 
    ---------- 
    resp : array-like of shape (n_samples, n_components) 
 
    X : array-like of shape (n_samples, n_features) 
 
    nk : array-like of shape (n_components,) 
 
    means : array-like of shape (n_components, n_features) 
 
    reg_covar : float 
 
    Returns 
    ------- 
    covariance : array, shape (n_features, n_features) 
        The tied covariance matrix of the components. 
    &quot;&quot;&quot;</span>
    <span class="s1">avg_X2 = np.dot(X.T</span><span class="s3">, </span><span class="s1">X)</span>
    <span class="s1">avg_means2 = np.dot(nk * means.T</span><span class="s3">, </span><span class="s1">means)</span>
    <span class="s1">covariance = avg_X2 - avg_means2</span>
    <span class="s1">covariance /= nk.sum()</span>
    <span class="s1">covariance.flat[:: len(covariance) + </span><span class="s5">1</span><span class="s1">] += reg_covar</span>
    <span class="s3">return </span><span class="s1">covariance</span>


<span class="s3">def </span><span class="s1">_estimate_gaussian_covariances_diag(resp</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">nk</span><span class="s3">, </span><span class="s1">means</span><span class="s3">, </span><span class="s1">reg_covar):</span>
    <span class="s0">&quot;&quot;&quot;Estimate the diagonal covariance vectors. 
 
    Parameters 
    ---------- 
    responsibilities : array-like of shape (n_samples, n_components) 
 
    X : array-like of shape (n_samples, n_features) 
 
    nk : array-like of shape (n_components,) 
 
    means : array-like of shape (n_components, n_features) 
 
    reg_covar : float 
 
    Returns 
    ------- 
    covariances : array, shape (n_components, n_features) 
        The covariance vector of the current components. 
    &quot;&quot;&quot;</span>
    <span class="s1">avg_X2 = np.dot(resp.T</span><span class="s3">, </span><span class="s1">X * X) / nk[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">avg_means2 = means**</span><span class="s5">2</span>
    <span class="s1">avg_X_means = means * np.dot(resp.T</span><span class="s3">, </span><span class="s1">X) / nk[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
    <span class="s3">return </span><span class="s1">avg_X2 - </span><span class="s5">2 </span><span class="s1">* avg_X_means + avg_means2 + reg_covar</span>


<span class="s3">def </span><span class="s1">_estimate_gaussian_covariances_spherical(resp</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">nk</span><span class="s3">, </span><span class="s1">means</span><span class="s3">, </span><span class="s1">reg_covar):</span>
    <span class="s0">&quot;&quot;&quot;Estimate the spherical variance values. 
 
    Parameters 
    ---------- 
    responsibilities : array-like of shape (n_samples, n_components) 
 
    X : array-like of shape (n_samples, n_features) 
 
    nk : array-like of shape (n_components,) 
 
    means : array-like of shape (n_components, n_features) 
 
    reg_covar : float 
 
    Returns 
    ------- 
    variances : array, shape (n_components,) 
        The variance values of each components. 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">_estimate_gaussian_covariances_diag(resp</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">nk</span><span class="s3">, </span><span class="s1">means</span><span class="s3">, </span><span class="s1">reg_covar).mean(</span><span class="s5">1</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_estimate_gaussian_parameters(X</span><span class="s3">, </span><span class="s1">resp</span><span class="s3">, </span><span class="s1">reg_covar</span><span class="s3">, </span><span class="s1">covariance_type):</span>
    <span class="s0">&quot;&quot;&quot;Estimate the Gaussian distribution parameters. 
 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
        The input data array. 
 
    resp : array-like of shape (n_samples, n_components) 
        The responsibilities for each data sample in X. 
 
    reg_covar : float 
        The regularization added to the diagonal of the covariance matrices. 
 
    covariance_type : {'full', 'tied', 'diag', 'spherical'} 
        The type of precision matrices. 
 
    Returns 
    ------- 
    nk : array-like of shape (n_components,) 
        The numbers of data samples in the current components. 
 
    means : array-like of shape (n_components, n_features) 
        The centers of the current components. 
 
    covariances : array-like 
        The covariance matrix of the current components. 
        The shape depends of the covariance_type. 
    &quot;&quot;&quot;</span>
    <span class="s1">nk = resp.sum(axis=</span><span class="s5">0</span><span class="s1">) + </span><span class="s5">10 </span><span class="s1">* np.finfo(resp.dtype).eps</span>
    <span class="s1">means = np.dot(resp.T</span><span class="s3">, </span><span class="s1">X) / nk[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">covariances = {</span>
        <span class="s4">&quot;full&quot;</span><span class="s1">: _estimate_gaussian_covariances_full</span><span class="s3">,</span>
        <span class="s4">&quot;tied&quot;</span><span class="s1">: _estimate_gaussian_covariances_tied</span><span class="s3">,</span>
        <span class="s4">&quot;diag&quot;</span><span class="s1">: _estimate_gaussian_covariances_diag</span><span class="s3">,</span>
        <span class="s4">&quot;spherical&quot;</span><span class="s1">: _estimate_gaussian_covariances_spherical</span><span class="s3">,</span>
    <span class="s1">}[covariance_type](resp</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">nk</span><span class="s3">, </span><span class="s1">means</span><span class="s3">, </span><span class="s1">reg_covar)</span>
    <span class="s3">return </span><span class="s1">nk</span><span class="s3">, </span><span class="s1">means</span><span class="s3">, </span><span class="s1">covariances</span>


<span class="s3">def </span><span class="s1">_compute_precision_cholesky(covariances</span><span class="s3">, </span><span class="s1">covariance_type):</span>
    <span class="s0">&quot;&quot;&quot;Compute the Cholesky decomposition of the precisions. 
 
    Parameters 
    ---------- 
    covariances : array-like 
        The covariance matrix of the current components. 
        The shape depends of the covariance_type. 
 
    covariance_type : {'full', 'tied', 'diag', 'spherical'} 
        The type of precision matrices. 
 
    Returns 
    ------- 
    precisions_cholesky : array-like 
        The cholesky decomposition of sample precisions of the current 
        components. The shape depends of the covariance_type. 
    &quot;&quot;&quot;</span>
    <span class="s1">estimate_precision_error_message = (</span>
        <span class="s4">&quot;Fitting the mixture model failed because some components have &quot;</span>
        <span class="s4">&quot;ill-defined empirical covariance (for instance caused by singleton &quot;</span>
        <span class="s4">&quot;or collapsed samples). Try to decrease the number of components, &quot;</span>
        <span class="s4">&quot;or increase reg_covar.&quot;</span>
    <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">covariance_type == </span><span class="s4">&quot;full&quot;</span><span class="s1">:</span>
        <span class="s1">n_components</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">, </span><span class="s1">_ = covariances.shape</span>
        <span class="s1">precisions_chol = np.empty((n_components</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">, </span><span class="s1">n_features))</span>
        <span class="s3">for </span><span class="s1">k</span><span class="s3">, </span><span class="s1">covariance </span><span class="s3">in </span><span class="s1">enumerate(covariances):</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s1">cov_chol = linalg.cholesky(covariance</span><span class="s3">, </span><span class="s1">lower=</span><span class="s3">True</span><span class="s1">)</span>
            <span class="s3">except </span><span class="s1">linalg.LinAlgError:</span>
                <span class="s3">raise </span><span class="s1">ValueError(estimate_precision_error_message)</span>
            <span class="s1">precisions_chol[k] = linalg.solve_triangular(</span>
                <span class="s1">cov_chol</span><span class="s3">, </span><span class="s1">np.eye(n_features)</span><span class="s3">, </span><span class="s1">lower=</span><span class="s3">True</span>
            <span class="s1">).T</span>
    <span class="s3">elif </span><span class="s1">covariance_type == </span><span class="s4">&quot;tied&quot;</span><span class="s1">:</span>
        <span class="s1">_</span><span class="s3">, </span><span class="s1">n_features = covariances.shape</span>
        <span class="s3">try</span><span class="s1">:</span>
            <span class="s1">cov_chol = linalg.cholesky(covariances</span><span class="s3">, </span><span class="s1">lower=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s3">except </span><span class="s1">linalg.LinAlgError:</span>
            <span class="s3">raise </span><span class="s1">ValueError(estimate_precision_error_message)</span>
        <span class="s1">precisions_chol = linalg.solve_triangular(</span>
            <span class="s1">cov_chol</span><span class="s3">, </span><span class="s1">np.eye(n_features)</span><span class="s3">, </span><span class="s1">lower=</span><span class="s3">True</span>
        <span class="s1">).T</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">np.any(np.less_equal(covariances</span><span class="s3">, </span><span class="s5">0.0</span><span class="s1">)):</span>
            <span class="s3">raise </span><span class="s1">ValueError(estimate_precision_error_message)</span>
        <span class="s1">precisions_chol = </span><span class="s5">1.0 </span><span class="s1">/ np.sqrt(covariances)</span>
    <span class="s3">return </span><span class="s1">precisions_chol</span>


<span class="s3">def </span><span class="s1">_flipudlr(array):</span>
    <span class="s0">&quot;&quot;&quot;Reverse the rows and columns of an array.&quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">np.flipud(np.fliplr(array))</span>


<span class="s3">def </span><span class="s1">_compute_precision_cholesky_from_precisions(precisions</span><span class="s3">, </span><span class="s1">covariance_type):</span>
    <span class="s0">r&quot;&quot;&quot;Compute the Cholesky decomposition of precisions using precisions themselves. 
 
    As implemented in :func:`_compute_precision_cholesky`, the `precisions_cholesky_` is 
    an upper-triangular matrix for each Gaussian component, which can be expressed as 
    the $UU^T$ factorization of the precision matrix for each Gaussian component, where 
    $U$ is an upper-triangular matrix. 
 
    In order to use the Cholesky decomposition to get $UU^T$, the precision matrix 
    $\Lambda$ needs to be permutated such that its rows and columns are reversed, which 
    can be done by applying a similarity transformation with an exchange matrix $J$, 
    where the 1 elements reside on the anti-diagonal and all other elements are 0. In 
    particular, the Cholesky decomposition of the transformed precision matrix is 
    $J\Lambda J=LL^T$, where $L$ is a lower-triangular matrix. Because $\Lambda=UU^T$ 
    and $J=J^{-1}=J^T$, the `precisions_cholesky_` for each Gaussian component can be 
    expressed as $JLJ$. 
 
    Refer to #26415 for details. 
 
    Parameters 
    ---------- 
    precisions : array-like 
        The precision matrix of the current components. 
        The shape depends on the covariance_type. 
 
    covariance_type : {'full', 'tied', 'diag', 'spherical'} 
        The type of precision matrices. 
 
    Returns 
    ------- 
    precisions_cholesky : array-like 
        The cholesky decomposition of sample precisions of the current 
        components. The shape depends on the covariance_type. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">covariance_type == </span><span class="s4">&quot;full&quot;</span><span class="s1">:</span>
        <span class="s1">precisions_cholesky = np.array(</span>
            <span class="s1">[</span>
                <span class="s1">_flipudlr(linalg.cholesky(_flipudlr(precision)</span><span class="s3">, </span><span class="s1">lower=</span><span class="s3">True</span><span class="s1">))</span>
                <span class="s3">for </span><span class="s1">precision </span><span class="s3">in </span><span class="s1">precisions</span>
            <span class="s1">]</span>
        <span class="s1">)</span>
    <span class="s3">elif </span><span class="s1">covariance_type == </span><span class="s4">&quot;tied&quot;</span><span class="s1">:</span>
        <span class="s1">precisions_cholesky = _flipudlr(</span>
            <span class="s1">linalg.cholesky(_flipudlr(precisions)</span><span class="s3">, </span><span class="s1">lower=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">precisions_cholesky = np.sqrt(precisions)</span>
    <span class="s3">return </span><span class="s1">precisions_cholesky</span>


<span class="s2">###############################################################################</span>
<span class="s2"># Gaussian mixture probability estimators</span>
<span class="s3">def </span><span class="s1">_compute_log_det_cholesky(matrix_chol</span><span class="s3">, </span><span class="s1">covariance_type</span><span class="s3">, </span><span class="s1">n_features):</span>
    <span class="s0">&quot;&quot;&quot;Compute the log-det of the cholesky decomposition of matrices. 
 
    Parameters 
    ---------- 
    matrix_chol : array-like 
        Cholesky decompositions of the matrices. 
        'full' : shape of (n_components, n_features, n_features) 
        'tied' : shape of (n_features, n_features) 
        'diag' : shape of (n_components, n_features) 
        'spherical' : shape of (n_components,) 
 
    covariance_type : {'full', 'tied', 'diag', 'spherical'} 
 
    n_features : int 
        Number of features. 
 
    Returns 
    ------- 
    log_det_precision_chol : array-like of shape (n_components,) 
        The determinant of the precision matrix for each component. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">covariance_type == </span><span class="s4">&quot;full&quot;</span><span class="s1">:</span>
        <span class="s1">n_components</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ = matrix_chol.shape</span>
        <span class="s1">log_det_chol = np.sum(</span>
            <span class="s1">np.log(matrix_chol.reshape(n_components</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">)[:</span><span class="s3">, </span><span class="s1">:: n_features + </span><span class="s5">1</span><span class="s1">])</span><span class="s3">, </span><span class="s5">1</span>
        <span class="s1">)</span>

    <span class="s3">elif </span><span class="s1">covariance_type == </span><span class="s4">&quot;tied&quot;</span><span class="s1">:</span>
        <span class="s1">log_det_chol = np.sum(np.log(np.diag(matrix_chol)))</span>

    <span class="s3">elif </span><span class="s1">covariance_type == </span><span class="s4">&quot;diag&quot;</span><span class="s1">:</span>
        <span class="s1">log_det_chol = np.sum(np.log(matrix_chol)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">log_det_chol = n_features * (np.log(matrix_chol))</span>

    <span class="s3">return </span><span class="s1">log_det_chol</span>


<span class="s3">def </span><span class="s1">_estimate_log_gaussian_prob(X</span><span class="s3">, </span><span class="s1">means</span><span class="s3">, </span><span class="s1">precisions_chol</span><span class="s3">, </span><span class="s1">covariance_type):</span>
    <span class="s0">&quot;&quot;&quot;Estimate the log Gaussian probability. 
 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
 
    means : array-like of shape (n_components, n_features) 
 
    precisions_chol : array-like 
        Cholesky decompositions of the precision matrices. 
        'full' : shape of (n_components, n_features, n_features) 
        'tied' : shape of (n_features, n_features) 
        'diag' : shape of (n_components, n_features) 
        'spherical' : shape of (n_components,) 
 
    covariance_type : {'full', 'tied', 'diag', 'spherical'} 
 
    Returns 
    ------- 
    log_prob : array, shape (n_samples, n_components) 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">n_components</span><span class="s3">, </span><span class="s1">_ = means.shape</span>
    <span class="s2"># The determinant of the precision matrix from the Cholesky decomposition</span>
    <span class="s2"># corresponds to the negative half of the determinant of the full precision</span>
    <span class="s2"># matrix.</span>
    <span class="s2"># In short: det(precision_chol) = - det(precision) / 2</span>
    <span class="s1">log_det = _compute_log_det_cholesky(precisions_chol</span><span class="s3">, </span><span class="s1">covariance_type</span><span class="s3">, </span><span class="s1">n_features)</span>

    <span class="s3">if </span><span class="s1">covariance_type == </span><span class="s4">&quot;full&quot;</span><span class="s1">:</span>
        <span class="s1">log_prob = np.empty((n_samples</span><span class="s3">, </span><span class="s1">n_components))</span>
        <span class="s3">for </span><span class="s1">k</span><span class="s3">, </span><span class="s1">(mu</span><span class="s3">, </span><span class="s1">prec_chol) </span><span class="s3">in </span><span class="s1">enumerate(zip(means</span><span class="s3">, </span><span class="s1">precisions_chol)):</span>
            <span class="s1">y = np.dot(X</span><span class="s3">, </span><span class="s1">prec_chol) - np.dot(mu</span><span class="s3">, </span><span class="s1">prec_chol)</span>
            <span class="s1">log_prob[:</span><span class="s3">, </span><span class="s1">k] = np.sum(np.square(y)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">elif </span><span class="s1">covariance_type == </span><span class="s4">&quot;tied&quot;</span><span class="s1">:</span>
        <span class="s1">log_prob = np.empty((n_samples</span><span class="s3">, </span><span class="s1">n_components))</span>
        <span class="s3">for </span><span class="s1">k</span><span class="s3">, </span><span class="s1">mu </span><span class="s3">in </span><span class="s1">enumerate(means):</span>
            <span class="s1">y = np.dot(X</span><span class="s3">, </span><span class="s1">precisions_chol) - np.dot(mu</span><span class="s3">, </span><span class="s1">precisions_chol)</span>
            <span class="s1">log_prob[:</span><span class="s3">, </span><span class="s1">k] = np.sum(np.square(y)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">elif </span><span class="s1">covariance_type == </span><span class="s4">&quot;diag&quot;</span><span class="s1">:</span>
        <span class="s1">precisions = precisions_chol**</span><span class="s5">2</span>
        <span class="s1">log_prob = (</span>
            <span class="s1">np.sum((means**</span><span class="s5">2 </span><span class="s1">* precisions)</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
            <span class="s1">- </span><span class="s5">2.0 </span><span class="s1">* np.dot(X</span><span class="s3">, </span><span class="s1">(means * precisions).T)</span>
            <span class="s1">+ np.dot(X**</span><span class="s5">2</span><span class="s3">, </span><span class="s1">precisions.T)</span>
        <span class="s1">)</span>

    <span class="s3">elif </span><span class="s1">covariance_type == </span><span class="s4">&quot;spherical&quot;</span><span class="s1">:</span>
        <span class="s1">precisions = precisions_chol**</span><span class="s5">2</span>
        <span class="s1">log_prob = (</span>
            <span class="s1">np.sum(means**</span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">) * precisions</span>
            <span class="s1">- </span><span class="s5">2 </span><span class="s1">* np.dot(X</span><span class="s3">, </span><span class="s1">means.T * precisions)</span>
            <span class="s1">+ np.outer(row_norms(X</span><span class="s3">, </span><span class="s1">squared=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">, </span><span class="s1">precisions)</span>
        <span class="s1">)</span>
    <span class="s2"># Since we are using the precision of the Cholesky decomposition,</span>
    <span class="s2"># `- 0.5 * log_det_precision` becomes `+ log_det_precision_chol`</span>
    <span class="s3">return </span><span class="s1">-</span><span class="s5">0.5 </span><span class="s1">* (n_features * np.log(</span><span class="s5">2 </span><span class="s1">* np.pi) + log_prob) + log_det</span>


<span class="s3">class </span><span class="s1">GaussianMixture(BaseMixture):</span>
    <span class="s0">&quot;&quot;&quot;Gaussian Mixture. 
 
    Representation of a Gaussian mixture model probability distribution. 
    This class allows to estimate the parameters of a Gaussian mixture 
    distribution. 
 
    Read more in the :ref:`User Guide &lt;gmm&gt;`. 
 
    .. versionadded:: 0.18 
 
    Parameters 
    ---------- 
    n_components : int, default=1 
        The number of mixture components. 
 
    covariance_type : {'full', 'tied', 'diag', 'spherical'}, default='full' 
        String describing the type of covariance parameters to use. 
        Must be one of: 
 
        - 'full': each component has its own general covariance matrix. 
        - 'tied': all components share the same general covariance matrix. 
        - 'diag': each component has its own diagonal covariance matrix. 
        - 'spherical': each component has its own single variance. 
 
    tol : float, default=1e-3 
        The convergence threshold. EM iterations will stop when the 
        lower bound average gain is below this threshold. 
 
    reg_covar : float, default=1e-6 
        Non-negative regularization added to the diagonal of covariance. 
        Allows to assure that the covariance matrices are all positive. 
 
    max_iter : int, default=100 
        The number of EM iterations to perform. 
 
    n_init : int, default=1 
        The number of initializations to perform. The best results are kept. 
 
    init_params : {'kmeans', 'k-means++', 'random', 'random_from_data'}, \ 
    default='kmeans' 
        The method used to initialize the weights, the means and the 
        precisions. 
        String must be one of: 
 
        - 'kmeans' : responsibilities are initialized using kmeans. 
        - 'k-means++' : use the k-means++ method to initialize. 
        - 'random' : responsibilities are initialized randomly. 
        - 'random_from_data' : initial means are randomly selected data points. 
 
        .. versionchanged:: v1.1 
            `init_params` now accepts 'random_from_data' and 'k-means++' as 
            initialization methods. 
 
    weights_init : array-like of shape (n_components, ), default=None 
        The user-provided initial weights. 
        If it is None, weights are initialized using the `init_params` method. 
 
    means_init : array-like of shape (n_components, n_features), default=None 
        The user-provided initial means, 
        If it is None, means are initialized using the `init_params` method. 
 
    precisions_init : array-like, default=None 
        The user-provided initial precisions (inverse of the covariance 
        matrices). 
        If it is None, precisions are initialized using the 'init_params' 
        method. 
        The shape depends on 'covariance_type':: 
 
            (n_components,)                        if 'spherical', 
            (n_features, n_features)               if 'tied', 
            (n_components, n_features)             if 'diag', 
            (n_components, n_features, n_features) if 'full' 
 
    random_state : int, RandomState instance or None, default=None 
        Controls the random seed given to the method chosen to initialize the 
        parameters (see `init_params`). 
        In addition, it controls the generation of random samples from the 
        fitted distribution (see the method `sample`). 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    warm_start : bool, default=False 
        If 'warm_start' is True, the solution of the last fitting is used as 
        initialization for the next call of fit(). This can speed up 
        convergence when fit is called several times on similar problems. 
        In that case, 'n_init' is ignored and only a single initialization 
        occurs upon the first call. 
        See :term:`the Glossary &lt;warm_start&gt;`. 
 
    verbose : int, default=0 
        Enable verbose output. If 1 then it prints the current 
        initialization and each iteration step. If greater than 1 then 
        it prints also the log probability and the time needed 
        for each step. 
 
    verbose_interval : int, default=10 
        Number of iteration done before the next print. 
 
    Attributes 
    ---------- 
    weights_ : array-like of shape (n_components,) 
        The weights of each mixture components. 
 
    means_ : array-like of shape (n_components, n_features) 
        The mean of each mixture component. 
 
    covariances_ : array-like 
        The covariance of each mixture component. 
        The shape depends on `covariance_type`:: 
 
            (n_components,)                        if 'spherical', 
            (n_features, n_features)               if 'tied', 
            (n_components, n_features)             if 'diag', 
            (n_components, n_features, n_features) if 'full' 
 
    precisions_ : array-like 
        The precision matrices for each component in the mixture. A precision 
        matrix is the inverse of a covariance matrix. A covariance matrix is 
        symmetric positive definite so the mixture of Gaussian can be 
        equivalently parameterized by the precision matrices. Storing the 
        precision matrices instead of the covariance matrices makes it more 
        efficient to compute the log-likelihood of new samples at test time. 
        The shape depends on `covariance_type`:: 
 
            (n_components,)                        if 'spherical', 
            (n_features, n_features)               if 'tied', 
            (n_components, n_features)             if 'diag', 
            (n_components, n_features, n_features) if 'full' 
 
    precisions_cholesky_ : array-like 
        The cholesky decomposition of the precision matrices of each mixture 
        component. A precision matrix is the inverse of a covariance matrix. 
        A covariance matrix is symmetric positive definite so the mixture of 
        Gaussian can be equivalently parameterized by the precision matrices. 
        Storing the precision matrices instead of the covariance matrices makes 
        it more efficient to compute the log-likelihood of new samples at test 
        time. The shape depends on `covariance_type`:: 
 
            (n_components,)                        if 'spherical', 
            (n_features, n_features)               if 'tied', 
            (n_components, n_features)             if 'diag', 
            (n_components, n_features, n_features) if 'full' 
 
    converged_ : bool 
        True when convergence was reached in fit(), False otherwise. 
 
    n_iter_ : int 
        Number of step used by the best fit of EM to reach the convergence. 
 
    lower_bound_ : float 
        Lower bound value on the log-likelihood (of the training data with 
        respect to the model) of the best fit of EM. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    BayesianGaussianMixture : Gaussian mixture model fit with a variational 
        inference. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.mixture import GaussianMixture 
    &gt;&gt;&gt; X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]]) 
    &gt;&gt;&gt; gm = GaussianMixture(n_components=2, random_state=0).fit(X) 
    &gt;&gt;&gt; gm.means_ 
    array([[10.,  2.], 
           [ 1.,  2.]]) 
    &gt;&gt;&gt; gm.predict([[0, 0], [12, 3]]) 
    array([1, 0]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**BaseMixture._parameter_constraints</span><span class="s3">,</span>
        <span class="s4">&quot;covariance_type&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;full&quot;</span><span class="s3">, </span><span class="s4">&quot;tied&quot;</span><span class="s3">, </span><span class="s4">&quot;diag&quot;</span><span class="s3">, </span><span class="s4">&quot;spherical&quot;</span><span class="s1">})]</span><span class="s3">,</span>
        <span class="s4">&quot;weights_init&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;means_init&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;precisions_init&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">n_components=</span><span class="s5">1</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">covariance_type=</span><span class="s4">&quot;full&quot;</span><span class="s3">,</span>
        <span class="s1">tol=</span><span class="s5">1e-3</span><span class="s3">,</span>
        <span class="s1">reg_covar=</span><span class="s5">1e-6</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s5">100</span><span class="s3">,</span>
        <span class="s1">n_init=</span><span class="s5">1</span><span class="s3">,</span>
        <span class="s1">init_params=</span><span class="s4">&quot;kmeans&quot;</span><span class="s3">,</span>
        <span class="s1">weights_init=</span><span class="s3">None,</span>
        <span class="s1">means_init=</span><span class="s3">None,</span>
        <span class="s1">precisions_init=</span><span class="s3">None,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
        <span class="s1">warm_start=</span><span class="s3">False,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">verbose_interval=</span><span class="s5">10</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">n_components=n_components</span><span class="s3">,</span>
            <span class="s1">tol=tol</span><span class="s3">,</span>
            <span class="s1">reg_covar=reg_covar</span><span class="s3">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
            <span class="s1">n_init=n_init</span><span class="s3">,</span>
            <span class="s1">init_params=init_params</span><span class="s3">,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
            <span class="s1">warm_start=warm_start</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
            <span class="s1">verbose_interval=verbose_interval</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s1">self.covariance_type = covariance_type</span>
        <span class="s1">self.weights_init = weights_init</span>
        <span class="s1">self.means_init = means_init</span>
        <span class="s1">self.precisions_init = precisions_init</span>

    <span class="s3">def </span><span class="s1">_check_parameters(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Check the Gaussian mixture parameters are well defined.&quot;&quot;&quot;</span>
        <span class="s1">_</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>

        <span class="s3">if </span><span class="s1">self.weights_init </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">self.weights_init = _check_weights(self.weights_init</span><span class="s3">, </span><span class="s1">self.n_components)</span>

        <span class="s3">if </span><span class="s1">self.means_init </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">self.means_init = _check_means(</span>
                <span class="s1">self.means_init</span><span class="s3">, </span><span class="s1">self.n_components</span><span class="s3">, </span><span class="s1">n_features</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.precisions_init </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">self.precisions_init = _check_precisions(</span>
                <span class="s1">self.precisions_init</span><span class="s3">,</span>
                <span class="s1">self.covariance_type</span><span class="s3">,</span>
                <span class="s1">self.n_components</span><span class="s3">,</span>
                <span class="s1">n_features</span><span class="s3">,</span>
            <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_initialize(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">resp):</span>
        <span class="s0">&quot;&quot;&quot;Initialization of the Gaussian mixture parameters. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
 
        resp : array-like of shape (n_samples, n_components) 
        &quot;&quot;&quot;</span>
        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">_ = X.shape</span>

        <span class="s1">weights</span><span class="s3">, </span><span class="s1">means</span><span class="s3">, </span><span class="s1">covariances = _estimate_gaussian_parameters(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">resp</span><span class="s3">, </span><span class="s1">self.reg_covar</span><span class="s3">, </span><span class="s1">self.covariance_type</span>
        <span class="s1">)</span>
        <span class="s1">weights /= n_samples</span>

        <span class="s1">self.weights_ = weights </span><span class="s3">if </span><span class="s1">self.weights_init </span><span class="s3">is None else </span><span class="s1">self.weights_init</span>
        <span class="s1">self.means_ = means </span><span class="s3">if </span><span class="s1">self.means_init </span><span class="s3">is None else </span><span class="s1">self.means_init</span>

        <span class="s3">if </span><span class="s1">self.precisions_init </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self.covariances_ = covariances</span>
            <span class="s1">self.precisions_cholesky_ = _compute_precision_cholesky(</span>
                <span class="s1">covariances</span><span class="s3">, </span><span class="s1">self.covariance_type</span>
            <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.precisions_cholesky_ = _compute_precision_cholesky_from_precisions(</span>
                <span class="s1">self.precisions_init</span><span class="s3">, </span><span class="s1">self.covariance_type</span>
            <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_m_step(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">log_resp):</span>
        <span class="s0">&quot;&quot;&quot;M step. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
 
        log_resp : array-like of shape (n_samples, n_components) 
            Logarithm of the posterior probabilities (or responsibilities) of 
            the point of each sample in X. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.weights_</span><span class="s3">, </span><span class="s1">self.means_</span><span class="s3">, </span><span class="s1">self.covariances_ = _estimate_gaussian_parameters(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">np.exp(log_resp)</span><span class="s3">, </span><span class="s1">self.reg_covar</span><span class="s3">, </span><span class="s1">self.covariance_type</span>
        <span class="s1">)</span>
        <span class="s1">self.weights_ /= self.weights_.sum()</span>
        <span class="s1">self.precisions_cholesky_ = _compute_precision_cholesky(</span>
            <span class="s1">self.covariances_</span><span class="s3">, </span><span class="s1">self.covariance_type</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_estimate_log_prob(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s3">return </span><span class="s1">_estimate_log_gaussian_prob(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">self.means_</span><span class="s3">, </span><span class="s1">self.precisions_cholesky_</span><span class="s3">, </span><span class="s1">self.covariance_type</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_estimate_log_weights(self):</span>
        <span class="s3">return </span><span class="s1">np.log(self.weights_)</span>

    <span class="s3">def </span><span class="s1">_compute_lower_bound(self</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">log_prob_norm):</span>
        <span class="s3">return </span><span class="s1">log_prob_norm</span>

    <span class="s3">def </span><span class="s1">_get_parameters(self):</span>
        <span class="s3">return </span><span class="s1">(</span>
            <span class="s1">self.weights_</span><span class="s3">,</span>
            <span class="s1">self.means_</span><span class="s3">,</span>
            <span class="s1">self.covariances_</span><span class="s3">,</span>
            <span class="s1">self.precisions_cholesky_</span><span class="s3">,</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_set_parameters(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s1">(</span>
            <span class="s1">self.weights_</span><span class="s3">,</span>
            <span class="s1">self.means_</span><span class="s3">,</span>
            <span class="s1">self.covariances_</span><span class="s3">,</span>
            <span class="s1">self.precisions_cholesky_</span><span class="s3">,</span>
        <span class="s1">) = params</span>

        <span class="s2"># Attributes computation</span>
        <span class="s1">_</span><span class="s3">, </span><span class="s1">n_features = self.means_.shape</span>

        <span class="s3">if </span><span class="s1">self.covariance_type == </span><span class="s4">&quot;full&quot;</span><span class="s1">:</span>
            <span class="s1">self.precisions_ = np.empty(self.precisions_cholesky_.shape)</span>
            <span class="s3">for </span><span class="s1">k</span><span class="s3">, </span><span class="s1">prec_chol </span><span class="s3">in </span><span class="s1">enumerate(self.precisions_cholesky_):</span>
                <span class="s1">self.precisions_[k] = np.dot(prec_chol</span><span class="s3">, </span><span class="s1">prec_chol.T)</span>

        <span class="s3">elif </span><span class="s1">self.covariance_type == </span><span class="s4">&quot;tied&quot;</span><span class="s1">:</span>
            <span class="s1">self.precisions_ = np.dot(</span>
                <span class="s1">self.precisions_cholesky_</span><span class="s3">, </span><span class="s1">self.precisions_cholesky_.T</span>
            <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.precisions_ = self.precisions_cholesky_**</span><span class="s5">2</span>

    <span class="s3">def </span><span class="s1">_n_parameters(self):</span>
        <span class="s0">&quot;&quot;&quot;Return the number of free parameters in the model.&quot;&quot;&quot;</span>
        <span class="s1">_</span><span class="s3">, </span><span class="s1">n_features = self.means_.shape</span>
        <span class="s3">if </span><span class="s1">self.covariance_type == </span><span class="s4">&quot;full&quot;</span><span class="s1">:</span>
            <span class="s1">cov_params = self.n_components * n_features * (n_features + </span><span class="s5">1</span><span class="s1">) / </span><span class="s5">2.0</span>
        <span class="s3">elif </span><span class="s1">self.covariance_type == </span><span class="s4">&quot;diag&quot;</span><span class="s1">:</span>
            <span class="s1">cov_params = self.n_components * n_features</span>
        <span class="s3">elif </span><span class="s1">self.covariance_type == </span><span class="s4">&quot;tied&quot;</span><span class="s1">:</span>
            <span class="s1">cov_params = n_features * (n_features + </span><span class="s5">1</span><span class="s1">) / </span><span class="s5">2.0</span>
        <span class="s3">elif </span><span class="s1">self.covariance_type == </span><span class="s4">&quot;spherical&quot;</span><span class="s1">:</span>
            <span class="s1">cov_params = self.n_components</span>
        <span class="s1">mean_params = n_features * self.n_components</span>
        <span class="s3">return </span><span class="s1">int(cov_params + mean_params + self.n_components - </span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">bic(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Bayesian information criterion for the current model on the input X. 
 
        You can refer to this :ref:`mathematical section &lt;aic_bic&gt;` for more 
        details regarding the formulation of the BIC used. 
 
        Parameters 
        ---------- 
        X : array of shape (n_samples, n_dimensions) 
            The input samples. 
 
        Returns 
        ------- 
        bic : float 
            The lower the better. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">-</span><span class="s5">2 </span><span class="s1">* self.score(X) * X.shape[</span><span class="s5">0</span><span class="s1">] + self._n_parameters() * np.log(</span>
            <span class="s1">X.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">aic(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Akaike information criterion for the current model on the input X. 
 
        You can refer to this :ref:`mathematical section &lt;aic_bic&gt;` for more 
        details regarding the formulation of the AIC used. 
 
        Parameters 
        ---------- 
        X : array of shape (n_samples, n_dimensions) 
            The input samples. 
 
        Returns 
        ------- 
        aic : float 
            The lower the better. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">-</span><span class="s5">2 </span><span class="s1">* self.score(X) * X.shape[</span><span class="s5">0</span><span class="s1">] + </span><span class="s5">2 </span><span class="s1">* self._n_parameters()</span>
</pre>
</body>
</html>