<html>
<head>
<title>_polynomial.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_polynomial.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
This file contains preprocessing tools based on polynomials. 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">collections</span>
<span class="s2">from </span><span class="s1">itertools </span><span class="s2">import </span><span class="s1">chain</span><span class="s2">, </span><span class="s1">combinations</span>
<span class="s2">from </span><span class="s1">itertools </span><span class="s2">import </span><span class="s1">combinations_with_replacement </span><span class="s2">as </span><span class="s1">combinations_w_r</span>
<span class="s2">from </span><span class="s1">numbers </span><span class="s2">import </span><span class="s1">Integral</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">sparse</span>
<span class="s2">from </span><span class="s1">scipy.interpolate </span><span class="s2">import </span><span class="s1">BSpline</span>
<span class="s2">from </span><span class="s1">scipy.special </span><span class="s2">import </span><span class="s1">comb</span>

<span class="s2">from </span><span class="s1">..base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">TransformerMixin</span><span class="s2">, </span><span class="s1">_fit_context</span>
<span class="s2">from </span><span class="s1">..utils </span><span class="s2">import </span><span class="s1">check_array</span>
<span class="s2">from </span><span class="s1">..utils._param_validation </span><span class="s2">import </span><span class="s1">Interval</span><span class="s2">, </span><span class="s1">StrOptions</span>
<span class="s2">from </span><span class="s1">..utils.fixes </span><span class="s2">import </span><span class="s1">parse_version</span><span class="s2">, </span><span class="s1">sp_version</span>
<span class="s2">from </span><span class="s1">..utils.stats </span><span class="s2">import </span><span class="s1">_weighted_percentile</span>
<span class="s2">from </span><span class="s1">..utils.validation </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">FLOAT_DTYPES</span><span class="s2">,</span>
    <span class="s1">_check_feature_names_in</span><span class="s2">,</span>
    <span class="s1">_check_sample_weight</span><span class="s2">,</span>
    <span class="s1">check_is_fitted</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">._csr_polynomial_expansion </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">_calc_expanded_nnz</span><span class="s2">,</span>
    <span class="s1">_calc_total_nnz</span><span class="s2">,</span>
    <span class="s1">_csr_polynomial_expansion</span><span class="s2">,</span>
<span class="s1">)</span>

<span class="s1">__all__ = [</span>
    <span class="s3">&quot;PolynomialFeatures&quot;</span><span class="s2">,</span>
    <span class="s3">&quot;SplineTransformer&quot;</span><span class="s2">,</span>
<span class="s1">]</span>


<span class="s2">def </span><span class="s1">_create_expansion(X</span><span class="s2">, </span><span class="s1">interaction_only</span><span class="s2">, </span><span class="s1">deg</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">cumulative_size=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Helper function for creating and appending sparse expansion matrices&quot;&quot;&quot;</span>

    <span class="s1">total_nnz = _calc_total_nnz(X.indptr</span><span class="s2">, </span><span class="s1">interaction_only</span><span class="s2">, </span><span class="s1">deg)</span>
    <span class="s1">expanded_col = _calc_expanded_nnz(n_features</span><span class="s2">, </span><span class="s1">interaction_only</span><span class="s2">, </span><span class="s1">deg)</span>

    <span class="s2">if </span><span class="s1">expanded_col == </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s2">return None</span>
    <span class="s5"># This only checks whether each block needs 64bit integers upon</span>
    <span class="s5"># expansion. We prefer to keep int32 indexing where we can,</span>
    <span class="s5"># since currently SciPy's CSR construction downcasts when possible,</span>
    <span class="s5"># so we prefer to avoid an unnecessary cast. The dtype may still</span>
    <span class="s5"># change in the concatenation process if needed.</span>
    <span class="s5"># See: https://github.com/scipy/scipy/issues/16569</span>
    <span class="s1">max_indices = expanded_col - </span><span class="s4">1</span>
    <span class="s1">max_indptr = total_nnz</span>
    <span class="s1">max_int32 = np.iinfo(np.int32).max</span>
    <span class="s1">needs_int64 = max(max_indices</span><span class="s2">, </span><span class="s1">max_indptr) &gt; max_int32</span>
    <span class="s1">index_dtype = np.int64 </span><span class="s2">if </span><span class="s1">needs_int64 </span><span class="s2">else </span><span class="s1">np.int32</span>

    <span class="s5"># This is a pretty specific bug that is hard to work around by a user,</span>
    <span class="s5"># hence we do not detail the entire bug and all possible avoidance</span>
    <span class="s5"># mechnasisms. Instead we recommend upgrading scipy or shrinking their data.</span>
    <span class="s1">cumulative_size += expanded_col</span>
    <span class="s2">if </span><span class="s1">(</span>
        <span class="s1">sp_version &lt; parse_version(</span><span class="s3">&quot;1.8.0&quot;</span><span class="s1">)</span>
        <span class="s2">and </span><span class="s1">cumulative_size - </span><span class="s4">1 </span><span class="s1">&gt; max_int32</span>
        <span class="s2">and not </span><span class="s1">needs_int64</span>
    <span class="s1">):</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span>
            <span class="s3">&quot;In scipy versions `&lt;1.8.0`, the function `scipy.sparse.hstack`&quot;</span>
            <span class="s3">&quot; sometimes produces negative columns when the output shape contains&quot;</span>
            <span class="s3">&quot; `n_cols` too large to be represented by a 32bit signed&quot;</span>
            <span class="s3">&quot; integer. To avoid this error, either use a version&quot;</span>
            <span class="s3">&quot; of scipy `&gt;=1.8.0` or alter the `PolynomialFeatures`&quot;</span>
            <span class="s3">&quot; transformer to produce fewer than 2^31 output features.&quot;</span>
        <span class="s1">)</span>

    <span class="s5"># Result of the expansion, modified in place by the</span>
    <span class="s5"># `_csr_polynomial_expansion` routine.</span>
    <span class="s1">expanded_data = np.empty(shape=total_nnz</span><span class="s2">, </span><span class="s1">dtype=X.data.dtype)</span>
    <span class="s1">expanded_indices = np.empty(shape=total_nnz</span><span class="s2">, </span><span class="s1">dtype=index_dtype)</span>
    <span class="s1">expanded_indptr = np.empty(shape=X.indptr.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=index_dtype)</span>
    <span class="s1">_csr_polynomial_expansion(</span>
        <span class="s1">X.data</span><span class="s2">,</span>
        <span class="s1">X.indices</span><span class="s2">,</span>
        <span class="s1">X.indptr</span><span class="s2">,</span>
        <span class="s1">X.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">expanded_data</span><span class="s2">,</span>
        <span class="s1">expanded_indices</span><span class="s2">,</span>
        <span class="s1">expanded_indptr</span><span class="s2">,</span>
        <span class="s1">interaction_only</span><span class="s2">,</span>
        <span class="s1">deg</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">return </span><span class="s1">sparse.csr_matrix(</span>
        <span class="s1">(expanded_data</span><span class="s2">, </span><span class="s1">expanded_indices</span><span class="s2">, </span><span class="s1">expanded_indptr)</span><span class="s2">,</span>
        <span class="s1">shape=(X.indptr.shape[</span><span class="s4">0</span><span class="s1">] - </span><span class="s4">1</span><span class="s2">, </span><span class="s1">expanded_col)</span><span class="s2">,</span>
        <span class="s1">dtype=X.dtype</span><span class="s2">,</span>
    <span class="s1">)</span>


<span class="s2">class </span><span class="s1">PolynomialFeatures(TransformerMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Generate polynomial and interaction features. 
 
    Generate a new feature matrix consisting of all polynomial combinations 
    of the features with degree less than or equal to the specified degree. 
    For example, if an input sample is two dimensional and of the form 
    [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]. 
 
    Read more in the :ref:`User Guide &lt;polynomial_features&gt;`. 
 
    Parameters 
    ---------- 
    degree : int or tuple (min_degree, max_degree), default=2 
        If a single int is given, it specifies the maximal degree of the 
        polynomial features. If a tuple `(min_degree, max_degree)` is passed, 
        then `min_degree` is the minimum and `max_degree` is the maximum 
        polynomial degree of the generated features. Note that `min_degree=0` 
        and `min_degree=1` are equivalent as outputting the degree zero term is 
        determined by `include_bias`. 
 
    interaction_only : bool, default=False 
        If `True`, only interaction features are produced: features that are 
        products of at most `degree` *distinct* input features, i.e. terms with 
        power of 2 or higher of the same input feature are excluded: 
 
            - included: `x[0]`, `x[1]`, `x[0] * x[1]`, etc. 
            - excluded: `x[0] ** 2`, `x[0] ** 2 * x[1]`, etc. 
 
    include_bias : bool, default=True 
        If `True` (default), then include a bias column, the feature in which 
        all polynomial powers are zero (i.e. a column of ones - acts as an 
        intercept term in a linear model). 
 
    order : {'C', 'F'}, default='C' 
        Order of output array in the dense case. `'F'` order is faster to 
        compute, but may slow down subsequent estimators. 
 
        .. versionadded:: 0.21 
 
    Attributes 
    ---------- 
    powers_ : ndarray of shape (`n_output_features_`, `n_features_in_`) 
        `powers_[i, j]` is the exponent of the jth input in the ith output. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    n_output_features_ : int 
        The total number of polynomial output features. The number of output 
        features is computed by iterating over all suitably sized combinations 
        of input features. 
 
    See Also 
    -------- 
    SplineTransformer : Transformer that generates univariate B-spline bases 
        for features. 
 
    Notes 
    ----- 
    Be aware that the number of features in the output array scales 
    polynomially in the number of features of the input array, and 
    exponentially in the degree. High degrees can cause overfitting. 
 
    See :ref:`examples/linear_model/plot_polynomial_interpolation.py 
    &lt;sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py&gt;` 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures 
    &gt;&gt;&gt; X = np.arange(6).reshape(3, 2) 
    &gt;&gt;&gt; X 
    array([[0, 1], 
           [2, 3], 
           [4, 5]]) 
    &gt;&gt;&gt; poly = PolynomialFeatures(2) 
    &gt;&gt;&gt; poly.fit_transform(X) 
    array([[ 1.,  0.,  1.,  0.,  0.,  1.], 
           [ 1.,  2.,  3.,  4.,  6.,  9.], 
           [ 1.,  4.,  5., 16., 20., 25.]]) 
    &gt;&gt;&gt; poly = PolynomialFeatures(interaction_only=True) 
    &gt;&gt;&gt; poly.fit_transform(X) 
    array([[ 1.,  0.,  1.,  0.], 
           [ 1.,  2.,  3.,  6.], 
           [ 1.,  4.,  5., 20.]]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s3">&quot;degree&quot;</span><span class="s1">: [Interval(Integral</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s3">&quot;left&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;array-like&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;interaction_only&quot;</span><span class="s1">: [</span><span class="s3">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;include_bias&quot;</span><span class="s1">: [</span><span class="s3">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;order&quot;</span><span class="s1">: [StrOptions({</span><span class="s3">&quot;C&quot;</span><span class="s2">, </span><span class="s3">&quot;F&quot;</span><span class="s1">})]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">, </span><span class="s1">degree=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">interaction_only=</span><span class="s2">False, </span><span class="s1">include_bias=</span><span class="s2">True, </span><span class="s1">order=</span><span class="s3">&quot;C&quot;</span>
    <span class="s1">):</span>
        <span class="s1">self.degree = degree</span>
        <span class="s1">self.interaction_only = interaction_only</span>
        <span class="s1">self.include_bias = include_bias</span>
        <span class="s1">self.order = order</span>

    <span class="s1">@staticmethod</span>
    <span class="s2">def </span><span class="s1">_combinations(</span>
        <span class="s1">n_features</span><span class="s2">, </span><span class="s1">min_degree</span><span class="s2">, </span><span class="s1">max_degree</span><span class="s2">, </span><span class="s1">interaction_only</span><span class="s2">, </span><span class="s1">include_bias</span>
    <span class="s1">):</span>
        <span class="s1">comb = combinations </span><span class="s2">if </span><span class="s1">interaction_only </span><span class="s2">else </span><span class="s1">combinations_w_r</span>
        <span class="s1">start = max(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">min_degree)</span>
        <span class="s1">iter = chain.from_iterable(</span>
            <span class="s1">comb(range(n_features)</span><span class="s2">, </span><span class="s1">i) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(start</span><span class="s2">, </span><span class="s1">max_degree + </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">include_bias:</span>
            <span class="s1">iter = chain(comb(range(n_features)</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">iter)</span>
        <span class="s2">return </span><span class="s1">iter</span>

    <span class="s1">@staticmethod</span>
    <span class="s2">def </span><span class="s1">_num_combinations(</span>
        <span class="s1">n_features</span><span class="s2">, </span><span class="s1">min_degree</span><span class="s2">, </span><span class="s1">max_degree</span><span class="s2">, </span><span class="s1">interaction_only</span><span class="s2">, </span><span class="s1">include_bias</span>
    <span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Calculate number of terms in polynomial expansion 
 
        This should be equivalent to counting the number of terms returned by 
        _combinations(...) but much faster. 
        &quot;&quot;&quot;</span>

        <span class="s2">if </span><span class="s1">interaction_only:</span>
            <span class="s1">combinations = sum(</span>
                <span class="s1">[</span>
                    <span class="s1">comb(n_features</span><span class="s2">, </span><span class="s1">i</span><span class="s2">, </span><span class="s1">exact=</span><span class="s2">True</span><span class="s1">)</span>
                    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(max(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">min_degree)</span><span class="s2">, </span><span class="s1">min(max_degree</span><span class="s2">, </span><span class="s1">n_features) + </span><span class="s4">1</span><span class="s1">)</span>
                <span class="s1">]</span>
            <span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">combinations = comb(n_features + max_degree</span><span class="s2">, </span><span class="s1">max_degree</span><span class="s2">, </span><span class="s1">exact=</span><span class="s2">True</span><span class="s1">) - </span><span class="s4">1</span>
            <span class="s2">if </span><span class="s1">min_degree &gt; </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s1">d = min_degree - </span><span class="s4">1</span>
                <span class="s1">combinations -= comb(n_features + d</span><span class="s2">, </span><span class="s1">d</span><span class="s2">, </span><span class="s1">exact=</span><span class="s2">True</span><span class="s1">) - </span><span class="s4">1</span>

        <span class="s2">if </span><span class="s1">include_bias:</span>
            <span class="s1">combinations += </span><span class="s4">1</span>

        <span class="s2">return </span><span class="s1">combinations</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">powers_(self):</span>
        <span class="s0">&quot;&quot;&quot;Exponent for each of the inputs in the output.&quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s1">combinations = self._combinations(</span>
            <span class="s1">n_features=self.n_features_in_</span><span class="s2">,</span>
            <span class="s1">min_degree=self._min_degree</span><span class="s2">,</span>
            <span class="s1">max_degree=self._max_degree</span><span class="s2">,</span>
            <span class="s1">interaction_only=self.interaction_only</span><span class="s2">,</span>
            <span class="s1">include_bias=self.include_bias</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s2">return </span><span class="s1">np.vstack(</span>
            <span class="s1">[np.bincount(c</span><span class="s2">, </span><span class="s1">minlength=self.n_features_in_) </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">combinations]</span>
        <span class="s1">)</span>

    <span class="s2">def </span><span class="s1">get_feature_names_out(self</span><span class="s2">, </span><span class="s1">input_features=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Get output feature names for transformation. 
 
        Parameters 
        ---------- 
        input_features : array-like of str or None, default=None 
            Input features. 
 
            - If `input_features is None`, then `feature_names_in_` is 
              used as feature names in. If `feature_names_in_` is not defined, 
              then the following input feature names are generated: 
              `[&quot;x0&quot;, &quot;x1&quot;, ..., &quot;x(n_features_in_ - 1)&quot;]`. 
            - If `input_features` is an array-like, then `input_features` must 
              match `feature_names_in_` if `feature_names_in_` is defined. 
 
        Returns 
        ------- 
        feature_names_out : ndarray of str objects 
            Transformed feature names. 
        &quot;&quot;&quot;</span>
        <span class="s1">powers = self.powers_</span>
        <span class="s1">input_features = _check_feature_names_in(self</span><span class="s2">, </span><span class="s1">input_features)</span>
        <span class="s1">feature_names = []</span>
        <span class="s2">for </span><span class="s1">row </span><span class="s2">in </span><span class="s1">powers:</span>
            <span class="s1">inds = np.where(row)[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s2">if </span><span class="s1">len(inds):</span>
                <span class="s1">name = </span><span class="s3">&quot; &quot;</span><span class="s1">.join(</span>
                    <span class="s1">(</span>
                        <span class="s3">&quot;%s^%d&quot; </span><span class="s1">% (input_features[ind]</span><span class="s2">, </span><span class="s1">exp)</span>
                        <span class="s2">if </span><span class="s1">exp != </span><span class="s4">1</span>
                        <span class="s2">else </span><span class="s1">input_features[ind]</span>
                    <span class="s1">)</span>
                    <span class="s2">for </span><span class="s1">ind</span><span class="s2">, </span><span class="s1">exp </span><span class="s2">in </span><span class="s1">zip(inds</span><span class="s2">, </span><span class="s1">row[inds])</span>
                <span class="s1">)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">name = </span><span class="s3">&quot;1&quot;</span>
            <span class="s1">feature_names.append(name)</span>
        <span class="s2">return </span><span class="s1">np.asarray(feature_names</span><span class="s2">, </span><span class="s1">dtype=object)</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Compute number of output features. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The data. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        Returns 
        ------- 
        self : object 
            Fitted transformer. 
        &quot;&quot;&quot;</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">n_features = self._validate_data(X</span><span class="s2">, </span><span class="s1">accept_sparse=</span><span class="s2">True</span><span class="s1">).shape</span>

        <span class="s2">if </span><span class="s1">isinstance(self.degree</span><span class="s2">, </span><span class="s1">Integral):</span>
            <span class="s2">if </span><span class="s1">self.degree == </span><span class="s4">0 </span><span class="s2">and not </span><span class="s1">self.include_bias:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s3">&quot;Setting degree to zero and include_bias to False would result in&quot;</span>
                    <span class="s3">&quot; an empty output array.&quot;</span>
                <span class="s1">)</span>

            <span class="s1">self._min_degree = </span><span class="s4">0</span>
            <span class="s1">self._max_degree = self.degree</span>
        <span class="s2">elif </span><span class="s1">(</span>
            <span class="s1">isinstance(self.degree</span><span class="s2">, </span><span class="s1">collections.abc.Iterable) </span><span class="s2">and </span><span class="s1">len(self.degree) == </span><span class="s4">2</span>
        <span class="s1">):</span>
            <span class="s1">self._min_degree</span><span class="s2">, </span><span class="s1">self._max_degree = self.degree</span>
            <span class="s2">if not </span><span class="s1">(</span>
                <span class="s1">isinstance(self._min_degree</span><span class="s2">, </span><span class="s1">Integral)</span>
                <span class="s2">and </span><span class="s1">isinstance(self._max_degree</span><span class="s2">, </span><span class="s1">Integral)</span>
                <span class="s2">and </span><span class="s1">self._min_degree &gt;= </span><span class="s4">0</span>
                <span class="s2">and </span><span class="s1">self._min_degree &lt;= self._max_degree</span>
            <span class="s1">):</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s3">&quot;degree=(min_degree, max_degree) must &quot;</span>
                    <span class="s3">&quot;be non-negative integers that fulfil &quot;</span>
                    <span class="s3">&quot;min_degree &lt;= max_degree, got &quot;</span>
                    <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">self.degree</span><span class="s2">}</span><span class="s3">.&quot;</span>
                <span class="s1">)</span>
            <span class="s2">elif </span><span class="s1">self._max_degree == </span><span class="s4">0 </span><span class="s2">and not </span><span class="s1">self.include_bias:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s3">&quot;Setting both min_degree and max_degree to zero and include_bias to&quot;</span>
                    <span class="s3">&quot; False would result in an empty output array.&quot;</span>
                <span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s3">&quot;degree must be a non-negative int or tuple &quot;</span>
                <span class="s3">&quot;(min_degree, max_degree), got &quot;</span>
                <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">self.degree</span><span class="s2">}</span><span class="s3">.&quot;</span>
            <span class="s1">)</span>

        <span class="s1">self.n_output_features_ = self._num_combinations(</span>
            <span class="s1">n_features=n_features</span><span class="s2">,</span>
            <span class="s1">min_degree=self._min_degree</span><span class="s2">,</span>
            <span class="s1">max_degree=self._max_degree</span><span class="s2">,</span>
            <span class="s1">interaction_only=self.interaction_only</span><span class="s2">,</span>
            <span class="s1">include_bias=self.include_bias</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">self.n_output_features_ &gt; np.iinfo(np.intp).max:</span>
            <span class="s1">msg = (</span>
                <span class="s3">&quot;The output that would result from the current configuration would&quot;</span>
                <span class="s3">f&quot; have </span><span class="s2">{</span><span class="s1">self.n_output_features_</span><span class="s2">} </span><span class="s3">features which is too large to be&quot;</span>
                <span class="s3">f&quot; indexed by </span><span class="s2">{</span><span class="s1">np.intp().dtype.name</span><span class="s2">}</span><span class="s3">. Please change some or all of the&quot;</span>
                <span class="s3">&quot; following:</span><span class="s2">\n</span><span class="s3">- The number of features in the input, currently&quot;</span>
                <span class="s3">f&quot; </span><span class="s2">{</span><span class="s1">n_features=</span><span class="s2">}\n</span><span class="s3">- The range of degrees to calculate, currently&quot;</span>
                <span class="s3">f&quot; [</span><span class="s2">{</span><span class="s1">self._min_degree</span><span class="s2">}</span><span class="s3">, </span><span class="s2">{</span><span class="s1">self._max_degree</span><span class="s2">}</span><span class="s3">]</span><span class="s2">\n</span><span class="s3">- Whether to include only&quot;</span>
                <span class="s3">f&quot; interaction terms, currently </span><span class="s2">{</span><span class="s1">self.interaction_only</span><span class="s2">}\n</span><span class="s3">- Whether to&quot;</span>
                <span class="s3">f&quot; include a bias term, currently </span><span class="s2">{</span><span class="s1">self.include_bias</span><span class="s2">}</span><span class="s3">.&quot;</span>
            <span class="s1">)</span>
            <span class="s2">if </span><span class="s1">(</span>
                <span class="s1">np.intp == np.int32</span>
                <span class="s2">and </span><span class="s1">self.n_output_features_ &lt;= np.iinfo(np.int64).max</span>
            <span class="s1">):  </span><span class="s5"># pragma: nocover</span>
                <span class="s1">msg += (</span>
                    <span class="s3">&quot;</span><span class="s2">\n</span><span class="s3">Note that the current Python runtime has a limited 32 bit &quot;</span>
                    <span class="s3">&quot;address space and that this configuration would have been &quot;</span>
                    <span class="s3">&quot;admissible if run on a 64 bit Python runtime.&quot;</span>
                <span class="s1">)</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg)</span>
        <span class="s5"># We also record the number of output features for</span>
        <span class="s5"># _max_degree = 0</span>
        <span class="s1">self._n_out_full = self._num_combinations(</span>
            <span class="s1">n_features=n_features</span><span class="s2">,</span>
            <span class="s1">min_degree=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">max_degree=self._max_degree</span><span class="s2">,</span>
            <span class="s1">interaction_only=self.interaction_only</span><span class="s2">,</span>
            <span class="s1">include_bias=self.include_bias</span><span class="s2">,</span>
        <span class="s1">)</span>

        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">transform(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Transform data to polynomial features. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The data to transform, row by row. 
 
            Prefer CSR over CSC for sparse input (for speed), but CSC is 
            required if the degree is 4 or higher. If the degree is less than 
            4 and the input format is CSC, it will be converted to CSR, have 
            its polynomial features generated, then converted back to CSC. 
 
            If the degree is 2 or 3, the method described in &quot;Leveraging 
            Sparsity to Speed Up Polynomial Feature Expansions of CSR Matrices 
            Using K-Simplex Numbers&quot; by Andrew Nystrom and John Hughes is 
            used, which is much faster than the method used on CSC input. For 
            this reason, a CSC input will be converted to CSR, and the output 
            will be converted back to CSC prior to being returned, hence the 
            preference of CSR. 
 
        Returns 
        ------- 
        XP : {ndarray, sparse matrix} of shape (n_samples, NP) 
            The matrix of features, where `NP` is the number of polynomial 
            features generated from the combination of inputs. If a sparse 
            matrix is provided, it will be converted into a sparse 
            `csr_matrix`. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s2">, </span><span class="s1">dtype=FLOAT_DTYPES</span><span class="s2">, </span><span class="s1">reset=</span><span class="s2">False, </span><span class="s1">accept_sparse=(</span><span class="s3">&quot;csr&quot;</span><span class="s2">, </span><span class="s3">&quot;csc&quot;</span><span class="s1">)</span>
        <span class="s1">)</span>

        <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X.shape</span>
        <span class="s1">max_int32 = np.iinfo(np.int32).max</span>
        <span class="s2">if </span><span class="s1">sparse.issparse(X) </span><span class="s2">and </span><span class="s1">X.format == </span><span class="s3">&quot;csr&quot;</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">self._max_degree &gt; </span><span class="s4">3</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">self.transform(X.tocsc()).tocsr()</span>
            <span class="s1">to_stack = []</span>
            <span class="s2">if </span><span class="s1">self.include_bias:</span>
                <span class="s1">to_stack.append(</span>
                    <span class="s1">sparse.csr_matrix(np.ones(shape=(n_samples</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype=X.dtype))</span>
                <span class="s1">)</span>
            <span class="s2">if </span><span class="s1">self._min_degree &lt;= </span><span class="s4">1 </span><span class="s2">and </span><span class="s1">self._max_degree &gt; </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s1">to_stack.append(X)</span>

            <span class="s1">cumulative_size = sum(mat.shape[</span><span class="s4">1</span><span class="s1">] </span><span class="s2">for </span><span class="s1">mat </span><span class="s2">in </span><span class="s1">to_stack)</span>
            <span class="s2">for </span><span class="s1">deg </span><span class="s2">in </span><span class="s1">range(max(</span><span class="s4">2</span><span class="s2">, </span><span class="s1">self._min_degree)</span><span class="s2">, </span><span class="s1">self._max_degree + </span><span class="s4">1</span><span class="s1">):</span>
                <span class="s1">expanded = _create_expansion(</span>
                    <span class="s1">X=X</span><span class="s2">,</span>
                    <span class="s1">interaction_only=self.interaction_only</span><span class="s2">,</span>
                    <span class="s1">deg=deg</span><span class="s2">,</span>
                    <span class="s1">n_features=n_features</span><span class="s2">,</span>
                    <span class="s1">cumulative_size=cumulative_size</span><span class="s2">,</span>
                <span class="s1">)</span>
                <span class="s2">if </span><span class="s1">expanded </span><span class="s2">is not None</span><span class="s1">:</span>
                    <span class="s1">to_stack.append(expanded)</span>
                    <span class="s1">cumulative_size += expanded.shape[</span><span class="s4">1</span><span class="s1">]</span>
            <span class="s2">if </span><span class="s1">len(to_stack) == </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s5"># edge case: deal with empty matrix</span>
                <span class="s1">XP = sparse.csr_matrix((n_samples</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s5"># `scipy.sparse.hstack` breaks in scipy&lt;1.9.2</span>
                <span class="s5"># when `n_output_features_ &gt; max_int32`</span>
                <span class="s1">all_int32 = all(mat.indices.dtype == np.int32 </span><span class="s2">for </span><span class="s1">mat </span><span class="s2">in </span><span class="s1">to_stack)</span>
                <span class="s2">if </span><span class="s1">(</span>
                    <span class="s1">sp_version &lt; parse_version(</span><span class="s3">&quot;1.9.2&quot;</span><span class="s1">)</span>
                    <span class="s2">and </span><span class="s1">self.n_output_features_ &gt; max_int32</span>
                    <span class="s2">and </span><span class="s1">all_int32</span>
                <span class="s1">):</span>
                    <span class="s2">raise </span><span class="s1">ValueError(  </span><span class="s5"># pragma: no cover</span>
                        <span class="s3">&quot;In scipy versions `&lt;1.9.2`, the function `scipy.sparse.hstack`&quot;</span>
                        <span class="s3">&quot; produces negative columns when:</span><span class="s2">\n</span><span class="s3">1. The output shape contains&quot;</span>
                        <span class="s3">&quot; `n_cols` too large to be represented by a 32bit signed&quot;</span>
                        <span class="s3">&quot; integer.</span><span class="s2">\n</span><span class="s3">2. All sub-matrices to be stacked have indices of&quot;</span>
                        <span class="s3">&quot; dtype `np.int32`.</span><span class="s2">\n</span><span class="s3">To avoid this error, either use a version&quot;</span>
                        <span class="s3">&quot; of scipy `&gt;=1.9.2` or alter the `PolynomialFeatures`&quot;</span>
                        <span class="s3">&quot; transformer to produce fewer than 2^31 output features&quot;</span>
                    <span class="s1">)</span>
                <span class="s1">XP = sparse.hstack(to_stack</span><span class="s2">, </span><span class="s1">dtype=X.dtype</span><span class="s2">, </span><span class="s1">format=</span><span class="s3">&quot;csr&quot;</span><span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">sparse.issparse(X) </span><span class="s2">and </span><span class="s1">X.format == </span><span class="s3">&quot;csc&quot; </span><span class="s2">and </span><span class="s1">self._max_degree &lt; </span><span class="s4">4</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">self.transform(X.tocsr()).tocsc()</span>
        <span class="s2">elif </span><span class="s1">sparse.issparse(X):</span>
            <span class="s1">combinations = self._combinations(</span>
                <span class="s1">n_features=n_features</span><span class="s2">,</span>
                <span class="s1">min_degree=self._min_degree</span><span class="s2">,</span>
                <span class="s1">max_degree=self._max_degree</span><span class="s2">,</span>
                <span class="s1">interaction_only=self.interaction_only</span><span class="s2">,</span>
                <span class="s1">include_bias=self.include_bias</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">columns = []</span>
            <span class="s2">for </span><span class="s1">combi </span><span class="s2">in </span><span class="s1">combinations:</span>
                <span class="s2">if </span><span class="s1">combi:</span>
                    <span class="s1">out_col = </span><span class="s4">1</span>
                    <span class="s2">for </span><span class="s1">col_idx </span><span class="s2">in </span><span class="s1">combi:</span>
                        <span class="s1">out_col = X[:</span><span class="s2">, </span><span class="s1">col_idx].multiply(out_col)</span>
                    <span class="s1">columns.append(out_col)</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">bias = sparse.csc_matrix(np.ones((X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)))</span>
                    <span class="s1">columns.append(bias)</span>
            <span class="s1">XP = sparse.hstack(columns</span><span class="s2">, </span><span class="s1">dtype=X.dtype).tocsc()</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s5"># Do as if _min_degree = 0 and cut down array after the</span>
            <span class="s5"># computation, i.e. use _n_out_full instead of n_output_features_.</span>
            <span class="s1">XP = np.empty(</span>
                <span class="s1">shape=(n_samples</span><span class="s2">, </span><span class="s1">self._n_out_full)</span><span class="s2">, </span><span class="s1">dtype=X.dtype</span><span class="s2">, </span><span class="s1">order=self.order</span>
            <span class="s1">)</span>

            <span class="s5"># What follows is a faster implementation of:</span>
            <span class="s5"># for i, comb in enumerate(combinations):</span>
            <span class="s5">#     XP[:, i] = X[:, comb].prod(1)</span>
            <span class="s5"># This implementation uses two optimisations.</span>
            <span class="s5"># First one is broadcasting,</span>
            <span class="s5"># multiply ([X1, ..., Xn], X1) -&gt; [X1 X1, ..., Xn X1]</span>
            <span class="s5"># multiply ([X2, ..., Xn], X2) -&gt; [X2 X2, ..., Xn X2]</span>
            <span class="s5"># ...</span>
            <span class="s5"># multiply ([X[:, start:end], X[:, start]) -&gt; ...</span>
            <span class="s5"># Second optimisation happens for degrees &gt;= 3.</span>
            <span class="s5"># Xi^3 is computed reusing previous computation:</span>
            <span class="s5"># Xi^3 = Xi^2 * Xi.</span>

            <span class="s5"># degree 0 term</span>
            <span class="s2">if </span><span class="s1">self.include_bias:</span>
                <span class="s1">XP[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">1</span>
                <span class="s1">current_col = </span><span class="s4">1</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">current_col = </span><span class="s4">0</span>

            <span class="s2">if </span><span class="s1">self._max_degree == </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">XP</span>

            <span class="s5"># degree 1 term</span>
            <span class="s1">XP[:</span><span class="s2">, </span><span class="s1">current_col : current_col + n_features] = X</span>
            <span class="s1">index = list(range(current_col</span><span class="s2">, </span><span class="s1">current_col + n_features))</span>
            <span class="s1">current_col += n_features</span>
            <span class="s1">index.append(current_col)</span>

            <span class="s5"># loop over degree &gt;= 2 terms</span>
            <span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">2</span><span class="s2">, </span><span class="s1">self._max_degree + </span><span class="s4">1</span><span class="s1">):</span>
                <span class="s1">new_index = []</span>
                <span class="s1">end = index[-</span><span class="s4">1</span><span class="s1">]</span>
                <span class="s2">for </span><span class="s1">feature_idx </span><span class="s2">in </span><span class="s1">range(n_features):</span>
                    <span class="s1">start = index[feature_idx]</span>
                    <span class="s1">new_index.append(current_col)</span>
                    <span class="s2">if </span><span class="s1">self.interaction_only:</span>
                        <span class="s1">start += index[feature_idx + </span><span class="s4">1</span><span class="s1">] - index[feature_idx]</span>
                    <span class="s1">next_col = current_col + end - start</span>
                    <span class="s2">if </span><span class="s1">next_col &lt;= current_col:</span>
                        <span class="s2">break</span>
                    <span class="s5"># XP[:, start:end] are terms of degree d - 1</span>
                    <span class="s5"># that exclude feature #feature_idx.</span>
                    <span class="s1">np.multiply(</span>
                        <span class="s1">XP[:</span><span class="s2">, </span><span class="s1">start:end]</span><span class="s2">,</span>
                        <span class="s1">X[:</span><span class="s2">, </span><span class="s1">feature_idx : feature_idx + </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
                        <span class="s1">out=XP[:</span><span class="s2">, </span><span class="s1">current_col:next_col]</span><span class="s2">,</span>
                        <span class="s1">casting=</span><span class="s3">&quot;no&quot;</span><span class="s2">,</span>
                    <span class="s1">)</span>
                    <span class="s1">current_col = next_col</span>

                <span class="s1">new_index.append(current_col)</span>
                <span class="s1">index = new_index</span>

            <span class="s2">if </span><span class="s1">self._min_degree &gt; </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">n_XP</span><span class="s2">, </span><span class="s1">n_Xout = self._n_out_full</span><span class="s2">, </span><span class="s1">self.n_output_features_</span>
                <span class="s2">if </span><span class="s1">self.include_bias:</span>
                    <span class="s1">Xout = np.empty(</span>
                        <span class="s1">shape=(n_samples</span><span class="s2">, </span><span class="s1">n_Xout)</span><span class="s2">, </span><span class="s1">dtype=XP.dtype</span><span class="s2">, </span><span class="s1">order=self.order</span>
                    <span class="s1">)</span>
                    <span class="s1">Xout[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">1</span>
                    <span class="s1">Xout[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">:] = XP[:</span><span class="s2">, </span><span class="s1">n_XP - n_Xout + </span><span class="s4">1 </span><span class="s1">:]</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">Xout = XP[:</span><span class="s2">, </span><span class="s1">n_XP - n_Xout :].copy()</span>
                <span class="s1">XP = Xout</span>
        <span class="s2">return </span><span class="s1">XP</span>


<span class="s2">class </span><span class="s1">SplineTransformer(TransformerMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Generate univariate B-spline bases for features. 
 
    Generate a new feature matrix consisting of 
    `n_splines=n_knots + degree - 1` (`n_knots - 1` for 
    `extrapolation=&quot;periodic&quot;`) spline basis functions 
    (B-splines) of polynomial order=`degree` for each feature. 
 
    Read more in the :ref:`User Guide &lt;spline_transformer&gt;`. 
 
    .. versionadded:: 1.0 
 
    Parameters 
    ---------- 
    n_knots : int, default=5 
        Number of knots of the splines if `knots` equals one of 
        {'uniform', 'quantile'}. Must be larger or equal 2. Ignored if `knots` 
        is array-like. 
 
    degree : int, default=3 
        The polynomial degree of the spline basis. Must be a non-negative 
        integer. 
 
    knots : {'uniform', 'quantile'} or array-like of shape \ 
        (n_knots, n_features), default='uniform' 
        Set knot positions such that first knot &lt;= features &lt;= last knot. 
 
        - If 'uniform', `n_knots` number of knots are distributed uniformly 
          from min to max values of the features. 
        - If 'quantile', they are distributed uniformly along the quantiles of 
          the features. 
        - If an array-like is given, it directly specifies the sorted knot 
          positions including the boundary knots. Note that, internally, 
          `degree` number of knots are added before the first knot, the same 
          after the last knot. 
 
    extrapolation : {'error', 'constant', 'linear', 'continue', 'periodic'}, \ 
        default='constant' 
        If 'error', values outside the min and max values of the training 
        features raises a `ValueError`. If 'constant', the value of the 
        splines at minimum and maximum value of the features is used as 
        constant extrapolation. If 'linear', a linear extrapolation is used. 
        If 'continue', the splines are extrapolated as is, i.e. option 
        `extrapolate=True` in :class:`scipy.interpolate.BSpline`. If 
        'periodic', periodic splines with a periodicity equal to the distance 
        between the first and last knot are used. Periodic splines enforce 
        equal function values and derivatives at the first and last knot. 
        For example, this makes it possible to avoid introducing an arbitrary 
        jump between Dec 31st and Jan 1st in spline features derived from a 
        naturally periodic &quot;day-of-year&quot; input feature. In this case it is 
        recommended to manually set the knot values to control the period. 
 
    include_bias : bool, default=True 
        If False, then the last spline element inside the data range 
        of a feature is dropped. As B-splines sum to one over the spline basis 
        functions for each data point, they implicitly include a bias term, 
        i.e. a column of ones. It acts as an intercept term in a linear models. 
 
    order : {'C', 'F'}, default='C' 
        Order of output array in the dense case. `'F'` order is faster to compute, but 
        may slow down subsequent estimators. 
 
    sparse_output : bool, default=False 
        Will return sparse CSR matrix if set True else will return an array. This 
        option is only available with `scipy&gt;=1.8`. 
 
        .. versionadded:: 1.2 
 
    Attributes 
    ---------- 
    bsplines_ : list of shape (n_features,) 
        List of BSplines objects, one for each feature. 
 
    n_features_in_ : int 
        The total number of input features. 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    n_features_out_ : int 
        The total number of output features, which is computed as 
        `n_features * n_splines`, where `n_splines` is 
        the number of bases elements of the B-splines, 
        `n_knots + degree - 1` for non-periodic splines and 
        `n_knots - 1` for periodic ones. 
        If `include_bias=False`, then it is only 
        `n_features * (n_splines - 1)`. 
 
    See Also 
    -------- 
    KBinsDiscretizer : Transformer that bins continuous data into intervals. 
 
    PolynomialFeatures : Transformer that generates polynomial and interaction 
        features. 
 
    Notes 
    ----- 
    High degrees and a high number of knots can cause overfitting. 
 
    See :ref:`examples/linear_model/plot_polynomial_interpolation.py 
    &lt;sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py&gt;`. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.preprocessing import SplineTransformer 
    &gt;&gt;&gt; X = np.arange(6).reshape(6, 1) 
    &gt;&gt;&gt; spline = SplineTransformer(degree=2, n_knots=3) 
    &gt;&gt;&gt; spline.fit_transform(X) 
    array([[0.5 , 0.5 , 0.  , 0.  ], 
           [0.18, 0.74, 0.08, 0.  ], 
           [0.02, 0.66, 0.32, 0.  ], 
           [0.  , 0.32, 0.66, 0.02], 
           [0.  , 0.08, 0.74, 0.18], 
           [0.  , 0.  , 0.5 , 0.5 ]]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s3">&quot;n_knots&quot;</span><span class="s1">: [Interval(Integral</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s3">&quot;left&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s3">&quot;degree&quot;</span><span class="s1">: [Interval(Integral</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s3">&quot;left&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s3">&quot;knots&quot;</span><span class="s1">: [StrOptions({</span><span class="s3">&quot;uniform&quot;</span><span class="s2">, </span><span class="s3">&quot;quantile&quot;</span><span class="s1">})</span><span class="s2">, </span><span class="s3">&quot;array-like&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;extrapolation&quot;</span><span class="s1">: [</span>
            <span class="s1">StrOptions({</span><span class="s3">&quot;error&quot;</span><span class="s2">, </span><span class="s3">&quot;constant&quot;</span><span class="s2">, </span><span class="s3">&quot;linear&quot;</span><span class="s2">, </span><span class="s3">&quot;continue&quot;</span><span class="s2">, </span><span class="s3">&quot;periodic&quot;</span><span class="s1">})</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;include_bias&quot;</span><span class="s1">: [</span><span class="s3">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;order&quot;</span><span class="s1">: [StrOptions({</span><span class="s3">&quot;C&quot;</span><span class="s2">, </span><span class="s3">&quot;F&quot;</span><span class="s1">})]</span><span class="s2">,</span>
        <span class="s3">&quot;sparse_output&quot;</span><span class="s1">: [</span><span class="s3">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">n_knots=</span><span class="s4">5</span><span class="s2">,</span>
        <span class="s1">degree=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">knots=</span><span class="s3">&quot;uniform&quot;</span><span class="s2">,</span>
        <span class="s1">extrapolation=</span><span class="s3">&quot;constant&quot;</span><span class="s2">,</span>
        <span class="s1">include_bias=</span><span class="s2">True,</span>
        <span class="s1">order=</span><span class="s3">&quot;C&quot;</span><span class="s2">,</span>
        <span class="s1">sparse_output=</span><span class="s2">False,</span>
    <span class="s1">):</span>
        <span class="s1">self.n_knots = n_knots</span>
        <span class="s1">self.degree = degree</span>
        <span class="s1">self.knots = knots</span>
        <span class="s1">self.extrapolation = extrapolation</span>
        <span class="s1">self.include_bias = include_bias</span>
        <span class="s1">self.order = order</span>
        <span class="s1">self.sparse_output = sparse_output</span>

    <span class="s1">@staticmethod</span>
    <span class="s2">def </span><span class="s1">_get_base_knot_positions(X</span><span class="s2">, </span><span class="s1">n_knots=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">knots=</span><span class="s3">&quot;uniform&quot;</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Calculate base knot positions. 
 
        Base knots such that first knot &lt;= feature &lt;= last knot. For the 
        B-spline construction with scipy.interpolate.BSpline, 2*degree knots 
        beyond the base interval are added. 
 
        Returns 
        ------- 
        knots : ndarray of shape (n_knots, n_features), dtype=np.float64 
            Knot positions (points) of base interval. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">knots == </span><span class="s3">&quot;quantile&quot;</span><span class="s1">:</span>
            <span class="s1">percentiles = </span><span class="s4">100 </span><span class="s1">* np.linspace(</span>
                <span class="s1">start=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">stop=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">num=n_knots</span><span class="s2">, </span><span class="s1">dtype=np.float64</span>
            <span class="s1">)</span>

            <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s1">knots = np.percentile(X</span><span class="s2">, </span><span class="s1">percentiles</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">knots = np.array(</span>
                    <span class="s1">[</span>
                        <span class="s1">_weighted_percentile(X</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">percentile)</span>
                        <span class="s2">for </span><span class="s1">percentile </span><span class="s2">in </span><span class="s1">percentiles</span>
                    <span class="s1">]</span>
                <span class="s1">)</span>

        <span class="s2">else</span><span class="s1">:</span>
            <span class="s5"># knots == 'uniform':</span>
            <span class="s5"># Note that the variable `knots` has already been validated and</span>
            <span class="s5"># `else` is therefore safe.</span>
            <span class="s5"># Disregard observations with zero weight.</span>
            <span class="s1">mask = slice(</span><span class="s2">None, None, </span><span class="s4">1</span><span class="s1">) </span><span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is None else </span><span class="s1">sample_weight &gt; </span><span class="s4">0</span>
            <span class="s1">x_min = np.amin(X[mask]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">x_max = np.amax(X[mask]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>

            <span class="s1">knots = np.linspace(</span>
                <span class="s1">start=x_min</span><span class="s2">,</span>
                <span class="s1">stop=x_max</span><span class="s2">,</span>
                <span class="s1">num=n_knots</span><span class="s2">,</span>
                <span class="s1">endpoint=</span><span class="s2">True,</span>
                <span class="s1">dtype=np.float64</span><span class="s2">,</span>
            <span class="s1">)</span>

        <span class="s2">return </span><span class="s1">knots</span>

    <span class="s2">def </span><span class="s1">get_feature_names_out(self</span><span class="s2">, </span><span class="s1">input_features=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Get output feature names for transformation. 
 
        Parameters 
        ---------- 
        input_features : array-like of str or None, default=None 
            Input features. 
 
            - If `input_features` is `None`, then `feature_names_in_` is 
              used as feature names in. If `feature_names_in_` is not defined, 
              then the following input feature names are generated: 
              `[&quot;x0&quot;, &quot;x1&quot;, ..., &quot;x(n_features_in_ - 1)&quot;]`. 
            - If `input_features` is an array-like, then `input_features` must 
              match `feature_names_in_` if `feature_names_in_` is defined. 
 
        Returns 
        ------- 
        feature_names_out : ndarray of str objects 
            Transformed feature names. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self</span><span class="s2">, </span><span class="s3">&quot;n_features_in_&quot;</span><span class="s1">)</span>
        <span class="s1">n_splines = self.bsplines_[</span><span class="s4">0</span><span class="s1">].c.shape[</span><span class="s4">1</span><span class="s1">]</span>

        <span class="s1">input_features = _check_feature_names_in(self</span><span class="s2">, </span><span class="s1">input_features)</span>
        <span class="s1">feature_names = []</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(self.n_features_in_):</span>
            <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(n_splines - </span><span class="s4">1 </span><span class="s1">+ self.include_bias):</span>
                <span class="s1">feature_names.append(</span><span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">input_features[i]</span><span class="s2">}</span><span class="s3">_sp_</span><span class="s2">{</span><span class="s1">j</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">np.asarray(feature_names</span><span class="s2">, </span><span class="s1">dtype=object)</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Compute knot positions of splines. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The data. 
 
        y : None 
            Ignored. 
 
        sample_weight : array-like of shape (n_samples,), default = None 
            Individual weights for each sample. Used to calculate quantiles if 
            `knots=&quot;quantile&quot;`. For `knots=&quot;uniform&quot;`, zero weighted 
            observations are ignored for finding the min and max of `X`. 
 
        Returns 
        ------- 
        self : object 
            Fitted transformer. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">reset=</span><span class="s2">True,</span>
            <span class="s1">accept_sparse=</span><span class="s2">False,</span>
            <span class="s1">ensure_min_samples=</span><span class="s4">2</span><span class="s2">,</span>
            <span class="s1">ensure_2d=</span><span class="s2">True,</span>
        <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s1">_</span><span class="s2">, </span><span class="s1">n_features = X.shape</span>

        <span class="s2">if </span><span class="s1">isinstance(self.knots</span><span class="s2">, </span><span class="s1">str):</span>
            <span class="s1">base_knots = self._get_base_knot_positions(</span>
                <span class="s1">X</span><span class="s2">, </span><span class="s1">n_knots=self.n_knots</span><span class="s2">, </span><span class="s1">knots=self.knots</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight</span>
            <span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">base_knots = check_array(self.knots</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
            <span class="s2">if </span><span class="s1">base_knots.shape[</span><span class="s4">0</span><span class="s1">] &lt; </span><span class="s4">2</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Number of knots, knots.shape[0], must be &gt;= 2.&quot;</span><span class="s1">)</span>
            <span class="s2">elif </span><span class="s1">base_knots.shape[</span><span class="s4">1</span><span class="s1">] != n_features:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;knots.shape[1] == n_features is violated.&quot;</span><span class="s1">)</span>
            <span class="s2">elif not </span><span class="s1">np.all(np.diff(base_knots</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">) &gt; </span><span class="s4">0</span><span class="s1">):</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;knots must be sorted without duplicates.&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">self.sparse_output </span><span class="s2">and </span><span class="s1">sp_version &lt; parse_version(</span><span class="s3">&quot;1.8.0&quot;</span><span class="s1">):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s3">&quot;Option sparse_output=True is only available with scipy&gt;=1.8.0, &quot;</span>
                <span class="s3">f&quot;but here scipy==</span><span class="s2">{</span><span class="s1">sp_version</span><span class="s2">} </span><span class="s3">is used.&quot;</span>
            <span class="s1">)</span>

        <span class="s5"># number of knots for base interval</span>
        <span class="s1">n_knots = base_knots.shape[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s2">if </span><span class="s1">self.extrapolation == </span><span class="s3">&quot;periodic&quot; </span><span class="s2">and </span><span class="s1">n_knots &lt;= self.degree:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s3">&quot;Periodic splines require degree &lt; n_knots. Got n_knots=&quot;</span>
                <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">n_knots</span><span class="s2">} </span><span class="s3">and degree=</span><span class="s2">{</span><span class="s1">self.degree</span><span class="s2">}</span><span class="s3">.&quot;</span>
            <span class="s1">)</span>

        <span class="s5"># number of splines basis functions</span>
        <span class="s2">if </span><span class="s1">self.extrapolation != </span><span class="s3">&quot;periodic&quot;</span><span class="s1">:</span>
            <span class="s1">n_splines = n_knots + self.degree - </span><span class="s4">1</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s5"># periodic splines have self.degree less degrees of freedom</span>
            <span class="s1">n_splines = n_knots - </span><span class="s4">1</span>

        <span class="s1">degree = self.degree</span>
        <span class="s1">n_out = n_features * n_splines</span>
        <span class="s5"># We have to add degree number of knots below, and degree number knots</span>
        <span class="s5"># above the base knots in order to make the spline basis complete.</span>
        <span class="s2">if </span><span class="s1">self.extrapolation == </span><span class="s3">&quot;periodic&quot;</span><span class="s1">:</span>
            <span class="s5"># For periodic splines the spacing of the first / last degree knots</span>
            <span class="s5"># needs to be a continuation of the spacing of the last / first</span>
            <span class="s5"># base knots.</span>
            <span class="s1">period = base_knots[-</span><span class="s4">1</span><span class="s1">] - base_knots[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">knots = np.r_[</span>
                <span class="s1">base_knots[-(degree + </span><span class="s4">1</span><span class="s1">) : -</span><span class="s4">1</span><span class="s1">] - period</span><span class="s2">,</span>
                <span class="s1">base_knots</span><span class="s2">,</span>
                <span class="s1">base_knots[</span><span class="s4">1 </span><span class="s1">: (degree + </span><span class="s4">1</span><span class="s1">)] + period</span><span class="s2">,</span>
            <span class="s1">]</span>

        <span class="s2">else</span><span class="s1">:</span>
            <span class="s5"># Eilers &amp; Marx in &quot;Flexible smoothing with B-splines and</span>
            <span class="s5"># penalties&quot; https://doi.org/10.1214/ss/1038425655 advice</span>
            <span class="s5"># against repeating first and last knot several times, which</span>
            <span class="s5"># would have inferior behaviour at boundaries if combined with</span>
            <span class="s5"># a penalty (hence P-Spline). We follow this advice even if our</span>
            <span class="s5"># splines are unpenalized. Meaning we do not:</span>
            <span class="s5"># knots = np.r_[</span>
            <span class="s5">#     np.tile(base_knots.min(axis=0), reps=[degree, 1]),</span>
            <span class="s5">#     base_knots,</span>
            <span class="s5">#     np.tile(base_knots.max(axis=0), reps=[degree, 1])</span>
            <span class="s5"># ]</span>
            <span class="s5"># Instead, we reuse the distance of the 2 fist/last knots.</span>
            <span class="s1">dist_min = base_knots[</span><span class="s4">1</span><span class="s1">] - base_knots[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">dist_max = base_knots[-</span><span class="s4">1</span><span class="s1">] - base_knots[-</span><span class="s4">2</span><span class="s1">]</span>

            <span class="s1">knots = np.r_[</span>
                <span class="s1">np.linspace(</span>
                    <span class="s1">base_knots[</span><span class="s4">0</span><span class="s1">] - degree * dist_min</span><span class="s2">,</span>
                    <span class="s1">base_knots[</span><span class="s4">0</span><span class="s1">] - dist_min</span><span class="s2">,</span>
                    <span class="s1">num=degree</span><span class="s2">,</span>
                <span class="s1">)</span><span class="s2">,</span>
                <span class="s1">base_knots</span><span class="s2">,</span>
                <span class="s1">np.linspace(</span>
                    <span class="s1">base_knots[-</span><span class="s4">1</span><span class="s1">] + dist_max</span><span class="s2">,</span>
                    <span class="s1">base_knots[-</span><span class="s4">1</span><span class="s1">] + degree * dist_max</span><span class="s2">,</span>
                    <span class="s1">num=degree</span><span class="s2">,</span>
                <span class="s1">)</span><span class="s2">,</span>
            <span class="s1">]</span>

        <span class="s5"># With a diagonal coefficient matrix, we get back the spline basis</span>
        <span class="s5"># elements, i.e. the design matrix of the spline.</span>
        <span class="s5"># Note, BSpline appreciates C-contiguous float64 arrays as c=coef.</span>
        <span class="s1">coef = np.eye(n_splines</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
        <span class="s2">if </span><span class="s1">self.extrapolation == </span><span class="s3">&quot;periodic&quot;</span><span class="s1">:</span>
            <span class="s1">coef = np.concatenate((coef</span><span class="s2">, </span><span class="s1">coef[:degree</span><span class="s2">, </span><span class="s1">:]))</span>

        <span class="s1">extrapolate = self.extrapolation </span><span class="s2">in </span><span class="s1">[</span><span class="s3">&quot;periodic&quot;</span><span class="s2">, </span><span class="s3">&quot;continue&quot;</span><span class="s1">]</span>

        <span class="s1">bsplines = [</span>
            <span class="s1">BSpline.construct_fast(</span>
                <span class="s1">knots[:</span><span class="s2">, </span><span class="s1">i]</span><span class="s2">, </span><span class="s1">coef</span><span class="s2">, </span><span class="s1">self.degree</span><span class="s2">, </span><span class="s1">extrapolate=extrapolate</span>
            <span class="s1">)</span>
            <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_features)</span>
        <span class="s1">]</span>
        <span class="s1">self.bsplines_ = bsplines</span>

        <span class="s1">self.n_features_out_ = n_out - n_features * (</span><span class="s4">1 </span><span class="s1">- self.include_bias)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">transform(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Transform each feature data to B-splines. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The data to transform. 
 
        Returns 
        ------- 
        XBS : {ndarray, sparse matrix} of shape (n_samples, n_features * n_splines) 
            The matrix of features, where n_splines is the number of bases 
            elements of the B-splines, n_knots + degree - 1. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s1">X = self._validate_data(X</span><span class="s2">, </span><span class="s1">reset=</span><span class="s2">False, </span><span class="s1">accept_sparse=</span><span class="s2">False, </span><span class="s1">ensure_2d=</span><span class="s2">True</span><span class="s1">)</span>

        <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X.shape</span>
        <span class="s1">n_splines = self.bsplines_[</span><span class="s4">0</span><span class="s1">].c.shape[</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">degree = self.degree</span>

        <span class="s5"># TODO: Remove this condition, once scipy 1.10 is the minimum version.</span>
        <span class="s5">#       Only scipy =&gt; 1.10 supports design_matrix(.., extrapolate=..).</span>
        <span class="s5">#       The default (implicit in scipy &lt; 1.10) is extrapolate=False.</span>
        <span class="s1">scipy_1_10 = sp_version &gt;= parse_version(</span><span class="s3">&quot;1.10.0&quot;</span><span class="s1">)</span>
        <span class="s5"># Note: self.bsplines_[0].extrapolate is True for extrapolation in</span>
        <span class="s5"># [&quot;periodic&quot;, &quot;continue&quot;]</span>
        <span class="s2">if </span><span class="s1">scipy_1_10:</span>
            <span class="s1">use_sparse = self.sparse_output</span>
            <span class="s1">kwargs_extrapolate = {</span><span class="s3">&quot;extrapolate&quot;</span><span class="s1">: self.bsplines_[</span><span class="s4">0</span><span class="s1">].extrapolate}</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">use_sparse = self.sparse_output </span><span class="s2">and not </span><span class="s1">self.bsplines_[</span><span class="s4">0</span><span class="s1">].extrapolate</span>
            <span class="s1">kwargs_extrapolate = dict()</span>

        <span class="s5"># Note that scipy BSpline returns float64 arrays and converts input</span>
        <span class="s5"># x=X[:, i] to c-contiguous float64.</span>
        <span class="s1">n_out = self.n_features_out_ + n_features * (</span><span class="s4">1 </span><span class="s1">- self.include_bias)</span>
        <span class="s2">if </span><span class="s1">X.dtype </span><span class="s2">in </span><span class="s1">FLOAT_DTYPES:</span>
            <span class="s1">dtype = X.dtype</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">dtype = np.float64</span>
        <span class="s2">if </span><span class="s1">use_sparse:</span>
            <span class="s1">output_list = []</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">XBS = np.zeros((n_samples</span><span class="s2">, </span><span class="s1">n_out)</span><span class="s2">, </span><span class="s1">dtype=dtype</span><span class="s2">, </span><span class="s1">order=self.order)</span>

        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_features):</span>
            <span class="s1">spl = self.bsplines_[i]</span>

            <span class="s2">if </span><span class="s1">self.extrapolation </span><span class="s2">in </span><span class="s1">(</span><span class="s3">&quot;continue&quot;</span><span class="s2">, </span><span class="s3">&quot;error&quot;</span><span class="s2">, </span><span class="s3">&quot;periodic&quot;</span><span class="s1">):</span>
                <span class="s2">if </span><span class="s1">self.extrapolation == </span><span class="s3">&quot;periodic&quot;</span><span class="s1">:</span>
                    <span class="s5"># With periodic extrapolation we map x to the segment</span>
                    <span class="s5"># [spl.t[k], spl.t[n]].</span>
                    <span class="s5"># This is equivalent to BSpline(.., extrapolate=&quot;periodic&quot;)</span>
                    <span class="s5"># for scipy&gt;=1.0.0.</span>
                    <span class="s1">n = spl.t.size - spl.k - </span><span class="s4">1</span>
                    <span class="s5"># Assign to new array to avoid inplace operation</span>
                    <span class="s1">x = spl.t[spl.k] + (X[:</span><span class="s2">, </span><span class="s1">i] - spl.t[spl.k]) % (</span>
                        <span class="s1">spl.t[n] - spl.t[spl.k]</span>
                    <span class="s1">)</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">x = X[:</span><span class="s2">, </span><span class="s1">i]</span>

                <span class="s2">if </span><span class="s1">use_sparse:</span>
                    <span class="s1">XBS_sparse = BSpline.design_matrix(</span>
                        <span class="s1">x</span><span class="s2">, </span><span class="s1">spl.t</span><span class="s2">, </span><span class="s1">spl.k</span><span class="s2">, </span><span class="s1">**kwargs_extrapolate</span>
                    <span class="s1">)</span>
                    <span class="s2">if </span><span class="s1">self.extrapolation == </span><span class="s3">&quot;periodic&quot;</span><span class="s1">:</span>
                        <span class="s5"># See the construction of coef in fit. We need to add the last</span>
                        <span class="s5"># degree spline basis function to the first degree ones and</span>
                        <span class="s5"># then drop the last ones.</span>
                        <span class="s5"># Note: See comment about SparseEfficiencyWarning below.</span>
                        <span class="s1">XBS_sparse = XBS_sparse.tolil()</span>
                        <span class="s1">XBS_sparse[:</span><span class="s2">, </span><span class="s1">:degree] += XBS_sparse[:</span><span class="s2">, </span><span class="s1">-degree:]</span>
                        <span class="s1">XBS_sparse = XBS_sparse[:</span><span class="s2">, </span><span class="s1">:-degree]</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">XBS[:</span><span class="s2">, </span><span class="s1">(i * n_splines) : ((i + </span><span class="s4">1</span><span class="s1">) * n_splines)] = spl(x)</span>
            <span class="s2">else</span><span class="s1">:  </span><span class="s5"># extrapolation in (&quot;constant&quot;, &quot;linear&quot;)</span>
                <span class="s1">xmin</span><span class="s2">, </span><span class="s1">xmax = spl.t[degree]</span><span class="s2">, </span><span class="s1">spl.t[-degree - </span><span class="s4">1</span><span class="s1">]</span>
                <span class="s5"># spline values at boundaries</span>
                <span class="s1">f_min</span><span class="s2">, </span><span class="s1">f_max = spl(xmin)</span><span class="s2">, </span><span class="s1">spl(xmax)</span>
                <span class="s1">mask = (xmin &lt;= X[:</span><span class="s2">, </span><span class="s1">i]) &amp; (X[:</span><span class="s2">, </span><span class="s1">i] &lt;= xmax)</span>
                <span class="s2">if </span><span class="s1">use_sparse:</span>
                    <span class="s1">mask_inv = ~mask</span>
                    <span class="s1">x = X[:</span><span class="s2">, </span><span class="s1">i].copy()</span>
                    <span class="s5"># Set some arbitrary values outside boundary that will be reassigned</span>
                    <span class="s5"># later.</span>
                    <span class="s1">x[mask_inv] = spl.t[self.degree]</span>
                    <span class="s1">XBS_sparse = BSpline.design_matrix(x</span><span class="s2">, </span><span class="s1">spl.t</span><span class="s2">, </span><span class="s1">spl.k)</span>
                    <span class="s5"># Note: Without converting to lil_matrix we would get:</span>
                    <span class="s5"># scipy.sparse._base.SparseEfficiencyWarning: Changing the sparsity</span>
                    <span class="s5"># structure of a csr_matrix is expensive. lil_matrix is more</span>
                    <span class="s5"># efficient.</span>
                    <span class="s2">if </span><span class="s1">np.any(mask_inv):</span>
                        <span class="s1">XBS_sparse = XBS_sparse.tolil()</span>
                        <span class="s1">XBS_sparse[mask_inv</span><span class="s2">, </span><span class="s1">:] = </span><span class="s4">0</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">XBS[mask</span><span class="s2">, </span><span class="s1">(i * n_splines) : ((i + </span><span class="s4">1</span><span class="s1">) * n_splines)] = spl(X[mask</span><span class="s2">, </span><span class="s1">i])</span>

            <span class="s5"># Note for extrapolation:</span>
            <span class="s5"># 'continue' is already returned as is by scipy BSplines</span>
            <span class="s2">if </span><span class="s1">self.extrapolation == </span><span class="s3">&quot;error&quot;</span><span class="s1">:</span>
                <span class="s5"># BSpline with extrapolate=False does not raise an error, but</span>
                <span class="s5"># outputs np.nan.</span>
                <span class="s2">if </span><span class="s1">(use_sparse </span><span class="s2">and </span><span class="s1">np.any(np.isnan(XBS_sparse.data))) </span><span class="s2">or </span><span class="s1">(</span>
                    <span class="s2">not </span><span class="s1">use_sparse</span>
                    <span class="s2">and </span><span class="s1">np.any(</span>
                        <span class="s1">np.isnan(XBS[:</span><span class="s2">, </span><span class="s1">(i * n_splines) : ((i + </span><span class="s4">1</span><span class="s1">) * n_splines)])</span>
                    <span class="s1">)</span>
                <span class="s1">):</span>
                    <span class="s2">raise </span><span class="s1">ValueError(</span>
                        <span class="s3">&quot;X contains values beyond the limits of the knots.&quot;</span>
                    <span class="s1">)</span>
            <span class="s2">elif </span><span class="s1">self.extrapolation == </span><span class="s3">&quot;constant&quot;</span><span class="s1">:</span>
                <span class="s5"># Set all values beyond xmin and xmax to the value of the</span>
                <span class="s5"># spline basis functions at those two positions.</span>
                <span class="s5"># Only the first degree and last degree number of splines</span>
                <span class="s5"># have non-zero values at the boundaries.</span>

                <span class="s1">mask = X[:</span><span class="s2">, </span><span class="s1">i] &lt; xmin</span>
                <span class="s2">if </span><span class="s1">np.any(mask):</span>
                    <span class="s2">if </span><span class="s1">use_sparse:</span>
                        <span class="s5"># Note: See comment about SparseEfficiencyWarning above.</span>
                        <span class="s1">XBS_sparse = XBS_sparse.tolil()</span>
                        <span class="s1">XBS_sparse[mask</span><span class="s2">, </span><span class="s1">:degree] = f_min[:degree]</span>

                    <span class="s2">else</span><span class="s1">:</span>
                        <span class="s1">XBS[mask</span><span class="s2">, </span><span class="s1">(i * n_splines) : (i * n_splines + degree)] = f_min[</span>
                            <span class="s1">:degree</span>
                        <span class="s1">]</span>

                <span class="s1">mask = X[:</span><span class="s2">, </span><span class="s1">i] &gt; xmax</span>
                <span class="s2">if </span><span class="s1">np.any(mask):</span>
                    <span class="s2">if </span><span class="s1">use_sparse:</span>
                        <span class="s5"># Note: See comment about SparseEfficiencyWarning above.</span>
                        <span class="s1">XBS_sparse = XBS_sparse.tolil()</span>
                        <span class="s1">XBS_sparse[mask</span><span class="s2">, </span><span class="s1">-degree:] = f_max[-degree:]</span>
                    <span class="s2">else</span><span class="s1">:</span>
                        <span class="s1">XBS[</span>
                            <span class="s1">mask</span><span class="s2">,</span>
                            <span class="s1">((i + </span><span class="s4">1</span><span class="s1">) * n_splines - degree) : ((i + </span><span class="s4">1</span><span class="s1">) * n_splines)</span><span class="s2">,</span>
                        <span class="s1">] = f_max[-degree:]</span>

            <span class="s2">elif </span><span class="s1">self.extrapolation == </span><span class="s3">&quot;linear&quot;</span><span class="s1">:</span>
                <span class="s5"># Continue the degree first and degree last spline bases</span>
                <span class="s5"># linearly beyond the boundaries, with slope = derivative at</span>
                <span class="s5"># the boundary.</span>
                <span class="s5"># Note that all others have derivative = value = 0 at the</span>
                <span class="s5"># boundaries.</span>

                <span class="s5"># spline derivatives = slopes at boundaries</span>
                <span class="s1">fp_min</span><span class="s2">, </span><span class="s1">fp_max = spl(xmin</span><span class="s2">, </span><span class="s1">nu=</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">spl(xmax</span><span class="s2">, </span><span class="s1">nu=</span><span class="s4">1</span><span class="s1">)</span>
                <span class="s5"># Compute the linear continuation.</span>
                <span class="s2">if </span><span class="s1">degree &lt;= </span><span class="s4">1</span><span class="s1">:</span>
                    <span class="s5"># For degree=1, the derivative of 2nd spline is not zero at</span>
                    <span class="s5"># boundary. For degree=0 it is the same as 'constant'.</span>
                    <span class="s1">degree += </span><span class="s4">1</span>
                <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(degree):</span>
                    <span class="s1">mask = X[:</span><span class="s2">, </span><span class="s1">i] &lt; xmin</span>
                    <span class="s2">if </span><span class="s1">np.any(mask):</span>
                        <span class="s1">linear_extr = f_min[j] + (X[mask</span><span class="s2">, </span><span class="s1">i] - xmin) * fp_min[j]</span>
                        <span class="s2">if </span><span class="s1">use_sparse:</span>
                            <span class="s5"># Note: See comment about SparseEfficiencyWarning above.</span>
                            <span class="s1">XBS_sparse = XBS_sparse.tolil()</span>
                            <span class="s1">XBS_sparse[mask</span><span class="s2">, </span><span class="s1">j] = linear_extr</span>
                        <span class="s2">else</span><span class="s1">:</span>
                            <span class="s1">XBS[mask</span><span class="s2">, </span><span class="s1">i * n_splines + j] = linear_extr</span>

                    <span class="s1">mask = X[:</span><span class="s2">, </span><span class="s1">i] &gt; xmax</span>
                    <span class="s2">if </span><span class="s1">np.any(mask):</span>
                        <span class="s1">k = n_splines - </span><span class="s4">1 </span><span class="s1">- j</span>
                        <span class="s1">linear_extr = f_max[k] + (X[mask</span><span class="s2">, </span><span class="s1">i] - xmax) * fp_max[k]</span>
                        <span class="s2">if </span><span class="s1">use_sparse:</span>
                            <span class="s5"># Note: See comment about SparseEfficiencyWarning above.</span>
                            <span class="s1">XBS_sparse = XBS_sparse.tolil()</span>
                            <span class="s1">XBS_sparse[mask</span><span class="s2">, </span><span class="s1">k : k + </span><span class="s4">1</span><span class="s1">] = linear_extr[:</span><span class="s2">, None</span><span class="s1">]</span>
                        <span class="s2">else</span><span class="s1">:</span>
                            <span class="s1">XBS[mask</span><span class="s2">, </span><span class="s1">i * n_splines + k] = linear_extr</span>

            <span class="s2">if </span><span class="s1">use_sparse:</span>
                <span class="s1">XBS_sparse = XBS_sparse.tocsr()</span>
                <span class="s1">output_list.append(XBS_sparse)</span>

        <span class="s2">if </span><span class="s1">use_sparse:</span>
            <span class="s5"># TODO: Remove this conditional error when the minimum supported version of</span>
            <span class="s5"># SciPy is 1.9.2</span>
            <span class="s5"># `scipy.sparse.hstack` breaks in scipy&lt;1.9.2</span>
            <span class="s5"># when `n_features_out_ &gt; max_int32`</span>
            <span class="s1">max_int32 = np.iinfo(np.int32).max</span>
            <span class="s1">all_int32 = </span><span class="s2">True</span>
            <span class="s2">for </span><span class="s1">mat </span><span class="s2">in </span><span class="s1">output_list:</span>
                <span class="s1">all_int32 &amp;= mat.indices.dtype == np.int32</span>
            <span class="s2">if </span><span class="s1">(</span>
                <span class="s1">sp_version &lt; parse_version(</span><span class="s3">&quot;1.9.2&quot;</span><span class="s1">)</span>
                <span class="s2">and </span><span class="s1">self.n_features_out_ &gt; max_int32</span>
                <span class="s2">and </span><span class="s1">all_int32</span>
            <span class="s1">):</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s3">&quot;In scipy versions `&lt;1.9.2`, the function `scipy.sparse.hstack`&quot;</span>
                    <span class="s3">&quot; produces negative columns when:</span><span class="s2">\n</span><span class="s3">1. The output shape contains&quot;</span>
                    <span class="s3">&quot; `n_cols` too large to be represented by a 32bit signed&quot;</span>
                    <span class="s3">&quot; integer.</span><span class="s2">\n</span><span class="s3">. All sub-matrices to be stacked have indices of&quot;</span>
                    <span class="s3">&quot; dtype `np.int32`.</span><span class="s2">\n</span><span class="s3">To avoid this error, either use a version&quot;</span>
                    <span class="s3">&quot; of scipy `&gt;=1.9.2` or alter the `SplineTransformer`&quot;</span>
                    <span class="s3">&quot; transformer to produce fewer than 2^31 output features&quot;</span>
                <span class="s1">)</span>
            <span class="s1">XBS = sparse.hstack(output_list</span><span class="s2">, </span><span class="s1">format=</span><span class="s3">&quot;csr&quot;</span><span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">self.sparse_output:</span>
            <span class="s5"># TODO: Remove ones scipy 1.10 is the minimum version. See comments above.</span>
            <span class="s1">XBS = sparse.csr_matrix(XBS)</span>

        <span class="s2">if </span><span class="s1">self.include_bias:</span>
            <span class="s2">return </span><span class="s1">XBS</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s5"># We throw away one spline basis per feature.</span>
            <span class="s5"># We chose the last one.</span>
            <span class="s1">indices = [j </span><span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(XBS.shape[</span><span class="s4">1</span><span class="s1">]) </span><span class="s2">if </span><span class="s1">(j + </span><span class="s4">1</span><span class="s1">) % n_splines != </span><span class="s4">0</span><span class="s1">]</span>
            <span class="s2">return </span><span class="s1">XBS[:</span><span class="s2">, </span><span class="s1">indices]</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span>
            <span class="s3">&quot;_xfail_checks&quot;</span><span class="s1">: {</span>
                <span class="s3">&quot;check_estimators_pickle&quot;</span><span class="s1">: (</span>
                    <span class="s3">&quot;Current Scipy implementation of _bsplines does not&quot;</span>
                    <span class="s3">&quot;support const memory views.&quot;</span>
                <span class="s1">)</span><span class="s2">,</span>
            <span class="s1">}</span>
        <span class="s1">}</span>
</pre>
</body>
</html>