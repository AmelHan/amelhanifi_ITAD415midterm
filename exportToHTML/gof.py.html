<html>
<head>
<title>gof.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
gof.py</font>
</center></td></tr></table>
<pre><span class="s0">'''extra statistical function and helper functions 
 
contains: 
 
* goodness-of-fit tests 
  - powerdiscrepancy 
  - gof_chisquare_discrete 
  - gof_binning_discrete 
 
 
 
Author: Josef Perktold 
License : BSD-3 
 
changes 
------- 
2013-02-25 : add chisquare_power, effectsize and &quot;value&quot; 
 
'''</span>
<span class="s2">from </span><span class="s1">statsmodels.compat.python </span><span class="s2">import </span><span class="s1">lrange</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">stats</span>


<span class="s3"># copied from regression/stats.utils</span>
<span class="s2">def </span><span class="s1">powerdiscrepancy(observed</span><span class="s2">, </span><span class="s1">expected</span><span class="s2">, </span><span class="s1">lambd=</span><span class="s4">0.0</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">ddof=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s0">r&quot;&quot;&quot;Calculates power discrepancy, a class of goodness-of-fit tests 
    as a measure of discrepancy between observed and expected data. 
 
    This contains several goodness-of-fit tests as special cases, see the 
    description of lambd, the exponent of the power discrepancy. The pvalue 
    is based on the asymptotic chi-square distribution of the test statistic. 
 
    freeman_tukey: 
    D(x|\theta) = \sum_j (\sqrt{x_j} - \sqrt{e_j})^2 
 
    Parameters 
    ---------- 
    o : Iterable 
        Observed values 
    e : Iterable 
        Expected values 
    lambd : {float, str} 
        * float : exponent `a` for power discrepancy 
        * 'loglikeratio': a = 0 
        * 'freeman_tukey': a = -0.5 
        * 'pearson': a = 1   (standard chisquare test statistic) 
        * 'modified_loglikeratio': a = -1 
        * 'cressie_read': a = 2/3 
        * 'neyman' : a = -2 (Neyman-modified chisquare, reference from a book?) 
    axis : int 
        axis for observations of one series 
    ddof : int 
        degrees of freedom correction, 
 
    Returns 
    ------- 
    D_obs : Discrepancy of observed values 
    pvalue : pvalue 
 
 
    References 
    ---------- 
    Cressie, Noel  and Timothy R. C. Read, Multinomial Goodness-of-Fit Tests, 
        Journal of the Royal Statistical Society. Series B (Methodological), 
        Vol. 46, No. 3 (1984), pp. 440-464 
 
    Campbell B. Read: Freeman-Tukey chi-squared goodness-of-fit statistics, 
        Statistics &amp; Probability Letters 18 (1993) 271-278 
 
    Nobuhiro Taneichi, Yuri Sekiya, Akio Suzukawa, Asymptotic Approximations 
        for the Distributions of the Multinomial Goodness-of-Fit Statistics 
        under Local Alternatives, Journal of Multivariate Analysis 81, 335?359 (2002) 
    Steele, M. 1,2, C. Hurst 3 and J. Chaseling, Simulated Power of Discrete 
        Goodness-of-Fit Tests for Likert Type Data 
 
    Examples 
    -------- 
 
    &gt;&gt;&gt; observed = np.array([ 2.,  4.,  2.,  1.,  1.]) 
    &gt;&gt;&gt; expected = np.array([ 0.2,  0.2,  0.2,  0.2,  0.2]) 
 
    for checking correct dimension with multiple series 
 
    &gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,observed)).T, 10*expected, lambd='freeman_tukey',axis=1) 
    (array([[ 2.745166,  2.745166]]), array([[ 0.6013346,  0.6013346]])) 
    &gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,observed)).T, 10*expected,axis=1) 
    (array([[ 2.77258872,  2.77258872]]), array([[ 0.59657359,  0.59657359]])) 
    &gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,observed)).T, 10*expected, lambd=0,axis=1) 
    (array([[ 2.77258872,  2.77258872]]), array([[ 0.59657359,  0.59657359]])) 
    &gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,observed)).T, 10*expected, lambd=1,axis=1) 
    (array([[ 3.,  3.]]), array([[ 0.5578254,  0.5578254]])) 
    &gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,observed)).T, 10*expected, lambd=2/3.0,axis=1) 
    (array([[ 2.89714546,  2.89714546]]), array([[ 0.57518277,  0.57518277]])) 
    &gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,observed)).T, expected, lambd=2/3.0,axis=1) 
    (array([[ 2.89714546,  2.89714546]]), array([[ 0.57518277,  0.57518277]])) 
    &gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,observed)), expected, lambd=2/3.0, axis=0) 
    (array([[ 2.89714546,  2.89714546]]), array([[ 0.57518277,  0.57518277]])) 
 
    each random variable can have different total count/sum 
 
    &gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,2*observed)), expected, lambd=2/3.0, axis=0) 
    (array([[ 2.89714546,  5.79429093]]), array([[ 0.57518277,  0.21504648]])) 
    &gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,2*observed)), expected, lambd=2/3.0, axis=0) 
    (array([[ 2.89714546,  5.79429093]]), array([[ 0.57518277,  0.21504648]])) 
    &gt;&gt;&gt; powerdiscrepancy(np.column_stack((2*observed,2*observed)), expected, lambd=2/3.0, axis=0) 
    (array([[ 5.79429093,  5.79429093]]), array([[ 0.21504648,  0.21504648]])) 
    &gt;&gt;&gt; powerdiscrepancy(np.column_stack((2*observed,2*observed)), 20*expected, lambd=2/3.0, axis=0) 
    (array([[ 5.79429093,  5.79429093]]), array([[ 0.21504648,  0.21504648]])) 
    &gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,2*observed)), np.column_stack((10*expected,20*expected)), lambd=2/3.0, axis=0) 
    (array([[ 2.89714546,  5.79429093]]), array([[ 0.57518277,  0.21504648]])) 
    &gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,2*observed)), np.column_stack((10*expected,20*expected)), lambd=-1, axis=0) 
    (array([[ 2.77258872,  5.54517744]]), array([[ 0.59657359,  0.2357868 ]])) 
    &quot;&quot;&quot;</span>
    <span class="s1">o = np.array(observed)</span>
    <span class="s1">e = np.array(expected)</span>

    <span class="s2">if not </span><span class="s1">isinstance(lambd</span><span class="s2">, </span><span class="s1">str):</span>
        <span class="s1">a = lambd</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">if </span><span class="s1">lambd == </span><span class="s5">'loglikeratio'</span><span class="s1">:</span>
            <span class="s1">a = </span><span class="s4">0</span>
        <span class="s2">elif </span><span class="s1">lambd == </span><span class="s5">'freeman_tukey'</span><span class="s1">:</span>
            <span class="s1">a = -</span><span class="s4">0.5</span>
        <span class="s2">elif </span><span class="s1">lambd == </span><span class="s5">'pearson'</span><span class="s1">:</span>
            <span class="s1">a = </span><span class="s4">1</span>
        <span class="s2">elif </span><span class="s1">lambd == </span><span class="s5">'modified_loglikeratio'</span><span class="s1">:</span>
            <span class="s1">a = -</span><span class="s4">1</span>
        <span class="s2">elif </span><span class="s1">lambd == </span><span class="s5">'cressie_read'</span><span class="s1">:</span>
            <span class="s1">a = </span><span class="s4">2</span><span class="s1">/</span><span class="s4">3.0</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'lambd has to be a number or one of '</span>
                             <span class="s5">'loglikeratio, freeman_tukey, pearson, '</span>
                             <span class="s5">'modified_loglikeratio or cressie_read'</span><span class="s1">)</span>

    <span class="s1">n = np.sum(o</span><span class="s2">, </span><span class="s1">axis=axis)</span>
    <span class="s1">nt = n</span>
    <span class="s2">if </span><span class="s1">n.size&gt;</span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">n = np.atleast_2d(n)</span>
        <span class="s2">if </span><span class="s1">axis == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">nt = n.T     </span><span class="s3"># need both for 2d, n and nt for broadcasting</span>
        <span class="s2">if </span><span class="s1">e.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">e = np.atleast_2d(e)</span>
            <span class="s2">if </span><span class="s1">axis == </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s1">e = e.T</span>

    <span class="s2">if </span><span class="s1">np.allclose(np.sum(e</span><span class="s2">, </span><span class="s1">axis=axis)</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-8</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">0</span><span class="s1">):</span>
        <span class="s1">p = e/(</span><span class="s4">1.0</span><span class="s1">*nt)</span>
    <span class="s2">elif </span><span class="s1">np.allclose(np.sum(e</span><span class="s2">, </span><span class="s1">axis=axis)</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-8</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">0</span><span class="s1">):</span>
        <span class="s1">p = e</span>
        <span class="s1">e = nt * e</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'observed and expected need to have the same '</span>
                         <span class="s5">'number of observations, or e needs to add to 1'</span><span class="s1">)</span>
    <span class="s1">k = o.shape[axis]</span>
    <span class="s2">if </span><span class="s1">e.shape[axis] != k:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'observed and expected need to have the same '</span>
                         <span class="s5">'number of bins'</span><span class="s1">)</span>

    <span class="s3"># Note: taken from formulas, to simplify cancel n</span>
    <span class="s2">if </span><span class="s1">a == </span><span class="s4">0</span><span class="s1">:   </span><span class="s3"># log likelihood ratio</span>
        <span class="s1">D_obs = </span><span class="s4">2</span><span class="s1">*n * np.sum(o/(</span><span class="s4">1.0</span><span class="s1">*nt) * np.log(o/e)</span><span class="s2">, </span><span class="s1">axis=axis)</span>
    <span class="s2">elif </span><span class="s1">a == -</span><span class="s4">1</span><span class="s1">:  </span><span class="s3"># modified log likelihood ratio</span>
        <span class="s1">D_obs = </span><span class="s4">2</span><span class="s1">*n * np.sum(e/(</span><span class="s4">1.0</span><span class="s1">*nt) * np.log(e/o)</span><span class="s2">, </span><span class="s1">axis=axis)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">D_obs = </span><span class="s4">2</span><span class="s1">*n/a/(a+</span><span class="s4">1</span><span class="s1">) * np.sum(o/(</span><span class="s4">1.0</span><span class="s1">*nt) * ((o/e)**a - </span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">axis=axis)</span>

    <span class="s2">return </span><span class="s1">D_obs</span><span class="s2">, </span><span class="s1">stats.chi2.sf(D_obs</span><span class="s2">,</span><span class="s1">k-</span><span class="s4">1</span><span class="s1">-ddof)</span>



<span class="s3">#todo: need also binning for continuous distribution</span>
<span class="s3">#      and separated binning function to be used for powerdiscrepancy</span>

<span class="s2">def </span><span class="s1">gof_chisquare_discrete(distfn</span><span class="s2">, </span><span class="s1">arg</span><span class="s2">, </span><span class="s1">rvs</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">msg):</span>
    <span class="s0">'''perform chisquare test for random sample of a discrete distribution 
 
    Parameters 
    ---------- 
    distname : str 
        name of distribution function 
    arg : sequence 
        parameters of distribution 
    alpha : float 
        significance level, threshold for p-value 
 
    Returns 
    ------- 
    result : bool 
        0 if test passes, 1 if test fails 
 
    Notes 
    ----- 
    originally written for scipy.stats test suite, 
    still needs to be checked for standalone usage, insufficient input checking 
    may not run yet (after copy/paste) 
 
    refactor: maybe a class, check returns, or separate binning from 
        test results 
    '''</span>

    <span class="s3"># define parameters for test</span>
<span class="s3">##    n=2000</span>
    <span class="s1">n = len(rvs)</span>
    <span class="s1">nsupp = </span><span class="s4">20</span>
    <span class="s1">wsupp = </span><span class="s4">1.0</span><span class="s1">/nsupp</span>

<span class="s3">##    distfn = getattr(stats, distname)</span>
<span class="s3">##    np.random.seed(9765456)</span>
<span class="s3">##    rvs = distfn.rvs(size=n,*arg)</span>

    <span class="s3"># construct intervals with minimum mass 1/nsupp</span>
    <span class="s3"># intervalls are left-half-open as in a cdf difference</span>
    <span class="s1">distsupport = lrange(max(distfn.a</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1000</span><span class="s1">)</span><span class="s2">, </span><span class="s1">min(distfn.b</span><span class="s2">, </span><span class="s4">1000</span><span class="s1">) + </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">last = </span><span class="s4">0</span>
    <span class="s1">distsupp = [max(distfn.a</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1000</span><span class="s1">)]</span>
    <span class="s1">distmass = []</span>
    <span class="s2">for </span><span class="s1">ii </span><span class="s2">in </span><span class="s1">distsupport:</span>
        <span class="s1">current = distfn.cdf(ii</span><span class="s2">,</span><span class="s1">*arg)</span>
        <span class="s2">if </span><span class="s1">current - last &gt;= wsupp-</span><span class="s4">1e-14</span><span class="s1">:</span>
            <span class="s1">distsupp.append(ii)</span>
            <span class="s1">distmass.append(current - last)</span>
            <span class="s1">last = current</span>
            <span class="s2">if </span><span class="s1">current &gt; (</span><span class="s4">1</span><span class="s1">-wsupp):</span>
                <span class="s2">break</span>
    <span class="s2">if </span><span class="s1">distsupp[-</span><span class="s4">1</span><span class="s1">]  &lt; distfn.b:</span>
        <span class="s1">distsupp.append(distfn.b)</span>
        <span class="s1">distmass.append(</span><span class="s4">1</span><span class="s1">-last)</span>
    <span class="s1">distsupp = np.array(distsupp)</span>
    <span class="s1">distmass = np.array(distmass)</span>

    <span class="s3"># convert intervals to right-half-open as required by histogram</span>
    <span class="s1">histsupp = distsupp+</span><span class="s4">1e-8</span>
    <span class="s1">histsupp[</span><span class="s4">0</span><span class="s1">] = distfn.a</span>

    <span class="s3"># find sample frequencies and perform chisquare test</span>
    <span class="s3">#TODO: move to compatibility.py</span>
    <span class="s1">freq</span><span class="s2">, </span><span class="s1">hsupp = np.histogram(rvs</span><span class="s2">,</span><span class="s1">histsupp)</span>
    <span class="s1">cdfs = distfn.cdf(distsupp</span><span class="s2">,</span><span class="s1">*arg)</span>
    <span class="s1">(chis</span><span class="s2">,</span><span class="s1">pval) = stats.chisquare(np.array(freq)</span><span class="s2">,</span><span class="s1">n*distmass)</span>

    <span class="s2">return </span><span class="s1">chis</span><span class="s2">, </span><span class="s1">pval</span><span class="s2">, </span><span class="s1">(pval &gt; alpha)</span><span class="s2">, </span><span class="s5">'chisquare - test for %s' </span><span class="s1">\</span>
           <span class="s5">'at arg = %s with pval = %s' </span><span class="s1">% (msg</span><span class="s2">,</span><span class="s1">str(arg)</span><span class="s2">,</span><span class="s1">str(pval))</span>

<span class="s3"># copy/paste, remove code duplication when it works</span>
<span class="s2">def </span><span class="s1">gof_binning_discrete(rvs</span><span class="s2">, </span><span class="s1">distfn</span><span class="s2">, </span><span class="s1">arg</span><span class="s2">, </span><span class="s1">nsupp=</span><span class="s4">20</span><span class="s1">):</span>
    <span class="s0">'''get bins for chisquare type gof tests for a discrete distribution 
 
    Parameters 
    ---------- 
    rvs : ndarray 
        sample data 
    distname : str 
        name of distribution function 
    arg : sequence 
        parameters of distribution 
    nsupp : int 
        number of bins. The algorithm tries to find bins with equal weights. 
        depending on the distribution, the actual number of bins can be smaller. 
 
    Returns 
    ------- 
    freq : ndarray 
        empirical frequencies for sample; not normalized, adds up to sample size 
    expfreq : ndarray 
        theoretical frequencies according to distribution 
    histsupp : ndarray 
        bin boundaries for histogram, (added 1e-8 for numerical robustness) 
 
    Notes 
    ----- 
    The results can be used for a chisquare test :: 
 
        (chis,pval) = stats.chisquare(freq, expfreq) 
 
    originally written for scipy.stats test suite, 
    still needs to be checked for standalone usage, insufficient input checking 
    may not run yet (after copy/paste) 
 
    refactor: maybe a class, check returns, or separate binning from 
        test results 
    todo : 
      optimal number of bins ? (check easyfit), 
      recommendation in literature at least 5 expected observations in each bin 
 
    '''</span>

    <span class="s3"># define parameters for test</span>
<span class="s3">##    n=2000</span>
    <span class="s1">n = len(rvs)</span>

    <span class="s1">wsupp = </span><span class="s4">1.0</span><span class="s1">/nsupp</span>

<span class="s3">##    distfn = getattr(stats, distname)</span>
<span class="s3">##    np.random.seed(9765456)</span>
<span class="s3">##    rvs = distfn.rvs(size=n,*arg)</span>

    <span class="s3"># construct intervals with minimum mass 1/nsupp</span>
    <span class="s3"># intervalls are left-half-open as in a cdf difference</span>
    <span class="s1">distsupport = lrange(max(distfn.a</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1000</span><span class="s1">)</span><span class="s2">, </span><span class="s1">min(distfn.b</span><span class="s2">, </span><span class="s4">1000</span><span class="s1">) + </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">last = </span><span class="s4">0</span>
    <span class="s1">distsupp = [max(distfn.a</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1000</span><span class="s1">)]</span>
    <span class="s1">distmass = []</span>
    <span class="s2">for </span><span class="s1">ii </span><span class="s2">in </span><span class="s1">distsupport:</span>
        <span class="s1">current = distfn.cdf(ii</span><span class="s2">,</span><span class="s1">*arg)</span>
        <span class="s2">if </span><span class="s1">current - last &gt;= wsupp-</span><span class="s4">1e-14</span><span class="s1">:</span>
            <span class="s1">distsupp.append(ii)</span>
            <span class="s1">distmass.append(current - last)</span>
            <span class="s1">last = current</span>
            <span class="s2">if </span><span class="s1">current &gt; (</span><span class="s4">1</span><span class="s1">-wsupp):</span>
                <span class="s2">break</span>
    <span class="s2">if </span><span class="s1">distsupp[-</span><span class="s4">1</span><span class="s1">]  &lt; distfn.b:</span>
        <span class="s1">distsupp.append(distfn.b)</span>
        <span class="s1">distmass.append(</span><span class="s4">1</span><span class="s1">-last)</span>
    <span class="s1">distsupp = np.array(distsupp)</span>
    <span class="s1">distmass = np.array(distmass)</span>

    <span class="s3"># convert intervals to right-half-open as required by histogram</span>
    <span class="s1">histsupp = distsupp+</span><span class="s4">1e-8</span>
    <span class="s1">histsupp[</span><span class="s4">0</span><span class="s1">] = distfn.a</span>

    <span class="s3"># find sample frequencies and perform chisquare test</span>
    <span class="s1">freq</span><span class="s2">,</span><span class="s1">hsupp = np.histogram(rvs</span><span class="s2">,</span><span class="s1">histsupp)</span>
    <span class="s3">#freq,hsupp = np.histogram(rvs,histsupp,new=True)</span>
    <span class="s1">cdfs = distfn.cdf(distsupp</span><span class="s2">,</span><span class="s1">*arg)</span>
    <span class="s2">return </span><span class="s1">np.array(freq)</span><span class="s2">, </span><span class="s1">n*distmass</span><span class="s2">, </span><span class="s1">histsupp</span>


<span class="s3"># -*- coding: utf-8 -*-</span>
<span class="s5">&quot;&quot;&quot;Extension to chisquare goodness-of-fit test 
 
Created on Mon Feb 25 13:46:53 2013 
 
Author: Josef Perktold 
License: BSD-3 
&quot;&quot;&quot;</span>



<span class="s2">def </span><span class="s1">chisquare(f_obs</span><span class="s2">, </span><span class="s1">f_exp=</span><span class="s2">None, </span><span class="s1">value=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">ddof=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">return_basic=</span><span class="s2">True</span><span class="s1">):</span>
    <span class="s0">'''chisquare goodness-of-fit test 
 
    The null hypothesis is that the distance between the expected distribution 
    and the observed frequencies is ``value``. The alternative hypothesis is 
    that the distance is larger than ``value``. ``value`` is normalized in 
    terms of effect size. 
 
    The standard chisquare test has the null hypothesis that ``value=0``, that 
    is the distributions are the same. 
 
 
    Notes 
    ----- 
    The case with value greater than zero is similar to an equivalence test, 
    that the exact null hypothesis is replaced by an approximate hypothesis. 
    However, TOST &quot;reverses&quot; null and alternative hypothesis, while here the 
    alternative hypothesis is that the distance (divergence) is larger than a 
    threshold. 
 
    References 
    ---------- 
    McLaren, ... 
    Drost,... 
 
    See Also 
    -------- 
    powerdiscrepancy 
    scipy.stats.chisquare 
 
    '''</span>

    <span class="s1">f_obs = np.asarray(f_obs)</span>
    <span class="s1">n_bins = len(f_obs)</span>
    <span class="s1">nobs = f_obs.sum(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">f_exp </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s3"># uniform distribution</span>
        <span class="s1">f_exp = np.empty(n_bins</span><span class="s2">, </span><span class="s1">float)</span>
        <span class="s1">f_exp.fill(nobs / float(n_bins))</span>

    <span class="s1">f_exp = np.asarray(f_exp</span><span class="s2">, </span><span class="s1">float)</span>

    <span class="s1">chisq = ((f_obs - f_exp)**</span><span class="s4">2 </span><span class="s1">/ f_exp).sum(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">value == </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s1">pvalue = stats.chi2.sf(chisq</span><span class="s2">, </span><span class="s1">n_bins - </span><span class="s4">1 </span><span class="s1">- ddof)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">pvalue = stats.ncx2.sf(chisq</span><span class="s2">, </span><span class="s1">n_bins - </span><span class="s4">1 </span><span class="s1">- ddof</span><span class="s2">, </span><span class="s1">value**</span><span class="s4">2 </span><span class="s1">* nobs)</span>

    <span class="s2">if </span><span class="s1">return_basic:</span>
        <span class="s2">return </span><span class="s1">chisq</span><span class="s2">, </span><span class="s1">pvalue</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">chisq</span><span class="s2">, </span><span class="s1">pvalue    </span><span class="s3">#TODO: replace with TestResults</span>


<span class="s2">def </span><span class="s1">chisquare_power(effect_size</span><span class="s2">, </span><span class="s1">nobs</span><span class="s2">, </span><span class="s1">n_bins</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s4">0.05</span><span class="s2">, </span><span class="s1">ddof=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s0">'''power of chisquare goodness of fit test 
 
    effect size is sqrt of chisquare statistic divided by nobs 
 
    Parameters 
    ---------- 
    effect_size : float 
        This is the deviation from the Null of the normalized chi_square 
        statistic. This follows Cohen's definition (sqrt). 
    nobs : int or float 
        number of observations 
    n_bins : int (or float) 
        number of bins, or points in the discrete distribution 
    alpha : float in (0,1) 
        significance level of the test, default alpha=0.05 
 
    Returns 
    ------- 
    power : float 
        power of the test at given significance level at effect size 
 
    Notes 
    ----- 
    This function also works vectorized if all arguments broadcast. 
 
    This can also be used to calculate the power for power divergence test. 
    However, for the range of more extreme values of the power divergence 
    parameter, this power is not a very good approximation for samples of 
    small to medium size (Drost et al. 1989) 
 
    References 
    ---------- 
    Drost, ... 
 
    See Also 
    -------- 
    chisquare_effectsize 
    statsmodels.stats.GofChisquarePower 
 
    '''</span>
    <span class="s1">crit = stats.chi2.isf(alpha</span><span class="s2">, </span><span class="s1">n_bins - </span><span class="s4">1 </span><span class="s1">- ddof)</span>
    <span class="s1">power = stats.ncx2.sf(crit</span><span class="s2">, </span><span class="s1">n_bins - </span><span class="s4">1 </span><span class="s1">- ddof</span><span class="s2">, </span><span class="s1">effect_size**</span><span class="s4">2 </span><span class="s1">* nobs)</span>
    <span class="s2">return </span><span class="s1">power</span>


<span class="s2">def </span><span class="s1">chisquare_effectsize(probs0</span><span class="s2">, </span><span class="s1">probs1</span><span class="s2">, </span><span class="s1">correction=</span><span class="s2">None, </span><span class="s1">cohen=</span><span class="s2">True, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s0">'''effect size for a chisquare goodness-of-fit test 
 
    Parameters 
    ---------- 
    probs0 : array_like 
        probabilities or cell frequencies under the Null hypothesis 
    probs1 : array_like 
        probabilities or cell frequencies under the Alternative hypothesis 
        probs0 and probs1 need to have the same length in the ``axis`` dimension. 
        and broadcast in the other dimensions 
        Both probs0 and probs1 are normalized to add to one (in the ``axis`` 
        dimension). 
    correction : None or tuple 
        If None, then the effect size is the chisquare statistic divide by 
        the number of observations. 
        If the correction is a tuple (nobs, df), then the effectsize is 
        corrected to have less bias and a smaller variance. However, the 
        correction can make the effectsize negative. In that case, the 
        effectsize is set to zero. 
        Pederson and Johnson (1990) as referenced in McLaren et all. (1994) 
    cohen : bool 
        If True, then the square root is returned as in the definition of the 
        effect size by Cohen (1977), If False, then the original effect size 
        is returned. 
    axis : int 
        If the probability arrays broadcast to more than 1 dimension, then 
        this is the axis over which the sums are taken. 
 
    Returns 
    ------- 
    effectsize : float 
        effect size of chisquare test 
 
    '''</span>
    <span class="s1">probs0 = np.asarray(probs0</span><span class="s2">, </span><span class="s1">float)</span>
    <span class="s1">probs1 = np.asarray(probs1</span><span class="s2">, </span><span class="s1">float)</span>
    <span class="s1">probs0 = probs0 / probs0.sum(axis)</span>
    <span class="s1">probs1 = probs1 / probs1.sum(axis)</span>

    <span class="s1">d2 = ((probs1 - probs0)**</span><span class="s4">2 </span><span class="s1">/ probs0).sum(axis)</span>

    <span class="s2">if </span><span class="s1">correction </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">nobs</span><span class="s2">, </span><span class="s1">df = correction</span>
        <span class="s1">diff = ((probs1 - probs0) / probs0).sum(axis)</span>
        <span class="s1">d2 = np.maximum((d2 * nobs - diff - df) / (nobs - </span><span class="s4">1.</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">cohen:</span>
        <span class="s2">return </span><span class="s1">np.sqrt(d2)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">d2</span>
</pre>
</body>
</html>