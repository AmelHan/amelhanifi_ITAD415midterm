<html>
<head>
<title>isotonic.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
isotonic.py</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: Fabian Pedregosa &lt;fabian@fseoane.net&gt;</span>
<span class="s0">#          Alexandre Gramfort &lt;alexandre.gramfort@inria.fr&gt;</span>
<span class="s0">#          Nelle Varoquaux &lt;nelle.varoquaux@gmail.com&gt;</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">import </span><span class="s1">math</span>
<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">numbers </span><span class="s2">import </span><span class="s1">Real</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">interpolate</span>
<span class="s2">from </span><span class="s1">scipy.stats </span><span class="s2">import </span><span class="s1">spearmanr</span>

<span class="s2">from </span><span class="s1">._isotonic </span><span class="s2">import </span><span class="s1">_inplace_contiguous_isotonic_regression</span><span class="s2">, </span><span class="s1">_make_unique</span>
<span class="s2">from </span><span class="s1">.base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">RegressorMixin</span><span class="s2">, </span><span class="s1">TransformerMixin</span><span class="s2">, </span><span class="s1">_fit_context</span>
<span class="s2">from </span><span class="s1">.utils </span><span class="s2">import </span><span class="s1">check_array</span><span class="s2">, </span><span class="s1">check_consistent_length</span>
<span class="s2">from </span><span class="s1">.utils._param_validation </span><span class="s2">import </span><span class="s1">Interval</span><span class="s2">, </span><span class="s1">StrOptions</span>
<span class="s2">from </span><span class="s1">.utils.validation </span><span class="s2">import </span><span class="s1">_check_sample_weight</span><span class="s2">, </span><span class="s1">check_is_fitted</span>

<span class="s1">__all__ = [</span><span class="s3">&quot;check_increasing&quot;</span><span class="s2">, </span><span class="s3">&quot;isotonic_regression&quot;</span><span class="s2">, </span><span class="s3">&quot;IsotonicRegression&quot;</span><span class="s1">]</span>


<span class="s2">def </span><span class="s1">check_increasing(x</span><span class="s2">, </span><span class="s1">y):</span>
    <span class="s4">&quot;&quot;&quot;Determine whether y is monotonically correlated with x. 
 
    y is found increasing or decreasing with respect to x based on a Spearman 
    correlation test. 
 
    Parameters 
    ---------- 
    x : array-like of shape (n_samples,) 
            Training data. 
 
    y : array-like of shape (n_samples,) 
        Training target. 
 
    Returns 
    ------- 
    increasing_bool : boolean 
        Whether the relationship is increasing or decreasing. 
 
    Notes 
    ----- 
    The Spearman correlation coefficient is estimated from the data, and the 
    sign of the resulting estimate is used as the result. 
 
    In the event that the 95% confidence interval based on Fisher transform 
    spans zero, a warning is raised. 
 
    References 
    ---------- 
    Fisher transformation. Wikipedia. 
    https://en.wikipedia.org/wiki/Fisher_transformation 
    &quot;&quot;&quot;</span>

    <span class="s0"># Calculate Spearman rho estimate and set return accordingly.</span>
    <span class="s1">rho</span><span class="s2">, </span><span class="s1">_ = spearmanr(x</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">increasing_bool = rho &gt;= </span><span class="s5">0</span>

    <span class="s0"># Run Fisher transform to get the rho CI, but handle rho=+/-1</span>
    <span class="s2">if </span><span class="s1">rho </span><span class="s2">not in </span><span class="s1">[-</span><span class="s5">1.0</span><span class="s2">, </span><span class="s5">1.0</span><span class="s1">] </span><span class="s2">and </span><span class="s1">len(x) &gt; </span><span class="s5">3</span><span class="s1">:</span>
        <span class="s1">F = </span><span class="s5">0.5 </span><span class="s1">* math.log((</span><span class="s5">1.0 </span><span class="s1">+ rho) / (</span><span class="s5">1.0 </span><span class="s1">- rho))</span>
        <span class="s1">F_se = </span><span class="s5">1 </span><span class="s1">/ math.sqrt(len(x) - </span><span class="s5">3</span><span class="s1">)</span>

        <span class="s0"># Use a 95% CI, i.e., +/-1.96 S.E.</span>
        <span class="s0"># https://en.wikipedia.org/wiki/Fisher_transformation</span>
        <span class="s1">rho_0 = math.tanh(F - </span><span class="s5">1.96 </span><span class="s1">* F_se)</span>
        <span class="s1">rho_1 = math.tanh(F + </span><span class="s5">1.96 </span><span class="s1">* F_se)</span>

        <span class="s0"># Warn if the CI spans zero.</span>
        <span class="s2">if </span><span class="s1">np.sign(rho_0) != np.sign(rho_1):</span>
            <span class="s1">warnings.warn(</span>
                <span class="s3">&quot;Confidence interval of the Spearman &quot;</span>
                <span class="s3">&quot;correlation coefficient spans zero. &quot;</span>
                <span class="s3">&quot;Determination of ``increasing`` may be &quot;</span>
                <span class="s3">&quot;suspect.&quot;</span>
            <span class="s1">)</span>

    <span class="s2">return </span><span class="s1">increasing_bool</span>


<span class="s2">def </span><span class="s1">isotonic_regression(</span>
    <span class="s1">y</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None, </span><span class="s1">y_min=</span><span class="s2">None, </span><span class="s1">y_max=</span><span class="s2">None, </span><span class="s1">increasing=</span><span class="s2">True</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Solve the isotonic regression model. 
 
    Read more in the :ref:`User Guide &lt;isotonic&gt;`. 
 
    Parameters 
    ---------- 
    y : array-like of shape (n_samples,) 
        The data. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Weights on each point of the regression. 
        If None, weight is set to 1 (equal weights). 
 
    y_min : float, default=None 
        Lower bound on the lowest predicted value (the minimum value may 
        still be higher). If not set, defaults to -inf. 
 
    y_max : float, default=None 
        Upper bound on the highest predicted value (the maximum may still be 
        lower). If not set, defaults to +inf. 
 
    increasing : bool, default=True 
        Whether to compute ``y_`` is increasing (if set to True) or decreasing 
        (if set to False). 
 
    Returns 
    ------- 
    y_ : ndarray of shape (n_samples,) 
        Isotonic fit of y. 
 
    References 
    ---------- 
    &quot;Active set algorithms for isotonic regression; A unifying framework&quot; 
    by Michael J. Best and Nilotpal Chakravarti, section 3. 
    &quot;&quot;&quot;</span>
    <span class="s1">order = np.s_[:] </span><span class="s2">if </span><span class="s1">increasing </span><span class="s2">else </span><span class="s1">np.s_[::-</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">y = check_array(y</span><span class="s2">, </span><span class="s1">ensure_2d=</span><span class="s2">False, </span><span class="s1">input_name=</span><span class="s3">&quot;y&quot;</span><span class="s2">, </span><span class="s1">dtype=[np.float64</span><span class="s2">, </span><span class="s1">np.float32])</span>
    <span class="s1">y = np.array(y[order]</span><span class="s2">, </span><span class="s1">dtype=y.dtype)</span>
    <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">dtype=y.dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">sample_weight = np.ascontiguousarray(sample_weight[order])</span>

    <span class="s1">_inplace_contiguous_isotonic_regression(y</span><span class="s2">, </span><span class="s1">sample_weight)</span>
    <span class="s2">if </span><span class="s1">y_min </span><span class="s2">is not None or </span><span class="s1">y_max </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s0"># Older versions of np.clip don't accept None as a bound, so use np.inf</span>
        <span class="s2">if </span><span class="s1">y_min </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">y_min = -np.inf</span>
        <span class="s2">if </span><span class="s1">y_max </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">y_max = np.inf</span>
        <span class="s1">np.clip(y</span><span class="s2">, </span><span class="s1">y_min</span><span class="s2">, </span><span class="s1">y_max</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">return </span><span class="s1">y[order]</span>


<span class="s2">class </span><span class="s1">IsotonicRegression(RegressorMixin</span><span class="s2">, </span><span class="s1">TransformerMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
    <span class="s4">&quot;&quot;&quot;Isotonic regression model. 
 
    Read more in the :ref:`User Guide &lt;isotonic&gt;`. 
 
    .. versionadded:: 0.13 
 
    Parameters 
    ---------- 
    y_min : float, default=None 
        Lower bound on the lowest predicted value (the minimum value may 
        still be higher). If not set, defaults to -inf. 
 
    y_max : float, default=None 
        Upper bound on the highest predicted value (the maximum may still be 
        lower). If not set, defaults to +inf. 
 
    increasing : bool or 'auto', default=True 
        Determines whether the predictions should be constrained to increase 
        or decrease with `X`. 'auto' will decide based on the Spearman 
        correlation estimate's sign. 
 
    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan' 
        Handles how `X` values outside of the training domain are handled 
        during prediction. 
 
        - 'nan', predictions will be NaN. 
        - 'clip', predictions will be set to the value corresponding to 
          the nearest train interval endpoint. 
        - 'raise', a `ValueError` is raised. 
 
    Attributes 
    ---------- 
    X_min_ : float 
        Minimum value of input array `X_` for left bound. 
 
    X_max_ : float 
        Maximum value of input array `X_` for right bound. 
 
    X_thresholds_ : ndarray of shape (n_thresholds,) 
        Unique ascending `X` values used to interpolate 
        the y = f(X) monotonic function. 
 
        .. versionadded:: 0.24 
 
    y_thresholds_ : ndarray of shape (n_thresholds,) 
        De-duplicated `y` values suitable to interpolate the y = f(X) 
        monotonic function. 
 
        .. versionadded:: 0.24 
 
    f_ : function 
        The stepwise interpolating function that covers the input domain ``X``. 
 
    increasing_ : bool 
        Inferred value for ``increasing``. 
 
    See Also 
    -------- 
    sklearn.linear_model.LinearRegression : Ordinary least squares Linear 
        Regression. 
    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that 
        is a non-parametric model accepting monotonicity constraints. 
    isotonic_regression : Function to solve the isotonic regression model. 
 
    Notes 
    ----- 
    Ties are broken using the secondary method from de Leeuw, 1977. 
 
    References 
    ---------- 
    Isotonic Median Regression: A Linear Programming Approach 
    Nilotpal Chakravarti 
    Mathematics of Operations Research 
    Vol. 14, No. 2 (May, 1989), pp. 303-308 
 
    Isotone Optimization in R : Pool-Adjacent-Violators 
    Algorithm (PAVA) and Active Set Methods 
    de Leeuw, Hornik, Mair 
    Journal of Statistical Software 2009 
 
    Correctness of Kruskal's algorithms for monotone regression with ties 
    de Leeuw, Psychometrica, 1977 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import make_regression 
    &gt;&gt;&gt; from sklearn.isotonic import IsotonicRegression 
    &gt;&gt;&gt; X, y = make_regression(n_samples=10, n_features=1, random_state=41) 
    &gt;&gt;&gt; iso_reg = IsotonicRegression().fit(X, y) 
    &gt;&gt;&gt; iso_reg.predict([.1, .2]) 
    array([1.8628..., 3.7256...]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s3">&quot;y_min&quot;</span><span class="s1">: [Interval(Real</span><span class="s2">, None, None, </span><span class="s1">closed=</span><span class="s3">&quot;both&quot;</span><span class="s1">)</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;y_max&quot;</span><span class="s1">: [Interval(Real</span><span class="s2">, None, None, </span><span class="s1">closed=</span><span class="s3">&quot;both&quot;</span><span class="s1">)</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;increasing&quot;</span><span class="s1">: [</span><span class="s3">&quot;boolean&quot;</span><span class="s2">, </span><span class="s1">StrOptions({</span><span class="s3">&quot;auto&quot;</span><span class="s1">})]</span><span class="s2">,</span>
        <span class="s3">&quot;out_of_bounds&quot;</span><span class="s1">: [StrOptions({</span><span class="s3">&quot;nan&quot;</span><span class="s2">, </span><span class="s3">&quot;clip&quot;</span><span class="s2">, </span><span class="s3">&quot;raise&quot;</span><span class="s1">})]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">y_min=</span><span class="s2">None, </span><span class="s1">y_max=</span><span class="s2">None, </span><span class="s1">increasing=</span><span class="s2">True, </span><span class="s1">out_of_bounds=</span><span class="s3">&quot;nan&quot;</span><span class="s1">):</span>
        <span class="s1">self.y_min = y_min</span>
        <span class="s1">self.y_max = y_max</span>
        <span class="s1">self.increasing = increasing</span>
        <span class="s1">self.out_of_bounds = out_of_bounds</span>

    <span class="s2">def </span><span class="s1">_check_input_data_shape(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s2">if not </span><span class="s1">(X.ndim == </span><span class="s5">1 </span><span class="s2">or </span><span class="s1">(X.ndim == </span><span class="s5">2 </span><span class="s2">and </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">1</span><span class="s1">)):</span>
            <span class="s1">msg = (</span>
                <span class="s3">&quot;Isotonic regression input X should be a 1d array or &quot;</span>
                <span class="s3">&quot;2d array with 1 feature&quot;</span>
            <span class="s1">)</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg)</span>

    <span class="s2">def </span><span class="s1">_build_f(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s4">&quot;&quot;&quot;Build the f_ interp1d function.&quot;&quot;&quot;</span>

        <span class="s1">bounds_error = self.out_of_bounds == </span><span class="s3">&quot;raise&quot;</span>
        <span class="s2">if </span><span class="s1">len(y) == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s0"># single y, constant prediction</span>
            <span class="s1">self.f_ = </span><span class="s2">lambda </span><span class="s1">x: y.repeat(x.shape)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self.f_ = interpolate.interp1d(</span>
                <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">kind=</span><span class="s3">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">bounds_error=bounds_error</span>
            <span class="s1">)</span>

    <span class="s2">def </span><span class="s1">_build_y(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">trim_duplicates=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s4">&quot;&quot;&quot;Build the y_ IsotonicRegression.&quot;&quot;&quot;</span>
        <span class="s1">self._check_input_data_shape(X)</span>
        <span class="s1">X = X.reshape(-</span><span class="s5">1</span><span class="s1">)  </span><span class="s0"># use 1d view</span>

        <span class="s0"># Determine increasing if auto-determination requested</span>
        <span class="s2">if </span><span class="s1">self.increasing == </span><span class="s3">&quot;auto&quot;</span><span class="s1">:</span>
            <span class="s1">self.increasing_ = check_increasing(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self.increasing_ = self.increasing</span>

        <span class="s0"># If sample_weights is passed, removed zero-weight values and clean</span>
        <span class="s0"># order</span>
        <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s1">mask = sample_weight &gt; </span><span class="s5">0</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight = X[mask]</span><span class="s2">, </span><span class="s1">y[mask]</span><span class="s2">, </span><span class="s1">sample_weight[mask]</span>

        <span class="s1">order = np.lexsort((y</span><span class="s2">, </span><span class="s1">X))</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight = [array[order] </span><span class="s2">for </span><span class="s1">array </span><span class="s2">in </span><span class="s1">[X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight]]</span>
        <span class="s1">unique_X</span><span class="s2">, </span><span class="s1">unique_y</span><span class="s2">, </span><span class="s1">unique_sample_weight = _make_unique(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight)</span>

        <span class="s1">X = unique_X</span>
        <span class="s1">y = isotonic_regression(</span>
            <span class="s1">unique_y</span><span class="s2">,</span>
            <span class="s1">sample_weight=unique_sample_weight</span><span class="s2">,</span>
            <span class="s1">y_min=self.y_min</span><span class="s2">,</span>
            <span class="s1">y_max=self.y_max</span><span class="s2">,</span>
            <span class="s1">increasing=self.increasing_</span><span class="s2">,</span>
        <span class="s1">)</span>

        <span class="s0"># Handle the left and right bounds on X</span>
        <span class="s1">self.X_min_</span><span class="s2">, </span><span class="s1">self.X_max_ = np.min(X)</span><span class="s2">, </span><span class="s1">np.max(X)</span>

        <span class="s2">if </span><span class="s1">trim_duplicates:</span>
            <span class="s0"># Remove unnecessary points for faster prediction</span>
            <span class="s1">keep_data = np.ones((len(y)</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype=bool)</span>
            <span class="s0"># Aside from the 1st and last point, remove points whose y values</span>
            <span class="s0"># are equal to both the point before and the point after it.</span>
            <span class="s1">keep_data[</span><span class="s5">1</span><span class="s1">:-</span><span class="s5">1</span><span class="s1">] = np.logical_or(</span>
                <span class="s1">np.not_equal(y[</span><span class="s5">1</span><span class="s1">:-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y[:-</span><span class="s5">2</span><span class="s1">])</span><span class="s2">, </span><span class="s1">np.not_equal(y[</span><span class="s5">1</span><span class="s1">:-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y[</span><span class="s5">2</span><span class="s1">:])</span>
            <span class="s1">)</span>
            <span class="s2">return </span><span class="s1">X[keep_data]</span><span class="s2">, </span><span class="s1">y[keep_data]</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s0"># The ability to turn off trim_duplicates is only used to it make</span>
            <span class="s0"># easier to unit test that removing duplicates in y does not have</span>
            <span class="s0"># any impact the resulting interpolation function (besides</span>
            <span class="s0"># prediction speed).</span>
            <span class="s2">return </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s4">&quot;&quot;&quot;Fit the model using X, y as training data. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples,) or (n_samples, 1) 
            Training data. 
 
            .. versionchanged:: 0.24 
               Also accepts 2d array with 1 feature. 
 
        y : array-like of shape (n_samples,) 
            Training target. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Weights. If set to None, all weights will be set to 1 (equal 
            weights). 
 
        Returns 
        ------- 
        self : object 
            Returns an instance of self. 
 
        Notes 
        ----- 
        X is stored for future use, as :meth:`transform` needs X to interpolate 
        new input data. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_params = dict(accept_sparse=</span><span class="s2">False, </span><span class="s1">ensure_2d=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">X = check_array(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">input_name=</span><span class="s3">&quot;X&quot;</span><span class="s2">, </span><span class="s1">dtype=[np.float64</span><span class="s2">, </span><span class="s1">np.float32]</span><span class="s2">, </span><span class="s1">**check_params</span>
        <span class="s1">)</span>
        <span class="s1">y = check_array(y</span><span class="s2">, </span><span class="s1">input_name=</span><span class="s3">&quot;y&quot;</span><span class="s2">, </span><span class="s1">dtype=X.dtype</span><span class="s2">, </span><span class="s1">**check_params)</span>
        <span class="s1">check_consistent_length(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight)</span>

        <span class="s0"># Transform y by running the isotonic regression algorithm and</span>
        <span class="s0"># transform X accordingly.</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._build_y(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight)</span>

        <span class="s0"># It is necessary to store the non-redundant part of the training set</span>
        <span class="s0"># on the model to make it possible to support model persistence via</span>
        <span class="s0"># the pickle module as the object built by scipy.interp1d is not</span>
        <span class="s0"># picklable directly.</span>
        <span class="s1">self.X_thresholds_</span><span class="s2">, </span><span class="s1">self.y_thresholds_ = X</span><span class="s2">, </span><span class="s1">y</span>

        <span class="s0"># Build the interpolation function</span>
        <span class="s1">self._build_f(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">_transform(self</span><span class="s2">, </span><span class="s1">T):</span>
        <span class="s4">&quot;&quot;&quot;`_transform` is called by both `transform` and `predict` methods. 
 
        Since `transform` is wrapped to output arrays of specific types (e.g. 
        NumPy arrays, pandas DataFrame), we cannot make `predict` call `transform` 
        directly. 
 
        The above behaviour could be changed in the future, if we decide to output 
        other type of arrays when calling `predict`. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s3">&quot;X_thresholds_&quot;</span><span class="s1">):</span>
            <span class="s1">dtype = self.X_thresholds_.dtype</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">dtype = np.float64</span>

        <span class="s1">T = check_array(T</span><span class="s2">, </span><span class="s1">dtype=dtype</span><span class="s2">, </span><span class="s1">ensure_2d=</span><span class="s2">False</span><span class="s1">)</span>

        <span class="s1">self._check_input_data_shape(T)</span>
        <span class="s1">T = T.reshape(-</span><span class="s5">1</span><span class="s1">)  </span><span class="s0"># use 1d view</span>

        <span class="s2">if </span><span class="s1">self.out_of_bounds == </span><span class="s3">&quot;clip&quot;</span><span class="s1">:</span>
            <span class="s1">T = np.clip(T</span><span class="s2">, </span><span class="s1">self.X_min_</span><span class="s2">, </span><span class="s1">self.X_max_)</span>

        <span class="s1">res = self.f_(T)</span>

        <span class="s0"># on scipy 0.17, interp1d up-casts to float64, so we cast back</span>
        <span class="s1">res = res.astype(T.dtype)</span>

        <span class="s2">return </span><span class="s1">res</span>

    <span class="s2">def </span><span class="s1">transform(self</span><span class="s2">, </span><span class="s1">T):</span>
        <span class="s4">&quot;&quot;&quot;Transform new data by linear interpolation. 
 
        Parameters 
        ---------- 
        T : array-like of shape (n_samples,) or (n_samples, 1) 
            Data to transform. 
 
            .. versionchanged:: 0.24 
               Also accepts 2d array with 1 feature. 
 
        Returns 
        ------- 
        y_pred : ndarray of shape (n_samples,) 
            The transformed data. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self._transform(T)</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">T):</span>
        <span class="s4">&quot;&quot;&quot;Predict new data by linear interpolation. 
 
        Parameters 
        ---------- 
        T : array-like of shape (n_samples,) or (n_samples, 1) 
            Data to transform. 
 
        Returns 
        ------- 
        y_pred : ndarray of shape (n_samples,) 
            Transformed data. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self._transform(T)</span>

    <span class="s0"># We implement get_feature_names_out here instead of using</span>
    <span class="s0"># `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.</span>
    <span class="s0"># `input_features` are ignored because `IsotonicRegression` accepts 1d</span>
    <span class="s0"># arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.</span>
    <span class="s2">def </span><span class="s1">get_feature_names_out(self</span><span class="s2">, </span><span class="s1">input_features=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s4">&quot;&quot;&quot;Get output feature names for transformation. 
 
        Parameters 
        ---------- 
        input_features : array-like of str or None, default=None 
            Ignored. 
 
        Returns 
        ------- 
        feature_names_out : ndarray of str objects 
            An ndarray with one string i.e. [&quot;isotonicregression0&quot;]. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self</span><span class="s2">, </span><span class="s3">&quot;f_&quot;</span><span class="s1">)</span>
        <span class="s1">class_name = self.__class__.__name__.lower()</span>
        <span class="s2">return </span><span class="s1">np.asarray([</span><span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">class_name</span><span class="s2">}</span><span class="s3">0&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=object)</span>

    <span class="s2">def </span><span class="s1">__getstate__(self):</span>
        <span class="s4">&quot;&quot;&quot;Pickle-protocol - return state of the estimator.&quot;&quot;&quot;</span>
        <span class="s1">state = super().__getstate__()</span>
        <span class="s0"># remove interpolation method</span>
        <span class="s1">state.pop(</span><span class="s3">&quot;f_&quot;</span><span class="s2">, None</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">state</span>

    <span class="s2">def </span><span class="s1">__setstate__(self</span><span class="s2">, </span><span class="s1">state):</span>
        <span class="s4">&quot;&quot;&quot;Pickle-protocol - set state of the estimator. 
 
        We need to rebuild the interpolation function. 
        &quot;&quot;&quot;</span>
        <span class="s1">super().__setstate__(state)</span>
        <span class="s2">if </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s3">&quot;X_thresholds_&quot;</span><span class="s1">) </span><span class="s2">and </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s3">&quot;y_thresholds_&quot;</span><span class="s1">):</span>
            <span class="s1">self._build_f(self.X_thresholds_</span><span class="s2">, </span><span class="s1">self.y_thresholds_)</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s3">&quot;X_types&quot;</span><span class="s1">: [</span><span class="s3">&quot;1darray&quot;</span><span class="s1">]}</span>
</pre>
</body>
</html>