<html>
<head>
<title>_multicomp.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #6a8759;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_multicomp.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">__future__ </span><span class="s0">import </span><span class="s1">annotations</span>

<span class="s0">import </span><span class="s1">warnings</span>
<span class="s0">from </span><span class="s1">dataclasses </span><span class="s0">import </span><span class="s1">dataclass</span><span class="s0">, </span><span class="s1">field</span>
<span class="s0">from </span><span class="s1">typing </span><span class="s0">import </span><span class="s1">TYPE_CHECKING</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>

<span class="s0">from </span><span class="s1">scipy </span><span class="s0">import </span><span class="s1">stats</span>
<span class="s0">from </span><span class="s1">scipy.optimize </span><span class="s0">import </span><span class="s1">minimize_scalar</span>
<span class="s0">from </span><span class="s1">scipy.stats._common </span><span class="s0">import </span><span class="s1">ConfidenceInterval</span>
<span class="s0">from </span><span class="s1">scipy.stats._qmc </span><span class="s0">import </span><span class="s1">check_random_state</span>
<span class="s0">from </span><span class="s1">scipy.stats._stats_py </span><span class="s0">import </span><span class="s1">_var</span>

<span class="s0">if </span><span class="s1">TYPE_CHECKING:</span>
    <span class="s0">import </span><span class="s1">numpy.typing </span><span class="s0">as </span><span class="s1">npt</span>
    <span class="s0">from </span><span class="s1">scipy._lib._util </span><span class="s0">import </span><span class="s1">DecimalNumber</span><span class="s0">, </span><span class="s1">SeedType</span>
    <span class="s0">from </span><span class="s1">typing </span><span class="s0">import </span><span class="s1">Literal</span><span class="s0">, </span><span class="s1">Sequence  </span><span class="s2"># noqa: UP035</span>


<span class="s1">__all__ = [</span>
    <span class="s3">'dunnett'</span>
<span class="s1">]</span>


<span class="s1">@dataclass</span>
<span class="s0">class </span><span class="s1">DunnettResult:</span>
    <span class="s4">&quot;&quot;&quot;Result object returned by `scipy.stats.dunnett`. 
 
    Attributes 
    ---------- 
    statistic : float ndarray 
        The computed statistic of the test for each comparison. The element 
        at index ``i`` is the statistic for the comparison between 
        groups ``i`` and the control. 
    pvalue : float ndarray 
        The computed p-value of the test for each comparison. The element 
        at index ``i`` is the p-value for the comparison between 
        group ``i`` and the control. 
    &quot;&quot;&quot;</span>
    <span class="s1">statistic: np.ndarray</span>
    <span class="s1">pvalue: np.ndarray</span>
    <span class="s1">_alternative: Literal[</span><span class="s3">'two-sided'</span><span class="s0">, </span><span class="s3">'less'</span><span class="s0">, </span><span class="s3">'greater'</span><span class="s1">] = field(repr=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">_rho: np.ndarray = field(repr=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">_df: int = field(repr=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">_std: float = field(repr=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">_mean_samples: np.ndarray = field(repr=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">_mean_control: np.ndarray = field(repr=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">_n_samples: np.ndarray = field(repr=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">_n_control: int = field(repr=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">_rng: SeedType = field(repr=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">_ci: ConfidenceInterval | </span><span class="s0">None </span><span class="s1">= field(default=</span><span class="s0">None, </span><span class="s1">repr=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">_ci_cl: DecimalNumber | </span><span class="s0">None </span><span class="s1">= field(default=</span><span class="s0">None, </span><span class="s1">repr=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">__str__(self):</span>
        <span class="s2"># Note: `__str__` prints the confidence intervals from the most</span>
        <span class="s2"># recent call to `confidence_interval`. If it has not been called,</span>
        <span class="s2"># it will be called with the default CL of .95.</span>
        <span class="s0">if </span><span class="s1">self._ci </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">self.confidence_interval(confidence_level=</span><span class="s5">.95</span><span class="s1">)</span>
        <span class="s1">s = (</span>
            <span class="s3">&quot;Dunnett's test&quot;</span>
            <span class="s3">f&quot; (</span><span class="s0">{</span><span class="s1">self._ci_cl*</span><span class="s5">100</span><span class="s0">:</span><span class="s3">.1f</span><span class="s0">}</span><span class="s3">% Confidence Interval)</span><span class="s0">\n</span><span class="s3">&quot;</span>
            <span class="s3">&quot;Comparison               Statistic  p-value  Lower CI  Upper CI</span><span class="s0">\n</span><span class="s3">&quot;</span>
        <span class="s1">)</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(self.pvalue.size):</span>
            <span class="s1">s += (</span><span class="s3">f&quot; (Sample </span><span class="s0">{</span><span class="s1">i</span><span class="s0">} </span><span class="s3">- Control) </span><span class="s0">{</span><span class="s1">self.statistic[i]</span><span class="s0">:</span><span class="s3">&gt;10.3f</span><span class="s0">}</span><span class="s3">&quot;</span>
                  <span class="s3">f&quot;</span><span class="s0">{</span><span class="s1">self.pvalue[i]</span><span class="s0">:</span><span class="s3">&gt;10.3f</span><span class="s0">}</span><span class="s3">&quot;</span>
                  <span class="s3">f&quot;</span><span class="s0">{</span><span class="s1">self._ci.low[i]</span><span class="s0">:</span><span class="s3">&gt;10.3f</span><span class="s0">}</span><span class="s3">&quot;</span>
                  <span class="s3">f&quot;</span><span class="s0">{</span><span class="s1">self._ci.high[i]</span><span class="s0">:</span><span class="s3">&gt;10.3f</span><span class="s0">}\n</span><span class="s3">&quot;</span><span class="s1">)</span>

        <span class="s0">return </span><span class="s1">s</span>

    <span class="s0">def </span><span class="s1">_allowance(</span>
        <span class="s1">self</span><span class="s0">, </span><span class="s1">confidence_level: DecimalNumber = </span><span class="s5">0.95</span><span class="s0">, </span><span class="s1">tol: DecimalNumber = </span><span class="s5">1e-3</span>
    <span class="s1">) -&gt; float:</span>
        <span class="s4">&quot;&quot;&quot;Allowance. 
 
        It is the quantity to add/subtract from the observed difference 
        between the means of observed groups and the mean of the control 
        group. The result gives confidence limits. 
 
        Parameters 
        ---------- 
        confidence_level : float, optional 
            Confidence level for the computed confidence interval. 
            Default is .95. 
        tol : float, optional 
            A tolerance for numerical optimization: the allowance will produce 
            a confidence within ``10*tol*(1 - confidence_level)`` of the 
            specified level, or a warning will be emitted. Tight tolerances 
            may be impractical due to noisy evaluation of the objective. 
            Default is 1e-3. 
 
        Returns 
        ------- 
        allowance : float 
            Allowance around the mean. 
        &quot;&quot;&quot;</span>
        <span class="s1">alpha = </span><span class="s5">1 </span><span class="s1">- confidence_level</span>

        <span class="s0">def </span><span class="s1">pvalue_from_stat(statistic):</span>
            <span class="s1">statistic = np.array(statistic)</span>
            <span class="s1">sf = _pvalue_dunnett(</span>
                <span class="s1">rho=self._rho</span><span class="s0">, </span><span class="s1">df=self._df</span><span class="s0">,</span>
                <span class="s1">statistic=statistic</span><span class="s0">, </span><span class="s1">alternative=self._alternative</span><span class="s0">,</span>
                <span class="s1">rng=self._rng</span>
            <span class="s1">)</span>
            <span class="s0">return </span><span class="s1">abs(sf - alpha)/alpha</span>

        <span class="s2"># Evaluation of `pvalue_from_stat` is noisy due to the use of RQMC to</span>
        <span class="s2"># evaluate `multivariate_t.cdf`. `minimize_scalar` is not designed</span>
        <span class="s2"># to tolerate a noisy objective function and may fail to find the</span>
        <span class="s2"># minimum accurately. We mitigate this possibility with the validation</span>
        <span class="s2"># step below, but implementation of a noise-tolerant root finder or</span>
        <span class="s2"># minimizer would be a welcome enhancement. See gh-18150.</span>
        <span class="s1">res = minimize_scalar(pvalue_from_stat</span><span class="s0">, </span><span class="s1">method=</span><span class="s3">'brent'</span><span class="s0">, </span><span class="s1">tol=tol)</span>
        <span class="s1">critical_value = res.x</span>

        <span class="s2"># validation</span>
        <span class="s2"># tol*10 because tol=1e-3 means we tolerate a 1% change at most</span>
        <span class="s0">if </span><span class="s1">res.success </span><span class="s0">is False or </span><span class="s1">res.fun &gt;= tol*</span><span class="s5">10</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s3">&quot;Computation of the confidence interval did not converge to &quot;</span>
                <span class="s3">&quot;the desired level. The confidence level corresponding with &quot;</span>
                <span class="s3">f&quot;the returned interval is approximately </span><span class="s0">{</span><span class="s1">alpha*(</span><span class="s5">1</span><span class="s1">+res.fun)</span><span class="s0">}</span><span class="s3">.&quot;</span><span class="s0">,</span>
                <span class="s1">stacklevel=</span><span class="s5">3</span>
            <span class="s1">)</span>

        <span class="s2"># From [1] p. 1101 between (1) and (3)</span>
        <span class="s1">allowance = critical_value*self._std*np.sqrt(</span>
            <span class="s5">1</span><span class="s1">/self._n_samples + </span><span class="s5">1</span><span class="s1">/self._n_control</span>
        <span class="s1">)</span>
        <span class="s0">return </span><span class="s1">abs(allowance)</span>

    <span class="s0">def </span><span class="s1">confidence_interval(</span>
        <span class="s1">self</span><span class="s0">, </span><span class="s1">confidence_level: DecimalNumber = </span><span class="s5">0.95</span>
    <span class="s1">) -&gt; ConfidenceInterval:</span>
        <span class="s4">&quot;&quot;&quot;Compute the confidence interval for the specified confidence level. 
 
        Parameters 
        ---------- 
        confidence_level : float, optional 
            Confidence level for the computed confidence interval. 
            Default is .95. 
 
        Returns 
        ------- 
        ci : ``ConfidenceInterval`` object 
            The object has attributes ``low`` and ``high`` that hold the 
            lower and upper bounds of the confidence intervals for each 
            comparison. The high and low values are accessible for each 
            comparison at index ``i`` for each group ``i``. 
 
        &quot;&quot;&quot;</span>
        <span class="s2"># check to see if the supplied confidence level matches that of the</span>
        <span class="s2"># previously computed CI.</span>
        <span class="s0">if </span><span class="s1">(self._ci </span><span class="s0">is not None</span><span class="s1">) </span><span class="s0">and </span><span class="s1">(confidence_level == self._ci_cl):</span>
            <span class="s0">return </span><span class="s1">self._ci</span>

        <span class="s0">if not </span><span class="s1">(</span><span class="s5">0 </span><span class="s1">&lt; confidence_level &lt; </span><span class="s5">1</span><span class="s1">):</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Confidence level must be between 0 and 1.&quot;</span><span class="s1">)</span>

        <span class="s1">allowance = self._allowance(confidence_level=confidence_level)</span>
        <span class="s1">diff_means = self._mean_samples - self._mean_control</span>

        <span class="s1">low = diff_means-allowance</span>
        <span class="s1">high = diff_means+allowance</span>

        <span class="s0">if </span><span class="s1">self._alternative == </span><span class="s3">'greater'</span><span class="s1">:</span>
            <span class="s1">high = [np.inf] * len(diff_means)</span>
        <span class="s0">elif </span><span class="s1">self._alternative == </span><span class="s3">'less'</span><span class="s1">:</span>
            <span class="s1">low = [-np.inf] * len(diff_means)</span>

        <span class="s1">self._ci_cl = confidence_level</span>
        <span class="s1">self._ci = ConfidenceInterval(</span>
            <span class="s1">low=low</span><span class="s0">,</span>
            <span class="s1">high=high</span>
        <span class="s1">)</span>
        <span class="s0">return </span><span class="s1">self._ci</span>


<span class="s0">def </span><span class="s1">dunnett(</span>
    <span class="s1">*samples: npt.ArrayLike</span><span class="s0">,  </span><span class="s2"># noqa: D417</span>
    <span class="s1">control: npt.ArrayLike</span><span class="s0">,</span>
    <span class="s1">alternative: Literal[</span><span class="s3">'two-sided'</span><span class="s0">, </span><span class="s3">'less'</span><span class="s0">, </span><span class="s3">'greater'</span><span class="s1">] = </span><span class="s3">&quot;two-sided&quot;</span><span class="s0">,</span>
    <span class="s1">random_state: SeedType = </span><span class="s0">None</span>
<span class="s1">) -&gt; DunnettResult:</span>
    <span class="s4">&quot;&quot;&quot;Dunnett's test: multiple comparisons of means against a control group. 
 
    This is an implementation of Dunnett's original, single-step test as 
    described in [1]_. 
 
    Parameters 
    ---------- 
    sample1, sample2, ... : 1D array_like 
        The sample measurements for each experimental group. 
    control : 1D array_like 
        The sample measurements for the control group. 
    alternative : {'two-sided', 'less', 'greater'}, optional 
        Defines the alternative hypothesis. 
 
        The null hypothesis is that the means of the distributions underlying 
        the samples and control are equal. The following alternative 
        hypotheses are available (default is 'two-sided'): 
 
        * 'two-sided': the means of the distributions underlying the samples 
          and control are unequal. 
        * 'less': the means of the distributions underlying the samples 
          are less than the mean of the distribution underlying the control. 
        * 'greater': the means of the distributions underlying the 
          samples are greater than the mean of the distribution underlying 
          the control. 
    random_state : {None, int, `numpy.random.Generator`}, optional 
        If `random_state` is an int or None, a new `numpy.random.Generator` is 
        created using ``np.random.default_rng(random_state)``. 
        If `random_state` is already a ``Generator`` instance, then the 
        provided instance is used. 
 
        The random number generator is used to control the randomized 
        Quasi-Monte Carlo integration of the multivariate-t distribution. 
 
    Returns 
    ------- 
    res : `~scipy.stats._result_classes.DunnettResult` 
        An object containing attributes: 
 
        statistic : float ndarray 
            The computed statistic of the test for each comparison. The element 
            at index ``i`` is the statistic for the comparison between 
            groups ``i`` and the control. 
        pvalue : float ndarray 
            The computed p-value of the test for each comparison. The element 
            at index ``i`` is the p-value for the comparison between 
            group ``i`` and the control. 
 
        And the following method: 
 
        confidence_interval(confidence_level=0.95) : 
            Compute the difference in means of the groups 
            with the control +- the allowance. 
 
    See Also 
    -------- 
    tukey_hsd : performs pairwise comparison of means. 
 
    Notes 
    ----- 
    Like the independent-sample t-test, Dunnett's test [1]_ is used to make 
    inferences about the means of distributions from which samples were drawn. 
    However, when multiple t-tests are performed at a fixed significance level, 
    the &quot;family-wise error rate&quot; - the probability of incorrectly rejecting the 
    null hypothesis in at least one test - will exceed the significance level. 
    Dunnett's test is designed to perform multiple comparisons while 
    controlling the family-wise error rate. 
 
    Dunnett's test compares the means of multiple experimental groups 
    against a single control group. Tukey's Honestly Significant Difference Test 
    is another multiple-comparison test that controls the family-wise error 
    rate, but `tukey_hsd` performs *all* pairwise comparisons between groups. 
    When pairwise comparisons between experimental groups are not needed, 
    Dunnett's test is preferable due to its higher power. 
 
 
    The use of this test relies on several assumptions. 
 
    1. The observations are independent within and among groups. 
    2. The observations within each group are normally distributed. 
    3. The distributions from which the samples are drawn have the same finite 
       variance. 
 
    References 
    ---------- 
    .. [1] Charles W. Dunnett. &quot;A Multiple Comparison Procedure for Comparing 
       Several Treatments with a Control.&quot; 
       Journal of the American Statistical Association, 50:272, 1096-1121, 
       :doi:`10.1080/01621459.1955.10501294`, 1955. 
 
    Examples 
    -------- 
    In [1]_, the influence of drugs on blood count measurements on three groups 
    of animal is investigated. 
 
    The following table summarizes the results of the experiment in which 
    two groups received different drugs, and one group acted as a control. 
    Blood counts (in millions of cells per cubic millimeter) were recorded:: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; control = np.array([7.40, 8.50, 7.20, 8.24, 9.84, 8.32]) 
    &gt;&gt;&gt; drug_a = np.array([9.76, 8.80, 7.68, 9.36]) 
    &gt;&gt;&gt; drug_b = np.array([12.80, 9.68, 12.16, 9.20, 10.55]) 
 
    We would like to see if the means between any of the groups are 
    significantly different. First, visually examine a box and whisker plot. 
 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; fig, ax = plt.subplots(1, 1) 
    &gt;&gt;&gt; ax.boxplot([control, drug_a, drug_b]) 
    &gt;&gt;&gt; ax.set_xticklabels([&quot;Control&quot;, &quot;Drug A&quot;, &quot;Drug B&quot;])  # doctest: +SKIP 
    &gt;&gt;&gt; ax.set_ylabel(&quot;mean&quot;)  # doctest: +SKIP 
    &gt;&gt;&gt; plt.show() 
 
    Note the overlapping interquartile ranges of the drug A group and control 
    group and the apparent separation between the drug B group and control 
    group. 
 
    Next, we will use Dunnett's test to assess whether the difference 
    between group means is significant while controlling the family-wise error 
    rate: the probability of making any false discoveries. 
    Let the null hypothesis be that the experimental groups have the same 
    mean as the control and the alternative be that an experimental group does 
    not have the same mean as the control. We will consider a 5% family-wise 
    error rate to be acceptable, and therefore we choose 0.05 as the threshold 
    for significance. 
 
    &gt;&gt;&gt; from scipy.stats import dunnett 
    &gt;&gt;&gt; res = dunnett(drug_a, drug_b, control=control) 
    &gt;&gt;&gt; res.pvalue 
    array([0.62004941, 0.0059035 ])  # may vary 
 
    The p-value corresponding with the comparison between group A and control 
    exceeds 0.05, so we do not reject the null hypothesis for that comparison. 
    However, the p-value corresponding with the comparison between group B 
    and control is less than 0.05, so we consider the experimental results 
    to be evidence against the null hypothesis in favor of the alternative: 
    group B has a different mean than the control group. 
 
    &quot;&quot;&quot;</span>
    <span class="s1">samples_</span><span class="s0">, </span><span class="s1">control_</span><span class="s0">, </span><span class="s1">rng = _iv_dunnett(</span>
        <span class="s1">samples=samples</span><span class="s0">, </span><span class="s1">control=control</span><span class="s0">,</span>
        <span class="s1">alternative=alternative</span><span class="s0">, </span><span class="s1">random_state=random_state</span>
    <span class="s1">)</span>

    <span class="s1">rho</span><span class="s0">, </span><span class="s1">df</span><span class="s0">, </span><span class="s1">n_group</span><span class="s0">, </span><span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_control = _params_dunnett(</span>
        <span class="s1">samples=samples_</span><span class="s0">, </span><span class="s1">control=control_</span>
    <span class="s1">)</span>

    <span class="s1">statistic</span><span class="s0">, </span><span class="s1">std</span><span class="s0">, </span><span class="s1">mean_control</span><span class="s0">, </span><span class="s1">mean_samples = _statistic_dunnett(</span>
        <span class="s1">samples_</span><span class="s0">, </span><span class="s1">control_</span><span class="s0">, </span><span class="s1">df</span><span class="s0">, </span><span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_control</span>
    <span class="s1">)</span>

    <span class="s1">pvalue = _pvalue_dunnett(</span>
        <span class="s1">rho=rho</span><span class="s0">, </span><span class="s1">df=df</span><span class="s0">, </span><span class="s1">statistic=statistic</span><span class="s0">, </span><span class="s1">alternative=alternative</span><span class="s0">, </span><span class="s1">rng=rng</span>
    <span class="s1">)</span>

    <span class="s0">return </span><span class="s1">DunnettResult(</span>
        <span class="s1">statistic=statistic</span><span class="s0">, </span><span class="s1">pvalue=pvalue</span><span class="s0">,</span>
        <span class="s1">_alternative=alternative</span><span class="s0">,</span>
        <span class="s1">_rho=rho</span><span class="s0">, </span><span class="s1">_df=df</span><span class="s0">, </span><span class="s1">_std=std</span><span class="s0">,</span>
        <span class="s1">_mean_samples=mean_samples</span><span class="s0">,</span>
        <span class="s1">_mean_control=mean_control</span><span class="s0">,</span>
        <span class="s1">_n_samples=n_samples</span><span class="s0">,</span>
        <span class="s1">_n_control=n_control</span><span class="s0">,</span>
        <span class="s1">_rng=rng</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">_iv_dunnett(</span>
    <span class="s1">samples: Sequence[npt.ArrayLike]</span><span class="s0">,</span>
    <span class="s1">control: npt.ArrayLike</span><span class="s0">,</span>
    <span class="s1">alternative: Literal[</span><span class="s3">'two-sided'</span><span class="s0">, </span><span class="s3">'less'</span><span class="s0">, </span><span class="s3">'greater'</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">random_state: SeedType</span>
<span class="s1">) -&gt; tuple[list[np.ndarray]</span><span class="s0">, </span><span class="s1">np.ndarray</span><span class="s0">, </span><span class="s1">SeedType]:</span>
    <span class="s4">&quot;&quot;&quot;Input validation for Dunnett's test.&quot;&quot;&quot;</span>
    <span class="s1">rng = check_random_state(random_state)</span>

    <span class="s0">if </span><span class="s1">alternative </span><span class="s0">not in </span><span class="s1">{</span><span class="s3">'two-sided'</span><span class="s0">, </span><span class="s3">'less'</span><span class="s0">, </span><span class="s3">'greater'</span><span class="s1">}:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span>
            <span class="s3">&quot;alternative must be 'less', 'greater' or 'two-sided'&quot;</span>
        <span class="s1">)</span>

    <span class="s1">ndim_msg = </span><span class="s3">&quot;Control and samples groups must be 1D arrays&quot;</span>
    <span class="s1">n_obs_msg = </span><span class="s3">&quot;Control and samples groups must have at least 1 observation&quot;</span>

    <span class="s1">control = np.asarray(control)</span>
    <span class="s1">samples_ = [np.asarray(sample) </span><span class="s0">for </span><span class="s1">sample </span><span class="s0">in </span><span class="s1">samples]</span>

    <span class="s2"># samples checks</span>
    <span class="s1">samples_control: list[np.ndarray] = samples_ + [control]</span>
    <span class="s0">for </span><span class="s1">sample </span><span class="s0">in </span><span class="s1">samples_control:</span>
        <span class="s0">if </span><span class="s1">sample.ndim &gt; </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s0">raise </span><span class="s1">ValueError(ndim_msg)</span>

        <span class="s0">if </span><span class="s1">sample.size &lt; </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s0">raise </span><span class="s1">ValueError(n_obs_msg)</span>

    <span class="s0">return </span><span class="s1">samples_</span><span class="s0">, </span><span class="s1">control</span><span class="s0">, </span><span class="s1">rng</span>


<span class="s0">def </span><span class="s1">_params_dunnett(</span>
    <span class="s1">samples: list[np.ndarray]</span><span class="s0">, </span><span class="s1">control: np.ndarray</span>
<span class="s1">) -&gt; tuple[np.ndarray</span><span class="s0">, </span><span class="s1">int</span><span class="s0">, </span><span class="s1">int</span><span class="s0">, </span><span class="s1">np.ndarray</span><span class="s0">, </span><span class="s1">int]:</span>
    <span class="s4">&quot;&quot;&quot;Specific parameters for Dunnett's test. 
 
    Degree of freedom is the number of observations minus the number of groups 
    including the control. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples = np.array([sample.size </span><span class="s0">for </span><span class="s1">sample </span><span class="s0">in </span><span class="s1">samples])</span>

    <span class="s2"># From [1] p. 1100 d.f. = (sum N)-(p+1)</span>
    <span class="s1">n_sample = n_samples.sum()</span>
    <span class="s1">n_control = control.size</span>
    <span class="s1">n = n_sample + n_control</span>
    <span class="s1">n_groups = len(samples)</span>
    <span class="s1">df = n - n_groups - </span><span class="s5">1</span>

    <span class="s2"># From [1] p. 1103 rho_ij = 1/sqrt((N0/Ni+1)(N0/Nj+1))</span>
    <span class="s1">rho = n_control/n_samples + </span><span class="s5">1</span>
    <span class="s1">rho = </span><span class="s5">1</span><span class="s1">/np.sqrt(rho[:</span><span class="s0">, None</span><span class="s1">] * rho[</span><span class="s0">None, </span><span class="s1">:])</span>
    <span class="s1">np.fill_diagonal(rho</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span>

    <span class="s0">return </span><span class="s1">rho</span><span class="s0">, </span><span class="s1">df</span><span class="s0">, </span><span class="s1">n_groups</span><span class="s0">, </span><span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_control</span>


<span class="s0">def </span><span class="s1">_statistic_dunnett(</span>
    <span class="s1">samples: list[np.ndarray]</span><span class="s0">, </span><span class="s1">control: np.ndarray</span><span class="s0">, </span><span class="s1">df: int</span><span class="s0">,</span>
    <span class="s1">n_samples: np.ndarray</span><span class="s0">, </span><span class="s1">n_control: int</span>
<span class="s1">) -&gt; tuple[np.ndarray</span><span class="s0">, </span><span class="s1">float</span><span class="s0">, </span><span class="s1">np.ndarray</span><span class="s0">, </span><span class="s1">np.ndarray]:</span>
    <span class="s4">&quot;&quot;&quot;Statistic of Dunnett's test. 
 
    Computation based on the original single-step test from [1]. 
    &quot;&quot;&quot;</span>
    <span class="s1">mean_control = np.mean(control)</span>
    <span class="s1">mean_samples = np.array([np.mean(sample) </span><span class="s0">for </span><span class="s1">sample </span><span class="s0">in </span><span class="s1">samples])</span>
    <span class="s1">all_samples = [control] + samples</span>
    <span class="s1">all_means = np.concatenate([[mean_control]</span><span class="s0">, </span><span class="s1">mean_samples])</span>

    <span class="s2"># Variance estimate s^2 from [1] Eq. 1</span>
    <span class="s1">s2 = np.sum([_var(sample</span><span class="s0">, </span><span class="s1">mean=mean)*sample.size</span>
                 <span class="s0">for </span><span class="s1">sample</span><span class="s0">, </span><span class="s1">mean </span><span class="s0">in </span><span class="s1">zip(all_samples</span><span class="s0">, </span><span class="s1">all_means)]) / df</span>
    <span class="s1">std = np.sqrt(s2)</span>

    <span class="s2"># z score inferred from [1] unlabeled equation after Eq. 1</span>
    <span class="s1">z = (mean_samples - mean_control) / np.sqrt(</span><span class="s5">1</span><span class="s1">/n_samples + </span><span class="s5">1</span><span class="s1">/n_control)</span>

    <span class="s0">return </span><span class="s1">z / std</span><span class="s0">, </span><span class="s1">std</span><span class="s0">, </span><span class="s1">mean_control</span><span class="s0">, </span><span class="s1">mean_samples</span>


<span class="s0">def </span><span class="s1">_pvalue_dunnett(</span>
    <span class="s1">rho: np.ndarray</span><span class="s0">, </span><span class="s1">df: int</span><span class="s0">, </span><span class="s1">statistic: np.ndarray</span><span class="s0">,</span>
    <span class="s1">alternative: Literal[</span><span class="s3">'two-sided'</span><span class="s0">, </span><span class="s3">'less'</span><span class="s0">, </span><span class="s3">'greater'</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">rng: SeedType = </span><span class="s0">None</span>
<span class="s1">) -&gt; np.ndarray:</span>
    <span class="s4">&quot;&quot;&quot;pvalue from the multivariate t-distribution. 
 
    Critical values come from the multivariate student-t distribution. 
    &quot;&quot;&quot;</span>
    <span class="s1">statistic = statistic.reshape(-</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span>

    <span class="s1">mvt = stats.multivariate_t(shape=rho</span><span class="s0">, </span><span class="s1">df=df</span><span class="s0">, </span><span class="s1">seed=rng)</span>
    <span class="s0">if </span><span class="s1">alternative == </span><span class="s3">&quot;two-sided&quot;</span><span class="s1">:</span>
        <span class="s1">statistic = abs(statistic)</span>
        <span class="s1">pvalue = </span><span class="s5">1 </span><span class="s1">- mvt.cdf(statistic</span><span class="s0">, </span><span class="s1">lower_limit=-statistic)</span>
    <span class="s0">elif </span><span class="s1">alternative == </span><span class="s3">&quot;greater&quot;</span><span class="s1">:</span>
        <span class="s1">pvalue = </span><span class="s5">1 </span><span class="s1">- mvt.cdf(statistic</span><span class="s0">, </span><span class="s1">lower_limit=-np.inf)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">pvalue = </span><span class="s5">1 </span><span class="s1">- mvt.cdf(np.inf</span><span class="s0">, </span><span class="s1">lower_limit=statistic)</span>

    <span class="s0">return </span><span class="s1">np.atleast_1d(pvalue)</span>
</pre>
</body>
</html>