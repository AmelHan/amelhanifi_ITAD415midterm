<html>
<head>
<title>test_hdbscan.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_hdbscan.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Tests for HDBSCAN clustering algorithm 
Based on the DBSCAN test code 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">sparse</span><span class="s2">, </span><span class="s1">stats</span>
<span class="s2">from </span><span class="s1">scipy.spatial </span><span class="s2">import </span><span class="s1">distance</span>

<span class="s2">from </span><span class="s1">sklearn.cluster </span><span class="s2">import </span><span class="s1">HDBSCAN</span>
<span class="s2">from </span><span class="s1">sklearn.cluster._hdbscan._tree </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">CONDENSED_dtype</span><span class="s2">,</span>
    <span class="s1">_condense_tree</span><span class="s2">,</span>
    <span class="s1">_do_labelling</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.cluster._hdbscan.hdbscan </span><span class="s2">import </span><span class="s1">_OUTLIER_ENCODING</span>
<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">make_blobs</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">fowlkes_mallows_score</span>
<span class="s2">from </span><span class="s1">sklearn.metrics.pairwise </span><span class="s2">import </span><span class="s1">_VALID_METRICS</span><span class="s2">, </span><span class="s1">euclidean_distances</span>
<span class="s2">from </span><span class="s1">sklearn.neighbors </span><span class="s2">import </span><span class="s1">BallTree</span><span class="s2">, </span><span class="s1">KDTree</span>
<span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">StandardScaler</span>
<span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">shuffle</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">assert_allclose</span><span class="s2">, </span><span class="s1">assert_array_equal</span>

<span class="s1">n_clusters_true = </span><span class="s3">3</span>
<span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(n_samples=</span><span class="s3">200</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">10</span><span class="s1">)</span>
<span class="s1">X</span><span class="s2">, </span><span class="s1">y = shuffle(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">7</span><span class="s1">)</span>
<span class="s1">X = StandardScaler().fit_transform(X)</span>

<span class="s1">ALGORITHMS = [</span>
    <span class="s4">&quot;kdtree&quot;</span><span class="s2">,</span>
    <span class="s4">&quot;balltree&quot;</span><span class="s2">,</span>
    <span class="s4">&quot;brute&quot;</span><span class="s2">,</span>
    <span class="s4">&quot;auto&quot;</span><span class="s2">,</span>
<span class="s1">]</span>

<span class="s1">OUTLIER_SET = {-</span><span class="s3">1</span><span class="s1">} | {out[</span><span class="s4">&quot;label&quot;</span><span class="s1">] </span><span class="s2">for </span><span class="s1">_</span><span class="s2">, </span><span class="s1">out </span><span class="s2">in </span><span class="s1">_OUTLIER_ENCODING.items()}</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;outlier_type&quot;</span><span class="s2">, </span><span class="s1">_OUTLIER_ENCODING)</span>
<span class="s2">def </span><span class="s1">test_outlier_data(outlier_type):</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests if np.inf and np.nan data are each treated as special outliers. 
    &quot;&quot;&quot;</span>
    <span class="s1">outlier = {</span>
        <span class="s4">&quot;infinite&quot;</span><span class="s1">: np.inf</span><span class="s2">,</span>
        <span class="s4">&quot;missing&quot;</span><span class="s1">: np.nan</span><span class="s2">,</span>
    <span class="s1">}[outlier_type]</span>
    <span class="s1">prob_check = {</span>
        <span class="s4">&quot;infinite&quot;</span><span class="s1">: </span><span class="s2">lambda </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: x == y</span><span class="s2">,</span>
        <span class="s4">&quot;missing&quot;</span><span class="s1">: </span><span class="s2">lambda </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: np.isnan(x)</span><span class="s2">,</span>
    <span class="s1">}[outlier_type]</span>
    <span class="s1">label = _OUTLIER_ENCODING[outlier_type][</span><span class="s4">&quot;label&quot;</span><span class="s1">]</span>
    <span class="s1">prob = _OUTLIER_ENCODING[outlier_type][</span><span class="s4">&quot;prob&quot;</span><span class="s1">]</span>

    <span class="s1">X_outlier = X.copy()</span>
    <span class="s1">X_outlier[</span><span class="s3">0</span><span class="s1">] = [outlier</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">X_outlier[</span><span class="s3">5</span><span class="s1">] = [outlier</span><span class="s2">, </span><span class="s1">outlier]</span>
    <span class="s1">model = HDBSCAN().fit(X_outlier)</span>

    <span class="s1">(missing_labels_idx</span><span class="s2">,</span><span class="s1">) = (model.labels_ == label).nonzero()</span>
    <span class="s1">assert_array_equal(missing_labels_idx</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">5</span><span class="s1">])</span>

    <span class="s1">(missing_probs_idx</span><span class="s2">,</span><span class="s1">) = (prob_check(model.probabilities_</span><span class="s2">, </span><span class="s1">prob)).nonzero()</span>
    <span class="s1">assert_array_equal(missing_probs_idx</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">5</span><span class="s1">])</span>

    <span class="s1">clean_indices = list(range(</span><span class="s3">1</span><span class="s2">, </span><span class="s3">5</span><span class="s1">)) + list(range(</span><span class="s3">6</span><span class="s2">, </span><span class="s3">200</span><span class="s1">))</span>
    <span class="s1">clean_model = HDBSCAN().fit(X_outlier[clean_indices])</span>
    <span class="s1">assert_array_equal(clean_model.labels_</span><span class="s2">, </span><span class="s1">model.labels_[clean_indices])</span>


<span class="s2">def </span><span class="s1">test_hdbscan_distance_matrix():</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN works with precomputed distance matrices, and throws the 
    appropriate errors when needed. 
    &quot;&quot;&quot;</span>
    <span class="s1">D = euclidean_distances(X)</span>
    <span class="s1">D_original = D.copy()</span>
    <span class="s1">labels = HDBSCAN(metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True</span><span class="s1">).fit_predict(D)</span>

    <span class="s1">assert_allclose(D</span><span class="s2">, </span><span class="s1">D_original)</span>
    <span class="s1">n_clusters = len(set(labels) - OUTLIER_SET)</span>
    <span class="s2">assert </span><span class="s1">n_clusters == n_clusters_true</span>

    <span class="s5"># Check that clustering is arbitrarily good</span>
    <span class="s5"># This is a heuristic to guard against regression</span>
    <span class="s1">score = fowlkes_mallows_score(y</span><span class="s2">, </span><span class="s1">labels)</span>
    <span class="s2">assert </span><span class="s1">score &gt;= </span><span class="s3">0.98</span>

    <span class="s1">msg = </span><span class="s4">r&quot;The precomputed distance matrix.*has shape&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">HDBSCAN(metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True</span><span class="s1">).fit_predict(X)</span>

    <span class="s1">msg = </span><span class="s4">r&quot;The precomputed distance matrix.*values&quot;</span>
    <span class="s5"># Ensure the matrix is not symmetric</span>
    <span class="s1">D[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s1">] = </span><span class="s3">10</span>
    <span class="s1">D[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s1">] = </span><span class="s3">1</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">HDBSCAN(metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s1">).fit_predict(D)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;sparse_constructor&quot;</span><span class="s2">, </span><span class="s1">[sparse.csr_matrix</span><span class="s2">, </span><span class="s1">sparse.csc_matrix])</span>
<span class="s2">def </span><span class="s1">test_hdbscan_sparse_distance_matrix(sparse_constructor):</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN works with sparse distance matrices. 
    &quot;&quot;&quot;</span>
    <span class="s1">D = distance.squareform(distance.pdist(X))</span>
    <span class="s1">D /= np.max(D)</span>

    <span class="s1">threshold = stats.scoreatpercentile(D.flatten()</span><span class="s2">, </span><span class="s3">50</span><span class="s1">)</span>

    <span class="s1">D[D &gt;= threshold] = </span><span class="s3">0.0</span>
    <span class="s1">D = sparse_constructor(D)</span>
    <span class="s1">D.eliminate_zeros()</span>

    <span class="s1">labels = HDBSCAN(metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s1">).fit_predict(D)</span>
    <span class="s1">n_clusters = len(set(labels) - OUTLIER_SET)</span>
    <span class="s2">assert </span><span class="s1">n_clusters == n_clusters_true</span>


<span class="s2">def </span><span class="s1">test_hdbscan_feature_array():</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN works with feature array, including an arbitrary 
    goodness of fit check. Note that the check is a simple heuristic. 
    &quot;&quot;&quot;</span>
    <span class="s1">labels = HDBSCAN().fit_predict(X)</span>
    <span class="s1">n_clusters = len(set(labels) - OUTLIER_SET)</span>
    <span class="s2">assert </span><span class="s1">n_clusters == n_clusters_true</span>

    <span class="s5"># Check that clustering is arbitrarily good</span>
    <span class="s5"># This is a heuristic to guard against regression</span>
    <span class="s1">score = fowlkes_mallows_score(y</span><span class="s2">, </span><span class="s1">labels)</span>
    <span class="s2">assert </span><span class="s1">score &gt;= </span><span class="s3">0.98</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;algo&quot;</span><span class="s2">, </span><span class="s1">ALGORITHMS)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;metric&quot;</span><span class="s2">, </span><span class="s1">_VALID_METRICS)</span>
<span class="s2">def </span><span class="s1">test_hdbscan_algorithms(algo</span><span class="s2">, </span><span class="s1">metric):</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN works with the expected combinations of algorithms and 
    metrics, or raises the expected errors. 
    &quot;&quot;&quot;</span>
    <span class="s1">labels = HDBSCAN(algorithm=algo).fit_predict(X)</span>
    <span class="s1">n_clusters = len(set(labels) - OUTLIER_SET)</span>
    <span class="s2">assert </span><span class="s1">n_clusters == n_clusters_true</span>

    <span class="s5"># Validation for brute is handled by `pairwise_distances`</span>
    <span class="s2">if </span><span class="s1">algo </span><span class="s2">in </span><span class="s1">(</span><span class="s4">&quot;brute&quot;</span><span class="s2">, </span><span class="s4">&quot;auto&quot;</span><span class="s1">):</span>
        <span class="s2">return</span>

    <span class="s1">ALGOS_TREES = {</span>
        <span class="s4">&quot;kdtree&quot;</span><span class="s1">: KDTree</span><span class="s2">,</span>
        <span class="s4">&quot;balltree&quot;</span><span class="s1">: BallTree</span><span class="s2">,</span>
    <span class="s1">}</span>
    <span class="s1">metric_params = {</span>
        <span class="s4">&quot;mahalanobis&quot;</span><span class="s1">: {</span><span class="s4">&quot;V&quot;</span><span class="s1">: np.eye(X.shape[</span><span class="s3">1</span><span class="s1">])}</span><span class="s2">,</span>
        <span class="s4">&quot;seuclidean&quot;</span><span class="s1">: {</span><span class="s4">&quot;V&quot;</span><span class="s1">: np.ones(X.shape[</span><span class="s3">1</span><span class="s1">])}</span><span class="s2">,</span>
        <span class="s4">&quot;minkowski&quot;</span><span class="s1">: {</span><span class="s4">&quot;p&quot;</span><span class="s1">: </span><span class="s3">2</span><span class="s1">}</span><span class="s2">,</span>
        <span class="s4">&quot;wminkowski&quot;</span><span class="s1">: {</span><span class="s4">&quot;p&quot;</span><span class="s1">: </span><span class="s3">2</span><span class="s2">, </span><span class="s4">&quot;w&quot;</span><span class="s1">: np.ones(X.shape[</span><span class="s3">1</span><span class="s1">])}</span><span class="s2">,</span>
    <span class="s1">}.get(metric</span><span class="s2">, None</span><span class="s1">)</span>

    <span class="s1">hdb = HDBSCAN(</span>
        <span class="s1">algorithm=algo</span><span class="s2">,</span>
        <span class="s1">metric=metric</span><span class="s2">,</span>
        <span class="s1">metric_params=metric_params</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s2">if </span><span class="s1">metric </span><span class="s2">not in </span><span class="s1">ALGOS_TREES[algo].valid_metrics:</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
            <span class="s1">hdb.fit(X)</span>
    <span class="s2">elif </span><span class="s1">metric == </span><span class="s4">&quot;wminkowski&quot;</span><span class="s1">:</span>
        <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning):</span>
            <span class="s1">hdb.fit(X)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">hdb.fit(X)</span>


<span class="s2">def </span><span class="s1">test_dbscan_clustering():</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN can generate a sufficiently accurate dbscan clustering. 
    This test is more of a sanity check than a rigorous evaluation. 
 
    TODO: Improve and strengthen this test if at all possible. 
    &quot;&quot;&quot;</span>
    <span class="s1">clusterer = HDBSCAN().fit(X)</span>
    <span class="s1">labels = clusterer.dbscan_clustering(</span><span class="s3">0.3</span><span class="s1">)</span>
    <span class="s1">n_clusters = len(set(labels) - OUTLIER_SET)</span>
    <span class="s2">assert </span><span class="s1">n_clusters == n_clusters_true</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;cut_distance&quot;</span><span class="s2">, </span><span class="s1">(</span><span class="s3">0.1</span><span class="s2">, </span><span class="s3">0.5</span><span class="s2">, </span><span class="s3">1</span><span class="s1">))</span>
<span class="s2">def </span><span class="s1">test_dbscan_clustering_outlier_data(cut_distance):</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests if np.inf and np.nan data are each treated as special outliers. 
    &quot;&quot;&quot;</span>
    <span class="s1">missing_label = _OUTLIER_ENCODING[</span><span class="s4">&quot;missing&quot;</span><span class="s1">][</span><span class="s4">&quot;label&quot;</span><span class="s1">]</span>
    <span class="s1">infinite_label = _OUTLIER_ENCODING[</span><span class="s4">&quot;infinite&quot;</span><span class="s1">][</span><span class="s4">&quot;label&quot;</span><span class="s1">]</span>

    <span class="s1">X_outlier = X.copy()</span>
    <span class="s1">X_outlier[</span><span class="s3">0</span><span class="s1">] = [np.inf</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">X_outlier[</span><span class="s3">2</span><span class="s1">] = [</span><span class="s3">1</span><span class="s2">, </span><span class="s1">np.nan]</span>
    <span class="s1">X_outlier[</span><span class="s3">5</span><span class="s1">] = [np.inf</span><span class="s2">, </span><span class="s1">np.nan]</span>
    <span class="s1">model = HDBSCAN().fit(X_outlier)</span>
    <span class="s1">labels = model.dbscan_clustering(cut_distance=cut_distance)</span>

    <span class="s1">missing_labels_idx = np.flatnonzero(labels == missing_label)</span>
    <span class="s1">assert_array_equal(missing_labels_idx</span><span class="s2">, </span><span class="s1">[</span><span class="s3">2</span><span class="s2">, </span><span class="s3">5</span><span class="s1">])</span>

    <span class="s1">infinite_labels_idx = np.flatnonzero(labels == infinite_label)</span>
    <span class="s1">assert_array_equal(infinite_labels_idx</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0</span><span class="s1">])</span>

    <span class="s1">clean_idx = list(set(range(</span><span class="s3">200</span><span class="s1">)) - set(missing_labels_idx + infinite_labels_idx))</span>
    <span class="s1">clean_model = HDBSCAN().fit(X_outlier[clean_idx])</span>
    <span class="s1">clean_labels = clean_model.dbscan_clustering(cut_distance=cut_distance)</span>
    <span class="s1">assert_array_equal(clean_labels</span><span class="s2">, </span><span class="s1">labels[clean_idx])</span>


<span class="s2">def </span><span class="s1">test_hdbscan_high_dimensional():</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN using `BallTree` works with higher-dimensional data. 
    &quot;&quot;&quot;</span>
    <span class="s1">H</span><span class="s2">, </span><span class="s1">y = make_blobs(n_samples=</span><span class="s3">50</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">64</span><span class="s1">)</span>
    <span class="s1">H = StandardScaler().fit_transform(H)</span>
    <span class="s1">labels = HDBSCAN(</span>
        <span class="s1">algorithm=</span><span class="s4">&quot;auto&quot;</span><span class="s2">,</span>
        <span class="s1">metric=</span><span class="s4">&quot;seuclidean&quot;</span><span class="s2">,</span>
        <span class="s1">metric_params={</span><span class="s4">&quot;V&quot;</span><span class="s1">: np.ones(H.shape[</span><span class="s3">1</span><span class="s1">])}</span><span class="s2">,</span>
    <span class="s1">).fit_predict(H)</span>
    <span class="s1">n_clusters = len(set(labels) - OUTLIER_SET)</span>
    <span class="s2">assert </span><span class="s1">n_clusters == n_clusters_true</span>


<span class="s2">def </span><span class="s1">test_hdbscan_best_balltree_metric():</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN using `BallTree` works. 
    &quot;&quot;&quot;</span>
    <span class="s1">labels = HDBSCAN(</span>
        <span class="s1">metric=</span><span class="s4">&quot;seuclidean&quot;</span><span class="s2">, </span><span class="s1">metric_params={</span><span class="s4">&quot;V&quot;</span><span class="s1">: np.ones(X.shape[</span><span class="s3">1</span><span class="s1">])}</span>
    <span class="s1">).fit_predict(X)</span>
    <span class="s1">n_clusters = len(set(labels) - OUTLIER_SET)</span>
    <span class="s2">assert </span><span class="s1">n_clusters == n_clusters_true</span>


<span class="s2">def </span><span class="s1">test_hdbscan_no_clusters():</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN correctly does not generate a valid cluster when the 
    `min_cluster_size` is too large for the data. 
    &quot;&quot;&quot;</span>
    <span class="s1">labels = HDBSCAN(min_cluster_size=len(X) - </span><span class="s3">1</span><span class="s1">).fit_predict(X)</span>
    <span class="s1">n_clusters = len(set(labels) - OUTLIER_SET)</span>
    <span class="s2">assert </span><span class="s1">n_clusters == </span><span class="s3">0</span>


<span class="s2">def </span><span class="s1">test_hdbscan_min_cluster_size():</span>
    <span class="s0">&quot;&quot;&quot; 
    Test that the smallest non-noise cluster has at least `min_cluster_size` 
    many points 
    &quot;&quot;&quot;</span>
    <span class="s2">for </span><span class="s1">min_cluster_size </span><span class="s2">in </span><span class="s1">range(</span><span class="s3">2</span><span class="s2">, </span><span class="s1">len(X)</span><span class="s2">, </span><span class="s3">1</span><span class="s1">):</span>
        <span class="s1">labels = HDBSCAN(min_cluster_size=min_cluster_size).fit_predict(X)</span>
        <span class="s1">true_labels = [label </span><span class="s2">for </span><span class="s1">label </span><span class="s2">in </span><span class="s1">labels </span><span class="s2">if </span><span class="s1">label != -</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s2">if </span><span class="s1">len(true_labels) != </span><span class="s3">0</span><span class="s1">:</span>
            <span class="s2">assert </span><span class="s1">np.min(np.bincount(true_labels)) &gt;= min_cluster_size</span>


<span class="s2">def </span><span class="s1">test_hdbscan_callable_metric():</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN works when passed a callable metric. 
    &quot;&quot;&quot;</span>
    <span class="s1">metric = distance.euclidean</span>
    <span class="s1">labels = HDBSCAN(metric=metric).fit_predict(X)</span>
    <span class="s1">n_clusters = len(set(labels) - OUTLIER_SET)</span>
    <span class="s2">assert </span><span class="s1">n_clusters == n_clusters_true</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;tree&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;kd&quot;</span><span class="s2">, </span><span class="s4">&quot;ball&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_hdbscan_precomputed_non_brute(tree):</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN correctly raises an error when passing precomputed data 
    while requesting a tree-based algorithm. 
    &quot;&quot;&quot;</span>
    <span class="s1">hdb = HDBSCAN(metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s2">, </span><span class="s1">algorithm=</span><span class="s4">f&quot;prims_</span><span class="s2">{</span><span class="s1">tree</span><span class="s2">}</span><span class="s4">tree&quot;</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">hdb.fit(X)</span>


<span class="s2">def </span><span class="s1">test_hdbscan_sparse():</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN works correctly when passing sparse feature data. 
    Evaluates correctness by comparing against the same data passed as a dense 
    array. 
    &quot;&quot;&quot;</span>

    <span class="s1">dense_labels = HDBSCAN().fit(X).labels_</span>
    <span class="s1">n_clusters = len(set(dense_labels) - OUTLIER_SET)</span>
    <span class="s2">assert </span><span class="s1">n_clusters == </span><span class="s3">3</span>

    <span class="s1">_X_sparse = sparse.csr_matrix(X)</span>
    <span class="s1">X_sparse = _X_sparse.copy()</span>
    <span class="s1">sparse_labels = HDBSCAN().fit(X_sparse).labels_</span>
    <span class="s1">assert_array_equal(dense_labels</span><span class="s2">, </span><span class="s1">sparse_labels)</span>

    <span class="s5"># Compare that the sparse and dense non-precomputed routines return the same labels</span>
    <span class="s5"># where the 0th observation contains the outlier.</span>
    <span class="s2">for </span><span class="s1">outlier_val</span><span class="s2">, </span><span class="s1">outlier_type </span><span class="s2">in </span><span class="s1">((np.inf</span><span class="s2">, </span><span class="s4">&quot;infinite&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(np.nan</span><span class="s2">, </span><span class="s4">&quot;missing&quot;</span><span class="s1">)):</span>
        <span class="s1">X_dense = X.copy()</span>
        <span class="s1">X_dense[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s1">] = outlier_val</span>
        <span class="s1">dense_labels = HDBSCAN().fit(X_dense).labels_</span>
        <span class="s1">n_clusters = len(set(dense_labels) - OUTLIER_SET)</span>
        <span class="s2">assert </span><span class="s1">n_clusters == </span><span class="s3">3</span>
        <span class="s2">assert </span><span class="s1">dense_labels[</span><span class="s3">0</span><span class="s1">] == _OUTLIER_ENCODING[outlier_type][</span><span class="s4">&quot;label&quot;</span><span class="s1">]</span>

        <span class="s1">X_sparse = _X_sparse.copy()</span>
        <span class="s1">X_sparse[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s1">] = outlier_val</span>
        <span class="s1">sparse_labels = HDBSCAN().fit(X_sparse).labels_</span>
        <span class="s1">assert_array_equal(dense_labels</span><span class="s2">, </span><span class="s1">sparse_labels)</span>

    <span class="s1">msg = </span><span class="s4">&quot;Sparse data matrices only support algorithm `brute`.&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">HDBSCAN(metric=</span><span class="s4">&quot;euclidean&quot;</span><span class="s2">, </span><span class="s1">algorithm=</span><span class="s4">&quot;balltree&quot;</span><span class="s1">).fit(X_sparse)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;algorithm&quot;</span><span class="s2">, </span><span class="s1">ALGORITHMS)</span>
<span class="s2">def </span><span class="s1">test_hdbscan_centers(algorithm):</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN centers are calculated and stored properly, and are 
    accurate to the data. 
    &quot;&quot;&quot;</span>
    <span class="s1">centers = [(</span><span class="s3">0.0</span><span class="s2">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s3">3.0</span><span class="s2">, </span><span class="s3">3.0</span><span class="s1">)]</span>
    <span class="s1">H</span><span class="s2">, </span><span class="s1">_ = make_blobs(n_samples=</span><span class="s3">1000</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">centers=centers</span><span class="s2">, </span><span class="s1">cluster_std=</span><span class="s3">0.5</span><span class="s1">)</span>
    <span class="s1">hdb = HDBSCAN(store_centers=</span><span class="s4">&quot;both&quot;</span><span class="s1">).fit(H)</span>

    <span class="s2">for </span><span class="s1">center</span><span class="s2">, </span><span class="s1">centroid</span><span class="s2">, </span><span class="s1">medoid </span><span class="s2">in </span><span class="s1">zip(centers</span><span class="s2">, </span><span class="s1">hdb.centroids_</span><span class="s2">, </span><span class="s1">hdb.medoids_):</span>
        <span class="s1">assert_allclose(center</span><span class="s2">, </span><span class="s1">centroid</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">0.05</span><span class="s1">)</span>
        <span class="s1">assert_allclose(center</span><span class="s2">, </span><span class="s1">medoid</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">0.05</span><span class="s1">)</span>

    <span class="s5"># Ensure that nothing is done for noise</span>
    <span class="s1">hdb = HDBSCAN(</span>
        <span class="s1">algorithm=algorithm</span><span class="s2">, </span><span class="s1">store_centers=</span><span class="s4">&quot;both&quot;</span><span class="s2">, </span><span class="s1">min_cluster_size=X.shape[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">).fit(X)</span>
    <span class="s2">assert </span><span class="s1">hdb.centroids_.shape[</span><span class="s3">0</span><span class="s1">] == </span><span class="s3">0</span>
    <span class="s2">assert </span><span class="s1">hdb.medoids_.shape[</span><span class="s3">0</span><span class="s1">] == </span><span class="s3">0</span>


<span class="s2">def </span><span class="s1">test_hdbscan_allow_single_cluster_with_epsilon():</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN single-cluster selection with epsilon works correctly. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">no_structure = rng.rand(</span><span class="s3">150</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s5"># without epsilon we should see many noise points as children of root.</span>
    <span class="s1">labels = HDBSCAN(</span>
        <span class="s1">min_cluster_size=</span><span class="s3">5</span><span class="s2">,</span>
        <span class="s1">cluster_selection_epsilon=</span><span class="s3">0.0</span><span class="s2">,</span>
        <span class="s1">cluster_selection_method=</span><span class="s4">&quot;eom&quot;</span><span class="s2">,</span>
        <span class="s1">allow_single_cluster=</span><span class="s2">True,</span>
    <span class="s1">).fit_predict(no_structure)</span>
    <span class="s1">unique_labels</span><span class="s2">, </span><span class="s1">counts = np.unique(labels</span><span class="s2">, </span><span class="s1">return_counts=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">len(unique_labels) == </span><span class="s3">2</span>

    <span class="s5"># Arbitrary heuristic. Would prefer something more precise.</span>
    <span class="s2">assert </span><span class="s1">counts[unique_labels == -</span><span class="s3">1</span><span class="s1">] &gt; </span><span class="s3">30</span>

    <span class="s5"># for this random seed an epsilon of 0.18 will produce exactly 2 noise</span>
    <span class="s5"># points at that cut in single linkage.</span>
    <span class="s1">labels = HDBSCAN(</span>
        <span class="s1">min_cluster_size=</span><span class="s3">5</span><span class="s2">,</span>
        <span class="s1">cluster_selection_epsilon=</span><span class="s3">0.18</span><span class="s2">,</span>
        <span class="s1">cluster_selection_method=</span><span class="s4">&quot;eom&quot;</span><span class="s2">,</span>
        <span class="s1">allow_single_cluster=</span><span class="s2">True,</span>
        <span class="s1">algorithm=</span><span class="s4">&quot;kdtree&quot;</span><span class="s2">,</span>
    <span class="s1">).fit_predict(no_structure)</span>
    <span class="s1">unique_labels</span><span class="s2">, </span><span class="s1">counts = np.unique(labels</span><span class="s2">, </span><span class="s1">return_counts=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">len(unique_labels) == </span><span class="s3">2</span>
    <span class="s2">assert </span><span class="s1">counts[unique_labels == -</span><span class="s3">1</span><span class="s1">] == </span><span class="s3">2</span>


<span class="s2">def </span><span class="s1">test_hdbscan_better_than_dbscan():</span>
    <span class="s0">&quot;&quot;&quot; 
    Validate that HDBSCAN can properly cluster this difficult synthetic 
    dataset. Note that DBSCAN fails on this (see HDBSCAN plotting 
    example) 
    &quot;&quot;&quot;</span>
    <span class="s1">centers = [[-</span><span class="s3">0.85</span><span class="s2">, </span><span class="s1">-</span><span class="s3">0.85</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s3">0.85</span><span class="s2">, </span><span class="s3">0.85</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">3</span><span class="s2">, </span><span class="s3">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">3</span><span class="s2">, </span><span class="s1">-</span><span class="s3">3</span><span class="s1">]]</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">_ = make_blobs(</span>
        <span class="s1">n_samples=</span><span class="s3">750</span><span class="s2">,</span>
        <span class="s1">centers=centers</span><span class="s2">,</span>
        <span class="s1">cluster_std=[</span><span class="s3">0.2</span><span class="s2">, </span><span class="s3">0.35</span><span class="s2">, </span><span class="s3">1.35</span><span class="s2">, </span><span class="s3">1.35</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">hdb = HDBSCAN().fit(X)</span>
    <span class="s1">n_clusters = len(set(hdb.labels_)) - int(-</span><span class="s3">1 </span><span class="s2">in </span><span class="s1">hdb.labels_)</span>
    <span class="s2">assert </span><span class="s1">n_clusters == </span><span class="s3">4</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;kwargs, X&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">({</span><span class="s4">&quot;metric&quot;</span><span class="s1">: </span><span class="s4">&quot;precomputed&quot;</span><span class="s1">}</span><span class="s2">, </span><span class="s1">np.array([[</span><span class="s3">1</span><span class="s2">, </span><span class="s1">np.inf]</span><span class="s2">, </span><span class="s1">[np.inf</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]]))</span><span class="s2">,</span>
        <span class="s1">({</span><span class="s4">&quot;metric&quot;</span><span class="s1">: </span><span class="s4">&quot;precomputed&quot;</span><span class="s1">}</span><span class="s2">, </span><span class="s1">[[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">2</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]])</span><span class="s2">,</span>
        <span class="s1">({}</span><span class="s2">, </span><span class="s1">[[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">3</span><span class="s2">, </span><span class="s3">4</span><span class="s1">]])</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_hdbscan_usable_inputs(X</span><span class="s2">, </span><span class="s1">kwargs):</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN works correctly for array-likes and precomputed inputs 
    with non-finite points. 
    &quot;&quot;&quot;</span>
    <span class="s1">HDBSCAN(min_samples=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">**kwargs).fit(X)</span>


<span class="s2">def </span><span class="s1">test_hdbscan_sparse_distances_too_few_nonzero():</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN raises the correct error when there are too few 
    non-zero distances. 
    &quot;&quot;&quot;</span>
    <span class="s1">X = sparse.csr_matrix(np.zeros((</span><span class="s3">10</span><span class="s2">, </span><span class="s3">10</span><span class="s1">)))</span>

    <span class="s1">msg = </span><span class="s4">&quot;There exists points with fewer than&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">HDBSCAN(metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s1">).fit(X)</span>


<span class="s2">def </span><span class="s1">test_hdbscan_tree_invalid_metric():</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN correctly raises an error for invalid metric choices. 
    &quot;&quot;&quot;</span>
    <span class="s1">metric_callable = </span><span class="s2">lambda </span><span class="s1">x: x</span>
    <span class="s1">msg = (</span>
        <span class="s4">&quot;.* is not a valid metric for a .*-based algorithm</span><span class="s2">\\</span><span class="s4">. Please select a different&quot;</span>
        <span class="s4">&quot; metric</span><span class="s2">\\</span><span class="s4">.&quot;</span>
    <span class="s1">)</span>

    <span class="s5"># Callables are not supported for either</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">HDBSCAN(algorithm=</span><span class="s4">&quot;kdtree&quot;</span><span class="s2">, </span><span class="s1">metric=metric_callable).fit(X)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">HDBSCAN(algorithm=</span><span class="s4">&quot;balltree&quot;</span><span class="s2">, </span><span class="s1">metric=metric_callable).fit(X)</span>

    <span class="s5"># The set of valid metrics for KDTree at the time of writing this test is a</span>
    <span class="s5"># strict subset of those supported in BallTree</span>
    <span class="s1">metrics_not_kd = list(set(BallTree.valid_metrics) - set(KDTree.valid_metrics))</span>
    <span class="s2">if </span><span class="s1">len(metrics_not_kd) &gt; </span><span class="s3">0</span><span class="s1">:</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
            <span class="s1">HDBSCAN(algorithm=</span><span class="s4">&quot;kdtree&quot;</span><span class="s2">, </span><span class="s1">metric=metrics_not_kd[</span><span class="s3">0</span><span class="s1">]).fit(X)</span>


<span class="s2">def </span><span class="s1">test_hdbscan_too_many_min_samples():</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN correctly raises an error when setting `min_samples` 
    larger than the number of samples. 
    &quot;&quot;&quot;</span>
    <span class="s1">hdb = HDBSCAN(min_samples=len(X) + </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">msg = </span><span class="s4">r&quot;min_samples (.*) must be at most&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">hdb.fit(X)</span>


<span class="s2">def </span><span class="s1">test_hdbscan_precomputed_dense_nan():</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that HDBSCAN correctly raises an error when providing precomputed 
    distances with `np.nan` values. 
    &quot;&quot;&quot;</span>
    <span class="s1">X_nan = X.copy()</span>
    <span class="s1">X_nan[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s1">] = np.nan</span>
    <span class="s1">msg = </span><span class="s4">&quot;np.nan values found in precomputed-dense&quot;</span>
    <span class="s1">hdb = HDBSCAN(metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">hdb.fit(X_nan)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;allow_single_cluster&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;epsilon&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0.1</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_labelling_distinct(global_random_seed</span><span class="s2">, </span><span class="s1">allow_single_cluster</span><span class="s2">, </span><span class="s1">epsilon):</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that the `_do_labelling` helper function correctly assigns labels. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s3">48</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(</span>
        <span class="s1">n_samples</span><span class="s2">,</span>
        <span class="s1">random_state=global_random_seed</span><span class="s2">,</span>
        <span class="s5"># Ensure the clusters are distinct with no overlap</span>
        <span class="s1">centers=[</span>
            <span class="s1">[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">10</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">10</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">est = HDBSCAN().fit(X)</span>
    <span class="s1">condensed_tree = _condense_tree(</span>
        <span class="s1">est._single_linkage_tree_</span><span class="s2">, </span><span class="s1">min_cluster_size=est.min_cluster_size</span>
    <span class="s1">)</span>
    <span class="s1">clusters = {n_samples + </span><span class="s3">2</span><span class="s2">, </span><span class="s1">n_samples + </span><span class="s3">3</span><span class="s2">, </span><span class="s1">n_samples + </span><span class="s3">4</span><span class="s1">}</span>
    <span class="s1">cluster_label_map = {n_samples + </span><span class="s3">2</span><span class="s1">: </span><span class="s3">0</span><span class="s2">, </span><span class="s1">n_samples + </span><span class="s3">3</span><span class="s1">: </span><span class="s3">1</span><span class="s2">, </span><span class="s1">n_samples + </span><span class="s3">4</span><span class="s1">: </span><span class="s3">2</span><span class="s1">}</span>
    <span class="s1">labels = _do_labelling(</span>
        <span class="s1">condensed_tree=condensed_tree</span><span class="s2">,</span>
        <span class="s1">clusters=clusters</span><span class="s2">,</span>
        <span class="s1">cluster_label_map=cluster_label_map</span><span class="s2">,</span>
        <span class="s1">allow_single_cluster=allow_single_cluster</span><span class="s2">,</span>
        <span class="s1">cluster_selection_epsilon=epsilon</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">first_with_label = {_y: np.where(y == _y)[</span><span class="s3">0</span><span class="s1">][</span><span class="s3">0</span><span class="s1">] </span><span class="s2">for </span><span class="s1">_y </span><span class="s2">in </span><span class="s1">list(set(y))}</span>
    <span class="s1">y_to_labels = {_y: labels[first_with_label[_y]] </span><span class="s2">for </span><span class="s1">_y </span><span class="s2">in </span><span class="s1">list(set(y))}</span>
    <span class="s1">aligned_target = np.vectorize(y_to_labels.get)(y)</span>
    <span class="s1">assert_array_equal(labels</span><span class="s2">, </span><span class="s1">aligned_target)</span>


<span class="s2">def </span><span class="s1">test_labelling_thresholding():</span>
    <span class="s0">&quot;&quot;&quot; 
    Tests that the `_do_labelling` helper function correctly thresholds the 
    incoming lambda values given various `cluster_selection_epsilon` values. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s3">5</span>
    <span class="s1">MAX_LAMBDA = </span><span class="s3">1.5</span>
    <span class="s1">condensed_tree = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">(</span><span class="s3">5</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s1">MAX_LAMBDA</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">(</span><span class="s3">5</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0.1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">(</span><span class="s3">5</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s1">MAX_LAMBDA</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">(</span><span class="s3">5</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s3">0.2</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">(</span><span class="s3">5</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">0.3</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s1">dtype=CONDENSED_dtype</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">labels = _do_labelling(</span>
        <span class="s1">condensed_tree=condensed_tree</span><span class="s2">,</span>
        <span class="s1">clusters={n_samples}</span><span class="s2">,</span>
        <span class="s1">cluster_label_map={n_samples: </span><span class="s3">0</span><span class="s2">, </span><span class="s1">n_samples + </span><span class="s3">1</span><span class="s1">: </span><span class="s3">1</span><span class="s1">}</span><span class="s2">,</span>
        <span class="s1">allow_single_cluster=</span><span class="s2">True,</span>
        <span class="s1">cluster_selection_epsilon=</span><span class="s3">1</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">num_noise = condensed_tree[</span><span class="s4">&quot;value&quot;</span><span class="s1">] &lt; </span><span class="s3">1</span>
    <span class="s2">assert </span><span class="s1">sum(num_noise) == sum(labels == -</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">labels = _do_labelling(</span>
        <span class="s1">condensed_tree=condensed_tree</span><span class="s2">,</span>
        <span class="s1">clusters={n_samples}</span><span class="s2">,</span>
        <span class="s1">cluster_label_map={n_samples: </span><span class="s3">0</span><span class="s2">, </span><span class="s1">n_samples + </span><span class="s3">1</span><span class="s1">: </span><span class="s3">1</span><span class="s1">}</span><span class="s2">,</span>
        <span class="s1">allow_single_cluster=</span><span class="s2">True,</span>
        <span class="s1">cluster_selection_epsilon=</span><span class="s3">0</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s5"># The threshold should be calculated per-sample based on the largest</span>
    <span class="s5"># lambda of any simbling node. In this case, all points are siblings</span>
    <span class="s5"># and the largest value is exactly MAX_LAMBDA.</span>
    <span class="s1">num_noise = condensed_tree[</span><span class="s4">&quot;value&quot;</span><span class="s1">] &lt; MAX_LAMBDA</span>
    <span class="s2">assert </span><span class="s1">sum(num_noise) == sum(labels == -</span><span class="s3">1</span><span class="s1">)</span>
</pre>
</body>
</html>