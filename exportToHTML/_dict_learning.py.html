<html>
<head>
<title>_dict_learning.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_dict_learning.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; Dictionary learning. 
&quot;&quot;&quot;</span>
<span class="s2"># Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">itertools</span>
<span class="s3">import </span><span class="s1">sys</span>
<span class="s3">import </span><span class="s1">time</span>
<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">math </span><span class="s3">import </span><span class="s1">ceil</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">joblib </span><span class="s3">import </span><span class="s1">effective_n_jobs</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">linalg</span>

<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">BaseEstimator</span><span class="s3">,</span>
    <span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s3">,</span>
    <span class="s1">TransformerMixin</span><span class="s3">,</span>
    <span class="s1">_fit_context</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">..linear_model </span><span class="s3">import </span><span class="s1">Lars</span><span class="s3">, </span><span class="s1">Lasso</span><span class="s3">, </span><span class="s1">LassoLars</span><span class="s3">, </span><span class="s1">orthogonal_mp_gram</span>
<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">check_array</span><span class="s3">, </span><span class="s1">check_random_state</span><span class="s3">, </span><span class="s1">gen_batches</span><span class="s3">, </span><span class="s1">gen_even_slices</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">Hidden</span><span class="s3">, </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">StrOptions</span><span class="s3">, </span><span class="s1">validate_params</span>
<span class="s3">from </span><span class="s1">..utils.extmath </span><span class="s3">import </span><span class="s1">randomized_svd</span><span class="s3">, </span><span class="s1">row_norms</span><span class="s3">, </span><span class="s1">svd_flip</span>
<span class="s3">from </span><span class="s1">..utils.parallel </span><span class="s3">import </span><span class="s1">Parallel</span><span class="s3">, </span><span class="s1">delayed</span>
<span class="s3">from </span><span class="s1">..utils.validation </span><span class="s3">import </span><span class="s1">check_is_fitted</span>


<span class="s3">def </span><span class="s1">_check_positive_coding(method</span><span class="s3">, </span><span class="s1">positive):</span>
    <span class="s3">if </span><span class="s1">positive </span><span class="s3">and </span><span class="s1">method </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;omp&quot;</span><span class="s3">, </span><span class="s4">&quot;lars&quot;</span><span class="s1">]:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;Positive constraint not supported for '{}' coding method.&quot;</span><span class="s1">.format(method)</span>
        <span class="s1">)</span>


<span class="s3">def </span><span class="s1">_sparse_encode_precomputed(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">dictionary</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">gram=</span><span class="s3">None,</span>
    <span class="s1">cov=</span><span class="s3">None,</span>
    <span class="s1">algorithm=</span><span class="s4">&quot;lasso_lars&quot;</span><span class="s3">,</span>
    <span class="s1">regularization=</span><span class="s3">None,</span>
    <span class="s1">copy_cov=</span><span class="s3">True,</span>
    <span class="s1">init=</span><span class="s3">None,</span>
    <span class="s1">max_iter=</span><span class="s5">1000</span><span class="s3">,</span>
    <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">positive=</span><span class="s3">False,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Generic sparse coding with precomputed Gram and/or covariance matrices. 
 
    Each row of the result is the solution to a Lasso problem. 
 
    Parameters 
    ---------- 
    X : ndarray of shape (n_samples, n_features) 
        Data matrix. 
 
    dictionary : ndarray of shape (n_components, n_features) 
        The dictionary matrix against which to solve the sparse coding of 
        the data. Some of the algorithms assume normalized rows. 
 
    gram : ndarray of shape (n_components, n_components), default=None 
        Precomputed Gram matrix, `dictionary * dictionary'` 
        gram can be `None` if method is 'threshold'. 
 
    cov : ndarray of shape (n_components, n_samples), default=None 
        Precomputed covariance, `dictionary * X'`. 
 
    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \ 
            default='lasso_lars' 
        The algorithm used: 
 
        * `'lars'`: uses the least angle regression method 
          (`linear_model.lars_path`); 
        * `'lasso_lars'`: uses Lars to compute the Lasso solution; 
        * `'lasso_cd'`: uses the coordinate descent method to compute the 
          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if 
          the estimated components are sparse; 
        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse 
          solution; 
        * `'threshold'`: squashes to zero all coefficients less than 
          regularization from the projection `dictionary * data'`. 
 
    regularization : int or float, default=None 
        The regularization parameter. It corresponds to alpha when 
        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`. 
        Otherwise it corresponds to `n_nonzero_coefs`. 
 
    init : ndarray of shape (n_samples, n_components), default=None 
        Initialization value of the sparse code. Only used if 
        `algorithm='lasso_cd'`. 
 
    max_iter : int, default=1000 
        Maximum number of iterations to perform if `algorithm='lasso_cd'` or 
        `'lasso_lars'`. 
 
    copy_cov : bool, default=True 
        Whether to copy the precomputed covariance matrix; if `False`, it may 
        be overwritten. 
 
    verbose : int, default=0 
        Controls the verbosity; the higher, the more messages. 
 
    positive: bool, default=False 
        Whether to enforce a positivity constraint on the sparse code. 
 
        .. versionadded:: 0.20 
 
    Returns 
    ------- 
    code : ndarray of shape (n_components, n_features) 
        The sparse codes. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">n_components = dictionary.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s3">if </span><span class="s1">algorithm == </span><span class="s4">&quot;lasso_lars&quot;</span><span class="s1">:</span>
        <span class="s1">alpha = float(regularization) / n_features  </span><span class="s2"># account for scaling</span>
        <span class="s3">try</span><span class="s1">:</span>
            <span class="s1">err_mgt = np.seterr(all=</span><span class="s4">&quot;ignore&quot;</span><span class="s1">)</span>

            <span class="s2"># Not passing in verbose=max(0, verbose-1) because Lars.fit already</span>
            <span class="s2"># corrects the verbosity level.</span>
            <span class="s1">lasso_lars = LassoLars(</span>
                <span class="s1">alpha=alpha</span><span class="s3">,</span>
                <span class="s1">fit_intercept=</span><span class="s3">False,</span>
                <span class="s1">verbose=verbose</span><span class="s3">,</span>
                <span class="s1">precompute=gram</span><span class="s3">,</span>
                <span class="s1">fit_path=</span><span class="s3">False,</span>
                <span class="s1">positive=positive</span><span class="s3">,</span>
                <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">lasso_lars.fit(dictionary.T</span><span class="s3">, </span><span class="s1">X.T</span><span class="s3">, </span><span class="s1">Xy=cov)</span>
            <span class="s1">new_code = lasso_lars.coef_</span>
        <span class="s3">finally</span><span class="s1">:</span>
            <span class="s1">np.seterr(**err_mgt)</span>

    <span class="s3">elif </span><span class="s1">algorithm == </span><span class="s4">&quot;lasso_cd&quot;</span><span class="s1">:</span>
        <span class="s1">alpha = float(regularization) / n_features  </span><span class="s2"># account for scaling</span>

        <span class="s2"># TODO: Make verbosity argument for Lasso?</span>
        <span class="s2"># sklearn.linear_model.coordinate_descent.enet_path has a verbosity</span>
        <span class="s2"># argument that we could pass in from Lasso.</span>
        <span class="s1">clf = Lasso(</span>
            <span class="s1">alpha=alpha</span><span class="s3">,</span>
            <span class="s1">fit_intercept=</span><span class="s3">False,</span>
            <span class="s1">precompute=gram</span><span class="s3">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
            <span class="s1">warm_start=</span><span class="s3">True,</span>
            <span class="s1">positive=positive</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">init </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s2"># In some workflows using coordinate descent algorithms:</span>
            <span class="s2">#  - users might provide NumPy arrays with read-only buffers</span>
            <span class="s2">#  - `joblib` might memmap arrays making their buffer read-only</span>
            <span class="s2"># TODO: move this handling (which is currently too broad)</span>
            <span class="s2"># closer to the actual private function which need buffers to be writable.</span>
            <span class="s3">if not </span><span class="s1">init.flags[</span><span class="s4">&quot;WRITEABLE&quot;</span><span class="s1">]:</span>
                <span class="s1">init = np.array(init)</span>
            <span class="s1">clf.coef_ = init</span>

        <span class="s1">clf.fit(dictionary.T</span><span class="s3">, </span><span class="s1">X.T</span><span class="s3">, </span><span class="s1">check_input=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">new_code = clf.coef_</span>

    <span class="s3">elif </span><span class="s1">algorithm == </span><span class="s4">&quot;lars&quot;</span><span class="s1">:</span>
        <span class="s3">try</span><span class="s1">:</span>
            <span class="s1">err_mgt = np.seterr(all=</span><span class="s4">&quot;ignore&quot;</span><span class="s1">)</span>

            <span class="s2"># Not passing in verbose=max(0, verbose-1) because Lars.fit already</span>
            <span class="s2"># corrects the verbosity level.</span>
            <span class="s1">lars = Lars(</span>
                <span class="s1">fit_intercept=</span><span class="s3">False,</span>
                <span class="s1">verbose=verbose</span><span class="s3">,</span>
                <span class="s1">precompute=gram</span><span class="s3">,</span>
                <span class="s1">n_nonzero_coefs=int(regularization)</span><span class="s3">,</span>
                <span class="s1">fit_path=</span><span class="s3">False,</span>
            <span class="s1">)</span>
            <span class="s1">lars.fit(dictionary.T</span><span class="s3">, </span><span class="s1">X.T</span><span class="s3">, </span><span class="s1">Xy=cov)</span>
            <span class="s1">new_code = lars.coef_</span>
        <span class="s3">finally</span><span class="s1">:</span>
            <span class="s1">np.seterr(**err_mgt)</span>

    <span class="s3">elif </span><span class="s1">algorithm == </span><span class="s4">&quot;threshold&quot;</span><span class="s1">:</span>
        <span class="s1">new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)).T</span>
        <span class="s3">if </span><span class="s1">positive:</span>
            <span class="s1">np.clip(new_code</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">out=new_code)</span>

    <span class="s3">elif </span><span class="s1">algorithm == </span><span class="s4">&quot;omp&quot;</span><span class="s1">:</span>
        <span class="s1">new_code = orthogonal_mp_gram(</span>
            <span class="s1">Gram=gram</span><span class="s3">,</span>
            <span class="s1">Xy=cov</span><span class="s3">,</span>
            <span class="s1">n_nonzero_coefs=int(regularization)</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s3">None,</span>
            <span class="s1">norms_squared=row_norms(X</span><span class="s3">, </span><span class="s1">squared=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">copy_Xy=copy_cov</span><span class="s3">,</span>
        <span class="s1">).T</span>

    <span class="s3">return </span><span class="s1">new_code.reshape(n_samples</span><span class="s3">, </span><span class="s1">n_components)</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s4">&quot;X&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;dictionary&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;gram&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;cov&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;algorithm&quot;</span><span class="s1">: [</span>
            <span class="s1">StrOptions({</span><span class="s4">&quot;lasso_lars&quot;</span><span class="s3">, </span><span class="s4">&quot;lasso_cd&quot;</span><span class="s3">, </span><span class="s4">&quot;lars&quot;</span><span class="s3">, </span><span class="s4">&quot;omp&quot;</span><span class="s3">, </span><span class="s4">&quot;threshold&quot;</span><span class="s1">})</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_nonzero_coefs&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;alpha&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;copy_cov&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;init&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;n_jobs&quot;</span><span class="s1">: [Integral</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;check_input&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s4">&quot;verbose&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;positive&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">True,</span>
<span class="s1">)</span>
<span class="s2"># XXX : could be moved to the linear_model module</span>
<span class="s3">def </span><span class="s1">sparse_encode(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">dictionary</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">gram=</span><span class="s3">None,</span>
    <span class="s1">cov=</span><span class="s3">None,</span>
    <span class="s1">algorithm=</span><span class="s4">&quot;lasso_lars&quot;</span><span class="s3">,</span>
    <span class="s1">n_nonzero_coefs=</span><span class="s3">None,</span>
    <span class="s1">alpha=</span><span class="s3">None,</span>
    <span class="s1">copy_cov=</span><span class="s3">True,</span>
    <span class="s1">init=</span><span class="s3">None,</span>
    <span class="s1">max_iter=</span><span class="s5">1000</span><span class="s3">,</span>
    <span class="s1">n_jobs=</span><span class="s3">None,</span>
    <span class="s1">check_input=</span><span class="s3">True,</span>
    <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">positive=</span><span class="s3">False,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Sparse coding. 
 
    Each row of the result is the solution to a sparse coding problem. 
    The goal is to find a sparse array `code` such that:: 
 
        X ~= code * dictionary 
 
    Read more in the :ref:`User Guide &lt;SparseCoder&gt;`. 
 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
        Data matrix. 
 
    dictionary : array-like of shape (n_components, n_features) 
        The dictionary matrix against which to solve the sparse coding of 
        the data. Some of the algorithms assume normalized rows for meaningful 
        output. 
 
    gram : array-like of shape (n_components, n_components), default=None 
        Precomputed Gram matrix, `dictionary * dictionary'`. 
 
    cov : array-like of shape (n_components, n_samples), default=None 
        Precomputed covariance, `dictionary' * X`. 
 
    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \ 
            default='lasso_lars' 
        The algorithm used: 
 
        * `'lars'`: uses the least angle regression method 
          (`linear_model.lars_path`); 
        * `'lasso_lars'`: uses Lars to compute the Lasso solution; 
        * `'lasso_cd'`: uses the coordinate descent method to compute the 
          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if 
          the estimated components are sparse; 
        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse 
          solution; 
        * `'threshold'`: squashes to zero all coefficients less than 
          regularization from the projection `dictionary * data'`. 
 
    n_nonzero_coefs : int, default=None 
        Number of nonzero coefficients to target in each column of the 
        solution. This is only used by `algorithm='lars'` and `algorithm='omp'` 
        and is overridden by `alpha` in the `omp` case. If `None`, then 
        `n_nonzero_coefs=int(n_features / 10)`. 
 
    alpha : float, default=None 
        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the 
        penalty applied to the L1 norm. 
        If `algorithm='threshold'`, `alpha` is the absolute value of the 
        threshold below which coefficients will be squashed to zero. 
        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of 
        the reconstruction error targeted. In this case, it overrides 
        `n_nonzero_coefs`. 
        If `None`, default to 1. 
 
    copy_cov : bool, default=True 
        Whether to copy the precomputed covariance matrix; if `False`, it may 
        be overwritten. 
 
    init : ndarray of shape (n_samples, n_components), default=None 
        Initialization value of the sparse codes. Only used if 
        `algorithm='lasso_cd'`. 
 
    max_iter : int, default=1000 
        Maximum number of iterations to perform if `algorithm='lasso_cd'` or 
        `'lasso_lars'`. 
 
    n_jobs : int, default=None 
        Number of parallel jobs to run. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
    check_input : bool, default=True 
        If `False`, the input arrays X and dictionary will not be checked. 
 
    verbose : int, default=0 
        Controls the verbosity; the higher, the more messages. 
 
    positive : bool, default=False 
        Whether to enforce positivity when finding the encoding. 
 
        .. versionadded:: 0.20 
 
    Returns 
    ------- 
    code : ndarray of shape (n_samples, n_components) 
        The sparse codes. 
 
    See Also 
    -------- 
    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso 
        path using LARS algorithm. 
    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems. 
    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer. 
    SparseCoder : Find a sparse representation of data from a fixed precomputed 
        dictionary. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">check_input:</span>
        <span class="s3">if </span><span class="s1">algorithm == </span><span class="s4">&quot;lasso_cd&quot;</span><span class="s1">:</span>
            <span class="s1">dictionary = check_array(</span>
                <span class="s1">dictionary</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s3">, </span><span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span>
            <span class="s1">)</span>
            <span class="s1">X = check_array(X</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s3">, </span><span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">dictionary = check_array(dictionary)</span>
            <span class="s1">X = check_array(X)</span>

    <span class="s3">if </span><span class="s1">dictionary.shape[</span><span class="s5">1</span><span class="s1">] != X.shape[</span><span class="s5">1</span><span class="s1">]:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;Dictionary and X have different numbers of features:&quot;</span>
            <span class="s4">&quot;dictionary.shape: {} X.shape{}&quot;</span><span class="s1">.format(dictionary.shape</span><span class="s3">, </span><span class="s1">X.shape)</span>
        <span class="s1">)</span>

    <span class="s1">_check_positive_coding(algorithm</span><span class="s3">, </span><span class="s1">positive)</span>

    <span class="s3">return </span><span class="s1">_sparse_encode(</span>
        <span class="s1">X</span><span class="s3">,</span>
        <span class="s1">dictionary</span><span class="s3">,</span>
        <span class="s1">gram=gram</span><span class="s3">,</span>
        <span class="s1">cov=cov</span><span class="s3">,</span>
        <span class="s1">algorithm=algorithm</span><span class="s3">,</span>
        <span class="s1">n_nonzero_coefs=n_nonzero_coefs</span><span class="s3">,</span>
        <span class="s1">alpha=alpha</span><span class="s3">,</span>
        <span class="s1">copy_cov=copy_cov</span><span class="s3">,</span>
        <span class="s1">init=init</span><span class="s3">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
        <span class="s1">n_jobs=n_jobs</span><span class="s3">,</span>
        <span class="s1">verbose=verbose</span><span class="s3">,</span>
        <span class="s1">positive=positive</span><span class="s3">,</span>
    <span class="s1">)</span>


<span class="s3">def </span><span class="s1">_sparse_encode(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">dictionary</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">gram=</span><span class="s3">None,</span>
    <span class="s1">cov=</span><span class="s3">None,</span>
    <span class="s1">algorithm=</span><span class="s4">&quot;lasso_lars&quot;</span><span class="s3">,</span>
    <span class="s1">n_nonzero_coefs=</span><span class="s3">None,</span>
    <span class="s1">alpha=</span><span class="s3">None,</span>
    <span class="s1">copy_cov=</span><span class="s3">True,</span>
    <span class="s1">init=</span><span class="s3">None,</span>
    <span class="s1">max_iter=</span><span class="s5">1000</span><span class="s3">,</span>
    <span class="s1">n_jobs=</span><span class="s3">None,</span>
    <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">positive=</span><span class="s3">False,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Sparse coding without input/parameter validation.&quot;&quot;&quot;</span>

    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">n_components = dictionary.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s3">if </span><span class="s1">algorithm </span><span class="s3">in </span><span class="s1">(</span><span class="s4">&quot;lars&quot;</span><span class="s3">, </span><span class="s4">&quot;omp&quot;</span><span class="s1">):</span>
        <span class="s1">regularization = n_nonzero_coefs</span>
        <span class="s3">if </span><span class="s1">regularization </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">regularization = min(max(n_features / </span><span class="s5">10</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">n_components)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">regularization = alpha</span>
        <span class="s3">if </span><span class="s1">regularization </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">regularization = </span><span class="s5">1.0</span>

    <span class="s3">if </span><span class="s1">gram </span><span class="s3">is None and </span><span class="s1">algorithm != </span><span class="s4">&quot;threshold&quot;</span><span class="s1">:</span>
        <span class="s1">gram = np.dot(dictionary</span><span class="s3">, </span><span class="s1">dictionary.T)</span>

    <span class="s3">if </span><span class="s1">cov </span><span class="s3">is None and </span><span class="s1">algorithm != </span><span class="s4">&quot;lasso_cd&quot;</span><span class="s1">:</span>
        <span class="s1">copy_cov = </span><span class="s3">False</span>
        <span class="s1">cov = np.dot(dictionary</span><span class="s3">, </span><span class="s1">X.T)</span>

    <span class="s3">if </span><span class="s1">effective_n_jobs(n_jobs) == </span><span class="s5">1 </span><span class="s3">or </span><span class="s1">algorithm == </span><span class="s4">&quot;threshold&quot;</span><span class="s1">:</span>
        <span class="s1">code = _sparse_encode_precomputed(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">dictionary</span><span class="s3">,</span>
            <span class="s1">gram=gram</span><span class="s3">,</span>
            <span class="s1">cov=cov</span><span class="s3">,</span>
            <span class="s1">algorithm=algorithm</span><span class="s3">,</span>
            <span class="s1">regularization=regularization</span><span class="s3">,</span>
            <span class="s1">copy_cov=copy_cov</span><span class="s3">,</span>
            <span class="s1">init=init</span><span class="s3">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
            <span class="s1">positive=positive</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">code</span>

    <span class="s2"># Enter parallel code block</span>
    <span class="s1">n_samples = X.shape[</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">n_components = dictionary.shape[</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">code = np.empty((n_samples</span><span class="s3">, </span><span class="s1">n_components))</span>
    <span class="s1">slices = list(gen_even_slices(n_samples</span><span class="s3">, </span><span class="s1">effective_n_jobs(n_jobs)))</span>

    <span class="s1">code_views = Parallel(n_jobs=n_jobs</span><span class="s3">, </span><span class="s1">verbose=verbose)(</span>
        <span class="s1">delayed(_sparse_encode_precomputed)(</span>
            <span class="s1">X[this_slice]</span><span class="s3">,</span>
            <span class="s1">dictionary</span><span class="s3">,</span>
            <span class="s1">gram=gram</span><span class="s3">,</span>
            <span class="s1">cov=cov[:</span><span class="s3">, </span><span class="s1">this_slice] </span><span class="s3">if </span><span class="s1">cov </span><span class="s3">is not None else None,</span>
            <span class="s1">algorithm=algorithm</span><span class="s3">,</span>
            <span class="s1">regularization=regularization</span><span class="s3">,</span>
            <span class="s1">copy_cov=copy_cov</span><span class="s3">,</span>
            <span class="s1">init=init[this_slice] </span><span class="s3">if </span><span class="s1">init </span><span class="s3">is not None else None,</span>
            <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
            <span class="s1">positive=positive</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s3">for </span><span class="s1">this_slice </span><span class="s3">in </span><span class="s1">slices</span>
    <span class="s1">)</span>
    <span class="s3">for </span><span class="s1">this_slice</span><span class="s3">, </span><span class="s1">this_view </span><span class="s3">in </span><span class="s1">zip(slices</span><span class="s3">, </span><span class="s1">code_views):</span>
        <span class="s1">code[this_slice] = this_view</span>
    <span class="s3">return </span><span class="s1">code</span>


<span class="s3">def </span><span class="s1">_update_dict(</span>
    <span class="s1">dictionary</span><span class="s3">,</span>
    <span class="s1">Y</span><span class="s3">,</span>
    <span class="s1">code</span><span class="s3">,</span>
    <span class="s1">A=</span><span class="s3">None,</span>
    <span class="s1">B=</span><span class="s3">None,</span>
    <span class="s1">verbose=</span><span class="s3">False,</span>
    <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">positive=</span><span class="s3">False,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Update the dense dictionary factor in place. 
 
    Parameters 
    ---------- 
    dictionary : ndarray of shape (n_components, n_features) 
        Value of the dictionary at the previous iteration. 
 
    Y : ndarray of shape (n_samples, n_features) 
        Data matrix. 
 
    code : ndarray of shape (n_samples, n_components) 
        Sparse coding of the data against which to optimize the dictionary. 
 
    A : ndarray of shape (n_components, n_components), default=None 
        Together with `B`, sufficient stats of the online model to update the 
        dictionary. 
 
    B : ndarray of shape (n_features, n_components), default=None 
        Together with `A`, sufficient stats of the online model to update the 
        dictionary. 
 
    verbose: bool, default=False 
        Degree of output the procedure will print. 
 
    random_state : int, RandomState instance or None, default=None 
        Used for randomly initializing the dictionary. Pass an int for 
        reproducible results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    positive : bool, default=False 
        Whether to enforce positivity when finding the dictionary. 
 
        .. versionadded:: 0.20 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_components = code.shape</span>
    <span class="s1">random_state = check_random_state(random_state)</span>

    <span class="s3">if </span><span class="s1">A </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">A = code.T @ code</span>
    <span class="s3">if </span><span class="s1">B </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">B = Y.T @ code</span>

    <span class="s1">n_unused = </span><span class="s5">0</span>

    <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range(n_components):</span>
        <span class="s3">if </span><span class="s1">A[k</span><span class="s3">, </span><span class="s1">k] &gt; </span><span class="s5">1e-6</span><span class="s1">:</span>
            <span class="s2"># 1e-6 is arbitrary but consistent with the spams implementation</span>
            <span class="s1">dictionary[k] += (B[:</span><span class="s3">, </span><span class="s1">k] - A[k] @ dictionary) / A[k</span><span class="s3">, </span><span class="s1">k]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># kth atom is almost never used -&gt; sample a new one from the data</span>
            <span class="s1">newd = Y[random_state.choice(n_samples)]</span>

            <span class="s2"># add small noise to avoid making the sparse coding ill conditioned</span>
            <span class="s1">noise_level = </span><span class="s5">0.01 </span><span class="s1">* (newd.std() </span><span class="s3">or </span><span class="s5">1</span><span class="s1">)  </span><span class="s2"># avoid 0 std</span>
            <span class="s1">noise = random_state.normal(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">noise_level</span><span class="s3">, </span><span class="s1">size=len(newd))</span>

            <span class="s1">dictionary[k] = newd + noise</span>
            <span class="s1">code[:</span><span class="s3">, </span><span class="s1">k] = </span><span class="s5">0</span>
            <span class="s1">n_unused += </span><span class="s5">1</span>

        <span class="s3">if </span><span class="s1">positive:</span>
            <span class="s1">np.clip(dictionary[k]</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">out=dictionary[k])</span>

        <span class="s2"># Projection on the constraint set ||V_k|| &lt;= 1</span>
        <span class="s1">dictionary[k] /= max(linalg.norm(dictionary[k])</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">verbose </span><span class="s3">and </span><span class="s1">n_unused &gt; </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s1">print(</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">n_unused</span><span class="s3">} </span><span class="s4">unused atoms resampled.&quot;</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">_dict_learning(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">n_components</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">alpha</span><span class="s3">,</span>
    <span class="s1">max_iter</span><span class="s3">,</span>
    <span class="s1">tol</span><span class="s3">,</span>
    <span class="s1">method</span><span class="s3">,</span>
    <span class="s1">n_jobs</span><span class="s3">,</span>
    <span class="s1">dict_init</span><span class="s3">,</span>
    <span class="s1">code_init</span><span class="s3">,</span>
    <span class="s1">callback</span><span class="s3">,</span>
    <span class="s1">verbose</span><span class="s3">,</span>
    <span class="s1">random_state</span><span class="s3">,</span>
    <span class="s1">return_n_iter</span><span class="s3">,</span>
    <span class="s1">positive_dict</span><span class="s3">,</span>
    <span class="s1">positive_code</span><span class="s3">,</span>
    <span class="s1">method_max_iter</span><span class="s3">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Main dictionary learning algorithm&quot;&quot;&quot;</span>
    <span class="s1">t0 = time.time()</span>
    <span class="s2"># Init the code and the dictionary with SVD of Y</span>
    <span class="s3">if </span><span class="s1">code_init </span><span class="s3">is not None and </span><span class="s1">dict_init </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">code = np.array(code_init</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;F&quot;</span><span class="s1">)</span>
        <span class="s2"># Don't copy V, it will happen below</span>
        <span class="s1">dictionary = dict_init</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">code</span><span class="s3">, </span><span class="s1">S</span><span class="s3">, </span><span class="s1">dictionary = linalg.svd(X</span><span class="s3">, </span><span class="s1">full_matrices=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s2"># flip the initial code's sign to enforce deterministic output</span>
        <span class="s1">code</span><span class="s3">, </span><span class="s1">dictionary = svd_flip(code</span><span class="s3">, </span><span class="s1">dictionary)</span>
        <span class="s1">dictionary = S[:</span><span class="s3">, </span><span class="s1">np.newaxis] * dictionary</span>
    <span class="s1">r = len(dictionary)</span>
    <span class="s3">if </span><span class="s1">n_components &lt;= r:  </span><span class="s2"># True even if n_components=None</span>
        <span class="s1">code = code[:</span><span class="s3">, </span><span class="s1">:n_components]</span>
        <span class="s1">dictionary = dictionary[:n_components</span><span class="s3">, </span><span class="s1">:]</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">code = np.c_[code</span><span class="s3">, </span><span class="s1">np.zeros((len(code)</span><span class="s3">, </span><span class="s1">n_components - r))]</span>
        <span class="s1">dictionary = np.r_[</span>
            <span class="s1">dictionary</span><span class="s3">, </span><span class="s1">np.zeros((n_components - r</span><span class="s3">, </span><span class="s1">dictionary.shape[</span><span class="s5">1</span><span class="s1">]))</span>
        <span class="s1">]</span>

    <span class="s2"># Fortran-order dict better suited for the sparse coding which is the</span>
    <span class="s2"># bottleneck of this algorithm.</span>
    <span class="s1">dictionary = np.asfortranarray(dictionary)</span>

    <span class="s1">errors = []</span>
    <span class="s1">current_cost = np.nan</span>

    <span class="s3">if </span><span class="s1">verbose == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">print(</span><span class="s4">&quot;[dict_learning]&quot;</span><span class="s3">, </span><span class="s1">end=</span><span class="s4">&quot; &quot;</span><span class="s1">)</span>

    <span class="s2"># If max_iter is 0, number of iterations returned should be zero</span>
    <span class="s1">ii = -</span><span class="s5">1</span>

    <span class="s3">for </span><span class="s1">ii </span><span class="s3">in </span><span class="s1">range(max_iter):</span>
        <span class="s1">dt = time.time() - t0</span>
        <span class="s3">if </span><span class="s1">verbose == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">sys.stdout.write(</span><span class="s4">&quot;.&quot;</span><span class="s1">)</span>
            <span class="s1">sys.stdout.flush()</span>
        <span class="s3">elif </span><span class="s1">verbose:</span>
            <span class="s1">print(</span>
                <span class="s4">&quot;Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)&quot;</span>
                <span class="s1">% (ii</span><span class="s3">, </span><span class="s1">dt</span><span class="s3">, </span><span class="s1">dt / </span><span class="s5">60</span><span class="s3">, </span><span class="s1">current_cost)</span>
            <span class="s1">)</span>

        <span class="s2"># Update code</span>
        <span class="s1">code = sparse_encode(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">dictionary</span><span class="s3">,</span>
            <span class="s1">algorithm=method</span><span class="s3">,</span>
            <span class="s1">alpha=alpha</span><span class="s3">,</span>
            <span class="s1">init=code</span><span class="s3">,</span>
            <span class="s1">n_jobs=n_jobs</span><span class="s3">,</span>
            <span class="s1">positive=positive_code</span><span class="s3">,</span>
            <span class="s1">max_iter=method_max_iter</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s2"># Update dictionary in place</span>
        <span class="s1">_update_dict(</span>
            <span class="s1">dictionary</span><span class="s3">,</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">code</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
            <span class="s1">positive=positive_dict</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s2"># Cost function</span>
        <span class="s1">current_cost = </span><span class="s5">0.5 </span><span class="s1">* np.sum((X - code @ dictionary) ** </span><span class="s5">2</span><span class="s1">) + alpha * np.sum(</span>
            <span class="s1">np.abs(code)</span>
        <span class="s1">)</span>
        <span class="s1">errors.append(current_cost)</span>

        <span class="s3">if </span><span class="s1">ii &gt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">dE = errors[-</span><span class="s5">2</span><span class="s1">] - errors[-</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s2"># assert(dE &gt;= -tol * errors[-1])</span>
            <span class="s3">if </span><span class="s1">dE &lt; tol * errors[-</span><span class="s5">1</span><span class="s1">]:</span>
                <span class="s3">if </span><span class="s1">verbose == </span><span class="s5">1</span><span class="s1">:</span>
                    <span class="s2"># A line return</span>
                    <span class="s1">print(</span><span class="s4">&quot;&quot;</span><span class="s1">)</span>
                <span class="s3">elif </span><span class="s1">verbose:</span>
                    <span class="s1">print(</span><span class="s4">&quot;--- Convergence reached after %d iterations&quot; </span><span class="s1">% ii)</span>
                <span class="s3">break</span>
        <span class="s3">if </span><span class="s1">ii % </span><span class="s5">5 </span><span class="s1">== </span><span class="s5">0 </span><span class="s3">and </span><span class="s1">callback </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">callback(locals())</span>

    <span class="s3">if </span><span class="s1">return_n_iter:</span>
        <span class="s3">return </span><span class="s1">code</span><span class="s3">, </span><span class="s1">dictionary</span><span class="s3">, </span><span class="s1">errors</span><span class="s3">, </span><span class="s1">ii + </span><span class="s5">1</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">code</span><span class="s3">, </span><span class="s1">dictionary</span><span class="s3">, </span><span class="s1">errors</span>


<span class="s3">def </span><span class="s1">_check_warn_deprecated(param</span><span class="s3">, </span><span class="s1">name</span><span class="s3">, </span><span class="s1">default</span><span class="s3">, </span><span class="s1">additional_message=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s3">if </span><span class="s1">param != </span><span class="s4">&quot;deprecated&quot;</span><span class="s1">:</span>
        <span class="s1">msg = (</span>
            <span class="s4">f&quot;'</span><span class="s3">{</span><span class="s1">name</span><span class="s3">}</span><span class="s4">' is deprecated in version 1.1 and will be removed in version 1.4.&quot;</span>
        <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">additional_message:</span>
            <span class="s1">msg += </span><span class="s4">f&quot; </span><span class="s3">{</span><span class="s1">additional_message</span><span class="s3">}</span><span class="s4">&quot;</span>
        <span class="s1">warnings.warn(msg</span><span class="s3">, </span><span class="s1">FutureWarning)</span>
        <span class="s3">return </span><span class="s1">param</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">default</span>


<span class="s3">def </span><span class="s1">dict_learning_online(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">n_components=</span><span class="s5">2</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">alpha=</span><span class="s5">1</span><span class="s3">,</span>
    <span class="s1">n_iter=</span><span class="s4">&quot;deprecated&quot;</span><span class="s3">,</span>
    <span class="s1">max_iter=</span><span class="s3">None,</span>
    <span class="s1">return_code=</span><span class="s3">True,</span>
    <span class="s1">dict_init=</span><span class="s3">None,</span>
    <span class="s1">callback=</span><span class="s3">None,</span>
    <span class="s1">batch_size=</span><span class="s5">256</span><span class="s3">,</span>
    <span class="s1">verbose=</span><span class="s3">False,</span>
    <span class="s1">shuffle=</span><span class="s3">True,</span>
    <span class="s1">n_jobs=</span><span class="s3">None,</span>
    <span class="s1">method=</span><span class="s4">&quot;lars&quot;</span><span class="s3">,</span>
    <span class="s1">iter_offset=</span><span class="s4">&quot;deprecated&quot;</span><span class="s3">,</span>
    <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">return_inner_stats=</span><span class="s4">&quot;deprecated&quot;</span><span class="s3">,</span>
    <span class="s1">inner_stats=</span><span class="s4">&quot;deprecated&quot;</span><span class="s3">,</span>
    <span class="s1">return_n_iter=</span><span class="s4">&quot;deprecated&quot;</span><span class="s3">,</span>
    <span class="s1">positive_dict=</span><span class="s3">False,</span>
    <span class="s1">positive_code=</span><span class="s3">False,</span>
    <span class="s1">method_max_iter=</span><span class="s5">1000</span><span class="s3">,</span>
    <span class="s1">tol=</span><span class="s5">1e-3</span><span class="s3">,</span>
    <span class="s1">max_no_improvement=</span><span class="s5">10</span><span class="s3">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Solve a dictionary learning matrix factorization problem online. 
 
    Finds the best dictionary and the corresponding sparse code for 
    approximating the data matrix X by solving:: 
 
        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1 
                     (U,V) 
                     with || V_k ||_2 = 1 for all  0 &lt;= k &lt; n_components 
 
    where V is the dictionary and U is the sparse code. ||.||_Fro stands for 
    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm 
    which is the sum of the absolute values of all the entries in the matrix. 
    This is accomplished by repeatedly iterating over mini-batches by slicing 
    the input data. 
 
    Read more in the :ref:`User Guide &lt;DictionaryLearning&gt;`. 
 
    Parameters 
    ---------- 
    X : ndarray of shape (n_samples, n_features) 
        Data matrix. 
 
    n_components : int or None, default=2 
        Number of dictionary atoms to extract. If None, then ``n_components`` 
        is set to ``n_features``. 
 
    alpha : float, default=1 
        Sparsity controlling parameter. 
 
    n_iter : int, default=100 
        Number of mini-batch iterations to perform. 
 
        .. deprecated:: 1.1 
           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use 
           `max_iter` instead. 
 
    max_iter : int, default=None 
        Maximum number of iterations over the complete dataset before 
        stopping independently of any early stopping criterion heuristics. 
        If ``max_iter`` is not None, ``n_iter`` is ignored. 
 
        .. versionadded:: 1.1 
 
    return_code : bool, default=True 
        Whether to also return the code U or just the dictionary `V`. 
 
    dict_init : ndarray of shape (n_components, n_features), default=None 
        Initial values for the dictionary for warm restart scenarios. 
        If `None`, the initial values for the dictionary are created 
        with an SVD decomposition of the data via 
        :func:`~sklearn.utils.extmath.randomized_svd`. 
 
    callback : callable, default=None 
        A callable that gets invoked at the end of each iteration. 
 
    batch_size : int, default=256 
        The number of samples to take in each batch. 
 
        .. versionchanged:: 1.3 
           The default value of `batch_size` changed from 3 to 256 in version 1.3. 
 
    verbose : bool, default=False 
        To control the verbosity of the procedure. 
 
    shuffle : bool, default=True 
        Whether to shuffle the data before splitting it in batches. 
 
    n_jobs : int, default=None 
        Number of parallel jobs to run. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
    method : {'lars', 'cd'}, default='lars' 
        * `'lars'`: uses the least angle regression method to solve the lasso 
          problem (`linear_model.lars_path`); 
        * `'cd'`: uses the coordinate descent method to compute the 
          Lasso solution (`linear_model.Lasso`). Lars will be faster if 
          the estimated components are sparse. 
 
    iter_offset : int, default=0 
        Number of previous iterations completed on the dictionary used for 
        initialization. 
 
        .. deprecated:: 1.1 
           `iter_offset` serves internal purpose only and will be removed in 1.4. 
 
    random_state : int, RandomState instance or None, default=None 
        Used for initializing the dictionary when ``dict_init`` is not 
        specified, randomly shuffling the data when ``shuffle`` is set to 
        ``True``, and updating the dictionary. Pass an int for reproducible 
        results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    return_inner_stats : bool, default=False 
        Return the inner statistics A (dictionary covariance) and B 
        (data approximation). Useful to restart the algorithm in an 
        online setting. If `return_inner_stats` is `True`, `return_code` is 
        ignored. 
 
        .. deprecated:: 1.1 
           `return_inner_stats` serves internal purpose only and will be removed in 1.4. 
 
    inner_stats : tuple of (A, B) ndarrays, default=None 
        Inner sufficient statistics that are kept by the algorithm. 
        Passing them at initialization is useful in online settings, to 
        avoid losing the history of the evolution. 
        `A` `(n_components, n_components)` is the dictionary covariance matrix. 
        `B` `(n_features, n_components)` is the data approximation matrix. 
 
        .. deprecated:: 1.1 
           `inner_stats` serves internal purpose only and will be removed in 1.4. 
 
    return_n_iter : bool, default=False 
        Whether or not to return the number of iterations. 
 
        .. deprecated:: 1.1 
           `return_n_iter` will be removed in 1.4 and n_iter will never be returned. 
 
    positive_dict : bool, default=False 
        Whether to enforce positivity when finding the dictionary. 
 
        .. versionadded:: 0.20 
 
    positive_code : bool, default=False 
        Whether to enforce positivity when finding the code. 
 
        .. versionadded:: 0.20 
 
    method_max_iter : int, default=1000 
        Maximum number of iterations to perform when solving the lasso problem. 
 
        .. versionadded:: 0.22 
 
    tol : float, default=1e-3 
        Control early stopping based on the norm of the differences in the 
        dictionary between 2 steps. Used only if `max_iter` is not None. 
 
        To disable early stopping based on changes in the dictionary, set 
        `tol` to 0.0. 
 
        .. versionadded:: 1.1 
 
    max_no_improvement : int, default=10 
        Control early stopping based on the consecutive number of mini batches 
        that does not yield an improvement on the smoothed cost function. Used only if 
        `max_iter` is not None. 
 
        To disable convergence detection based on cost function, set 
        `max_no_improvement` to None. 
 
        .. versionadded:: 1.1 
 
    Returns 
    ------- 
    code : ndarray of shape (n_samples, n_components), 
        The sparse code (only returned if `return_code=True`). 
 
    dictionary : ndarray of shape (n_components, n_features), 
        The solutions to the dictionary learning problem. 
 
    n_iter : int 
        Number of iterations run. Returned only if `return_n_iter` is 
        set to `True`. 
 
    See Also 
    -------- 
    dict_learning : Solve a dictionary learning matrix factorization problem. 
    DictionaryLearning : Find a dictionary that sparsely encodes data. 
    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary 
        learning algorithm. 
    SparsePCA : Sparse Principal Components Analysis. 
    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis. 
    &quot;&quot;&quot;</span>
    <span class="s1">deps = (return_n_iter</span><span class="s3">, </span><span class="s1">return_inner_stats</span><span class="s3">, </span><span class="s1">iter_offset</span><span class="s3">, </span><span class="s1">inner_stats)</span>
    <span class="s3">if </span><span class="s1">max_iter </span><span class="s3">is not None and not </span><span class="s1">all(arg == </span><span class="s4">&quot;deprecated&quot; </span><span class="s3">for </span><span class="s1">arg </span><span class="s3">in </span><span class="s1">deps):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;The following arguments are incompatible with 'max_iter': &quot;</span>
            <span class="s4">&quot;return_n_iter, return_inner_stats, iter_offset, inner_stats&quot;</span>
        <span class="s1">)</span>

    <span class="s1">iter_offset = _check_warn_deprecated(iter_offset</span><span class="s3">, </span><span class="s4">&quot;iter_offset&quot;</span><span class="s3">, </span><span class="s1">default=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">return_inner_stats = _check_warn_deprecated(</span>
        <span class="s1">return_inner_stats</span><span class="s3">,</span>
        <span class="s4">&quot;return_inner_stats&quot;</span><span class="s3">,</span>
        <span class="s1">default=</span><span class="s3">False,</span>
        <span class="s1">additional_message=</span><span class="s4">&quot;From 1.4 inner_stats will never be returned.&quot;</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s1">inner_stats = _check_warn_deprecated(inner_stats</span><span class="s3">, </span><span class="s4">&quot;inner_stats&quot;</span><span class="s3">, </span><span class="s1">default=</span><span class="s3">None</span><span class="s1">)</span>
    <span class="s1">return_n_iter = _check_warn_deprecated(</span>
        <span class="s1">return_n_iter</span><span class="s3">,</span>
        <span class="s4">&quot;return_n_iter&quot;</span><span class="s3">,</span>
        <span class="s1">default=</span><span class="s3">False,</span>
        <span class="s1">additional_message=(</span>
            <span class="s4">&quot;From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and &quot;</span>
            <span class="s4">&quot;'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.&quot;</span>
        <span class="s1">)</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">max_iter </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">transform_algorithm = </span><span class="s4">&quot;lasso_&quot; </span><span class="s1">+ method</span>

        <span class="s1">est = MiniBatchDictionaryLearning(</span>
            <span class="s1">n_components=n_components</span><span class="s3">,</span>
            <span class="s1">alpha=alpha</span><span class="s3">,</span>
            <span class="s1">n_iter=n_iter</span><span class="s3">,</span>
            <span class="s1">n_jobs=n_jobs</span><span class="s3">,</span>
            <span class="s1">fit_algorithm=method</span><span class="s3">,</span>
            <span class="s1">batch_size=batch_size</span><span class="s3">,</span>
            <span class="s1">shuffle=shuffle</span><span class="s3">,</span>
            <span class="s1">dict_init=dict_init</span><span class="s3">,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
            <span class="s1">transform_algorithm=transform_algorithm</span><span class="s3">,</span>
            <span class="s1">transform_alpha=alpha</span><span class="s3">,</span>
            <span class="s1">positive_code=positive_code</span><span class="s3">,</span>
            <span class="s1">positive_dict=positive_dict</span><span class="s3">,</span>
            <span class="s1">transform_max_iter=method_max_iter</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
            <span class="s1">callback=callback</span><span class="s3">,</span>
            <span class="s1">tol=tol</span><span class="s3">,</span>
            <span class="s1">max_no_improvement=max_no_improvement</span><span class="s3">,</span>
        <span class="s1">).fit(X)</span>

        <span class="s3">if not </span><span class="s1">return_code:</span>
            <span class="s3">return </span><span class="s1">est.components_</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">code = est.transform(X)</span>
            <span class="s3">return </span><span class="s1">code</span><span class="s3">, </span><span class="s1">est.components_</span>

    <span class="s2"># TODO(1.4) remove the whole old behavior</span>
    <span class="s2"># Fallback to old behavior</span>

    <span class="s1">n_iter = _check_warn_deprecated(</span>
        <span class="s1">n_iter</span><span class="s3">, </span><span class="s4">&quot;n_iter&quot;</span><span class="s3">, </span><span class="s1">default=</span><span class="s5">100</span><span class="s3">, </span><span class="s1">additional_message=</span><span class="s4">&quot;Use 'max_iter' instead.&quot;</span>
    <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">n_components </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">n_components = X.shape[</span><span class="s5">1</span><span class="s1">]</span>

    <span class="s3">if </span><span class="s1">method </span><span class="s3">not in </span><span class="s1">(</span><span class="s4">&quot;lars&quot;</span><span class="s3">, </span><span class="s4">&quot;cd&quot;</span><span class="s1">):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Coding method not supported as a fit algorithm.&quot;</span><span class="s1">)</span>

    <span class="s1">_check_positive_coding(method</span><span class="s3">, </span><span class="s1">positive_code)</span>

    <span class="s1">method = </span><span class="s4">&quot;lasso_&quot; </span><span class="s1">+ method</span>

    <span class="s1">t0 = time.time()</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>
    <span class="s2"># Avoid integer division problems</span>
    <span class="s1">alpha = float(alpha)</span>
    <span class="s1">random_state = check_random_state(random_state)</span>

    <span class="s2"># Init V with SVD of X</span>
    <span class="s3">if </span><span class="s1">dict_init </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">dictionary = dict_init</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">_</span><span class="s3">, </span><span class="s1">S</span><span class="s3">, </span><span class="s1">dictionary = randomized_svd(X</span><span class="s3">, </span><span class="s1">n_components</span><span class="s3">, </span><span class="s1">random_state=random_state)</span>
        <span class="s1">dictionary = S[:</span><span class="s3">, </span><span class="s1">np.newaxis] * dictionary</span>
    <span class="s1">r = len(dictionary)</span>
    <span class="s3">if </span><span class="s1">n_components &lt;= r:</span>
        <span class="s1">dictionary = dictionary[:n_components</span><span class="s3">, </span><span class="s1">:]</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">dictionary = np.r_[</span>
            <span class="s1">dictionary</span><span class="s3">,</span>
            <span class="s1">np.zeros((n_components - r</span><span class="s3">, </span><span class="s1">dictionary.shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">dtype=dictionary.dtype)</span><span class="s3">,</span>
        <span class="s1">]</span>

    <span class="s3">if </span><span class="s1">verbose == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">print(</span><span class="s4">&quot;[dict_learning]&quot;</span><span class="s3">, </span><span class="s1">end=</span><span class="s4">&quot; &quot;</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">shuffle:</span>
        <span class="s1">X_train = X.copy()</span>
        <span class="s1">random_state.shuffle(X_train)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">X_train = X</span>

    <span class="s1">X_train = check_array(</span>
        <span class="s1">X_train</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s3">, </span><span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span>
    <span class="s1">)</span>

    <span class="s2"># Fortran-order dict better suited for the sparse coding which is the</span>
    <span class="s2"># bottleneck of this algorithm.</span>
    <span class="s1">dictionary = check_array(dictionary</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;F&quot;</span><span class="s3">, </span><span class="s1">dtype=X_train.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">dictionary = np.require(dictionary</span><span class="s3">, </span><span class="s1">requirements=</span><span class="s4">&quot;W&quot;</span><span class="s1">)</span>

    <span class="s1">batches = gen_batches(n_samples</span><span class="s3">, </span><span class="s1">batch_size)</span>
    <span class="s1">batches = itertools.cycle(batches)</span>

    <span class="s2"># The covariance of the dictionary</span>
    <span class="s3">if </span><span class="s1">inner_stats </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">A = np.zeros((n_components</span><span class="s3">, </span><span class="s1">n_components)</span><span class="s3">, </span><span class="s1">dtype=X_train.dtype)</span>
        <span class="s2"># The data approximation</span>
        <span class="s1">B = np.zeros((n_features</span><span class="s3">, </span><span class="s1">n_components)</span><span class="s3">, </span><span class="s1">dtype=X_train.dtype)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">A = inner_stats[</span><span class="s5">0</span><span class="s1">].copy()</span>
        <span class="s1">B = inner_stats[</span><span class="s5">1</span><span class="s1">].copy()</span>

    <span class="s2"># If n_iter is zero, we need to return zero.</span>
    <span class="s1">ii = iter_offset - </span><span class="s5">1</span>

    <span class="s3">for </span><span class="s1">ii</span><span class="s3">, </span><span class="s1">batch </span><span class="s3">in </span><span class="s1">zip(range(iter_offset</span><span class="s3">, </span><span class="s1">iter_offset + n_iter)</span><span class="s3">, </span><span class="s1">batches):</span>
        <span class="s1">this_X = X_train[batch]</span>
        <span class="s1">dt = time.time() - t0</span>
        <span class="s3">if </span><span class="s1">verbose == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">sys.stdout.write(</span><span class="s4">&quot;.&quot;</span><span class="s1">)</span>
            <span class="s1">sys.stdout.flush()</span>
        <span class="s3">elif </span><span class="s1">verbose:</span>
            <span class="s3">if </span><span class="s1">verbose &gt; </span><span class="s5">10 </span><span class="s3">or </span><span class="s1">ii % ceil(</span><span class="s5">100.0 </span><span class="s1">/ verbose) == </span><span class="s5">0</span><span class="s1">:</span>
                <span class="s1">print(</span>
                    <span class="s4">&quot;Iteration % 3i (elapsed time: % 3is, % 4.1fmn)&quot; </span><span class="s1">% (ii</span><span class="s3">, </span><span class="s1">dt</span><span class="s3">, </span><span class="s1">dt / </span><span class="s5">60</span><span class="s1">)</span>
                <span class="s1">)</span>

        <span class="s1">this_code = sparse_encode(</span>
            <span class="s1">this_X</span><span class="s3">,</span>
            <span class="s1">dictionary</span><span class="s3">,</span>
            <span class="s1">algorithm=method</span><span class="s3">,</span>
            <span class="s1">alpha=alpha</span><span class="s3">,</span>
            <span class="s1">n_jobs=n_jobs</span><span class="s3">,</span>
            <span class="s1">check_input=</span><span class="s3">False,</span>
            <span class="s1">positive=positive_code</span><span class="s3">,</span>
            <span class="s1">max_iter=method_max_iter</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s2"># Update the auxiliary variables</span>
        <span class="s3">if </span><span class="s1">ii &lt; batch_size - </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">theta = float((ii + </span><span class="s5">1</span><span class="s1">) * batch_size)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">theta = float(batch_size**</span><span class="s5">2 </span><span class="s1">+ ii + </span><span class="s5">1 </span><span class="s1">- batch_size)</span>
        <span class="s1">beta = (theta + </span><span class="s5">1 </span><span class="s1">- batch_size) / (theta + </span><span class="s5">1</span><span class="s1">)</span>

        <span class="s1">A *= beta</span>
        <span class="s1">A += np.dot(this_code.T</span><span class="s3">, </span><span class="s1">this_code)</span>
        <span class="s1">B *= beta</span>
        <span class="s1">B += np.dot(this_X.T</span><span class="s3">, </span><span class="s1">this_code)</span>

        <span class="s2"># Update dictionary in place</span>
        <span class="s1">_update_dict(</span>
            <span class="s1">dictionary</span><span class="s3">,</span>
            <span class="s1">this_X</span><span class="s3">,</span>
            <span class="s1">this_code</span><span class="s3">,</span>
            <span class="s1">A</span><span class="s3">,</span>
            <span class="s1">B</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
            <span class="s1">positive=positive_dict</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s2"># Maybe we need a stopping criteria based on the amount of</span>
        <span class="s2"># modification in the dictionary</span>
        <span class="s3">if </span><span class="s1">callback </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">callback(locals())</span>

    <span class="s3">if </span><span class="s1">return_inner_stats:</span>
        <span class="s3">if </span><span class="s1">return_n_iter:</span>
            <span class="s3">return </span><span class="s1">dictionary</span><span class="s3">, </span><span class="s1">(A</span><span class="s3">, </span><span class="s1">B)</span><span class="s3">, </span><span class="s1">ii - iter_offset + </span><span class="s5">1</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">dictionary</span><span class="s3">, </span><span class="s1">(A</span><span class="s3">, </span><span class="s1">B)</span>
    <span class="s3">if </span><span class="s1">return_code:</span>
        <span class="s3">if </span><span class="s1">verbose &gt; </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">print(</span><span class="s4">&quot;Learning code...&quot;</span><span class="s3">, </span><span class="s1">end=</span><span class="s4">&quot; &quot;</span><span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">verbose == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">print(</span><span class="s4">&quot;|&quot;</span><span class="s3">, </span><span class="s1">end=</span><span class="s4">&quot; &quot;</span><span class="s1">)</span>
        <span class="s1">code = sparse_encode(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">dictionary</span><span class="s3">,</span>
            <span class="s1">algorithm=method</span><span class="s3">,</span>
            <span class="s1">alpha=alpha</span><span class="s3">,</span>
            <span class="s1">n_jobs=n_jobs</span><span class="s3">,</span>
            <span class="s1">check_input=</span><span class="s3">False,</span>
            <span class="s1">positive=positive_code</span><span class="s3">,</span>
            <span class="s1">max_iter=method_max_iter</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">verbose &gt; </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">dt = time.time() - t0</span>
            <span class="s1">print(</span><span class="s4">&quot;done (total time: % 3is, % 4.1fmn)&quot; </span><span class="s1">% (dt</span><span class="s3">, </span><span class="s1">dt / </span><span class="s5">60</span><span class="s1">))</span>
        <span class="s3">if </span><span class="s1">return_n_iter:</span>
            <span class="s3">return </span><span class="s1">code</span><span class="s3">, </span><span class="s1">dictionary</span><span class="s3">, </span><span class="s1">ii - iter_offset + </span><span class="s5">1</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">code</span><span class="s3">, </span><span class="s1">dictionary</span>

    <span class="s3">if </span><span class="s1">return_n_iter:</span>
        <span class="s3">return </span><span class="s1">dictionary</span><span class="s3">, </span><span class="s1">ii - iter_offset + </span><span class="s5">1</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">dictionary</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s4">&quot;X&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;method&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;lars&quot;</span><span class="s3">, </span><span class="s4">&quot;cd&quot;</span><span class="s1">})]</span><span class="s3">,</span>
        <span class="s4">&quot;return_n_iter&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;method_max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">False,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">dict_learning(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">n_components</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">alpha</span><span class="s3">,</span>
    <span class="s1">max_iter=</span><span class="s5">100</span><span class="s3">,</span>
    <span class="s1">tol=</span><span class="s5">1e-8</span><span class="s3">,</span>
    <span class="s1">method=</span><span class="s4">&quot;lars&quot;</span><span class="s3">,</span>
    <span class="s1">n_jobs=</span><span class="s3">None,</span>
    <span class="s1">dict_init=</span><span class="s3">None,</span>
    <span class="s1">code_init=</span><span class="s3">None,</span>
    <span class="s1">callback=</span><span class="s3">None,</span>
    <span class="s1">verbose=</span><span class="s3">False,</span>
    <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">return_n_iter=</span><span class="s3">False,</span>
    <span class="s1">positive_dict=</span><span class="s3">False,</span>
    <span class="s1">positive_code=</span><span class="s3">False,</span>
    <span class="s1">method_max_iter=</span><span class="s5">1000</span><span class="s3">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Solve a dictionary learning matrix factorization problem. 
 
    Finds the best dictionary and the corresponding sparse code for 
    approximating the data matrix X by solving:: 
 
        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1 
                     (U,V) 
                    with || V_k ||_2 = 1 for all  0 &lt;= k &lt; n_components 
 
    where V is the dictionary and U is the sparse code. ||.||_Fro stands for 
    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm 
    which is the sum of the absolute values of all the entries in the matrix. 
 
    Read more in the :ref:`User Guide &lt;DictionaryLearning&gt;`. 
 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
        Data matrix. 
 
    n_components : int 
        Number of dictionary atoms to extract. 
 
    alpha : int or float 
        Sparsity controlling parameter. 
 
    max_iter : int, default=100 
        Maximum number of iterations to perform. 
 
    tol : float, default=1e-8 
        Tolerance for the stopping condition. 
 
    method : {'lars', 'cd'}, default='lars' 
        The method used: 
 
        * `'lars'`: uses the least angle regression method to solve the lasso 
           problem (`linear_model.lars_path`); 
        * `'cd'`: uses the coordinate descent method to compute the 
          Lasso solution (`linear_model.Lasso`). Lars will be faster if 
          the estimated components are sparse. 
 
    n_jobs : int, default=None 
        Number of parallel jobs to run. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
    dict_init : ndarray of shape (n_components, n_features), default=None 
        Initial value for the dictionary for warm restart scenarios. Only used 
        if `code_init` and `dict_init` are not None. 
 
    code_init : ndarray of shape (n_samples, n_components), default=None 
        Initial value for the sparse code for warm restart scenarios. Only used 
        if `code_init` and `dict_init` are not None. 
 
    callback : callable, default=None 
        Callable that gets invoked every five iterations. 
 
    verbose : bool, default=False 
        To control the verbosity of the procedure. 
 
    random_state : int, RandomState instance or None, default=None 
        Used for randomly initializing the dictionary. Pass an int for 
        reproducible results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    return_n_iter : bool, default=False 
        Whether or not to return the number of iterations. 
 
    positive_dict : bool, default=False 
        Whether to enforce positivity when finding the dictionary. 
 
        .. versionadded:: 0.20 
 
    positive_code : bool, default=False 
        Whether to enforce positivity when finding the code. 
 
        .. versionadded:: 0.20 
 
    method_max_iter : int, default=1000 
        Maximum number of iterations to perform. 
 
        .. versionadded:: 0.22 
 
    Returns 
    ------- 
    code : ndarray of shape (n_samples, n_components) 
        The sparse code factor in the matrix factorization. 
 
    dictionary : ndarray of shape (n_components, n_features), 
        The dictionary factor in the matrix factorization. 
 
    errors : array 
        Vector of errors at each iteration. 
 
    n_iter : int 
        Number of iterations run. Returned only if `return_n_iter` is 
        set to True. 
 
    See Also 
    -------- 
    dict_learning_online : Solve a dictionary learning matrix factorization 
        problem online. 
    DictionaryLearning : Find a dictionary that sparsely encodes data. 
    MiniBatchDictionaryLearning : A faster, less accurate version 
        of the dictionary learning algorithm. 
    SparsePCA : Sparse Principal Components Analysis. 
    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis. 
    &quot;&quot;&quot;</span>
    <span class="s1">estimator = DictionaryLearning(</span>
        <span class="s1">n_components=n_components</span><span class="s3">,</span>
        <span class="s1">alpha=alpha</span><span class="s3">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
        <span class="s1">tol=tol</span><span class="s3">,</span>
        <span class="s1">fit_algorithm=method</span><span class="s3">,</span>
        <span class="s1">n_jobs=n_jobs</span><span class="s3">,</span>
        <span class="s1">dict_init=dict_init</span><span class="s3">,</span>
        <span class="s1">callback=callback</span><span class="s3">,</span>
        <span class="s1">code_init=code_init</span><span class="s3">,</span>
        <span class="s1">verbose=verbose</span><span class="s3">,</span>
        <span class="s1">random_state=random_state</span><span class="s3">,</span>
        <span class="s1">positive_code=positive_code</span><span class="s3">,</span>
        <span class="s1">positive_dict=positive_dict</span><span class="s3">,</span>
        <span class="s1">transform_max_iter=method_max_iter</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s1">code = estimator.fit_transform(X)</span>
    <span class="s3">if </span><span class="s1">return_n_iter:</span>
        <span class="s3">return </span><span class="s1">(</span>
            <span class="s1">code</span><span class="s3">,</span>
            <span class="s1">estimator.components_</span><span class="s3">,</span>
            <span class="s1">estimator.error_</span><span class="s3">,</span>
            <span class="s1">estimator.n_iter_</span><span class="s3">,</span>
        <span class="s1">)</span>
    <span class="s3">return </span><span class="s1">code</span><span class="s3">, </span><span class="s1">estimator.components_</span><span class="s3">, </span><span class="s1">estimator.error_</span>


<span class="s3">class </span><span class="s1">_BaseSparseCoding(ClassNamePrefixFeaturesOutMixin</span><span class="s3">, </span><span class="s1">TransformerMixin):</span>
    <span class="s0">&quot;&quot;&quot;Base class from SparseCoder and DictionaryLearning algorithms.&quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">transform_algorithm</span><span class="s3">,</span>
        <span class="s1">transform_n_nonzero_coefs</span><span class="s3">,</span>
        <span class="s1">transform_alpha</span><span class="s3">,</span>
        <span class="s1">split_sign</span><span class="s3">,</span>
        <span class="s1">n_jobs</span><span class="s3">,</span>
        <span class="s1">positive_code</span><span class="s3">,</span>
        <span class="s1">transform_max_iter</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">self.transform_algorithm = transform_algorithm</span>
        <span class="s1">self.transform_n_nonzero_coefs = transform_n_nonzero_coefs</span>
        <span class="s1">self.transform_alpha = transform_alpha</span>
        <span class="s1">self.transform_max_iter = transform_max_iter</span>
        <span class="s1">self.split_sign = split_sign</span>
        <span class="s1">self.n_jobs = n_jobs</span>
        <span class="s1">self.positive_code = positive_code</span>

    <span class="s3">def </span><span class="s1">_transform(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dictionary):</span>
        <span class="s0">&quot;&quot;&quot;Private method allowing to accommodate both DictionaryLearning and 
        SparseCoder.&quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;alpha&quot;</span><span class="s1">) </span><span class="s3">and </span><span class="s1">self.transform_alpha </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">transform_alpha = self.alpha</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">transform_alpha = self.transform_alpha</span>

        <span class="s1">code = sparse_encode(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">dictionary</span><span class="s3">,</span>
            <span class="s1">algorithm=self.transform_algorithm</span><span class="s3">,</span>
            <span class="s1">n_nonzero_coefs=self.transform_n_nonzero_coefs</span><span class="s3">,</span>
            <span class="s1">alpha=transform_alpha</span><span class="s3">,</span>
            <span class="s1">max_iter=self.transform_max_iter</span><span class="s3">,</span>
            <span class="s1">n_jobs=self.n_jobs</span><span class="s3">,</span>
            <span class="s1">positive=self.positive_code</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.split_sign:</span>
            <span class="s2"># feature vector is split into a positive and negative side</span>
            <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = code.shape</span>
            <span class="s1">split_code = np.empty((n_samples</span><span class="s3">, </span><span class="s5">2 </span><span class="s1">* n_features))</span>
            <span class="s1">split_code[:</span><span class="s3">, </span><span class="s1">:n_features] = np.maximum(code</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
            <span class="s1">split_code[:</span><span class="s3">, </span><span class="s1">n_features:] = -np.minimum(code</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
            <span class="s1">code = split_code</span>

        <span class="s3">return </span><span class="s1">code</span>

    <span class="s3">def </span><span class="s1">transform(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Encode the data as a sparse combination of the dictionary atoms. 
 
        Coding method is determined by the object parameter 
        `transform_algorithm`. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples, n_features) 
            Test data to be transformed, must have the same number of 
            features as the data used to train the model. 
 
        Returns 
        ------- 
        X_new : ndarray of shape (n_samples, n_components) 
            Transformed data. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">return </span><span class="s1">self._transform(X</span><span class="s3">, </span><span class="s1">self.components_)</span>


<span class="s3">class </span><span class="s1">SparseCoder(_BaseSparseCoding</span><span class="s3">, </span><span class="s1">BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Sparse coding. 
 
    Finds a sparse representation of data against a fixed, precomputed 
    dictionary. 
 
    Each row of the result is the solution to a sparse coding problem. 
    The goal is to find a sparse array `code` such that:: 
 
        X ~= code * dictionary 
 
    Read more in the :ref:`User Guide &lt;SparseCoder&gt;`. 
 
    Parameters 
    ---------- 
    dictionary : ndarray of shape (n_components, n_features) 
        The dictionary atoms used for sparse coding. Lines are assumed to be 
        normalized to unit norm. 
 
    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \ 
            'threshold'}, default='omp' 
        Algorithm used to transform the data: 
 
        - `'lars'`: uses the least angle regression method 
          (`linear_model.lars_path`); 
        - `'lasso_lars'`: uses Lars to compute the Lasso solution; 
        - `'lasso_cd'`: uses the coordinate descent method to compute the 
          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if 
          the estimated components are sparse; 
        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse 
          solution; 
        - `'threshold'`: squashes to zero all coefficients less than alpha from 
          the projection ``dictionary * X'``. 
 
    transform_n_nonzero_coefs : int, default=None 
        Number of nonzero coefficients to target in each column of the 
        solution. This is only used by `algorithm='lars'` and `algorithm='omp'` 
        and is overridden by `alpha` in the `omp` case. If `None`, then 
        `transform_n_nonzero_coefs=int(n_features / 10)`. 
 
    transform_alpha : float, default=None 
        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the 
        penalty applied to the L1 norm. 
        If `algorithm='threshold'`, `alpha` is the absolute value of the 
        threshold below which coefficients will be squashed to zero. 
        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of 
        the reconstruction error targeted. In this case, it overrides 
        `n_nonzero_coefs`. 
        If `None`, default to 1. 
 
    split_sign : bool, default=False 
        Whether to split the sparse feature vector into the concatenation of 
        its negative part and its positive part. This can improve the 
        performance of downstream classifiers. 
 
    n_jobs : int, default=None 
        Number of parallel jobs to run. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
    positive_code : bool, default=False 
        Whether to enforce positivity when finding the code. 
 
        .. versionadded:: 0.20 
 
    transform_max_iter : int, default=1000 
        Maximum number of iterations to perform if `algorithm='lasso_cd'` or 
        `lasso_lars`. 
 
        .. versionadded:: 0.22 
 
    Attributes 
    ---------- 
    n_components_ : int 
        Number of atoms. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    DictionaryLearning : Find a dictionary that sparsely encodes data. 
    MiniBatchDictionaryLearning : A faster, less accurate, version of the 
        dictionary learning algorithm. 
    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis. 
    SparsePCA : Sparse Principal Components Analysis. 
    sparse_encode : Sparse coding where each row of the result is the solution 
        to a sparse coding problem. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.decomposition import SparseCoder 
    &gt;&gt;&gt; X = np.array([[-1, -1, -1], [0, 0, 3]]) 
    &gt;&gt;&gt; dictionary = np.array( 
    ...     [[0, 1, 0], 
    ...      [-1, -1, 2], 
    ...      [1, 1, 1], 
    ...      [0, 1, 1], 
    ...      [0, 2, 1]], 
    ...    dtype=np.float64 
    ... ) 
    &gt;&gt;&gt; coder = SparseCoder( 
    ...     dictionary=dictionary, transform_algorithm='lasso_lars', 
    ...     transform_alpha=1e-10, 
    ... ) 
    &gt;&gt;&gt; coder.transform(X) 
    array([[ 0.,  0., -1.,  0.,  0.], 
           [ 0.,  1.,  1.,  0.,  0.]]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_required_parameters = [</span><span class="s4">&quot;dictionary&quot;</span><span class="s1">]</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">dictionary</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">transform_algorithm=</span><span class="s4">&quot;omp&quot;</span><span class="s3">,</span>
        <span class="s1">transform_n_nonzero_coefs=</span><span class="s3">None,</span>
        <span class="s1">transform_alpha=</span><span class="s3">None,</span>
        <span class="s1">split_sign=</span><span class="s3">False,</span>
        <span class="s1">n_jobs=</span><span class="s3">None,</span>
        <span class="s1">positive_code=</span><span class="s3">False,</span>
        <span class="s1">transform_max_iter=</span><span class="s5">1000</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">transform_algorithm</span><span class="s3">,</span>
            <span class="s1">transform_n_nonzero_coefs</span><span class="s3">,</span>
            <span class="s1">transform_alpha</span><span class="s3">,</span>
            <span class="s1">split_sign</span><span class="s3">,</span>
            <span class="s1">n_jobs</span><span class="s3">,</span>
            <span class="s1">positive_code</span><span class="s3">,</span>
            <span class="s1">transform_max_iter</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">self.dictionary = dictionary</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Do nothing and return the estimator unchanged. 
 
        This method is just there to implement the usual API and hence 
        work in pipelines. 
 
        Parameters 
        ---------- 
        X : Ignored 
            Not used, present for API consistency by convention. 
 
        y : Ignored 
            Not used, present for API consistency by convention. 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">transform(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Encode the data as a sparse combination of the dictionary atoms. 
 
        Coding method is determined by the object parameter 
        `transform_algorithm`. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples, n_features) 
            Training vector, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        y : Ignored 
            Not used, present for API consistency by convention. 
 
        Returns 
        ------- 
        X_new : ndarray of shape (n_samples, n_components) 
            Transformed data. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">super()._transform(X</span><span class="s3">, </span><span class="s1">self.dictionary)</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span>
            <span class="s4">&quot;requires_fit&quot;</span><span class="s1">: </span><span class="s3">False,</span>
            <span class="s4">&quot;preserves_dtype&quot;</span><span class="s1">: [np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">,</span>
        <span class="s1">}</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">n_components_(self):</span>
        <span class="s0">&quot;&quot;&quot;Number of atoms.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.dictionary.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">n_features_in_(self):</span>
        <span class="s0">&quot;&quot;&quot;Number of features seen during `fit`.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.dictionary.shape[</span><span class="s5">1</span><span class="s1">]</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">_n_features_out(self):</span>
        <span class="s0">&quot;&quot;&quot;Number of transformed output features.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.n_components_</span>


<span class="s3">class </span><span class="s1">DictionaryLearning(_BaseSparseCoding</span><span class="s3">, </span><span class="s1">BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Dictionary learning. 
 
    Finds a dictionary (a set of atoms) that performs well at sparsely 
    encoding the fitted data. 
 
    Solves the optimization problem:: 
 
        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1 
                    (U,V) 
                    with || V_k ||_2 &lt;= 1 for all  0 &lt;= k &lt; n_components 
 
    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for 
    the entry-wise matrix norm which is the sum of the absolute values 
    of all the entries in the matrix. 
 
    Read more in the :ref:`User Guide &lt;DictionaryLearning&gt;`. 
 
    Parameters 
    ---------- 
    n_components : int, default=None 
        Number of dictionary elements to extract. If None, then ``n_components`` 
        is set to ``n_features``. 
 
    alpha : float, default=1.0 
        Sparsity controlling parameter. 
 
    max_iter : int, default=1000 
        Maximum number of iterations to perform. 
 
    tol : float, default=1e-8 
        Tolerance for numerical error. 
 
    fit_algorithm : {'lars', 'cd'}, default='lars' 
        * `'lars'`: uses the least angle regression method to solve the lasso 
          problem (:func:`~sklearn.linear_model.lars_path`); 
        * `'cd'`: uses the coordinate descent method to compute the 
          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be 
          faster if the estimated components are sparse. 
 
        .. versionadded:: 0.17 
           *cd* coordinate descent method to improve speed. 
 
    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \ 
            'threshold'}, default='omp' 
        Algorithm used to transform the data: 
 
        - `'lars'`: uses the least angle regression method 
          (:func:`~sklearn.linear_model.lars_path`); 
        - `'lasso_lars'`: uses Lars to compute the Lasso solution. 
        - `'lasso_cd'`: uses the coordinate descent method to compute the 
          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'` 
          will be faster if the estimated components are sparse. 
        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse 
          solution. 
        - `'threshold'`: squashes to zero all coefficients less than alpha from 
          the projection ``dictionary * X'``. 
 
        .. versionadded:: 0.17 
           *lasso_cd* coordinate descent method to improve speed. 
 
    transform_n_nonzero_coefs : int, default=None 
        Number of nonzero coefficients to target in each column of the 
        solution. This is only used by `algorithm='lars'` and 
        `algorithm='omp'`. If `None`, then 
        `transform_n_nonzero_coefs=int(n_features / 10)`. 
 
    transform_alpha : float, default=None 
        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the 
        penalty applied to the L1 norm. 
        If `algorithm='threshold'`, `alpha` is the absolute value of the 
        threshold below which coefficients will be squashed to zero. 
        If `None`, defaults to `alpha`. 
 
        .. versionchanged:: 1.2 
            When None, default value changed from 1.0 to `alpha`. 
 
    n_jobs : int or None, default=None 
        Number of parallel jobs to run. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
    code_init : ndarray of shape (n_samples, n_components), default=None 
        Initial value for the code, for warm restart. Only used if `code_init` 
        and `dict_init` are not None. 
 
    dict_init : ndarray of shape (n_components, n_features), default=None 
        Initial values for the dictionary, for warm restart. Only used if 
        `code_init` and `dict_init` are not None. 
 
    callback : callable, default=None 
        Callable that gets invoked every five iterations. 
 
        .. versionadded:: 1.3 
 
    verbose : bool, default=False 
        To control the verbosity of the procedure. 
 
    split_sign : bool, default=False 
        Whether to split the sparse feature vector into the concatenation of 
        its negative part and its positive part. This can improve the 
        performance of downstream classifiers. 
 
    random_state : int, RandomState instance or None, default=None 
        Used for initializing the dictionary when ``dict_init`` is not 
        specified, randomly shuffling the data when ``shuffle`` is set to 
        ``True``, and updating the dictionary. Pass an int for reproducible 
        results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    positive_code : bool, default=False 
        Whether to enforce positivity when finding the code. 
 
        .. versionadded:: 0.20 
 
    positive_dict : bool, default=False 
        Whether to enforce positivity when finding the dictionary. 
 
        .. versionadded:: 0.20 
 
    transform_max_iter : int, default=1000 
        Maximum number of iterations to perform if `algorithm='lasso_cd'` or 
        `'lasso_lars'`. 
 
        .. versionadded:: 0.22 
 
    Attributes 
    ---------- 
    components_ : ndarray of shape (n_components, n_features) 
        dictionary atoms extracted from the data 
 
    error_ : array 
        vector of errors at each iteration 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    n_iter_ : int 
        Number of iterations run. 
 
    See Also 
    -------- 
    MiniBatchDictionaryLearning: A faster, less accurate, version of the 
        dictionary learning algorithm. 
    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis. 
    SparseCoder : Find a sparse representation of data from a fixed, 
        precomputed dictionary. 
    SparsePCA : Sparse Principal Components Analysis. 
 
    References 
    ---------- 
 
    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning 
    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf) 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.datasets import make_sparse_coded_signal 
    &gt;&gt;&gt; from sklearn.decomposition import DictionaryLearning 
    &gt;&gt;&gt; X, dictionary, code = make_sparse_coded_signal( 
    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10, 
    ...     random_state=42, 
    ... ) 
    &gt;&gt;&gt; dict_learner = DictionaryLearning( 
    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1, 
    ...     random_state=42, 
    ... ) 
    &gt;&gt;&gt; X_transformed = dict_learner.fit(X).transform(X) 
 
    We can check the level of sparsity of `X_transformed`: 
 
    &gt;&gt;&gt; np.mean(X_transformed == 0) 
    0.52... 
 
    We can compare the average squared euclidean norm of the reconstruction 
    error of the sparse coded signal relative to the squared euclidean norm of 
    the original signal: 
 
    &gt;&gt;&gt; X_hat = X_transformed @ dict_learner.components_ 
    &gt;&gt;&gt; np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1)) 
    0.05... 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;n_components&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;alpha&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;fit_algorithm&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;lars&quot;</span><span class="s3">, </span><span class="s4">&quot;cd&quot;</span><span class="s1">})]</span><span class="s3">,</span>
        <span class="s4">&quot;transform_algorithm&quot;</span><span class="s1">: [</span>
            <span class="s1">StrOptions({</span><span class="s4">&quot;lasso_lars&quot;</span><span class="s3">, </span><span class="s4">&quot;lasso_cd&quot;</span><span class="s3">, </span><span class="s4">&quot;lars&quot;</span><span class="s3">, </span><span class="s4">&quot;omp&quot;</span><span class="s3">, </span><span class="s4">&quot;threshold&quot;</span><span class="s1">})</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;transform_n_nonzero_coefs&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;transform_alpha&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_jobs&quot;</span><span class="s1">: [Integral</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;code_init&quot;</span><span class="s1">: [np.ndarray</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;dict_init&quot;</span><span class="s1">: [np.ndarray</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;callback&quot;</span><span class="s1">: [callable</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s4">&quot;verbose&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;split_sign&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;positive_code&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;positive_dict&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;transform_max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">n_components=</span><span class="s3">None,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">alpha=</span><span class="s5">1</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s5">1000</span><span class="s3">,</span>
        <span class="s1">tol=</span><span class="s5">1e-8</span><span class="s3">,</span>
        <span class="s1">fit_algorithm=</span><span class="s4">&quot;lars&quot;</span><span class="s3">,</span>
        <span class="s1">transform_algorithm=</span><span class="s4">&quot;omp&quot;</span><span class="s3">,</span>
        <span class="s1">transform_n_nonzero_coefs=</span><span class="s3">None,</span>
        <span class="s1">transform_alpha=</span><span class="s3">None,</span>
        <span class="s1">n_jobs=</span><span class="s3">None,</span>
        <span class="s1">code_init=</span><span class="s3">None,</span>
        <span class="s1">dict_init=</span><span class="s3">None,</span>
        <span class="s1">callback=</span><span class="s3">None,</span>
        <span class="s1">verbose=</span><span class="s3">False,</span>
        <span class="s1">split_sign=</span><span class="s3">False,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
        <span class="s1">positive_code=</span><span class="s3">False,</span>
        <span class="s1">positive_dict=</span><span class="s3">False,</span>
        <span class="s1">transform_max_iter=</span><span class="s5">1000</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">transform_algorithm</span><span class="s3">,</span>
            <span class="s1">transform_n_nonzero_coefs</span><span class="s3">,</span>
            <span class="s1">transform_alpha</span><span class="s3">,</span>
            <span class="s1">split_sign</span><span class="s3">,</span>
            <span class="s1">n_jobs</span><span class="s3">,</span>
            <span class="s1">positive_code</span><span class="s3">,</span>
            <span class="s1">transform_max_iter</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">self.n_components = n_components</span>
        <span class="s1">self.alpha = alpha</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.fit_algorithm = fit_algorithm</span>
        <span class="s1">self.code_init = code_init</span>
        <span class="s1">self.dict_init = dict_init</span>
        <span class="s1">self.callback = callback</span>
        <span class="s1">self.verbose = verbose</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.positive_dict = positive_dict</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model from data in X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training vector, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        y : Ignored 
            Not used, present for API consistency by convention. 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.fit_transform(X)</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit_transform(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model from data in X and return the transformed data. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training vector, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        y : Ignored 
            Not used, present for API consistency by convention. 
 
        Returns 
        ------- 
        V : ndarray of shape (n_samples, n_components) 
            Transformed data. 
        &quot;&quot;&quot;</span>
        <span class="s1">_check_positive_coding(method=self.fit_algorithm</span><span class="s3">, </span><span class="s1">positive=self.positive_code)</span>

        <span class="s1">method = </span><span class="s4">&quot;lasso_&quot; </span><span class="s1">+ self.fit_algorithm</span>

        <span class="s1">random_state = check_random_state(self.random_state)</span>
        <span class="s1">X = self._validate_data(X)</span>

        <span class="s3">if </span><span class="s1">self.n_components </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">n_components = X.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">n_components = self.n_components</span>

        <span class="s1">V</span><span class="s3">, </span><span class="s1">U</span><span class="s3">, </span><span class="s1">E</span><span class="s3">, </span><span class="s1">self.n_iter_ = _dict_learning(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">n_components</span><span class="s3">,</span>
            <span class="s1">alpha=self.alpha</span><span class="s3">,</span>
            <span class="s1">tol=self.tol</span><span class="s3">,</span>
            <span class="s1">max_iter=self.max_iter</span><span class="s3">,</span>
            <span class="s1">method=method</span><span class="s3">,</span>
            <span class="s1">method_max_iter=self.transform_max_iter</span><span class="s3">,</span>
            <span class="s1">n_jobs=self.n_jobs</span><span class="s3">,</span>
            <span class="s1">code_init=self.code_init</span><span class="s3">,</span>
            <span class="s1">dict_init=self.dict_init</span><span class="s3">,</span>
            <span class="s1">callback=self.callback</span><span class="s3">,</span>
            <span class="s1">verbose=self.verbose</span><span class="s3">,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
            <span class="s1">return_n_iter=</span><span class="s3">True,</span>
            <span class="s1">positive_dict=self.positive_dict</span><span class="s3">,</span>
            <span class="s1">positive_code=self.positive_code</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">self.components_ = U</span>
        <span class="s1">self.error_ = E</span>

        <span class="s3">return </span><span class="s1">V</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">_n_features_out(self):</span>
        <span class="s0">&quot;&quot;&quot;Number of transformed output features.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.components_.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span>
            <span class="s4">&quot;preserves_dtype&quot;</span><span class="s1">: [np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">,</span>
        <span class="s1">}</span>


<span class="s3">class </span><span class="s1">MiniBatchDictionaryLearning(_BaseSparseCoding</span><span class="s3">, </span><span class="s1">BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Mini-batch dictionary learning. 
 
    Finds a dictionary (a set of atoms) that performs well at sparsely 
    encoding the fitted data. 
 
    Solves the optimization problem:: 
 
       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1 
                    (U,V) 
                    with || V_k ||_2 &lt;= 1 for all  0 &lt;= k &lt; n_components 
 
    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for 
    the entry-wise matrix norm which is the sum of the absolute values 
    of all the entries in the matrix. 
 
    Read more in the :ref:`User Guide &lt;DictionaryLearning&gt;`. 
 
    Parameters 
    ---------- 
    n_components : int, default=None 
        Number of dictionary elements to extract. 
 
    alpha : float, default=1 
        Sparsity controlling parameter. 
 
    n_iter : int, default=1000 
        Total number of iterations over data batches to perform. 
 
        .. deprecated:: 1.1 
           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use 
           ``max_iter`` instead. 
 
    max_iter : int, default=None 
        Maximum number of iterations over the complete dataset before 
        stopping independently of any early stopping criterion heuristics. 
        If ``max_iter`` is not None, ``n_iter`` is ignored. 
 
        .. versionadded:: 1.1 
 
    fit_algorithm : {'lars', 'cd'}, default='lars' 
        The algorithm used: 
 
        - `'lars'`: uses the least angle regression method to solve the lasso 
          problem (`linear_model.lars_path`) 
        - `'cd'`: uses the coordinate descent method to compute the 
          Lasso solution (`linear_model.Lasso`). Lars will be faster if 
          the estimated components are sparse. 
 
    n_jobs : int, default=None 
        Number of parallel jobs to run. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
    batch_size : int, default=256 
        Number of samples in each mini-batch. 
 
        .. versionchanged:: 1.3 
           The default value of `batch_size` changed from 3 to 256 in version 1.3. 
 
    shuffle : bool, default=True 
        Whether to shuffle the samples before forming batches. 
 
    dict_init : ndarray of shape (n_components, n_features), default=None 
        Initial value of the dictionary for warm restart scenarios. 
 
    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \ 
            'threshold'}, default='omp' 
        Algorithm used to transform the data: 
 
        - `'lars'`: uses the least angle regression method 
          (`linear_model.lars_path`); 
        - `'lasso_lars'`: uses Lars to compute the Lasso solution. 
        - `'lasso_cd'`: uses the coordinate descent method to compute the 
          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster 
          if the estimated components are sparse. 
        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse 
          solution. 
        - `'threshold'`: squashes to zero all coefficients less than alpha from 
          the projection ``dictionary * X'``. 
 
    transform_n_nonzero_coefs : int, default=None 
        Number of nonzero coefficients to target in each column of the 
        solution. This is only used by `algorithm='lars'` and 
        `algorithm='omp'`. If `None`, then 
        `transform_n_nonzero_coefs=int(n_features / 10)`. 
 
    transform_alpha : float, default=None 
        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the 
        penalty applied to the L1 norm. 
        If `algorithm='threshold'`, `alpha` is the absolute value of the 
        threshold below which coefficients will be squashed to zero. 
        If `None`, defaults to `alpha`. 
 
        .. versionchanged:: 1.2 
            When None, default value changed from 1.0 to `alpha`. 
 
    verbose : bool or int, default=False 
        To control the verbosity of the procedure. 
 
    split_sign : bool, default=False 
        Whether to split the sparse feature vector into the concatenation of 
        its negative part and its positive part. This can improve the 
        performance of downstream classifiers. 
 
    random_state : int, RandomState instance or None, default=None 
        Used for initializing the dictionary when ``dict_init`` is not 
        specified, randomly shuffling the data when ``shuffle`` is set to 
        ``True``, and updating the dictionary. Pass an int for reproducible 
        results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    positive_code : bool, default=False 
        Whether to enforce positivity when finding the code. 
 
        .. versionadded:: 0.20 
 
    positive_dict : bool, default=False 
        Whether to enforce positivity when finding the dictionary. 
 
        .. versionadded:: 0.20 
 
    transform_max_iter : int, default=1000 
        Maximum number of iterations to perform if `algorithm='lasso_cd'` or 
        `'lasso_lars'`. 
 
        .. versionadded:: 0.22 
 
    callback : callable, default=None 
        A callable that gets invoked at the end of each iteration. 
 
        .. versionadded:: 1.1 
 
    tol : float, default=1e-3 
        Control early stopping based on the norm of the differences in the 
        dictionary between 2 steps. Used only if `max_iter` is not None. 
 
        To disable early stopping based on changes in the dictionary, set 
        `tol` to 0.0. 
 
        .. versionadded:: 1.1 
 
    max_no_improvement : int, default=10 
        Control early stopping based on the consecutive number of mini batches 
        that does not yield an improvement on the smoothed cost function. Used only if 
        `max_iter` is not None. 
 
        To disable convergence detection based on cost function, set 
        `max_no_improvement` to None. 
 
        .. versionadded:: 1.1 
 
    Attributes 
    ---------- 
    components_ : ndarray of shape (n_components, n_features) 
        Components extracted from the data. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    n_iter_ : int 
        Number of iterations over the full dataset. 
 
    n_steps_ : int 
        Number of mini-batches processed. 
 
        .. versionadded:: 1.1 
 
    See Also 
    -------- 
    DictionaryLearning : Find a dictionary that sparsely encodes data. 
    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis. 
    SparseCoder : Find a sparse representation of data from a fixed, 
        precomputed dictionary. 
    SparsePCA : Sparse Principal Components Analysis. 
 
    References 
    ---------- 
 
    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning 
    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf) 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.datasets import make_sparse_coded_signal 
    &gt;&gt;&gt; from sklearn.decomposition import MiniBatchDictionaryLearning 
    &gt;&gt;&gt; X, dictionary, code = make_sparse_coded_signal( 
    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10, 
    ...     random_state=42) 
    &gt;&gt;&gt; dict_learner = MiniBatchDictionaryLearning( 
    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars', 
    ...     transform_alpha=0.1, max_iter=20, random_state=42) 
    &gt;&gt;&gt; X_transformed = dict_learner.fit_transform(X) 
 
    We can check the level of sparsity of `X_transformed`: 
 
    &gt;&gt;&gt; np.mean(X_transformed == 0) &gt; 0.5 
    True 
 
    We can compare the average squared euclidean norm of the reconstruction 
    error of the sparse coded signal relative to the squared euclidean norm of 
    the original signal: 
 
    &gt;&gt;&gt; X_hat = X_transformed @ dict_learner.components_ 
    &gt;&gt;&gt; np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1)) 
    0.052... 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;n_components&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;alpha&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;n_iter&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">Hidden(StrOptions({</span><span class="s4">&quot;deprecated&quot;</span><span class="s1">}))</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;fit_algorithm&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;cd&quot;</span><span class="s3">, </span><span class="s4">&quot;lars&quot;</span><span class="s1">})]</span><span class="s3">,</span>
        <span class="s4">&quot;n_jobs&quot;</span><span class="s1">: [</span><span class="s3">None, </span><span class="s1">Integral]</span><span class="s3">,</span>
        <span class="s4">&quot;batch_size&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;shuffle&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;dict_init&quot;</span><span class="s1">: [</span><span class="s3">None, </span><span class="s1">np.ndarray]</span><span class="s3">,</span>
        <span class="s4">&quot;transform_algorithm&quot;</span><span class="s1">: [</span>
            <span class="s1">StrOptions({</span><span class="s4">&quot;lasso_lars&quot;</span><span class="s3">, </span><span class="s4">&quot;lasso_cd&quot;</span><span class="s3">, </span><span class="s4">&quot;lars&quot;</span><span class="s3">, </span><span class="s4">&quot;omp&quot;</span><span class="s3">, </span><span class="s4">&quot;threshold&quot;</span><span class="s1">})</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;transform_n_nonzero_coefs&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;transform_alpha&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s4">&quot;verbose&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;split_sign&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;positive_code&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;positive_dict&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;transform_max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;callback&quot;</span><span class="s1">: [</span><span class="s3">None, </span><span class="s1">callable]</span><span class="s3">,</span>
        <span class="s4">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;max_no_improvement&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">n_components=</span><span class="s3">None,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">alpha=</span><span class="s5">1</span><span class="s3">,</span>
        <span class="s1">n_iter=</span><span class="s4">&quot;deprecated&quot;</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s3">None,</span>
        <span class="s1">fit_algorithm=</span><span class="s4">&quot;lars&quot;</span><span class="s3">,</span>
        <span class="s1">n_jobs=</span><span class="s3">None,</span>
        <span class="s1">batch_size=</span><span class="s5">256</span><span class="s3">,</span>
        <span class="s1">shuffle=</span><span class="s3">True,</span>
        <span class="s1">dict_init=</span><span class="s3">None,</span>
        <span class="s1">transform_algorithm=</span><span class="s4">&quot;omp&quot;</span><span class="s3">,</span>
        <span class="s1">transform_n_nonzero_coefs=</span><span class="s3">None,</span>
        <span class="s1">transform_alpha=</span><span class="s3">None,</span>
        <span class="s1">verbose=</span><span class="s3">False,</span>
        <span class="s1">split_sign=</span><span class="s3">False,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
        <span class="s1">positive_code=</span><span class="s3">False,</span>
        <span class="s1">positive_dict=</span><span class="s3">False,</span>
        <span class="s1">transform_max_iter=</span><span class="s5">1000</span><span class="s3">,</span>
        <span class="s1">callback=</span><span class="s3">None,</span>
        <span class="s1">tol=</span><span class="s5">1e-3</span><span class="s3">,</span>
        <span class="s1">max_no_improvement=</span><span class="s5">10</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">transform_algorithm</span><span class="s3">,</span>
            <span class="s1">transform_n_nonzero_coefs</span><span class="s3">,</span>
            <span class="s1">transform_alpha</span><span class="s3">,</span>
            <span class="s1">split_sign</span><span class="s3">,</span>
            <span class="s1">n_jobs</span><span class="s3">,</span>
            <span class="s1">positive_code</span><span class="s3">,</span>
            <span class="s1">transform_max_iter</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">self.n_components = n_components</span>
        <span class="s1">self.alpha = alpha</span>
        <span class="s1">self.n_iter = n_iter</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.fit_algorithm = fit_algorithm</span>
        <span class="s1">self.dict_init = dict_init</span>
        <span class="s1">self.verbose = verbose</span>
        <span class="s1">self.shuffle = shuffle</span>
        <span class="s1">self.batch_size = batch_size</span>
        <span class="s1">self.split_sign = split_sign</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.positive_dict = positive_dict</span>
        <span class="s1">self.callback = callback</span>
        <span class="s1">self.max_no_improvement = max_no_improvement</span>
        <span class="s1">self.tol = tol</span>

    <span class="s3">def </span><span class="s1">_check_params(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s2"># n_components</span>
        <span class="s1">self._n_components = self.n_components</span>
        <span class="s3">if </span><span class="s1">self._n_components </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self._n_components = X.shape[</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s2"># fit_algorithm</span>
        <span class="s1">_check_positive_coding(self.fit_algorithm</span><span class="s3">, </span><span class="s1">self.positive_code)</span>
        <span class="s1">self._fit_algorithm = </span><span class="s4">&quot;lasso_&quot; </span><span class="s1">+ self.fit_algorithm</span>

        <span class="s2"># batch_size</span>
        <span class="s1">self._batch_size = min(self.batch_size</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s5">0</span><span class="s1">])</span>

    <span class="s3">def </span><span class="s1">_initialize_dict(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">random_state):</span>
        <span class="s0">&quot;&quot;&quot;Initialization of the dictionary.&quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self.dict_init </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">dictionary = self.dict_init</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># Init V with SVD of X</span>
            <span class="s1">_</span><span class="s3">, </span><span class="s1">S</span><span class="s3">, </span><span class="s1">dictionary = randomized_svd(</span>
                <span class="s1">X</span><span class="s3">, </span><span class="s1">self._n_components</span><span class="s3">, </span><span class="s1">random_state=random_state</span>
            <span class="s1">)</span>
            <span class="s1">dictionary = S[:</span><span class="s3">, </span><span class="s1">np.newaxis] * dictionary</span>

        <span class="s3">if </span><span class="s1">self._n_components &lt;= len(dictionary):</span>
            <span class="s1">dictionary = dictionary[: self._n_components</span><span class="s3">, </span><span class="s1">:]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">dictionary = np.concatenate(</span>
                <span class="s1">(</span>
                    <span class="s1">dictionary</span><span class="s3">,</span>
                    <span class="s1">np.zeros(</span>
                        <span class="s1">(self._n_components - len(dictionary)</span><span class="s3">, </span><span class="s1">dictionary.shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">,</span>
                        <span class="s1">dtype=dictionary.dtype</span><span class="s3">,</span>
                    <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">)</span>
            <span class="s1">)</span>

        <span class="s1">dictionary = check_array(dictionary</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;F&quot;</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">dictionary = np.require(dictionary</span><span class="s3">, </span><span class="s1">requirements=</span><span class="s4">&quot;W&quot;</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">dictionary</span>

    <span class="s3">def </span><span class="s1">_update_inner_stats(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">code</span><span class="s3">, </span><span class="s1">batch_size</span><span class="s3">, </span><span class="s1">step):</span>
        <span class="s0">&quot;&quot;&quot;Update the inner stats inplace.&quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">step &lt; batch_size - </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">theta = (step + </span><span class="s5">1</span><span class="s1">) * batch_size</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">theta = batch_size**</span><span class="s5">2 </span><span class="s1">+ step + </span><span class="s5">1 </span><span class="s1">- batch_size</span>
        <span class="s1">beta = (theta + </span><span class="s5">1 </span><span class="s1">- batch_size) / (theta + </span><span class="s5">1</span><span class="s1">)</span>

        <span class="s1">self._A *= beta</span>
        <span class="s1">self._A += code.T @ code / batch_size</span>
        <span class="s1">self._B *= beta</span>
        <span class="s1">self._B += X.T @ code / batch_size</span>

    <span class="s3">def </span><span class="s1">_minibatch_step(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dictionary</span><span class="s3">, </span><span class="s1">random_state</span><span class="s3">, </span><span class="s1">step):</span>
        <span class="s0">&quot;&quot;&quot;Perform the update on the dictionary for one minibatch.&quot;&quot;&quot;</span>
        <span class="s1">batch_size = X.shape[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s2"># Compute code for this batch</span>
        <span class="s1">code = _sparse_encode(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">dictionary</span><span class="s3">,</span>
            <span class="s1">algorithm=self._fit_algorithm</span><span class="s3">,</span>
            <span class="s1">alpha=self.alpha</span><span class="s3">,</span>
            <span class="s1">n_jobs=self.n_jobs</span><span class="s3">,</span>
            <span class="s1">positive=self.positive_code</span><span class="s3">,</span>
            <span class="s1">max_iter=self.transform_max_iter</span><span class="s3">,</span>
            <span class="s1">verbose=self.verbose</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s1">batch_cost = (</span>
            <span class="s5">0.5 </span><span class="s1">* ((X - code @ dictionary) ** </span><span class="s5">2</span><span class="s1">).sum()</span>
            <span class="s1">+ self.alpha * np.sum(np.abs(code))</span>
        <span class="s1">) / batch_size</span>

        <span class="s2"># Update inner stats</span>
        <span class="s1">self._update_inner_stats(X</span><span class="s3">, </span><span class="s1">code</span><span class="s3">, </span><span class="s1">batch_size</span><span class="s3">, </span><span class="s1">step)</span>

        <span class="s2"># Update dictionary</span>
        <span class="s1">_update_dict(</span>
            <span class="s1">dictionary</span><span class="s3">,</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">code</span><span class="s3">,</span>
            <span class="s1">self._A</span><span class="s3">,</span>
            <span class="s1">self._B</span><span class="s3">,</span>
            <span class="s1">verbose=self.verbose</span><span class="s3">,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
            <span class="s1">positive=self.positive_dict</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s3">return </span><span class="s1">batch_cost</span>

    <span class="s3">def </span><span class="s1">_check_convergence(</span>
        <span class="s1">self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">batch_cost</span><span class="s3">, </span><span class="s1">new_dict</span><span class="s3">, </span><span class="s1">old_dict</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">step</span><span class="s3">, </span><span class="s1">n_steps</span>
    <span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Helper function to encapsulate the early stopping logic. 
 
        Early stopping is based on two factors: 
        - A small change of the dictionary between two minibatch updates. This is 
          controlled by the tol parameter. 
        - No more improvement on a smoothed estimate of the objective function for a 
          a certain number of consecutive minibatch updates. This is controlled by 
          the max_no_improvement parameter. 
        &quot;&quot;&quot;</span>
        <span class="s1">batch_size = X.shape[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s2"># counts steps starting from 1 for user friendly verbose mode.</span>
        <span class="s1">step = step + </span><span class="s5">1</span>

        <span class="s2"># Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a</span>
        <span class="s2"># too bad value</span>
        <span class="s3">if </span><span class="s1">step &lt;= min(</span><span class="s5">100</span><span class="s3">, </span><span class="s1">n_samples / batch_size):</span>
            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">print(</span><span class="s4">f&quot;Minibatch step </span><span class="s3">{</span><span class="s1">step</span><span class="s3">}</span><span class="s4">/</span><span class="s3">{</span><span class="s1">n_steps</span><span class="s3">}</span><span class="s4">: mean batch cost: </span><span class="s3">{</span><span class="s1">batch_cost</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
            <span class="s3">return False</span>

        <span class="s2"># Compute an Exponentially Weighted Average of the cost function to</span>
        <span class="s2"># monitor the convergence while discarding minibatch-local stochastic</span>
        <span class="s2"># variability: https://en.wikipedia.org/wiki/Moving_average</span>
        <span class="s3">if </span><span class="s1">self._ewa_cost </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self._ewa_cost = batch_cost</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = batch_size / (n_samples + </span><span class="s5">1</span><span class="s1">)</span>
            <span class="s1">alpha = min(alpha</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
            <span class="s1">self._ewa_cost = self._ewa_cost * (</span><span class="s5">1 </span><span class="s1">- alpha) + batch_cost * alpha</span>

        <span class="s3">if </span><span class="s1">self.verbose:</span>
            <span class="s1">print(</span>
                <span class="s4">f&quot;Minibatch step </span><span class="s3">{</span><span class="s1">step</span><span class="s3">}</span><span class="s4">/</span><span class="s3">{</span><span class="s1">n_steps</span><span class="s3">}</span><span class="s4">: mean batch cost: &quot;</span>
                <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">batch_cost</span><span class="s3">}</span><span class="s4">, ewa cost: </span><span class="s3">{</span><span class="s1">self._ewa_cost</span><span class="s3">}</span><span class="s4">&quot;</span>
            <span class="s1">)</span>

        <span class="s2"># Early stopping based on change of dictionary</span>
        <span class="s1">dict_diff = linalg.norm(new_dict - old_dict) / self._n_components</span>
        <span class="s3">if </span><span class="s1">self.tol &gt; </span><span class="s5">0 </span><span class="s3">and </span><span class="s1">dict_diff &lt;= self.tol:</span>
            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">print(</span><span class="s4">f&quot;Converged (small dictionary change) at step </span><span class="s3">{</span><span class="s1">step</span><span class="s3">}</span><span class="s4">/</span><span class="s3">{</span><span class="s1">n_steps</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
            <span class="s3">return True</span>

        <span class="s2"># Early stopping heuristic due to lack of improvement on smoothed</span>
        <span class="s2"># cost function</span>
        <span class="s3">if </span><span class="s1">self._ewa_cost_min </span><span class="s3">is None or </span><span class="s1">self._ewa_cost &lt; self._ewa_cost_min:</span>
            <span class="s1">self._no_improvement = </span><span class="s5">0</span>
            <span class="s1">self._ewa_cost_min = self._ewa_cost</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._no_improvement += </span><span class="s5">1</span>

        <span class="s3">if </span><span class="s1">(</span>
            <span class="s1">self.max_no_improvement </span><span class="s3">is not None</span>
            <span class="s3">and </span><span class="s1">self._no_improvement &gt;= self.max_no_improvement</span>
        <span class="s1">):</span>
            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">print(</span>
                    <span class="s4">&quot;Converged (lack of improvement in objective function) &quot;</span>
                    <span class="s4">f&quot;at step </span><span class="s3">{</span><span class="s1">step</span><span class="s3">}</span><span class="s4">/</span><span class="s3">{</span><span class="s1">n_steps</span><span class="s3">}</span><span class="s4">&quot;</span>
                <span class="s1">)</span>
            <span class="s3">return True</span>

        <span class="s3">return False</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model from data in X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training vector, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        y : Ignored 
            Not used, present for API consistency by convention. 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span>
        <span class="s1">)</span>

        <span class="s1">self._check_params(X)</span>

        <span class="s3">if </span><span class="s1">self.n_iter != </span><span class="s4">&quot;deprecated&quot;</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s4">&quot;'n_iter' is deprecated in version 1.1 and will be removed &quot;</span>
                    <span class="s4">&quot;in version 1.4. Use 'max_iter' and let 'n_iter' to its default &quot;</span>
                    <span class="s4">&quot;value instead. 'n_iter' is also ignored if 'max_iter' is &quot;</span>
                    <span class="s4">&quot;specified.&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">FutureWarning</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">n_iter = self.n_iter</span>

        <span class="s1">self._random_state = check_random_state(self.random_state)</span>

        <span class="s1">dictionary = self._initialize_dict(X</span><span class="s3">, </span><span class="s1">self._random_state)</span>
        <span class="s1">old_dict = dictionary.copy()</span>

        <span class="s3">if </span><span class="s1">self.shuffle:</span>
            <span class="s1">X_train = X.copy()</span>
            <span class="s1">self._random_state.shuffle(X_train)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">X_train = X</span>

        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X_train.shape</span>

        <span class="s3">if </span><span class="s1">self.verbose:</span>
            <span class="s1">print(</span><span class="s4">&quot;[dict_learning]&quot;</span><span class="s1">)</span>

        <span class="s2"># Inner stats</span>
        <span class="s1">self._A = np.zeros(</span>
            <span class="s1">(self._n_components</span><span class="s3">, </span><span class="s1">self._n_components)</span><span class="s3">, </span><span class="s1">dtype=X_train.dtype</span>
        <span class="s1">)</span>
        <span class="s1">self._B = np.zeros((n_features</span><span class="s3">, </span><span class="s1">self._n_components)</span><span class="s3">, </span><span class="s1">dtype=X_train.dtype)</span>

        <span class="s3">if </span><span class="s1">self.max_iter </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s2"># Attributes to monitor the convergence</span>
            <span class="s1">self._ewa_cost = </span><span class="s3">None</span>
            <span class="s1">self._ewa_cost_min = </span><span class="s3">None</span>
            <span class="s1">self._no_improvement = </span><span class="s5">0</span>

            <span class="s1">batches = gen_batches(n_samples</span><span class="s3">, </span><span class="s1">self._batch_size)</span>
            <span class="s1">batches = itertools.cycle(batches)</span>
            <span class="s1">n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))</span>
            <span class="s1">n_steps = self.max_iter * n_steps_per_iter</span>

            <span class="s1">i = -</span><span class="s5">1  </span><span class="s2"># to allow max_iter = 0</span>

            <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">batch </span><span class="s3">in </span><span class="s1">zip(range(n_steps)</span><span class="s3">, </span><span class="s1">batches):</span>
                <span class="s1">X_batch = X_train[batch]</span>

                <span class="s1">batch_cost = self._minibatch_step(</span>
                    <span class="s1">X_batch</span><span class="s3">, </span><span class="s1">dictionary</span><span class="s3">, </span><span class="s1">self._random_state</span><span class="s3">, </span><span class="s1">i</span>
                <span class="s1">)</span>

                <span class="s3">if </span><span class="s1">self._check_convergence(</span>
                    <span class="s1">X_batch</span><span class="s3">, </span><span class="s1">batch_cost</span><span class="s3">, </span><span class="s1">dictionary</span><span class="s3">, </span><span class="s1">old_dict</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">i</span><span class="s3">, </span><span class="s1">n_steps</span>
                <span class="s1">):</span>
                    <span class="s3">break</span>

                <span class="s2"># XXX callback param added for backward compat in #18975 but a common</span>
                <span class="s2"># unified callback API should be preferred</span>
                <span class="s3">if </span><span class="s1">self.callback </span><span class="s3">is not None</span><span class="s1">:</span>
                    <span class="s1">self.callback(locals())</span>

                <span class="s1">old_dict[:] = dictionary</span>

            <span class="s1">self.n_steps_ = i + </span><span class="s5">1</span>
            <span class="s1">self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># TODO remove this branch in 1.4</span>
            <span class="s1">n_iter = </span><span class="s5">1000 </span><span class="s3">if </span><span class="s1">self.n_iter == </span><span class="s4">&quot;deprecated&quot; </span><span class="s3">else </span><span class="s1">self.n_iter</span>

            <span class="s1">batches = gen_batches(n_samples</span><span class="s3">, </span><span class="s1">self._batch_size)</span>
            <span class="s1">batches = itertools.cycle(batches)</span>

            <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">batch </span><span class="s3">in </span><span class="s1">zip(range(n_iter)</span><span class="s3">, </span><span class="s1">batches):</span>
                <span class="s1">self._minibatch_step(X_train[batch]</span><span class="s3">, </span><span class="s1">dictionary</span><span class="s3">, </span><span class="s1">self._random_state</span><span class="s3">, </span><span class="s1">i)</span>

                <span class="s1">trigger_verbose = self.verbose </span><span class="s3">and </span><span class="s1">i % ceil(</span><span class="s5">100.0 </span><span class="s1">/ self.verbose) == </span><span class="s5">0</span>
                <span class="s3">if </span><span class="s1">self.verbose &gt; </span><span class="s5">10 </span><span class="s3">or </span><span class="s1">trigger_verbose:</span>
                    <span class="s1">print(</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">i</span><span class="s3">} </span><span class="s4">batches processed.&quot;</span><span class="s1">)</span>

                <span class="s3">if </span><span class="s1">self.callback </span><span class="s3">is not None</span><span class="s1">:</span>
                    <span class="s1">self.callback(locals())</span>

            <span class="s1">self.n_steps_ = n_iter</span>
            <span class="s1">self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))</span>

        <span class="s1">self.components_ = dictionary</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">partial_fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Update the model using the data in X as a mini-batch. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training vector, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        y : Ignored 
            Not used, present for API consistency by convention. 
 
        Returns 
        ------- 
        self : object 
            Return the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">has_components = hasattr(self</span><span class="s3">, </span><span class="s4">&quot;components_&quot;</span><span class="s1">)</span>

        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">not </span><span class="s1">has_components</span>
        <span class="s1">)</span>

        <span class="s3">if not </span><span class="s1">has_components:</span>
            <span class="s2"># This instance has not been fitted yet (fit or partial_fit)</span>
            <span class="s1">self._check_params(X)</span>
            <span class="s1">self._random_state = check_random_state(self.random_state)</span>

            <span class="s1">dictionary = self._initialize_dict(X</span><span class="s3">, </span><span class="s1">self._random_state)</span>

            <span class="s1">self.n_steps_ = </span><span class="s5">0</span>

            <span class="s1">self._A = np.zeros((self._n_components</span><span class="s3">, </span><span class="s1">self._n_components)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
            <span class="s1">self._B = np.zeros((X.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">self._n_components)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">dictionary = self.components_</span>

        <span class="s1">self._minibatch_step(X</span><span class="s3">, </span><span class="s1">dictionary</span><span class="s3">, </span><span class="s1">self._random_state</span><span class="s3">, </span><span class="s1">self.n_steps_)</span>

        <span class="s1">self.components_ = dictionary</span>
        <span class="s1">self.n_steps_ += </span><span class="s5">1</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">_n_features_out(self):</span>
        <span class="s0">&quot;&quot;&quot;Number of transformed output features.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.components_.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span>
            <span class="s4">&quot;preserves_dtype&quot;</span><span class="s1">: [np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">,</span>
        <span class="s1">}</span>
</pre>
</body>
</html>