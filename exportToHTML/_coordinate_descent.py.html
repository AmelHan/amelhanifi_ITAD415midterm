<html>
<head>
<title>_coordinate_descent.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_coordinate_descent.py</font>
</center></td></tr></table>
<pre><span class="s0"># Author: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr&gt;</span>
<span class="s0">#         Fabian Pedregosa &lt;fabian.pedregosa@inria.fr&gt;</span>
<span class="s0">#         Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span>
<span class="s0">#         Gael Varoquaux &lt;gael.varoquaux@inria.fr&gt;</span>
<span class="s0">#</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">import </span><span class="s1">numbers</span>
<span class="s2">import </span><span class="s1">sys</span>
<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">abc </span><span class="s2">import </span><span class="s1">ABC</span><span class="s2">, </span><span class="s1">abstractmethod</span>
<span class="s2">from </span><span class="s1">functools </span><span class="s2">import </span><span class="s1">partial</span>
<span class="s2">from </span><span class="s1">numbers </span><span class="s2">import </span><span class="s1">Integral</span><span class="s2">, </span><span class="s1">Real</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">joblib </span><span class="s2">import </span><span class="s1">effective_n_jobs</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">sparse</span>

<span class="s2">from </span><span class="s1">..base </span><span class="s2">import </span><span class="s1">MultiOutputMixin</span><span class="s2">, </span><span class="s1">RegressorMixin</span><span class="s2">, </span><span class="s1">_fit_context</span>
<span class="s2">from </span><span class="s1">..model_selection </span><span class="s2">import </span><span class="s1">check_cv</span>
<span class="s2">from </span><span class="s1">..utils </span><span class="s2">import </span><span class="s1">check_array</span><span class="s2">, </span><span class="s1">check_scalar</span>
<span class="s2">from </span><span class="s1">..utils._param_validation </span><span class="s2">import </span><span class="s1">Interval</span><span class="s2">, </span><span class="s1">StrOptions</span>
<span class="s2">from </span><span class="s1">..utils.extmath </span><span class="s2">import </span><span class="s1">safe_sparse_dot</span>
<span class="s2">from </span><span class="s1">..utils.parallel </span><span class="s2">import </span><span class="s1">Parallel</span><span class="s2">, </span><span class="s1">delayed</span>
<span class="s2">from </span><span class="s1">..utils.validation </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">_check_sample_weight</span><span class="s2">,</span>
    <span class="s1">check_consistent_length</span><span class="s2">,</span>
    <span class="s1">check_is_fitted</span><span class="s2">,</span>
    <span class="s1">check_random_state</span><span class="s2">,</span>
    <span class="s1">column_or_1d</span><span class="s2">,</span>
<span class="s1">)</span>

<span class="s0"># mypy error: Module 'sklearn.linear_model' has no attribute '_cd_fast'</span>
<span class="s2">from </span><span class="s1">. </span><span class="s2">import </span><span class="s1">_cd_fast </span><span class="s2">as </span><span class="s1">cd_fast  </span><span class="s0"># type: ignore</span>
<span class="s2">from </span><span class="s1">._base </span><span class="s2">import </span><span class="s1">LinearModel</span><span class="s2">, </span><span class="s1">_pre_fit</span><span class="s2">, </span><span class="s1">_preprocess_data</span>


<span class="s2">def </span><span class="s1">_set_order(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;C&quot;</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Change the order of X and y if necessary. 
 
    Parameters 
    ---------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) 
        Training data. 
 
    y : ndarray of shape (n_samples,) 
        Target values. 
 
    order : {None, 'C', 'F'} 
        If 'C', dense arrays are returned as C-ordered, sparse matrices in csr 
        format. If 'F', dense arrays are return as F-ordered, sparse matrices 
        in csc format. 
 
    Returns 
    ------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) 
        Training data with guaranteed order. 
 
    y : ndarray of shape (n_samples,) 
        Target values with guaranteed order. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">order </span><span class="s2">not in </span><span class="s1">[</span><span class="s2">None, </span><span class="s3">&quot;C&quot;</span><span class="s2">, </span><span class="s3">&quot;F&quot;</span><span class="s1">]:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span>
            <span class="s3">&quot;Unknown value for order. Got {} instead of None, 'C' or 'F'.&quot;</span><span class="s1">.format(order)</span>
        <span class="s1">)</span>
    <span class="s1">sparse_X = sparse.issparse(X)</span>
    <span class="s1">sparse_y = sparse.issparse(y)</span>
    <span class="s2">if </span><span class="s1">order </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">sparse_format = </span><span class="s3">&quot;csc&quot; </span><span class="s2">if </span><span class="s1">order == </span><span class="s3">&quot;F&quot; </span><span class="s2">else </span><span class="s3">&quot;csr&quot;</span>
        <span class="s2">if </span><span class="s1">sparse_X:</span>
            <span class="s1">X = X.asformat(sparse_format</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">X = np.asarray(X</span><span class="s2">, </span><span class="s1">order=order)</span>
        <span class="s2">if </span><span class="s1">sparse_y:</span>
            <span class="s1">y = y.asformat(sparse_format)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">y = np.asarray(y</span><span class="s2">, </span><span class="s1">order=order)</span>
    <span class="s2">return </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span>


<span class="s0">###############################################################################</span>
<span class="s0"># Paths functions</span>


<span class="s2">def </span><span class="s1">_alpha_grid(</span>
    <span class="s1">X</span><span class="s2">,</span>
    <span class="s1">y</span><span class="s2">,</span>
    <span class="s1">Xy=</span><span class="s2">None,</span>
    <span class="s1">l1_ratio=</span><span class="s5">1.0</span><span class="s2">,</span>
    <span class="s1">fit_intercept=</span><span class="s2">True,</span>
    <span class="s1">eps=</span><span class="s5">1e-3</span><span class="s2">,</span>
    <span class="s1">n_alphas=</span><span class="s5">100</span><span class="s2">,</span>
    <span class="s1">copy_X=</span><span class="s2">True,</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Compute the grid of alpha values for elastic net parameter search 
 
    Parameters 
    ---------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) 
        Training data. Pass directly as Fortran-contiguous data to avoid 
        unnecessary memory duplication 
 
    y : ndarray of shape (n_samples,) or (n_samples, n_outputs) 
        Target values 
 
    Xy : array-like of shape (n_features,) or (n_features, n_outputs),\ 
         default=None 
        Xy = np.dot(X.T, y) that can be precomputed. 
 
    l1_ratio : float, default=1.0 
        The elastic net mixing parameter, with ``0 &lt; l1_ratio &lt;= 1``. 
        For ``l1_ratio = 0`` the penalty is an L2 penalty. (currently not 
        supported) ``For l1_ratio = 1`` it is an L1 penalty. For 
        ``0 &lt; l1_ratio &lt;1``, the penalty is a combination of L1 and L2. 
 
    eps : float, default=1e-3 
        Length of the path. ``eps=1e-3`` means that 
        ``alpha_min / alpha_max = 1e-3`` 
 
    n_alphas : int, default=100 
        Number of alphas along the regularization path 
 
    fit_intercept : bool, default=True 
        Whether to fit an intercept or not 
 
    copy_X : bool, default=True 
        If ``True``, X will be copied; else, it may be overwritten. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">l1_ratio == </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span>
            <span class="s3">&quot;Automatic alpha grid generation is not supported for&quot;</span>
            <span class="s3">&quot; l1_ratio=0. Please supply a grid by providing &quot;</span>
            <span class="s3">&quot;your estimator with the appropriate `alphas=` &quot;</span>
            <span class="s3">&quot;argument.&quot;</span>
        <span class="s1">)</span>
    <span class="s1">n_samples = len(y)</span>

    <span class="s1">sparse_center = </span><span class="s2">False</span>
    <span class="s2">if </span><span class="s1">Xy </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">X_sparse = sparse.issparse(X)</span>
        <span class="s1">sparse_center = X_sparse </span><span class="s2">and </span><span class="s1">fit_intercept</span>
        <span class="s1">X = check_array(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">accept_sparse=</span><span class="s3">&quot;csc&quot;</span><span class="s2">, </span><span class="s1">copy=(copy_X </span><span class="s2">and </span><span class="s1">fit_intercept </span><span class="s2">and not </span><span class="s1">X_sparse)</span>
        <span class="s1">)</span>
        <span class="s2">if not </span><span class="s1">X_sparse:</span>
            <span class="s0"># X can be touched inplace thanks to the above line</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = _preprocess_data(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">fit_intercept</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">Xy = safe_sparse_dot(X.T</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">dense_output=</span><span class="s2">True</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">sparse_center:</span>
            <span class="s0"># Workaround to find alpha_max for sparse matrices.</span>
            <span class="s0"># since we should not destroy the sparsity of such matrices.</span>
            <span class="s1">_</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">X_offset</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">X_scale = _preprocess_data(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">fit_intercept)</span>
            <span class="s1">mean_dot = X_offset * np.sum(y)</span>

    <span class="s2">if </span><span class="s1">Xy.ndim == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">Xy = Xy[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>

    <span class="s2">if </span><span class="s1">sparse_center:</span>
        <span class="s2">if </span><span class="s1">fit_intercept:</span>
            <span class="s1">Xy -= mean_dot[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>

    <span class="s1">alpha_max = np.sqrt(np.sum(Xy**</span><span class="s5">2</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)).max() / (n_samples * l1_ratio)</span>

    <span class="s2">if </span><span class="s1">alpha_max &lt;= np.finfo(float).resolution:</span>
        <span class="s1">alphas = np.empty(n_alphas)</span>
        <span class="s1">alphas.fill(np.finfo(float).resolution)</span>
        <span class="s2">return </span><span class="s1">alphas</span>

    <span class="s2">return </span><span class="s1">np.geomspace(alpha_max</span><span class="s2">, </span><span class="s1">alpha_max * eps</span><span class="s2">, </span><span class="s1">num=n_alphas)</span>


<span class="s2">def </span><span class="s1">lasso_path(</span>
    <span class="s1">X</span><span class="s2">,</span>
    <span class="s1">y</span><span class="s2">,</span>
    <span class="s1">*</span><span class="s2">,</span>
    <span class="s1">eps=</span><span class="s5">1e-3</span><span class="s2">,</span>
    <span class="s1">n_alphas=</span><span class="s5">100</span><span class="s2">,</span>
    <span class="s1">alphas=</span><span class="s2">None,</span>
    <span class="s1">precompute=</span><span class="s3">&quot;auto&quot;</span><span class="s2">,</span>
    <span class="s1">Xy=</span><span class="s2">None,</span>
    <span class="s1">copy_X=</span><span class="s2">True,</span>
    <span class="s1">coef_init=</span><span class="s2">None,</span>
    <span class="s1">verbose=</span><span class="s2">False,</span>
    <span class="s1">return_n_iter=</span><span class="s2">False,</span>
    <span class="s1">positive=</span><span class="s2">False,</span>
    <span class="s1">**params</span><span class="s2">,</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Compute Lasso path with coordinate descent. 
 
    The Lasso optimization function varies for mono and multi-outputs. 
 
    For mono-output tasks it is:: 
 
        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1 
 
    For multi-output tasks it is:: 
 
        (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21 
 
    Where:: 
 
        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2} 
 
    i.e. the sum of norm of each row. 
 
    Read more in the :ref:`User Guide &lt;lasso&gt;`. 
 
    Parameters 
    ---------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) 
        Training data. Pass directly as Fortran-contiguous data to avoid 
        unnecessary memory duplication. If ``y`` is mono-output then ``X`` 
        can be sparse. 
 
    y : {array-like, sparse matrix} of shape (n_samples,) or \ 
        (n_samples, n_targets) 
        Target values. 
 
    eps : float, default=1e-3 
        Length of the path. ``eps=1e-3`` means that 
        ``alpha_min / alpha_max = 1e-3``. 
 
    n_alphas : int, default=100 
        Number of alphas along the regularization path. 
 
    alphas : ndarray, default=None 
        List of alphas where to compute the models. 
        If ``None`` alphas are set automatically. 
 
    precompute : 'auto', bool or array-like of shape \ 
            (n_features, n_features), default='auto' 
        Whether to use a precomputed Gram matrix to speed up 
        calculations. If set to ``'auto'`` let us decide. The Gram 
        matrix can also be passed as argument. 
 
    Xy : array-like of shape (n_features,) or (n_features, n_targets),\ 
         default=None 
        Xy = np.dot(X.T, y) that can be precomputed. It is useful 
        only when the Gram matrix is precomputed. 
 
    copy_X : bool, default=True 
        If ``True``, X will be copied; else, it may be overwritten. 
 
    coef_init : ndarray of shape (n_features, ), default=None 
        The initial values of the coefficients. 
 
    verbose : bool or int, default=False 
        Amount of verbosity. 
 
    return_n_iter : bool, default=False 
        Whether to return the number of iterations or not. 
 
    positive : bool, default=False 
        If set to True, forces coefficients to be positive. 
        (Only allowed when ``y.ndim == 1``). 
 
    **params : kwargs 
        Keyword arguments passed to the coordinate descent solver. 
 
    Returns 
    ------- 
    alphas : ndarray of shape (n_alphas,) 
        The alphas along the path where models are computed. 
 
    coefs : ndarray of shape (n_features, n_alphas) or \ 
            (n_targets, n_features, n_alphas) 
        Coefficients along the path. 
 
    dual_gaps : ndarray of shape (n_alphas,) 
        The dual gaps at the end of the optimization for each alpha. 
 
    n_iters : list of int 
        The number of iterations taken by the coordinate descent optimizer to 
        reach the specified tolerance for each alpha. 
 
    See Also 
    -------- 
    lars_path : Compute Least Angle Regression or Lasso path using LARS 
        algorithm. 
    Lasso : The Lasso is a linear model that estimates sparse coefficients. 
    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars. 
    LassoCV : Lasso linear model with iterative fitting along a regularization 
        path. 
    LassoLarsCV : Cross-validated Lasso using the LARS algorithm. 
    sklearn.decomposition.sparse_encode : Estimator that can be used to 
        transform signals into sparse linear combination of atoms from a fixed. 
 
    Notes 
    ----- 
    For an example, see 
    :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py 
    &lt;sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py&gt;`. 
 
    To avoid unnecessary memory duplication the X argument of the fit method 
    should be directly passed as a Fortran-contiguous numpy array. 
 
    Note that in certain cases, the Lars solver may be significantly 
    faster to implement this functionality. In particular, linear 
    interpolation can be used to retrieve model coefficients between the 
    values output by lars_path 
 
    Examples 
    -------- 
 
    Comparing lasso_path and lars_path with interpolation: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.linear_model import lasso_path 
    &gt;&gt;&gt; X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T 
    &gt;&gt;&gt; y = np.array([1, 2, 3.1]) 
    &gt;&gt;&gt; # Use lasso_path to compute a coefficient path 
    &gt;&gt;&gt; _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5]) 
    &gt;&gt;&gt; print(coef_path) 
    [[0.         0.         0.46874778] 
     [0.2159048  0.4425765  0.23689075]] 
 
    &gt;&gt;&gt; # Now use lars_path and 1D linear interpolation to compute the 
    &gt;&gt;&gt; # same path 
    &gt;&gt;&gt; from sklearn.linear_model import lars_path 
    &gt;&gt;&gt; alphas, active, coef_path_lars = lars_path(X, y, method='lasso') 
    &gt;&gt;&gt; from scipy import interpolate 
    &gt;&gt;&gt; coef_path_continuous = interpolate.interp1d(alphas[::-1], 
    ...                                             coef_path_lars[:, ::-1]) 
    &gt;&gt;&gt; print(coef_path_continuous([5., 1., .5])) 
    [[0.         0.         0.46915237] 
     [0.2159048  0.4425765  0.23668876]] 
    &quot;&quot;&quot;</span>
    <span class="s2">return </span><span class="s1">enet_path(</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">y</span><span class="s2">,</span>
        <span class="s1">l1_ratio=</span><span class="s5">1.0</span><span class="s2">,</span>
        <span class="s1">eps=eps</span><span class="s2">,</span>
        <span class="s1">n_alphas=n_alphas</span><span class="s2">,</span>
        <span class="s1">alphas=alphas</span><span class="s2">,</span>
        <span class="s1">precompute=precompute</span><span class="s2">,</span>
        <span class="s1">Xy=Xy</span><span class="s2">,</span>
        <span class="s1">copy_X=copy_X</span><span class="s2">,</span>
        <span class="s1">coef_init=coef_init</span><span class="s2">,</span>
        <span class="s1">verbose=verbose</span><span class="s2">,</span>
        <span class="s1">positive=positive</span><span class="s2">,</span>
        <span class="s1">return_n_iter=return_n_iter</span><span class="s2">,</span>
        <span class="s1">**params</span><span class="s2">,</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">enet_path(</span>
    <span class="s1">X</span><span class="s2">,</span>
    <span class="s1">y</span><span class="s2">,</span>
    <span class="s1">*</span><span class="s2">,</span>
    <span class="s1">l1_ratio=</span><span class="s5">0.5</span><span class="s2">,</span>
    <span class="s1">eps=</span><span class="s5">1e-3</span><span class="s2">,</span>
    <span class="s1">n_alphas=</span><span class="s5">100</span><span class="s2">,</span>
    <span class="s1">alphas=</span><span class="s2">None,</span>
    <span class="s1">precompute=</span><span class="s3">&quot;auto&quot;</span><span class="s2">,</span>
    <span class="s1">Xy=</span><span class="s2">None,</span>
    <span class="s1">copy_X=</span><span class="s2">True,</span>
    <span class="s1">coef_init=</span><span class="s2">None,</span>
    <span class="s1">verbose=</span><span class="s2">False,</span>
    <span class="s1">return_n_iter=</span><span class="s2">False,</span>
    <span class="s1">positive=</span><span class="s2">False,</span>
    <span class="s1">check_input=</span><span class="s2">True,</span>
    <span class="s1">**params</span><span class="s2">,</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Compute elastic net path with coordinate descent. 
 
    The elastic net optimization function varies for mono and multi-outputs. 
 
    For mono-output tasks it is:: 
 
        1 / (2 * n_samples) * ||y - Xw||^2_2 
        + alpha * l1_ratio * ||w||_1 
        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2 
 
    For multi-output tasks it is:: 
 
        (1 / (2 * n_samples)) * ||Y - XW||_Fro^2 
        + alpha * l1_ratio * ||W||_21 
        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2 
 
    Where:: 
 
        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2} 
 
    i.e. the sum of norm of each row. 
 
    Read more in the :ref:`User Guide &lt;elastic_net&gt;`. 
 
    Parameters 
    ---------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) 
        Training data. Pass directly as Fortran-contiguous data to avoid 
        unnecessary memory duplication. If ``y`` is mono-output then ``X`` 
        can be sparse. 
 
    y : {array-like, sparse matrix} of shape (n_samples,) or \ 
        (n_samples, n_targets) 
        Target values. 
 
    l1_ratio : float, default=0.5 
        Number between 0 and 1 passed to elastic net (scaling between 
        l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso. 
 
    eps : float, default=1e-3 
        Length of the path. ``eps=1e-3`` means that 
        ``alpha_min / alpha_max = 1e-3``. 
 
    n_alphas : int, default=100 
        Number of alphas along the regularization path. 
 
    alphas : ndarray, default=None 
        List of alphas where to compute the models. 
        If None alphas are set automatically. 
 
    precompute : 'auto', bool or array-like of shape \ 
            (n_features, n_features), default='auto' 
        Whether to use a precomputed Gram matrix to speed up 
        calculations. If set to ``'auto'`` let us decide. The Gram 
        matrix can also be passed as argument. 
 
    Xy : array-like of shape (n_features,) or (n_features, n_targets),\ 
         default=None 
        Xy = np.dot(X.T, y) that can be precomputed. It is useful 
        only when the Gram matrix is precomputed. 
 
    copy_X : bool, default=True 
        If ``True``, X will be copied; else, it may be overwritten. 
 
    coef_init : ndarray of shape (n_features, ), default=None 
        The initial values of the coefficients. 
 
    verbose : bool or int, default=False 
        Amount of verbosity. 
 
    return_n_iter : bool, default=False 
        Whether to return the number of iterations or not. 
 
    positive : bool, default=False 
        If set to True, forces coefficients to be positive. 
        (Only allowed when ``y.ndim == 1``). 
 
    check_input : bool, default=True 
        If set to False, the input validation checks are skipped (including the 
        Gram matrix when provided). It is assumed that they are handled 
        by the caller. 
 
    **params : kwargs 
        Keyword arguments passed to the coordinate descent solver. 
 
    Returns 
    ------- 
    alphas : ndarray of shape (n_alphas,) 
        The alphas along the path where models are computed. 
 
    coefs : ndarray of shape (n_features, n_alphas) or \ 
            (n_targets, n_features, n_alphas) 
        Coefficients along the path. 
 
    dual_gaps : ndarray of shape (n_alphas,) 
        The dual gaps at the end of the optimization for each alpha. 
 
    n_iters : list of int 
        The number of iterations taken by the coordinate descent optimizer to 
        reach the specified tolerance for each alpha. 
        (Is returned when ``return_n_iter`` is set to True). 
 
    See Also 
    -------- 
    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm \ 
    as regularizer. 
    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation. 
    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer. 
    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path. 
 
    Notes 
    ----- 
    For an example, see 
    :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py 
    &lt;sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py&gt;`. 
    &quot;&quot;&quot;</span>
    <span class="s1">X_offset_param = params.pop(</span><span class="s3">&quot;X_offset&quot;</span><span class="s2">, None</span><span class="s1">)</span>
    <span class="s1">X_scale_param = params.pop(</span><span class="s3">&quot;X_scale&quot;</span><span class="s2">, None</span><span class="s1">)</span>
    <span class="s1">sample_weight = params.pop(</span><span class="s3">&quot;sample_weight&quot;</span><span class="s2">, None</span><span class="s1">)</span>
    <span class="s1">tol = params.pop(</span><span class="s3">&quot;tol&quot;</span><span class="s2">, </span><span class="s5">1e-4</span><span class="s1">)</span>
    <span class="s1">max_iter = params.pop(</span><span class="s3">&quot;max_iter&quot;</span><span class="s2">, </span><span class="s5">1000</span><span class="s1">)</span>
    <span class="s1">random_state = params.pop(</span><span class="s3">&quot;random_state&quot;</span><span class="s2">, None</span><span class="s1">)</span>
    <span class="s1">selection = params.pop(</span><span class="s3">&quot;selection&quot;</span><span class="s2">, </span><span class="s3">&quot;cyclic&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">len(params) &gt; </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Unexpected parameters in params&quot;</span><span class="s2">, </span><span class="s1">params.keys())</span>

    <span class="s0"># We expect X and y to be already Fortran ordered when bypassing</span>
    <span class="s0"># checks</span>
    <span class="s2">if </span><span class="s1">check_input:</span>
        <span class="s1">X = check_array(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">accept_sparse=</span><span class="s3">&quot;csc&quot;</span><span class="s2">,</span>
            <span class="s1">dtype=[np.float64</span><span class="s2">, </span><span class="s1">np.float32]</span><span class="s2">,</span>
            <span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s2">,</span>
            <span class="s1">copy=copy_X</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">y = check_array(</span>
            <span class="s1">y</span><span class="s2">,</span>
            <span class="s1">accept_sparse=</span><span class="s3">&quot;csc&quot;</span><span class="s2">,</span>
            <span class="s1">dtype=X.dtype.type</span><span class="s2">,</span>
            <span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s2">,</span>
            <span class="s1">copy=</span><span class="s2">False,</span>
            <span class="s1">ensure_2d=</span><span class="s2">False,</span>
        <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">Xy </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s0"># Xy should be a 1d contiguous array or a 2D C ordered array</span>
            <span class="s1">Xy = check_array(</span>
                <span class="s1">Xy</span><span class="s2">, </span><span class="s1">dtype=X.dtype.type</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;C&quot;</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False, </span><span class="s1">ensure_2d=</span><span class="s2">False</span>
            <span class="s1">)</span>

    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X.shape</span>

    <span class="s1">multi_output = </span><span class="s2">False</span>
    <span class="s2">if </span><span class="s1">y.ndim != </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">multi_output = </span><span class="s2">True</span>
        <span class="s1">n_targets = y.shape[</span><span class="s5">1</span><span class="s1">]</span>

    <span class="s2">if </span><span class="s1">multi_output </span><span class="s2">and </span><span class="s1">positive:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;positive=True is not allowed for multi-output (y.ndim != 1)&quot;</span><span class="s1">)</span>

    <span class="s0"># MultiTaskElasticNet does not support sparse matrices</span>
    <span class="s2">if not </span><span class="s1">multi_output </span><span class="s2">and </span><span class="s1">sparse.issparse(X):</span>
        <span class="s2">if </span><span class="s1">X_offset_param </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s0"># As sparse matrices are not actually centered we need this to be passed to</span>
            <span class="s0"># the CD solver.</span>
            <span class="s1">X_sparse_scaling = X_offset_param / X_scale_param</span>
            <span class="s1">X_sparse_scaling = np.asarray(X_sparse_scaling</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">X_sparse_scaling = np.zeros(n_features</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s0"># X should have been passed through _pre_fit already if function is called</span>
    <span class="s0"># from ElasticNet.fit</span>
    <span class="s2">if </span><span class="s1">check_input:</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">precompute</span><span class="s2">, </span><span class="s1">Xy = _pre_fit(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">y</span><span class="s2">,</span>
            <span class="s1">Xy</span><span class="s2">,</span>
            <span class="s1">precompute</span><span class="s2">,</span>
            <span class="s1">normalize=</span><span class="s2">False,</span>
            <span class="s1">fit_intercept=</span><span class="s2">False,</span>
            <span class="s1">copy=</span><span class="s2">False,</span>
            <span class="s1">check_input=check_input</span><span class="s2">,</span>
        <span class="s1">)</span>
    <span class="s2">if </span><span class="s1">alphas </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s0"># No need to normalize of fit_intercept: it has been done</span>
        <span class="s0"># above</span>
        <span class="s1">alphas = _alpha_grid(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">y</span><span class="s2">,</span>
            <span class="s1">Xy=Xy</span><span class="s2">,</span>
            <span class="s1">l1_ratio=l1_ratio</span><span class="s2">,</span>
            <span class="s1">fit_intercept=</span><span class="s2">False,</span>
            <span class="s1">eps=eps</span><span class="s2">,</span>
            <span class="s1">n_alphas=n_alphas</span><span class="s2">,</span>
            <span class="s1">copy_X=</span><span class="s2">False,</span>
        <span class="s1">)</span>
    <span class="s2">elif </span><span class="s1">len(alphas) &gt; </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">alphas = np.sort(alphas)[::-</span><span class="s5">1</span><span class="s1">]  </span><span class="s0"># make sure alphas are properly ordered</span>

    <span class="s1">n_alphas = len(alphas)</span>
    <span class="s1">dual_gaps = np.empty(n_alphas)</span>
    <span class="s1">n_iters = []</span>

    <span class="s1">rng = check_random_state(random_state)</span>
    <span class="s2">if </span><span class="s1">selection </span><span class="s2">not in </span><span class="s1">[</span><span class="s3">&quot;random&quot;</span><span class="s2">, </span><span class="s3">&quot;cyclic&quot;</span><span class="s1">]:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;selection should be either random or cyclic.&quot;</span><span class="s1">)</span>
    <span class="s1">random = selection == </span><span class="s3">&quot;random&quot;</span>

    <span class="s2">if not </span><span class="s1">multi_output:</span>
        <span class="s1">coefs = np.empty((n_features</span><span class="s2">, </span><span class="s1">n_alphas)</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">coefs = np.empty((n_targets</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_alphas)</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s2">if </span><span class="s1">coef_init </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">coef_ = np.zeros(coefs.shape[:-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=X.dtype</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">coef_ = np.asfortranarray(coef_init</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">alpha </span><span class="s2">in </span><span class="s1">enumerate(alphas):</span>
        <span class="s0"># account for n_samples scaling in objectives between here and cd_fast</span>
        <span class="s1">l1_reg = alpha * l1_ratio * n_samples</span>
        <span class="s1">l2_reg = alpha * (</span><span class="s5">1.0 </span><span class="s1">- l1_ratio) * n_samples</span>
        <span class="s2">if not </span><span class="s1">multi_output </span><span class="s2">and </span><span class="s1">sparse.issparse(X):</span>
            <span class="s1">model = cd_fast.sparse_enet_coordinate_descent(</span>
                <span class="s1">w=coef_</span><span class="s2">,</span>
                <span class="s1">alpha=l1_reg</span><span class="s2">,</span>
                <span class="s1">beta=l2_reg</span><span class="s2">,</span>
                <span class="s1">X_data=X.data</span><span class="s2">,</span>
                <span class="s1">X_indices=X.indices</span><span class="s2">,</span>
                <span class="s1">X_indptr=X.indptr</span><span class="s2">,</span>
                <span class="s1">y=y</span><span class="s2">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s2">,</span>
                <span class="s1">X_mean=X_sparse_scaling</span><span class="s2">,</span>
                <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
                <span class="s1">tol=tol</span><span class="s2">,</span>
                <span class="s1">rng=rng</span><span class="s2">,</span>
                <span class="s1">random=random</span><span class="s2">,</span>
                <span class="s1">positive=positive</span><span class="s2">,</span>
            <span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">multi_output:</span>
            <span class="s1">model = cd_fast.enet_coordinate_descent_multi_task(</span>
                <span class="s1">coef_</span><span class="s2">, </span><span class="s1">l1_reg</span><span class="s2">, </span><span class="s1">l2_reg</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">max_iter</span><span class="s2">, </span><span class="s1">tol</span><span class="s2">, </span><span class="s1">rng</span><span class="s2">, </span><span class="s1">random</span>
            <span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">isinstance(precompute</span><span class="s2">, </span><span class="s1">np.ndarray):</span>
            <span class="s0"># We expect precompute to be already Fortran ordered when bypassing</span>
            <span class="s0"># checks</span>
            <span class="s2">if </span><span class="s1">check_input:</span>
                <span class="s1">precompute = check_array(precompute</span><span class="s2">, </span><span class="s1">dtype=X.dtype.type</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;C&quot;</span><span class="s1">)</span>
            <span class="s1">model = cd_fast.enet_coordinate_descent_gram(</span>
                <span class="s1">coef_</span><span class="s2">,</span>
                <span class="s1">l1_reg</span><span class="s2">,</span>
                <span class="s1">l2_reg</span><span class="s2">,</span>
                <span class="s1">precompute</span><span class="s2">,</span>
                <span class="s1">Xy</span><span class="s2">,</span>
                <span class="s1">y</span><span class="s2">,</span>
                <span class="s1">max_iter</span><span class="s2">,</span>
                <span class="s1">tol</span><span class="s2">,</span>
                <span class="s1">rng</span><span class="s2">,</span>
                <span class="s1">random</span><span class="s2">,</span>
                <span class="s1">positive</span><span class="s2">,</span>
            <span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">precompute </span><span class="s2">is False</span><span class="s1">:</span>
            <span class="s1">model = cd_fast.enet_coordinate_descent(</span>
                <span class="s1">coef_</span><span class="s2">, </span><span class="s1">l1_reg</span><span class="s2">, </span><span class="s1">l2_reg</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">max_iter</span><span class="s2">, </span><span class="s1">tol</span><span class="s2">, </span><span class="s1">rng</span><span class="s2">, </span><span class="s1">random</span><span class="s2">, </span><span class="s1">positive</span>
            <span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s3">&quot;Precompute should be one of True, False, 'auto' or array-like. Got %r&quot;</span>
                <span class="s1">% precompute</span>
            <span class="s1">)</span>
        <span class="s1">coef_</span><span class="s2">, </span><span class="s1">dual_gap_</span><span class="s2">, </span><span class="s1">eps_</span><span class="s2">, </span><span class="s1">n_iter_ = model</span>
        <span class="s1">coefs[...</span><span class="s2">, </span><span class="s1">i] = coef_</span>
        <span class="s0"># we correct the scale of the returned dual gap, as the objective</span>
        <span class="s0"># in cd_fast is n_samples * the objective in this docstring.</span>
        <span class="s1">dual_gaps[i] = dual_gap_ / n_samples</span>
        <span class="s1">n_iters.append(n_iter_)</span>

        <span class="s2">if </span><span class="s1">verbose:</span>
            <span class="s2">if </span><span class="s1">verbose &gt; </span><span class="s5">2</span><span class="s1">:</span>
                <span class="s1">print(model)</span>
            <span class="s2">elif </span><span class="s1">verbose &gt; </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">print(</span><span class="s3">&quot;Path: %03i out of %03i&quot; </span><span class="s1">% (i</span><span class="s2">, </span><span class="s1">n_alphas))</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">sys.stderr.write(</span><span class="s3">&quot;.&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">return_n_iter:</span>
        <span class="s2">return </span><span class="s1">alphas</span><span class="s2">, </span><span class="s1">coefs</span><span class="s2">, </span><span class="s1">dual_gaps</span><span class="s2">, </span><span class="s1">n_iters</span>
    <span class="s2">return </span><span class="s1">alphas</span><span class="s2">, </span><span class="s1">coefs</span><span class="s2">, </span><span class="s1">dual_gaps</span>


<span class="s0">###############################################################################</span>
<span class="s0"># ElasticNet model</span>


<span class="s2">class </span><span class="s1">ElasticNet(MultiOutputMixin</span><span class="s2">, </span><span class="s1">RegressorMixin</span><span class="s2">, </span><span class="s1">LinearModel):</span>
    <span class="s4">&quot;&quot;&quot;Linear regression with combined L1 and L2 priors as regularizer. 
 
    Minimizes the objective function:: 
 
            1 / (2 * n_samples) * ||y - Xw||^2_2 
            + alpha * l1_ratio * ||w||_1 
            + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2 
 
    If you are interested in controlling the L1 and L2 penalty 
    separately, keep in mind that this is equivalent to:: 
 
            a * ||w||_1 + 0.5 * b * ||w||_2^2 
 
    where:: 
 
            alpha = a + b and l1_ratio = a / (a + b) 
 
    The parameter l1_ratio corresponds to alpha in the glmnet R package while 
    alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio 
    = 1 is the lasso penalty. Currently, l1_ratio &lt;= 0.01 is not reliable, 
    unless you supply your own sequence of alpha. 
 
    Read more in the :ref:`User Guide &lt;elastic_net&gt;`. 
 
    Parameters 
    ---------- 
    alpha : float, default=1.0 
        Constant that multiplies the penalty terms. Defaults to 1.0. 
        See the notes for the exact mathematical meaning of this 
        parameter. ``alpha = 0`` is equivalent to an ordinary least square, 
        solved by the :class:`LinearRegression` object. For numerical 
        reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised. 
        Given this, you should use the :class:`LinearRegression` object. 
 
    l1_ratio : float, default=0.5 
        The ElasticNet mixing parameter, with ``0 &lt;= l1_ratio &lt;= 1``. For 
        ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it 
        is an L1 penalty.  For ``0 &lt; l1_ratio &lt; 1``, the penalty is a 
        combination of L1 and L2. 
 
    fit_intercept : bool, default=True 
        Whether the intercept should be estimated or not. If ``False``, the 
        data is assumed to be already centered. 
 
    precompute : bool or array-like of shape (n_features, n_features),\ 
                 default=False 
        Whether to use a precomputed Gram matrix to speed up 
        calculations. The Gram matrix can also be passed as argument. 
        For sparse input this option is always ``False`` to preserve sparsity. 
 
    max_iter : int, default=1000 
        The maximum number of iterations. 
 
    copy_X : bool, default=True 
        If ``True``, X will be copied; else, it may be overwritten. 
 
    tol : float, default=1e-4 
        The tolerance for the optimization: if the updates are 
        smaller than ``tol``, the optimization code checks the 
        dual gap for optimality and continues until it is smaller 
        than ``tol``, see Notes below. 
 
    warm_start : bool, default=False 
        When set to ``True``, reuse the solution of the previous call to fit as 
        initialization, otherwise, just erase the previous solution. 
        See :term:`the Glossary &lt;warm_start&gt;`. 
 
    positive : bool, default=False 
        When set to ``True``, forces the coefficients to be positive. 
 
    random_state : int, RandomState instance, default=None 
        The seed of the pseudo random number generator that selects a random 
        feature to update. Used when ``selection`` == 'random'. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    selection : {'cyclic', 'random'}, default='cyclic' 
        If set to 'random', a random coefficient is updated every iteration 
        rather than looping over features sequentially by default. This 
        (setting to 'random') often leads to significantly faster convergence 
        especially when tol is higher than 1e-4. 
 
    Attributes 
    ---------- 
    coef_ : ndarray of shape (n_features,) or (n_targets, n_features) 
        Parameter vector (w in the cost function formula). 
 
    sparse_coef_ : sparse matrix of shape (n_features,) or \ 
            (n_targets, n_features) 
        Sparse representation of the `coef_`. 
 
    intercept_ : float or ndarray of shape (n_targets,) 
        Independent term in decision function. 
 
    n_iter_ : list of int 
        Number of iterations run by the coordinate descent solver to reach 
        the specified tolerance. 
 
    dual_gap_ : float or ndarray of shape (n_targets,) 
        Given param alpha, the dual gaps at the end of the optimization, 
        same shape as each observation of y. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    ElasticNetCV : Elastic net model with best model selection by 
        cross-validation. 
    SGDRegressor : Implements elastic net regression with incremental training. 
    SGDClassifier : Implements logistic regression with elastic net penalty 
        (``SGDClassifier(loss=&quot;log_loss&quot;, penalty=&quot;elasticnet&quot;)``). 
 
    Notes 
    ----- 
    To avoid unnecessary memory duplication the X argument of the fit method 
    should be directly passed as a Fortran-contiguous numpy array. 
 
    The precise stopping criteria based on `tol` are the following: First, check that 
    that maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|` 
    is smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`. 
    If so, then additionally check whether the dual gap is smaller than `tol` times 
    :math:`||y||_2^2 / n_{\text{samples}}`. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.linear_model import ElasticNet 
    &gt;&gt;&gt; from sklearn.datasets import make_regression 
 
    &gt;&gt;&gt; X, y = make_regression(n_features=2, random_state=0) 
    &gt;&gt;&gt; regr = ElasticNet(random_state=0) 
    &gt;&gt;&gt; regr.fit(X, y) 
    ElasticNet(random_state=0) 
    &gt;&gt;&gt; print(regr.coef_) 
    [18.83816048 64.55968825] 
    &gt;&gt;&gt; print(regr.intercept_) 
    1.451... 
    &gt;&gt;&gt; print(regr.predict([[0, 0]])) 
    [1.451...] 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s3">&quot;alpha&quot;</span><span class="s1">: [Interval(Real</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s3">&quot;left&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s3">&quot;l1_ratio&quot;</span><span class="s1">: [Interval(Real</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s1">closed=</span><span class="s3">&quot;both&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s3">&quot;fit_intercept&quot;</span><span class="s1">: [</span><span class="s3">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;precompute&quot;</span><span class="s1">: [</span><span class="s3">&quot;boolean&quot;</span><span class="s2">, </span><span class="s3">&quot;array-like&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s3">&quot;left&quot;</span><span class="s1">)</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;copy_X&quot;</span><span class="s1">: [</span><span class="s3">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s3">&quot;left&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s3">&quot;warm_start&quot;</span><span class="s1">: [</span><span class="s3">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;positive&quot;</span><span class="s1">: [</span><span class="s3">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s3">&quot;random_state&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;selection&quot;</span><span class="s1">: [StrOptions({</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">, </span><span class="s3">&quot;random&quot;</span><span class="s1">})]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s1">path = staticmethod(enet_path)</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">alpha=</span><span class="s5">1.0</span><span class="s2">,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">l1_ratio=</span><span class="s5">0.5</span><span class="s2">,</span>
        <span class="s1">fit_intercept=</span><span class="s2">True,</span>
        <span class="s1">precompute=</span><span class="s2">False,</span>
        <span class="s1">max_iter=</span><span class="s5">1000</span><span class="s2">,</span>
        <span class="s1">copy_X=</span><span class="s2">True,</span>
        <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s2">,</span>
        <span class="s1">warm_start=</span><span class="s2">False,</span>
        <span class="s1">positive=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s2">None,</span>
        <span class="s1">selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">self.alpha = alpha</span>
        <span class="s1">self.l1_ratio = l1_ratio</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.precompute = precompute</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.copy_X = copy_X</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.warm_start = warm_start</span>
        <span class="s1">self.positive = positive</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.selection = selection</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None, </span><span class="s1">check_input=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s4">&quot;&quot;&quot;Fit model with coordinate descent. 
 
        Parameters 
        ---------- 
        X : {ndarray, sparse matrix} of (n_samples, n_features) 
            Data. 
 
        y : ndarray of shape (n_samples,) or (n_samples, n_targets) 
            Target. Will be cast to X's dtype if necessary. 
 
        sample_weight : float or array-like of shape (n_samples,), default=None 
            Sample weights. Internally, the `sample_weight` vector will be 
            rescaled to sum to `n_samples`. 
 
            .. versionadded:: 0.23 
 
        check_input : bool, default=True 
            Allow to bypass several input checking. 
            Don't use this parameter unless you know what you do. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
 
        Notes 
        ----- 
        Coordinate descent is an algorithm that considers each column of 
        data at a time hence it will automatically convert the X input 
        as a Fortran-contiguous numpy array if necessary. 
 
        To avoid memory re-allocation it is advised to allocate the 
        initial data in memory directly using that format. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">self.alpha == </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s3">&quot;With alpha=0, this algorithm does not converge &quot;</span>
                    <span class="s3">&quot;well. You are advised to use the LinearRegression &quot;</span>
                    <span class="s3">&quot;estimator&quot;</span>
                <span class="s1">)</span><span class="s2">,</span>
                <span class="s1">stacklevel=</span><span class="s5">2</span><span class="s2">,</span>
            <span class="s1">)</span>

        <span class="s0"># Remember if X is copied</span>
        <span class="s1">X_copied = </span><span class="s2">False</span>
        <span class="s0"># We expect X and y to be float64 or float32 Fortran ordered arrays</span>
        <span class="s0"># when bypassing checks</span>
        <span class="s2">if </span><span class="s1">check_input:</span>
            <span class="s1">X_copied = self.copy_X </span><span class="s2">and </span><span class="s1">self.fit_intercept</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(</span>
                <span class="s1">X</span><span class="s2">,</span>
                <span class="s1">y</span><span class="s2">,</span>
                <span class="s1">accept_sparse=</span><span class="s3">&quot;csc&quot;</span><span class="s2">,</span>
                <span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s2">,</span>
                <span class="s1">dtype=[np.float64</span><span class="s2">, </span><span class="s1">np.float32]</span><span class="s2">,</span>
                <span class="s1">copy=X_copied</span><span class="s2">,</span>
                <span class="s1">multi_output=</span><span class="s2">True,</span>
                <span class="s1">y_numeric=</span><span class="s2">True,</span>
            <span class="s1">)</span>
            <span class="s1">y = check_array(</span>
                <span class="s1">y</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False, </span><span class="s1">dtype=X.dtype.type</span><span class="s2">, </span><span class="s1">ensure_2d=</span><span class="s2">False</span>
            <span class="s1">)</span>

        <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X.shape</span>
        <span class="s1">alpha = self.alpha</span>

        <span class="s2">if </span><span class="s1">isinstance(sample_weight</span><span class="s2">, </span><span class="s1">numbers.Number):</span>
            <span class="s1">sample_weight = </span><span class="s2">None</span>
        <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">check_input:</span>
                <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>
            <span class="s0"># TLDR: Rescale sw to sum up to n_samples.</span>
            <span class="s0"># Long: The objective function of Enet</span>
            <span class="s0">#</span>
            <span class="s0">#    1/2 * np.average(squared error, weights=sw)</span>
            <span class="s0">#    + alpha * penalty                                             (1)</span>
            <span class="s0">#</span>
            <span class="s0"># is invariant under rescaling of sw.</span>
            <span class="s0"># But enet_path coordinate descent minimizes</span>
            <span class="s0">#</span>
            <span class="s0">#     1/2 * sum(squared error) + alpha' * penalty                  (2)</span>
            <span class="s0">#</span>
            <span class="s0"># and therefore sets</span>
            <span class="s0">#</span>
            <span class="s0">#     alpha' = n_samples * alpha                                   (3)</span>
            <span class="s0">#</span>
            <span class="s0"># inside its function body, which results in objective (2) being</span>
            <span class="s0"># equivalent to (1) in case of no sw.</span>
            <span class="s0"># With sw, however, enet_path should set</span>
            <span class="s0">#</span>
            <span class="s0">#     alpha' = sum(sw) * alpha                                     (4)</span>
            <span class="s0">#</span>
            <span class="s0"># Therefore, we use the freedom of Eq. (1) to rescale sw before</span>
            <span class="s0"># calling enet_path, i.e.</span>
            <span class="s0">#</span>
            <span class="s0">#     sw *= n_samples / sum(sw)</span>
            <span class="s0">#</span>
            <span class="s0"># such that sum(sw) = n_samples. This way, (3) and (4) are the same.</span>
            <span class="s1">sample_weight = sample_weight * (n_samples / np.sum(sample_weight))</span>
            <span class="s0"># Note: Alternatively, we could also have rescaled alpha instead</span>
            <span class="s0"># of sample_weight:</span>
            <span class="s0">#</span>
            <span class="s0">#     alpha *= np.sum(sample_weight) / n_samples</span>

        <span class="s0"># Ensure copying happens only once, don't do it again if done above.</span>
        <span class="s0"># X and y will be rescaled if sample_weight is not None, order='F'</span>
        <span class="s0"># ensures that the returned X and y are still F-contiguous.</span>
        <span class="s1">should_copy = self.copy_X </span><span class="s2">and not </span><span class="s1">X_copied</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">X_offset</span><span class="s2">, </span><span class="s1">y_offset</span><span class="s2">, </span><span class="s1">X_scale</span><span class="s2">, </span><span class="s1">precompute</span><span class="s2">, </span><span class="s1">Xy = _pre_fit(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">y</span><span class="s2">,</span>
            <span class="s2">None,</span>
            <span class="s1">self.precompute</span><span class="s2">,</span>
            <span class="s1">normalize=</span><span class="s2">False,</span>
            <span class="s1">fit_intercept=self.fit_intercept</span><span class="s2">,</span>
            <span class="s1">copy=should_copy</span><span class="s2">,</span>
            <span class="s1">check_input=check_input</span><span class="s2">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s0"># coordinate descent needs F-ordered arrays and _pre_fit might have</span>
        <span class="s0"># called _rescale_data</span>
        <span class="s2">if </span><span class="s1">check_input </span><span class="s2">or </span><span class="s1">sample_weight </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y = _set_order(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">y.ndim == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">y = y[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s2">if </span><span class="s1">Xy </span><span class="s2">is not None and </span><span class="s1">Xy.ndim == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">Xy = Xy[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>

        <span class="s1">n_targets = y.shape[</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s2">if not </span><span class="s1">self.warm_start </span><span class="s2">or not </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s3">&quot;coef_&quot;</span><span class="s1">):</span>
            <span class="s1">coef_ = np.zeros((n_targets</span><span class="s2">, </span><span class="s1">n_features)</span><span class="s2">, </span><span class="s1">dtype=X.dtype</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">coef_ = self.coef_</span>
            <span class="s2">if </span><span class="s1">coef_.ndim == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">coef_ = coef_[np.newaxis</span><span class="s2">, </span><span class="s1">:]</span>

        <span class="s1">dual_gaps_ = np.zeros(n_targets</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s1">self.n_iter_ = []</span>

        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(n_targets):</span>
            <span class="s2">if </span><span class="s1">Xy </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">this_Xy = Xy[:</span><span class="s2">, </span><span class="s1">k]</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">this_Xy = </span><span class="s2">None</span>
            <span class="s1">_</span><span class="s2">, </span><span class="s1">this_coef</span><span class="s2">, </span><span class="s1">this_dual_gap</span><span class="s2">, </span><span class="s1">this_iter = self.path(</span>
                <span class="s1">X</span><span class="s2">,</span>
                <span class="s1">y[:</span><span class="s2">, </span><span class="s1">k]</span><span class="s2">,</span>
                <span class="s1">l1_ratio=self.l1_ratio</span><span class="s2">,</span>
                <span class="s1">eps=</span><span class="s2">None,</span>
                <span class="s1">n_alphas=</span><span class="s2">None,</span>
                <span class="s1">alphas=[alpha]</span><span class="s2">,</span>
                <span class="s1">precompute=precompute</span><span class="s2">,</span>
                <span class="s1">Xy=this_Xy</span><span class="s2">,</span>
                <span class="s1">copy_X=</span><span class="s2">True,</span>
                <span class="s1">coef_init=coef_[k]</span><span class="s2">,</span>
                <span class="s1">verbose=</span><span class="s2">False,</span>
                <span class="s1">return_n_iter=</span><span class="s2">True,</span>
                <span class="s1">positive=self.positive</span><span class="s2">,</span>
                <span class="s1">check_input=</span><span class="s2">False,</span>
                <span class="s0"># from here on **params</span>
                <span class="s1">tol=self.tol</span><span class="s2">,</span>
                <span class="s1">X_offset=X_offset</span><span class="s2">,</span>
                <span class="s1">X_scale=X_scale</span><span class="s2">,</span>
                <span class="s1">max_iter=self.max_iter</span><span class="s2">,</span>
                <span class="s1">random_state=self.random_state</span><span class="s2">,</span>
                <span class="s1">selection=self.selection</span><span class="s2">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">coef_[k] = this_coef[:</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">dual_gaps_[k] = this_dual_gap[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">self.n_iter_.append(this_iter[</span><span class="s5">0</span><span class="s1">])</span>

        <span class="s2">if </span><span class="s1">n_targets == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">self.n_iter_ = self.n_iter_[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">self.coef_ = coef_[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">self.dual_gap_ = dual_gaps_[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self.coef_ = coef_</span>
            <span class="s1">self.dual_gap_ = dual_gaps_</span>

        <span class="s1">self._set_intercept(X_offset</span><span class="s2">, </span><span class="s1">y_offset</span><span class="s2">, </span><span class="s1">X_scale)</span>

        <span class="s0"># check for finiteness of coefficients</span>
        <span class="s2">if not </span><span class="s1">all(np.isfinite(w).all() </span><span class="s2">for </span><span class="s1">w </span><span class="s2">in </span><span class="s1">[self.coef_</span><span class="s2">, </span><span class="s1">self.intercept_]):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s3">&quot;Coordinate descent iterations resulted in non-finite parameter&quot;</span>
                <span class="s3">&quot; values. The input data may contain large values and need to&quot;</span>
                <span class="s3">&quot; be preprocessed.&quot;</span>
            <span class="s1">)</span>

        <span class="s0"># return self for chaining fit and predict calls</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">sparse_coef_(self):</span>
        <span class="s4">&quot;&quot;&quot;Sparse representation of the fitted `coef_`.&quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">sparse.csr_matrix(self.coef_)</span>

    <span class="s2">def </span><span class="s1">_decision_function(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s4">&quot;&quot;&quot;Decision function of the linear model. 
 
        Parameters 
        ---------- 
        X : numpy array or scipy.sparse matrix of shape (n_samples, n_features) 
 
        Returns 
        ------- 
        T : ndarray of shape (n_samples,) 
            The predicted decision function. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s2">if </span><span class="s1">sparse.issparse(X):</span>
            <span class="s2">return </span><span class="s1">safe_sparse_dot(X</span><span class="s2">, </span><span class="s1">self.coef_.T</span><span class="s2">, </span><span class="s1">dense_output=</span><span class="s2">True</span><span class="s1">) + self.intercept_</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">super()._decision_function(X)</span>


<span class="s0">###############################################################################</span>
<span class="s0"># Lasso model</span>


<span class="s2">class </span><span class="s1">Lasso(ElasticNet):</span>
    <span class="s4">&quot;&quot;&quot;Linear Model trained with L1 prior as regularizer (aka the Lasso). 
 
    The optimization objective for Lasso is:: 
 
        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1 
 
    Technically the Lasso model is optimizing the same objective function as 
    the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty). 
 
    Read more in the :ref:`User Guide &lt;lasso&gt;`. 
 
    Parameters 
    ---------- 
    alpha : float, default=1.0 
        Constant that multiplies the L1 term, controlling regularization 
        strength. `alpha` must be a non-negative float i.e. in `[0, inf)`. 
 
        When `alpha = 0`, the objective is equivalent to ordinary least 
        squares, solved by the :class:`LinearRegression` object. For numerical 
        reasons, using `alpha = 0` with the `Lasso` object is not advised. 
        Instead, you should use the :class:`LinearRegression` object. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. If set 
        to False, no intercept will be used in calculations 
        (i.e. data is expected to be centered). 
 
    precompute : bool or array-like of shape (n_features, n_features),\ 
                 default=False 
        Whether to use a precomputed Gram matrix to speed up 
        calculations. The Gram matrix can also be passed as argument. 
        For sparse input this option is always ``False`` to preserve sparsity. 
 
    copy_X : bool, default=True 
        If ``True``, X will be copied; else, it may be overwritten. 
 
    max_iter : int, default=1000 
        The maximum number of iterations. 
 
    tol : float, default=1e-4 
        The tolerance for the optimization: if the updates are 
        smaller than ``tol``, the optimization code checks the 
        dual gap for optimality and continues until it is smaller 
        than ``tol``, see Notes below. 
 
    warm_start : bool, default=False 
        When set to True, reuse the solution of the previous call to fit as 
        initialization, otherwise, just erase the previous solution. 
        See :term:`the Glossary &lt;warm_start&gt;`. 
 
    positive : bool, default=False 
        When set to ``True``, forces the coefficients to be positive. 
 
    random_state : int, RandomState instance, default=None 
        The seed of the pseudo random number generator that selects a random 
        feature to update. Used when ``selection`` == 'random'. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    selection : {'cyclic', 'random'}, default='cyclic' 
        If set to 'random', a random coefficient is updated every iteration 
        rather than looping over features sequentially by default. This 
        (setting to 'random') often leads to significantly faster convergence 
        especially when tol is higher than 1e-4. 
 
    Attributes 
    ---------- 
    coef_ : ndarray of shape (n_features,) or (n_targets, n_features) 
        Parameter vector (w in the cost function formula). 
 
    dual_gap_ : float or ndarray of shape (n_targets,) 
        Given param alpha, the dual gaps at the end of the optimization, 
        same shape as each observation of y. 
 
    sparse_coef_ : sparse matrix of shape (n_features, 1) or \ 
            (n_targets, n_features) 
        Readonly property derived from ``coef_``. 
 
    intercept_ : float or ndarray of shape (n_targets,) 
        Independent term in decision function. 
 
    n_iter_ : int or list of int 
        Number of iterations run by the coordinate descent solver to reach 
        the specified tolerance. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    lars_path : Regularization path using LARS. 
    lasso_path : Regularization path using Lasso. 
    LassoLars : Lasso Path along the regularization parameter using LARS algorithm. 
    LassoCV : Lasso alpha parameter by cross-validation. 
    LassoLarsCV : Lasso least angle parameter algorithm by cross-validation. 
    sklearn.decomposition.sparse_encode : Sparse coding array estimator. 
 
    Notes 
    ----- 
    The algorithm used to fit the model is coordinate descent. 
 
    To avoid unnecessary memory duplication the X argument of the fit method 
    should be directly passed as a Fortran-contiguous numpy array. 
 
    Regularization improves the conditioning of the problem and 
    reduces the variance of the estimates. Larger values specify stronger 
    regularization. Alpha corresponds to `1 / (2C)` in other linear 
    models such as :class:`~sklearn.linear_model.LogisticRegression` or 
    :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are 
    assumed to be specific to the targets. Hence they must correspond in 
    number. 
 
    The precise stopping criteria based on `tol` are the following: First, check that 
    that maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|` 
    is smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`. 
    If so, then additionally check whether the dual gap is smaller than `tol` times 
    :math:`||y||_2^2 / n_{\\text{samples}}`. 
 
    The target can be a 2-dimensional array, resulting in the optimization of the 
    following objective:: 
 
        (1 / (2 * n_samples)) * ||Y - XW||^2_F + alpha * ||W||_11 
 
    where :math:`||W||_{1,1}` is the sum of the magnitude of the matrix coefficients. 
    It should not be confused with :class:`~sklearn.linear_model.MultiTaskLasso` which 
    instead penalizes the :math:`L_{2,1}` norm of the coefficients, yielding row-wise 
    sparsity in the coefficients. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn import linear_model 
    &gt;&gt;&gt; clf = linear_model.Lasso(alpha=0.1) 
    &gt;&gt;&gt; clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2]) 
    Lasso(alpha=0.1) 
    &gt;&gt;&gt; print(clf.coef_) 
    [0.85 0.  ] 
    &gt;&gt;&gt; print(clf.intercept_) 
    0.15... 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**ElasticNet._parameter_constraints</span><span class="s2">,</span>
    <span class="s1">}</span>
    <span class="s1">_parameter_constraints.pop(</span><span class="s3">&quot;l1_ratio&quot;</span><span class="s1">)</span>

    <span class="s1">path = staticmethod(enet_path)</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">alpha=</span><span class="s5">1.0</span><span class="s2">,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">fit_intercept=</span><span class="s2">True,</span>
        <span class="s1">precompute=</span><span class="s2">False,</span>
        <span class="s1">copy_X=</span><span class="s2">True,</span>
        <span class="s1">max_iter=</span><span class="s5">1000</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s2">,</span>
        <span class="s1">warm_start=</span><span class="s2">False,</span>
        <span class="s1">positive=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s2">None,</span>
        <span class="s1">selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">alpha=alpha</span><span class="s2">,</span>
            <span class="s1">l1_ratio=</span><span class="s5">1.0</span><span class="s2">,</span>
            <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
            <span class="s1">precompute=precompute</span><span class="s2">,</span>
            <span class="s1">copy_X=copy_X</span><span class="s2">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
            <span class="s1">tol=tol</span><span class="s2">,</span>
            <span class="s1">warm_start=warm_start</span><span class="s2">,</span>
            <span class="s1">positive=positive</span><span class="s2">,</span>
            <span class="s1">random_state=random_state</span><span class="s2">,</span>
            <span class="s1">selection=selection</span><span class="s2">,</span>
        <span class="s1">)</span>


<span class="s0">###############################################################################</span>
<span class="s0"># Functions for CV with paths functions</span>


<span class="s2">def </span><span class="s1">_path_residuals(</span>
    <span class="s1">X</span><span class="s2">,</span>
    <span class="s1">y</span><span class="s2">,</span>
    <span class="s1">sample_weight</span><span class="s2">,</span>
    <span class="s1">train</span><span class="s2">,</span>
    <span class="s1">test</span><span class="s2">,</span>
    <span class="s1">fit_intercept</span><span class="s2">,</span>
    <span class="s1">path</span><span class="s2">,</span>
    <span class="s1">path_params</span><span class="s2">,</span>
    <span class="s1">alphas=</span><span class="s2">None,</span>
    <span class="s1">l1_ratio=</span><span class="s5">1</span><span class="s2">,</span>
    <span class="s1">X_order=</span><span class="s2">None,</span>
    <span class="s1">dtype=</span><span class="s2">None,</span>
<span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Returns the MSE for the models computed by 'path'. 
 
    Parameters 
    ---------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) 
        Training data. 
 
    y : array-like of shape (n_samples,) or (n_samples, n_targets) 
        Target values. 
 
    sample_weight : None or array-like of shape (n_samples,) 
        Sample weights. 
 
    train : list of indices 
        The indices of the train set. 
 
    test : list of indices 
        The indices of the test set. 
 
    path : callable 
        Function returning a list of models on the path. See 
        enet_path for an example of signature. 
 
    path_params : dictionary 
        Parameters passed to the path function. 
 
    alphas : array-like, default=None 
        Array of float that is used for cross-validation. If not 
        provided, computed using 'path'. 
 
    l1_ratio : float, default=1 
        float between 0 and 1 passed to ElasticNet (scaling between 
        l1 and l2 penalties). For ``l1_ratio = 0`` the penalty is an 
        L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty. For ``0 
        &lt; l1_ratio &lt; 1``, the penalty is a combination of L1 and L2. 
 
    X_order : {'F', 'C'}, default=None 
        The order of the arrays expected by the path function to 
        avoid memory copies. 
 
    dtype : a numpy dtype, default=None 
        The dtype of the arrays expected by the path function to 
        avoid memory copies. 
    &quot;&quot;&quot;</span>
    <span class="s1">X_train = X[train]</span>
    <span class="s1">y_train = y[train]</span>
    <span class="s1">X_test = X[test]</span>
    <span class="s1">y_test = y[test]</span>
    <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">sw_train</span><span class="s2">, </span><span class="s1">sw_test = </span><span class="s2">None, None</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">sw_train = sample_weight[train]</span>
        <span class="s1">sw_test = sample_weight[test]</span>
        <span class="s1">n_samples = X_train.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s0"># TLDR: Rescale sw_train to sum up to n_samples on the training set.</span>
        <span class="s0"># See TLDR and long comment inside ElasticNet.fit.</span>
        <span class="s1">sw_train *= n_samples / np.sum(sw_train)</span>
        <span class="s0"># Note: Alternatively, we could also have rescaled alpha instead</span>
        <span class="s0"># of sample_weight:</span>
        <span class="s0">#</span>
        <span class="s0">#     alpha *= np.sum(sample_weight) / n_samples</span>

    <span class="s2">if not </span><span class="s1">sparse.issparse(X):</span>
        <span class="s2">for </span><span class="s1">array</span><span class="s2">, </span><span class="s1">array_input </span><span class="s2">in </span><span class="s1">(</span>
            <span class="s1">(X_train</span><span class="s2">, </span><span class="s1">X)</span><span class="s2">,</span>
            <span class="s1">(y_train</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">,</span>
            <span class="s1">(X_test</span><span class="s2">, </span><span class="s1">X)</span><span class="s2">,</span>
            <span class="s1">(y_test</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">,</span>
        <span class="s1">):</span>
            <span class="s2">if </span><span class="s1">array.base </span><span class="s2">is not </span><span class="s1">array_input </span><span class="s2">and not </span><span class="s1">array.flags[</span><span class="s3">&quot;WRITEABLE&quot;</span><span class="s1">]:</span>
                <span class="s0"># fancy indexing should create a writable copy but it doesn't</span>
                <span class="s0"># for read-only memmaps (cf. numpy#14132).</span>
                <span class="s1">array.setflags(write=</span><span class="s2">True</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">y.ndim == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">precompute = path_params[</span><span class="s3">&quot;precompute&quot;</span><span class="s1">]</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s0"># No Gram variant of multi-task exists right now.</span>
        <span class="s0"># Fall back to default enet_multitask</span>
        <span class="s1">precompute = </span><span class="s2">False</span>

    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">X_offset</span><span class="s2">, </span><span class="s1">y_offset</span><span class="s2">, </span><span class="s1">X_scale</span><span class="s2">, </span><span class="s1">precompute</span><span class="s2">, </span><span class="s1">Xy = _pre_fit(</span>
        <span class="s1">X_train</span><span class="s2">,</span>
        <span class="s1">y_train</span><span class="s2">,</span>
        <span class="s2">None,</span>
        <span class="s1">precompute</span><span class="s2">,</span>
        <span class="s1">normalize=</span><span class="s2">False,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">copy=</span><span class="s2">False,</span>
        <span class="s1">sample_weight=sw_train</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">path_params = path_params.copy()</span>
    <span class="s1">path_params[</span><span class="s3">&quot;Xy&quot;</span><span class="s1">] = Xy</span>
    <span class="s1">path_params[</span><span class="s3">&quot;X_offset&quot;</span><span class="s1">] = X_offset</span>
    <span class="s1">path_params[</span><span class="s3">&quot;X_scale&quot;</span><span class="s1">] = X_scale</span>
    <span class="s1">path_params[</span><span class="s3">&quot;precompute&quot;</span><span class="s1">] = precompute</span>
    <span class="s1">path_params[</span><span class="s3">&quot;copy_X&quot;</span><span class="s1">] = </span><span class="s2">False</span>
    <span class="s1">path_params[</span><span class="s3">&quot;alphas&quot;</span><span class="s1">] = alphas</span>
    <span class="s0"># needed for sparse cd solver</span>
    <span class="s1">path_params[</span><span class="s3">&quot;sample_weight&quot;</span><span class="s1">] = sw_train</span>

    <span class="s2">if </span><span class="s3">&quot;l1_ratio&quot; </span><span class="s2">in </span><span class="s1">path_params:</span>
        <span class="s1">path_params[</span><span class="s3">&quot;l1_ratio&quot;</span><span class="s1">] = l1_ratio</span>

    <span class="s0"># Do the ordering and type casting here, as if it is done in the path,</span>
    <span class="s0"># X is copied and a reference is kept here</span>
    <span class="s1">X_train = check_array(X_train</span><span class="s2">, </span><span class="s1">accept_sparse=</span><span class="s3">&quot;csc&quot;</span><span class="s2">, </span><span class="s1">dtype=dtype</span><span class="s2">, </span><span class="s1">order=X_order)</span>
    <span class="s1">alphas</span><span class="s2">, </span><span class="s1">coefs</span><span class="s2">, </span><span class="s1">_ = path(X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">**path_params)</span>
    <span class="s2">del </span><span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span>

    <span class="s2">if </span><span class="s1">y.ndim == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s0"># Doing this so that it becomes coherent with multioutput.</span>
        <span class="s1">coefs = coefs[np.newaxis</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">:]</span>
        <span class="s1">y_offset = np.atleast_1d(y_offset)</span>
        <span class="s1">y_test = y_test[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>

    <span class="s1">intercepts = y_offset[:</span><span class="s2">, </span><span class="s1">np.newaxis] - np.dot(X_offset</span><span class="s2">, </span><span class="s1">coefs)</span>
    <span class="s1">X_test_coefs = safe_sparse_dot(X_test</span><span class="s2">, </span><span class="s1">coefs)</span>
    <span class="s1">residues = X_test_coefs - y_test[:</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">residues += intercepts</span>
    <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">this_mse = (residues**</span><span class="s5">2</span><span class="s1">).mean(axis=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">this_mse = np.average(residues**</span><span class="s5">2</span><span class="s2">, </span><span class="s1">weights=sw_test</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s2">return </span><span class="s1">this_mse.mean(axis=</span><span class="s5">0</span><span class="s1">)</span>


<span class="s2">class </span><span class="s1">LinearModelCV(MultiOutputMixin</span><span class="s2">, </span><span class="s1">LinearModel</span><span class="s2">, </span><span class="s1">ABC):</span>
    <span class="s4">&quot;&quot;&quot;Base class for iterative model fitting along a regularization path.&quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s3">&quot;eps&quot;</span><span class="s1">: [Interval(Real</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s3">&quot;neither&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s3">&quot;n_alphas&quot;</span><span class="s1">: [Interval(Integral</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s3">&quot;left&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s3">&quot;alphas&quot;</span><span class="s1">: [</span><span class="s3">&quot;array-like&quot;</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;fit_intercept&quot;</span><span class="s1">: [</span><span class="s3">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;precompute&quot;</span><span class="s1">: [StrOptions({</span><span class="s3">&quot;auto&quot;</span><span class="s1">})</span><span class="s2">, </span><span class="s3">&quot;array-like&quot;</span><span class="s2">, </span><span class="s3">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s3">&quot;left&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s3">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s3">&quot;left&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s3">&quot;copy_X&quot;</span><span class="s1">: [</span><span class="s3">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;cv&quot;</span><span class="s1">: [</span><span class="s3">&quot;cv_object&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s3">&quot;verbose&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;n_jobs&quot;</span><span class="s1">: [Integral</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;positive&quot;</span><span class="s1">: [</span><span class="s3">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s3">&quot;random_state&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;selection&quot;</span><span class="s1">: [StrOptions({</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">, </span><span class="s3">&quot;random&quot;</span><span class="s1">})]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s1">@abstractmethod</span>
    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">eps=</span><span class="s5">1e-3</span><span class="s2">,</span>
        <span class="s1">n_alphas=</span><span class="s5">100</span><span class="s2">,</span>
        <span class="s1">alphas=</span><span class="s2">None,</span>
        <span class="s1">fit_intercept=</span><span class="s2">True,</span>
        <span class="s1">precompute=</span><span class="s3">&quot;auto&quot;</span><span class="s2">,</span>
        <span class="s1">max_iter=</span><span class="s5">1000</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s2">,</span>
        <span class="s1">copy_X=</span><span class="s2">True,</span>
        <span class="s1">cv=</span><span class="s2">None,</span>
        <span class="s1">verbose=</span><span class="s2">False,</span>
        <span class="s1">n_jobs=</span><span class="s2">None,</span>
        <span class="s1">positive=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s2">None,</span>
        <span class="s1">selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">self.eps = eps</span>
        <span class="s1">self.n_alphas = n_alphas</span>
        <span class="s1">self.alphas = alphas</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.precompute = precompute</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.copy_X = copy_X</span>
        <span class="s1">self.cv = cv</span>
        <span class="s1">self.verbose = verbose</span>
        <span class="s1">self.n_jobs = n_jobs</span>
        <span class="s1">self.positive = positive</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.selection = selection</span>

    <span class="s1">@abstractmethod</span>
    <span class="s2">def </span><span class="s1">_get_estimator(self):</span>
        <span class="s4">&quot;&quot;&quot;Model to be fitted after the best alpha has been determined.&quot;&quot;&quot;</span>

    <span class="s1">@abstractmethod</span>
    <span class="s2">def </span><span class="s1">_is_multitask(self):</span>
        <span class="s4">&quot;&quot;&quot;Bool indicating if class is meant for multidimensional target.&quot;&quot;&quot;</span>

    <span class="s1">@staticmethod</span>
    <span class="s1">@abstractmethod</span>
    <span class="s2">def </span><span class="s1">path(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">**kwargs):</span>
        <span class="s4">&quot;&quot;&quot;Compute path with coordinate descent.&quot;&quot;&quot;</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s4">&quot;&quot;&quot;Fit linear model with coordinate descent. 
 
        Fit is on grid of alphas and best alpha estimated by cross-validation. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training data. Pass directly as Fortran-contiguous data 
            to avoid unnecessary memory duplication. If y is mono-output, 
            X can be sparse. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_targets) 
            Target values. 
 
        sample_weight : float or array-like of shape (n_samples,), \ 
                default=None 
            Sample weights used for fitting and evaluation of the weighted 
            mean squared error of each cv-fold. Note that the cross validated 
            MSE that is finally used to find the best model is the unweighted 
            mean over the (weighted) MSEs of each test fold. 
 
        Returns 
        ------- 
        self : object 
            Returns an instance of fitted model. 
        &quot;&quot;&quot;</span>
        <span class="s0"># This makes sure that there is no duplication in memory.</span>
        <span class="s0"># Dealing right with copy_X is important in the following:</span>
        <span class="s0"># Multiple functions touch X and subsamples of X and can induce a</span>
        <span class="s0"># lot of duplication of memory</span>
        <span class="s1">copy_X = self.copy_X </span><span class="s2">and </span><span class="s1">self.fit_intercept</span>

        <span class="s1">check_y_params = dict(</span>
            <span class="s1">copy=</span><span class="s2">False, </span><span class="s1">dtype=[np.float64</span><span class="s2">, </span><span class="s1">np.float32]</span><span class="s2">, </span><span class="s1">ensure_2d=</span><span class="s2">False</span>
        <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">isinstance(X</span><span class="s2">, </span><span class="s1">np.ndarray) </span><span class="s2">or </span><span class="s1">sparse.issparse(X):</span>
            <span class="s0"># Keep a reference to X</span>
            <span class="s1">reference_to_old_X = X</span>
            <span class="s0"># Let us not impose fortran ordering so far: it is</span>
            <span class="s0"># not useful for the cross-validation loop and will be done</span>
            <span class="s0"># by the model fitting itself</span>

            <span class="s0"># Need to validate separately here.</span>
            <span class="s0"># We can't pass multi_output=True because that would allow y to be</span>
            <span class="s0"># csr. We also want to allow y to be 64 or 32 but check_X_y only</span>
            <span class="s0"># allows to convert for 64.</span>
            <span class="s1">check_X_params = dict(</span>
                <span class="s1">accept_sparse=</span><span class="s3">&quot;csc&quot;</span><span class="s2">, </span><span class="s1">dtype=[np.float64</span><span class="s2">, </span><span class="s1">np.float32]</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span>
            <span class="s1">)</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(</span>
                <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">validate_separately=(check_X_params</span><span class="s2">, </span><span class="s1">check_y_params)</span>
            <span class="s1">)</span>
            <span class="s2">if </span><span class="s1">sparse.issparse(X):</span>
                <span class="s2">if </span><span class="s1">hasattr(reference_to_old_X</span><span class="s2">, </span><span class="s3">&quot;data&quot;</span><span class="s1">) </span><span class="s2">and not </span><span class="s1">np.may_share_memory(</span>
                    <span class="s1">reference_to_old_X.data</span><span class="s2">, </span><span class="s1">X.data</span>
                <span class="s1">):</span>
                    <span class="s0"># X is a sparse matrix and has been copied</span>
                    <span class="s1">copy_X = </span><span class="s2">False</span>
            <span class="s2">elif not </span><span class="s1">np.may_share_memory(reference_to_old_X</span><span class="s2">, </span><span class="s1">X):</span>
                <span class="s0"># X has been copied</span>
                <span class="s1">copy_X = </span><span class="s2">False</span>
            <span class="s2">del </span><span class="s1">reference_to_old_X</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s0"># Need to validate separately here.</span>
            <span class="s0"># We can't pass multi_output=True because that would allow y to be</span>
            <span class="s0"># csr. We also want to allow y to be 64 or 32 but check_X_y only</span>
            <span class="s0"># allows to convert for 64.</span>
            <span class="s1">check_X_params = dict(</span>
                <span class="s1">accept_sparse=</span><span class="s3">&quot;csc&quot;</span><span class="s2">,</span>
                <span class="s1">dtype=[np.float64</span><span class="s2">, </span><span class="s1">np.float32]</span><span class="s2">,</span>
                <span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s2">,</span>
                <span class="s1">copy=copy_X</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(</span>
                <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">validate_separately=(check_X_params</span><span class="s2">, </span><span class="s1">check_y_params)</span>
            <span class="s1">)</span>
            <span class="s1">copy_X = </span><span class="s2">False</span>

        <span class="s1">check_consistent_length(X</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s2">if not </span><span class="s1">self._is_multitask():</span>
            <span class="s2">if </span><span class="s1">y.ndim &gt; </span><span class="s5">1 </span><span class="s2">and </span><span class="s1">y.shape[</span><span class="s5">1</span><span class="s1">] &gt; </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s3">&quot;For multi-task outputs, use MultiTask%s&quot; </span><span class="s1">% self.__class__.__name__</span>
                <span class="s1">)</span>
            <span class="s1">y = column_or_1d(y</span><span class="s2">, </span><span class="s1">warn=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">sparse.issparse(X):</span>
                <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;X should be dense but a sparse matrix waspassed&quot;</span><span class="s1">)</span>
            <span class="s2">elif </span><span class="s1">y.ndim == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s3">&quot;For mono-task outputs, use %sCV&quot; </span><span class="s1">% self.__class__.__name__[</span><span class="s5">9</span><span class="s1">:]</span>
                <span class="s1">)</span>

        <span class="s2">if </span><span class="s1">isinstance(sample_weight</span><span class="s2">, </span><span class="s1">numbers.Number):</span>
            <span class="s1">sample_weight = </span><span class="s2">None</span>
        <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s1">model = self._get_estimator()</span>

        <span class="s0"># All LinearModelCV parameters except 'cv' are acceptable</span>
        <span class="s1">path_params = self.get_params()</span>

        <span class="s0"># Pop `intercept` that is not parameter of the path function</span>
        <span class="s1">path_params.pop(</span><span class="s3">&quot;fit_intercept&quot;</span><span class="s2">, None</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s3">&quot;l1_ratio&quot; </span><span class="s2">in </span><span class="s1">path_params:</span>
            <span class="s1">l1_ratios = np.atleast_1d(path_params[</span><span class="s3">&quot;l1_ratio&quot;</span><span class="s1">])</span>
            <span class="s0"># For the first path, we need to set l1_ratio</span>
            <span class="s1">path_params[</span><span class="s3">&quot;l1_ratio&quot;</span><span class="s1">] = l1_ratios[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">l1_ratios = [</span>
                <span class="s5">1</span><span class="s2">,</span>
            <span class="s1">]</span>
        <span class="s1">path_params.pop(</span><span class="s3">&quot;cv&quot;</span><span class="s2">, None</span><span class="s1">)</span>
        <span class="s1">path_params.pop(</span><span class="s3">&quot;n_jobs&quot;</span><span class="s2">, None</span><span class="s1">)</span>

        <span class="s1">alphas = self.alphas</span>
        <span class="s1">n_l1_ratio = len(l1_ratios)</span>

        <span class="s1">check_scalar_alpha = partial(</span>
            <span class="s1">check_scalar</span><span class="s2">,</span>
            <span class="s1">target_type=Real</span><span class="s2">,</span>
            <span class="s1">min_val=</span><span class="s5">0.0</span><span class="s2">,</span>
            <span class="s1">include_boundaries=</span><span class="s3">&quot;left&quot;</span><span class="s2">,</span>
        <span class="s1">)</span>

        <span class="s2">if </span><span class="s1">alphas </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">alphas = [</span>
                <span class="s1">_alpha_grid(</span>
                    <span class="s1">X</span><span class="s2">,</span>
                    <span class="s1">y</span><span class="s2">,</span>
                    <span class="s1">l1_ratio=l1_ratio</span><span class="s2">,</span>
                    <span class="s1">fit_intercept=self.fit_intercept</span><span class="s2">,</span>
                    <span class="s1">eps=self.eps</span><span class="s2">,</span>
                    <span class="s1">n_alphas=self.n_alphas</span><span class="s2">,</span>
                    <span class="s1">copy_X=self.copy_X</span><span class="s2">,</span>
                <span class="s1">)</span>
                <span class="s2">for </span><span class="s1">l1_ratio </span><span class="s2">in </span><span class="s1">l1_ratios</span>
            <span class="s1">]</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s0"># Making sure alphas entries are scalars.</span>
            <span class="s2">for </span><span class="s1">index</span><span class="s2">, </span><span class="s1">alpha </span><span class="s2">in </span><span class="s1">enumerate(alphas):</span>
                <span class="s1">check_scalar_alpha(alpha</span><span class="s2">, </span><span class="s3">f&quot;alphas[</span><span class="s2">{</span><span class="s1">index</span><span class="s2">}</span><span class="s3">]&quot;</span><span class="s1">)</span>
            <span class="s0"># Making sure alphas is properly ordered.</span>
            <span class="s1">alphas = np.tile(np.sort(alphas)[::-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">(n_l1_ratio</span><span class="s2">, </span><span class="s5">1</span><span class="s1">))</span>

        <span class="s0"># We want n_alphas to be the number of alphas used for each l1_ratio.</span>
        <span class="s1">n_alphas = len(alphas[</span><span class="s5">0</span><span class="s1">])</span>
        <span class="s1">path_params.update({</span><span class="s3">&quot;n_alphas&quot;</span><span class="s1">: n_alphas})</span>

        <span class="s1">path_params[</span><span class="s3">&quot;copy_X&quot;</span><span class="s1">] = copy_X</span>
        <span class="s0"># We are not computing in parallel, we can modify X</span>
        <span class="s0"># inplace in the folds</span>
        <span class="s2">if </span><span class="s1">effective_n_jobs(self.n_jobs) &gt; </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">path_params[</span><span class="s3">&quot;copy_X&quot;</span><span class="s1">] = </span><span class="s2">False</span>

        <span class="s0"># init cross-validation generator</span>
        <span class="s1">cv = check_cv(self.cv)</span>

        <span class="s0"># Compute path for all folds and compute MSE to get the best alpha</span>
        <span class="s1">folds = list(cv.split(X</span><span class="s2">, </span><span class="s1">y))</span>
        <span class="s1">best_mse = np.inf</span>

        <span class="s0"># We do a double for loop folded in one, in order to be able to</span>
        <span class="s0"># iterate in parallel on l1_ratio and folds</span>
        <span class="s1">jobs = (</span>
            <span class="s1">delayed(_path_residuals)(</span>
                <span class="s1">X</span><span class="s2">,</span>
                <span class="s1">y</span><span class="s2">,</span>
                <span class="s1">sample_weight</span><span class="s2">,</span>
                <span class="s1">train</span><span class="s2">,</span>
                <span class="s1">test</span><span class="s2">,</span>
                <span class="s1">self.fit_intercept</span><span class="s2">,</span>
                <span class="s1">self.path</span><span class="s2">,</span>
                <span class="s1">path_params</span><span class="s2">,</span>
                <span class="s1">alphas=this_alphas</span><span class="s2">,</span>
                <span class="s1">l1_ratio=this_l1_ratio</span><span class="s2">,</span>
                <span class="s1">X_order=</span><span class="s3">&quot;F&quot;</span><span class="s2">,</span>
                <span class="s1">dtype=X.dtype.type</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s2">for </span><span class="s1">this_l1_ratio</span><span class="s2">, </span><span class="s1">this_alphas </span><span class="s2">in </span><span class="s1">zip(l1_ratios</span><span class="s2">, </span><span class="s1">alphas)</span>
            <span class="s2">for </span><span class="s1">train</span><span class="s2">, </span><span class="s1">test </span><span class="s2">in </span><span class="s1">folds</span>
        <span class="s1">)</span>
        <span class="s1">mse_paths = Parallel(</span>
            <span class="s1">n_jobs=self.n_jobs</span><span class="s2">,</span>
            <span class="s1">verbose=self.verbose</span><span class="s2">,</span>
            <span class="s1">prefer=</span><span class="s3">&quot;threads&quot;</span><span class="s2">,</span>
        <span class="s1">)(jobs)</span>
        <span class="s1">mse_paths = np.reshape(mse_paths</span><span class="s2">, </span><span class="s1">(n_l1_ratio</span><span class="s2">, </span><span class="s1">len(folds)</span><span class="s2">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">))</span>
        <span class="s0"># The mean is computed over folds.</span>
        <span class="s1">mean_mse = np.mean(mse_paths</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">self.mse_path_ = np.squeeze(np.moveaxis(mse_paths</span><span class="s2">, </span><span class="s5">2</span><span class="s2">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s2">for </span><span class="s1">l1_ratio</span><span class="s2">, </span><span class="s1">l1_alphas</span><span class="s2">, </span><span class="s1">mse_alphas </span><span class="s2">in </span><span class="s1">zip(l1_ratios</span><span class="s2">, </span><span class="s1">alphas</span><span class="s2">, </span><span class="s1">mean_mse):</span>
            <span class="s1">i_best_alpha = np.argmin(mse_alphas)</span>
            <span class="s1">this_best_mse = mse_alphas[i_best_alpha]</span>
            <span class="s2">if </span><span class="s1">this_best_mse &lt; best_mse:</span>
                <span class="s1">best_alpha = l1_alphas[i_best_alpha]</span>
                <span class="s1">best_l1_ratio = l1_ratio</span>
                <span class="s1">best_mse = this_best_mse</span>

        <span class="s1">self.l1_ratio_ = best_l1_ratio</span>
        <span class="s1">self.alpha_ = best_alpha</span>
        <span class="s2">if </span><span class="s1">self.alphas </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">self.alphas_ = np.asarray(alphas)</span>
            <span class="s2">if </span><span class="s1">n_l1_ratio == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">self.alphas_ = self.alphas_[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s0"># Remove duplicate alphas in case alphas is provided.</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self.alphas_ = np.asarray(alphas[</span><span class="s5">0</span><span class="s1">])</span>

        <span class="s0"># Refit the model with the parameters selected</span>
        <span class="s1">common_params = {</span>
            <span class="s1">name: value</span>
            <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">value </span><span class="s2">in </span><span class="s1">self.get_params().items()</span>
            <span class="s2">if </span><span class="s1">name </span><span class="s2">in </span><span class="s1">model.get_params()</span>
        <span class="s1">}</span>
        <span class="s1">model.set_params(**common_params)</span>
        <span class="s1">model.alpha = best_alpha</span>
        <span class="s1">model.l1_ratio = best_l1_ratio</span>
        <span class="s1">model.copy_X = copy_X</span>
        <span class="s1">precompute = getattr(self</span><span class="s2">, </span><span class="s3">&quot;precompute&quot;</span><span class="s2">, None</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">isinstance(precompute</span><span class="s2">, </span><span class="s1">str) </span><span class="s2">and </span><span class="s1">precompute == </span><span class="s3">&quot;auto&quot;</span><span class="s1">:</span>
            <span class="s1">model.precompute = </span><span class="s2">False</span>

        <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s0"># MultiTaskElasticNetCV does not (yet) support sample_weight, even</span>
            <span class="s0"># not sample_weight=None.</span>
            <span class="s1">model.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">model.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
        <span class="s2">if not </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s3">&quot;l1_ratio&quot;</span><span class="s1">):</span>
            <span class="s2">del </span><span class="s1">self.l1_ratio_</span>
        <span class="s1">self.coef_ = model.coef_</span>
        <span class="s1">self.intercept_ = model.intercept_</span>
        <span class="s1">self.dual_gap_ = model.dual_gap_</span>
        <span class="s1">self.n_iter_ = model.n_iter_</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s0"># Note: check_sample_weights_invariance(kind='ones') should work, but</span>
        <span class="s0"># currently we can only mark a whole test as xfail.</span>
        <span class="s2">return </span><span class="s1">{</span>
            <span class="s3">&quot;_xfail_checks&quot;</span><span class="s1">: {</span>
                <span class="s3">&quot;check_sample_weights_invariance&quot;</span><span class="s1">: (</span>
                    <span class="s3">&quot;zero sample_weight is not equivalent to removing samples&quot;</span>
                <span class="s1">)</span><span class="s2">,</span>
            <span class="s1">}</span>
        <span class="s1">}</span>


<span class="s2">class </span><span class="s1">LassoCV(RegressorMixin</span><span class="s2">, </span><span class="s1">LinearModelCV):</span>
    <span class="s4">&quot;&quot;&quot;Lasso linear model with iterative fitting along a regularization path. 
 
    See glossary entry for :term:`cross-validation estimator`. 
 
    The best model is selected by cross-validation. 
 
    The optimization objective for Lasso is:: 
 
        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1 
 
    Read more in the :ref:`User Guide &lt;lasso&gt;`. 
 
    Parameters 
    ---------- 
    eps : float, default=1e-3 
        Length of the path. ``eps=1e-3`` means that 
        ``alpha_min / alpha_max = 1e-3``. 
 
    n_alphas : int, default=100 
        Number of alphas along the regularization path. 
 
    alphas : array-like, default=None 
        List of alphas where to compute the models. 
        If ``None`` alphas are set automatically. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. If set 
        to false, no intercept will be used in calculations 
        (i.e. data is expected to be centered). 
 
    precompute : 'auto', bool or array-like of shape \ 
            (n_features, n_features), default='auto' 
        Whether to use a precomputed Gram matrix to speed up 
        calculations. If set to ``'auto'`` let us decide. The Gram 
        matrix can also be passed as argument. 
 
    max_iter : int, default=1000 
        The maximum number of iterations. 
 
    tol : float, default=1e-4 
        The tolerance for the optimization: if the updates are 
        smaller than ``tol``, the optimization code checks the 
        dual gap for optimality and continues until it is smaller 
        than ``tol``. 
 
    copy_X : bool, default=True 
        If ``True``, X will be copied; else, it may be overwritten. 
 
    cv : int, cross-validation generator or iterable, default=None 
        Determines the cross-validation splitting strategy. 
        Possible inputs for cv are: 
 
        - None, to use the default 5-fold cross-validation, 
        - int, to specify the number of folds. 
        - :term:`CV splitter`, 
        - An iterable yielding (train, test) splits as arrays of indices. 
 
        For int/None inputs, :class:`~sklearn.model_selection.KFold` is used. 
 
        Refer :ref:`User Guide &lt;cross_validation&gt;` for the various 
        cross-validation strategies that can be used here. 
 
        .. versionchanged:: 0.22 
            ``cv`` default value if None changed from 3-fold to 5-fold. 
 
    verbose : bool or int, default=False 
        Amount of verbosity. 
 
    n_jobs : int, default=None 
        Number of CPUs to use during the cross validation. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
    positive : bool, default=False 
        If positive, restrict regression coefficients to be positive. 
 
    random_state : int, RandomState instance, default=None 
        The seed of the pseudo random number generator that selects a random 
        feature to update. Used when ``selection`` == 'random'. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    selection : {'cyclic', 'random'}, default='cyclic' 
        If set to 'random', a random coefficient is updated every iteration 
        rather than looping over features sequentially by default. This 
        (setting to 'random') often leads to significantly faster convergence 
        especially when tol is higher than 1e-4. 
 
    Attributes 
    ---------- 
    alpha_ : float 
        The amount of penalization chosen by cross validation. 
 
    coef_ : ndarray of shape (n_features,) or (n_targets, n_features) 
        Parameter vector (w in the cost function formula). 
 
    intercept_ : float or ndarray of shape (n_targets,) 
        Independent term in decision function. 
 
    mse_path_ : ndarray of shape (n_alphas, n_folds) 
        Mean square error for the test set on each fold, varying alpha. 
 
    alphas_ : ndarray of shape (n_alphas,) 
        The grid of alphas used for fitting. 
 
    dual_gap_ : float or ndarray of shape (n_targets,) 
        The dual gap at the end of the optimization for the optimal alpha 
        (``alpha_``). 
 
    n_iter_ : int 
        Number of iterations run by the coordinate descent solver to reach 
        the specified tolerance for the optimal alpha. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    lars_path : Compute Least Angle Regression or Lasso path using LARS 
        algorithm. 
    lasso_path : Compute Lasso path with coordinate descent. 
    Lasso : The Lasso is a linear model that estimates sparse coefficients. 
    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars. 
    LassoCV : Lasso linear model with iterative fitting along a regularization 
        path. 
    LassoLarsCV : Cross-validated Lasso using the LARS algorithm. 
 
    Notes 
    ----- 
    In `fit`, once the best parameter `alpha` is found through 
    cross-validation, the model is fit again using the entire training set. 
 
    To avoid unnecessary memory duplication the `X` argument of the `fit` 
    method should be directly passed as a Fortran-contiguous numpy array. 
 
     For an example, see 
     :ref:`examples/linear_model/plot_lasso_model_selection.py 
     &lt;sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py&gt;`. 
 
    :class:`LassoCV` leads to different results than a hyperparameter 
    search using :class:`~sklearn.model_selection.GridSearchCV` with a 
    :class:`Lasso` model. In :class:`LassoCV`, a model for a given 
    penalty `alpha` is warm started using the coefficients of the 
    closest model (trained at the previous iteration) on the 
    regularization path. It tends to speed up the hyperparameter 
    search. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.linear_model import LassoCV 
    &gt;&gt;&gt; from sklearn.datasets import make_regression 
    &gt;&gt;&gt; X, y = make_regression(noise=4, random_state=0) 
    &gt;&gt;&gt; reg = LassoCV(cv=5, random_state=0).fit(X, y) 
    &gt;&gt;&gt; reg.score(X, y) 
    0.9993... 
    &gt;&gt;&gt; reg.predict(X[:1,]) 
    array([-78.4951...]) 
    &quot;&quot;&quot;</span>

    <span class="s1">path = staticmethod(lasso_path)</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">eps=</span><span class="s5">1e-3</span><span class="s2">,</span>
        <span class="s1">n_alphas=</span><span class="s5">100</span><span class="s2">,</span>
        <span class="s1">alphas=</span><span class="s2">None,</span>
        <span class="s1">fit_intercept=</span><span class="s2">True,</span>
        <span class="s1">precompute=</span><span class="s3">&quot;auto&quot;</span><span class="s2">,</span>
        <span class="s1">max_iter=</span><span class="s5">1000</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s2">,</span>
        <span class="s1">copy_X=</span><span class="s2">True,</span>
        <span class="s1">cv=</span><span class="s2">None,</span>
        <span class="s1">verbose=</span><span class="s2">False,</span>
        <span class="s1">n_jobs=</span><span class="s2">None,</span>
        <span class="s1">positive=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s2">None,</span>
        <span class="s1">selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">eps=eps</span><span class="s2">,</span>
            <span class="s1">n_alphas=n_alphas</span><span class="s2">,</span>
            <span class="s1">alphas=alphas</span><span class="s2">,</span>
            <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
            <span class="s1">precompute=precompute</span><span class="s2">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
            <span class="s1">tol=tol</span><span class="s2">,</span>
            <span class="s1">copy_X=copy_X</span><span class="s2">,</span>
            <span class="s1">cv=cv</span><span class="s2">,</span>
            <span class="s1">verbose=verbose</span><span class="s2">,</span>
            <span class="s1">n_jobs=n_jobs</span><span class="s2">,</span>
            <span class="s1">positive=positive</span><span class="s2">,</span>
            <span class="s1">random_state=random_state</span><span class="s2">,</span>
            <span class="s1">selection=selection</span><span class="s2">,</span>
        <span class="s1">)</span>

    <span class="s2">def </span><span class="s1">_get_estimator(self):</span>
        <span class="s2">return </span><span class="s1">Lasso()</span>

    <span class="s2">def </span><span class="s1">_is_multitask(self):</span>
        <span class="s2">return False</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s3">&quot;multioutput&quot;</span><span class="s1">: </span><span class="s2">False</span><span class="s1">}</span>


<span class="s2">class </span><span class="s1">ElasticNetCV(RegressorMixin</span><span class="s2">, </span><span class="s1">LinearModelCV):</span>
    <span class="s4">&quot;&quot;&quot;Elastic Net model with iterative fitting along a regularization path. 
 
    See glossary entry for :term:`cross-validation estimator`. 
 
    Read more in the :ref:`User Guide &lt;elastic_net&gt;`. 
 
    Parameters 
    ---------- 
    l1_ratio : float or list of float, default=0.5 
        Float between 0 and 1 passed to ElasticNet (scaling between 
        l1 and l2 penalties). For ``l1_ratio = 0`` 
        the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty. 
        For ``0 &lt; l1_ratio &lt; 1``, the penalty is a combination of L1 and L2 
        This parameter can be a list, in which case the different 
        values are tested by cross-validation and the one giving the best 
        prediction score is used. Note that a good choice of list of 
        values for l1_ratio is often to put more values close to 1 
        (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7, 
        .9, .95, .99, 1]``. 
 
    eps : float, default=1e-3 
        Length of the path. ``eps=1e-3`` means that 
        ``alpha_min / alpha_max = 1e-3``. 
 
    n_alphas : int, default=100 
        Number of alphas along the regularization path, used for each l1_ratio. 
 
    alphas : array-like, default=None 
        List of alphas where to compute the models. 
        If None alphas are set automatically. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. If set 
        to false, no intercept will be used in calculations 
        (i.e. data is expected to be centered). 
 
    precompute : 'auto', bool or array-like of shape \ 
            (n_features, n_features), default='auto' 
        Whether to use a precomputed Gram matrix to speed up 
        calculations. If set to ``'auto'`` let us decide. The Gram 
        matrix can also be passed as argument. 
 
    max_iter : int, default=1000 
        The maximum number of iterations. 
 
    tol : float, default=1e-4 
        The tolerance for the optimization: if the updates are 
        smaller than ``tol``, the optimization code checks the 
        dual gap for optimality and continues until it is smaller 
        than ``tol``. 
 
    cv : int, cross-validation generator or iterable, default=None 
        Determines the cross-validation splitting strategy. 
        Possible inputs for cv are: 
 
        - None, to use the default 5-fold cross-validation, 
        - int, to specify the number of folds. 
        - :term:`CV splitter`, 
        - An iterable yielding (train, test) splits as arrays of indices. 
 
        For int/None inputs, :class:`~sklearn.model_selection.KFold` is used. 
 
        Refer :ref:`User Guide &lt;cross_validation&gt;` for the various 
        cross-validation strategies that can be used here. 
 
        .. versionchanged:: 0.22 
            ``cv`` default value if None changed from 3-fold to 5-fold. 
 
    copy_X : bool, default=True 
        If ``True``, X will be copied; else, it may be overwritten. 
 
    verbose : bool or int, default=0 
        Amount of verbosity. 
 
    n_jobs : int, default=None 
        Number of CPUs to use during the cross validation. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
    positive : bool, default=False 
        When set to ``True``, forces the coefficients to be positive. 
 
    random_state : int, RandomState instance, default=None 
        The seed of the pseudo random number generator that selects a random 
        feature to update. Used when ``selection`` == 'random'. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    selection : {'cyclic', 'random'}, default='cyclic' 
        If set to 'random', a random coefficient is updated every iteration 
        rather than looping over features sequentially by default. This 
        (setting to 'random') often leads to significantly faster convergence 
        especially when tol is higher than 1e-4. 
 
    Attributes 
    ---------- 
    alpha_ : float 
        The amount of penalization chosen by cross validation. 
 
    l1_ratio_ : float 
        The compromise between l1 and l2 penalization chosen by 
        cross validation. 
 
    coef_ : ndarray of shape (n_features,) or (n_targets, n_features) 
        Parameter vector (w in the cost function formula). 
 
    intercept_ : float or ndarray of shape (n_targets, n_features) 
        Independent term in the decision function. 
 
    mse_path_ : ndarray of shape (n_l1_ratio, n_alpha, n_folds) 
        Mean square error for the test set on each fold, varying l1_ratio and 
        alpha. 
 
    alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas) 
        The grid of alphas used for fitting, for each l1_ratio. 
 
    dual_gap_ : float 
        The dual gaps at the end of the optimization for the optimal alpha. 
 
    n_iter_ : int 
        Number of iterations run by the coordinate descent solver to reach 
        the specified tolerance for the optimal alpha. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    enet_path : Compute elastic net path with coordinate descent. 
    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer. 
 
    Notes 
    ----- 
    In `fit`, once the best parameters `l1_ratio` and `alpha` are found through 
    cross-validation, the model is fit again using the entire training set. 
 
    To avoid unnecessary memory duplication the `X` argument of the `fit` 
    method should be directly passed as a Fortran-contiguous numpy array. 
 
    The parameter `l1_ratio` corresponds to alpha in the glmnet R package 
    while alpha corresponds to the lambda parameter in glmnet. 
    More specifically, the optimization objective is:: 
 
        1 / (2 * n_samples) * ||y - Xw||^2_2 
        + alpha * l1_ratio * ||w||_1 
        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2 
 
    If you are interested in controlling the L1 and L2 penalty 
    separately, keep in mind that this is equivalent to:: 
 
        a * L1 + b * L2 
 
    for:: 
 
        alpha = a + b and l1_ratio = a / (a + b). 
 
    For an example, see 
    :ref:`examples/linear_model/plot_lasso_model_selection.py 
    &lt;sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py&gt;`. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.linear_model import ElasticNetCV 
    &gt;&gt;&gt; from sklearn.datasets import make_regression 
 
    &gt;&gt;&gt; X, y = make_regression(n_features=2, random_state=0) 
    &gt;&gt;&gt; regr = ElasticNetCV(cv=5, random_state=0) 
    &gt;&gt;&gt; regr.fit(X, y) 
    ElasticNetCV(cv=5, random_state=0) 
    &gt;&gt;&gt; print(regr.alpha_) 
    0.199... 
    &gt;&gt;&gt; print(regr.intercept_) 
    0.398... 
    &gt;&gt;&gt; print(regr.predict([[0, 0]])) 
    [0.398...] 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**LinearModelCV._parameter_constraints</span><span class="s2">,</span>
        <span class="s3">&quot;l1_ratio&quot;</span><span class="s1">: [Interval(Real</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s1">closed=</span><span class="s3">&quot;both&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;array-like&quot;</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s1">path = staticmethod(enet_path)</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">l1_ratio=</span><span class="s5">0.5</span><span class="s2">,</span>
        <span class="s1">eps=</span><span class="s5">1e-3</span><span class="s2">,</span>
        <span class="s1">n_alphas=</span><span class="s5">100</span><span class="s2">,</span>
        <span class="s1">alphas=</span><span class="s2">None,</span>
        <span class="s1">fit_intercept=</span><span class="s2">True,</span>
        <span class="s1">precompute=</span><span class="s3">&quot;auto&quot;</span><span class="s2">,</span>
        <span class="s1">max_iter=</span><span class="s5">1000</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s2">,</span>
        <span class="s1">cv=</span><span class="s2">None,</span>
        <span class="s1">copy_X=</span><span class="s2">True,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s2">,</span>
        <span class="s1">n_jobs=</span><span class="s2">None,</span>
        <span class="s1">positive=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s2">None,</span>
        <span class="s1">selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">self.l1_ratio = l1_ratio</span>
        <span class="s1">self.eps = eps</span>
        <span class="s1">self.n_alphas = n_alphas</span>
        <span class="s1">self.alphas = alphas</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.precompute = precompute</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.cv = cv</span>
        <span class="s1">self.copy_X = copy_X</span>
        <span class="s1">self.verbose = verbose</span>
        <span class="s1">self.n_jobs = n_jobs</span>
        <span class="s1">self.positive = positive</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.selection = selection</span>

    <span class="s2">def </span><span class="s1">_get_estimator(self):</span>
        <span class="s2">return </span><span class="s1">ElasticNet()</span>

    <span class="s2">def </span><span class="s1">_is_multitask(self):</span>
        <span class="s2">return False</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s3">&quot;multioutput&quot;</span><span class="s1">: </span><span class="s2">False</span><span class="s1">}</span>


<span class="s0">###############################################################################</span>
<span class="s0"># Multi Task ElasticNet and Lasso models (with joint feature selection)</span>


<span class="s2">class </span><span class="s1">MultiTaskElasticNet(Lasso):</span>
    <span class="s4">&quot;&quot;&quot;Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer. 
 
    The optimization objective for MultiTaskElasticNet is:: 
 
        (1 / (2 * n_samples)) * ||Y - XW||_Fro^2 
        + alpha * l1_ratio * ||W||_21 
        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2 
 
    Where:: 
 
        ||W||_21 = sum_i sqrt(sum_j W_ij ^ 2) 
 
    i.e. the sum of norms of each row. 
 
    Read more in the :ref:`User Guide &lt;multi_task_elastic_net&gt;`. 
 
    Parameters 
    ---------- 
    alpha : float, default=1.0 
        Constant that multiplies the L1/L2 term. Defaults to 1.0. 
 
    l1_ratio : float, default=0.5 
        The ElasticNet mixing parameter, with 0 &lt; l1_ratio &lt;= 1. 
        For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it 
        is an L2 penalty. 
        For ``0 &lt; l1_ratio &lt; 1``, the penalty is a combination of L1/L2 and L2. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. If set 
        to false, no intercept will be used in calculations 
        (i.e. data is expected to be centered). 
 
    copy_X : bool, default=True 
        If ``True``, X will be copied; else, it may be overwritten. 
 
    max_iter : int, default=1000 
        The maximum number of iterations. 
 
    tol : float, default=1e-4 
        The tolerance for the optimization: if the updates are 
        smaller than ``tol``, the optimization code checks the 
        dual gap for optimality and continues until it is smaller 
        than ``tol``. 
 
    warm_start : bool, default=False 
        When set to ``True``, reuse the solution of the previous call to fit as 
        initialization, otherwise, just erase the previous solution. 
        See :term:`the Glossary &lt;warm_start&gt;`. 
 
    random_state : int, RandomState instance, default=None 
        The seed of the pseudo random number generator that selects a random 
        feature to update. Used when ``selection`` == 'random'. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    selection : {'cyclic', 'random'}, default='cyclic' 
        If set to 'random', a random coefficient is updated every iteration 
        rather than looping over features sequentially by default. This 
        (setting to 'random') often leads to significantly faster convergence 
        especially when tol is higher than 1e-4. 
 
    Attributes 
    ---------- 
    intercept_ : ndarray of shape (n_targets,) 
        Independent term in decision function. 
 
    coef_ : ndarray of shape (n_targets, n_features) 
        Parameter vector (W in the cost function formula). If a 1D y is 
        passed in at fit (non multi-task usage), ``coef_`` is then a 1D array. 
        Note that ``coef_`` stores the transpose of ``W``, ``W.T``. 
 
    n_iter_ : int 
        Number of iterations run by the coordinate descent solver to reach 
        the specified tolerance. 
 
    dual_gap_ : float 
        The dual gaps at the end of the optimization. 
 
    eps_ : float 
        The tolerance scaled scaled by the variance of the target `y`. 
 
    sparse_coef_ : sparse matrix of shape (n_features,) or \ 
            (n_targets, n_features) 
        Sparse representation of the `coef_`. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in 
        cross-validation. 
    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer. 
    MultiTaskLasso : Multi-task L1/L2 Lasso with built-in cross-validation. 
 
    Notes 
    ----- 
    The algorithm used to fit the model is coordinate descent. 
 
    To avoid unnecessary memory duplication the X and y arguments of the fit 
    method should be directly passed as Fortran-contiguous numpy arrays. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn import linear_model 
    &gt;&gt;&gt; clf = linear_model.MultiTaskElasticNet(alpha=0.1) 
    &gt;&gt;&gt; clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]]) 
    MultiTaskElasticNet(alpha=0.1) 
    &gt;&gt;&gt; print(clf.coef_) 
    [[0.45663524 0.45612256] 
     [0.45663524 0.45612256]] 
    &gt;&gt;&gt; print(clf.intercept_) 
    [0.0872422 0.0872422] 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**ElasticNet._parameter_constraints</span><span class="s2">,</span>
    <span class="s1">}</span>
    <span class="s2">for </span><span class="s1">param </span><span class="s2">in </span><span class="s1">(</span><span class="s3">&quot;precompute&quot;</span><span class="s2">, </span><span class="s3">&quot;positive&quot;</span><span class="s1">):</span>
        <span class="s1">_parameter_constraints.pop(param)</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">alpha=</span><span class="s5">1.0</span><span class="s2">,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">l1_ratio=</span><span class="s5">0.5</span><span class="s2">,</span>
        <span class="s1">fit_intercept=</span><span class="s2">True,</span>
        <span class="s1">copy_X=</span><span class="s2">True,</span>
        <span class="s1">max_iter=</span><span class="s5">1000</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s2">,</span>
        <span class="s1">warm_start=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s2">None,</span>
        <span class="s1">selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">self.l1_ratio = l1_ratio</span>
        <span class="s1">self.alpha = alpha</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.copy_X = copy_X</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.warm_start = warm_start</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.selection = selection</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s4">&quot;&quot;&quot;Fit MultiTaskElasticNet model with coordinate descent. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples, n_features) 
            Data. 
        y : ndarray of shape (n_samples, n_targets) 
            Target. Will be cast to X's dtype if necessary. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
 
        Notes 
        ----- 
        Coordinate descent is an algorithm that considers each column of 
        data at a time hence it will automatically convert the X input 
        as a Fortran-contiguous numpy array if necessary. 
 
        To avoid memory re-allocation it is advised to allocate the 
        initial data in memory directly using that format. 
        &quot;&quot;&quot;</span>
        <span class="s0"># Need to validate separately here.</span>
        <span class="s0"># We can't pass multi_output=True because that would allow y to be csr.</span>
        <span class="s1">check_X_params = dict(</span>
            <span class="s1">dtype=[np.float64</span><span class="s2">, </span><span class="s1">np.float32]</span><span class="s2">,</span>
            <span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s2">,</span>
            <span class="s1">copy=self.copy_X </span><span class="s2">and </span><span class="s1">self.fit_intercept</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">check_y_params = dict(ensure_2d=</span><span class="s2">False, </span><span class="s1">order=</span><span class="s3">&quot;F&quot;</span><span class="s1">)</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">validate_separately=(check_X_params</span><span class="s2">, </span><span class="s1">check_y_params)</span>
        <span class="s1">)</span>
        <span class="s1">check_consistent_length(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">y = y.astype(X.dtype)</span>

        <span class="s2">if </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s3">&quot;l1_ratio&quot;</span><span class="s1">):</span>
            <span class="s1">model_str = </span><span class="s3">&quot;ElasticNet&quot;</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">model_str = </span><span class="s3">&quot;Lasso&quot;</span>
        <span class="s2">if </span><span class="s1">y.ndim == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;For mono-task outputs, use %s&quot; </span><span class="s1">% model_str)</span>

        <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X.shape</span>
        <span class="s1">n_targets = y.shape[</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">X_offset</span><span class="s2">, </span><span class="s1">y_offset</span><span class="s2">, </span><span class="s1">X_scale = _preprocess_data(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">self.fit_intercept</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span>
        <span class="s1">)</span>

        <span class="s2">if not </span><span class="s1">self.warm_start </span><span class="s2">or not </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s3">&quot;coef_&quot;</span><span class="s1">):</span>
            <span class="s1">self.coef_ = np.zeros(</span>
                <span class="s1">(n_targets</span><span class="s2">, </span><span class="s1">n_features)</span><span class="s2">, </span><span class="s1">dtype=X.dtype.type</span><span class="s2">, </span><span class="s1">order=</span><span class="s3">&quot;F&quot;</span>
            <span class="s1">)</span>

        <span class="s1">l1_reg = self.alpha * self.l1_ratio * n_samples</span>
        <span class="s1">l2_reg = self.alpha * (</span><span class="s5">1.0 </span><span class="s1">- self.l1_ratio) * n_samples</span>

        <span class="s1">self.coef_ = np.asfortranarray(self.coef_)  </span><span class="s0"># coef contiguous in memory</span>

        <span class="s1">random = self.selection == </span><span class="s3">&quot;random&quot;</span>

        <span class="s1">(</span>
            <span class="s1">self.coef_</span><span class="s2">,</span>
            <span class="s1">self.dual_gap_</span><span class="s2">,</span>
            <span class="s1">self.eps_</span><span class="s2">,</span>
            <span class="s1">self.n_iter_</span><span class="s2">,</span>
        <span class="s1">) = cd_fast.enet_coordinate_descent_multi_task(</span>
            <span class="s1">self.coef_</span><span class="s2">,</span>
            <span class="s1">l1_reg</span><span class="s2">,</span>
            <span class="s1">l2_reg</span><span class="s2">,</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">y</span><span class="s2">,</span>
            <span class="s1">self.max_iter</span><span class="s2">,</span>
            <span class="s1">self.tol</span><span class="s2">,</span>
            <span class="s1">check_random_state(self.random_state)</span><span class="s2">,</span>
            <span class="s1">random</span><span class="s2">,</span>
        <span class="s1">)</span>

        <span class="s0"># account for different objective scaling here and in cd_fast</span>
        <span class="s1">self.dual_gap_ /= n_samples</span>

        <span class="s1">self._set_intercept(X_offset</span><span class="s2">, </span><span class="s1">y_offset</span><span class="s2">, </span><span class="s1">X_scale)</span>

        <span class="s0"># return self for chaining fit and predict calls</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s3">&quot;multioutput_only&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>


<span class="s2">class </span><span class="s1">MultiTaskLasso(MultiTaskElasticNet):</span>
    <span class="s4">&quot;&quot;&quot;Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer. 
 
    The optimization objective for Lasso is:: 
 
        (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21 
 
    Where:: 
 
        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2} 
 
    i.e. the sum of norm of each row. 
 
    Read more in the :ref:`User Guide &lt;multi_task_lasso&gt;`. 
 
    Parameters 
    ---------- 
    alpha : float, default=1.0 
        Constant that multiplies the L1/L2 term. Defaults to 1.0. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. If set 
        to false, no intercept will be used in calculations 
        (i.e. data is expected to be centered). 
 
    copy_X : bool, default=True 
        If ``True``, X will be copied; else, it may be overwritten. 
 
    max_iter : int, default=1000 
        The maximum number of iterations. 
 
    tol : float, default=1e-4 
        The tolerance for the optimization: if the updates are 
        smaller than ``tol``, the optimization code checks the 
        dual gap for optimality and continues until it is smaller 
        than ``tol``. 
 
    warm_start : bool, default=False 
        When set to ``True``, reuse the solution of the previous call to fit as 
        initialization, otherwise, just erase the previous solution. 
        See :term:`the Glossary &lt;warm_start&gt;`. 
 
    random_state : int, RandomState instance, default=None 
        The seed of the pseudo random number generator that selects a random 
        feature to update. Used when ``selection`` == 'random'. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    selection : {'cyclic', 'random'}, default='cyclic' 
        If set to 'random', a random coefficient is updated every iteration 
        rather than looping over features sequentially by default. This 
        (setting to 'random') often leads to significantly faster convergence 
        especially when tol is higher than 1e-4. 
 
    Attributes 
    ---------- 
    coef_ : ndarray of shape (n_targets, n_features) 
        Parameter vector (W in the cost function formula). 
        Note that ``coef_`` stores the transpose of ``W``, ``W.T``. 
 
    intercept_ : ndarray of shape (n_targets,) 
        Independent term in decision function. 
 
    n_iter_ : int 
        Number of iterations run by the coordinate descent solver to reach 
        the specified tolerance. 
 
    dual_gap_ : ndarray of shape (n_alphas,) 
        The dual gaps at the end of the optimization for each alpha. 
 
    eps_ : float 
        The tolerance scaled scaled by the variance of the target `y`. 
 
    sparse_coef_ : sparse matrix of shape (n_features,) or \ 
            (n_targets, n_features) 
        Sparse representation of the `coef_`. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    Lasso: Linear Model trained with L1 prior as regularizer (aka the Lasso). 
    MultiTaskLasso: Multi-task L1/L2 Lasso with built-in cross-validation. 
    MultiTaskElasticNet: Multi-task L1/L2 ElasticNet with built-in cross-validation. 
 
    Notes 
    ----- 
    The algorithm used to fit the model is coordinate descent. 
 
    To avoid unnecessary memory duplication the X and y arguments of the fit 
    method should be directly passed as Fortran-contiguous numpy arrays. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn import linear_model 
    &gt;&gt;&gt; clf = linear_model.MultiTaskLasso(alpha=0.1) 
    &gt;&gt;&gt; clf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]]) 
    MultiTaskLasso(alpha=0.1) 
    &gt;&gt;&gt; print(clf.coef_) 
    [[0.         0.60809415] 
    [0.         0.94592424]] 
    &gt;&gt;&gt; print(clf.intercept_) 
    [-0.41888636 -0.87382323] 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**MultiTaskElasticNet._parameter_constraints</span><span class="s2">,</span>
    <span class="s1">}</span>
    <span class="s1">_parameter_constraints.pop(</span><span class="s3">&quot;l1_ratio&quot;</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">alpha=</span><span class="s5">1.0</span><span class="s2">,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">fit_intercept=</span><span class="s2">True,</span>
        <span class="s1">copy_X=</span><span class="s2">True,</span>
        <span class="s1">max_iter=</span><span class="s5">1000</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s2">,</span>
        <span class="s1">warm_start=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s2">None,</span>
        <span class="s1">selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">self.alpha = alpha</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.copy_X = copy_X</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.warm_start = warm_start</span>
        <span class="s1">self.l1_ratio = </span><span class="s5">1.0</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.selection = selection</span>


<span class="s2">class </span><span class="s1">MultiTaskElasticNetCV(RegressorMixin</span><span class="s2">, </span><span class="s1">LinearModelCV):</span>
    <span class="s4">&quot;&quot;&quot;Multi-task L1/L2 ElasticNet with built-in cross-validation. 
 
    See glossary entry for :term:`cross-validation estimator`. 
 
    The optimization objective for MultiTaskElasticNet is:: 
 
        (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 
        + alpha * l1_ratio * ||W||_21 
        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2 
 
    Where:: 
 
        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2} 
 
    i.e. the sum of norm of each row. 
 
    Read more in the :ref:`User Guide &lt;multi_task_elastic_net&gt;`. 
 
    .. versionadded:: 0.15 
 
    Parameters 
    ---------- 
    l1_ratio : float or list of float, default=0.5 
        The ElasticNet mixing parameter, with 0 &lt; l1_ratio &lt;= 1. 
        For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it 
        is an L2 penalty. 
        For ``0 &lt; l1_ratio &lt; 1``, the penalty is a combination of L1/L2 and L2. 
        This parameter can be a list, in which case the different 
        values are tested by cross-validation and the one giving the best 
        prediction score is used. Note that a good choice of list of 
        values for l1_ratio is often to put more values close to 1 
        (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7, 
        .9, .95, .99, 1]``. 
 
    eps : float, default=1e-3 
        Length of the path. ``eps=1e-3`` means that 
        ``alpha_min / alpha_max = 1e-3``. 
 
    n_alphas : int, default=100 
        Number of alphas along the regularization path. 
 
    alphas : array-like, default=None 
        List of alphas where to compute the models. 
        If not provided, set automatically. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. If set 
        to false, no intercept will be used in calculations 
        (i.e. data is expected to be centered). 
 
    max_iter : int, default=1000 
        The maximum number of iterations. 
 
    tol : float, default=1e-4 
        The tolerance for the optimization: if the updates are 
        smaller than ``tol``, the optimization code checks the 
        dual gap for optimality and continues until it is smaller 
        than ``tol``. 
 
    cv : int, cross-validation generator or iterable, default=None 
        Determines the cross-validation splitting strategy. 
        Possible inputs for cv are: 
 
        - None, to use the default 5-fold cross-validation, 
        - int, to specify the number of folds. 
        - :term:`CV splitter`, 
        - An iterable yielding (train, test) splits as arrays of indices. 
 
        For int/None inputs, :class:`~sklearn.model_selection.KFold` is used. 
 
        Refer :ref:`User Guide &lt;cross_validation&gt;` for the various 
        cross-validation strategies that can be used here. 
 
        .. versionchanged:: 0.22 
            ``cv`` default value if None changed from 3-fold to 5-fold. 
 
    copy_X : bool, default=True 
        If ``True``, X will be copied; else, it may be overwritten. 
 
    verbose : bool or int, default=0 
        Amount of verbosity. 
 
    n_jobs : int, default=None 
        Number of CPUs to use during the cross validation. Note that this is 
        used only if multiple values for l1_ratio are given. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
    random_state : int, RandomState instance, default=None 
        The seed of the pseudo random number generator that selects a random 
        feature to update. Used when ``selection`` == 'random'. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    selection : {'cyclic', 'random'}, default='cyclic' 
        If set to 'random', a random coefficient is updated every iteration 
        rather than looping over features sequentially by default. This 
        (setting to 'random') often leads to significantly faster convergence 
        especially when tol is higher than 1e-4. 
 
    Attributes 
    ---------- 
    intercept_ : ndarray of shape (n_targets,) 
        Independent term in decision function. 
 
    coef_ : ndarray of shape (n_targets, n_features) 
        Parameter vector (W in the cost function formula). 
        Note that ``coef_`` stores the transpose of ``W``, ``W.T``. 
 
    alpha_ : float 
        The amount of penalization chosen by cross validation. 
 
    mse_path_ : ndarray of shape (n_alphas, n_folds) or \ 
                (n_l1_ratio, n_alphas, n_folds) 
        Mean square error for the test set on each fold, varying alpha. 
 
    alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas) 
        The grid of alphas used for fitting, for each l1_ratio. 
 
    l1_ratio_ : float 
        Best l1_ratio obtained by cross-validation. 
 
    n_iter_ : int 
        Number of iterations run by the coordinate descent solver to reach 
        the specified tolerance for the optimal alpha. 
 
    dual_gap_ : float 
        The dual gap at the end of the optimization for the optimal alpha. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    MultiTaskElasticNet : Multi-task L1/L2 ElasticNet with built-in cross-validation. 
    ElasticNetCV : Elastic net model with best model selection by 
        cross-validation. 
    MultiTaskLassoCV : Multi-task Lasso model trained with L1/L2 
        mixed-norm as regularizer. 
 
    Notes 
    ----- 
    The algorithm used to fit the model is coordinate descent. 
 
    In `fit`, once the best parameters `l1_ratio` and `alpha` are found through 
    cross-validation, the model is fit again using the entire training set. 
 
    To avoid unnecessary memory duplication the `X` and `y` arguments of the 
    `fit` method should be directly passed as Fortran-contiguous numpy arrays. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn import linear_model 
    &gt;&gt;&gt; clf = linear_model.MultiTaskElasticNetCV(cv=3) 
    &gt;&gt;&gt; clf.fit([[0,0], [1, 1], [2, 2]], 
    ...         [[0, 0], [1, 1], [2, 2]]) 
    MultiTaskElasticNetCV(cv=3) 
    &gt;&gt;&gt; print(clf.coef_) 
    [[0.52875032 0.46958558] 
     [0.52875032 0.46958558]] 
    &gt;&gt;&gt; print(clf.intercept_) 
    [0.00166409 0.00166409] 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**LinearModelCV._parameter_constraints</span><span class="s2">,</span>
        <span class="s3">&quot;l1_ratio&quot;</span><span class="s1">: [Interval(Real</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s1">closed=</span><span class="s3">&quot;both&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s3">&quot;array-like&quot;</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">}</span>
    <span class="s1">_parameter_constraints.pop(</span><span class="s3">&quot;precompute&quot;</span><span class="s1">)</span>
    <span class="s1">_parameter_constraints.pop(</span><span class="s3">&quot;positive&quot;</span><span class="s1">)</span>

    <span class="s1">path = staticmethod(enet_path)</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">l1_ratio=</span><span class="s5">0.5</span><span class="s2">,</span>
        <span class="s1">eps=</span><span class="s5">1e-3</span><span class="s2">,</span>
        <span class="s1">n_alphas=</span><span class="s5">100</span><span class="s2">,</span>
        <span class="s1">alphas=</span><span class="s2">None,</span>
        <span class="s1">fit_intercept=</span><span class="s2">True,</span>
        <span class="s1">max_iter=</span><span class="s5">1000</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s2">,</span>
        <span class="s1">cv=</span><span class="s2">None,</span>
        <span class="s1">copy_X=</span><span class="s2">True,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s2">,</span>
        <span class="s1">n_jobs=</span><span class="s2">None,</span>
        <span class="s1">random_state=</span><span class="s2">None,</span>
        <span class="s1">selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">self.l1_ratio = l1_ratio</span>
        <span class="s1">self.eps = eps</span>
        <span class="s1">self.n_alphas = n_alphas</span>
        <span class="s1">self.alphas = alphas</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.cv = cv</span>
        <span class="s1">self.copy_X = copy_X</span>
        <span class="s1">self.verbose = verbose</span>
        <span class="s1">self.n_jobs = n_jobs</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.selection = selection</span>

    <span class="s2">def </span><span class="s1">_get_estimator(self):</span>
        <span class="s2">return </span><span class="s1">MultiTaskElasticNet()</span>

    <span class="s2">def </span><span class="s1">_is_multitask(self):</span>
        <span class="s2">return True</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s3">&quot;multioutput_only&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>

    <span class="s0"># This is necessary as LinearModelCV now supports sample_weight while</span>
    <span class="s0"># MultiTaskElasticNet does not (yet).</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s4">&quot;&quot;&quot;Fit MultiTaskElasticNet model with coordinate descent. 
 
        Fit is on grid of alphas and best alpha estimated by cross-validation. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples, n_features) 
            Training data. 
        y : ndarray of shape (n_samples, n_targets) 
            Training target variable. Will be cast to X's dtype if necessary. 
 
        Returns 
        ------- 
        self : object 
            Returns MultiTaskElasticNet instance. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">super().fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">class </span><span class="s1">MultiTaskLassoCV(RegressorMixin</span><span class="s2">, </span><span class="s1">LinearModelCV):</span>
    <span class="s4">&quot;&quot;&quot;Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer. 
 
    See glossary entry for :term:`cross-validation estimator`. 
 
    The optimization objective for MultiTaskLasso is:: 
 
        (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21 
 
    Where:: 
 
        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2} 
 
    i.e. the sum of norm of each row. 
 
    Read more in the :ref:`User Guide &lt;multi_task_lasso&gt;`. 
 
    .. versionadded:: 0.15 
 
    Parameters 
    ---------- 
    eps : float, default=1e-3 
        Length of the path. ``eps=1e-3`` means that 
        ``alpha_min / alpha_max = 1e-3``. 
 
    n_alphas : int, default=100 
        Number of alphas along the regularization path. 
 
    alphas : array-like, default=None 
        List of alphas where to compute the models. 
        If not provided, set automatically. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. If set 
        to false, no intercept will be used in calculations 
        (i.e. data is expected to be centered). 
 
    max_iter : int, default=1000 
        The maximum number of iterations. 
 
    tol : float, default=1e-4 
        The tolerance for the optimization: if the updates are 
        smaller than ``tol``, the optimization code checks the 
        dual gap for optimality and continues until it is smaller 
        than ``tol``. 
 
    copy_X : bool, default=True 
        If ``True``, X will be copied; else, it may be overwritten. 
 
    cv : int, cross-validation generator or iterable, default=None 
        Determines the cross-validation splitting strategy. 
        Possible inputs for cv are: 
 
        - None, to use the default 5-fold cross-validation, 
        - int, to specify the number of folds. 
        - :term:`CV splitter`, 
        - An iterable yielding (train, test) splits as arrays of indices. 
 
        For int/None inputs, :class:`~sklearn.model_selection.KFold` is used. 
 
        Refer :ref:`User Guide &lt;cross_validation&gt;` for the various 
        cross-validation strategies that can be used here. 
 
        .. versionchanged:: 0.22 
            ``cv`` default value if None changed from 3-fold to 5-fold. 
 
    verbose : bool or int, default=False 
        Amount of verbosity. 
 
    n_jobs : int, default=None 
        Number of CPUs to use during the cross validation. Note that this is 
        used only if multiple values for l1_ratio are given. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
    random_state : int, RandomState instance, default=None 
        The seed of the pseudo random number generator that selects a random 
        feature to update. Used when ``selection`` == 'random'. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    selection : {'cyclic', 'random'}, default='cyclic' 
        If set to 'random', a random coefficient is updated every iteration 
        rather than looping over features sequentially by default. This 
        (setting to 'random') often leads to significantly faster convergence 
        especially when tol is higher than 1e-4. 
 
    Attributes 
    ---------- 
    intercept_ : ndarray of shape (n_targets,) 
        Independent term in decision function. 
 
    coef_ : ndarray of shape (n_targets, n_features) 
        Parameter vector (W in the cost function formula). 
        Note that ``coef_`` stores the transpose of ``W``, ``W.T``. 
 
    alpha_ : float 
        The amount of penalization chosen by cross validation. 
 
    mse_path_ : ndarray of shape (n_alphas, n_folds) 
        Mean square error for the test set on each fold, varying alpha. 
 
    alphas_ : ndarray of shape (n_alphas,) 
        The grid of alphas used for fitting. 
 
    n_iter_ : int 
        Number of iterations run by the coordinate descent solver to reach 
        the specified tolerance for the optimal alpha. 
 
    dual_gap_ : float 
        The dual gap at the end of the optimization for the optimal alpha. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 
        mixed-norm as regularizer. 
    ElasticNetCV : Elastic net model with best model selection by 
        cross-validation. 
    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in 
        cross-validation. 
 
    Notes 
    ----- 
    The algorithm used to fit the model is coordinate descent. 
 
    In `fit`, once the best parameter `alpha` is found through 
    cross-validation, the model is fit again using the entire training set. 
 
    To avoid unnecessary memory duplication the `X` and `y` arguments of the 
    `fit` method should be directly passed as Fortran-contiguous numpy arrays. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.linear_model import MultiTaskLassoCV 
    &gt;&gt;&gt; from sklearn.datasets import make_regression 
    &gt;&gt;&gt; from sklearn.metrics import r2_score 
    &gt;&gt;&gt; X, y = make_regression(n_targets=2, noise=4, random_state=0) 
    &gt;&gt;&gt; reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y) 
    &gt;&gt;&gt; r2_score(y, reg.predict(X)) 
    0.9994... 
    &gt;&gt;&gt; reg.alpha_ 
    0.5713... 
    &gt;&gt;&gt; reg.predict(X[:1,]) 
    array([[153.7971...,  94.9015...]]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**LinearModelCV._parameter_constraints</span><span class="s2">,</span>
    <span class="s1">}</span>
    <span class="s1">_parameter_constraints.pop(</span><span class="s3">&quot;precompute&quot;</span><span class="s1">)</span>
    <span class="s1">_parameter_constraints.pop(</span><span class="s3">&quot;positive&quot;</span><span class="s1">)</span>

    <span class="s1">path = staticmethod(lasso_path)</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">eps=</span><span class="s5">1e-3</span><span class="s2">,</span>
        <span class="s1">n_alphas=</span><span class="s5">100</span><span class="s2">,</span>
        <span class="s1">alphas=</span><span class="s2">None,</span>
        <span class="s1">fit_intercept=</span><span class="s2">True,</span>
        <span class="s1">max_iter=</span><span class="s5">1000</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s2">,</span>
        <span class="s1">copy_X=</span><span class="s2">True,</span>
        <span class="s1">cv=</span><span class="s2">None,</span>
        <span class="s1">verbose=</span><span class="s2">False,</span>
        <span class="s1">n_jobs=</span><span class="s2">None,</span>
        <span class="s1">random_state=</span><span class="s2">None,</span>
        <span class="s1">selection=</span><span class="s3">&quot;cyclic&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">eps=eps</span><span class="s2">,</span>
            <span class="s1">n_alphas=n_alphas</span><span class="s2">,</span>
            <span class="s1">alphas=alphas</span><span class="s2">,</span>
            <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
            <span class="s1">tol=tol</span><span class="s2">,</span>
            <span class="s1">copy_X=copy_X</span><span class="s2">,</span>
            <span class="s1">cv=cv</span><span class="s2">,</span>
            <span class="s1">verbose=verbose</span><span class="s2">,</span>
            <span class="s1">n_jobs=n_jobs</span><span class="s2">,</span>
            <span class="s1">random_state=random_state</span><span class="s2">,</span>
            <span class="s1">selection=selection</span><span class="s2">,</span>
        <span class="s1">)</span>

    <span class="s2">def </span><span class="s1">_get_estimator(self):</span>
        <span class="s2">return </span><span class="s1">MultiTaskLasso()</span>

    <span class="s2">def </span><span class="s1">_is_multitask(self):</span>
        <span class="s2">return True</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s3">&quot;multioutput_only&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>

    <span class="s0"># This is necessary as LinearModelCV now supports sample_weight while</span>
    <span class="s0"># MultiTaskElasticNet does not (yet).</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s4">&quot;&quot;&quot;Fit MultiTaskLasso model with coordinate descent. 
 
        Fit is on grid of alphas and best alpha estimated by cross-validation. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples, n_features) 
            Data. 
        y : ndarray of shape (n_samples, n_targets) 
            Target. Will be cast to X's dtype if necessary. 
 
        Returns 
        ------- 
        self : object 
            Returns an instance of fitted model. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">super().fit(X</span><span class="s2">, </span><span class="s1">y)</span>
</pre>
</body>
</html>