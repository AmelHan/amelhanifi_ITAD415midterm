<html>
<head>
<title>_encoders.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_encoders.py</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: Andreas Mueller &lt;amueller@ais.uni-bonn.de&gt;</span>
<span class="s0">#          Joris Van den Bossche &lt;jorisvandenbossche@gmail.com&gt;</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">import </span><span class="s1">numbers</span>
<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">numbers </span><span class="s2">import </span><span class="s1">Integral</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">sparse</span>

<span class="s2">from </span><span class="s1">..base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">OneToOneFeatureMixin</span><span class="s2">, </span><span class="s1">TransformerMixin</span><span class="s2">, </span><span class="s1">_fit_context</span>
<span class="s2">from </span><span class="s1">..utils </span><span class="s2">import </span><span class="s1">_safe_indexing</span><span class="s2">, </span><span class="s1">check_array</span><span class="s2">, </span><span class="s1">is_scalar_nan</span>
<span class="s2">from </span><span class="s1">..utils._encode </span><span class="s2">import </span><span class="s1">_check_unknown</span><span class="s2">, </span><span class="s1">_encode</span><span class="s2">, </span><span class="s1">_get_counts</span><span class="s2">, </span><span class="s1">_unique</span>
<span class="s2">from </span><span class="s1">..utils._mask </span><span class="s2">import </span><span class="s1">_get_mask</span>
<span class="s2">from </span><span class="s1">..utils._param_validation </span><span class="s2">import </span><span class="s1">Hidden</span><span class="s2">, </span><span class="s1">Interval</span><span class="s2">, </span><span class="s1">RealNotInt</span><span class="s2">, </span><span class="s1">StrOptions</span>
<span class="s2">from </span><span class="s1">..utils._set_output </span><span class="s2">import </span><span class="s1">_get_output_config</span>
<span class="s2">from </span><span class="s1">..utils.validation </span><span class="s2">import </span><span class="s1">_check_feature_names_in</span><span class="s2">, </span><span class="s1">check_is_fitted</span>

<span class="s1">__all__ = [</span><span class="s3">&quot;OneHotEncoder&quot;</span><span class="s2">, </span><span class="s3">&quot;OrdinalEncoder&quot;</span><span class="s1">]</span>


<span class="s2">class </span><span class="s1">_BaseEncoder(TransformerMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
    <span class="s4">&quot;&quot;&quot; 
    Base class for encoders that includes the code to categorize and 
    transform the input features. 
 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">_check_X(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">force_all_finite=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s4">&quot;&quot;&quot; 
        Perform custom check_array: 
        - convert list of strings to object dtype 
        - check for missing values for object dtype data (check_array does 
          not do that) 
        - return list of features (arrays): this list of features is 
          constructed feature by feature to preserve the data types 
          of pandas DataFrame columns, as otherwise information is lost 
          and cannot be used, e.g. for the `categories_` attribute. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">if not </span><span class="s1">(hasattr(X</span><span class="s2">, </span><span class="s3">&quot;iloc&quot;</span><span class="s1">) </span><span class="s2">and </span><span class="s1">getattr(X</span><span class="s2">, </span><span class="s3">&quot;ndim&quot;</span><span class="s2">, </span><span class="s5">0</span><span class="s1">) == </span><span class="s5">2</span><span class="s1">):</span>
            <span class="s0"># if not a dataframe, do normal check_array validation</span>
            <span class="s1">X_temp = check_array(X</span><span class="s2">, </span><span class="s1">dtype=</span><span class="s2">None, </span><span class="s1">force_all_finite=force_all_finite)</span>
            <span class="s2">if not </span><span class="s1">hasattr(X</span><span class="s2">, </span><span class="s3">&quot;dtype&quot;</span><span class="s1">) </span><span class="s2">and </span><span class="s1">np.issubdtype(X_temp.dtype</span><span class="s2">, </span><span class="s1">np.str_):</span>
                <span class="s1">X = check_array(X</span><span class="s2">, </span><span class="s1">dtype=object</span><span class="s2">, </span><span class="s1">force_all_finite=force_all_finite)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">X = X_temp</span>
            <span class="s1">needs_validation = </span><span class="s2">False</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s0"># pandas dataframe, do validation later column by column, in order</span>
            <span class="s0"># to keep the dtype information to be used in the encoder.</span>
            <span class="s1">needs_validation = force_all_finite</span>

        <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X.shape</span>
        <span class="s1">X_columns = []</span>

        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_features):</span>
            <span class="s1">Xi = _safe_indexing(X</span><span class="s2">, </span><span class="s1">indices=i</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>
            <span class="s1">Xi = check_array(</span>
                <span class="s1">Xi</span><span class="s2">, </span><span class="s1">ensure_2d=</span><span class="s2">False, </span><span class="s1">dtype=</span><span class="s2">None, </span><span class="s1">force_all_finite=needs_validation</span>
            <span class="s1">)</span>
            <span class="s1">X_columns.append(Xi)</span>

        <span class="s2">return </span><span class="s1">X_columns</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features</span>

    <span class="s2">def </span><span class="s1">_fit(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">handle_unknown=</span><span class="s3">&quot;error&quot;</span><span class="s2">,</span>
        <span class="s1">force_all_finite=</span><span class="s2">True,</span>
        <span class="s1">return_counts=</span><span class="s2">False,</span>
        <span class="s1">return_and_ignore_missing_for_infrequent=</span><span class="s2">False,</span>
    <span class="s1">):</span>
        <span class="s1">self._check_infrequent_enabled()</span>
        <span class="s1">self._check_n_features(X</span><span class="s2">, </span><span class="s1">reset=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">self._check_feature_names(X</span><span class="s2">, </span><span class="s1">reset=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">X_list</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = self._check_X(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">force_all_finite=force_all_finite</span>
        <span class="s1">)</span>
        <span class="s1">self.n_features_in_ = n_features</span>

        <span class="s2">if </span><span class="s1">self.categories != </span><span class="s3">&quot;auto&quot;</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">len(self.categories) != n_features:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s3">&quot;Shape mismatch: if categories is an array,&quot;</span>
                    <span class="s3">&quot; it has to be of shape (n_features,).&quot;</span>
                <span class="s1">)</span>

        <span class="s1">self.categories_ = []</span>
        <span class="s1">category_counts = []</span>
        <span class="s1">compute_counts = return_counts </span><span class="s2">or </span><span class="s1">self._infrequent_enabled</span>

        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_features):</span>
            <span class="s1">Xi = X_list[i]</span>

            <span class="s2">if </span><span class="s1">self.categories == </span><span class="s3">&quot;auto&quot;</span><span class="s1">:</span>
                <span class="s1">result = _unique(Xi</span><span class="s2">, </span><span class="s1">return_counts=compute_counts)</span>
                <span class="s2">if </span><span class="s1">compute_counts:</span>
                    <span class="s1">cats</span><span class="s2">, </span><span class="s1">counts = result</span>
                    <span class="s1">category_counts.append(counts)</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">cats = result</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s2">if </span><span class="s1">np.issubdtype(Xi.dtype</span><span class="s2">, </span><span class="s1">np.str_):</span>
                    <span class="s0"># Always convert string categories to objects to avoid</span>
                    <span class="s0"># unexpected string truncation for longer category labels</span>
                    <span class="s0"># passed in the constructor.</span>
                    <span class="s1">Xi_dtype = object</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">Xi_dtype = Xi.dtype</span>

                <span class="s1">cats = np.array(self.categories[i]</span><span class="s2">, </span><span class="s1">dtype=Xi_dtype)</span>
                <span class="s2">if </span><span class="s1">(</span>
                    <span class="s1">cats.dtype == object</span>
                    <span class="s2">and </span><span class="s1">isinstance(cats[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">bytes)</span>
                    <span class="s2">and </span><span class="s1">Xi.dtype.kind != </span><span class="s3">&quot;S&quot;</span>
                <span class="s1">):</span>
                    <span class="s1">msg = (</span>
                        <span class="s3">f&quot;In column </span><span class="s2">{</span><span class="s1">i</span><span class="s2">}</span><span class="s3">, the predefined categories have type 'bytes'&quot;</span>
                        <span class="s3">&quot; which is incompatible with values of type&quot;</span>
                        <span class="s3">f&quot; '</span><span class="s2">{</span><span class="s1">type(Xi[</span><span class="s5">0</span><span class="s1">]).__name__</span><span class="s2">}</span><span class="s3">'.&quot;</span>
                    <span class="s1">)</span>
                    <span class="s2">raise </span><span class="s1">ValueError(msg)</span>

                <span class="s2">if </span><span class="s1">Xi.dtype.kind </span><span class="s2">not in </span><span class="s3">&quot;OUS&quot;</span><span class="s1">:</span>
                    <span class="s1">sorted_cats = np.sort(cats)</span>
                    <span class="s1">error_msg = (</span>
                        <span class="s3">&quot;Unsorted categories are not supported for numerical categories&quot;</span>
                    <span class="s1">)</span>
                    <span class="s0"># if there are nans, nan should be the last element</span>
                    <span class="s1">stop_idx = -</span><span class="s5">1 </span><span class="s2">if </span><span class="s1">np.isnan(sorted_cats[-</span><span class="s5">1</span><span class="s1">]) </span><span class="s2">else None</span>
                    <span class="s2">if </span><span class="s1">np.any(sorted_cats[:stop_idx] != cats[:stop_idx]) </span><span class="s2">or </span><span class="s1">(</span>
                        <span class="s1">np.isnan(sorted_cats[-</span><span class="s5">1</span><span class="s1">]) </span><span class="s2">and not </span><span class="s1">np.isnan(sorted_cats[-</span><span class="s5">1</span><span class="s1">])</span>
                    <span class="s1">):</span>
                        <span class="s2">raise </span><span class="s1">ValueError(error_msg)</span>

                <span class="s2">if </span><span class="s1">handle_unknown == </span><span class="s3">&quot;error&quot;</span><span class="s1">:</span>
                    <span class="s1">diff = _check_unknown(Xi</span><span class="s2">, </span><span class="s1">cats)</span>
                    <span class="s2">if </span><span class="s1">diff:</span>
                        <span class="s1">msg = (</span>
                            <span class="s3">&quot;Found unknown categories {0} in column {1}&quot;</span>
                            <span class="s3">&quot; during fit&quot;</span><span class="s1">.format(diff</span><span class="s2">, </span><span class="s1">i)</span>
                        <span class="s1">)</span>
                        <span class="s2">raise </span><span class="s1">ValueError(msg)</span>
                <span class="s2">if </span><span class="s1">compute_counts:</span>
                    <span class="s1">category_counts.append(_get_counts(Xi</span><span class="s2">, </span><span class="s1">cats))</span>

            <span class="s1">self.categories_.append(cats)</span>

        <span class="s1">output = {</span><span class="s3">&quot;n_samples&quot;</span><span class="s1">: n_samples}</span>
        <span class="s2">if </span><span class="s1">return_counts:</span>
            <span class="s1">output[</span><span class="s3">&quot;category_counts&quot;</span><span class="s1">] = category_counts</span>

        <span class="s1">missing_indices = {}</span>
        <span class="s2">if </span><span class="s1">return_and_ignore_missing_for_infrequent:</span>
            <span class="s2">for </span><span class="s1">feature_idx</span><span class="s2">, </span><span class="s1">categories_for_idx </span><span class="s2">in </span><span class="s1">enumerate(self.categories_):</span>
                <span class="s2">for </span><span class="s1">category_idx</span><span class="s2">, </span><span class="s1">category </span><span class="s2">in </span><span class="s1">enumerate(categories_for_idx):</span>
                    <span class="s2">if </span><span class="s1">is_scalar_nan(category):</span>
                        <span class="s1">missing_indices[feature_idx] = category_idx</span>
                        <span class="s2">break</span>
            <span class="s1">output[</span><span class="s3">&quot;missing_indices&quot;</span><span class="s1">] = missing_indices</span>

        <span class="s2">if </span><span class="s1">self._infrequent_enabled:</span>
            <span class="s1">self._fit_infrequent_category_mapping(</span>
                <span class="s1">n_samples</span><span class="s2">,</span>
                <span class="s1">category_counts</span><span class="s2">,</span>
                <span class="s1">missing_indices</span><span class="s2">,</span>
            <span class="s1">)</span>
        <span class="s2">return </span><span class="s1">output</span>

    <span class="s2">def </span><span class="s1">_transform(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">handle_unknown=</span><span class="s3">&quot;error&quot;</span><span class="s2">,</span>
        <span class="s1">force_all_finite=</span><span class="s2">True,</span>
        <span class="s1">warn_on_unknown=</span><span class="s2">False,</span>
        <span class="s1">ignore_category_indices=</span><span class="s2">None,</span>
    <span class="s1">):</span>
        <span class="s1">X_list</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = self._check_X(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">force_all_finite=force_all_finite</span>
        <span class="s1">)</span>
        <span class="s1">self._check_feature_names(X</span><span class="s2">, </span><span class="s1">reset=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">self._check_n_features(X</span><span class="s2">, </span><span class="s1">reset=</span><span class="s2">False</span><span class="s1">)</span>

        <span class="s1">X_int = np.zeros((n_samples</span><span class="s2">, </span><span class="s1">n_features)</span><span class="s2">, </span><span class="s1">dtype=int)</span>
        <span class="s1">X_mask = np.ones((n_samples</span><span class="s2">, </span><span class="s1">n_features)</span><span class="s2">, </span><span class="s1">dtype=bool)</span>

        <span class="s1">columns_with_unknown = []</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_features):</span>
            <span class="s1">Xi = X_list[i]</span>
            <span class="s1">diff</span><span class="s2">, </span><span class="s1">valid_mask = _check_unknown(Xi</span><span class="s2">, </span><span class="s1">self.categories_[i]</span><span class="s2">, </span><span class="s1">return_mask=</span><span class="s2">True</span><span class="s1">)</span>

            <span class="s2">if not </span><span class="s1">np.all(valid_mask):</span>
                <span class="s2">if </span><span class="s1">handle_unknown == </span><span class="s3">&quot;error&quot;</span><span class="s1">:</span>
                    <span class="s1">msg = (</span>
                        <span class="s3">&quot;Found unknown categories {0} in column {1}&quot;</span>
                        <span class="s3">&quot; during transform&quot;</span><span class="s1">.format(diff</span><span class="s2">, </span><span class="s1">i)</span>
                    <span class="s1">)</span>
                    <span class="s2">raise </span><span class="s1">ValueError(msg)</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s2">if </span><span class="s1">warn_on_unknown:</span>
                        <span class="s1">columns_with_unknown.append(i)</span>
                    <span class="s0"># Set the problematic rows to an acceptable value and</span>
                    <span class="s0"># continue `The rows are marked `X_mask` and will be</span>
                    <span class="s0"># removed later.</span>
                    <span class="s1">X_mask[:</span><span class="s2">, </span><span class="s1">i] = valid_mask</span>
                    <span class="s0"># cast Xi into the largest string type necessary</span>
                    <span class="s0"># to handle different lengths of numpy strings</span>
                    <span class="s2">if </span><span class="s1">(</span>
                        <span class="s1">self.categories_[i].dtype.kind </span><span class="s2">in </span><span class="s1">(</span><span class="s3">&quot;U&quot;</span><span class="s2">, </span><span class="s3">&quot;S&quot;</span><span class="s1">)</span>
                        <span class="s2">and </span><span class="s1">self.categories_[i].itemsize &gt; Xi.itemsize</span>
                    <span class="s1">):</span>
                        <span class="s1">Xi = Xi.astype(self.categories_[i].dtype)</span>
                    <span class="s2">elif </span><span class="s1">self.categories_[i].dtype.kind == </span><span class="s3">&quot;O&quot; </span><span class="s2">and </span><span class="s1">Xi.dtype.kind == </span><span class="s3">&quot;U&quot;</span><span class="s1">:</span>
                        <span class="s0"># categories are objects and Xi are numpy strings.</span>
                        <span class="s0"># Cast Xi to an object dtype to prevent truncation</span>
                        <span class="s0"># when setting invalid values.</span>
                        <span class="s1">Xi = Xi.astype(</span><span class="s3">&quot;O&quot;</span><span class="s1">)</span>
                    <span class="s2">else</span><span class="s1">:</span>
                        <span class="s1">Xi = Xi.copy()</span>

                    <span class="s1">Xi[~valid_mask] = self.categories_[i][</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s0"># We use check_unknown=False, since _check_unknown was</span>
            <span class="s0"># already called above.</span>
            <span class="s1">X_int[:</span><span class="s2">, </span><span class="s1">i] = _encode(Xi</span><span class="s2">, </span><span class="s1">uniques=self.categories_[i]</span><span class="s2">, </span><span class="s1">check_unknown=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">columns_with_unknown:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s3">&quot;Found unknown categories in columns &quot;</span>
                    <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">columns_with_unknown</span><span class="s2">} </span><span class="s3">during transform. These &quot;</span>
                    <span class="s3">&quot;unknown categories will be encoded as all zeros&quot;</span>
                <span class="s1">)</span><span class="s2">,</span>
                <span class="s1">UserWarning</span><span class="s2">,</span>
            <span class="s1">)</span>

        <span class="s1">self._map_infrequent_categories(X_int</span><span class="s2">, </span><span class="s1">X_mask</span><span class="s2">, </span><span class="s1">ignore_category_indices)</span>
        <span class="s2">return </span><span class="s1">X_int</span><span class="s2">, </span><span class="s1">X_mask</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">infrequent_categories_(self):</span>
        <span class="s4">&quot;&quot;&quot;Infrequent categories for each feature.&quot;&quot;&quot;</span>
        <span class="s0"># raises an AttributeError if `_infrequent_indices` is not defined</span>
        <span class="s1">infrequent_indices = self._infrequent_indices</span>
        <span class="s2">return </span><span class="s1">[</span>
            <span class="s2">None if </span><span class="s1">indices </span><span class="s2">is None else </span><span class="s1">category[indices]</span>
            <span class="s2">for </span><span class="s1">category</span><span class="s2">, </span><span class="s1">indices </span><span class="s2">in </span><span class="s1">zip(self.categories_</span><span class="s2">, </span><span class="s1">infrequent_indices)</span>
        <span class="s1">]</span>

    <span class="s2">def </span><span class="s1">_check_infrequent_enabled(self):</span>
        <span class="s4">&quot;&quot;&quot; 
        This functions checks whether _infrequent_enabled is True or False. 
        This has to be called after parameter validation in the fit function. 
        &quot;&quot;&quot;</span>
        <span class="s1">max_categories = getattr(self</span><span class="s2">, </span><span class="s3">&quot;max_categories&quot;</span><span class="s2">, None</span><span class="s1">)</span>
        <span class="s1">min_frequency = getattr(self</span><span class="s2">, </span><span class="s3">&quot;min_frequency&quot;</span><span class="s2">, None</span><span class="s1">)</span>
        <span class="s1">self._infrequent_enabled = (</span>
            <span class="s1">max_categories </span><span class="s2">is not None and </span><span class="s1">max_categories &gt;= </span><span class="s5">1</span>
        <span class="s1">) </span><span class="s2">or </span><span class="s1">min_frequency </span><span class="s2">is not None</span>

    <span class="s2">def </span><span class="s1">_identify_infrequent(self</span><span class="s2">, </span><span class="s1">category_count</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">col_idx):</span>
        <span class="s4">&quot;&quot;&quot;Compute the infrequent indices. 
 
        Parameters 
        ---------- 
        category_count : ndarray of shape (n_cardinality,) 
            Category counts. 
 
        n_samples : int 
            Number of samples. 
 
        col_idx : int 
            Index of the current category. Only used for the error message. 
 
        Returns 
        ------- 
        output : ndarray of shape (n_infrequent_categories,) or None 
            If there are infrequent categories, indices of infrequent 
            categories. Otherwise None. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">isinstance(self.min_frequency</span><span class="s2">, </span><span class="s1">numbers.Integral):</span>
            <span class="s1">infrequent_mask = category_count &lt; self.min_frequency</span>
        <span class="s2">elif </span><span class="s1">isinstance(self.min_frequency</span><span class="s2">, </span><span class="s1">numbers.Real):</span>
            <span class="s1">min_frequency_abs = n_samples * self.min_frequency</span>
            <span class="s1">infrequent_mask = category_count &lt; min_frequency_abs</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">infrequent_mask = np.zeros(category_count.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=bool)</span>

        <span class="s1">n_current_features = category_count.size - infrequent_mask.sum() + </span><span class="s5">1</span>
        <span class="s2">if </span><span class="s1">self.max_categories </span><span class="s2">is not None and </span><span class="s1">self.max_categories &lt; n_current_features:</span>
            <span class="s0"># max_categories includes the one infrequent category</span>
            <span class="s1">frequent_category_count = self.max_categories - </span><span class="s5">1</span>
            <span class="s2">if </span><span class="s1">frequent_category_count == </span><span class="s5">0</span><span class="s1">:</span>
                <span class="s0"># All categories are infrequent</span>
                <span class="s1">infrequent_mask[:] = </span><span class="s2">True</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s0"># stable sort to preserve original count order</span>
                <span class="s1">smallest_levels = np.argsort(category_count</span><span class="s2">, </span><span class="s1">kind=</span><span class="s3">&quot;mergesort&quot;</span><span class="s1">)[</span>
                    <span class="s1">:-frequent_category_count</span>
                <span class="s1">]</span>
                <span class="s1">infrequent_mask[smallest_levels] = </span><span class="s2">True</span>

        <span class="s1">output = np.flatnonzero(infrequent_mask)</span>
        <span class="s2">return </span><span class="s1">output </span><span class="s2">if </span><span class="s1">output.size &gt; </span><span class="s5">0 </span><span class="s2">else None</span>

    <span class="s2">def </span><span class="s1">_fit_infrequent_category_mapping(</span>
        <span class="s1">self</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">category_counts</span><span class="s2">, </span><span class="s1">missing_indices</span>
    <span class="s1">):</span>
        <span class="s4">&quot;&quot;&quot;Fit infrequent categories. 
 
        Defines the private attribute: `_default_to_infrequent_mappings`. For 
        feature `i`, `_default_to_infrequent_mappings[i]` defines the mapping 
        from the integer encoding returned by `super().transform()` into 
        infrequent categories. If `_default_to_infrequent_mappings[i]` is None, 
        there were no infrequent categories in the training set. 
 
        For example if categories 0, 2 and 4 were frequent, while categories 
        1, 3, 5 were infrequent for feature 7, then these categories are mapped 
        to a single output: 
        `_default_to_infrequent_mappings[7] = array([0, 3, 1, 3, 2, 3])` 
 
        Defines private attribute: `_infrequent_indices`. `_infrequent_indices[i]` 
        is an array of indices such that 
        `categories_[i][_infrequent_indices[i]]` are all the infrequent category 
        labels. If the feature `i` has no infrequent categories 
        `_infrequent_indices[i]` is None. 
 
        .. versionadded:: 1.1 
 
        Parameters 
        ---------- 
        n_samples : int 
            Number of samples in training set. 
        category_counts: list of ndarray 
            `category_counts[i]` is the category counts corresponding to 
            `self.categories_[i]`. 
        missing_indices : dict 
            Dict mapping from feature_idx to category index with a missing value. 
        &quot;&quot;&quot;</span>
        <span class="s0"># Remove missing value from counts, so it is not considered as infrequent</span>
        <span class="s2">if </span><span class="s1">missing_indices:</span>
            <span class="s1">category_counts_ = []</span>
            <span class="s2">for </span><span class="s1">feature_idx</span><span class="s2">, </span><span class="s1">count </span><span class="s2">in </span><span class="s1">enumerate(category_counts):</span>
                <span class="s2">if </span><span class="s1">feature_idx </span><span class="s2">in </span><span class="s1">missing_indices:</span>
                    <span class="s1">category_counts_.append(</span>
                        <span class="s1">np.delete(count</span><span class="s2">, </span><span class="s1">missing_indices[feature_idx])</span>
                    <span class="s1">)</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">category_counts_.append(count)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">category_counts_ = category_counts</span>

        <span class="s1">self._infrequent_indices = [</span>
            <span class="s1">self._identify_infrequent(category_count</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">col_idx)</span>
            <span class="s2">for </span><span class="s1">col_idx</span><span class="s2">, </span><span class="s1">category_count </span><span class="s2">in </span><span class="s1">enumerate(category_counts_)</span>
        <span class="s1">]</span>

        <span class="s0"># compute mapping from default mapping to infrequent mapping</span>
        <span class="s1">self._default_to_infrequent_mappings = []</span>

        <span class="s2">for </span><span class="s1">feature_idx</span><span class="s2">, </span><span class="s1">infreq_idx </span><span class="s2">in </span><span class="s1">enumerate(self._infrequent_indices):</span>
            <span class="s1">cats = self.categories_[feature_idx]</span>
            <span class="s0"># no infrequent categories</span>
            <span class="s2">if </span><span class="s1">infreq_idx </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s1">self._default_to_infrequent_mappings.append(</span><span class="s2">None</span><span class="s1">)</span>
                <span class="s2">continue</span>

            <span class="s1">n_cats = len(cats)</span>
            <span class="s2">if </span><span class="s1">feature_idx </span><span class="s2">in </span><span class="s1">missing_indices:</span>
                <span class="s0"># Missing index was removed from this category when computing</span>
                <span class="s0"># infrequent indices, thus we need to decrease the number of</span>
                <span class="s0"># total categories when considering the infrequent mapping.</span>
                <span class="s1">n_cats -= </span><span class="s5">1</span>

            <span class="s0"># infrequent indices exist</span>
            <span class="s1">mapping = np.empty(n_cats</span><span class="s2">, </span><span class="s1">dtype=np.int64)</span>
            <span class="s1">n_infrequent_cats = infreq_idx.size</span>

            <span class="s0"># infrequent categories are mapped to the last element.</span>
            <span class="s1">n_frequent_cats = n_cats - n_infrequent_cats</span>
            <span class="s1">mapping[infreq_idx] = n_frequent_cats</span>

            <span class="s1">frequent_indices = np.setdiff1d(np.arange(n_cats)</span><span class="s2">, </span><span class="s1">infreq_idx)</span>
            <span class="s1">mapping[frequent_indices] = np.arange(n_frequent_cats)</span>

            <span class="s1">self._default_to_infrequent_mappings.append(mapping)</span>

    <span class="s2">def </span><span class="s1">_map_infrequent_categories(self</span><span class="s2">, </span><span class="s1">X_int</span><span class="s2">, </span><span class="s1">X_mask</span><span class="s2">, </span><span class="s1">ignore_category_indices):</span>
        <span class="s4">&quot;&quot;&quot;Map infrequent categories to integer representing the infrequent category. 
 
        This modifies X_int in-place. Values that were invalid based on `X_mask` 
        are mapped to the infrequent category if there was an infrequent 
        category for that feature. 
 
        Parameters 
        ---------- 
        X_int: ndarray of shape (n_samples, n_features) 
            Integer encoded categories. 
 
        X_mask: ndarray of shape (n_samples, n_features) 
            Bool mask for valid values in `X_int`. 
 
        ignore_category_indices : dict 
            Dictionary mapping from feature_idx to category index to ignore. 
            Ignored indexes will not be grouped and the original ordinal encoding 
            will remain. 
        &quot;&quot;&quot;</span>
        <span class="s2">if not </span><span class="s1">self._infrequent_enabled:</span>
            <span class="s2">return</span>

        <span class="s1">ignore_category_indices = ignore_category_indices </span><span class="s2">or </span><span class="s1">{}</span>

        <span class="s2">for </span><span class="s1">col_idx </span><span class="s2">in </span><span class="s1">range(X_int.shape[</span><span class="s5">1</span><span class="s1">]):</span>
            <span class="s1">infrequent_idx = self._infrequent_indices[col_idx]</span>
            <span class="s2">if </span><span class="s1">infrequent_idx </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s2">continue</span>

            <span class="s1">X_int[~X_mask[:</span><span class="s2">, </span><span class="s1">col_idx]</span><span class="s2">, </span><span class="s1">col_idx] = infrequent_idx[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s2">if </span><span class="s1">self.handle_unknown == </span><span class="s3">&quot;infrequent_if_exist&quot;</span><span class="s1">:</span>
                <span class="s0"># All the unknown values are now mapped to the</span>
                <span class="s0"># infrequent_idx[0], which makes the unknown values valid</span>
                <span class="s0"># This is needed in `transform` when the encoding is formed</span>
                <span class="s0"># using `X_mask`.</span>
                <span class="s1">X_mask[:</span><span class="s2">, </span><span class="s1">col_idx] = </span><span class="s2">True</span>

        <span class="s0"># Remaps encoding in `X_int` where the infrequent categories are</span>
        <span class="s0"># grouped together.</span>
        <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">mapping </span><span class="s2">in </span><span class="s1">enumerate(self._default_to_infrequent_mappings):</span>
            <span class="s2">if </span><span class="s1">mapping </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s2">continue</span>

            <span class="s2">if </span><span class="s1">i </span><span class="s2">in </span><span class="s1">ignore_category_indices:</span>
                <span class="s0"># Update rows that are **not** ignored</span>
                <span class="s1">rows_to_update = X_int[:</span><span class="s2">, </span><span class="s1">i] != ignore_category_indices[i]</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">rows_to_update = slice(</span><span class="s2">None</span><span class="s1">)</span>

            <span class="s1">X_int[rows_to_update</span><span class="s2">, </span><span class="s1">i] = np.take(mapping</span><span class="s2">, </span><span class="s1">X_int[rows_to_update</span><span class="s2">, </span><span class="s1">i])</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s3">&quot;X_types&quot;</span><span class="s1">: [</span><span class="s3">&quot;2darray&quot;</span><span class="s2">, </span><span class="s3">&quot;categorical&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s3">&quot;allow_nan&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>


<span class="s2">class </span><span class="s1">OneHotEncoder(_BaseEncoder):</span>
    <span class="s4">&quot;&quot;&quot; 
    Encode categorical features as a one-hot numeric array. 
 
    The input to this transformer should be an array-like of integers or 
    strings, denoting the values taken on by categorical (discrete) features. 
    The features are encoded using a one-hot (aka 'one-of-K' or 'dummy') 
    encoding scheme. This creates a binary column for each category and 
    returns a sparse matrix or dense array (depending on the ``sparse_output`` 
    parameter) 
 
    By default, the encoder derives the categories based on the unique values 
    in each feature. Alternatively, you can also specify the `categories` 
    manually. 
 
    This encoding is needed for feeding categorical data to many scikit-learn 
    estimators, notably linear models and SVMs with the standard kernels. 
 
    Note: a one-hot encoding of y labels should use a LabelBinarizer 
    instead. 
 
    Read more in the :ref:`User Guide &lt;preprocessing_categorical_features&gt;`. 
    For a comparison of different encoders, refer to: 
    :ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder.py`. 
 
    Parameters 
    ---------- 
    categories : 'auto' or a list of array-like, default='auto' 
        Categories (unique values) per feature: 
 
        - 'auto' : Determine categories automatically from the training data. 
        - list : ``categories[i]`` holds the categories expected in the ith 
          column. The passed categories should not mix strings and numeric 
          values within a single feature, and should be sorted in case of 
          numeric values. 
 
        The used categories can be found in the ``categories_`` attribute. 
 
        .. versionadded:: 0.20 
 
    drop : {'first', 'if_binary'} or an array-like of shape (n_features,), \ 
            default=None 
        Specifies a methodology to use to drop one of the categories per 
        feature. This is useful in situations where perfectly collinear 
        features cause problems, such as when feeding the resulting data 
        into an unregularized linear regression model. 
 
        However, dropping one category breaks the symmetry of the original 
        representation and can therefore induce a bias in downstream models, 
        for instance for penalized linear classification or regression models. 
 
        - None : retain all features (the default). 
        - 'first' : drop the first category in each feature. If only one 
          category is present, the feature will be dropped entirely. 
        - 'if_binary' : drop the first category in each feature with two 
          categories. Features with 1 or more than 2 categories are 
          left intact. 
        - array : ``drop[i]`` is the category in feature ``X[:, i]`` that 
          should be dropped. 
 
        When `max_categories` or `min_frequency` is configured to group 
        infrequent categories, the dropping behavior is handled after the 
        grouping. 
 
        .. versionadded:: 0.21 
           The parameter `drop` was added in 0.21. 
 
        .. versionchanged:: 0.23 
           The option `drop='if_binary'` was added in 0.23. 
 
        .. versionchanged:: 1.1 
            Support for dropping infrequent categories. 
 
    sparse : bool, default=True 
        Will return sparse matrix if set True else will return an array. 
 
        .. deprecated:: 1.2 
           `sparse` is deprecated in 1.2 and will be removed in 1.4. Use 
           `sparse_output` instead. 
 
    sparse_output : bool, default=True 
        Will return sparse matrix if set True else will return an array. 
 
        .. versionadded:: 1.2 
           `sparse` was renamed to `sparse_output` 
 
    dtype : number type, default=np.float64 
        Desired dtype of output. 
 
    handle_unknown : {'error', 'ignore', 'infrequent_if_exist'}, \ 
                     default='error' 
        Specifies the way unknown categories are handled during :meth:`transform`. 
 
        - 'error' : Raise an error if an unknown category is present during transform. 
        - 'ignore' : When an unknown category is encountered during 
          transform, the resulting one-hot encoded columns for this feature 
          will be all zeros. In the inverse transform, an unknown category 
          will be denoted as None. 
        - 'infrequent_if_exist' : When an unknown category is encountered 
          during transform, the resulting one-hot encoded columns for this 
          feature will map to the infrequent category if it exists. The 
          infrequent category will be mapped to the last position in the 
          encoding. During inverse transform, an unknown category will be 
          mapped to the category denoted `'infrequent'` if it exists. If the 
          `'infrequent'` category does not exist, then :meth:`transform` and 
          :meth:`inverse_transform` will handle an unknown category as with 
          `handle_unknown='ignore'`. Infrequent categories exist based on 
          `min_frequency` and `max_categories`. Read more in the 
          :ref:`User Guide &lt;encoder_infrequent_categories&gt;`. 
 
        .. versionchanged:: 1.1 
            `'infrequent_if_exist'` was added to automatically handle unknown 
            categories and infrequent categories. 
 
    min_frequency : int or float, default=None 
        Specifies the minimum frequency below which a category will be 
        considered infrequent. 
 
        - If `int`, categories with a smaller cardinality will be considered 
          infrequent. 
 
        - If `float`, categories with a smaller cardinality than 
          `min_frequency * n_samples`  will be considered infrequent. 
 
        .. versionadded:: 1.1 
            Read more in the :ref:`User Guide &lt;encoder_infrequent_categories&gt;`. 
 
    max_categories : int, default=None 
        Specifies an upper limit to the number of output features for each input 
        feature when considering infrequent categories. If there are infrequent 
        categories, `max_categories` includes the category representing the 
        infrequent categories along with the frequent categories. If `None`, 
        there is no limit to the number of output features. 
 
        .. versionadded:: 1.1 
            Read more in the :ref:`User Guide &lt;encoder_infrequent_categories&gt;`. 
 
    feature_name_combiner : &quot;concat&quot; or callable, default=&quot;concat&quot; 
        Callable with signature `def callable(input_feature, category)` that returns a 
        string. This is used to create feature names to be returned by 
        :meth:`get_feature_names_out`. 
 
        `&quot;concat&quot;` concatenates encoded feature name and category with 
        `feature + &quot;_&quot; + str(category)`.E.g. feature X with values 1, 6, 7 create 
        feature names `X_1, X_6, X_7`. 
 
        .. versionadded:: 1.3 
 
    Attributes 
    ---------- 
    categories_ : list of arrays 
        The categories of each feature determined during fitting 
        (in order of the features in X and corresponding with the output 
        of ``transform``). This includes the category specified in ``drop`` 
        (if any). 
 
    drop_idx_ : array of shape (n_features,) 
        - ``drop_idx_[i]`` is the index in ``categories_[i]`` of the category 
          to be dropped for each feature. 
        - ``drop_idx_[i] = None`` if no category is to be dropped from the 
          feature with index ``i``, e.g. when `drop='if_binary'` and the 
          feature isn't binary. 
        - ``drop_idx_ = None`` if all the transformed features will be 
          retained. 
 
        If infrequent categories are enabled by setting `min_frequency` or 
        `max_categories` to a non-default value and `drop_idx[i]` corresponds 
        to a infrequent category, then the entire infrequent category is 
        dropped. 
 
        .. versionchanged:: 0.23 
           Added the possibility to contain `None` values. 
 
    infrequent_categories_ : list of ndarray 
        Defined only if infrequent categories are enabled by setting 
        `min_frequency` or `max_categories` to a non-default value. 
        `infrequent_categories_[i]` are the infrequent categories for feature 
        `i`. If the feature `i` has no infrequent categories 
        `infrequent_categories_[i]` is None. 
 
        .. versionadded:: 1.1 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 1.0 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    feature_name_combiner : callable or None 
        Callable with signature `def callable(input_feature, category)` that returns a 
        string. This is used to create feature names to be returned by 
        :meth:`get_feature_names_out`. 
 
        .. versionadded:: 1.3 
 
    See Also 
    -------- 
    OrdinalEncoder : Performs an ordinal (integer) 
      encoding of the categorical features. 
    TargetEncoder : Encodes categorical features using the target. 
    sklearn.feature_extraction.DictVectorizer : Performs a one-hot encoding of 
      dictionary items (also handles string-valued features). 
    sklearn.feature_extraction.FeatureHasher : Performs an approximate one-hot 
      encoding of dictionary items or strings. 
    LabelBinarizer : Binarizes labels in a one-vs-all 
      fashion. 
    MultiLabelBinarizer : Transforms between iterable of 
      iterables and a multilabel format, e.g. a (samples x classes) binary 
      matrix indicating the presence of a class label. 
 
    Examples 
    -------- 
    Given a dataset with two features, we let the encoder find the unique 
    values per feature and transform the data to a binary one-hot encoding. 
 
    &gt;&gt;&gt; from sklearn.preprocessing import OneHotEncoder 
 
    One can discard categories not seen during `fit`: 
 
    &gt;&gt;&gt; enc = OneHotEncoder(handle_unknown='ignore') 
    &gt;&gt;&gt; X = [['Male', 1], ['Female', 3], ['Female', 2]] 
    &gt;&gt;&gt; enc.fit(X) 
    OneHotEncoder(handle_unknown='ignore') 
    &gt;&gt;&gt; enc.categories_ 
    [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)] 
    &gt;&gt;&gt; enc.transform([['Female', 1], ['Male', 4]]).toarray() 
    array([[1., 0., 1., 0., 0.], 
           [0., 1., 0., 0., 0.]]) 
    &gt;&gt;&gt; enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]]) 
    array([['Male', 1], 
           [None, 2]], dtype=object) 
    &gt;&gt;&gt; enc.get_feature_names_out(['gender', 'group']) 
    array(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'], ...) 
 
    One can always drop the first column for each feature: 
 
    &gt;&gt;&gt; drop_enc = OneHotEncoder(drop='first').fit(X) 
    &gt;&gt;&gt; drop_enc.categories_ 
    [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)] 
    &gt;&gt;&gt; drop_enc.transform([['Female', 1], ['Male', 2]]).toarray() 
    array([[0., 0., 0.], 
           [1., 1., 0.]]) 
 
    Or drop a column for feature only having 2 categories: 
 
    &gt;&gt;&gt; drop_binary_enc = OneHotEncoder(drop='if_binary').fit(X) 
    &gt;&gt;&gt; drop_binary_enc.transform([['Female', 1], ['Male', 2]]).toarray() 
    array([[0., 1., 0., 0.], 
           [1., 0., 1., 0.]]) 
 
    One can change the way feature names are created. 
 
    &gt;&gt;&gt; def custom_combiner(feature, category): 
    ...     return str(feature) + &quot;_&quot; + type(category).__name__ + &quot;_&quot; + str(category) 
    &gt;&gt;&gt; custom_fnames_enc = OneHotEncoder(feature_name_combiner=custom_combiner).fit(X) 
    &gt;&gt;&gt; custom_fnames_enc.get_feature_names_out() 
    array(['x0_str_Female', 'x0_str_Male', 'x1_int_1', 'x1_int_2', 'x1_int_3'], 
          dtype=object) 
 
    Infrequent categories are enabled by setting `max_categories` or `min_frequency`. 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; X = np.array([[&quot;a&quot;] * 5 + [&quot;b&quot;] * 20 + [&quot;c&quot;] * 10 + [&quot;d&quot;] * 3], dtype=object).T 
    &gt;&gt;&gt; ohe = OneHotEncoder(max_categories=3, sparse_output=False).fit(X) 
    &gt;&gt;&gt; ohe.infrequent_categories_ 
    [array(['a', 'd'], dtype=object)] 
    &gt;&gt;&gt; ohe.transform([[&quot;a&quot;], [&quot;b&quot;]]) 
    array([[0., 0., 1.], 
           [1., 0., 0.]]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s3">&quot;categories&quot;</span><span class="s1">: [StrOptions({</span><span class="s3">&quot;auto&quot;</span><span class="s1">})</span><span class="s2">, </span><span class="s1">list]</span><span class="s2">,</span>
        <span class="s3">&quot;drop&quot;</span><span class="s1">: [StrOptions({</span><span class="s3">&quot;first&quot;</span><span class="s2">, </span><span class="s3">&quot;if_binary&quot;</span><span class="s1">})</span><span class="s2">, </span><span class="s3">&quot;array-like&quot;</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;dtype&quot;</span><span class="s1">: </span><span class="s3">&quot;no_validation&quot;</span><span class="s2">,  </span><span class="s0"># validation delegated to numpy</span>
        <span class="s3">&quot;handle_unknown&quot;</span><span class="s1">: [StrOptions({</span><span class="s3">&quot;error&quot;</span><span class="s2">, </span><span class="s3">&quot;ignore&quot;</span><span class="s2">, </span><span class="s3">&quot;infrequent_if_exist&quot;</span><span class="s1">})]</span><span class="s2">,</span>
        <span class="s3">&quot;max_categories&quot;</span><span class="s1">: [Interval(Integral</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s3">&quot;left&quot;</span><span class="s1">)</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;min_frequency&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Integral</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s3">&quot;left&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">Interval(RealNotInt</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s1">closed=</span><span class="s3">&quot;neither&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s2">None,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;sparse&quot;</span><span class="s1">: [Hidden(StrOptions({</span><span class="s3">&quot;deprecated&quot;</span><span class="s1">}))</span><span class="s2">, </span><span class="s3">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,  </span><span class="s0"># deprecated</span>
        <span class="s3">&quot;sparse_output&quot;</span><span class="s1">: [</span><span class="s3">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;feature_name_combiner&quot;</span><span class="s1">: [StrOptions({</span><span class="s3">&quot;concat&quot;</span><span class="s1">})</span><span class="s2">, </span><span class="s1">callable]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">categories=</span><span class="s3">&quot;auto&quot;</span><span class="s2">,</span>
        <span class="s1">drop=</span><span class="s2">None,</span>
        <span class="s1">sparse=</span><span class="s3">&quot;deprecated&quot;</span><span class="s2">,</span>
        <span class="s1">sparse_output=</span><span class="s2">True,</span>
        <span class="s1">dtype=np.float64</span><span class="s2">,</span>
        <span class="s1">handle_unknown=</span><span class="s3">&quot;error&quot;</span><span class="s2">,</span>
        <span class="s1">min_frequency=</span><span class="s2">None,</span>
        <span class="s1">max_categories=</span><span class="s2">None,</span>
        <span class="s1">feature_name_combiner=</span><span class="s3">&quot;concat&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">self.categories = categories</span>
        <span class="s0"># TODO(1.4): Remove self.sparse</span>
        <span class="s1">self.sparse = sparse</span>
        <span class="s1">self.sparse_output = sparse_output</span>
        <span class="s1">self.dtype = dtype</span>
        <span class="s1">self.handle_unknown = handle_unknown</span>
        <span class="s1">self.drop = drop</span>
        <span class="s1">self.min_frequency = min_frequency</span>
        <span class="s1">self.max_categories = max_categories</span>
        <span class="s1">self.feature_name_combiner = feature_name_combiner</span>

    <span class="s2">def </span><span class="s1">_map_drop_idx_to_infrequent(self</span><span class="s2">, </span><span class="s1">feature_idx</span><span class="s2">, </span><span class="s1">drop_idx):</span>
        <span class="s4">&quot;&quot;&quot;Convert `drop_idx` into the index for infrequent categories. 
 
        If there are no infrequent categories, then `drop_idx` is 
        returned. This method is called in `_set_drop_idx` when the `drop` 
        parameter is an array-like. 
        &quot;&quot;&quot;</span>
        <span class="s2">if not </span><span class="s1">self._infrequent_enabled:</span>
            <span class="s2">return </span><span class="s1">drop_idx</span>

        <span class="s1">default_to_infrequent = self._default_to_infrequent_mappings[feature_idx]</span>
        <span class="s2">if </span><span class="s1">default_to_infrequent </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">drop_idx</span>

        <span class="s0"># Raise error when explicitly dropping a category that is infrequent</span>
        <span class="s1">infrequent_indices = self._infrequent_indices[feature_idx]</span>
        <span class="s2">if </span><span class="s1">infrequent_indices </span><span class="s2">is not None and </span><span class="s1">drop_idx </span><span class="s2">in </span><span class="s1">infrequent_indices:</span>
            <span class="s1">categories = self.categories_[feature_idx]</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s3">f&quot;Unable to drop category </span><span class="s2">{</span><span class="s1">categories[drop_idx].item()</span><span class="s2">!r} </span><span class="s3">from&quot;</span>
                <span class="s3">f&quot; feature </span><span class="s2">{</span><span class="s1">feature_idx</span><span class="s2">} </span><span class="s3">because it is infrequent&quot;</span>
            <span class="s1">)</span>
        <span class="s2">return </span><span class="s1">default_to_infrequent[drop_idx]</span>

    <span class="s2">def </span><span class="s1">_set_drop_idx(self):</span>
        <span class="s4">&quot;&quot;&quot;Compute the drop indices associated with `self.categories_`. 
 
        If `self.drop` is: 
        - `None`, No categories have been dropped. 
        - `'first'`, All zeros to drop the first category. 
        - `'if_binary'`, All zeros if the category is binary and `None` 
          otherwise. 
        - array-like, The indices of the categories that match the 
          categories in `self.drop`. If the dropped category is an infrequent 
          category, then the index for the infrequent category is used. This 
          means that the entire infrequent category is dropped. 
 
        This methods defines a public `drop_idx_` and a private 
        `_drop_idx_after_grouping`. 
 
        - `drop_idx_`: Public facing API that references the drop category in 
          `self.categories_`. 
        - `_drop_idx_after_grouping`: Used internally to drop categories *after* the 
          infrequent categories are grouped together. 
 
        If there are no infrequent categories or drop is `None`, then 
        `drop_idx_=_drop_idx_after_grouping`. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">self.drop </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">drop_idx_after_grouping = </span><span class="s2">None</span>
        <span class="s2">elif </span><span class="s1">isinstance(self.drop</span><span class="s2">, </span><span class="s1">str):</span>
            <span class="s2">if </span><span class="s1">self.drop == </span><span class="s3">&quot;first&quot;</span><span class="s1">:</span>
                <span class="s1">drop_idx_after_grouping = np.zeros(len(self.categories_)</span><span class="s2">, </span><span class="s1">dtype=object)</span>
            <span class="s2">elif </span><span class="s1">self.drop == </span><span class="s3">&quot;if_binary&quot;</span><span class="s1">:</span>
                <span class="s1">n_features_out_no_drop = [len(cat) </span><span class="s2">for </span><span class="s1">cat </span><span class="s2">in </span><span class="s1">self.categories_]</span>
                <span class="s2">if </span><span class="s1">self._infrequent_enabled:</span>
                    <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">infreq_idx </span><span class="s2">in </span><span class="s1">enumerate(self._infrequent_indices):</span>
                        <span class="s2">if </span><span class="s1">infreq_idx </span><span class="s2">is None</span><span class="s1">:</span>
                            <span class="s2">continue</span>
                        <span class="s1">n_features_out_no_drop[i] -= infreq_idx.size - </span><span class="s5">1</span>

                <span class="s1">drop_idx_after_grouping = np.array(</span>
                    <span class="s1">[</span>
                        <span class="s5">0 </span><span class="s2">if </span><span class="s1">n_features_out == </span><span class="s5">2 </span><span class="s2">else None</span>
                        <span class="s2">for </span><span class="s1">n_features_out </span><span class="s2">in </span><span class="s1">n_features_out_no_drop</span>
                    <span class="s1">]</span><span class="s2">,</span>
                    <span class="s1">dtype=object</span><span class="s2">,</span>
                <span class="s1">)</span>

        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">drop_array = np.asarray(self.drop</span><span class="s2">, </span><span class="s1">dtype=object)</span>
            <span class="s1">droplen = len(drop_array)</span>

            <span class="s2">if </span><span class="s1">droplen != len(self.categories_):</span>
                <span class="s1">msg = (</span>
                    <span class="s3">&quot;`drop` should have length equal to the number &quot;</span>
                    <span class="s3">&quot;of features ({}), got {}&quot;</span>
                <span class="s1">)</span>
                <span class="s2">raise </span><span class="s1">ValueError(msg.format(len(self.categories_)</span><span class="s2">, </span><span class="s1">droplen))</span>
            <span class="s1">missing_drops = []</span>
            <span class="s1">drop_indices = []</span>
            <span class="s2">for </span><span class="s1">feature_idx</span><span class="s2">, </span><span class="s1">(drop_val</span><span class="s2">, </span><span class="s1">cat_list) </span><span class="s2">in </span><span class="s1">enumerate(</span>
                <span class="s1">zip(drop_array</span><span class="s2">, </span><span class="s1">self.categories_)</span>
            <span class="s1">):</span>
                <span class="s2">if not </span><span class="s1">is_scalar_nan(drop_val):</span>
                    <span class="s1">drop_idx = np.where(cat_list == drop_val)[</span><span class="s5">0</span><span class="s1">]</span>
                    <span class="s2">if </span><span class="s1">drop_idx.size:  </span><span class="s0"># found drop idx</span>
                        <span class="s1">drop_indices.append(</span>
                            <span class="s1">self._map_drop_idx_to_infrequent(feature_idx</span><span class="s2">, </span><span class="s1">drop_idx[</span><span class="s5">0</span><span class="s1">])</span>
                        <span class="s1">)</span>
                    <span class="s2">else</span><span class="s1">:</span>
                        <span class="s1">missing_drops.append((feature_idx</span><span class="s2">, </span><span class="s1">drop_val))</span>
                    <span class="s2">continue</span>

                <span class="s0"># drop_val is nan, find nan in categories manually</span>
                <span class="s2">for </span><span class="s1">cat_idx</span><span class="s2">, </span><span class="s1">cat </span><span class="s2">in </span><span class="s1">enumerate(cat_list):</span>
                    <span class="s2">if </span><span class="s1">is_scalar_nan(cat):</span>
                        <span class="s1">drop_indices.append(</span>
                            <span class="s1">self._map_drop_idx_to_infrequent(feature_idx</span><span class="s2">, </span><span class="s1">cat_idx)</span>
                        <span class="s1">)</span>
                        <span class="s2">break</span>
                <span class="s2">else</span><span class="s1">:  </span><span class="s0"># loop did not break thus drop is missing</span>
                    <span class="s1">missing_drops.append((feature_idx</span><span class="s2">, </span><span class="s1">drop_val))</span>

            <span class="s2">if </span><span class="s1">any(missing_drops):</span>
                <span class="s1">msg = (</span>
                    <span class="s3">&quot;The following categories were supposed to be &quot;</span>
                    <span class="s3">&quot;dropped, but were not found in the training &quot;</span>
                    <span class="s3">&quot;data.</span><span class="s2">\n</span><span class="s3">{}&quot;</span><span class="s1">.format(</span>
                        <span class="s3">&quot;</span><span class="s2">\n</span><span class="s3">&quot;</span><span class="s1">.join(</span>
                            <span class="s1">[</span>
                                <span class="s3">&quot;Category: {}, Feature: {}&quot;</span><span class="s1">.format(c</span><span class="s2">, </span><span class="s1">v)</span>
                                <span class="s2">for </span><span class="s1">c</span><span class="s2">, </span><span class="s1">v </span><span class="s2">in </span><span class="s1">missing_drops</span>
                            <span class="s1">]</span>
                        <span class="s1">)</span>
                    <span class="s1">)</span>
                <span class="s1">)</span>
                <span class="s2">raise </span><span class="s1">ValueError(msg)</span>
            <span class="s1">drop_idx_after_grouping = np.array(drop_indices</span><span class="s2">, </span><span class="s1">dtype=object)</span>

        <span class="s0"># `_drop_idx_after_grouping` are the categories to drop *after* the infrequent</span>
        <span class="s0"># categories are grouped together. If needed, we remap `drop_idx` back</span>
        <span class="s0"># to the categories seen in `self.categories_`.</span>
        <span class="s1">self._drop_idx_after_grouping = drop_idx_after_grouping</span>

        <span class="s2">if not </span><span class="s1">self._infrequent_enabled </span><span class="s2">or </span><span class="s1">drop_idx_after_grouping </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">self.drop_idx_ = self._drop_idx_after_grouping</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">drop_idx_ = []</span>
            <span class="s2">for </span><span class="s1">feature_idx</span><span class="s2">, </span><span class="s1">drop_idx </span><span class="s2">in </span><span class="s1">enumerate(drop_idx_after_grouping):</span>
                <span class="s1">default_to_infrequent = self._default_to_infrequent_mappings[</span>
                    <span class="s1">feature_idx</span>
                <span class="s1">]</span>
                <span class="s2">if </span><span class="s1">drop_idx </span><span class="s2">is None or </span><span class="s1">default_to_infrequent </span><span class="s2">is None</span><span class="s1">:</span>
                    <span class="s1">orig_drop_idx = drop_idx</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">orig_drop_idx = np.flatnonzero(default_to_infrequent == drop_idx)[</span><span class="s5">0</span><span class="s1">]</span>

                <span class="s1">drop_idx_.append(orig_drop_idx)</span>

            <span class="s1">self.drop_idx_ = np.asarray(drop_idx_</span><span class="s2">, </span><span class="s1">dtype=object)</span>

    <span class="s2">def </span><span class="s1">_compute_transformed_categories(self</span><span class="s2">, </span><span class="s1">i</span><span class="s2">, </span><span class="s1">remove_dropped=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s4">&quot;&quot;&quot;Compute the transformed categories used for column `i`. 
 
        1. If there are infrequent categories, the category is named 
        'infrequent_sklearn'. 
        2. Dropped columns are removed when remove_dropped=True. 
        &quot;&quot;&quot;</span>
        <span class="s1">cats = self.categories_[i]</span>

        <span class="s2">if </span><span class="s1">self._infrequent_enabled:</span>
            <span class="s1">infreq_map = self._default_to_infrequent_mappings[i]</span>
            <span class="s2">if </span><span class="s1">infreq_map </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">frequent_mask = infreq_map &lt; infreq_map.max()</span>
                <span class="s1">infrequent_cat = </span><span class="s3">&quot;infrequent_sklearn&quot;</span>
                <span class="s0"># infrequent category is always at the end</span>
                <span class="s1">cats = np.concatenate(</span>
                    <span class="s1">(cats[frequent_mask]</span><span class="s2">, </span><span class="s1">np.array([infrequent_cat]</span><span class="s2">, </span><span class="s1">dtype=object))</span>
                <span class="s1">)</span>

        <span class="s2">if </span><span class="s1">remove_dropped:</span>
            <span class="s1">cats = self._remove_dropped_categories(cats</span><span class="s2">, </span><span class="s1">i)</span>
        <span class="s2">return </span><span class="s1">cats</span>

    <span class="s2">def </span><span class="s1">_remove_dropped_categories(self</span><span class="s2">, </span><span class="s1">categories</span><span class="s2">, </span><span class="s1">i):</span>
        <span class="s4">&quot;&quot;&quot;Remove dropped categories.&quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">(</span>
            <span class="s1">self._drop_idx_after_grouping </span><span class="s2">is not None</span>
            <span class="s2">and </span><span class="s1">self._drop_idx_after_grouping[i] </span><span class="s2">is not None</span>
        <span class="s1">):</span>
            <span class="s2">return </span><span class="s1">np.delete(categories</span><span class="s2">, </span><span class="s1">self._drop_idx_after_grouping[i])</span>
        <span class="s2">return </span><span class="s1">categories</span>

    <span class="s2">def </span><span class="s1">_compute_n_features_outs(self):</span>
        <span class="s4">&quot;&quot;&quot;Compute the n_features_out for each input feature.&quot;&quot;&quot;</span>
        <span class="s1">output = [len(cats) </span><span class="s2">for </span><span class="s1">cats </span><span class="s2">in </span><span class="s1">self.categories_]</span>

        <span class="s2">if </span><span class="s1">self._drop_idx_after_grouping </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">drop_idx </span><span class="s2">in </span><span class="s1">enumerate(self._drop_idx_after_grouping):</span>
                <span class="s2">if </span><span class="s1">drop_idx </span><span class="s2">is not None</span><span class="s1">:</span>
                    <span class="s1">output[i] -= </span><span class="s5">1</span>

        <span class="s2">if not </span><span class="s1">self._infrequent_enabled:</span>
            <span class="s2">return </span><span class="s1">output</span>

        <span class="s0"># infrequent is enabled, the number of features out are reduced</span>
        <span class="s0"># because the infrequent categories are grouped together</span>
        <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">infreq_idx </span><span class="s2">in </span><span class="s1">enumerate(self._infrequent_indices):</span>
            <span class="s2">if </span><span class="s1">infreq_idx </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s2">continue</span>
            <span class="s1">output[i] -= infreq_idx.size - </span><span class="s5">1</span>

        <span class="s2">return </span><span class="s1">output</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s4">&quot;&quot;&quot; 
        Fit OneHotEncoder to X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The data to determine the categories of each feature. 
 
        y : None 
            Ignored. This parameter exists only for compatibility with 
            :class:`~sklearn.pipeline.Pipeline`. 
 
        Returns 
        ------- 
        self 
            Fitted encoder. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">self.sparse != </span><span class="s3">&quot;deprecated&quot;</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s3">&quot;`sparse` was renamed to `sparse_output` in version 1.2 and &quot;</span>
                    <span class="s3">&quot;will be removed in 1.4. `sparse_output` is ignored unless you &quot;</span>
                    <span class="s3">&quot;leave `sparse` to its default value.&quot;</span>
                <span class="s1">)</span><span class="s2">,</span>
                <span class="s1">FutureWarning</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">self.sparse_output = self.sparse</span>

        <span class="s1">self._fit(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">handle_unknown=self.handle_unknown</span><span class="s2">,</span>
            <span class="s1">force_all_finite=</span><span class="s3">&quot;allow-nan&quot;</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">self._set_drop_idx()</span>
        <span class="s1">self._n_features_outs = self._compute_n_features_outs()</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">transform(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s4">&quot;&quot;&quot; 
        Transform X using one-hot encoding. 
 
        If there are infrequent categories for a feature, the infrequent 
        categories will be grouped into a single category. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The data to encode. 
 
        Returns 
        ------- 
        X_out : {ndarray, sparse matrix} of shape \ 
                (n_samples, n_encoded_features) 
            Transformed input. If `sparse_output=True`, a sparse matrix will be 
            returned. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">transform_output = _get_output_config(</span><span class="s3">&quot;transform&quot;</span><span class="s2">, </span><span class="s1">estimator=self)[</span><span class="s3">&quot;dense&quot;</span><span class="s1">]</span>
        <span class="s2">if </span><span class="s1">transform_output == </span><span class="s3">&quot;pandas&quot; </span><span class="s2">and </span><span class="s1">self.sparse_output:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s3">&quot;Pandas output does not support sparse data. Set sparse_output=False to&quot;</span>
                <span class="s3">&quot; output pandas DataFrames or disable pandas output via&quot;</span>
                <span class="s3">' `ohe.set_output(transform=&quot;default&quot;).'</span>
            <span class="s1">)</span>

        <span class="s0"># validation of X happens in _check_X called by _transform</span>
        <span class="s1">warn_on_unknown = self.drop </span><span class="s2">is not None and </span><span class="s1">self.handle_unknown </span><span class="s2">in </span><span class="s1">{</span>
            <span class="s3">&quot;ignore&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;infrequent_if_exist&quot;</span><span class="s2">,</span>
        <span class="s1">}</span>
        <span class="s1">X_int</span><span class="s2">, </span><span class="s1">X_mask = self._transform(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">handle_unknown=self.handle_unknown</span><span class="s2">,</span>
            <span class="s1">force_all_finite=</span><span class="s3">&quot;allow-nan&quot;</span><span class="s2">,</span>
            <span class="s1">warn_on_unknown=warn_on_unknown</span><span class="s2">,</span>
        <span class="s1">)</span>

        <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X_int.shape</span>

        <span class="s2">if </span><span class="s1">self._drop_idx_after_grouping </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">to_drop = self._drop_idx_after_grouping.copy()</span>
            <span class="s0"># We remove all the dropped categories from mask, and decrement all</span>
            <span class="s0"># categories that occur after them to avoid an empty column.</span>
            <span class="s1">keep_cells = X_int != to_drop</span>
            <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">cats </span><span class="s2">in </span><span class="s1">enumerate(self.categories_):</span>
                <span class="s0"># drop='if_binary' but feature isn't binary</span>
                <span class="s2">if </span><span class="s1">to_drop[i] </span><span class="s2">is None</span><span class="s1">:</span>
                    <span class="s0"># set to cardinality to not drop from X_int</span>
                    <span class="s1">to_drop[i] = len(cats)</span>

            <span class="s1">to_drop = to_drop.reshape(</span><span class="s5">1</span><span class="s2">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">)</span>
            <span class="s1">X_int[X_int &gt; to_drop] -= </span><span class="s5">1</span>
            <span class="s1">X_mask &amp;= keep_cells</span>

        <span class="s1">mask = X_mask.ravel()</span>
        <span class="s1">feature_indices = np.cumsum([</span><span class="s5">0</span><span class="s1">] + self._n_features_outs)</span>
        <span class="s1">indices = (X_int + feature_indices[:-</span><span class="s5">1</span><span class="s1">]).ravel()[mask]</span>

        <span class="s1">indptr = np.empty(n_samples + </span><span class="s5">1</span><span class="s2">, </span><span class="s1">dtype=int)</span>
        <span class="s1">indptr[</span><span class="s5">0</span><span class="s1">] = </span><span class="s5">0</span>
        <span class="s1">np.sum(X_mask</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">out=indptr[</span><span class="s5">1</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">dtype=indptr.dtype)</span>
        <span class="s1">np.cumsum(indptr[</span><span class="s5">1</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">out=indptr[</span><span class="s5">1</span><span class="s1">:])</span>
        <span class="s1">data = np.ones(indptr[-</span><span class="s5">1</span><span class="s1">])</span>

        <span class="s1">out = sparse.csr_matrix(</span>
            <span class="s1">(data</span><span class="s2">, </span><span class="s1">indices</span><span class="s2">, </span><span class="s1">indptr)</span><span class="s2">,</span>
            <span class="s1">shape=(n_samples</span><span class="s2">, </span><span class="s1">feature_indices[-</span><span class="s5">1</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">dtype=self.dtype</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s2">if not </span><span class="s1">self.sparse_output:</span>
            <span class="s2">return </span><span class="s1">out.toarray()</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">inverse_transform(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s4">&quot;&quot;&quot; 
        Convert the data back to the original representation. 
 
        When unknown categories are encountered (all zeros in the 
        one-hot encoding), ``None`` is used to represent this category. If the 
        feature with the unknown category has a dropped category, the dropped 
        category will be its inverse. 
 
        For a given input feature, if there is an infrequent category, 
        'infrequent_sklearn' will be used to represent the infrequent category. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape \ 
                (n_samples, n_encoded_features) 
            The transformed data. 
 
        Returns 
        ------- 
        X_tr : ndarray of shape (n_samples, n_features) 
            Inverse transformed array. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">X = check_array(X</span><span class="s2">, </span><span class="s1">accept_sparse=</span><span class="s3">&quot;csr&quot;</span><span class="s1">)</span>

        <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">_ = X.shape</span>
        <span class="s1">n_features = len(self.categories_)</span>

        <span class="s1">n_features_out = np.sum(self._n_features_outs)</span>

        <span class="s0"># validate shape of passed X</span>
        <span class="s1">msg = (</span>
            <span class="s3">&quot;Shape of the passed X data is not correct. Expected {0} columns, got {1}.&quot;</span>
        <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">] != n_features_out:</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg.format(n_features_out</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">]))</span>

        <span class="s1">transformed_features = [</span>
            <span class="s1">self._compute_transformed_categories(i</span><span class="s2">, </span><span class="s1">remove_dropped=</span><span class="s2">False</span><span class="s1">)</span>
            <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">enumerate(self.categories_)</span>
        <span class="s1">]</span>

        <span class="s0"># create resulting array of appropriate dtype</span>
        <span class="s1">dt = np.result_type(*[cat.dtype </span><span class="s2">for </span><span class="s1">cat </span><span class="s2">in </span><span class="s1">transformed_features])</span>
        <span class="s1">X_tr = np.empty((n_samples</span><span class="s2">, </span><span class="s1">n_features)</span><span class="s2">, </span><span class="s1">dtype=dt)</span>

        <span class="s1">j = </span><span class="s5">0</span>
        <span class="s1">found_unknown = {}</span>

        <span class="s2">if </span><span class="s1">self._infrequent_enabled:</span>
            <span class="s1">infrequent_indices = self._infrequent_indices</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">infrequent_indices = [</span><span class="s2">None</span><span class="s1">] * n_features</span>

        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_features):</span>
            <span class="s1">cats_wo_dropped = self._remove_dropped_categories(</span>
                <span class="s1">transformed_features[i]</span><span class="s2">, </span><span class="s1">i</span>
            <span class="s1">)</span>
            <span class="s1">n_categories = cats_wo_dropped.shape[</span><span class="s5">0</span><span class="s1">]</span>

            <span class="s0"># Only happens if there was a column with a unique</span>
            <span class="s0"># category. In this case we just fill the column with this</span>
            <span class="s0"># unique category value.</span>
            <span class="s2">if </span><span class="s1">n_categories == </span><span class="s5">0</span><span class="s1">:</span>
                <span class="s1">X_tr[:</span><span class="s2">, </span><span class="s1">i] = self.categories_[i][self._drop_idx_after_grouping[i]]</span>
                <span class="s1">j += n_categories</span>
                <span class="s2">continue</span>
            <span class="s1">sub = X[:</span><span class="s2">, </span><span class="s1">j : j + n_categories]</span>
            <span class="s0"># for sparse X argmax returns 2D matrix, ensure 1D array</span>
            <span class="s1">labels = np.asarray(sub.argmax(axis=</span><span class="s5">1</span><span class="s1">)).flatten()</span>
            <span class="s1">X_tr[:</span><span class="s2">, </span><span class="s1">i] = cats_wo_dropped[labels]</span>

            <span class="s2">if </span><span class="s1">self.handle_unknown == </span><span class="s3">&quot;ignore&quot; </span><span class="s2">or </span><span class="s1">(</span>
                <span class="s1">self.handle_unknown == </span><span class="s3">&quot;infrequent_if_exist&quot;</span>
                <span class="s2">and </span><span class="s1">infrequent_indices[i] </span><span class="s2">is None</span>
            <span class="s1">):</span>
                <span class="s1">unknown = np.asarray(sub.sum(axis=</span><span class="s5">1</span><span class="s1">) == </span><span class="s5">0</span><span class="s1">).flatten()</span>
                <span class="s0"># ignored unknown categories: we have a row of all zero</span>
                <span class="s2">if </span><span class="s1">unknown.any():</span>
                    <span class="s0"># if categories were dropped then unknown categories will</span>
                    <span class="s0"># be mapped to the dropped category</span>
                    <span class="s2">if </span><span class="s1">(</span>
                        <span class="s1">self._drop_idx_after_grouping </span><span class="s2">is None</span>
                        <span class="s2">or </span><span class="s1">self._drop_idx_after_grouping[i] </span><span class="s2">is None</span>
                    <span class="s1">):</span>
                        <span class="s1">found_unknown[i] = unknown</span>
                    <span class="s2">else</span><span class="s1">:</span>
                        <span class="s1">X_tr[unknown</span><span class="s2">, </span><span class="s1">i] = self.categories_[i][</span>
                            <span class="s1">self._drop_idx_after_grouping[i]</span>
                        <span class="s1">]</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">dropped = np.asarray(sub.sum(axis=</span><span class="s5">1</span><span class="s1">) == </span><span class="s5">0</span><span class="s1">).flatten()</span>
                <span class="s2">if </span><span class="s1">dropped.any():</span>
                    <span class="s2">if </span><span class="s1">self._drop_idx_after_grouping </span><span class="s2">is None</span><span class="s1">:</span>
                        <span class="s1">all_zero_samples = np.flatnonzero(dropped)</span>
                        <span class="s2">raise </span><span class="s1">ValueError(</span>
                            <span class="s3">f&quot;Samples </span><span class="s2">{</span><span class="s1">all_zero_samples</span><span class="s2">} </span><span class="s3">can not be inverted &quot;</span>
                            <span class="s3">&quot;when drop=None and handle_unknown='error' &quot;</span>
                            <span class="s3">&quot;because they contain all zeros&quot;</span>
                        <span class="s1">)</span>
                    <span class="s0"># we can safely assume that all of the nulls in each column</span>
                    <span class="s0"># are the dropped value</span>
                    <span class="s1">drop_idx = self._drop_idx_after_grouping[i]</span>
                    <span class="s1">X_tr[dropped</span><span class="s2">, </span><span class="s1">i] = transformed_features[i][drop_idx]</span>

            <span class="s1">j += n_categories</span>

        <span class="s0"># if ignored are found: potentially need to upcast result to</span>
        <span class="s0"># insert None values</span>
        <span class="s2">if </span><span class="s1">found_unknown:</span>
            <span class="s2">if </span><span class="s1">X_tr.dtype != object:</span>
                <span class="s1">X_tr = X_tr.astype(object)</span>

            <span class="s2">for </span><span class="s1">idx</span><span class="s2">, </span><span class="s1">mask </span><span class="s2">in </span><span class="s1">found_unknown.items():</span>
                <span class="s1">X_tr[mask</span><span class="s2">, </span><span class="s1">idx] = </span><span class="s2">None</span>

        <span class="s2">return </span><span class="s1">X_tr</span>

    <span class="s2">def </span><span class="s1">get_feature_names_out(self</span><span class="s2">, </span><span class="s1">input_features=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s4">&quot;&quot;&quot;Get output feature names for transformation. 
 
        Parameters 
        ---------- 
        input_features : array-like of str or None, default=None 
            Input features. 
 
            - If `input_features` is `None`, then `feature_names_in_` is 
              used as feature names in. If `feature_names_in_` is not defined, 
              then the following input feature names are generated: 
              `[&quot;x0&quot;, &quot;x1&quot;, ..., &quot;x(n_features_in_ - 1)&quot;]`. 
            - If `input_features` is an array-like, then `input_features` must 
              match `feature_names_in_` if `feature_names_in_` is defined. 
 
        Returns 
        ------- 
        feature_names_out : ndarray of str objects 
            Transformed feature names. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">input_features = _check_feature_names_in(self</span><span class="s2">, </span><span class="s1">input_features)</span>
        <span class="s1">cats = [</span>
            <span class="s1">self._compute_transformed_categories(i)</span>
            <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">enumerate(self.categories_)</span>
        <span class="s1">]</span>

        <span class="s1">name_combiner = self._check_get_feature_name_combiner()</span>
        <span class="s1">feature_names = []</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(len(cats)):</span>
            <span class="s1">names = [name_combiner(input_features[i]</span><span class="s2">, </span><span class="s1">t) </span><span class="s2">for </span><span class="s1">t </span><span class="s2">in </span><span class="s1">cats[i]]</span>
            <span class="s1">feature_names.extend(names)</span>

        <span class="s2">return </span><span class="s1">np.array(feature_names</span><span class="s2">, </span><span class="s1">dtype=object)</span>

    <span class="s2">def </span><span class="s1">_check_get_feature_name_combiner(self):</span>
        <span class="s2">if </span><span class="s1">self.feature_name_combiner == </span><span class="s3">&quot;concat&quot;</span><span class="s1">:</span>
            <span class="s2">return lambda </span><span class="s1">feature</span><span class="s2">, </span><span class="s1">category: feature + </span><span class="s3">&quot;_&quot; </span><span class="s1">+ str(category)</span>
        <span class="s2">else</span><span class="s1">:  </span><span class="s0"># callable</span>
            <span class="s1">dry_run_combiner = self.feature_name_combiner(</span><span class="s3">&quot;feature&quot;</span><span class="s2">, </span><span class="s3">&quot;category&quot;</span><span class="s1">)</span>
            <span class="s2">if not </span><span class="s1">isinstance(dry_run_combiner</span><span class="s2">, </span><span class="s1">str):</span>
                <span class="s2">raise </span><span class="s1">TypeError(</span>
                    <span class="s3">&quot;When `feature_name_combiner` is a callable, it should return a &quot;</span>
                    <span class="s3">f&quot;Python string. Got </span><span class="s2">{</span><span class="s1">type(dry_run_combiner)</span><span class="s2">} </span><span class="s3">instead.&quot;</span>
                <span class="s1">)</span>
            <span class="s2">return </span><span class="s1">self.feature_name_combiner</span>


<span class="s2">class </span><span class="s1">OrdinalEncoder(OneToOneFeatureMixin</span><span class="s2">, </span><span class="s1">_BaseEncoder):</span>
    <span class="s4">&quot;&quot;&quot; 
    Encode categorical features as an integer array. 
 
    The input to this transformer should be an array-like of integers or 
    strings, denoting the values taken on by categorical (discrete) features. 
    The features are converted to ordinal integers. This results in 
    a single column of integers (0 to n_categories - 1) per feature. 
 
    Read more in the :ref:`User Guide &lt;preprocessing_categorical_features&gt;`. 
    For a comparison of different encoders, refer to: 
    :ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder.py`. 
 
    .. versionadded:: 0.20 
 
    Parameters 
    ---------- 
    categories : 'auto' or a list of array-like, default='auto' 
        Categories (unique values) per feature: 
 
        - 'auto' : Determine categories automatically from the training data. 
        - list : ``categories[i]`` holds the categories expected in the ith 
          column. The passed categories should not mix strings and numeric 
          values, and should be sorted in case of numeric values. 
 
        The used categories can be found in the ``categories_`` attribute. 
 
    dtype : number type, default=np.float64 
        Desired dtype of output. 
 
    handle_unknown : {'error', 'use_encoded_value'}, default='error' 
        When set to 'error' an error will be raised in case an unknown 
        categorical feature is present during transform. When set to 
        'use_encoded_value', the encoded value of unknown categories will be 
        set to the value given for the parameter `unknown_value`. In 
        :meth:`inverse_transform`, an unknown category will be denoted as None. 
 
        .. versionadded:: 0.24 
 
    unknown_value : int or np.nan, default=None 
        When the parameter handle_unknown is set to 'use_encoded_value', this 
        parameter is required and will set the encoded value of unknown 
        categories. It has to be distinct from the values used to encode any of 
        the categories in `fit`. If set to np.nan, the `dtype` parameter must 
        be a float dtype. 
 
        .. versionadded:: 0.24 
 
    encoded_missing_value : int or np.nan, default=np.nan 
        Encoded value of missing categories. If set to `np.nan`, then the `dtype` 
        parameter must be a float dtype. 
 
        .. versionadded:: 1.1 
 
    min_frequency : int or float, default=None 
        Specifies the minimum frequency below which a category will be 
        considered infrequent. 
 
        - If `int`, categories with a smaller cardinality will be considered 
          infrequent. 
 
        - If `float`, categories with a smaller cardinality than 
          `min_frequency * n_samples`  will be considered infrequent. 
 
        .. versionadded:: 1.3 
            Read more in the :ref:`User Guide &lt;encoder_infrequent_categories&gt;`. 
 
    max_categories : int, default=None 
        Specifies an upper limit to the number of output categories for each input 
        feature when considering infrequent categories. If there are infrequent 
        categories, `max_categories` includes the category representing the 
        infrequent categories along with the frequent categories. If `None`, 
        there is no limit to the number of output features. 
 
        `max_categories` do **not** take into account missing or unknown 
        categories. Setting `unknown_value` or `encoded_missing_value` to an 
        integer will increase the number of unique integer codes by one each. 
        This can result in up to `max_categories + 2` integer codes. 
 
        .. versionadded:: 1.3 
            Read more in the :ref:`User Guide &lt;encoder_infrequent_categories&gt;`. 
 
    Attributes 
    ---------- 
    categories_ : list of arrays 
        The categories of each feature determined during ``fit`` (in order of 
        the features in X and corresponding with the output of ``transform``). 
        This does not include categories that weren't seen during ``fit``. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 1.0 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    infrequent_categories_ : list of ndarray 
        Defined only if infrequent categories are enabled by setting 
        `min_frequency` or `max_categories` to a non-default value. 
        `infrequent_categories_[i]` are the infrequent categories for feature 
        `i`. If the feature `i` has no infrequent categories 
        `infrequent_categories_[i]` is None. 
 
        .. versionadded:: 1.3 
 
    See Also 
    -------- 
    OneHotEncoder : Performs a one-hot encoding of categorical features. This encoding 
        is suitable for low to medium cardinality categorical variables, both in 
        supervised and unsupervised settings. 
    TargetEncoder : Encodes categorical features using supervised signal 
        in a classification or regression pipeline. This encoding is typically 
        suitable for high cardinality categorical variables. 
    LabelEncoder : Encodes target labels with values between 0 and 
        ``n_classes-1``. 
 
    Notes 
    ----- 
    With a high proportion of `nan` values, inferring categories becomes slow with 
    Python versions before 3.10. The handling of `nan` values was improved 
    from Python 3.10 onwards, (c.f. 
    `bpo-43475 &lt;https://github.com/python/cpython/issues/87641&gt;`_). 
 
    Examples 
    -------- 
    Given a dataset with two features, we let the encoder find the unique 
    values per feature and transform the data to an ordinal encoding. 
 
    &gt;&gt;&gt; from sklearn.preprocessing import OrdinalEncoder 
    &gt;&gt;&gt; enc = OrdinalEncoder() 
    &gt;&gt;&gt; X = [['Male', 1], ['Female', 3], ['Female', 2]] 
    &gt;&gt;&gt; enc.fit(X) 
    OrdinalEncoder() 
    &gt;&gt;&gt; enc.categories_ 
    [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)] 
    &gt;&gt;&gt; enc.transform([['Female', 3], ['Male', 1]]) 
    array([[0., 2.], 
           [1., 0.]]) 
 
    &gt;&gt;&gt; enc.inverse_transform([[1, 0], [0, 1]]) 
    array([['Male', 1], 
           ['Female', 2]], dtype=object) 
 
    By default, :class:`OrdinalEncoder` is lenient towards missing values by 
    propagating them. 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; X = [['Male', 1], ['Female', 3], ['Female', np.nan]] 
    &gt;&gt;&gt; enc.fit_transform(X) 
    array([[ 1.,  0.], 
           [ 0.,  1.], 
           [ 0., nan]]) 
 
    You can use the parameter `encoded_missing_value` to encode missing values. 
 
    &gt;&gt;&gt; enc.set_params(encoded_missing_value=-1).fit_transform(X) 
    array([[ 1.,  0.], 
           [ 0.,  1.], 
           [ 0., -1.]]) 
 
    Infrequent categories are enabled by setting `max_categories` or `min_frequency`. 
    In the following example, &quot;a&quot; and &quot;d&quot; are considered infrequent and grouped 
    together into a single category, &quot;b&quot; and &quot;c&quot; are their own categories, unknown 
    values are encoded as 3 and missing values are encoded as 4. 
 
    &gt;&gt;&gt; X_train = np.array( 
    ...     [[&quot;a&quot;] * 5 + [&quot;b&quot;] * 20 + [&quot;c&quot;] * 10 + [&quot;d&quot;] * 3 + [np.nan]], 
    ...     dtype=object).T 
    &gt;&gt;&gt; enc = OrdinalEncoder( 
    ...     handle_unknown=&quot;use_encoded_value&quot;, unknown_value=3, 
    ...     max_categories=3, encoded_missing_value=4) 
    &gt;&gt;&gt; _ = enc.fit(X_train) 
    &gt;&gt;&gt; X_test = np.array([[&quot;a&quot;], [&quot;b&quot;], [&quot;c&quot;], [&quot;d&quot;], [&quot;e&quot;], [np.nan]], dtype=object) 
    &gt;&gt;&gt; enc.transform(X_test) 
    array([[2.], 
           [0.], 
           [1.], 
           [2.], 
           [3.], 
           [4.]]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s3">&quot;categories&quot;</span><span class="s1">: [StrOptions({</span><span class="s3">&quot;auto&quot;</span><span class="s1">})</span><span class="s2">, </span><span class="s1">list]</span><span class="s2">,</span>
        <span class="s3">&quot;dtype&quot;</span><span class="s1">: </span><span class="s3">&quot;no_validation&quot;</span><span class="s2">,  </span><span class="s0"># validation delegated to numpy</span>
        <span class="s3">&quot;encoded_missing_value&quot;</span><span class="s1">: [Integral</span><span class="s2">, </span><span class="s1">type(np.nan)]</span><span class="s2">,</span>
        <span class="s3">&quot;handle_unknown&quot;</span><span class="s1">: [StrOptions({</span><span class="s3">&quot;error&quot;</span><span class="s2">, </span><span class="s3">&quot;use_encoded_value&quot;</span><span class="s1">})]</span><span class="s2">,</span>
        <span class="s3">&quot;unknown_value&quot;</span><span class="s1">: [Integral</span><span class="s2">, </span><span class="s1">type(np.nan)</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;max_categories&quot;</span><span class="s1">: [Interval(Integral</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s3">&quot;left&quot;</span><span class="s1">)</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;min_frequency&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Integral</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s3">&quot;left&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">Interval(RealNotInt</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s1">closed=</span><span class="s3">&quot;neither&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s2">None,</span>
        <span class="s1">]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">*</span><span class="s2">,</span>
        <span class="s1">categories=</span><span class="s3">&quot;auto&quot;</span><span class="s2">,</span>
        <span class="s1">dtype=np.float64</span><span class="s2">,</span>
        <span class="s1">handle_unknown=</span><span class="s3">&quot;error&quot;</span><span class="s2">,</span>
        <span class="s1">unknown_value=</span><span class="s2">None,</span>
        <span class="s1">encoded_missing_value=np.nan</span><span class="s2">,</span>
        <span class="s1">min_frequency=</span><span class="s2">None,</span>
        <span class="s1">max_categories=</span><span class="s2">None,</span>
    <span class="s1">):</span>
        <span class="s1">self.categories = categories</span>
        <span class="s1">self.dtype = dtype</span>
        <span class="s1">self.handle_unknown = handle_unknown</span>
        <span class="s1">self.unknown_value = unknown_value</span>
        <span class="s1">self.encoded_missing_value = encoded_missing_value</span>
        <span class="s1">self.min_frequency = min_frequency</span>
        <span class="s1">self.max_categories = max_categories</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s4">&quot;&quot;&quot; 
        Fit the OrdinalEncoder to X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The data to determine the categories of each feature. 
 
        y : None 
            Ignored. This parameter exists only for compatibility with 
            :class:`~sklearn.pipeline.Pipeline`. 
 
        Returns 
        ------- 
        self : object 
            Fitted encoder. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">self.handle_unknown == </span><span class="s3">&quot;use_encoded_value&quot;</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">is_scalar_nan(self.unknown_value):</span>
                <span class="s2">if </span><span class="s1">np.dtype(self.dtype).kind != </span><span class="s3">&quot;f&quot;</span><span class="s1">:</span>
                    <span class="s2">raise </span><span class="s1">ValueError(</span>
                        <span class="s3">&quot;When unknown_value is np.nan, the dtype &quot;</span>
                        <span class="s3">&quot;parameter should be &quot;</span>
                        <span class="s3">f&quot;a float dtype. Got </span><span class="s2">{</span><span class="s1">self.dtype</span><span class="s2">}</span><span class="s3">.&quot;</span>
                    <span class="s1">)</span>
            <span class="s2">elif not </span><span class="s1">isinstance(self.unknown_value</span><span class="s2">, </span><span class="s1">numbers.Integral):</span>
                <span class="s2">raise </span><span class="s1">TypeError(</span>
                    <span class="s3">&quot;unknown_value should be an integer or &quot;</span>
                    <span class="s3">&quot;np.nan when &quot;</span>
                    <span class="s3">&quot;handle_unknown is 'use_encoded_value', &quot;</span>
                    <span class="s3">f&quot;got </span><span class="s2">{</span><span class="s1">self.unknown_value</span><span class="s2">}</span><span class="s3">.&quot;</span>
                <span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">self.unknown_value </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">TypeError(</span>
                <span class="s3">&quot;unknown_value should only be set when &quot;</span>
                <span class="s3">&quot;handle_unknown is 'use_encoded_value', &quot;</span>
                <span class="s3">f&quot;got </span><span class="s2">{</span><span class="s1">self.unknown_value</span><span class="s2">}</span><span class="s3">.&quot;</span>
            <span class="s1">)</span>

        <span class="s0"># `_fit` will only raise an error when `self.handle_unknown=&quot;error&quot;`</span>
        <span class="s1">fit_results = self._fit(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">handle_unknown=self.handle_unknown</span><span class="s2">,</span>
            <span class="s1">force_all_finite=</span><span class="s3">&quot;allow-nan&quot;</span><span class="s2">,</span>
            <span class="s1">return_and_ignore_missing_for_infrequent=</span><span class="s2">True,</span>
        <span class="s1">)</span>
        <span class="s1">self._missing_indices = fit_results[</span><span class="s3">&quot;missing_indices&quot;</span><span class="s1">]</span>

        <span class="s1">cardinalities = [len(categories) </span><span class="s2">for </span><span class="s1">categories </span><span class="s2">in </span><span class="s1">self.categories_]</span>
        <span class="s2">if </span><span class="s1">self._infrequent_enabled:</span>
            <span class="s0"># Cardinality decreases because the infrequent categories are grouped</span>
            <span class="s0"># together</span>
            <span class="s2">for </span><span class="s1">feature_idx</span><span class="s2">, </span><span class="s1">infrequent </span><span class="s2">in </span><span class="s1">enumerate(self.infrequent_categories_):</span>
                <span class="s2">if </span><span class="s1">infrequent </span><span class="s2">is not None</span><span class="s1">:</span>
                    <span class="s1">cardinalities[feature_idx] -= len(infrequent)</span>

        <span class="s0"># stores the missing indices per category</span>
        <span class="s1">self._missing_indices = {}</span>
        <span class="s2">for </span><span class="s1">cat_idx</span><span class="s2">, </span><span class="s1">categories_for_idx </span><span class="s2">in </span><span class="s1">enumerate(self.categories_):</span>
            <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">cat </span><span class="s2">in </span><span class="s1">enumerate(categories_for_idx):</span>
                <span class="s2">if </span><span class="s1">is_scalar_nan(cat):</span>
                    <span class="s1">self._missing_indices[cat_idx] = i</span>

                    <span class="s0"># missing values are not considered part of the cardinality</span>
                    <span class="s0"># when considering unknown categories or encoded_missing_value</span>
                    <span class="s1">cardinalities[cat_idx] -= </span><span class="s5">1</span>
                    <span class="s2">continue</span>

        <span class="s2">if </span><span class="s1">self.handle_unknown == </span><span class="s3">&quot;use_encoded_value&quot;</span><span class="s1">:</span>
            <span class="s2">for </span><span class="s1">cardinality </span><span class="s2">in </span><span class="s1">cardinalities:</span>
                <span class="s2">if </span><span class="s5">0 </span><span class="s1">&lt;= self.unknown_value &lt; cardinality:</span>
                    <span class="s2">raise </span><span class="s1">ValueError(</span>
                        <span class="s3">&quot;The used value for unknown_value &quot;</span>
                        <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">self.unknown_value</span><span class="s2">} </span><span class="s3">is one of the &quot;</span>
                        <span class="s3">&quot;values already used for encoding the &quot;</span>
                        <span class="s3">&quot;seen categories.&quot;</span>
                    <span class="s1">)</span>

        <span class="s2">if </span><span class="s1">self._missing_indices:</span>
            <span class="s2">if </span><span class="s1">np.dtype(self.dtype).kind != </span><span class="s3">&quot;f&quot; </span><span class="s2">and </span><span class="s1">is_scalar_nan(</span>
                <span class="s1">self.encoded_missing_value</span>
            <span class="s1">):</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s3">&quot;There are missing values in features &quot;</span>
                    <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">list(self._missing_indices)</span><span class="s2">}</span><span class="s3">. For OrdinalEncoder to &quot;</span>
                    <span class="s3">f&quot;encode missing values with dtype: </span><span class="s2">{</span><span class="s1">self.dtype</span><span class="s2">}</span><span class="s3">, set &quot;</span>
                    <span class="s3">&quot;encoded_missing_value to a non-nan value, or &quot;</span>
                    <span class="s3">&quot;set dtype to a float&quot;</span>
                <span class="s1">)</span>

            <span class="s2">if not </span><span class="s1">is_scalar_nan(self.encoded_missing_value):</span>
                <span class="s0"># Features are invalid when they contain a missing category</span>
                <span class="s0"># and encoded_missing_value was already used to encode a</span>
                <span class="s0"># known category</span>
                <span class="s1">invalid_features = [</span>
                    <span class="s1">cat_idx</span>
                    <span class="s2">for </span><span class="s1">cat_idx</span><span class="s2">, </span><span class="s1">cardinality </span><span class="s2">in </span><span class="s1">enumerate(cardinalities)</span>
                    <span class="s2">if </span><span class="s1">cat_idx </span><span class="s2">in </span><span class="s1">self._missing_indices</span>
                    <span class="s2">and </span><span class="s5">0 </span><span class="s1">&lt;= self.encoded_missing_value &lt; cardinality</span>
                <span class="s1">]</span>

                <span class="s2">if </span><span class="s1">invalid_features:</span>
                    <span class="s0"># Use feature names if they are available</span>
                    <span class="s2">if </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s3">&quot;feature_names_in_&quot;</span><span class="s1">):</span>
                        <span class="s1">invalid_features = self.feature_names_in_[invalid_features]</span>
                    <span class="s2">raise </span><span class="s1">ValueError(</span>
                        <span class="s3">f&quot;encoded_missing_value (</span><span class="s2">{</span><span class="s1">self.encoded_missing_value</span><span class="s2">}</span><span class="s3">) &quot;</span>
                        <span class="s3">&quot;is already used to encode a known category in features: &quot;</span>
                        <span class="s3">f&quot;</span><span class="s2">{</span><span class="s1">invalid_features</span><span class="s2">}</span><span class="s3">&quot;</span>
                    <span class="s1">)</span>

        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">transform(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s4">&quot;&quot;&quot; 
        Transform X to ordinal codes. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The data to encode. 
 
        Returns 
        ------- 
        X_out : ndarray of shape (n_samples, n_features) 
            Transformed input. 
        &quot;&quot;&quot;</span>
        <span class="s1">X_int</span><span class="s2">, </span><span class="s1">X_mask = self._transform(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">handle_unknown=self.handle_unknown</span><span class="s2">,</span>
            <span class="s1">force_all_finite=</span><span class="s3">&quot;allow-nan&quot;</span><span class="s2">,</span>
            <span class="s1">ignore_category_indices=self._missing_indices</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">X_trans = X_int.astype(self.dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>

        <span class="s2">for </span><span class="s1">cat_idx</span><span class="s2">, </span><span class="s1">missing_idx </span><span class="s2">in </span><span class="s1">self._missing_indices.items():</span>
            <span class="s1">X_missing_mask = X_int[:</span><span class="s2">, </span><span class="s1">cat_idx] == missing_idx</span>
            <span class="s1">X_trans[X_missing_mask</span><span class="s2">, </span><span class="s1">cat_idx] = self.encoded_missing_value</span>

        <span class="s0"># create separate category for unknown values</span>
        <span class="s2">if </span><span class="s1">self.handle_unknown == </span><span class="s3">&quot;use_encoded_value&quot;</span><span class="s1">:</span>
            <span class="s1">X_trans[~X_mask] = self.unknown_value</span>
        <span class="s2">return </span><span class="s1">X_trans</span>

    <span class="s2">def </span><span class="s1">inverse_transform(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s4">&quot;&quot;&quot; 
        Convert the data back to the original representation. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_encoded_features) 
            The transformed data. 
 
        Returns 
        ------- 
        X_tr : ndarray of shape (n_samples, n_features) 
            Inverse transformed array. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">X = check_array(X</span><span class="s2">, </span><span class="s1">force_all_finite=</span><span class="s3">&quot;allow-nan&quot;</span><span class="s1">)</span>

        <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">_ = X.shape</span>
        <span class="s1">n_features = len(self.categories_)</span>

        <span class="s0"># validate shape of passed X</span>
        <span class="s1">msg = (</span>
            <span class="s3">&quot;Shape of the passed X data is not correct. Expected {0} columns, got {1}.&quot;</span>
        <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">] != n_features:</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg.format(n_features</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">]))</span>

        <span class="s0"># create resulting array of appropriate dtype</span>
        <span class="s1">dt = np.result_type(*[cat.dtype </span><span class="s2">for </span><span class="s1">cat </span><span class="s2">in </span><span class="s1">self.categories_])</span>
        <span class="s1">X_tr = np.empty((n_samples</span><span class="s2">, </span><span class="s1">n_features)</span><span class="s2">, </span><span class="s1">dtype=dt)</span>

        <span class="s1">found_unknown = {}</span>
        <span class="s1">infrequent_masks = {}</span>

        <span class="s1">infrequent_indices = getattr(self</span><span class="s2">, </span><span class="s3">&quot;_infrequent_indices&quot;</span><span class="s2">, None</span><span class="s1">)</span>

        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_features):</span>
            <span class="s1">labels = X[:</span><span class="s2">, </span><span class="s1">i]</span>

            <span class="s0"># replace values of X[:, i] that were nan with actual indices</span>
            <span class="s2">if </span><span class="s1">i </span><span class="s2">in </span><span class="s1">self._missing_indices:</span>
                <span class="s1">X_i_mask = _get_mask(labels</span><span class="s2">, </span><span class="s1">self.encoded_missing_value)</span>
                <span class="s1">labels[X_i_mask] = self._missing_indices[i]</span>

            <span class="s1">rows_to_update = slice(</span><span class="s2">None</span><span class="s1">)</span>
            <span class="s1">categories = self.categories_[i]</span>

            <span class="s2">if </span><span class="s1">infrequent_indices </span><span class="s2">is not None and </span><span class="s1">infrequent_indices[i] </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s0"># Compute mask for frequent categories</span>
                <span class="s1">infrequent_encoding_value = len(categories) - len(infrequent_indices[i])</span>
                <span class="s1">infrequent_masks[i] = labels == infrequent_encoding_value</span>
                <span class="s1">rows_to_update = ~infrequent_masks[i]</span>

                <span class="s0"># Remap categories to be only frequent categories. The infrequent</span>
                <span class="s0"># categories will be mapped to &quot;infrequent_sklearn&quot; later</span>
                <span class="s1">frequent_categories_mask = np.ones_like(categories</span><span class="s2">, </span><span class="s1">dtype=bool)</span>
                <span class="s1">frequent_categories_mask[infrequent_indices[i]] = </span><span class="s2">False</span>
                <span class="s1">categories = categories[frequent_categories_mask]</span>

            <span class="s2">if </span><span class="s1">self.handle_unknown == </span><span class="s3">&quot;use_encoded_value&quot;</span><span class="s1">:</span>
                <span class="s1">unknown_labels = _get_mask(labels</span><span class="s2">, </span><span class="s1">self.unknown_value)</span>
                <span class="s1">found_unknown[i] = unknown_labels</span>

                <span class="s1">known_labels = ~unknown_labels</span>
                <span class="s2">if </span><span class="s1">isinstance(rows_to_update</span><span class="s2">, </span><span class="s1">np.ndarray):</span>
                    <span class="s1">rows_to_update &amp;= known_labels</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">rows_to_update = known_labels</span>

            <span class="s1">labels_int = labels[rows_to_update].astype(</span><span class="s3">&quot;int64&quot;</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
            <span class="s1">X_tr[rows_to_update</span><span class="s2">, </span><span class="s1">i] = categories[labels_int]</span>

        <span class="s2">if </span><span class="s1">found_unknown </span><span class="s2">or </span><span class="s1">infrequent_masks:</span>
            <span class="s1">X_tr = X_tr.astype(object</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>

        <span class="s0"># insert None values for unknown values</span>
        <span class="s2">if </span><span class="s1">found_unknown:</span>
            <span class="s2">for </span><span class="s1">idx</span><span class="s2">, </span><span class="s1">mask </span><span class="s2">in </span><span class="s1">found_unknown.items():</span>
                <span class="s1">X_tr[mask</span><span class="s2">, </span><span class="s1">idx] = </span><span class="s2">None</span>

        <span class="s2">if </span><span class="s1">infrequent_masks:</span>
            <span class="s2">for </span><span class="s1">idx</span><span class="s2">, </span><span class="s1">mask </span><span class="s2">in </span><span class="s1">infrequent_masks.items():</span>
                <span class="s1">X_tr[mask</span><span class="s2">, </span><span class="s1">idx] = </span><span class="s3">&quot;infrequent_sklearn&quot;</span>

        <span class="s2">return </span><span class="s1">X_tr</span>
</pre>
</body>
</html>