<html>
<head>
<title>elm.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
elm.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
High-level Extreme Learning Machine modules 
&quot;&quot;&quot;</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">scipy.special </span><span class="s2">import </span><span class="s1">expit</span>

<span class="s2">from </span><span class="s1">sklearn.base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">ClassifierMixin</span><span class="s2">, </span><span class="s1">RegressorMixin</span><span class="s2">, </span><span class="s1">clone</span>
<span class="s2">from </span><span class="s1">sklearn.utils.validation </span><span class="s2">import </span><span class="s1">check_X_y</span><span class="s2">, </span><span class="s1">check_array</span><span class="s2">, </span><span class="s1">check_is_fitted</span>
<span class="s2">from </span><span class="s1">sklearn.utils.multiclass </span><span class="s2">import </span><span class="s1">unique_labels</span><span class="s2">, </span><span class="s1">type_of_target</span>

<span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">LabelBinarizer</span><span class="s2">, </span><span class="s1">MultiLabelBinarizer</span>
<span class="s2">from </span><span class="s1">sklearn.exceptions </span><span class="s2">import </span><span class="s1">DataConversionWarning</span><span class="s2">, </span><span class="s1">DataDimensionalityWarning</span>

<span class="s2">from </span><span class="s1">.hidden_layer </span><span class="s2">import </span><span class="s1">HiddenLayer</span>
<span class="s2">from </span><span class="s1">.solver_batch </span><span class="s2">import </span><span class="s1">BatchCholeskySolver</span>
<span class="s2">from </span><span class="s1">.utils </span><span class="s2">import </span><span class="s1">_dense</span>

<span class="s1">warnings.simplefilter(</span><span class="s3">&quot;ignore&quot;</span><span class="s2">, </span><span class="s1">DataDimensionalityWarning)</span>


<span class="s2">class </span><span class="s1">_BaseELM(BaseEstimator):</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s4">1e-7</span><span class="s2">, </span><span class="s1">batch_size=</span><span class="s2">None, </span><span class="s1">include_original_features=</span><span class="s2">False,</span>
                 <span class="s1">n_neurons=</span><span class="s2">None, </span><span class="s1">ufunc=</span><span class="s3">&quot;tanh&quot;</span><span class="s2">, </span><span class="s1">density=</span><span class="s2">None, </span><span class="s1">pairwise_metric=</span><span class="s2">None,</span>
                 <span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>

        <span class="s1">self.alpha = alpha</span>
        <span class="s1">self.n_neurons = n_neurons</span>
        <span class="s1">self.batch_size = batch_size</span>
        <span class="s1">self.ufunc = ufunc</span>
        <span class="s1">self.include_original_features = include_original_features</span>
        <span class="s1">self.density = density</span>
        <span class="s1">self.pairwise_metric = pairwise_metric</span>
        <span class="s1">self.random_state = random_state</span>

    <span class="s2">def </span><span class="s1">_init_hidden_layers(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Init an empty model, creating objects for hidden layers and solver. 
 
        Also validates inputs for several hidden layers. 
        &quot;&quot;&quot;</span>
        <span class="s5"># only one type of neurons</span>
        <span class="s2">if not </span><span class="s1">hasattr(self.n_neurons</span><span class="s2">, </span><span class="s3">'__iter__'</span><span class="s1">):</span>
            <span class="s1">hl = HiddenLayer(n_neurons=self.n_neurons</span><span class="s2">, </span><span class="s1">density=self.density</span><span class="s2">, </span><span class="s1">ufunc=self.ufunc</span><span class="s2">,</span>
                             <span class="s1">pairwise_metric=self.pairwise_metric</span><span class="s2">, </span><span class="s1">random_state=self.random_state)</span>
            <span class="s1">hl.fit(X)</span>
            <span class="s1">self.hidden_layers_ = (hl</span><span class="s2">, </span><span class="s1">)</span>

        <span class="s5"># several different types of neurons</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">k = len(self.n_neurons)</span>

            <span class="s5"># fix default values</span>
            <span class="s1">ufuncs = self.ufunc</span>
            <span class="s2">if </span><span class="s1">isinstance(ufuncs</span><span class="s2">, </span><span class="s1">str) </span><span class="s2">or not </span><span class="s1">hasattr(ufuncs</span><span class="s2">, </span><span class="s3">&quot;__iter__&quot;</span><span class="s1">):</span>
                <span class="s1">ufuncs = [ufuncs] * k</span>

            <span class="s1">densities = self.density</span>
            <span class="s2">if </span><span class="s1">densities </span><span class="s2">is None or not </span><span class="s1">hasattr(densities</span><span class="s2">, </span><span class="s3">&quot;__iter__&quot;</span><span class="s1">):</span>
                <span class="s1">densities = [densities] * k</span>

            <span class="s1">pw_metrics = self.pairwise_metric</span>
            <span class="s2">if </span><span class="s1">pw_metrics </span><span class="s2">is None or </span><span class="s1">isinstance(pw_metrics</span><span class="s2">, </span><span class="s1">str):</span>
                <span class="s1">pw_metrics = [pw_metrics] * k</span>

            <span class="s2">if not </span><span class="s1">k == len(ufuncs) == len(densities) == len(pw_metrics):</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Inconsistent parameter lengths for model with {} different types of neurons.</span><span class="s2">\n</span><span class="s3">&quot;</span>
                                 <span class="s3">&quot;Set 'ufunc', 'density' and 'pairwise_distances' by lists &quot;</span>
                                 <span class="s3">&quot;with {} elements, or leave the default values.&quot;</span><span class="s1">.format(k</span><span class="s2">, </span><span class="s1">k))</span>

            <span class="s1">self.hidden_layers_ = []</span>
            <span class="s2">for </span><span class="s1">n_neurons</span><span class="s2">, </span><span class="s1">ufunc</span><span class="s2">, </span><span class="s1">density</span><span class="s2">, </span><span class="s1">metric </span><span class="s2">in </span><span class="s1">zip(self.n_neurons</span><span class="s2">, </span><span class="s1">ufuncs</span><span class="s2">, </span><span class="s1">densities</span><span class="s2">, </span><span class="s1">pw_metrics):</span>
                <span class="s1">hl = HiddenLayer(n_neurons=n_neurons</span><span class="s2">, </span><span class="s1">density=density</span><span class="s2">, </span><span class="s1">ufunc=ufunc</span><span class="s2">,</span>
                                 <span class="s1">pairwise_metric=metric</span><span class="s2">, </span><span class="s1">random_state=self.random_state)</span>
                <span class="s1">hl.fit(X)</span>
                <span class="s1">self.hidden_layers_.append(hl)</span>

    <span class="s2">def </span><span class="s1">_reset(self):</span>
        <span class="s1">[delattr(self</span><span class="s2">, </span><span class="s1">attr) </span><span class="s2">for </span><span class="s1">attr </span><span class="s2">in </span><span class="s1">(</span><span class="s3">'n_features_'</span><span class="s2">, </span><span class="s3">'solver_'</span><span class="s2">, </span><span class="s3">'hidden_layers_'</span><span class="s2">, </span><span class="s3">'is_fitted_'</span><span class="s2">, </span><span class="s3">'label_binarizer_'</span><span class="s1">) </span><span class="s2">if </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s1">attr)]</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">n_neurons_(self):</span>
        <span class="s2">if not </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s3">'hidden_layers_'</span><span class="s1">):</span>
            <span class="s2">return None</span>

        <span class="s1">neurons_count = sum([hl.n_neurons_ </span><span class="s2">for </span><span class="s1">hl </span><span class="s2">in </span><span class="s1">self.hidden_layers_])</span>
        <span class="s2">if </span><span class="s1">self.include_original_features:</span>
            <span class="s1">neurons_count += self.n_features_</span>

        <span class="s2">return </span><span class="s1">neurons_count</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">coef_(self):</span>
        <span class="s2">return </span><span class="s1">self.solver_.coef_</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">intercept_(self):</span>
        <span class="s2">return </span><span class="s1">self.solver_.intercept_</span>

    <span class="s2">def </span><span class="s1">partial_fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None, </span><span class="s1">forget=</span><span class="s2">False, </span><span class="s1">compute_output_weights=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Update model with a new batch of data. 
 
        |method_partial_fit| 
 
        .. |method_partial_fit| replace:: Output weight computation can be temporary turned off 
            for faster processing. This will mark model as not fit. Enable `compute_output_weights` 
            in the final call to `partial_fit`. 
 
        .. |param_forget| replace:: Performs a negative update, effectively removing the information 
            given by training samples from the model. Output weights need to be re-computed after forgetting 
            data. Forgetting data that have not been learned before leads to unpredictable results. 
 
        .. |param_compute_output_weights| replace::  Whether to compute new output weights 
            (coef_, intercept_). Disable this in intermediate `partial_fit` 
            steps to run computations faster, then enable in the last call to compute the new solution. 
 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix}, shape=[n_samples, n_features] 
            Training input samples 
 
        y : array-like, shape=[n_samples, n_targets] 
            Training targets 
 
        forget : boolean, default False 
            |param_forget| 
 
        compute_output_weights : boolean, optional, default True 
            |param_compute_output_weights| 
 
            .. Note:: 
                Solution can be updated without extra data by setting `X=None` and `y=None`. 
 
            Example: 
                &gt;&gt;&gt; model.partial_fit(X_1, y_1) 
                ... model.partial_fit(X_2, y_2) 
                ... model.partial_fit(X_3, y_3)    # doctest: +SKIP 
 
            Faster, option 1: 
                &gt;&gt;&gt; model.partial_fit(X_1, y_1, compute_output_weights=False) 
                ... model.partial_fit(X_2, y_2, compute_output_weights=False) 
                ... model.partial_fit(X_3, y_3)    # doctest: +SKIP 
 
            Faster, option 2: 
                &gt;&gt;&gt; model.partial_fit(X_1, y_1, compute_output_weights=False) 
                ... model.partial_fit(X_2, y_2, compute_output_weights=False) 
                ... model.partial_fit(X_3, y_3, compute_output_weights=False) 
                ... model.partial_fit(X=None, y=None)    # doctest: +SKIP 
        &quot;&quot;&quot;</span>
        <span class="s5"># compute output weights only</span>
        <span class="s2">if </span><span class="s1">X </span><span class="s2">is None and </span><span class="s1">y </span><span class="s2">is None and </span><span class="s1">compute_output_weights:</span>
            <span class="s1">self.solver_.partial_fit(</span><span class="s2">None, None, </span><span class="s1">compute_output_weights=</span><span class="s2">True</span><span class="s1">)</span>
            <span class="s1">self.is_fitted_ = </span><span class="s2">True</span>
            <span class="s2">return </span><span class="s1">self</span>

        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = check_X_y(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">accept_sparse=</span><span class="s2">True, </span><span class="s1">multi_output=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">len(y.shape) &gt; </span><span class="s4">1 </span><span class="s2">and </span><span class="s1">y.shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">msg = (</span><span class="s3">&quot;A column-vector y was passed when a 1d array was expected. &quot;</span>
                   <span class="s3">&quot;Please change the shape of y to (n_samples, ), for example using ravel().&quot;</span><span class="s1">)</span>
            <span class="s1">warnings.warn(msg</span><span class="s2">, </span><span class="s1">DataConversionWarning)</span>

        <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X.shape</span>
        <span class="s2">if </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s3">'n_features_'</span><span class="s1">) </span><span class="s2">and </span><span class="s1">self.n_features_ != n_features:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">'Shape of input is different from what was seen in `fit`'</span><span class="s1">)</span>

        <span class="s5"># set batch size, default is bsize=2000 or all-at-once with less than 10_000 samples</span>
        <span class="s1">self.bsize_ = self.batch_size</span>
        <span class="s2">if </span><span class="s1">self.bsize_ </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">self.bsize_ = n_samples </span><span class="s2">if </span><span class="s1">n_samples &lt; </span><span class="s4">10 </span><span class="s1">* </span><span class="s4">1000 </span><span class="s2">else </span><span class="s4">2000</span>

        <span class="s5"># init model if not fit yet</span>
        <span class="s2">if not </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s3">'hidden_layers_'</span><span class="s1">):</span>
            <span class="s1">self.n_features_ = n_features</span>
            <span class="s1">self.solver_ = BatchCholeskySolver(alpha=self.alpha)</span>
            <span class="s1">self._init_hidden_layers(X)</span>

        <span class="s5"># special case of one-shot processing</span>
        <span class="s2">if </span><span class="s1">self.bsize_ &gt;= n_samples:</span>
            <span class="s1">H = [hl.transform(X) </span><span class="s2">for </span><span class="s1">hl </span><span class="s2">in </span><span class="s1">self.hidden_layers_]</span>
            <span class="s1">H = np.hstack(H </span><span class="s2">if not </span><span class="s1">self.include_original_features </span><span class="s2">else </span><span class="s1">[_dense(X)] + H)</span>
            <span class="s1">self.solver_.partial_fit(H</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">forget=forget</span><span class="s2">, </span><span class="s1">compute_output_weights=</span><span class="s2">False</span><span class="s1">)</span>

        <span class="s2">else</span><span class="s1">:  </span><span class="s5"># batch processing</span>
            <span class="s2">for </span><span class="s1">b_start </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">self.bsize_):</span>
                <span class="s1">b_end = min(b_start + self.bsize_</span><span class="s2">, </span><span class="s1">n_samples)</span>
                <span class="s1">b_X = X[b_start:b_end]</span>
                <span class="s1">b_y = y[b_start:b_end]</span>

                <span class="s1">b_H = [hl.transform(b_X) </span><span class="s2">for </span><span class="s1">hl </span><span class="s2">in </span><span class="s1">self.hidden_layers_]</span>
                <span class="s1">b_H = np.hstack(b_H </span><span class="s2">if not </span><span class="s1">self.include_original_features </span><span class="s2">else </span><span class="s1">[_dense(b_X)] + b_H)</span>
                <span class="s1">self.solver_.partial_fit(b_H</span><span class="s2">, </span><span class="s1">b_y</span><span class="s2">, </span><span class="s1">forget=forget</span><span class="s2">, </span><span class="s1">compute_output_weights=</span><span class="s2">False</span><span class="s1">)</span>

        <span class="s5"># output weights if needed</span>
        <span class="s2">if </span><span class="s1">compute_output_weights:</span>
            <span class="s1">self.solver_.partial_fit(</span><span class="s2">None, None, </span><span class="s1">compute_output_weights=</span><span class="s2">True</span><span class="s1">)</span>
            <span class="s1">self.is_fitted_ = </span><span class="s2">True</span>

        <span class="s5"># mark as needing a solution</span>
        <span class="s2">elif </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s3">'is_fitted_'</span><span class="s1">):</span>
            <span class="s2">del </span><span class="s1">self.is_fitted_</span>

        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Reset model and fit on the given data. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix}, shape (n_samples, n_features) 
            Training data samples. 
 
        y : array-like, shape (n_samples,) or (n_samples, n_outputs) 
            Target values used as real numbers. 
 
        Returns 
        ------- 
        self : object 
            Returns self. 
        &quot;&quot;&quot;</span>

        <span class="s5">#todo: add X as bunch of files support</span>

        <span class="s1">self._reset()</span>
        <span class="s1">self.partial_fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict real valued outputs for new inputs X. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix}, shape (n_samples, n_features) 
            Input data samples. 
 
        Returns 
        ------- 
        y : ndarray, shape (n_samples,) or (n_samples, n_outputs) 
            Predicted outputs for inputs X. 
 
            .. attention:: 
 
                :mod:`predict` always returns a dense matrix of predicted outputs -- unlike 
                in :meth:`fit`, this may cause memory issues at high number of outputs 
                and very high number of samples. Feed data by smaller batches in such case. 
        &quot;&quot;&quot;</span>

        <span class="s1">X = check_array(X</span><span class="s2">, </span><span class="s1">accept_sparse=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">check_is_fitted(self</span><span class="s2">, </span><span class="s3">&quot;is_fitted_&quot;</span><span class="s1">)</span>

        <span class="s1">H = [hl.transform(X) </span><span class="s2">for </span><span class="s1">hl </span><span class="s2">in </span><span class="s1">self.hidden_layers_]</span>
        <span class="s2">if </span><span class="s1">self.include_original_features:</span>
            <span class="s1">H = [_dense(X)] + H</span>
        <span class="s1">H = np.hstack(H)</span>

        <span class="s2">return </span><span class="s1">self.solver_.predict(H)</span>


<span class="s2">class </span><span class="s1">ELMRegressor(_BaseELM</span><span class="s2">, </span><span class="s1">RegressorMixin):</span>
    <span class="s0">&quot;&quot;&quot;Extreme Learning Machine for regression problems. 
 
    This model solves a regression problem, that is a problem of predicting continuous outputs. 
    It supports multi-variate regression (when ``y`` is a 2d array of shape [n_samples, n_targets].) 
    ELM uses ``L2`` regularization, and optionally includes the original data features to 
    capture linear dependencies in the data natively. 
 
    Parameters 
    ---------- 
    alpha : float 
        Regularization strength; must be a positive float. Larger values specify stronger effect. 
        Regularization improves model stability and reduces over-fitting at the cost of some learning 
        capacity. The same value is used for all targets in multi-variate regression. 
 
        The optimal regularization strength is suggested to select from a large range of logarithmically 
        distributed values, e.g. :math:`[10^{-5}, 10^{-4}, 10^{-3}, ..., 10^4, 10^5]`. A small default 
        regularization value of :math:`10^{-7}` should always be present to counter numerical instabilities 
        in the solution; it does not affect overall model performance. 
 
        .. attention:: 
            The model may automatically increase the regularization value if the solution 
            becomes unfeasible otherwise. The actual used value contains in ``alpha_`` attribute. 
 
    batch_size : int, optional 
        Actual computations will proceed in batches of this size, except the last batch that may be smaller. 
        Default behavior is to process all data at once with &lt;10,000 samples, otherwise use batches 
        of size 2000. 
 
    include_original_features : boolean, default=False 
        Adds extra hidden layer neurons that simpy copy the input data features, adding a linear part 
        to the final model solution that can directly capture linear relations between data and 
        outputs. Effectively increases `n_neurons` by `n_inputs` leading to a larger model. 
        Including original features is generally a good thing if the number of data features is low. 
 
    n_neurons : int or [int], optional 
        Number of hidden layer neurons in ELM model, controls model size and learning capacity. 
        Generally number of neurons should be less than the number of training data samples, as 
        otherwise the model will learn the training set perfectly resulting in overfitting. 
 
        Several different kinds of neurons can be used in the same model by specifying a list of 
        neuron counts. ELM will create a separate neuron type for each element in the list. 
        In that case, the following attributes ``ufunc``, ``density`` and ``pairwise_metric`` 
        should be lists of the same length; default values will be automatically expanded into a list. 
 
        .. note:: 
            Models with &lt;1,000 neurons are very fast to compute, while GPU acceleration is efficient 
            starting from 1,000-2,000 neurons. A standard computer should handle up to 10,000 neurons. 
            Very large models will not fit in memory but can still be trained by an out-of-core solver. 
 
    ufunc : {'tanh', 'sigm', 'relu', 'lin' or callable}, or a list of those (see n_neurons) 
        Transformation function of hidden layer neurons. Includes the following options: 
            - 'tanh' for hyperbolic tangent 
            - 'sigm' for sigmoid 
            - 'relu' for rectified linear unit (clamps negative values to zero) 
            - 'lin' for linear neurons, transformation function does nothing 
            - any custom callable function like members of ``Numpu.ufunc`` 
 
    density : float in range (0, 1], or a list of those (see n_neurons), optional 
        Specifying density replaces dense projection layer by a sparse one with the specified 
        density of the connections. For instance, ``density=0.1`` means each hidden neuron will 
        be connected to a random 10% of input features. Useful for working on very high-dimensional 
        data, or for large numbers of neurons. 
 
    pairwise_metric : {'euclidean', 'cityblock', 'cosine' or other}, or a list of those (see n_neurons), optional 
        Specifying pairwise metric replaces multiplicative hidden neurons by distance-based hidden 
        neurons. This ELM model is known as Radial Basis Function ELM (RBF-ELM). 
 
        .. note:: 
            Pairwise function neurons ignore ufunc and density. 
 
        Typical metrics are `euclidean`, `cityblock` and `cosine`. For a full list of metrics check 
        the `webpage &lt;https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html&gt;`_ 
        of :mod:`sklearn.metrics.pairwise_distances`. 
 
    random_state : int, RandomState instance or None, optional, default None 
        The seed of the pseudo random number generator to use when generating random numbers e.g. 
        for hidden neuron parameters. Random state instance is passed to lower level objects and routines. 
        Use it for repeatable experiments. 
 
    Attributes 
    ---------- 
    n_neurons_ : int 
        Number of automatically generated neurons. 
 
    ufunc_ : function 
        Tranformation function of hidden neurons. 
 
    projection_ : object 
        Hidden layer projection function. 
 
    solver_ : object 
        Solver instance, read solution from there. 
 
 
    Examples 
    -------- 
 
    Combining ten sigmoid and twenty RBF neurons in one model: 
 
    &gt;&gt;&gt; model = ELMRegressor(n_neurons=(10, 20), 
    ...                      ufunc=('sigm', None), 
    ...                      density=(None, None), 
    ...                      pairwise_metric=(None, 'euclidean'))   # doctest: +SKIP 
 
    Default values in multi-neuron ELM are automatically expanded to a list 
 
    &gt;&gt;&gt;  model = ELMRegressor(n_neurons=(10, 20), 
    ...                       ufunc=('sigm', None), 
    ...                       pairwise_metric=(None, 'euclidean'))   # doctest: +SKIP 
 
    &gt;&gt;&gt;  model = ELMRegressor(n_neurons=(30, 30), 
    ...                       pairwise_metric=('cityblock', 'cosine'))   # doctest: +SKIP 
    &quot;&quot;&quot;</span>
    <span class="s2">pass</span>

<span class="s2">class </span><span class="s1">ELMClassifier(_BaseELM</span><span class="s2">, </span><span class="s1">ClassifierMixin):</span>
    <span class="s0">&quot;&quot;&quot;ELM classifier, modified for multi-label classification support. 
 
    :param classes: Set of classes to consider in the model; can be expanded at runtime. 
                    Samples of other classes will have their output set to zero. 
    :param solver: Solver to use, &quot;default&quot; for build-in Least Squares or &quot;ridge&quot; for Ridge regression 
 
 
    Example descr... 
 
    Attributes 
    ---------- 
    X_ : ndarray, shape (n_samples, n_features) 
        The input passed during :meth:`fit`. 
    y_ : ndarray, shape (n_samples,) 
        The labels passed during :meth:`fit`. 
    classes_ : ndarray, shape (n_classes,) 
        The classes seen at :meth:`fit`. 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">classes=</span><span class="s2">None, </span><span class="s1">alpha=</span><span class="s4">1e-7</span><span class="s2">, </span><span class="s1">batch_size=</span><span class="s2">None, </span><span class="s1">include_original_features=</span><span class="s2">False, </span><span class="s1">n_neurons=</span><span class="s2">None,</span>
                 <span class="s1">ufunc=</span><span class="s3">&quot;tanh&quot;</span><span class="s2">, </span><span class="s1">density=</span><span class="s2">None, </span><span class="s1">pairwise_metric=</span><span class="s2">None, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(alpha</span><span class="s2">, </span><span class="s1">batch_size</span><span class="s2">, </span><span class="s1">include_original_features</span><span class="s2">, </span><span class="s1">n_neurons</span><span class="s2">, </span><span class="s1">ufunc</span><span class="s2">, </span><span class="s1">density</span><span class="s2">, </span><span class="s1">pairwise_metric</span><span class="s2">,</span>
                         <span class="s1">random_state)</span>
        <span class="s1">self.classes = classes</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">classes_(self):</span>
        <span class="s2">return </span><span class="s1">self.label_binarizer_.classes_</span>

    <span class="s2">def </span><span class="s1">_get_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s3">&quot;multioutput&quot;</span><span class="s1">: </span><span class="s2">True, </span><span class="s3">&quot;multilabel&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>

    <span class="s2">def </span><span class="s1">_update_classes(self</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">if not </span><span class="s1">isinstance(self.solver_</span><span class="s2">, </span><span class="s1">BatchCholeskySolver):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Only iterative solver supports dynamic class update&quot;</span><span class="s1">)</span>

        <span class="s1">old_classes = self.label_binarizer_.classes_</span>
        <span class="s1">partial_classes = clone(self.label_binarizer_).fit(y).classes_</span>

        <span class="s5"># no new classes detected</span>
        <span class="s2">if </span><span class="s1">set(partial_classes) &lt;= set(old_classes):</span>
            <span class="s2">return</span>

        <span class="s2">if </span><span class="s1">len(old_classes) &lt; </span><span class="s4">3</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Dynamic class update has to start with at least 3 classes to function correctly; &quot;</span>
                             <span class="s3">&quot;provide 3 or more 'classes=[...]' during initialization.&quot;</span><span class="s1">)</span>

        <span class="s5"># get new classes sorted by LabelBinarizer</span>
        <span class="s1">self.label_binarizer_.fit(np.hstack((old_classes</span><span class="s2">, </span><span class="s1">partial_classes)))</span>
        <span class="s1">new_classes = self.label_binarizer_.classes_</span>

        <span class="s5"># convert existing XtY matrix to new classes</span>
        <span class="s2">if </span><span class="s1">hasattr(self.solver_</span><span class="s2">, </span><span class="s3">'XtY_'</span><span class="s1">):</span>
            <span class="s1">XtY_old = self.solver_.XtY_</span>
            <span class="s1">XtY_new = np.zeros((XtY_old.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">new_classes.shape[</span><span class="s4">0</span><span class="s1">]))</span>
            <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">c </span><span class="s2">in </span><span class="s1">enumerate(old_classes):</span>
                <span class="s1">j = np.where(new_classes == c)[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span>
                <span class="s1">XtY_new[:</span><span class="s2">, </span><span class="s1">j] = XtY_old[:</span><span class="s2">, </span><span class="s1">i]</span>
            <span class="s1">self.solver_.XtY_ = XtY_new</span>

        <span class="s5"># reset the solution</span>
        <span class="s2">if </span><span class="s1">hasattr(self.solver_</span><span class="s2">, </span><span class="s3">'is_fitted_'</span><span class="s1">):</span>
            <span class="s2">del </span><span class="s1">self.solver_.is_fitted_</span>

    <span class="s2">def </span><span class="s1">partial_fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None, </span><span class="s1">forget=</span><span class="s2">False, </span><span class="s1">update_classes=</span><span class="s2">False, </span><span class="s1">compute_output_weights=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Update classifier with a new batch of data. 
 
        |method_partial_fit| 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix}, shape=[n_samples, n_features] 
            Training input samples 
 
        y : array-like, shape=[n_samples, n_targets] 
            Training targets 
 
        forget : boolean, default False 
            |param_forget| 
 
        update_classes : boolean, default False 
            Include new classes from `y` into the model, assuming they were 0 in all previous samples. 
 
        compute_output_weights : boolean, optional, default True 
            |param_compute_output_weights| 
        &quot;&quot;&quot;</span>

        <span class="s5">#todo: Warning on strongly non-normalized data</span>

        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = check_X_y(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">accept_sparse=</span><span class="s2">True, </span><span class="s1">multi_output=</span><span class="s2">True</span><span class="s1">)</span>

        <span class="s5"># init label binarizer if needed</span>
        <span class="s2">if not </span><span class="s1">hasattr(self</span><span class="s2">, </span><span class="s3">'label_binarizer_'</span><span class="s1">):</span>
            <span class="s1">self.label_binarizer_ = LabelBinarizer()</span>
            <span class="s2">if </span><span class="s1">type_of_target(y).endswith(</span><span class="s3">&quot;-multioutput&quot;</span><span class="s1">):</span>
                <span class="s1">self.label_binarizer_ = MultiLabelBinarizer()</span>
            <span class="s1">self.label_binarizer_.fit(self.classes </span><span class="s2">if </span><span class="s1">self.classes </span><span class="s2">is not None else </span><span class="s1">y)</span>

        <span class="s2">if </span><span class="s1">update_classes:</span>
            <span class="s1">self._update_classes(y)</span>

        <span class="s1">y_numeric = self.label_binarizer_.transform(y)</span>
        <span class="s2">if </span><span class="s1">len(y_numeric.shape) &gt; </span><span class="s4">1 </span><span class="s2">and </span><span class="s1">y_numeric.shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">y_numeric = y_numeric[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>

        <span class="s1">super().partial_fit(X</span><span class="s2">, </span><span class="s1">y_numeric</span><span class="s2">, </span><span class="s1">forget=forget</span><span class="s2">, </span><span class="s1">compute_output_weights=compute_output_weights)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit a classifier erasing any previously trained model. 
 
        Returns 
        ------- 
        self : object 
            Returns self. 
        &quot;&quot;&quot;</span>
        <span class="s1">self._reset()</span>
        <span class="s1">self.partial_fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">compute_output_weights=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict classes of new inputs X. 
 
        Parameters 
        ---------- 
        X : array-like, shape (n_samples, n_features) 
            The input samples. 
 
        Returns 
        ------- 
        y : ndarray, shape (n_samples,) or (n_samples, n_outputs) 
            Returns one most probable class for multi-class problem, or 
            a binary vector of all relevant classes for multi-label problem. 
        &quot;&quot;&quot;</span>

        <span class="s1">check_is_fitted(self</span><span class="s2">, </span><span class="s3">&quot;is_fitted_&quot;</span><span class="s1">)</span>
        <span class="s1">scores = super().predict(X)</span>
        <span class="s2">return </span><span class="s1">self.label_binarizer_.inverse_transform(scores)</span>

    <span class="s2">def </span><span class="s1">predict_proba(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Probability estimation for all classes. 
 
        Positive class probabilities are computed as 
        1. / (1. + np.exp(-self.decision_function(X))); 
        multiclass is handled by normalizing that over all classes. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self</span><span class="s2">, </span><span class="s3">&quot;is_fitted_&quot;</span><span class="s1">)</span>
        <span class="s1">prob = super().predict(X)</span>
        <span class="s1">expit(prob</span><span class="s2">, </span><span class="s1">out=prob)</span>
        <span class="s2">if </span><span class="s1">prob.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">np.vstack([</span><span class="s4">1 </span><span class="s1">- prob</span><span class="s2">, </span><span class="s1">prob]).T</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s5"># OvR normalization, like LibLinear's predict_probability</span>
            <span class="s1">prob /= prob.sum(axis=</span><span class="s4">1</span><span class="s1">).reshape((prob.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">))</span>
            <span class="s2">return </span><span class="s1">prob</span></pre>
</body>
</html>