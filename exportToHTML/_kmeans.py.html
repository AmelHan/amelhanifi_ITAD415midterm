<html>
<head>
<title>_kmeans.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_kmeans.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;K-means clustering.&quot;&quot;&quot;</span>

<span class="s2"># Authors: Gael Varoquaux &lt;gael.varoquaux@normalesup.org&gt;</span>
<span class="s2">#          Thomas Rueckstiess &lt;ruecksti@in.tum.de&gt;</span>
<span class="s2">#          James Bergstra &lt;james.bergstra@umontreal.ca&gt;</span>
<span class="s2">#          Jan Schlueter &lt;scikit-learn@jan-schlueter.de&gt;</span>
<span class="s2">#          Nelle Varoquaux</span>
<span class="s2">#          Peter Prettenhofer &lt;peter.prettenhofer@gmail.com&gt;</span>
<span class="s2">#          Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span>
<span class="s2">#          Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span>
<span class="s2">#          Robert Layton &lt;robertlayton@gmail.com&gt;</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">abc </span><span class="s3">import </span><span class="s1">ABC</span><span class="s3">, </span><span class="s1">abstractmethod</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">scipy.sparse </span><span class="s3">as </span><span class="s1">sp</span>

<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">BaseEstimator</span><span class="s3">,</span>
    <span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s3">,</span>
    <span class="s1">ClusterMixin</span><span class="s3">,</span>
    <span class="s1">TransformerMixin</span><span class="s3">,</span>
    <span class="s1">_fit_context</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">..exceptions </span><span class="s3">import </span><span class="s1">ConvergenceWarning</span>
<span class="s3">from </span><span class="s1">..metrics.pairwise </span><span class="s3">import </span><span class="s1">_euclidean_distances</span><span class="s3">, </span><span class="s1">euclidean_distances</span>
<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">check_array</span><span class="s3">, </span><span class="s1">check_random_state</span>
<span class="s3">from </span><span class="s1">..utils._openmp_helpers </span><span class="s3">import </span><span class="s1">_openmp_effective_n_threads</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">Hidden</span><span class="s3">, </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">StrOptions</span><span class="s3">, </span><span class="s1">validate_params</span>
<span class="s3">from </span><span class="s1">..utils.extmath </span><span class="s3">import </span><span class="s1">row_norms</span><span class="s3">, </span><span class="s1">stable_cumsum</span>
<span class="s3">from </span><span class="s1">..utils.fixes </span><span class="s3">import </span><span class="s1">threadpool_info</span><span class="s3">, </span><span class="s1">threadpool_limits</span>
<span class="s3">from </span><span class="s1">..utils.sparsefuncs </span><span class="s3">import </span><span class="s1">mean_variance_axis</span>
<span class="s3">from </span><span class="s1">..utils.sparsefuncs_fast </span><span class="s3">import </span><span class="s1">assign_rows_csr</span>
<span class="s3">from </span><span class="s1">..utils.validation </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">_check_sample_weight</span><span class="s3">,</span>
    <span class="s1">_is_arraylike_not_scalar</span><span class="s3">,</span>
    <span class="s1">check_is_fitted</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">._k_means_common </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">CHUNK_SIZE</span><span class="s3">,</span>
    <span class="s1">_inertia_dense</span><span class="s3">,</span>
    <span class="s1">_inertia_sparse</span><span class="s3">,</span>
    <span class="s1">_is_same_clustering</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">._k_means_elkan </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">elkan_iter_chunked_dense</span><span class="s3">,</span>
    <span class="s1">elkan_iter_chunked_sparse</span><span class="s3">,</span>
    <span class="s1">init_bounds_dense</span><span class="s3">,</span>
    <span class="s1">init_bounds_sparse</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">._k_means_lloyd </span><span class="s3">import </span><span class="s1">lloyd_iter_chunked_dense</span><span class="s3">, </span><span class="s1">lloyd_iter_chunked_sparse</span>
<span class="s3">from </span><span class="s1">._k_means_minibatch </span><span class="s3">import </span><span class="s1">_minibatch_update_dense</span><span class="s3">, </span><span class="s1">_minibatch_update_sparse</span>

<span class="s2">###############################################################################</span>
<span class="s2"># Initialization heuristic</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s4">&quot;X&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, </span><span class="s4">&quot;sparse matrix&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_clusters&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;sample_weight&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;x_squared_norms&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_local_trials&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">True,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">kmeans_plusplus(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">n_clusters</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">sample_weight=</span><span class="s3">None,</span>
    <span class="s1">x_squared_norms=</span><span class="s3">None,</span>
    <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">n_local_trials=</span><span class="s3">None,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Init n_clusters seeds according to k-means++. 
 
    .. versionadded:: 0.24 
 
    Parameters 
    ---------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) 
        The data to pick seeds from. 
 
    n_clusters : int 
        The number of centroids to initialize. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        The weights for each observation in `X`. If `None`, all observations 
        are assigned equal weight. `sample_weight` is ignored if `init` 
        is a callable or a user provided array. 
 
        .. versionadded:: 1.3 
 
    x_squared_norms : array-like of shape (n_samples,), default=None 
        Squared Euclidean norm of each data point. 
 
    random_state : int or RandomState instance, default=None 
        Determines random number generation for centroid initialization. Pass 
        an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    n_local_trials : int, default=None 
        The number of seeding trials for each center (except the first), 
        of which the one reducing inertia the most is greedily chosen. 
        Set to None to make the number of trials depend logarithmically 
        on the number of seeds (2+log(k)) which is the recommended setting. 
        Setting to 1 disables the greedy cluster selection and recovers the 
        vanilla k-means++ algorithm which was empirically shown to work less 
        well than its greedy variant. 
 
    Returns 
    ------- 
    centers : ndarray of shape (n_clusters, n_features) 
        The initial centers for k-means. 
 
    indices : ndarray of shape (n_clusters,) 
        The index location of the chosen centers in the data array X. For a 
        given index and center, X[index] = center. 
 
    Notes 
    ----- 
    Selects initial cluster centers for k-mean clustering in a smart way 
    to speed up convergence. see: Arthur, D. and Vassilvitskii, S. 
    &quot;k-means++: the advantages of careful seeding&quot;. ACM-SIAM symposium 
    on Discrete algorithms. 2007 
 
    Examples 
    -------- 
 
    &gt;&gt;&gt; from sklearn.cluster import kmeans_plusplus 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; X = np.array([[1, 2], [1, 4], [1, 0], 
    ...               [10, 2], [10, 4], [10, 0]]) 
    &gt;&gt;&gt; centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0) 
    &gt;&gt;&gt; centers 
    array([[10,  2], 
           [ 1,  0]]) 
    &gt;&gt;&gt; indices 
    array([3, 2]) 
    &quot;&quot;&quot;</span>
    <span class="s2"># Check data</span>
    <span class="s1">check_array(X</span><span class="s3">, </span><span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32])</span>
    <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s3">if </span><span class="s1">X.shape[</span><span class="s5">0</span><span class="s1">] &lt; n_clusters:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">f&quot;n_samples=</span><span class="s3">{</span><span class="s1">X.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">} </span><span class="s4">should be &gt;= n_clusters=</span><span class="s3">{</span><span class="s1">n_clusters</span><span class="s3">}</span><span class="s4">.&quot;</span>
        <span class="s1">)</span>

    <span class="s2"># Check parameters</span>
    <span class="s3">if </span><span class="s1">x_squared_norms </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">x_squared_norms = row_norms(X</span><span class="s3">, </span><span class="s1">squared=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">x_squared_norms = check_array(x_squared_norms</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span><span class="s3">, </span><span class="s1">ensure_2d=</span><span class="s3">False</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">x_squared_norms.shape[</span><span class="s5">0</span><span class="s1">] != X.shape[</span><span class="s5">0</span><span class="s1">]:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">f&quot;The length of x_squared_norms </span><span class="s3">{</span><span class="s1">x_squared_norms.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">} </span><span class="s4">should &quot;</span>
            <span class="s4">f&quot;be equal to the length of n_samples </span><span class="s3">{</span><span class="s1">X.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">}</span><span class="s4">.&quot;</span>
        <span class="s1">)</span>

    <span class="s1">random_state = check_random_state(random_state)</span>

    <span class="s2"># Call private k-means++</span>
    <span class="s1">centers</span><span class="s3">, </span><span class="s1">indices = _kmeans_plusplus(</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">n_clusters</span><span class="s3">, </span><span class="s1">x_squared_norms</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">random_state</span><span class="s3">, </span><span class="s1">n_local_trials</span>
    <span class="s1">)</span>

    <span class="s3">return </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">indices</span>


<span class="s3">def </span><span class="s1">_kmeans_plusplus(</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">n_clusters</span><span class="s3">, </span><span class="s1">x_squared_norms</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">random_state</span><span class="s3">, </span><span class="s1">n_local_trials=</span><span class="s3">None</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Computational component for initialization of n_clusters by 
    k-means++. Prior validation of data is assumed. 
 
    Parameters 
    ---------- 
    X : {ndarray, sparse matrix} of shape (n_samples, n_features) 
        The data to pick seeds for. 
 
    n_clusters : int 
        The number of seeds to choose. 
 
    sample_weight : ndarray of shape (n_samples,) 
        The weights for each observation in `X`. 
 
    x_squared_norms : ndarray of shape (n_samples,) 
        Squared Euclidean norm of each data point. 
 
    random_state : RandomState instance 
        The generator used to initialize the centers. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    n_local_trials : int, default=None 
        The number of seeding trials for each center (except the first), 
        of which the one reducing inertia the most is greedily chosen. 
        Set to None to make the number of trials depend logarithmically 
        on the number of seeds (2+log(k)); this is the default. 
 
    Returns 
    ------- 
    centers : ndarray of shape (n_clusters, n_features) 
        The initial centers for k-means. 
 
    indices : ndarray of shape (n_clusters,) 
        The index location of the chosen centers in the data array X. For a 
        given index and center, X[index] = center. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>

    <span class="s1">centers = np.empty((n_clusters</span><span class="s3">, </span><span class="s1">n_features)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s2"># Set the number of local seeding trials if none is given</span>
    <span class="s3">if </span><span class="s1">n_local_trials </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s2"># This is what Arthur/Vassilvitskii tried, but did not report</span>
        <span class="s2"># specific results for other than mentioning in the conclusion</span>
        <span class="s2"># that it helped.</span>
        <span class="s1">n_local_trials = </span><span class="s5">2 </span><span class="s1">+ int(np.log(n_clusters))</span>

    <span class="s2"># Pick first center randomly and track index of point</span>
    <span class="s1">center_id = random_state.choice(n_samples</span><span class="s3">, </span><span class="s1">p=sample_weight / sample_weight.sum())</span>
    <span class="s1">indices = np.full(n_clusters</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">dtype=int)</span>
    <span class="s3">if </span><span class="s1">sp.issparse(X):</span>
        <span class="s1">centers[</span><span class="s5">0</span><span class="s1">] = X[center_id].toarray()</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">centers[</span><span class="s5">0</span><span class="s1">] = X[center_id]</span>
    <span class="s1">indices[</span><span class="s5">0</span><span class="s1">] = center_id</span>

    <span class="s2"># Initialize list of closest distances and calculate current potential</span>
    <span class="s1">closest_dist_sq = _euclidean_distances(</span>
        <span class="s1">centers[</span><span class="s5">0</span><span class="s3">, </span><span class="s1">np.newaxis]</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">Y_norm_squared=x_squared_norms</span><span class="s3">, </span><span class="s1">squared=</span><span class="s3">True</span>
    <span class="s1">)</span>
    <span class="s1">current_pot = closest_dist_sq @ sample_weight</span>

    <span class="s2"># Pick the remaining n_clusters-1 points</span>
    <span class="s3">for </span><span class="s1">c </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">n_clusters):</span>
        <span class="s2"># Choose center candidates by sampling with probability proportional</span>
        <span class="s2"># to the squared distance to the closest existing center</span>
        <span class="s1">rand_vals = random_state.uniform(size=n_local_trials) * current_pot</span>
        <span class="s1">candidate_ids = np.searchsorted(</span>
            <span class="s1">stable_cumsum(sample_weight * closest_dist_sq)</span><span class="s3">, </span><span class="s1">rand_vals</span>
        <span class="s1">)</span>
        <span class="s2"># XXX: numerical imprecision can result in a candidate_id out of range</span>
        <span class="s1">np.clip(candidate_ids</span><span class="s3">, None, </span><span class="s1">closest_dist_sq.size - </span><span class="s5">1</span><span class="s3">, </span><span class="s1">out=candidate_ids)</span>

        <span class="s2"># Compute distances to center candidates</span>
        <span class="s1">distance_to_candidates = _euclidean_distances(</span>
            <span class="s1">X[candidate_ids]</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">Y_norm_squared=x_squared_norms</span><span class="s3">, </span><span class="s1">squared=</span><span class="s3">True</span>
        <span class="s1">)</span>

        <span class="s2"># update closest distances squared and potential for each candidate</span>
        <span class="s1">np.minimum(closest_dist_sq</span><span class="s3">, </span><span class="s1">distance_to_candidates</span><span class="s3">, </span><span class="s1">out=distance_to_candidates)</span>
        <span class="s1">candidates_pot = distance_to_candidates @ sample_weight.reshape(-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>

        <span class="s2"># Decide which candidate is the best</span>
        <span class="s1">best_candidate = np.argmin(candidates_pot)</span>
        <span class="s1">current_pot = candidates_pot[best_candidate]</span>
        <span class="s1">closest_dist_sq = distance_to_candidates[best_candidate]</span>
        <span class="s1">best_candidate = candidate_ids[best_candidate]</span>

        <span class="s2"># Permanently add best center candidate found in local tries</span>
        <span class="s3">if </span><span class="s1">sp.issparse(X):</span>
            <span class="s1">centers[c] = X[best_candidate].toarray()</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">centers[c] = X[best_candidate]</span>
        <span class="s1">indices[c] = best_candidate</span>

    <span class="s3">return </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">indices</span>


<span class="s2">###############################################################################</span>
<span class="s2"># K-means batch estimation by EM (expectation maximization)</span>


<span class="s3">def </span><span class="s1">_tolerance(X</span><span class="s3">, </span><span class="s1">tol):</span>
    <span class="s0">&quot;&quot;&quot;Return a tolerance which is dependent on the dataset.&quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">tol == </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s5">0</span>
    <span class="s3">if </span><span class="s1">sp.issparse(X):</span>
        <span class="s1">variances = mean_variance_axis(X</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)[</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">variances = np.var(X</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s3">return </span><span class="s1">np.mean(variances) * tol</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s4">&quot;X&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, </span><span class="s4">&quot;sparse matrix&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;sample_weight&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;return_n_iter&quot;</span><span class="s1">: [bool]</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">False,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">k_means(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">n_clusters</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">sample_weight=</span><span class="s3">None,</span>
    <span class="s1">init=</span><span class="s4">&quot;k-means++&quot;</span><span class="s3">,</span>
    <span class="s1">n_init=</span><span class="s4">&quot;warn&quot;</span><span class="s3">,</span>
    <span class="s1">max_iter=</span><span class="s5">300</span><span class="s3">,</span>
    <span class="s1">verbose=</span><span class="s3">False,</span>
    <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
    <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">copy_x=</span><span class="s3">True,</span>
    <span class="s1">algorithm=</span><span class="s4">&quot;lloyd&quot;</span><span class="s3">,</span>
    <span class="s1">return_n_iter=</span><span class="s3">False,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Perform K-means clustering algorithm. 
 
    Read more in the :ref:`User Guide &lt;k_means&gt;`. 
 
    Parameters 
    ---------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) 
        The observations to cluster. It must be noted that the data 
        will be converted to C ordering, which will cause a memory copy 
        if the given data is not C-contiguous. 
 
    n_clusters : int 
        The number of clusters to form as well as the number of 
        centroids to generate. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        The weights for each observation in `X`. If `None`, all observations 
        are assigned equal weight. `sample_weight` is not used during 
        initialization if `init` is a callable or a user provided array. 
 
    init : {'k-means++', 'random'}, callable or array-like of shape \ 
            (n_clusters, n_features), default='k-means++' 
        Method for initialization: 
 
        - `'k-means++'` : selects initial cluster centers for k-mean 
          clustering in a smart way to speed up convergence. See section 
          Notes in k_init for more details. 
        - `'random'`: choose `n_clusters` observations (rows) at random from data 
          for the initial centroids. 
        - If an array is passed, it should be of shape `(n_clusters, n_features)` 
          and gives the initial centers. 
        - If a callable is passed, it should take arguments `X`, `n_clusters` and a 
          random state and return an initialization. 
 
    n_init : 'auto' or int, default=10 
        Number of time the k-means algorithm will be run with different 
        centroid seeds. The final results will be the best output of 
        n_init consecutive runs in terms of inertia. 
 
        When `n_init='auto'`, the number of runs depends on the value of init: 
        10 if using `init='random'` or `init` is a callable; 
        1 if using `init='k-means++'` or `init` is an array-like. 
 
        .. versionadded:: 1.2 
           Added 'auto' option for `n_init`. 
 
        .. versionchanged:: 1.4 
           Default value for `n_init` will change from 10 to `'auto'` in version 1.4. 
 
    max_iter : int, default=300 
        Maximum number of iterations of the k-means algorithm to run. 
 
    verbose : bool, default=False 
        Verbosity mode. 
 
    tol : float, default=1e-4 
        Relative tolerance with regards to Frobenius norm of the difference 
        in the cluster centers of two consecutive iterations to declare 
        convergence. 
 
    random_state : int, RandomState instance or None, default=None 
        Determines random number generation for centroid initialization. Use 
        an int to make the randomness deterministic. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    copy_x : bool, default=True 
        When pre-computing distances it is more numerically accurate to center 
        the data first. If `copy_x` is True (default), then the original data is 
        not modified. If False, the original data is modified, and put back 
        before the function returns, but small numerical differences may be 
        introduced by subtracting and then adding the data mean. Note that if 
        the original data is not C-contiguous, a copy will be made even if 
        `copy_x` is False. If the original data is sparse, but not in CSR format, 
        a copy will be made even if `copy_x` is False. 
 
    algorithm : {&quot;lloyd&quot;, &quot;elkan&quot;, &quot;auto&quot;, &quot;full&quot;}, default=&quot;lloyd&quot; 
        K-means algorithm to use. The classical EM-style algorithm is `&quot;lloyd&quot;`. 
        The `&quot;elkan&quot;` variation can be more efficient on some datasets with 
        well-defined clusters, by using the triangle inequality. However it's 
        more memory intensive due to the allocation of an extra array of shape 
        `(n_samples, n_clusters)`. 
 
        `&quot;auto&quot;` and `&quot;full&quot;` are deprecated and they will be removed in 
        Scikit-Learn 1.3. They are both aliases for `&quot;lloyd&quot;`. 
 
        .. versionchanged:: 0.18 
            Added Elkan algorithm 
 
        .. versionchanged:: 1.1 
            Renamed &quot;full&quot; to &quot;lloyd&quot;, and deprecated &quot;auto&quot; and &quot;full&quot;. 
            Changed &quot;auto&quot; to use &quot;lloyd&quot; instead of &quot;elkan&quot;. 
 
    return_n_iter : bool, default=False 
        Whether or not to return the number of iterations. 
 
    Returns 
    ------- 
    centroid : ndarray of shape (n_clusters, n_features) 
        Centroids found at the last iteration of k-means. 
 
    label : ndarray of shape (n_samples,) 
        The `label[i]` is the code or index of the centroid the 
        i'th observation is closest to. 
 
    inertia : float 
        The final value of the inertia criterion (sum of squared distances to 
        the closest centroid for all observations in the training set). 
 
    best_n_iter : int 
        Number of iterations corresponding to the best results. 
        Returned only if `return_n_iter` is set to True. 
    &quot;&quot;&quot;</span>
    <span class="s1">est = KMeans(</span>
        <span class="s1">n_clusters=n_clusters</span><span class="s3">,</span>
        <span class="s1">init=init</span><span class="s3">,</span>
        <span class="s1">n_init=n_init</span><span class="s3">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
        <span class="s1">verbose=verbose</span><span class="s3">,</span>
        <span class="s1">tol=tol</span><span class="s3">,</span>
        <span class="s1">random_state=random_state</span><span class="s3">,</span>
        <span class="s1">copy_x=copy_x</span><span class="s3">,</span>
        <span class="s1">algorithm=algorithm</span><span class="s3">,</span>
    <span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s3">if </span><span class="s1">return_n_iter:</span>
        <span class="s3">return </span><span class="s1">est.cluster_centers_</span><span class="s3">, </span><span class="s1">est.labels_</span><span class="s3">, </span><span class="s1">est.inertia_</span><span class="s3">, </span><span class="s1">est.n_iter_</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">est.cluster_centers_</span><span class="s3">, </span><span class="s1">est.labels_</span><span class="s3">, </span><span class="s1">est.inertia_</span>


<span class="s3">def </span><span class="s1">_kmeans_single_elkan(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">sample_weight</span><span class="s3">,</span>
    <span class="s1">centers_init</span><span class="s3">,</span>
    <span class="s1">max_iter=</span><span class="s5">300</span><span class="s3">,</span>
    <span class="s1">verbose=</span><span class="s3">False,</span>
    <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
    <span class="s1">n_threads=</span><span class="s5">1</span><span class="s3">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;A single run of k-means elkan, assumes preparation completed prior. 
 
    Parameters 
    ---------- 
    X : {ndarray, sparse matrix} of shape (n_samples, n_features) 
        The observations to cluster. If sparse matrix, must be in CSR format. 
 
    sample_weight : array-like of shape (n_samples,) 
        The weights for each observation in X. 
 
    centers_init : ndarray of shape (n_clusters, n_features) 
        The initial centers. 
 
    max_iter : int, default=300 
        Maximum number of iterations of the k-means algorithm to run. 
 
    verbose : bool, default=False 
        Verbosity mode. 
 
    tol : float, default=1e-4 
        Relative tolerance with regards to Frobenius norm of the difference 
        in the cluster centers of two consecutive iterations to declare 
        convergence. 
        It's not advised to set `tol=0` since convergence might never be 
        declared due to rounding errors. Use a very small number instead. 
 
    n_threads : int, default=1 
        The number of OpenMP threads to use for the computation. Parallelism is 
        sample-wise on the main cython loop which assigns each sample to its 
        closest center. 
 
    Returns 
    ------- 
    centroid : ndarray of shape (n_clusters, n_features) 
        Centroids found at the last iteration of k-means. 
 
    label : ndarray of shape (n_samples,) 
        label[i] is the code or index of the centroid the 
        i'th observation is closest to. 
 
    inertia : float 
        The final value of the inertia criterion (sum of squared distances to 
        the closest centroid for all observations in the training set). 
 
    n_iter : int 
        Number of iterations run. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples = X.shape[</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">n_clusters = centers_init.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s2"># Buffers to avoid new allocations at each iteration.</span>
    <span class="s1">centers = centers_init</span>
    <span class="s1">centers_new = np.zeros_like(centers)</span>
    <span class="s1">weight_in_clusters = np.zeros(n_clusters</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
    <span class="s1">labels = np.full(n_samples</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">dtype=np.int32)</span>
    <span class="s1">labels_old = labels.copy()</span>
    <span class="s1">center_half_distances = euclidean_distances(centers) / </span><span class="s5">2</span>
    <span class="s1">distance_next_center = np.partition(</span>
        <span class="s1">np.asarray(center_half_distances)</span><span class="s3">, </span><span class="s1">kth=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span>
    <span class="s1">)[</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">upper_bounds = np.zeros(n_samples</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
    <span class="s1">lower_bounds = np.zeros((n_samples</span><span class="s3">, </span><span class="s1">n_clusters)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
    <span class="s1">center_shift = np.zeros(n_clusters</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s3">if </span><span class="s1">sp.issparse(X):</span>
        <span class="s1">init_bounds = init_bounds_sparse</span>
        <span class="s1">elkan_iter = elkan_iter_chunked_sparse</span>
        <span class="s1">_inertia = _inertia_sparse</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">init_bounds = init_bounds_dense</span>
        <span class="s1">elkan_iter = elkan_iter_chunked_dense</span>
        <span class="s1">_inertia = _inertia_dense</span>

    <span class="s1">init_bounds(</span>
        <span class="s1">X</span><span class="s3">,</span>
        <span class="s1">centers</span><span class="s3">,</span>
        <span class="s1">center_half_distances</span><span class="s3">,</span>
        <span class="s1">labels</span><span class="s3">,</span>
        <span class="s1">upper_bounds</span><span class="s3">,</span>
        <span class="s1">lower_bounds</span><span class="s3">,</span>
        <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s1">strict_convergence = </span><span class="s3">False</span>

    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(max_iter):</span>
        <span class="s1">elkan_iter(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">sample_weight</span><span class="s3">,</span>
            <span class="s1">centers</span><span class="s3">,</span>
            <span class="s1">centers_new</span><span class="s3">,</span>
            <span class="s1">weight_in_clusters</span><span class="s3">,</span>
            <span class="s1">center_half_distances</span><span class="s3">,</span>
            <span class="s1">distance_next_center</span><span class="s3">,</span>
            <span class="s1">upper_bounds</span><span class="s3">,</span>
            <span class="s1">lower_bounds</span><span class="s3">,</span>
            <span class="s1">labels</span><span class="s3">,</span>
            <span class="s1">center_shift</span><span class="s3">,</span>
            <span class="s1">n_threads</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s2"># compute new pairwise distances between centers and closest other</span>
        <span class="s2"># center of each center for next iterations</span>
        <span class="s1">center_half_distances = euclidean_distances(centers_new) / </span><span class="s5">2</span>
        <span class="s1">distance_next_center = np.partition(</span>
            <span class="s1">np.asarray(center_half_distances)</span><span class="s3">, </span><span class="s1">kth=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span>
        <span class="s1">)[</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s3">if </span><span class="s1">verbose:</span>
            <span class="s1">inertia = _inertia(X</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">labels</span><span class="s3">, </span><span class="s1">n_threads)</span>
            <span class="s1">print(</span><span class="s4">f&quot;Iteration </span><span class="s3">{</span><span class="s1">i</span><span class="s3">}</span><span class="s4">, inertia </span><span class="s3">{</span><span class="s1">inertia</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>

        <span class="s1">centers</span><span class="s3">, </span><span class="s1">centers_new = centers_new</span><span class="s3">, </span><span class="s1">centers</span>

        <span class="s3">if </span><span class="s1">np.array_equal(labels</span><span class="s3">, </span><span class="s1">labels_old):</span>
            <span class="s2"># First check the labels for strict convergence.</span>
            <span class="s3">if </span><span class="s1">verbose:</span>
                <span class="s1">print(</span><span class="s4">f&quot;Converged at iteration </span><span class="s3">{</span><span class="s1">i</span><span class="s3">}</span><span class="s4">: strict convergence.&quot;</span><span class="s1">)</span>
            <span class="s1">strict_convergence = </span><span class="s3">True</span>
            <span class="s3">break</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># No strict convergence, check for tol based convergence.</span>
            <span class="s1">center_shift_tot = (center_shift**</span><span class="s5">2</span><span class="s1">).sum()</span>
            <span class="s3">if </span><span class="s1">center_shift_tot &lt;= tol:</span>
                <span class="s3">if </span><span class="s1">verbose:</span>
                    <span class="s1">print(</span>
                        <span class="s4">f&quot;Converged at iteration </span><span class="s3">{</span><span class="s1">i</span><span class="s3">}</span><span class="s4">: center shift &quot;</span>
                        <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">center_shift_tot</span><span class="s3">} </span><span class="s4">within tolerance </span><span class="s3">{</span><span class="s1">tol</span><span class="s3">}</span><span class="s4">.&quot;</span>
                    <span class="s1">)</span>
                <span class="s3">break</span>

        <span class="s1">labels_old[:] = labels</span>

    <span class="s3">if not </span><span class="s1">strict_convergence:</span>
        <span class="s2"># rerun E-step so that predicted labels match cluster centers</span>
        <span class="s1">elkan_iter(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">sample_weight</span><span class="s3">,</span>
            <span class="s1">centers</span><span class="s3">,</span>
            <span class="s1">centers</span><span class="s3">,</span>
            <span class="s1">weight_in_clusters</span><span class="s3">,</span>
            <span class="s1">center_half_distances</span><span class="s3">,</span>
            <span class="s1">distance_next_center</span><span class="s3">,</span>
            <span class="s1">upper_bounds</span><span class="s3">,</span>
            <span class="s1">lower_bounds</span><span class="s3">,</span>
            <span class="s1">labels</span><span class="s3">,</span>
            <span class="s1">center_shift</span><span class="s3">,</span>
            <span class="s1">n_threads</span><span class="s3">,</span>
            <span class="s1">update_centers=</span><span class="s3">False,</span>
        <span class="s1">)</span>

    <span class="s1">inertia = _inertia(X</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">labels</span><span class="s3">, </span><span class="s1">n_threads)</span>

    <span class="s3">return </span><span class="s1">labels</span><span class="s3">, </span><span class="s1">inertia</span><span class="s3">, </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">i + </span><span class="s5">1</span>


<span class="s3">def </span><span class="s1">_kmeans_single_lloyd(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">sample_weight</span><span class="s3">,</span>
    <span class="s1">centers_init</span><span class="s3">,</span>
    <span class="s1">max_iter=</span><span class="s5">300</span><span class="s3">,</span>
    <span class="s1">verbose=</span><span class="s3">False,</span>
    <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
    <span class="s1">n_threads=</span><span class="s5">1</span><span class="s3">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;A single run of k-means lloyd, assumes preparation completed prior. 
 
    Parameters 
    ---------- 
    X : {ndarray, sparse matrix} of shape (n_samples, n_features) 
        The observations to cluster. If sparse matrix, must be in CSR format. 
 
    sample_weight : ndarray of shape (n_samples,) 
        The weights for each observation in X. 
 
    centers_init : ndarray of shape (n_clusters, n_features) 
        The initial centers. 
 
    max_iter : int, default=300 
        Maximum number of iterations of the k-means algorithm to run. 
 
    verbose : bool, default=False 
        Verbosity mode 
 
    tol : float, default=1e-4 
        Relative tolerance with regards to Frobenius norm of the difference 
        in the cluster centers of two consecutive iterations to declare 
        convergence. 
        It's not advised to set `tol=0` since convergence might never be 
        declared due to rounding errors. Use a very small number instead. 
 
    n_threads : int, default=1 
        The number of OpenMP threads to use for the computation. Parallelism is 
        sample-wise on the main cython loop which assigns each sample to its 
        closest center. 
 
    Returns 
    ------- 
    centroid : ndarray of shape (n_clusters, n_features) 
        Centroids found at the last iteration of k-means. 
 
    label : ndarray of shape (n_samples,) 
        label[i] is the code or index of the centroid the 
        i'th observation is closest to. 
 
    inertia : float 
        The final value of the inertia criterion (sum of squared distances to 
        the closest centroid for all observations in the training set). 
 
    n_iter : int 
        Number of iterations run. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_clusters = centers_init.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s2"># Buffers to avoid new allocations at each iteration.</span>
    <span class="s1">centers = centers_init</span>
    <span class="s1">centers_new = np.zeros_like(centers)</span>
    <span class="s1">labels = np.full(X.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">dtype=np.int32)</span>
    <span class="s1">labels_old = labels.copy()</span>
    <span class="s1">weight_in_clusters = np.zeros(n_clusters</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
    <span class="s1">center_shift = np.zeros(n_clusters</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s3">if </span><span class="s1">sp.issparse(X):</span>
        <span class="s1">lloyd_iter = lloyd_iter_chunked_sparse</span>
        <span class="s1">_inertia = _inertia_sparse</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">lloyd_iter = lloyd_iter_chunked_dense</span>
        <span class="s1">_inertia = _inertia_dense</span>

    <span class="s1">strict_convergence = </span><span class="s3">False</span>

    <span class="s2"># Threadpoolctl context to limit the number of threads in second level of</span>
    <span class="s2"># nested parallelism (i.e. BLAS) to avoid oversubscription.</span>
    <span class="s3">with </span><span class="s1">threadpool_limits(limits=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">user_api=</span><span class="s4">&quot;blas&quot;</span><span class="s1">):</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(max_iter):</span>
            <span class="s1">lloyd_iter(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">sample_weight</span><span class="s3">,</span>
                <span class="s1">centers</span><span class="s3">,</span>
                <span class="s1">centers_new</span><span class="s3">,</span>
                <span class="s1">weight_in_clusters</span><span class="s3">,</span>
                <span class="s1">labels</span><span class="s3">,</span>
                <span class="s1">center_shift</span><span class="s3">,</span>
                <span class="s1">n_threads</span><span class="s3">,</span>
            <span class="s1">)</span>

            <span class="s3">if </span><span class="s1">verbose:</span>
                <span class="s1">inertia = _inertia(X</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">labels</span><span class="s3">, </span><span class="s1">n_threads)</span>
                <span class="s1">print(</span><span class="s4">f&quot;Iteration </span><span class="s3">{</span><span class="s1">i</span><span class="s3">}</span><span class="s4">, inertia </span><span class="s3">{</span><span class="s1">inertia</span><span class="s3">}</span><span class="s4">.&quot;</span><span class="s1">)</span>

            <span class="s1">centers</span><span class="s3">, </span><span class="s1">centers_new = centers_new</span><span class="s3">, </span><span class="s1">centers</span>

            <span class="s3">if </span><span class="s1">np.array_equal(labels</span><span class="s3">, </span><span class="s1">labels_old):</span>
                <span class="s2"># First check the labels for strict convergence.</span>
                <span class="s3">if </span><span class="s1">verbose:</span>
                    <span class="s1">print(</span><span class="s4">f&quot;Converged at iteration </span><span class="s3">{</span><span class="s1">i</span><span class="s3">}</span><span class="s4">: strict convergence.&quot;</span><span class="s1">)</span>
                <span class="s1">strict_convergence = </span><span class="s3">True</span>
                <span class="s3">break</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s2"># No strict convergence, check for tol based convergence.</span>
                <span class="s1">center_shift_tot = (center_shift**</span><span class="s5">2</span><span class="s1">).sum()</span>
                <span class="s3">if </span><span class="s1">center_shift_tot &lt;= tol:</span>
                    <span class="s3">if </span><span class="s1">verbose:</span>
                        <span class="s1">print(</span>
                            <span class="s4">f&quot;Converged at iteration </span><span class="s3">{</span><span class="s1">i</span><span class="s3">}</span><span class="s4">: center shift &quot;</span>
                            <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">center_shift_tot</span><span class="s3">} </span><span class="s4">within tolerance </span><span class="s3">{</span><span class="s1">tol</span><span class="s3">}</span><span class="s4">.&quot;</span>
                        <span class="s1">)</span>
                    <span class="s3">break</span>

            <span class="s1">labels_old[:] = labels</span>

        <span class="s3">if not </span><span class="s1">strict_convergence:</span>
            <span class="s2"># rerun E-step so that predicted labels match cluster centers</span>
            <span class="s1">lloyd_iter(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">sample_weight</span><span class="s3">,</span>
                <span class="s1">centers</span><span class="s3">,</span>
                <span class="s1">centers</span><span class="s3">,</span>
                <span class="s1">weight_in_clusters</span><span class="s3">,</span>
                <span class="s1">labels</span><span class="s3">,</span>
                <span class="s1">center_shift</span><span class="s3">,</span>
                <span class="s1">n_threads</span><span class="s3">,</span>
                <span class="s1">update_centers=</span><span class="s3">False,</span>
            <span class="s1">)</span>

    <span class="s1">inertia = _inertia(X</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">labels</span><span class="s3">, </span><span class="s1">n_threads)</span>

    <span class="s3">return </span><span class="s1">labels</span><span class="s3">, </span><span class="s1">inertia</span><span class="s3">, </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">i + </span><span class="s5">1</span>


<span class="s3">def </span><span class="s1">_labels_inertia(X</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">n_threads=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">return_inertia=</span><span class="s3">True</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;E step of the K-means EM algorithm. 
 
    Compute the labels and the inertia of the given samples and centers. 
 
    Parameters 
    ---------- 
    X : {ndarray, sparse matrix} of shape (n_samples, n_features) 
        The input samples to assign to the labels. If sparse matrix, must 
        be in CSR format. 
 
    sample_weight : ndarray of shape (n_samples,) 
        The weights for each observation in X. 
 
    x_squared_norms : ndarray of shape (n_samples,) 
        Precomputed squared euclidean norm of each data point, to speed up 
        computations. 
 
    centers : ndarray of shape (n_clusters, n_features) 
        The cluster centers. 
 
    n_threads : int, default=1 
        The number of OpenMP threads to use for the computation. Parallelism is 
        sample-wise on the main cython loop which assigns each sample to its 
        closest center. 
 
    return_inertia : bool, default=True 
        Whether to compute and return the inertia. 
 
    Returns 
    ------- 
    labels : ndarray of shape (n_samples,) 
        The resulting assignment. 
 
    inertia : float 
        Sum of squared distances of samples to their closest cluster center. 
        Inertia is only returned if return_inertia is True. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples = X.shape[</span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">n_clusters = centers.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s1">labels = np.full(n_samples</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">dtype=np.int32)</span>
    <span class="s1">center_shift = np.zeros(n_clusters</span><span class="s3">, </span><span class="s1">dtype=centers.dtype)</span>

    <span class="s3">if </span><span class="s1">sp.issparse(X):</span>
        <span class="s1">_labels = lloyd_iter_chunked_sparse</span>
        <span class="s1">_inertia = _inertia_sparse</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">_labels = lloyd_iter_chunked_dense</span>
        <span class="s1">_inertia = _inertia_dense</span>

    <span class="s1">_labels(</span>
        <span class="s1">X</span><span class="s3">,</span>
        <span class="s1">sample_weight</span><span class="s3">,</span>
        <span class="s1">centers</span><span class="s3">,</span>
        <span class="s1">centers_new=</span><span class="s3">None,</span>
        <span class="s1">weight_in_clusters=</span><span class="s3">None,</span>
        <span class="s1">labels=labels</span><span class="s3">,</span>
        <span class="s1">center_shift=center_shift</span><span class="s3">,</span>
        <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
        <span class="s1">update_centers=</span><span class="s3">False,</span>
    <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">return_inertia:</span>
        <span class="s1">inertia = _inertia(X</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">labels</span><span class="s3">, </span><span class="s1">n_threads)</span>
        <span class="s3">return </span><span class="s1">labels</span><span class="s3">, </span><span class="s1">inertia</span>

    <span class="s3">return </span><span class="s1">labels</span>


<span class="s3">def </span><span class="s1">_labels_inertia_threadpool_limit(</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">n_threads=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">return_inertia=</span><span class="s3">True</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Same as _labels_inertia but in a threadpool_limits context.&quot;&quot;&quot;</span>
    <span class="s3">with </span><span class="s1">threadpool_limits(limits=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">user_api=</span><span class="s4">&quot;blas&quot;</span><span class="s1">):</span>
        <span class="s1">result = _labels_inertia(X</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">n_threads</span><span class="s3">, </span><span class="s1">return_inertia)</span>

    <span class="s3">return </span><span class="s1">result</span>


<span class="s3">class </span><span class="s1">_BaseKMeans(</span>
    <span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s3">, </span><span class="s1">TransformerMixin</span><span class="s3">, </span><span class="s1">ClusterMixin</span><span class="s3">, </span><span class="s1">BaseEstimator</span><span class="s3">, </span><span class="s1">ABC</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Base class for KMeans and MiniBatchKMeans&quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;n_clusters&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;init&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;k-means++&quot;</span><span class="s3">, </span><span class="s4">&quot;random&quot;</span><span class="s1">})</span><span class="s3">, </span><span class="s1">callable</span><span class="s3">, </span><span class="s4">&quot;array-like&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_init&quot;</span><span class="s1">: [</span>
            <span class="s1">StrOptions({</span><span class="s4">&quot;auto&quot;</span><span class="s1">})</span><span class="s3">,</span>
            <span class="s1">Hidden(StrOptions({</span><span class="s4">&quot;warn&quot;</span><span class="s1">}))</span><span class="s3">,</span>
            <span class="s1">Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s4">&quot;verbose&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">n_clusters</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">init</span><span class="s3">,</span>
        <span class="s1">n_init</span><span class="s3">,</span>
        <span class="s1">max_iter</span><span class="s3">,</span>
        <span class="s1">tol</span><span class="s3">,</span>
        <span class="s1">verbose</span><span class="s3">,</span>
        <span class="s1">random_state</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">self.n_clusters = n_clusters</span>
        <span class="s1">self.init = init</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.n_init = n_init</span>
        <span class="s1">self.verbose = verbose</span>
        <span class="s1">self.random_state = random_state</span>

    <span class="s3">def </span><span class="s1">_check_params_vs_input(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">default_n_init=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2"># n_clusters</span>
        <span class="s3">if </span><span class="s1">X.shape[</span><span class="s5">0</span><span class="s1">] &lt; self.n_clusters:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">f&quot;n_samples=</span><span class="s3">{</span><span class="s1">X.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">} </span><span class="s4">should be &gt;= n_clusters=</span><span class="s3">{</span><span class="s1">self.n_clusters</span><span class="s3">}</span><span class="s4">.&quot;</span>
            <span class="s1">)</span>

        <span class="s2"># tol</span>
        <span class="s1">self._tol = _tolerance(X</span><span class="s3">, </span><span class="s1">self.tol)</span>

        <span class="s2"># n-init</span>
        <span class="s2"># TODO(1.4): Remove</span>
        <span class="s1">self._n_init = self.n_init</span>
        <span class="s3">if </span><span class="s1">self._n_init == </span><span class="s4">&quot;warn&quot;</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s4">&quot;The default value of `n_init` will change from &quot;</span>
                    <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">default_n_init</span><span class="s3">} </span><span class="s4">to 'auto' in 1.4. Set the value of `n_init`&quot;</span>
                    <span class="s4">&quot; explicitly to suppress the warning&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">FutureWarning</span><span class="s3">,</span>
                <span class="s1">stacklevel=</span><span class="s5">2</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">self._n_init = default_n_init</span>
        <span class="s3">if </span><span class="s1">self._n_init == </span><span class="s4">&quot;auto&quot;</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">isinstance(self.init</span><span class="s3">, </span><span class="s1">str) </span><span class="s3">and </span><span class="s1">self.init == </span><span class="s4">&quot;k-means++&quot;</span><span class="s1">:</span>
                <span class="s1">self._n_init = </span><span class="s5">1</span>
            <span class="s3">elif </span><span class="s1">isinstance(self.init</span><span class="s3">, </span><span class="s1">str) </span><span class="s3">and </span><span class="s1">self.init == </span><span class="s4">&quot;random&quot;</span><span class="s1">:</span>
                <span class="s1">self._n_init = default_n_init</span>
            <span class="s3">elif </span><span class="s1">callable(self.init):</span>
                <span class="s1">self._n_init = default_n_init</span>
            <span class="s3">else</span><span class="s1">:  </span><span class="s2"># array-like</span>
                <span class="s1">self._n_init = </span><span class="s5">1</span>

        <span class="s3">if </span><span class="s1">_is_arraylike_not_scalar(self.init) </span><span class="s3">and </span><span class="s1">self._n_init != </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s4">&quot;Explicit initial center position passed: performing only&quot;</span>
                    <span class="s4">f&quot; one init in </span><span class="s3">{</span><span class="s1">self.__class__.__name__</span><span class="s3">} </span><span class="s4">instead of &quot;</span>
                    <span class="s4">f&quot;n_init=</span><span class="s3">{</span><span class="s1">self._n_init</span><span class="s3">}</span><span class="s4">.&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">RuntimeWarning</span><span class="s3">,</span>
                <span class="s1">stacklevel=</span><span class="s5">2</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">self._n_init = </span><span class="s5">1</span>

    <span class="s1">@abstractmethod</span>
    <span class="s3">def </span><span class="s1">_warn_mkl_vcomp(self</span><span class="s3">, </span><span class="s1">n_active_threads):</span>
        <span class="s0">&quot;&quot;&quot;Issue an estimator specific warning when vcomp and mkl are both present 
 
        This method is called by `_check_mkl_vcomp`. 
        &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">_check_mkl_vcomp(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">n_samples):</span>
        <span class="s0">&quot;&quot;&quot;Check when vcomp and mkl are both present&quot;&quot;&quot;</span>
        <span class="s2"># The BLAS call inside a prange in lloyd_iter_chunked_dense is known to</span>
        <span class="s2"># cause a small memory leak when there are less chunks than the number</span>
        <span class="s2"># of available threads. It only happens when the OpenMP library is</span>
        <span class="s2"># vcomp (microsoft OpenMP) and the BLAS library is MKL. see #18653</span>
        <span class="s3">if </span><span class="s1">sp.issparse(X):</span>
            <span class="s3">return</span>

        <span class="s1">n_active_threads = int(np.ceil(n_samples / CHUNK_SIZE))</span>
        <span class="s3">if </span><span class="s1">n_active_threads &lt; self._n_threads:</span>
            <span class="s1">modules = threadpool_info()</span>
            <span class="s1">has_vcomp = </span><span class="s4">&quot;vcomp&quot; </span><span class="s3">in </span><span class="s1">[module[</span><span class="s4">&quot;prefix&quot;</span><span class="s1">] </span><span class="s3">for </span><span class="s1">module </span><span class="s3">in </span><span class="s1">modules]</span>
            <span class="s1">has_mkl = (</span><span class="s4">&quot;mkl&quot;</span><span class="s3">, </span><span class="s4">&quot;intel&quot;</span><span class="s1">) </span><span class="s3">in </span><span class="s1">[</span>
                <span class="s1">(module[</span><span class="s4">&quot;internal_api&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">module.get(</span><span class="s4">&quot;threading_layer&quot;</span><span class="s3">, None</span><span class="s1">))</span>
                <span class="s3">for </span><span class="s1">module </span><span class="s3">in </span><span class="s1">modules</span>
            <span class="s1">]</span>
            <span class="s3">if </span><span class="s1">has_vcomp </span><span class="s3">and </span><span class="s1">has_mkl:</span>
                <span class="s1">self._warn_mkl_vcomp(n_active_threads)</span>

    <span class="s3">def </span><span class="s1">_validate_center_shape(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">centers):</span>
        <span class="s0">&quot;&quot;&quot;Check if centers is compatible with X and n_clusters.&quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">centers.shape[</span><span class="s5">0</span><span class="s1">] != self.n_clusters:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">f&quot;The shape of the initial centers </span><span class="s3">{</span><span class="s1">centers.shape</span><span class="s3">} </span><span class="s4">does not &quot;</span>
                <span class="s4">f&quot;match the number of clusters </span><span class="s3">{</span><span class="s1">self.n_clusters</span><span class="s3">}</span><span class="s4">.&quot;</span>
            <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">centers.shape[</span><span class="s5">1</span><span class="s1">] != X.shape[</span><span class="s5">1</span><span class="s1">]:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">f&quot;The shape of the initial centers </span><span class="s3">{</span><span class="s1">centers.shape</span><span class="s3">} </span><span class="s4">does not &quot;</span>
                <span class="s4">f&quot;match the number of features of the data </span><span class="s3">{</span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">}</span><span class="s4">.&quot;</span>
            <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_check_test_data(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">,</span>
            <span class="s1">reset=</span><span class="s3">False,</span>
            <span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">,</span>
            <span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s3">,</span>
            <span class="s1">accept_large_sparse=</span><span class="s3">False,</span>
        <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">X</span>

    <span class="s3">def </span><span class="s1">_init_centroids(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">X</span><span class="s3">,</span>
        <span class="s1">x_squared_norms</span><span class="s3">,</span>
        <span class="s1">init</span><span class="s3">,</span>
        <span class="s1">random_state</span><span class="s3">,</span>
        <span class="s1">sample_weight</span><span class="s3">,</span>
        <span class="s1">init_size=</span><span class="s3">None,</span>
        <span class="s1">n_centroids=</span><span class="s3">None,</span>
    <span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Compute the initial centroids. 
 
        Parameters 
        ---------- 
        X : {ndarray, sparse matrix} of shape (n_samples, n_features) 
            The input samples. 
 
        x_squared_norms : ndarray of shape (n_samples,) 
            Squared euclidean norm of each data point. Pass it if you have it 
            at hands already to avoid it being recomputed here. 
 
        init : {'k-means++', 'random'}, callable or ndarray of shape \ 
                (n_clusters, n_features) 
            Method for initialization. 
 
        random_state : RandomState instance 
            Determines random number generation for centroid initialization. 
            See :term:`Glossary &lt;random_state&gt;`. 
 
        sample_weight : ndarray of shape (n_samples,) 
            The weights for each observation in X. `sample_weight` is not used 
            during initialization if `init` is a callable or a user provided 
            array. 
 
        init_size : int, default=None 
            Number of samples to randomly sample for speeding up the 
            initialization (sometimes at the expense of accuracy). 
 
        n_centroids : int, default=None 
            Number of centroids to initialize. 
            If left to 'None' the number of centroids will be equal to 
            number of clusters to form (self.n_clusters). 
 
        Returns 
        ------- 
        centers : ndarray of shape (n_clusters, n_features) 
            Initial centroids of clusters. 
        &quot;&quot;&quot;</span>
        <span class="s1">n_samples = X.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">n_clusters = self.n_clusters </span><span class="s3">if </span><span class="s1">n_centroids </span><span class="s3">is None else </span><span class="s1">n_centroids</span>

        <span class="s3">if </span><span class="s1">init_size </span><span class="s3">is not None and </span><span class="s1">init_size &lt; n_samples:</span>
            <span class="s1">init_indices = random_state.randint(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">init_size)</span>
            <span class="s1">X = X[init_indices]</span>
            <span class="s1">x_squared_norms = x_squared_norms[init_indices]</span>
            <span class="s1">n_samples = X.shape[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">sample_weight = sample_weight[init_indices]</span>

        <span class="s3">if </span><span class="s1">isinstance(init</span><span class="s3">, </span><span class="s1">str) </span><span class="s3">and </span><span class="s1">init == </span><span class="s4">&quot;k-means++&quot;</span><span class="s1">:</span>
            <span class="s1">centers</span><span class="s3">, </span><span class="s1">_ = _kmeans_plusplus(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">n_clusters</span><span class="s3">,</span>
                <span class="s1">random_state=random_state</span><span class="s3">,</span>
                <span class="s1">x_squared_norms=x_squared_norms</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
            <span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">isinstance(init</span><span class="s3">, </span><span class="s1">str) </span><span class="s3">and </span><span class="s1">init == </span><span class="s4">&quot;random&quot;</span><span class="s1">:</span>
            <span class="s1">seeds = random_state.choice(</span>
                <span class="s1">n_samples</span><span class="s3">,</span>
                <span class="s1">size=n_clusters</span><span class="s3">,</span>
                <span class="s1">replace=</span><span class="s3">False,</span>
                <span class="s1">p=sample_weight / sample_weight.sum()</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">centers = X[seeds]</span>
        <span class="s3">elif </span><span class="s1">_is_arraylike_not_scalar(self.init):</span>
            <span class="s1">centers = init</span>
        <span class="s3">elif </span><span class="s1">callable(init):</span>
            <span class="s1">centers = init(X</span><span class="s3">, </span><span class="s1">n_clusters</span><span class="s3">, </span><span class="s1">random_state=random_state)</span>
            <span class="s1">centers = check_array(centers</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False, </span><span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s1">)</span>
            <span class="s1">self._validate_center_shape(X</span><span class="s3">, </span><span class="s1">centers)</span>

        <span class="s3">if </span><span class="s1">sp.issparse(centers):</span>
            <span class="s1">centers = centers.toarray()</span>

        <span class="s3">return </span><span class="s1">centers</span>

    <span class="s3">def </span><span class="s1">fit_predict(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Compute cluster centers and predict cluster index for each sample. 
 
        Convenience method; equivalent to calling fit(X) followed by 
        predict(X). 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            New data to transform. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            The weights for each observation in X. If None, all observations 
            are assigned equal weight. 
 
        Returns 
        ------- 
        labels : ndarray of shape (n_samples,) 
            Index of the cluster each sample belongs to. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.fit(X</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight).labels_</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s4">&quot;deprecated&quot;</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Predict the closest cluster each sample in X belongs to. 
 
        In the vector quantization literature, `cluster_centers_` is called 
        the code book and each value returned by `predict` is the index of 
        the closest code in the code book. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            New data to predict. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            The weights for each observation in X. If None, all observations 
            are assigned equal weight. 
 
            .. deprecated:: 1.3 
               The parameter `sample_weight` is deprecated in version 1.3 
               and will be removed in 1.5. 
 
        Returns 
        ------- 
        labels : ndarray of shape (n_samples,) 
            Index of the cluster each sample belongs to. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s1">X = self._check_test_data(X)</span>
        <span class="s3">if not </span><span class="s1">(isinstance(sample_weight</span><span class="s3">, </span><span class="s1">str) </span><span class="s3">and </span><span class="s1">sample_weight == </span><span class="s4">&quot;deprecated&quot;</span><span class="s1">):</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s4">&quot;'sample_weight' was deprecated in version 1.3 and &quot;</span>
                    <span class="s4">&quot;will be removed in 1.5.&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">FutureWarning</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">sample_weight = _check_sample_weight(</span><span class="s3">None, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s1">labels = _labels_inertia_threadpool_limit(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">sample_weight</span><span class="s3">,</span>
            <span class="s1">self.cluster_centers_</span><span class="s3">,</span>
            <span class="s1">n_threads=self._n_threads</span><span class="s3">,</span>
            <span class="s1">return_inertia=</span><span class="s3">False,</span>
        <span class="s1">)</span>

        <span class="s3">return </span><span class="s1">labels</span>

    <span class="s3">def </span><span class="s1">fit_transform(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Compute clustering and transform X to cluster-distance space. 
 
        Equivalent to fit(X).transform(X), but more efficiently implemented. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            New data to transform. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            The weights for each observation in X. If None, all observations 
            are assigned equal weight. 
 
        Returns 
        ------- 
        X_new : ndarray of shape (n_samples, n_clusters) 
            X transformed in the new space. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.fit(X</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)._transform(X)</span>

    <span class="s3">def </span><span class="s1">transform(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Transform X to a cluster-distance space. 
 
        In the new space, each dimension is the distance to the cluster 
        centers. Note that even if X is sparse, the array returned by 
        `transform` will typically be dense. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            New data to transform. 
 
        Returns 
        ------- 
        X_new : ndarray of shape (n_samples, n_clusters) 
            X transformed in the new space. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s1">X = self._check_test_data(X)</span>
        <span class="s3">return </span><span class="s1">self._transform(X)</span>

    <span class="s3">def </span><span class="s1">_transform(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Guts of transform method; no input validation.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">euclidean_distances(X</span><span class="s3">, </span><span class="s1">self.cluster_centers_)</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Opposite of the value of X on the K-means objective. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            New data. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            The weights for each observation in X. If None, all observations 
            are assigned equal weight. 
 
        Returns 
        ------- 
        score : float 
            Opposite of the value of X on the K-means objective. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s1">X = self._check_test_data(X)</span>
        <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s1">_</span><span class="s3">, </span><span class="s1">scores = _labels_inertia_threadpool_limit(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">self.cluster_centers_</span><span class="s3">, </span><span class="s1">self._n_threads</span>
        <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">-scores</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span>
            <span class="s4">&quot;_xfail_checks&quot;</span><span class="s1">: {</span>
                <span class="s4">&quot;check_sample_weights_invariance&quot;</span><span class="s1">: (</span>
                    <span class="s4">&quot;zero sample_weight is not equivalent to removing samples&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">}</span><span class="s3">,</span>
        <span class="s1">}</span>


<span class="s3">class </span><span class="s1">KMeans(_BaseKMeans):</span>
    <span class="s0">&quot;&quot;&quot;K-Means clustering. 
 
    Read more in the :ref:`User Guide &lt;k_means&gt;`. 
 
    Parameters 
    ---------- 
 
    n_clusters : int, default=8 
        The number of clusters to form as well as the number of 
        centroids to generate. 
 
    init : {'k-means++', 'random'}, callable or array-like of shape \ 
            (n_clusters, n_features), default='k-means++' 
        Method for initialization: 
 
        * 'k-means++' : selects initial cluster centroids using sampling \ 
            based on an empirical probability distribution of the points' \ 
            contribution to the overall inertia. This technique speeds up \ 
            convergence. The algorithm implemented is &quot;greedy k-means++&quot;. It \ 
            differs from the vanilla k-means++ by making several trials at \ 
            each sampling step and choosing the best centroid among them. 
 
        * 'random': choose `n_clusters` observations (rows) at random from \ 
        data for the initial centroids. 
 
        * If an array is passed, it should be of shape (n_clusters, n_features)\ 
        and gives the initial centers. 
 
        * If a callable is passed, it should take arguments X, n_clusters and a\ 
        random state and return an initialization. 
 
        For an example of how to use the different `init` strategy, see the example 
        entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`. 
 
    n_init : 'auto' or int, default=10 
        Number of times the k-means algorithm is run with different centroid 
        seeds. The final results is the best output of `n_init` consecutive runs 
        in terms of inertia. Several runs are recommended for sparse 
        high-dimensional problems (see :ref:`kmeans_sparse_high_dim`). 
 
        When `n_init='auto'`, the number of runs depends on the value of init: 
        10 if using `init='random'` or `init` is a callable; 
        1 if using `init='k-means++'` or `init` is an array-like. 
 
        .. versionadded:: 1.2 
           Added 'auto' option for `n_init`. 
 
        .. versionchanged:: 1.4 
           Default value for `n_init` will change from 10 to `'auto'` in version 1.4. 
 
    max_iter : int, default=300 
        Maximum number of iterations of the k-means algorithm for a 
        single run. 
 
    tol : float, default=1e-4 
        Relative tolerance with regards to Frobenius norm of the difference 
        in the cluster centers of two consecutive iterations to declare 
        convergence. 
 
    verbose : int, default=0 
        Verbosity mode. 
 
    random_state : int, RandomState instance or None, default=None 
        Determines random number generation for centroid initialization. Use 
        an int to make the randomness deterministic. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    copy_x : bool, default=True 
        When pre-computing distances it is more numerically accurate to center 
        the data first. If copy_x is True (default), then the original data is 
        not modified. If False, the original data is modified, and put back 
        before the function returns, but small numerical differences may be 
        introduced by subtracting and then adding the data mean. Note that if 
        the original data is not C-contiguous, a copy will be made even if 
        copy_x is False. If the original data is sparse, but not in CSR format, 
        a copy will be made even if copy_x is False. 
 
    algorithm : {&quot;lloyd&quot;, &quot;elkan&quot;, &quot;auto&quot;, &quot;full&quot;}, default=&quot;lloyd&quot; 
        K-means algorithm to use. The classical EM-style algorithm is `&quot;lloyd&quot;`. 
        The `&quot;elkan&quot;` variation can be more efficient on some datasets with 
        well-defined clusters, by using the triangle inequality. However it's 
        more memory intensive due to the allocation of an extra array of shape 
        `(n_samples, n_clusters)`. 
 
        `&quot;auto&quot;` and `&quot;full&quot;` are deprecated and they will be removed in 
        Scikit-Learn 1.3. They are both aliases for `&quot;lloyd&quot;`. 
 
        .. versionchanged:: 0.18 
            Added Elkan algorithm 
 
        .. versionchanged:: 1.1 
            Renamed &quot;full&quot; to &quot;lloyd&quot;, and deprecated &quot;auto&quot; and &quot;full&quot;. 
            Changed &quot;auto&quot; to use &quot;lloyd&quot; instead of &quot;elkan&quot;. 
 
    Attributes 
    ---------- 
    cluster_centers_ : ndarray of shape (n_clusters, n_features) 
        Coordinates of cluster centers. If the algorithm stops before fully 
        converging (see ``tol`` and ``max_iter``), these will not be 
        consistent with ``labels_``. 
 
    labels_ : ndarray of shape (n_samples,) 
        Labels of each point 
 
    inertia_ : float 
        Sum of squared distances of samples to their closest cluster center, 
        weighted by the sample weights if provided. 
 
    n_iter_ : int 
        Number of iterations run. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    MiniBatchKMeans : Alternative online implementation that does incremental 
        updates of the centers positions using mini-batches. 
        For large scale learning (say n_samples &gt; 10k) MiniBatchKMeans is 
        probably much faster than the default batch implementation. 
 
    Notes 
    ----- 
    The k-means problem is solved using either Lloyd's or Elkan's algorithm. 
 
    The average complexity is given by O(k n T), where n is the number of 
    samples and T is the number of iteration. 
 
    The worst case complexity is given by O(n^(k+2/p)) with 
    n = n_samples, p = n_features. 
    Refer to :doi:`&quot;How slow is the k-means method?&quot; D. Arthur and S. Vassilvitskii - 
    SoCG2006.&lt;10.1145/1137856.1137880&gt;` for more details. 
 
    In practice, the k-means algorithm is very fast (one of the fastest 
    clustering algorithms available), but it falls in local minima. That's why 
    it can be useful to restart it several times. 
 
    If the algorithm stops before fully converging (because of ``tol`` or 
    ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent, 
    i.e. the ``cluster_centers_`` will not be the means of the points in each 
    cluster. Also, the estimator will reassign ``labels_`` after the last 
    iteration to make ``labels_`` consistent with ``predict`` on the training 
    set. 
 
    Examples 
    -------- 
 
    &gt;&gt;&gt; from sklearn.cluster import KMeans 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; X = np.array([[1, 2], [1, 4], [1, 0], 
    ...               [10, 2], [10, 4], [10, 0]]) 
    &gt;&gt;&gt; kmeans = KMeans(n_clusters=2, random_state=0, n_init=&quot;auto&quot;).fit(X) 
    &gt;&gt;&gt; kmeans.labels_ 
    array([1, 1, 1, 0, 0, 0], dtype=int32) 
    &gt;&gt;&gt; kmeans.predict([[0, 0], [12, 3]]) 
    array([1, 0], dtype=int32) 
    &gt;&gt;&gt; kmeans.cluster_centers_ 
    array([[10.,  2.], 
           [ 1.,  2.]]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**_BaseKMeans._parameter_constraints</span><span class="s3">,</span>
        <span class="s4">&quot;copy_x&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;algorithm&quot;</span><span class="s1">: [</span>
            <span class="s1">StrOptions({</span><span class="s4">&quot;lloyd&quot;</span><span class="s3">, </span><span class="s4">&quot;elkan&quot;</span><span class="s3">, </span><span class="s4">&quot;auto&quot;</span><span class="s3">, </span><span class="s4">&quot;full&quot;</span><span class="s1">}</span><span class="s3">, </span><span class="s1">deprecated={</span><span class="s4">&quot;auto&quot;</span><span class="s3">, </span><span class="s4">&quot;full&quot;</span><span class="s1">})</span>
        <span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">n_clusters=</span><span class="s5">8</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">init=</span><span class="s4">&quot;k-means++&quot;</span><span class="s3">,</span>
        <span class="s1">n_init=</span><span class="s4">&quot;warn&quot;</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s5">300</span><span class="s3">,</span>
        <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
        <span class="s1">copy_x=</span><span class="s3">True,</span>
        <span class="s1">algorithm=</span><span class="s4">&quot;lloyd&quot;</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">n_clusters=n_clusters</span><span class="s3">,</span>
            <span class="s1">init=init</span><span class="s3">,</span>
            <span class="s1">n_init=n_init</span><span class="s3">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
            <span class="s1">tol=tol</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s1">self.copy_x = copy_x</span>
        <span class="s1">self.algorithm = algorithm</span>

    <span class="s3">def </span><span class="s1">_check_params_vs_input(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s1">super()._check_params_vs_input(X</span><span class="s3">, </span><span class="s1">default_n_init=</span><span class="s5">10</span><span class="s1">)</span>

        <span class="s1">self._algorithm = self.algorithm</span>
        <span class="s3">if </span><span class="s1">self._algorithm </span><span class="s3">in </span><span class="s1">(</span><span class="s4">&quot;auto&quot;</span><span class="s3">, </span><span class="s4">&quot;full&quot;</span><span class="s1">):</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s4">f&quot;algorithm='</span><span class="s3">{</span><span class="s1">self._algorithm</span><span class="s3">}</span><span class="s4">' is deprecated, it will be &quot;</span>
                    <span class="s4">&quot;removed in 1.3. Using 'lloyd' instead.&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">FutureWarning</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">self._algorithm = </span><span class="s4">&quot;lloyd&quot;</span>
        <span class="s3">if </span><span class="s1">self._algorithm == </span><span class="s4">&quot;elkan&quot; </span><span class="s3">and </span><span class="s1">self.n_clusters == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s4">&quot;algorithm='elkan' doesn't make sense for a single &quot;</span>
                    <span class="s4">&quot;cluster. Using 'lloyd' instead.&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">RuntimeWarning</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">self._algorithm = </span><span class="s4">&quot;lloyd&quot;</span>

    <span class="s3">def </span><span class="s1">_warn_mkl_vcomp(self</span><span class="s3">, </span><span class="s1">n_active_threads):</span>
        <span class="s0">&quot;&quot;&quot;Warn when vcomp and mkl are both present&quot;&quot;&quot;</span>
        <span class="s1">warnings.warn(</span>
            <span class="s4">&quot;KMeans is known to have a memory leak on Windows &quot;</span>
            <span class="s4">&quot;with MKL, when there are less chunks than available &quot;</span>
            <span class="s4">&quot;threads. You can avoid it by setting the environment&quot;</span>
            <span class="s4">f&quot; variable OMP_NUM_THREADS=</span><span class="s3">{</span><span class="s1">n_active_threads</span><span class="s3">}</span><span class="s4">.&quot;</span>
        <span class="s1">)</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Compute k-means clustering. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training instances to cluster. It must be noted that the data 
            will be converted to C ordering, which will cause a memory 
            copy if the given data is not C-contiguous. 
            If a sparse matrix is passed, a copy will be made if it's not in 
            CSR format. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            The weights for each observation in X. If None, all observations 
            are assigned equal weight. `sample_weight` is not used during 
            initialization if `init` is a callable or a user provided array. 
 
            .. versionadded:: 0.20 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">,</span>
            <span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">,</span>
            <span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s3">,</span>
            <span class="s1">copy=self.copy_x</span><span class="s3">,</span>
            <span class="s1">accept_large_sparse=</span><span class="s3">False,</span>
        <span class="s1">)</span>

        <span class="s1">self._check_params_vs_input(X)</span>

        <span class="s1">random_state = check_random_state(self.random_state)</span>
        <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s1">self._n_threads = _openmp_effective_n_threads()</span>

        <span class="s2"># Validate init array</span>
        <span class="s1">init = self.init</span>
        <span class="s1">init_is_array_like = _is_arraylike_not_scalar(init)</span>
        <span class="s3">if </span><span class="s1">init_is_array_like:</span>
            <span class="s1">init = check_array(init</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">True, </span><span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s1">)</span>
            <span class="s1">self._validate_center_shape(X</span><span class="s3">, </span><span class="s1">init)</span>

        <span class="s2"># subtract of mean of x for more accurate distance computations</span>
        <span class="s3">if not </span><span class="s1">sp.issparse(X):</span>
            <span class="s1">X_mean = X.mean(axis=</span><span class="s5">0</span><span class="s1">)</span>
            <span class="s2"># The copy was already done above</span>
            <span class="s1">X -= X_mean</span>

            <span class="s3">if </span><span class="s1">init_is_array_like:</span>
                <span class="s1">init -= X_mean</span>

        <span class="s2"># precompute squared norms of data points</span>
        <span class="s1">x_squared_norms = row_norms(X</span><span class="s3">, </span><span class="s1">squared=</span><span class="s3">True</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self._algorithm == </span><span class="s4">&quot;elkan&quot;</span><span class="s1">:</span>
            <span class="s1">kmeans_single = _kmeans_single_elkan</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">kmeans_single = _kmeans_single_lloyd</span>
            <span class="s1">self._check_mkl_vcomp(X</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s5">0</span><span class="s1">])</span>

        <span class="s1">best_inertia</span><span class="s3">, </span><span class="s1">best_labels = </span><span class="s3">None, None</span>

        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self._n_init):</span>
            <span class="s2"># Initialize centers</span>
            <span class="s1">centers_init = self._init_centroids(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">x_squared_norms=x_squared_norms</span><span class="s3">,</span>
                <span class="s1">init=init</span><span class="s3">,</span>
                <span class="s1">random_state=random_state</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">print(</span><span class="s4">&quot;Initialization complete&quot;</span><span class="s1">)</span>

            <span class="s2"># run a k-means once</span>
            <span class="s1">labels</span><span class="s3">, </span><span class="s1">inertia</span><span class="s3">, </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">n_iter_ = kmeans_single(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">sample_weight</span><span class="s3">,</span>
                <span class="s1">centers_init</span><span class="s3">,</span>
                <span class="s1">max_iter=self.max_iter</span><span class="s3">,</span>
                <span class="s1">verbose=self.verbose</span><span class="s3">,</span>
                <span class="s1">tol=self._tol</span><span class="s3">,</span>
                <span class="s1">n_threads=self._n_threads</span><span class="s3">,</span>
            <span class="s1">)</span>

            <span class="s2"># determine if these results are the best so far</span>
            <span class="s2"># we chose a new run if it has a better inertia and the clustering is</span>
            <span class="s2"># different from the best so far (it's possible that the inertia is</span>
            <span class="s2"># slightly better even if the clustering is the same with potentially</span>
            <span class="s2"># permuted labels, due to rounding errors)</span>
            <span class="s3">if </span><span class="s1">best_inertia </span><span class="s3">is None or </span><span class="s1">(</span>
                <span class="s1">inertia &lt; best_inertia</span>
                <span class="s3">and not </span><span class="s1">_is_same_clustering(labels</span><span class="s3">, </span><span class="s1">best_labels</span><span class="s3">, </span><span class="s1">self.n_clusters)</span>
            <span class="s1">):</span>
                <span class="s1">best_labels = labels</span>
                <span class="s1">best_centers = centers</span>
                <span class="s1">best_inertia = inertia</span>
                <span class="s1">best_n_iter = n_iter_</span>

        <span class="s3">if not </span><span class="s1">sp.issparse(X):</span>
            <span class="s3">if not </span><span class="s1">self.copy_x:</span>
                <span class="s1">X += X_mean</span>
            <span class="s1">best_centers += X_mean</span>

        <span class="s1">distinct_clusters = len(set(best_labels))</span>
        <span class="s3">if </span><span class="s1">distinct_clusters &lt; self.n_clusters:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s4">&quot;Number of distinct clusters ({}) found smaller than &quot;</span>
                <span class="s4">&quot;n_clusters ({}). Possibly due to duplicate points &quot;</span>
                <span class="s4">&quot;in X.&quot;</span><span class="s1">.format(distinct_clusters</span><span class="s3">, </span><span class="s1">self.n_clusters)</span><span class="s3">,</span>
                <span class="s1">ConvergenceWarning</span><span class="s3">,</span>
                <span class="s1">stacklevel=</span><span class="s5">2</span><span class="s3">,</span>
            <span class="s1">)</span>

        <span class="s1">self.cluster_centers_ = best_centers</span>
        <span class="s1">self._n_features_out = self.cluster_centers_.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">self.labels_ = best_labels</span>
        <span class="s1">self.inertia_ = best_inertia</span>
        <span class="s1">self.n_iter_ = best_n_iter</span>
        <span class="s3">return </span><span class="s1">self</span>


<span class="s3">def </span><span class="s1">_mini_batch_step(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">sample_weight</span><span class="s3">,</span>
    <span class="s1">centers</span><span class="s3">,</span>
    <span class="s1">centers_new</span><span class="s3">,</span>
    <span class="s1">weight_sums</span><span class="s3">,</span>
    <span class="s1">random_state</span><span class="s3">,</span>
    <span class="s1">random_reassign=</span><span class="s3">False,</span>
    <span class="s1">reassignment_ratio=</span><span class="s5">0.01</span><span class="s3">,</span>
    <span class="s1">verbose=</span><span class="s3">False,</span>
    <span class="s1">n_threads=</span><span class="s5">1</span><span class="s3">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Incremental update of the centers for the Minibatch K-Means algorithm. 
 
    Parameters 
    ---------- 
 
    X : {ndarray, sparse matrix} of shape (n_samples, n_features) 
        The original data array. If sparse, must be in CSR format. 
 
    x_squared_norms : ndarray of shape (n_samples,) 
        Squared euclidean norm of each data point. 
 
    sample_weight : ndarray of shape (n_samples,) 
        The weights for each observation in `X`. 
 
    centers : ndarray of shape (n_clusters, n_features) 
        The cluster centers before the current iteration 
 
    centers_new : ndarray of shape (n_clusters, n_features) 
        The cluster centers after the current iteration. Modified in-place. 
 
    weight_sums : ndarray of shape (n_clusters,) 
        The vector in which we keep track of the numbers of points in a 
        cluster. This array is modified in place. 
 
    random_state : RandomState instance 
        Determines random number generation for low count centers reassignment. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    random_reassign : boolean, default=False 
        If True, centers with very low counts are randomly reassigned 
        to observations. 
 
    reassignment_ratio : float, default=0.01 
        Control the fraction of the maximum number of counts for a 
        center to be reassigned. A higher value means that low count 
        centers are more likely to be reassigned, which means that the 
        model will take longer to converge, but should converge in a 
        better clustering. 
 
    verbose : bool, default=False 
        Controls the verbosity. 
 
    n_threads : int, default=1 
        The number of OpenMP threads to use for the computation. 
 
    Returns 
    ------- 
    inertia : float 
        Sum of squared distances of samples to their closest cluster center. 
        The inertia is computed after finding the labels and before updating 
        the centers. 
    &quot;&quot;&quot;</span>
    <span class="s2"># Perform label assignment to nearest centers</span>
    <span class="s2"># For better efficiency, it's better to run _mini_batch_step in a</span>
    <span class="s2"># threadpool_limit context than using _labels_inertia_threadpool_limit here</span>
    <span class="s1">labels</span><span class="s3">, </span><span class="s1">inertia = _labels_inertia(X</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">n_threads=n_threads)</span>

    <span class="s2"># Update centers according to the labels</span>
    <span class="s3">if </span><span class="s1">sp.issparse(X):</span>
        <span class="s1">_minibatch_update_sparse(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">centers</span><span class="s3">, </span><span class="s1">centers_new</span><span class="s3">, </span><span class="s1">weight_sums</span><span class="s3">, </span><span class="s1">labels</span><span class="s3">, </span><span class="s1">n_threads</span>
        <span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">_minibatch_update_dense(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">sample_weight</span><span class="s3">,</span>
            <span class="s1">centers</span><span class="s3">,</span>
            <span class="s1">centers_new</span><span class="s3">,</span>
            <span class="s1">weight_sums</span><span class="s3">,</span>
            <span class="s1">labels</span><span class="s3">,</span>
            <span class="s1">n_threads</span><span class="s3">,</span>
        <span class="s1">)</span>

    <span class="s2"># Reassign clusters that have very low weight</span>
    <span class="s3">if </span><span class="s1">random_reassign </span><span class="s3">and </span><span class="s1">reassignment_ratio &gt; </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s1">to_reassign = weight_sums &lt; reassignment_ratio * weight_sums.max()</span>

        <span class="s2"># pick at most .5 * batch_size samples as new centers</span>
        <span class="s3">if </span><span class="s1">to_reassign.sum() &gt; </span><span class="s5">0.5 </span><span class="s1">* X.shape[</span><span class="s5">0</span><span class="s1">]:</span>
            <span class="s1">indices_dont_reassign = np.argsort(weight_sums)[int(</span><span class="s5">0.5 </span><span class="s1">* X.shape[</span><span class="s5">0</span><span class="s1">]) :]</span>
            <span class="s1">to_reassign[indices_dont_reassign] = </span><span class="s3">False</span>
        <span class="s1">n_reassigns = to_reassign.sum()</span>

        <span class="s3">if </span><span class="s1">n_reassigns:</span>
            <span class="s2"># Pick new clusters amongst observations with uniform probability</span>
            <span class="s1">new_centers = random_state.choice(</span>
                <span class="s1">X.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">replace=</span><span class="s3">False, </span><span class="s1">size=n_reassigns</span>
            <span class="s1">)</span>
            <span class="s3">if </span><span class="s1">verbose:</span>
                <span class="s1">print(</span><span class="s4">f&quot;[MiniBatchKMeans] Reassigning </span><span class="s3">{</span><span class="s1">n_reassigns</span><span class="s3">} </span><span class="s4">cluster centers.&quot;</span><span class="s1">)</span>

            <span class="s3">if </span><span class="s1">sp.issparse(X):</span>
                <span class="s1">assign_rows_csr(</span>
                    <span class="s1">X</span><span class="s3">,</span>
                    <span class="s1">new_centers.astype(np.intp</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">np.where(to_reassign)[</span><span class="s5">0</span><span class="s1">].astype(np.intp</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">centers_new</span><span class="s3">,</span>
                <span class="s1">)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">centers_new[to_reassign] = X[new_centers]</span>

        <span class="s2"># reset counts of reassigned centers, but don't reset them too small</span>
        <span class="s2"># to avoid instant reassignment. This is a pretty dirty hack as it</span>
        <span class="s2"># also modifies the learning rates.</span>
        <span class="s1">weight_sums[to_reassign] = np.min(weight_sums[~to_reassign])</span>

    <span class="s3">return </span><span class="s1">inertia</span>


<span class="s3">class </span><span class="s1">MiniBatchKMeans(_BaseKMeans):</span>
    <span class="s0">&quot;&quot;&quot; 
    Mini-Batch K-Means clustering. 
 
    Read more in the :ref:`User Guide &lt;mini_batch_kmeans&gt;`. 
 
    Parameters 
    ---------- 
 
    n_clusters : int, default=8 
        The number of clusters to form as well as the number of 
        centroids to generate. 
 
    init : {'k-means++', 'random'}, callable or array-like of shape \ 
            (n_clusters, n_features), default='k-means++' 
        Method for initialization: 
 
        'k-means++' : selects initial cluster centroids using sampling based on 
        an empirical probability distribution of the points' contribution to the 
        overall inertia. This technique speeds up convergence. The algorithm 
        implemented is &quot;greedy k-means++&quot;. It differs from the vanilla k-means++ 
        by making several trials at each sampling step and choosing the best centroid 
        among them. 
 
        'random': choose `n_clusters` observations (rows) at random from data 
        for the initial centroids. 
 
        If an array is passed, it should be of shape (n_clusters, n_features) 
        and gives the initial centers. 
 
        If a callable is passed, it should take arguments X, n_clusters and a 
        random state and return an initialization. 
 
    max_iter : int, default=100 
        Maximum number of iterations over the complete dataset before 
        stopping independently of any early stopping criterion heuristics. 
 
    batch_size : int, default=1024 
        Size of the mini batches. 
        For faster computations, you can set the ``batch_size`` greater than 
        256 * number of cores to enable parallelism on all cores. 
 
        .. versionchanged:: 1.0 
           `batch_size` default changed from 100 to 1024. 
 
    verbose : int, default=0 
        Verbosity mode. 
 
    compute_labels : bool, default=True 
        Compute label assignment and inertia for the complete dataset 
        once the minibatch optimization has converged in fit. 
 
    random_state : int, RandomState instance or None, default=None 
        Determines random number generation for centroid initialization and 
        random reassignment. Use an int to make the randomness deterministic. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    tol : float, default=0.0 
        Control early stopping based on the relative center changes as 
        measured by a smoothed, variance-normalized of the mean center 
        squared position changes. This early stopping heuristics is 
        closer to the one used for the batch variant of the algorithms 
        but induces a slight computational and memory overhead over the 
        inertia heuristic. 
 
        To disable convergence detection based on normalized center 
        change, set tol to 0.0 (default). 
 
    max_no_improvement : int, default=10 
        Control early stopping based on the consecutive number of mini 
        batches that does not yield an improvement on the smoothed inertia. 
 
        To disable convergence detection based on inertia, set 
        max_no_improvement to None. 
 
    init_size : int, default=None 
        Number of samples to randomly sample for speeding up the 
        initialization (sometimes at the expense of accuracy): the 
        only algorithm is initialized by running a batch KMeans on a 
        random subset of the data. This needs to be larger than n_clusters. 
 
        If `None`, the heuristic is `init_size = 3 * batch_size` if 
        `3 * batch_size &lt; n_clusters`, else `init_size = 3 * n_clusters`. 
 
    n_init : 'auto' or int, default=3 
        Number of random initializations that are tried. 
        In contrast to KMeans, the algorithm is only run once, using the best of 
        the `n_init` initializations as measured by inertia. Several runs are 
        recommended for sparse high-dimensional problems (see 
        :ref:`kmeans_sparse_high_dim`). 
 
        When `n_init='auto'`, the number of runs depends on the value of init: 
        3 if using `init='random'` or `init` is a callable; 
        1 if using `init='k-means++'` or `init` is an array-like. 
 
        .. versionadded:: 1.2 
           Added 'auto' option for `n_init`. 
 
        .. versionchanged:: 1.4 
           Default value for `n_init` will change from 3 to `'auto'` in version 1.4. 
 
    reassignment_ratio : float, default=0.01 
        Control the fraction of the maximum number of counts for a center to 
        be reassigned. A higher value means that low count centers are more 
        easily reassigned, which means that the model will take longer to 
        converge, but should converge in a better clustering. However, too high 
        a value may cause convergence issues, especially with a small batch 
        size. 
 
    Attributes 
    ---------- 
 
    cluster_centers_ : ndarray of shape (n_clusters, n_features) 
        Coordinates of cluster centers. 
 
    labels_ : ndarray of shape (n_samples,) 
        Labels of each point (if compute_labels is set to True). 
 
    inertia_ : float 
        The value of the inertia criterion associated with the chosen 
        partition if compute_labels is set to True. If compute_labels is set to 
        False, it's an approximation of the inertia based on an exponentially 
        weighted average of the batch inertiae. 
        The inertia is defined as the sum of square distances of samples to 
        their cluster center, weighted by the sample weights if provided. 
 
    n_iter_ : int 
        Number of iterations over the full dataset. 
 
    n_steps_ : int 
        Number of minibatches processed. 
 
        .. versionadded:: 1.0 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    KMeans : The classic implementation of the clustering method based on the 
        Lloyd's algorithm. It consumes the whole set of input data at each 
        iteration. 
 
    Notes 
    ----- 
    See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf 
 
    When there are too few points in the dataset, some centers may be 
    duplicated, which means that a proper clustering in terms of the number 
    of requesting clusters and the number of returned clusters will not 
    always match. One solution is to set `reassignment_ratio=0`, which 
    prevents reassignments of clusters that are too small. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.cluster import MiniBatchKMeans 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; X = np.array([[1, 2], [1, 4], [1, 0], 
    ...               [4, 2], [4, 0], [4, 4], 
    ...               [4, 5], [0, 1], [2, 2], 
    ...               [3, 2], [5, 5], [1, -1]]) 
    &gt;&gt;&gt; # manually fit on batches 
    &gt;&gt;&gt; kmeans = MiniBatchKMeans(n_clusters=2, 
    ...                          random_state=0, 
    ...                          batch_size=6, 
    ...                          n_init=&quot;auto&quot;) 
    &gt;&gt;&gt; kmeans = kmeans.partial_fit(X[0:6,:]) 
    &gt;&gt;&gt; kmeans = kmeans.partial_fit(X[6:12,:]) 
    &gt;&gt;&gt; kmeans.cluster_centers_ 
    array([[3.375, 3.  ], 
           [0.75 , 0.5 ]]) 
    &gt;&gt;&gt; kmeans.predict([[0, 0], [4, 4]]) 
    array([1, 0], dtype=int32) 
    &gt;&gt;&gt; # fit on the whole data 
    &gt;&gt;&gt; kmeans = MiniBatchKMeans(n_clusters=2, 
    ...                          random_state=0, 
    ...                          batch_size=6, 
    ...                          max_iter=10, 
    ...                          n_init=&quot;auto&quot;).fit(X) 
    &gt;&gt;&gt; kmeans.cluster_centers_ 
    array([[3.55102041, 2.48979592], 
           [1.06896552, 1.        ]]) 
    &gt;&gt;&gt; kmeans.predict([[0, 0], [4, 4]]) 
    array([1, 0], dtype=int32) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**_BaseKMeans._parameter_constraints</span><span class="s3">,</span>
        <span class="s4">&quot;batch_size&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;compute_labels&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;max_no_improvement&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;init_size&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;reassignment_ratio&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">n_clusters=</span><span class="s5">8</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">init=</span><span class="s4">&quot;k-means++&quot;</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s5">100</span><span class="s3">,</span>
        <span class="s1">batch_size=</span><span class="s5">1024</span><span class="s3">,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">compute_labels=</span><span class="s3">True,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
        <span class="s1">tol=</span><span class="s5">0.0</span><span class="s3">,</span>
        <span class="s1">max_no_improvement=</span><span class="s5">10</span><span class="s3">,</span>
        <span class="s1">init_size=</span><span class="s3">None,</span>
        <span class="s1">n_init=</span><span class="s4">&quot;warn&quot;</span><span class="s3">,</span>
        <span class="s1">reassignment_ratio=</span><span class="s5">0.01</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">n_clusters=n_clusters</span><span class="s3">,</span>
            <span class="s1">init=init</span><span class="s3">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
            <span class="s1">tol=tol</span><span class="s3">,</span>
            <span class="s1">n_init=n_init</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s1">self.max_no_improvement = max_no_improvement</span>
        <span class="s1">self.batch_size = batch_size</span>
        <span class="s1">self.compute_labels = compute_labels</span>
        <span class="s1">self.init_size = init_size</span>
        <span class="s1">self.reassignment_ratio = reassignment_ratio</span>

    <span class="s3">def </span><span class="s1">_check_params_vs_input(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s1">super()._check_params_vs_input(X</span><span class="s3">, </span><span class="s1">default_n_init=</span><span class="s5">3</span><span class="s1">)</span>

        <span class="s1">self._batch_size = min(self.batch_size</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s5">0</span><span class="s1">])</span>

        <span class="s2"># init_size</span>
        <span class="s1">self._init_size = self.init_size</span>
        <span class="s3">if </span><span class="s1">self._init_size </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self._init_size = </span><span class="s5">3 </span><span class="s1">* self._batch_size</span>
            <span class="s3">if </span><span class="s1">self._init_size &lt; self.n_clusters:</span>
                <span class="s1">self._init_size = </span><span class="s5">3 </span><span class="s1">* self.n_clusters</span>
        <span class="s3">elif </span><span class="s1">self._init_size &lt; self.n_clusters:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s4">f&quot;init_size=</span><span class="s3">{</span><span class="s1">self._init_size</span><span class="s3">} </span><span class="s4">should be larger than &quot;</span>
                    <span class="s4">f&quot;n_clusters=</span><span class="s3">{</span><span class="s1">self.n_clusters</span><span class="s3">}</span><span class="s4">. Setting it to &quot;</span>
                    <span class="s4">&quot;min(3*n_clusters, n_samples)&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">RuntimeWarning</span><span class="s3">,</span>
                <span class="s1">stacklevel=</span><span class="s5">2</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">self._init_size = </span><span class="s5">3 </span><span class="s1">* self.n_clusters</span>
        <span class="s1">self._init_size = min(self._init_size</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s5">0</span><span class="s1">])</span>

        <span class="s2"># reassignment_ratio</span>
        <span class="s3">if </span><span class="s1">self.reassignment_ratio &lt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;reassignment_ratio should be &gt;= 0, got &quot;</span>
                <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">self.reassignment_ratio</span><span class="s3">} </span><span class="s4">instead.&quot;</span>
            <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_warn_mkl_vcomp(self</span><span class="s3">, </span><span class="s1">n_active_threads):</span>
        <span class="s0">&quot;&quot;&quot;Warn when vcomp and mkl are both present&quot;&quot;&quot;</span>
        <span class="s1">warnings.warn(</span>
            <span class="s4">&quot;MiniBatchKMeans is known to have a memory leak on &quot;</span>
            <span class="s4">&quot;Windows with MKL, when there are less chunks than &quot;</span>
            <span class="s4">&quot;available threads. You can prevent it by setting &quot;</span>
            <span class="s4">f&quot;batch_size &gt;= </span><span class="s3">{</span><span class="s1">self._n_threads * CHUNK_SIZE</span><span class="s3">} </span><span class="s4">or by &quot;</span>
            <span class="s4">&quot;setting the environment variable &quot;</span>
            <span class="s4">f&quot;OMP_NUM_THREADS=</span><span class="s3">{</span><span class="s1">n_active_threads</span><span class="s3">}</span><span class="s4">&quot;</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_mini_batch_convergence(</span>
        <span class="s1">self</span><span class="s3">, </span><span class="s1">step</span><span class="s3">, </span><span class="s1">n_steps</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">centers_squared_diff</span><span class="s3">, </span><span class="s1">batch_inertia</span>
    <span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Helper function to encapsulate the early stopping logic&quot;&quot;&quot;</span>
        <span class="s2"># Normalize inertia to be able to compare values when</span>
        <span class="s2"># batch_size changes</span>
        <span class="s1">batch_inertia /= self._batch_size</span>

        <span class="s2"># count steps starting from 1 for user friendly verbose mode.</span>
        <span class="s1">step = step + </span><span class="s5">1</span>

        <span class="s2"># Ignore first iteration because it's inertia from initialization.</span>
        <span class="s3">if </span><span class="s1">step == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">print(</span>
                    <span class="s4">f&quot;Minibatch step </span><span class="s3">{</span><span class="s1">step</span><span class="s3">}</span><span class="s4">/</span><span class="s3">{</span><span class="s1">n_steps</span><span class="s3">}</span><span class="s4">: mean batch &quot;</span>
                    <span class="s4">f&quot;inertia: </span><span class="s3">{</span><span class="s1">batch_inertia</span><span class="s3">}</span><span class="s4">&quot;</span>
                <span class="s1">)</span>
            <span class="s3">return False</span>

        <span class="s2"># Compute an Exponentially Weighted Average of the inertia to</span>
        <span class="s2"># monitor the convergence while discarding minibatch-local stochastic</span>
        <span class="s2"># variability: https://en.wikipedia.org/wiki/Moving_average</span>
        <span class="s3">if </span><span class="s1">self._ewa_inertia </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self._ewa_inertia = batch_inertia</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = self._batch_size * </span><span class="s5">2.0 </span><span class="s1">/ (n_samples + </span><span class="s5">1</span><span class="s1">)</span>
            <span class="s1">alpha = min(alpha</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
            <span class="s1">self._ewa_inertia = self._ewa_inertia * (</span><span class="s5">1 </span><span class="s1">- alpha) + batch_inertia * alpha</span>

        <span class="s2"># Log progress to be able to monitor convergence</span>
        <span class="s3">if </span><span class="s1">self.verbose:</span>
            <span class="s1">print(</span>
                <span class="s4">f&quot;Minibatch step </span><span class="s3">{</span><span class="s1">step</span><span class="s3">}</span><span class="s4">/</span><span class="s3">{</span><span class="s1">n_steps</span><span class="s3">}</span><span class="s4">: mean batch inertia: &quot;</span>
                <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">batch_inertia</span><span class="s3">}</span><span class="s4">, ewa inertia: </span><span class="s3">{</span><span class="s1">self._ewa_inertia</span><span class="s3">}</span><span class="s4">&quot;</span>
            <span class="s1">)</span>

        <span class="s2"># Early stopping based on absolute tolerance on squared change of</span>
        <span class="s2"># centers position</span>
        <span class="s3">if </span><span class="s1">self._tol &gt; </span><span class="s5">0.0 </span><span class="s3">and </span><span class="s1">centers_squared_diff &lt;= self._tol:</span>
            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">print(</span><span class="s4">f&quot;Converged (small centers change) at step </span><span class="s3">{</span><span class="s1">step</span><span class="s3">}</span><span class="s4">/</span><span class="s3">{</span><span class="s1">n_steps</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
            <span class="s3">return True</span>

        <span class="s2"># Early stopping heuristic due to lack of improvement on smoothed</span>
        <span class="s2"># inertia</span>
        <span class="s3">if </span><span class="s1">self._ewa_inertia_min </span><span class="s3">is None or </span><span class="s1">self._ewa_inertia &lt; self._ewa_inertia_min:</span>
            <span class="s1">self._no_improvement = </span><span class="s5">0</span>
            <span class="s1">self._ewa_inertia_min = self._ewa_inertia</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._no_improvement += </span><span class="s5">1</span>

        <span class="s3">if </span><span class="s1">(</span>
            <span class="s1">self.max_no_improvement </span><span class="s3">is not None</span>
            <span class="s3">and </span><span class="s1">self._no_improvement &gt;= self.max_no_improvement</span>
        <span class="s1">):</span>
            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">print(</span>
                    <span class="s4">&quot;Converged (lack of improvement in inertia) at step &quot;</span>
                    <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">step</span><span class="s3">}</span><span class="s4">/</span><span class="s3">{</span><span class="s1">n_steps</span><span class="s3">}</span><span class="s4">&quot;</span>
                <span class="s1">)</span>
            <span class="s3">return True</span>

        <span class="s3">return False</span>

    <span class="s3">def </span><span class="s1">_random_reassign(self):</span>
        <span class="s0">&quot;&quot;&quot;Check if a random reassignment needs to be done. 
 
        Do random reassignments each time 10 * n_clusters samples have been 
        processed. 
 
        If there are empty clusters we always want to reassign. 
        &quot;&quot;&quot;</span>
        <span class="s1">self._n_since_last_reassign += self._batch_size</span>
        <span class="s3">if </span><span class="s1">(self._counts == </span><span class="s5">0</span><span class="s1">).any() </span><span class="s3">or </span><span class="s1">self._n_since_last_reassign &gt;= (</span>
            <span class="s5">10 </span><span class="s1">* self.n_clusters</span>
        <span class="s1">):</span>
            <span class="s1">self._n_since_last_reassign = </span><span class="s5">0</span>
            <span class="s3">return True</span>
        <span class="s3">return False</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Compute the centroids on X by chunking it into mini-batches. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training instances to cluster. It must be noted that the data 
            will be converted to C ordering, which will cause a memory copy 
            if the given data is not C-contiguous. 
            If a sparse matrix is passed, a copy will be made if it's not in 
            CSR format. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            The weights for each observation in X. If None, all observations 
            are assigned equal weight. `sample_weight` is not used during 
            initialization if `init` is a callable or a user provided array. 
 
            .. versionadded:: 0.20 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">,</span>
            <span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">,</span>
            <span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s3">,</span>
            <span class="s1">accept_large_sparse=</span><span class="s3">False,</span>
        <span class="s1">)</span>

        <span class="s1">self._check_params_vs_input(X)</span>
        <span class="s1">random_state = check_random_state(self.random_state)</span>
        <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s1">self._n_threads = _openmp_effective_n_threads()</span>
        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>

        <span class="s2"># Validate init array</span>
        <span class="s1">init = self.init</span>
        <span class="s3">if </span><span class="s1">_is_arraylike_not_scalar(init):</span>
            <span class="s1">init = check_array(init</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">True, </span><span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s1">)</span>
            <span class="s1">self._validate_center_shape(X</span><span class="s3">, </span><span class="s1">init)</span>

        <span class="s1">self._check_mkl_vcomp(X</span><span class="s3">, </span><span class="s1">self._batch_size)</span>

        <span class="s2"># precompute squared norms of data points</span>
        <span class="s1">x_squared_norms = row_norms(X</span><span class="s3">, </span><span class="s1">squared=</span><span class="s3">True</span><span class="s1">)</span>

        <span class="s2"># Validation set for the init</span>
        <span class="s1">validation_indices = random_state.randint(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">self._init_size)</span>
        <span class="s1">X_valid = X[validation_indices]</span>
        <span class="s1">sample_weight_valid = sample_weight[validation_indices]</span>

        <span class="s2"># perform several inits with random subsets</span>
        <span class="s1">best_inertia = </span><span class="s3">None</span>
        <span class="s3">for </span><span class="s1">init_idx </span><span class="s3">in </span><span class="s1">range(self._n_init):</span>
            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">print(</span><span class="s4">f&quot;Init </span><span class="s3">{</span><span class="s1">init_idx + </span><span class="s5">1</span><span class="s3">}</span><span class="s4">/</span><span class="s3">{</span><span class="s1">self._n_init</span><span class="s3">} </span><span class="s4">with method </span><span class="s3">{</span><span class="s1">init</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>

            <span class="s2"># Initialize the centers using only a fraction of the data as we</span>
            <span class="s2"># expect n_samples to be very large when using MiniBatchKMeans.</span>
            <span class="s1">cluster_centers = self._init_centroids(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">x_squared_norms=x_squared_norms</span><span class="s3">,</span>
                <span class="s1">init=init</span><span class="s3">,</span>
                <span class="s1">random_state=random_state</span><span class="s3">,</span>
                <span class="s1">init_size=self._init_size</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
            <span class="s1">)</span>

            <span class="s2"># Compute inertia on a validation set.</span>
            <span class="s1">_</span><span class="s3">, </span><span class="s1">inertia = _labels_inertia_threadpool_limit(</span>
                <span class="s1">X_valid</span><span class="s3">,</span>
                <span class="s1">sample_weight_valid</span><span class="s3">,</span>
                <span class="s1">cluster_centers</span><span class="s3">,</span>
                <span class="s1">n_threads=self._n_threads</span><span class="s3">,</span>
            <span class="s1">)</span>

            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">print(</span><span class="s4">f&quot;Inertia for init </span><span class="s3">{</span><span class="s1">init_idx + </span><span class="s5">1</span><span class="s3">}</span><span class="s4">/</span><span class="s3">{</span><span class="s1">self._n_init</span><span class="s3">}</span><span class="s4">: </span><span class="s3">{</span><span class="s1">inertia</span><span class="s3">}</span><span class="s4">&quot;</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">best_inertia </span><span class="s3">is None or </span><span class="s1">inertia &lt; best_inertia:</span>
                <span class="s1">init_centers = cluster_centers</span>
                <span class="s1">best_inertia = inertia</span>

        <span class="s1">centers = init_centers</span>
        <span class="s1">centers_new = np.empty_like(centers)</span>

        <span class="s2"># Initialize counts</span>
        <span class="s1">self._counts = np.zeros(self.n_clusters</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s2"># Attributes to monitor the convergence</span>
        <span class="s1">self._ewa_inertia = </span><span class="s3">None</span>
        <span class="s1">self._ewa_inertia_min = </span><span class="s3">None</span>
        <span class="s1">self._no_improvement = </span><span class="s5">0</span>

        <span class="s2"># Initialize number of samples seen since last reassignment</span>
        <span class="s1">self._n_since_last_reassign = </span><span class="s5">0</span>

        <span class="s1">n_steps = (self.max_iter * n_samples) // self._batch_size</span>

        <span class="s3">with </span><span class="s1">threadpool_limits(limits=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">user_api=</span><span class="s4">&quot;blas&quot;</span><span class="s1">):</span>
            <span class="s2"># Perform the iterative optimization until convergence</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_steps):</span>
                <span class="s2"># Sample a minibatch from the full dataset</span>
                <span class="s1">minibatch_indices = random_state.randint(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">self._batch_size)</span>

                <span class="s2"># Perform the actual update step on the minibatch data</span>
                <span class="s1">batch_inertia = _mini_batch_step(</span>
                    <span class="s1">X=X[minibatch_indices]</span><span class="s3">,</span>
                    <span class="s1">sample_weight=sample_weight[minibatch_indices]</span><span class="s3">,</span>
                    <span class="s1">centers=centers</span><span class="s3">,</span>
                    <span class="s1">centers_new=centers_new</span><span class="s3">,</span>
                    <span class="s1">weight_sums=self._counts</span><span class="s3">,</span>
                    <span class="s1">random_state=random_state</span><span class="s3">,</span>
                    <span class="s1">random_reassign=self._random_reassign()</span><span class="s3">,</span>
                    <span class="s1">reassignment_ratio=self.reassignment_ratio</span><span class="s3">,</span>
                    <span class="s1">verbose=self.verbose</span><span class="s3">,</span>
                    <span class="s1">n_threads=self._n_threads</span><span class="s3">,</span>
                <span class="s1">)</span>

                <span class="s3">if </span><span class="s1">self._tol &gt; </span><span class="s5">0.0</span><span class="s1">:</span>
                    <span class="s1">centers_squared_diff = np.sum((centers_new - centers) ** </span><span class="s5">2</span><span class="s1">)</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">centers_squared_diff = </span><span class="s5">0</span>

                <span class="s1">centers</span><span class="s3">, </span><span class="s1">centers_new = centers_new</span><span class="s3">, </span><span class="s1">centers</span>

                <span class="s2"># Monitor convergence and do early stopping if necessary</span>
                <span class="s3">if </span><span class="s1">self._mini_batch_convergence(</span>
                    <span class="s1">i</span><span class="s3">, </span><span class="s1">n_steps</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">centers_squared_diff</span><span class="s3">, </span><span class="s1">batch_inertia</span>
                <span class="s1">):</span>
                    <span class="s3">break</span>

        <span class="s1">self.cluster_centers_ = centers</span>
        <span class="s1">self._n_features_out = self.cluster_centers_.shape[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s1">self.n_steps_ = i + </span><span class="s5">1</span>
        <span class="s1">self.n_iter_ = int(np.ceil(((i + </span><span class="s5">1</span><span class="s1">) * self._batch_size) / n_samples))</span>

        <span class="s3">if </span><span class="s1">self.compute_labels:</span>
            <span class="s1">self.labels_</span><span class="s3">, </span><span class="s1">self.inertia_ = _labels_inertia_threadpool_limit(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">sample_weight</span><span class="s3">,</span>
                <span class="s1">self.cluster_centers_</span><span class="s3">,</span>
                <span class="s1">n_threads=self._n_threads</span><span class="s3">,</span>
            <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.inertia_ = self._ewa_inertia * n_samples</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">partial_fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Update k means estimate on a single mini-batch X. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training instances to cluster. It must be noted that the data 
            will be converted to C ordering, which will cause a memory copy 
            if the given data is not C-contiguous. 
            If a sparse matrix is passed, a copy will be made if it's not in 
            CSR format. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            The weights for each observation in X. If None, all observations 
            are assigned equal weight. `sample_weight` is not used during 
            initialization if `init` is a callable or a user provided array. 
 
        Returns 
        ------- 
        self : object 
            Return updated estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">has_centers = hasattr(self</span><span class="s3">, </span><span class="s4">&quot;cluster_centers_&quot;</span><span class="s1">)</span>

        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">,</span>
            <span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">,</span>
            <span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s3">,</span>
            <span class="s1">accept_large_sparse=</span><span class="s3">False,</span>
            <span class="s1">reset=</span><span class="s3">not </span><span class="s1">has_centers</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s1">self._random_state = getattr(</span>
            <span class="s1">self</span><span class="s3">, </span><span class="s4">&quot;_random_state&quot;</span><span class="s3">, </span><span class="s1">check_random_state(self.random_state)</span>
        <span class="s1">)</span>
        <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s1">self.n_steps_ = getattr(self</span><span class="s3">, </span><span class="s4">&quot;n_steps_&quot;</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>

        <span class="s2"># precompute squared norms of data points</span>
        <span class="s1">x_squared_norms = row_norms(X</span><span class="s3">, </span><span class="s1">squared=</span><span class="s3">True</span><span class="s1">)</span>

        <span class="s3">if not </span><span class="s1">has_centers:</span>
            <span class="s2"># this instance has not been fitted yet (fit or partial_fit)</span>
            <span class="s1">self._check_params_vs_input(X)</span>
            <span class="s1">self._n_threads = _openmp_effective_n_threads()</span>

            <span class="s2"># Validate init array</span>
            <span class="s1">init = self.init</span>
            <span class="s3">if </span><span class="s1">_is_arraylike_not_scalar(init):</span>
                <span class="s1">init = check_array(init</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">True, </span><span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s1">)</span>
                <span class="s1">self._validate_center_shape(X</span><span class="s3">, </span><span class="s1">init)</span>

            <span class="s1">self._check_mkl_vcomp(X</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s5">0</span><span class="s1">])</span>

            <span class="s2"># initialize the cluster centers</span>
            <span class="s1">self.cluster_centers_ = self._init_centroids(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">x_squared_norms=x_squared_norms</span><span class="s3">,</span>
                <span class="s1">init=init</span><span class="s3">,</span>
                <span class="s1">random_state=self._random_state</span><span class="s3">,</span>
                <span class="s1">init_size=self._init_size</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
            <span class="s1">)</span>

            <span class="s2"># Initialize counts</span>
            <span class="s1">self._counts = np.zeros(self.n_clusters</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

            <span class="s2"># Initialize number of samples seen since last reassignment</span>
            <span class="s1">self._n_since_last_reassign = </span><span class="s5">0</span>

        <span class="s3">with </span><span class="s1">threadpool_limits(limits=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">user_api=</span><span class="s4">&quot;blas&quot;</span><span class="s1">):</span>
            <span class="s1">_mini_batch_step(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
                <span class="s1">centers=self.cluster_centers_</span><span class="s3">,</span>
                <span class="s1">centers_new=self.cluster_centers_</span><span class="s3">,</span>
                <span class="s1">weight_sums=self._counts</span><span class="s3">,</span>
                <span class="s1">random_state=self._random_state</span><span class="s3">,</span>
                <span class="s1">random_reassign=self._random_reassign()</span><span class="s3">,</span>
                <span class="s1">reassignment_ratio=self.reassignment_ratio</span><span class="s3">,</span>
                <span class="s1">verbose=self.verbose</span><span class="s3">,</span>
                <span class="s1">n_threads=self._n_threads</span><span class="s3">,</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.compute_labels:</span>
            <span class="s1">self.labels_</span><span class="s3">, </span><span class="s1">self.inertia_ = _labels_inertia_threadpool_limit(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">sample_weight</span><span class="s3">,</span>
                <span class="s1">self.cluster_centers_</span><span class="s3">,</span>
                <span class="s1">n_threads=self._n_threads</span><span class="s3">,</span>
            <span class="s1">)</span>

        <span class="s1">self.n_steps_ += </span><span class="s5">1</span>
        <span class="s1">self._n_features_out = self.cluster_centers_.shape[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s3">return </span><span class="s1">self</span>
</pre>
</body>
</html>