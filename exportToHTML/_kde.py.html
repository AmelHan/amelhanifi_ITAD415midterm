<html>
<head>
<title>_kde.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_kde.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Kernel Density Estimation 
------------------------- 
&quot;&quot;&quot;</span>
<span class="s2"># Author: Jake Vanderplas &lt;jakevdp@cs.washington.edu&gt;</span>
<span class="s3">import </span><span class="s1">itertools</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy.special </span><span class="s3">import </span><span class="s1">gammainc</span>

<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">BaseEstimator</span><span class="s3">, </span><span class="s1">_fit_context</span>
<span class="s3">from </span><span class="s1">..neighbors._base </span><span class="s3">import </span><span class="s1">VALID_METRICS</span>
<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">check_random_state</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">StrOptions</span>
<span class="s3">from </span><span class="s1">..utils.extmath </span><span class="s3">import </span><span class="s1">row_norms</span>
<span class="s3">from </span><span class="s1">..utils.validation </span><span class="s3">import </span><span class="s1">_check_sample_weight</span><span class="s3">, </span><span class="s1">check_is_fitted</span>
<span class="s3">from </span><span class="s1">._ball_tree </span><span class="s3">import </span><span class="s1">BallTree</span>
<span class="s3">from </span><span class="s1">._kd_tree </span><span class="s3">import </span><span class="s1">KDTree</span>

<span class="s1">VALID_KERNELS = [</span>
    <span class="s4">&quot;gaussian&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;tophat&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;epanechnikov&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;exponential&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;linear&quot;</span><span class="s3">,</span>
    <span class="s4">&quot;cosine&quot;</span><span class="s3">,</span>
<span class="s1">]</span>

<span class="s1">TREE_DICT = {</span><span class="s4">&quot;ball_tree&quot;</span><span class="s1">: BallTree</span><span class="s3">, </span><span class="s4">&quot;kd_tree&quot;</span><span class="s1">: KDTree}</span>


<span class="s2"># TODO: implement a brute force version for testing purposes</span>
<span class="s2"># TODO: create a density estimation base class?</span>
<span class="s3">class </span><span class="s1">KernelDensity(BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Kernel Density Estimation. 
 
    Read more in the :ref:`User Guide &lt;kernel_density&gt;`. 
 
    Parameters 
    ---------- 
    bandwidth : float or {&quot;scott&quot;, &quot;silverman&quot;}, default=1.0 
        The bandwidth of the kernel. If bandwidth is a float, it defines the 
        bandwidth of the kernel. If bandwidth is a string, one of the estimation 
        methods is implemented. 
 
    algorithm : {'kd_tree', 'ball_tree', 'auto'}, default='auto' 
        The tree algorithm to use. 
 
    kernel : {'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', \ 
                 'cosine'}, default='gaussian' 
        The kernel to use. 
 
    metric : str, default='euclidean' 
        Metric to use for distance computation. See the 
        documentation of `scipy.spatial.distance 
        &lt;https://docs.scipy.org/doc/scipy/reference/spatial.distance.html&gt;`_ and 
        the metrics listed in 
        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric 
        values. 
 
        Not all metrics are valid with all algorithms: refer to the 
        documentation of :class:`BallTree` and :class:`KDTree`. Note that the 
        normalization of the density output is correct only for the Euclidean 
        distance metric. 
 
    atol : float, default=0 
        The desired absolute tolerance of the result.  A larger tolerance will 
        generally lead to faster execution. 
 
    rtol : float, default=0 
        The desired relative tolerance of the result.  A larger tolerance will 
        generally lead to faster execution. 
 
    breadth_first : bool, default=True 
        If true (default), use a breadth-first approach to the problem. 
        Otherwise use a depth-first approach. 
 
    leaf_size : int, default=40 
        Specify the leaf size of the underlying tree.  See :class:`BallTree` 
        or :class:`KDTree` for details. 
 
    metric_params : dict, default=None 
        Additional parameters to be passed to the tree for use with the 
        metric.  For more information, see the documentation of 
        :class:`BallTree` or :class:`KDTree`. 
 
    Attributes 
    ---------- 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    tree_ : ``BinaryTree`` instance 
        The tree algorithm for fast generalized N-point problems. 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
    bandwidth_ : float 
        Value of the bandwidth, given directly by the bandwidth parameter or 
        estimated using the 'scott' or 'silverman' method. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    sklearn.neighbors.KDTree : K-dimensional tree for fast generalized N-point 
        problems. 
    sklearn.neighbors.BallTree : Ball tree for fast generalized N-point 
        problems. 
 
    Examples 
    -------- 
    Compute a gaussian kernel density estimate with a fixed bandwidth. 
 
    &gt;&gt;&gt; from sklearn.neighbors import KernelDensity 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; rng = np.random.RandomState(42) 
    &gt;&gt;&gt; X = rng.random_sample((100, 3)) 
    &gt;&gt;&gt; kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X) 
    &gt;&gt;&gt; log_density = kde.score_samples(X[:3]) 
    &gt;&gt;&gt; log_density 
    array([-1.52955942, -1.51462041, -1.60244657]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;bandwidth&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">StrOptions({</span><span class="s4">&quot;scott&quot;</span><span class="s3">, </span><span class="s4">&quot;silverman&quot;</span><span class="s1">})</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;algorithm&quot;</span><span class="s1">: [StrOptions(set(TREE_DICT.keys()) | {</span><span class="s4">&quot;auto&quot;</span><span class="s1">})]</span><span class="s3">,</span>
        <span class="s4">&quot;kernel&quot;</span><span class="s1">: [StrOptions(set(VALID_KERNELS))]</span><span class="s3">,</span>
        <span class="s4">&quot;metric&quot;</span><span class="s1">: [</span>
            <span class="s1">StrOptions(</span>
                <span class="s1">set(itertools.chain(*[VALID_METRICS[alg] </span><span class="s3">for </span><span class="s1">alg </span><span class="s3">in </span><span class="s1">TREE_DICT.keys()]))</span>
            <span class="s1">)</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;atol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;rtol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;breadth_first&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;leaf_size&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;metric_params&quot;</span><span class="s1">: [</span><span class="s3">None, </span><span class="s1">dict]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">bandwidth=</span><span class="s5">1.0</span><span class="s3">,</span>
        <span class="s1">algorithm=</span><span class="s4">&quot;auto&quot;</span><span class="s3">,</span>
        <span class="s1">kernel=</span><span class="s4">&quot;gaussian&quot;</span><span class="s3">,</span>
        <span class="s1">metric=</span><span class="s4">&quot;euclidean&quot;</span><span class="s3">,</span>
        <span class="s1">atol=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">rtol=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">breadth_first=</span><span class="s3">True,</span>
        <span class="s1">leaf_size=</span><span class="s5">40</span><span class="s3">,</span>
        <span class="s1">metric_params=</span><span class="s3">None,</span>
    <span class="s1">):</span>
        <span class="s1">self.algorithm = algorithm</span>
        <span class="s1">self.bandwidth = bandwidth</span>
        <span class="s1">self.kernel = kernel</span>
        <span class="s1">self.metric = metric</span>
        <span class="s1">self.atol = atol</span>
        <span class="s1">self.rtol = rtol</span>
        <span class="s1">self.breadth_first = breadth_first</span>
        <span class="s1">self.leaf_size = leaf_size</span>
        <span class="s1">self.metric_params = metric_params</span>

    <span class="s3">def </span><span class="s1">_choose_algorithm(self</span><span class="s3">, </span><span class="s1">algorithm</span><span class="s3">, </span><span class="s1">metric):</span>
        <span class="s2"># given the algorithm string + metric string, choose the optimal</span>
        <span class="s2"># algorithm to compute the result.</span>
        <span class="s3">if </span><span class="s1">algorithm == </span><span class="s4">&quot;auto&quot;</span><span class="s1">:</span>
            <span class="s2"># use KD Tree if possible</span>
            <span class="s3">if </span><span class="s1">metric </span><span class="s3">in </span><span class="s1">KDTree.valid_metrics:</span>
                <span class="s3">return </span><span class="s4">&quot;kd_tree&quot;</span>
            <span class="s3">elif </span><span class="s1">metric </span><span class="s3">in </span><span class="s1">BallTree.valid_metrics:</span>
                <span class="s3">return </span><span class="s4">&quot;ball_tree&quot;</span>
        <span class="s3">else</span><span class="s1">:  </span><span class="s2"># kd_tree or ball_tree</span>
            <span class="s3">if </span><span class="s1">metric </span><span class="s3">not in </span><span class="s1">TREE_DICT[algorithm].valid_metrics:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;invalid metric for {0}: '{1}'&quot;</span><span class="s1">.format(TREE_DICT[algorithm]</span><span class="s3">, </span><span class="s1">metric)</span>
                <span class="s1">)</span>
            <span class="s3">return </span><span class="s1">algorithm</span>

    <span class="s1">@_fit_context(</span>
        <span class="s2"># KernelDensity.metric is not validated yet</span>
        <span class="s1">prefer_skip_nested_validation=</span><span class="s3">False</span>
    <span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the Kernel Density model on the data. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            List of n_features-dimensional data points.  Each row 
            corresponds to a single data point. 
 
        y : None 
            Ignored. This parameter exists only for compatibility with 
            :class:`~sklearn.pipeline.Pipeline`. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            List of sample weights attached to the data X. 
 
            .. versionadded:: 0.20 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">algorithm = self._choose_algorithm(self.algorithm</span><span class="s3">, </span><span class="s1">self.metric)</span>

        <span class="s3">if </span><span class="s1">isinstance(self.bandwidth</span><span class="s3">, </span><span class="s1">str):</span>
            <span class="s3">if </span><span class="s1">self.bandwidth == </span><span class="s4">&quot;scott&quot;</span><span class="s1">:</span>
                <span class="s1">self.bandwidth_ = X.shape[</span><span class="s5">0</span><span class="s1">] ** (-</span><span class="s5">1 </span><span class="s1">/ (X.shape[</span><span class="s5">1</span><span class="s1">] + </span><span class="s5">4</span><span class="s1">))</span>
            <span class="s3">elif </span><span class="s1">self.bandwidth == </span><span class="s4">&quot;silverman&quot;</span><span class="s1">:</span>
                <span class="s1">self.bandwidth_ = (X.shape[</span><span class="s5">0</span><span class="s1">] * (X.shape[</span><span class="s5">1</span><span class="s1">] + </span><span class="s5">2</span><span class="s1">) / </span><span class="s5">4</span><span class="s1">) ** (</span>
                    <span class="s1">-</span><span class="s5">1 </span><span class="s1">/ (X.shape[</span><span class="s5">1</span><span class="s1">] + </span><span class="s5">4</span><span class="s1">)</span>
                <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.bandwidth_ = self.bandwidth</span>

        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s3">, </span><span class="s1">dtype=np.float64)</span>

        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">sample_weight = _check_sample_weight(</span>
                <span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=np.float64</span><span class="s3">, </span><span class="s1">only_non_negative=</span><span class="s3">True</span>
            <span class="s1">)</span>

        <span class="s1">kwargs = self.metric_params</span>
        <span class="s3">if </span><span class="s1">kwargs </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">kwargs = {}</span>
        <span class="s1">self.tree_ = TREE_DICT[algorithm](</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">metric=self.metric</span><span class="s3">,</span>
            <span class="s1">leaf_size=self.leaf_size</span><span class="s3">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
            <span class="s1">**kwargs</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">score_samples(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Compute the log-likelihood of each sample under the model. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            An array of points to query.  Last dimension should match dimension 
            of training data (n_features). 
 
        Returns 
        ------- 
        density : ndarray of shape (n_samples,) 
            Log-likelihood of each sample in `X`. These are normalized to be 
            probability densities, so values will be low for high-dimensional 
            data. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s2"># The returned density is normalized to the number of points.</span>
        <span class="s2"># For it to be a probability, we must scale it.  For this reason</span>
        <span class="s2"># we'll also scale atol.</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s3">, </span><span class="s1">dtype=np.float64</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">self.tree_.sample_weight </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">N = self.tree_.data.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">N = self.tree_.sum_weight</span>
        <span class="s1">atol_N = self.atol * N</span>
        <span class="s1">log_density = self.tree_.kernel_density(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">h=self.bandwidth_</span><span class="s3">,</span>
            <span class="s1">kernel=self.kernel</span><span class="s3">,</span>
            <span class="s1">atol=atol_N</span><span class="s3">,</span>
            <span class="s1">rtol=self.rtol</span><span class="s3">,</span>
            <span class="s1">breadth_first=self.breadth_first</span><span class="s3">,</span>
            <span class="s1">return_log=</span><span class="s3">True,</span>
        <span class="s1">)</span>
        <span class="s1">log_density -= np.log(N)</span>
        <span class="s3">return </span><span class="s1">log_density</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Compute the total log-likelihood under the model. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            List of n_features-dimensional data points.  Each row 
            corresponds to a single data point. 
 
        y : None 
            Ignored. This parameter exists only for compatibility with 
            :class:`~sklearn.pipeline.Pipeline`. 
 
        Returns 
        ------- 
        logprob : float 
            Total log-likelihood of the data in X. This is normalized to be a 
            probability density, so the value will be low for high-dimensional 
            data. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.sum(self.score_samples(X))</span>

    <span class="s3">def </span><span class="s1">sample(self</span><span class="s3">, </span><span class="s1">n_samples=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Generate random samples from the model. 
 
        Currently, this is implemented only for gaussian and tophat kernels. 
 
        Parameters 
        ---------- 
        n_samples : int, default=1 
            Number of samples to generate. 
 
        random_state : int, RandomState instance or None, default=None 
            Determines random number generation used to generate 
            random samples. Pass an int for reproducible results 
            across multiple function calls. 
            See :term:`Glossary &lt;random_state&gt;`. 
 
        Returns 
        ------- 
        X : array-like of shape (n_samples, n_features) 
            List of samples. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s2"># TODO: implement sampling for other valid kernel shapes</span>
        <span class="s3">if </span><span class="s1">self.kernel </span><span class="s3">not in </span><span class="s1">[</span><span class="s4">&quot;gaussian&quot;</span><span class="s3">, </span><span class="s4">&quot;tophat&quot;</span><span class="s1">]:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError()</span>

        <span class="s1">data = np.asarray(self.tree_.data)</span>

        <span class="s1">rng = check_random_state(random_state)</span>
        <span class="s1">u = rng.uniform(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">size=n_samples)</span>
        <span class="s3">if </span><span class="s1">self.tree_.sample_weight </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">i = (u * data.shape[</span><span class="s5">0</span><span class="s1">]).astype(np.int64)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">cumsum_weight = np.cumsum(np.asarray(self.tree_.sample_weight))</span>
            <span class="s1">sum_weight = cumsum_weight[-</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s1">i = np.searchsorted(cumsum_weight</span><span class="s3">, </span><span class="s1">u * sum_weight)</span>
        <span class="s3">if </span><span class="s1">self.kernel == </span><span class="s4">&quot;gaussian&quot;</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">np.atleast_2d(rng.normal(data[i]</span><span class="s3">, </span><span class="s1">self.bandwidth_))</span>

        <span class="s3">elif </span><span class="s1">self.kernel == </span><span class="s4">&quot;tophat&quot;</span><span class="s1">:</span>
            <span class="s2"># we first draw points from a d-dimensional normal distribution,</span>
            <span class="s2"># then use an incomplete gamma function to map them to a uniform</span>
            <span class="s2"># d-dimensional tophat distribution.</span>
            <span class="s1">dim = data.shape[</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s1">X = rng.normal(size=(n_samples</span><span class="s3">, </span><span class="s1">dim))</span>
            <span class="s1">s_sq = row_norms(X</span><span class="s3">, </span><span class="s1">squared=</span><span class="s3">True</span><span class="s1">)</span>
            <span class="s1">correction = (</span>
                <span class="s1">gammainc(</span><span class="s5">0.5 </span><span class="s1">* dim</span><span class="s3">, </span><span class="s5">0.5 </span><span class="s1">* s_sq) ** (</span><span class="s5">1.0 </span><span class="s1">/ dim)</span>
                <span class="s1">* self.bandwidth_</span>
                <span class="s1">/ np.sqrt(s_sq)</span>
            <span class="s1">)</span>
            <span class="s3">return </span><span class="s1">data[i] + X * correction[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span>
            <span class="s4">&quot;_xfail_checks&quot;</span><span class="s1">: {</span>
                <span class="s4">&quot;check_sample_weights_invariance&quot;</span><span class="s1">: (</span>
                    <span class="s4">&quot;sample_weight must have positive values&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">}</span>
        <span class="s1">}</span>
</pre>
</body>
</html>