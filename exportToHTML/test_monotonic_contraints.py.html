<html>
<head>
<title>test_monotonic_contraints.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6897bb;}
.s3 { color: #808080;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_monotonic_contraints.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">re</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>

<span class="s0">from </span><span class="s1">sklearn.ensemble </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">HistGradientBoostingClassifier</span><span class="s0">,</span>
    <span class="s1">HistGradientBoostingRegressor</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble._hist_gradient_boosting.common </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">G_H_DTYPE</span><span class="s0">,</span>
    <span class="s1">X_BINNED_DTYPE</span><span class="s0">,</span>
    <span class="s1">MonotonicConstraint</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble._hist_gradient_boosting.grower </span><span class="s0">import </span><span class="s1">TreeGrower</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble._hist_gradient_boosting.histogram </span><span class="s0">import </span><span class="s1">HistogramBuilder</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble._hist_gradient_boosting.splitting </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">Splitter</span><span class="s0">,</span>
    <span class="s1">compute_node_value</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.utils._openmp_helpers </span><span class="s0">import </span><span class="s1">_openmp_effective_n_threads</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">_convert_container</span>

<span class="s1">n_threads = _openmp_effective_n_threads()</span>


<span class="s0">def </span><span class="s1">is_increasing(a):</span>
    <span class="s0">return </span><span class="s1">(np.diff(a) &gt;= </span><span class="s2">0.0</span><span class="s1">).all()</span>


<span class="s0">def </span><span class="s1">is_decreasing(a):</span>
    <span class="s0">return </span><span class="s1">(np.diff(a) &lt;= </span><span class="s2">0.0</span><span class="s1">).all()</span>


<span class="s0">def </span><span class="s1">assert_leaves_values_monotonic(predictor</span><span class="s0">, </span><span class="s1">monotonic_cst):</span>
    <span class="s3"># make sure leaves values (from left to right) are either all increasing</span>
    <span class="s3"># or all decreasing (or neither) depending on the monotonic constraint.</span>
    <span class="s1">nodes = predictor.nodes</span>

    <span class="s0">def </span><span class="s1">get_leaves_values():</span>
        <span class="s4">&quot;&quot;&quot;get leaves values from left to right&quot;&quot;&quot;</span>
        <span class="s1">values = []</span>

        <span class="s0">def </span><span class="s1">depth_first_collect_leaf_values(node_idx):</span>
            <span class="s1">node = nodes[node_idx]</span>
            <span class="s0">if </span><span class="s1">node[</span><span class="s5">&quot;is_leaf&quot;</span><span class="s1">]:</span>
                <span class="s1">values.append(node[</span><span class="s5">&quot;value&quot;</span><span class="s1">])</span>
                <span class="s0">return</span>
            <span class="s1">depth_first_collect_leaf_values(node[</span><span class="s5">&quot;left&quot;</span><span class="s1">])</span>
            <span class="s1">depth_first_collect_leaf_values(node[</span><span class="s5">&quot;right&quot;</span><span class="s1">])</span>

        <span class="s1">depth_first_collect_leaf_values(</span><span class="s2">0</span><span class="s1">)  </span><span class="s3"># start at root (0)</span>
        <span class="s0">return </span><span class="s1">values</span>

    <span class="s1">values = get_leaves_values()</span>

    <span class="s0">if </span><span class="s1">monotonic_cst == MonotonicConstraint.NO_CST:</span>
        <span class="s3"># some increasing, some decreasing</span>
        <span class="s0">assert not </span><span class="s1">is_increasing(values) </span><span class="s0">and not </span><span class="s1">is_decreasing(values)</span>
    <span class="s0">elif </span><span class="s1">monotonic_cst == MonotonicConstraint.POS:</span>
        <span class="s3"># all increasing</span>
        <span class="s0">assert </span><span class="s1">is_increasing(values)</span>
    <span class="s0">else</span><span class="s1">:  </span><span class="s3"># NEG</span>
        <span class="s3"># all decreasing</span>
        <span class="s0">assert </span><span class="s1">is_decreasing(values)</span>


<span class="s0">def </span><span class="s1">assert_children_values_monotonic(predictor</span><span class="s0">, </span><span class="s1">monotonic_cst):</span>
    <span class="s3"># Make sure siblings values respect the monotonic constraints. Left should</span>
    <span class="s3"># be lower (resp greater) than right child if constraint is POS (resp.</span>
    <span class="s3"># NEG).</span>
    <span class="s3"># Note that this property alone isn't enough to ensure full monotonicity,</span>
    <span class="s3"># since we also need to guanrantee that all the descendents of the left</span>
    <span class="s3"># child won't be greater (resp. lower) than the right child, or its</span>
    <span class="s3"># descendents. That's why we need to bound the predicted values (this is</span>
    <span class="s3"># tested in assert_children_values_bounded)</span>
    <span class="s1">nodes = predictor.nodes</span>
    <span class="s1">left_lower = []</span>
    <span class="s1">left_greater = []</span>
    <span class="s0">for </span><span class="s1">node </span><span class="s0">in </span><span class="s1">nodes:</span>
        <span class="s0">if </span><span class="s1">node[</span><span class="s5">&quot;is_leaf&quot;</span><span class="s1">]:</span>
            <span class="s0">continue</span>

        <span class="s1">left_idx = node[</span><span class="s5">&quot;left&quot;</span><span class="s1">]</span>
        <span class="s1">right_idx = node[</span><span class="s5">&quot;right&quot;</span><span class="s1">]</span>

        <span class="s0">if </span><span class="s1">nodes[left_idx][</span><span class="s5">&quot;value&quot;</span><span class="s1">] &lt; nodes[right_idx][</span><span class="s5">&quot;value&quot;</span><span class="s1">]:</span>
            <span class="s1">left_lower.append(node)</span>
        <span class="s0">elif </span><span class="s1">nodes[left_idx][</span><span class="s5">&quot;value&quot;</span><span class="s1">] &gt; nodes[right_idx][</span><span class="s5">&quot;value&quot;</span><span class="s1">]:</span>
            <span class="s1">left_greater.append(node)</span>

    <span class="s0">if </span><span class="s1">monotonic_cst == MonotonicConstraint.NO_CST:</span>
        <span class="s0">assert </span><span class="s1">left_lower </span><span class="s0">and </span><span class="s1">left_greater</span>
    <span class="s0">elif </span><span class="s1">monotonic_cst == MonotonicConstraint.POS:</span>
        <span class="s0">assert </span><span class="s1">left_lower </span><span class="s0">and not </span><span class="s1">left_greater</span>
    <span class="s0">else</span><span class="s1">:  </span><span class="s3"># NEG</span>
        <span class="s0">assert not </span><span class="s1">left_lower </span><span class="s0">and </span><span class="s1">left_greater</span>


<span class="s0">def </span><span class="s1">assert_children_values_bounded(grower</span><span class="s0">, </span><span class="s1">monotonic_cst):</span>
    <span class="s3"># Make sure that the values of the children of a node are bounded by the</span>
    <span class="s3"># middle value between that node and its sibling (if there is a monotonic</span>
    <span class="s3"># constraint).</span>
    <span class="s3"># As a bonus, we also check that the siblings values are properly ordered</span>
    <span class="s3"># which is slightly redundant with assert_children_values_monotonic (but</span>
    <span class="s3"># this check is done on the grower nodes whereas</span>
    <span class="s3"># assert_children_values_monotonic is done on the predictor nodes)</span>

    <span class="s0">if </span><span class="s1">monotonic_cst == MonotonicConstraint.NO_CST:</span>
        <span class="s0">return</span>

    <span class="s0">def </span><span class="s1">recursively_check_children_node_values(node</span><span class="s0">, </span><span class="s1">right_sibling=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s0">if </span><span class="s1">node.is_leaf:</span>
            <span class="s0">return</span>
        <span class="s0">if </span><span class="s1">right_sibling </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">middle = (node.value + right_sibling.value) / </span><span class="s2">2</span>
            <span class="s0">if </span><span class="s1">monotonic_cst == MonotonicConstraint.POS:</span>
                <span class="s0">assert </span><span class="s1">node.left_child.value &lt;= node.right_child.value &lt;= middle</span>
                <span class="s0">if not </span><span class="s1">right_sibling.is_leaf:</span>
                    <span class="s0">assert </span><span class="s1">(</span>
                        <span class="s1">middle</span>
                        <span class="s1">&lt;= right_sibling.left_child.value</span>
                        <span class="s1">&lt;= right_sibling.right_child.value</span>
                    <span class="s1">)</span>
            <span class="s0">else</span><span class="s1">:  </span><span class="s3"># NEG</span>
                <span class="s0">assert </span><span class="s1">node.left_child.value &gt;= node.right_child.value &gt;= middle</span>
                <span class="s0">if not </span><span class="s1">right_sibling.is_leaf:</span>
                    <span class="s0">assert </span><span class="s1">(</span>
                        <span class="s1">middle</span>
                        <span class="s1">&gt;= right_sibling.left_child.value</span>
                        <span class="s1">&gt;= right_sibling.right_child.value</span>
                    <span class="s1">)</span>

        <span class="s1">recursively_check_children_node_values(</span>
            <span class="s1">node.left_child</span><span class="s0">, </span><span class="s1">right_sibling=node.right_child</span>
        <span class="s1">)</span>
        <span class="s1">recursively_check_children_node_values(node.right_child)</span>

    <span class="s1">recursively_check_children_node_values(grower.root)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;seed&quot;</span><span class="s0">, </span><span class="s1">range(</span><span class="s2">3</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;monotonic_cst&quot;</span><span class="s0">,</span>
    <span class="s1">(</span>
        <span class="s1">MonotonicConstraint.NO_CST</span><span class="s0">,</span>
        <span class="s1">MonotonicConstraint.POS</span><span class="s0">,</span>
        <span class="s1">MonotonicConstraint.NEG</span><span class="s0">,</span>
    <span class="s1">)</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_nodes_values(monotonic_cst</span><span class="s0">, </span><span class="s1">seed):</span>
    <span class="s3"># Build a single tree with only one feature, and make sure the nodes</span>
    <span class="s3"># values respect the monotonic constraints.</span>

    <span class="s3"># Considering the following tree with a monotonic POS constraint, we</span>
    <span class="s3"># should have:</span>
    <span class="s3">#</span>
    <span class="s3">#       root</span>
    <span class="s3">#      /    \</span>
    <span class="s3">#     5     10    # middle = 7.5</span>
    <span class="s3">#    / \   / \</span>
    <span class="s3">#   a  b  c  d</span>
    <span class="s3">#</span>
    <span class="s3"># a &lt;= b and c &lt;= d  (assert_children_values_monotonic)</span>
    <span class="s3"># a, b &lt;= middle &lt;= c, d (assert_children_values_bounded)</span>
    <span class="s3"># a &lt;= b &lt;= c &lt;= d (assert_leaves_values_monotonic)</span>
    <span class="s3">#</span>
    <span class="s3"># The last one is a consequence of the others, but can't hurt to check</span>

    <span class="s1">rng = np.random.RandomState(seed)</span>
    <span class="s1">n_samples = </span><span class="s2">1000</span>
    <span class="s1">n_features = </span><span class="s2">1</span>
    <span class="s1">X_binned = rng.randint(</span><span class="s2">0</span><span class="s0">, </span><span class="s2">255</span><span class="s0">, </span><span class="s1">size=(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span><span class="s0">, </span><span class="s1">dtype=np.uint8)</span>
    <span class="s1">X_binned = np.asfortranarray(X_binned)</span>

    <span class="s1">gradients = rng.normal(size=n_samples).astype(G_H_DTYPE)</span>
    <span class="s1">hessians = np.ones(shape=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE)</span>

    <span class="s1">grower = TreeGrower(</span>
        <span class="s1">X_binned</span><span class="s0">, </span><span class="s1">gradients</span><span class="s0">, </span><span class="s1">hessians</span><span class="s0">, </span><span class="s1">monotonic_cst=[monotonic_cst]</span><span class="s0">, </span><span class="s1">shrinkage=</span><span class="s2">0.1</span>
    <span class="s1">)</span>
    <span class="s1">grower.grow()</span>

    <span class="s3"># grow() will shrink the leaves values at the very end. For our comparison</span>
    <span class="s3"># tests, we need to revert the shrinkage of the leaves, else we would</span>
    <span class="s3"># compare the value of a leaf (shrunk) with a node (not shrunk) and the</span>
    <span class="s3"># test would not be correct.</span>
    <span class="s0">for </span><span class="s1">leave </span><span class="s0">in </span><span class="s1">grower.finalized_leaves:</span>
        <span class="s1">leave.value /= grower.shrinkage</span>

    <span class="s3"># We pass undefined binning_thresholds because we won't use predict anyway</span>
    <span class="s1">predictor = grower.make_predictor(</span>
        <span class="s1">binning_thresholds=np.zeros((X_binned.shape[</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">X_binned.max() + </span><span class="s2">1</span><span class="s1">))</span>
    <span class="s1">)</span>

    <span class="s3"># The consistency of the bounds can only be checked on the tree grower</span>
    <span class="s3"># as the node bounds are not copied into the predictor tree. The</span>
    <span class="s3"># consistency checks on the values of node children and leaves can be</span>
    <span class="s3"># done either on the grower tree or on the predictor tree. We only</span>
    <span class="s3"># do those checks on the predictor tree as the latter is derived from</span>
    <span class="s3"># the former.</span>
    <span class="s1">assert_children_values_monotonic(predictor</span><span class="s0">, </span><span class="s1">monotonic_cst)</span>
    <span class="s1">assert_children_values_bounded(grower</span><span class="s0">, </span><span class="s1">monotonic_cst)</span>
    <span class="s1">assert_leaves_values_monotonic(predictor</span><span class="s0">, </span><span class="s1">monotonic_cst)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;use_feature_names&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s0">True, False</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_predictions(global_random_seed</span><span class="s0">, </span><span class="s1">use_feature_names):</span>
    <span class="s3"># Train a model with a POS constraint on the first feature and a NEG</span>
    <span class="s3"># constraint on the second feature, and make sure the constraints are</span>
    <span class="s3"># respected by checking the predictions.</span>
    <span class="s3"># test adapted from lightgbm's test_monotone_constraint(), itself inspired</span>
    <span class="s3"># by https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html</span>

    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>

    <span class="s1">n_samples = </span><span class="s2">1000</span>
    <span class="s1">f_0 = rng.rand(n_samples)  </span><span class="s3"># positive correlation with y</span>
    <span class="s1">f_1 = rng.rand(n_samples)  </span><span class="s3"># negative correslation with y</span>
    <span class="s1">X = np.c_[f_0</span><span class="s0">, </span><span class="s1">f_1]</span>
    <span class="s1">columns_name = [</span><span class="s5">&quot;f_0&quot;</span><span class="s0">, </span><span class="s5">&quot;f_1&quot;</span><span class="s1">]</span>
    <span class="s1">constructor_name = </span><span class="s5">&quot;dataframe&quot; </span><span class="s0">if </span><span class="s1">use_feature_names </span><span class="s0">else </span><span class="s5">&quot;array&quot;</span>
    <span class="s1">X = _convert_container(X</span><span class="s0">, </span><span class="s1">constructor_name</span><span class="s0">, </span><span class="s1">columns_name=columns_name)</span>

    <span class="s1">noise = rng.normal(loc=</span><span class="s2">0.0</span><span class="s0">, </span><span class="s1">scale=</span><span class="s2">0.01</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
    <span class="s1">y = </span><span class="s2">5 </span><span class="s1">* f_0 + np.sin(</span><span class="s2">10 </span><span class="s1">* np.pi * f_0) - </span><span class="s2">5 </span><span class="s1">* f_1 - np.cos(</span><span class="s2">10 </span><span class="s1">* np.pi * f_1) + noise</span>

    <span class="s0">if </span><span class="s1">use_feature_names:</span>
        <span class="s1">monotonic_cst = {</span><span class="s5">&quot;f_0&quot;</span><span class="s1">: +</span><span class="s2">1</span><span class="s0">, </span><span class="s5">&quot;f_1&quot;</span><span class="s1">: -</span><span class="s2">1</span><span class="s1">}</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">monotonic_cst = [+</span><span class="s2">1</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s1">]</span>

    <span class="s1">gbdt = HistGradientBoostingRegressor(monotonic_cst=monotonic_cst)</span>
    <span class="s1">gbdt.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">linspace = np.linspace(</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">100</span><span class="s1">)</span>
    <span class="s1">sin = np.sin(linspace)</span>
    <span class="s1">constant = np.full_like(linspace</span><span class="s0">, </span><span class="s1">fill_value=</span><span class="s2">0.5</span><span class="s1">)</span>

    <span class="s3"># We now assert the predictions properly respect the constraints, on each</span>
    <span class="s3"># feature. When testing for a feature we need to set the other one to a</span>
    <span class="s3"># constant, because the monotonic constraints are only a &quot;all else being</span>
    <span class="s3"># equal&quot; type of constraints:</span>
    <span class="s3"># a constraint on the first feature only means that</span>
    <span class="s3"># x0 &lt; x0' =&gt; f(x0, x1) &lt; f(x0', x1)</span>
    <span class="s3"># while x1 stays constant.</span>
    <span class="s3"># The constraint does not guanrantee that</span>
    <span class="s3"># x0 &lt; x0' =&gt; f(x0, x1) &lt; f(x0', x1')</span>

    <span class="s3"># First feature (POS)</span>
    <span class="s3"># assert pred is all increasing when f_0 is all increasing</span>
    <span class="s1">X = np.c_[linspace</span><span class="s0">, </span><span class="s1">constant]</span>
    <span class="s1">X = _convert_container(X</span><span class="s0">, </span><span class="s1">constructor_name</span><span class="s0">, </span><span class="s1">columns_name=columns_name)</span>
    <span class="s1">pred = gbdt.predict(X)</span>
    <span class="s0">assert </span><span class="s1">is_increasing(pred)</span>
    <span class="s3"># assert pred actually follows the variations of f_0</span>
    <span class="s1">X = np.c_[sin</span><span class="s0">, </span><span class="s1">constant]</span>
    <span class="s1">X = _convert_container(X</span><span class="s0">, </span><span class="s1">constructor_name</span><span class="s0">, </span><span class="s1">columns_name=columns_name)</span>
    <span class="s1">pred = gbdt.predict(X)</span>
    <span class="s0">assert </span><span class="s1">np.all((np.diff(pred) &gt;= </span><span class="s2">0</span><span class="s1">) == (np.diff(sin) &gt;= </span><span class="s2">0</span><span class="s1">))</span>

    <span class="s3"># Second feature (NEG)</span>
    <span class="s3"># assert pred is all decreasing when f_1 is all increasing</span>
    <span class="s1">X = np.c_[constant</span><span class="s0">, </span><span class="s1">linspace]</span>
    <span class="s1">X = _convert_container(X</span><span class="s0">, </span><span class="s1">constructor_name</span><span class="s0">, </span><span class="s1">columns_name=columns_name)</span>
    <span class="s1">pred = gbdt.predict(X)</span>
    <span class="s0">assert </span><span class="s1">is_decreasing(pred)</span>
    <span class="s3"># assert pred actually follows the inverse variations of f_1</span>
    <span class="s1">X = np.c_[constant</span><span class="s0">, </span><span class="s1">sin]</span>
    <span class="s1">X = _convert_container(X</span><span class="s0">, </span><span class="s1">constructor_name</span><span class="s0">, </span><span class="s1">columns_name=columns_name)</span>
    <span class="s1">pred = gbdt.predict(X)</span>
    <span class="s0">assert </span><span class="s1">((np.diff(pred) &lt;= </span><span class="s2">0</span><span class="s1">) == (np.diff(sin) &gt;= </span><span class="s2">0</span><span class="s1">)).all()</span>


<span class="s0">def </span><span class="s1">test_input_error():</span>
    <span class="s1">X = [[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">4</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span>

    <span class="s1">gbdt = HistGradientBoostingRegressor(monotonic_cst=[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s1">])</span>
    <span class="s0">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s0">, </span><span class="s1">match=re.escape(</span><span class="s5">&quot;monotonic_cst has shape (3,) but the input data&quot;</span><span class="s1">)</span>
    <span class="s1">):</span>
        <span class="s1">gbdt.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">for </span><span class="s1">monotonic_cst </span><span class="s0">in </span><span class="s1">([</span><span class="s2">1</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s0">, </span><span class="s1">-</span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">0.3</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.7</span><span class="s1">]):</span>
        <span class="s1">gbdt = HistGradientBoostingRegressor(monotonic_cst=monotonic_cst)</span>
        <span class="s1">expected_msg = re.escape(</span>
            <span class="s5">&quot;must be an array-like of -1, 0 or 1. Observed values:&quot;</span>
        <span class="s1">)</span>
        <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=expected_msg):</span>
            <span class="s1">gbdt.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">gbdt = HistGradientBoostingClassifier(monotonic_cst=[</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">])</span>
    <span class="s0">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s0">,</span>
        <span class="s1">match=</span><span class="s5">&quot;monotonic constraints are not supported for multiclass classification&quot;</span><span class="s0">,</span>
    <span class="s1">):</span>
        <span class="s1">gbdt.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_input_error_related_to_feature_names():</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s5">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">X = pd.DataFrame({</span><span class="s5">&quot;a&quot;</span><span class="s1">: [</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s5">&quot;b&quot;</span><span class="s1">: [</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]})</span>
    <span class="s1">y = np.array([</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">])</span>

    <span class="s1">monotonic_cst = {</span><span class="s5">&quot;d&quot;</span><span class="s1">: </span><span class="s2">1</span><span class="s0">, </span><span class="s5">&quot;a&quot;</span><span class="s1">: </span><span class="s2">1</span><span class="s0">, </span><span class="s5">&quot;c&quot;</span><span class="s1">: -</span><span class="s2">1</span><span class="s1">}</span>
    <span class="s1">gbdt = HistGradientBoostingRegressor(monotonic_cst=monotonic_cst)</span>
    <span class="s1">expected_msg = re.escape(</span>
        <span class="s5">&quot;monotonic_cst contains 2 unexpected feature names: ['c', 'd'].&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=expected_msg):</span>
        <span class="s1">gbdt.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">monotonic_cst = {k: </span><span class="s2">1 </span><span class="s0">for </span><span class="s1">k </span><span class="s0">in </span><span class="s5">&quot;abcdefghijklmnopqrstuvwxyz&quot;</span><span class="s1">}</span>
    <span class="s1">gbdt = HistGradientBoostingRegressor(monotonic_cst=monotonic_cst)</span>
    <span class="s1">expected_msg = re.escape(</span>
        <span class="s5">&quot;monotonic_cst contains 24 unexpected feature names: &quot;</span>
        <span class="s5">&quot;['c', 'd', 'e', 'f', 'g', '...'].&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=expected_msg):</span>
        <span class="s1">gbdt.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">monotonic_cst = {</span><span class="s5">&quot;a&quot;</span><span class="s1">: </span><span class="s2">1</span><span class="s1">}</span>
    <span class="s1">gbdt = HistGradientBoostingRegressor(monotonic_cst=monotonic_cst)</span>
    <span class="s1">expected_msg = re.escape(</span>
        <span class="s5">&quot;HistGradientBoostingRegressor was not fitted on data with feature &quot;</span>
        <span class="s5">&quot;names. Pass monotonic_cst as an integer array instead.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=expected_msg):</span>
        <span class="s1">gbdt.fit(X.values</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">monotonic_cst = {</span><span class="s5">&quot;b&quot;</span><span class="s1">: -</span><span class="s2">1</span><span class="s0">, </span><span class="s5">&quot;a&quot;</span><span class="s1">: </span><span class="s5">&quot;+&quot;</span><span class="s1">}</span>
    <span class="s1">gbdt = HistGradientBoostingRegressor(monotonic_cst=monotonic_cst)</span>
    <span class="s1">expected_msg = re.escape(</span><span class="s5">&quot;monotonic_cst['a'] must be either -1, 0 or 1. Got '+'.&quot;</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=expected_msg):</span>
        <span class="s1">gbdt.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_bounded_value_min_gain_to_split():</span>
    <span class="s3"># The purpose of this test is to show that when computing the gain at a</span>
    <span class="s3"># given split, the value of the current node should be properly bounded to</span>
    <span class="s3"># respect the monotonic constraints, because it strongly interacts with</span>
    <span class="s3"># min_gain_to_split. We build a simple example where gradients are [1, 1,</span>
    <span class="s3"># 100, 1, 1] (hessians are all ones). The best split happens on the 3rd</span>
    <span class="s3"># bin, and depending on whether the value of the node is bounded or not,</span>
    <span class="s3"># the min_gain_to_split constraint is or isn't satisfied.</span>
    <span class="s1">l2_regularization = </span><span class="s2">0</span>
    <span class="s1">min_hessian_to_split = </span><span class="s2">0</span>
    <span class="s1">min_samples_leaf = </span><span class="s2">1</span>
    <span class="s1">n_bins = n_samples = </span><span class="s2">5</span>
    <span class="s1">X_binned = np.arange(n_samples).reshape(-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">).astype(X_BINNED_DTYPE)</span>
    <span class="s1">sample_indices = np.arange(n_samples</span><span class="s0">, </span><span class="s1">dtype=np.uint32)</span>
    <span class="s1">all_hessians = np.ones(n_samples</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE)</span>
    <span class="s1">all_gradients = np.array([</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">100</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE)</span>
    <span class="s1">sum_gradients = all_gradients.sum()</span>
    <span class="s1">sum_hessians = all_hessians.sum()</span>
    <span class="s1">hessians_are_constant = </span><span class="s0">False</span>

    <span class="s1">builder = HistogramBuilder(</span>
        <span class="s1">X_binned</span><span class="s0">, </span><span class="s1">n_bins</span><span class="s0">, </span><span class="s1">all_gradients</span><span class="s0">, </span><span class="s1">all_hessians</span><span class="s0">, </span><span class="s1">hessians_are_constant</span><span class="s0">, </span><span class="s1">n_threads</span>
    <span class="s1">)</span>
    <span class="s1">n_bins_non_missing = np.array([n_bins - </span><span class="s2">1</span><span class="s1">] * X_binned.shape[</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=np.uint32)</span>
    <span class="s1">has_missing_values = np.array([</span><span class="s0">False</span><span class="s1">] * X_binned.shape[</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=np.uint8)</span>
    <span class="s1">monotonic_cst = np.array(</span>
        <span class="s1">[MonotonicConstraint.NO_CST] * X_binned.shape[</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=np.int8</span>
    <span class="s1">)</span>
    <span class="s1">is_categorical = np.zeros_like(monotonic_cst</span><span class="s0">, </span><span class="s1">dtype=np.uint8)</span>
    <span class="s1">missing_values_bin_idx = n_bins - </span><span class="s2">1</span>
    <span class="s1">children_lower_bound</span><span class="s0">, </span><span class="s1">children_upper_bound = -np.inf</span><span class="s0">, </span><span class="s1">np.inf</span>

    <span class="s1">min_gain_to_split = </span><span class="s2">2000</span>
    <span class="s1">splitter = Splitter(</span>
        <span class="s1">X_binned</span><span class="s0">,</span>
        <span class="s1">n_bins_non_missing</span><span class="s0">,</span>
        <span class="s1">missing_values_bin_idx</span><span class="s0">,</span>
        <span class="s1">has_missing_values</span><span class="s0">,</span>
        <span class="s1">is_categorical</span><span class="s0">,</span>
        <span class="s1">monotonic_cst</span><span class="s0">,</span>
        <span class="s1">l2_regularization</span><span class="s0">,</span>
        <span class="s1">min_hessian_to_split</span><span class="s0">,</span>
        <span class="s1">min_samples_leaf</span><span class="s0">,</span>
        <span class="s1">min_gain_to_split</span><span class="s0">,</span>
        <span class="s1">hessians_are_constant</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">histograms = builder.compute_histograms_brute(sample_indices)</span>

    <span class="s3"># Since the gradient array is [1, 1, 100, 1, 1]</span>
    <span class="s3"># the max possible gain happens on the 3rd bin (or equivalently in the 2nd)</span>
    <span class="s3"># and is equal to about 1307, which less than min_gain_to_split = 2000, so</span>
    <span class="s3"># the node is considered unsplittable (gain = -1)</span>
    <span class="s1">current_lower_bound</span><span class="s0">, </span><span class="s1">current_upper_bound = -np.inf</span><span class="s0">, </span><span class="s1">np.inf</span>
    <span class="s1">value = compute_node_value(</span>
        <span class="s1">sum_gradients</span><span class="s0">,</span>
        <span class="s1">sum_hessians</span><span class="s0">,</span>
        <span class="s1">current_lower_bound</span><span class="s0">,</span>
        <span class="s1">current_upper_bound</span><span class="s0">,</span>
        <span class="s1">l2_regularization</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s3"># the unbounded value is equal to -sum_gradients / sum_hessians</span>
    <span class="s0">assert </span><span class="s1">value == pytest.approx(-</span><span class="s2">104 </span><span class="s1">/ </span><span class="s2">5</span><span class="s1">)</span>
    <span class="s1">split_info = splitter.find_node_split(</span>
        <span class="s1">n_samples</span><span class="s0">,</span>
        <span class="s1">histograms</span><span class="s0">,</span>
        <span class="s1">sum_gradients</span><span class="s0">,</span>
        <span class="s1">sum_hessians</span><span class="s0">,</span>
        <span class="s1">value</span><span class="s0">,</span>
        <span class="s1">lower_bound=children_lower_bound</span><span class="s0">,</span>
        <span class="s1">upper_bound=children_upper_bound</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">split_info.gain == -</span><span class="s2">1  </span><span class="s3"># min_gain_to_split not respected</span>

    <span class="s3"># here again the max possible gain is on the 3rd bin but we now cap the</span>
    <span class="s3"># value of the node into [-10, inf].</span>
    <span class="s3"># This means the gain is now about 2430 which is more than the</span>
    <span class="s3"># min_gain_to_split constraint.</span>
    <span class="s1">current_lower_bound</span><span class="s0">, </span><span class="s1">current_upper_bound = -</span><span class="s2">10</span><span class="s0">, </span><span class="s1">np.inf</span>
    <span class="s1">value = compute_node_value(</span>
        <span class="s1">sum_gradients</span><span class="s0">,</span>
        <span class="s1">sum_hessians</span><span class="s0">,</span>
        <span class="s1">current_lower_bound</span><span class="s0">,</span>
        <span class="s1">current_upper_bound</span><span class="s0">,</span>
        <span class="s1">l2_regularization</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">value == -</span><span class="s2">10</span>
    <span class="s1">split_info = splitter.find_node_split(</span>
        <span class="s1">n_samples</span><span class="s0">,</span>
        <span class="s1">histograms</span><span class="s0">,</span>
        <span class="s1">sum_gradients</span><span class="s0">,</span>
        <span class="s1">sum_hessians</span><span class="s0">,</span>
        <span class="s1">value</span><span class="s0">,</span>
        <span class="s1">lower_bound=children_lower_bound</span><span class="s0">,</span>
        <span class="s1">upper_bound=children_upper_bound</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">split_info.gain &gt; min_gain_to_split</span>
</pre>
</body>
</html>