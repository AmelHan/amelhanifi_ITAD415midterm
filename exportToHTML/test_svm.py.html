<html>
<head>
<title>test_svm.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_svm.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Testing for Support Vector Machine module (sklearn.svm) 
 
TODO: remove hard coded numerical results when possible 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">re</span>
<span class="s2">import </span><span class="s1">warnings</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">numpy.testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s2">,</span>
    <span class="s1">assert_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_equal</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">sparse</span>

<span class="s2">from </span><span class="s1">sklearn </span><span class="s2">import </span><span class="s1">base</span><span class="s2">, </span><span class="s1">datasets</span><span class="s2">, </span><span class="s1">linear_model</span><span class="s2">, </span><span class="s1">metrics</span><span class="s2">, </span><span class="s1">svm</span>
<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">make_blobs</span><span class="s2">, </span><span class="s1">make_classification</span>
<span class="s2">from </span><span class="s1">sklearn.exceptions </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">ConvergenceWarning</span><span class="s2">,</span>
    <span class="s1">NotFittedError</span><span class="s2">,</span>
    <span class="s1">UndefinedMetricWarning</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">f1_score</span>
<span class="s2">from </span><span class="s1">sklearn.metrics.pairwise </span><span class="s2">import </span><span class="s1">rbf_kernel</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">train_test_split</span>
<span class="s2">from </span><span class="s1">sklearn.multiclass </span><span class="s2">import </span><span class="s1">OneVsRestClassifier</span>

<span class="s3"># mypy error: Module 'sklearn.svm' has no attribute '_libsvm'</span>
<span class="s2">from </span><span class="s1">sklearn.svm </span><span class="s2">import </span><span class="s1">(  </span><span class="s3"># type: ignore</span>
    <span class="s1">SVR</span><span class="s2">,</span>
    <span class="s1">LinearSVC</span><span class="s2">,</span>
    <span class="s1">LinearSVR</span><span class="s2">,</span>
    <span class="s1">NuSVR</span><span class="s2">,</span>
    <span class="s1">OneClassSVM</span><span class="s2">,</span>
    <span class="s1">_libsvm</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.svm._classes </span><span class="s2">import </span><span class="s1">_validate_dual_parameter</span>
<span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">check_random_state</span><span class="s2">, </span><span class="s1">shuffle</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">ignore_warnings</span>
<span class="s2">from </span><span class="s1">sklearn.utils.validation </span><span class="s2">import </span><span class="s1">_num_samples</span>

<span class="s3"># toy sample</span>
<span class="s1">X = [[-</span><span class="s4">2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]]</span>
<span class="s1">Y = [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span>
<span class="s1">T = [[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]]</span>
<span class="s1">true_result = [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span>

<span class="s3"># also load the iris dataset</span>
<span class="s1">iris = datasets.load_iris()</span>
<span class="s1">rng = check_random_state(</span><span class="s4">42</span><span class="s1">)</span>
<span class="s1">perm = rng.permutation(iris.target.size)</span>
<span class="s1">iris.data = iris.data[perm]</span>
<span class="s1">iris.target = iris.target[perm]</span>


<span class="s2">def </span><span class="s1">test_libsvm_parameters():</span>
    <span class="s3"># Test parameters on classes that make use of libsvm.</span>
    <span class="s1">clf = svm.SVC(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">assert_array_equal(clf.dual_coef_</span><span class="s2">, </span><span class="s1">[[-</span><span class="s4">0.25</span><span class="s2">, </span><span class="s4">0.25</span><span class="s1">]])</span>
    <span class="s1">assert_array_equal(clf.support_</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(clf.support_vectors_</span><span class="s2">, </span><span class="s1">(X[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X[</span><span class="s4">3</span><span class="s1">]))</span>
    <span class="s1">assert_array_equal(clf.intercept_</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.0</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(clf.predict(X)</span><span class="s2">, </span><span class="s1">Y)</span>


<span class="s2">def </span><span class="s1">test_libsvm_iris():</span>
    <span class="s3"># Check consistency on dataset iris.</span>

    <span class="s3"># shuffle the dataset so that labels are not ordered</span>
    <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">(</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s5">&quot;rbf&quot;</span><span class="s1">):</span>
        <span class="s1">clf = svm.SVC(kernel=k).fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
        <span class="s2">assert </span><span class="s1">np.mean(clf.predict(iris.data) == iris.target) &gt; </span><span class="s4">0.9</span>
        <span class="s2">assert </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s5">&quot;coef_&quot;</span><span class="s1">) == (k == </span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span>

    <span class="s1">assert_array_equal(clf.classes_</span><span class="s2">, </span><span class="s1">np.sort(clf.classes_))</span>

    <span class="s3"># check also the low-level API</span>
    <span class="s3"># We unpack the values to create a dictionary with some of the return values</span>
    <span class="s3"># from Libsvm's fit.</span>
    <span class="s1">(</span>
        <span class="s1">libsvm_support</span><span class="s2">,</span>
        <span class="s1">libsvm_support_vectors</span><span class="s2">,</span>
        <span class="s1">libsvm_n_class_SV</span><span class="s2">,</span>
        <span class="s1">libsvm_sv_coef</span><span class="s2">,</span>
        <span class="s1">libsvm_intercept</span><span class="s2">,</span>
        <span class="s1">libsvm_probA</span><span class="s2">,</span>
        <span class="s1">libsvm_probB</span><span class="s2">,</span>
        <span class="s3"># libsvm_fit_status and libsvm_n_iter won't be used below.</span>
        <span class="s1">libsvm_fit_status</span><span class="s2">,</span>
        <span class="s1">libsvm_n_iter</span><span class="s2">,</span>
    <span class="s1">) = _libsvm.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target.astype(np.float64))</span>

    <span class="s1">model_params = {</span>
        <span class="s5">&quot;support&quot;</span><span class="s1">: libsvm_support</span><span class="s2">,</span>
        <span class="s5">&quot;SV&quot;</span><span class="s1">: libsvm_support_vectors</span><span class="s2">,</span>
        <span class="s5">&quot;nSV&quot;</span><span class="s1">: libsvm_n_class_SV</span><span class="s2">,</span>
        <span class="s5">&quot;sv_coef&quot;</span><span class="s1">: libsvm_sv_coef</span><span class="s2">,</span>
        <span class="s5">&quot;intercept&quot;</span><span class="s1">: libsvm_intercept</span><span class="s2">,</span>
        <span class="s5">&quot;probA&quot;</span><span class="s1">: libsvm_probA</span><span class="s2">,</span>
        <span class="s5">&quot;probB&quot;</span><span class="s1">: libsvm_probB</span><span class="s2">,</span>
    <span class="s1">}</span>
    <span class="s1">pred = _libsvm.predict(iris.data</span><span class="s2">, </span><span class="s1">**model_params)</span>
    <span class="s2">assert </span><span class="s1">np.mean(pred == iris.target) &gt; </span><span class="s4">0.95</span>

    <span class="s3"># We unpack the values to create a dictionary with some of the return values</span>
    <span class="s3"># from Libsvm's fit.</span>
    <span class="s1">(</span>
        <span class="s1">libsvm_support</span><span class="s2">,</span>
        <span class="s1">libsvm_support_vectors</span><span class="s2">,</span>
        <span class="s1">libsvm_n_class_SV</span><span class="s2">,</span>
        <span class="s1">libsvm_sv_coef</span><span class="s2">,</span>
        <span class="s1">libsvm_intercept</span><span class="s2">,</span>
        <span class="s1">libsvm_probA</span><span class="s2">,</span>
        <span class="s1">libsvm_probB</span><span class="s2">,</span>
        <span class="s3"># libsvm_fit_status and libsvm_n_iter won't be used below.</span>
        <span class="s1">libsvm_fit_status</span><span class="s2">,</span>
        <span class="s1">libsvm_n_iter</span><span class="s2">,</span>
    <span class="s1">) = _libsvm.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target.astype(np.float64)</span><span class="s2">, </span><span class="s1">kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span>

    <span class="s1">model_params = {</span>
        <span class="s5">&quot;support&quot;</span><span class="s1">: libsvm_support</span><span class="s2">,</span>
        <span class="s5">&quot;SV&quot;</span><span class="s1">: libsvm_support_vectors</span><span class="s2">,</span>
        <span class="s5">&quot;nSV&quot;</span><span class="s1">: libsvm_n_class_SV</span><span class="s2">,</span>
        <span class="s5">&quot;sv_coef&quot;</span><span class="s1">: libsvm_sv_coef</span><span class="s2">,</span>
        <span class="s5">&quot;intercept&quot;</span><span class="s1">: libsvm_intercept</span><span class="s2">,</span>
        <span class="s5">&quot;probA&quot;</span><span class="s1">: libsvm_probA</span><span class="s2">,</span>
        <span class="s5">&quot;probB&quot;</span><span class="s1">: libsvm_probB</span><span class="s2">,</span>
    <span class="s1">}</span>
    <span class="s1">pred = _libsvm.predict(iris.data</span><span class="s2">, </span><span class="s1">**model_params</span><span class="s2">, </span><span class="s1">kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">np.mean(pred == iris.target) &gt; </span><span class="s4">0.95</span>

    <span class="s1">pred = _libsvm.cross_validation(</span>
        <span class="s1">iris.data</span><span class="s2">, </span><span class="s1">iris.target.astype(np.float64)</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s1">kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">random_seed=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">np.mean(pred == iris.target) &gt; </span><span class="s4">0.95</span>

    <span class="s3"># If random_seed &gt;= 0, the libsvm rng is seeded (by calling `srand`), hence</span>
    <span class="s3"># we should get deterministic results (assuming that there is no other</span>
    <span class="s3"># thread calling this wrapper calling `srand` concurrently).</span>
    <span class="s1">pred2 = _libsvm.cross_validation(</span>
        <span class="s1">iris.data</span><span class="s2">, </span><span class="s1">iris.target.astype(np.float64)</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s1">kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">random_seed=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s2">, </span><span class="s1">pred2)</span>


<span class="s2">def </span><span class="s1">test_precomputed():</span>
    <span class="s3"># SVC with a precomputed kernel.</span>
    <span class="s3"># We test it with a toy dataset and with iris.</span>
    <span class="s1">clf = svm.SVC(kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s1">)</span>
    <span class="s3"># Gram matrix for train data (square matrix)</span>
    <span class="s3"># (we use just a linear kernel)</span>
    <span class="s1">K = np.dot(X</span><span class="s2">, </span><span class="s1">np.array(X).T)</span>
    <span class="s1">clf.fit(K</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s3"># Gram matrix for test data (rectangular matrix)</span>
    <span class="s1">KT = np.dot(T</span><span class="s2">, </span><span class="s1">np.array(X).T)</span>
    <span class="s1">pred = clf.predict(KT)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.predict(KT.T)</span>

    <span class="s1">assert_array_equal(clf.dual_coef_</span><span class="s2">, </span><span class="s1">[[-</span><span class="s4">0.25</span><span class="s2">, </span><span class="s4">0.25</span><span class="s1">]])</span>
    <span class="s1">assert_array_equal(clf.support_</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(clf.intercept_</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(clf.support_</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(pred</span><span class="s2">, </span><span class="s1">true_result)</span>

    <span class="s3"># Gram matrix for test data but compute KT[i,j]</span>
    <span class="s3"># for support vectors j only.</span>
    <span class="s1">KT = np.zeros_like(KT)</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(len(T)):</span>
        <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">clf.support_:</span>
            <span class="s1">KT[i</span><span class="s2">, </span><span class="s1">j] = np.dot(T[i]</span><span class="s2">, </span><span class="s1">X[j])</span>

    <span class="s1">pred = clf.predict(KT)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s2">, </span><span class="s1">true_result)</span>

    <span class="s3"># same as before, but using a callable function instead of the kernel</span>
    <span class="s3"># matrix. kernel is just a linear kernel</span>

    <span class="s2">def </span><span class="s1">kfunc(x</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">return </span><span class="s1">np.dot(x</span><span class="s2">, </span><span class="s1">y.T)</span>

    <span class="s1">clf = svm.SVC(kernel=kfunc)</span>
    <span class="s1">clf.fit(np.array(X)</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">pred = clf.predict(T)</span>

    <span class="s1">assert_array_equal(clf.dual_coef_</span><span class="s2">, </span><span class="s1">[[-</span><span class="s4">0.25</span><span class="s2">, </span><span class="s4">0.25</span><span class="s1">]])</span>
    <span class="s1">assert_array_equal(clf.intercept_</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(clf.support_</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(pred</span><span class="s2">, </span><span class="s1">true_result)</span>

    <span class="s3"># test a precomputed kernel with the iris dataset</span>
    <span class="s3"># and check parameters against a linear SVC</span>
    <span class="s1">clf = svm.SVC(kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s1">)</span>
    <span class="s1">clf2 = svm.SVC(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span>
    <span class="s1">K = np.dot(iris.data</span><span class="s2">, </span><span class="s1">iris.data.T)</span>
    <span class="s1">clf.fit(K</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">clf2.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">pred = clf.predict(K)</span>
    <span class="s1">assert_array_almost_equal(clf.support_</span><span class="s2">, </span><span class="s1">clf2.support_)</span>
    <span class="s1">assert_array_almost_equal(clf.dual_coef_</span><span class="s2">, </span><span class="s1">clf2.dual_coef_)</span>
    <span class="s1">assert_array_almost_equal(clf.intercept_</span><span class="s2">, </span><span class="s1">clf2.intercept_)</span>
    <span class="s1">assert_almost_equal(np.mean(pred == iris.target)</span><span class="s2">, </span><span class="s4">0.99</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s3"># Gram matrix for test data but compute KT[i,j]</span>
    <span class="s3"># for support vectors j only.</span>
    <span class="s1">K = np.zeros_like(K)</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(len(iris.data)):</span>
        <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">clf.support_:</span>
            <span class="s1">K[i</span><span class="s2">, </span><span class="s1">j] = np.dot(iris.data[i]</span><span class="s2">, </span><span class="s1">iris.data[j])</span>

    <span class="s1">pred = clf.predict(K)</span>
    <span class="s1">assert_almost_equal(np.mean(pred == iris.target)</span><span class="s2">, </span><span class="s4">0.99</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s1">clf = svm.SVC(kernel=kfunc)</span>
    <span class="s1">clf.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">assert_almost_equal(np.mean(pred == iris.target)</span><span class="s2">, </span><span class="s4">0.99</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_svr():</span>
    <span class="s3"># Test Support Vector Regression</span>

    <span class="s1">diabetes = datasets.load_diabetes()</span>
    <span class="s2">for </span><span class="s1">clf </span><span class="s2">in </span><span class="s1">(</span>
        <span class="s1">svm.NuSVR(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">nu=</span><span class="s4">0.4</span><span class="s2">, </span><span class="s1">C=</span><span class="s4">1.0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">svm.NuSVR(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">nu=</span><span class="s4">0.4</span><span class="s2">, </span><span class="s1">C=</span><span class="s4">10.0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">svm.SVR(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s4">10.0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">svm.LinearSVR(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s4">10.0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">svm.LinearSVR(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s4">10.0</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">clf.fit(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>
        <span class="s2">assert </span><span class="s1">clf.score(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target) &gt; </span><span class="s4">0.02</span>

    <span class="s3"># non-regression test; previously, BaseLibSVM would check that</span>
    <span class="s3"># len(np.unique(y)) &lt; 2, which must only be done for SVC</span>
    <span class="s1">svm.SVR().fit(diabetes.data</span><span class="s2">, </span><span class="s1">np.ones(len(diabetes.data)))</span>
    <span class="s1">svm.LinearSVR(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s1">).fit(diabetes.data</span><span class="s2">, </span><span class="s1">np.ones(len(diabetes.data)))</span>


<span class="s2">def </span><span class="s1">test_linearsvr():</span>
    <span class="s3"># check that SVR(kernel='linear') and LinearSVC() give</span>
    <span class="s3"># comparable results</span>
    <span class="s1">diabetes = datasets.load_diabetes()</span>
    <span class="s1">lsvr = svm.LinearSVR(C=</span><span class="s4">1e3</span><span class="s2">, </span><span class="s1">dual=</span><span class="s5">&quot;auto&quot;</span><span class="s1">).fit(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>
    <span class="s1">score1 = lsvr.score(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>

    <span class="s1">svr = svm.SVR(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s4">1e3</span><span class="s1">).fit(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>
    <span class="s1">score2 = svr.score(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>

    <span class="s1">assert_allclose(np.linalg.norm(lsvr.coef_)</span><span class="s2">, </span><span class="s1">np.linalg.norm(svr.coef_)</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0.0001</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(score1</span><span class="s2">, </span><span class="s1">score2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_linearsvr_fit_sampleweight():</span>
    <span class="s3"># check correct result when sample_weight is 1</span>
    <span class="s3"># check that SVR(kernel='linear') and LinearSVC() give</span>
    <span class="s3"># comparable results</span>
    <span class="s1">diabetes = datasets.load_diabetes()</span>
    <span class="s1">n_samples = len(diabetes.target)</span>
    <span class="s1">unit_weight = np.ones(n_samples)</span>
    <span class="s1">lsvr = svm.LinearSVR(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s4">1e3</span><span class="s2">, </span><span class="s1">tol=</span><span class="s4">1e-12</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">10000</span><span class="s1">).fit(</span>
        <span class="s1">diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target</span><span class="s2">, </span><span class="s1">sample_weight=unit_weight</span>
    <span class="s1">)</span>
    <span class="s1">score1 = lsvr.score(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>

    <span class="s1">lsvr_no_weight = svm.LinearSVR(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s4">1e3</span><span class="s2">, </span><span class="s1">tol=</span><span class="s4">1e-12</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">10000</span><span class="s1">).fit(</span>
        <span class="s1">diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target</span>
    <span class="s1">)</span>
    <span class="s1">score2 = lsvr_no_weight.score(diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target)</span>

    <span class="s1">assert_allclose(</span>
        <span class="s1">np.linalg.norm(lsvr.coef_)</span><span class="s2">, </span><span class="s1">np.linalg.norm(lsvr_no_weight.coef_)</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0.0001</span>
    <span class="s1">)</span>
    <span class="s1">assert_almost_equal(score1</span><span class="s2">, </span><span class="s1">score2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>

    <span class="s3"># check that fit(X)  = fit([X1, X2, X3], sample_weight = [n1, n2, n3]) where</span>
    <span class="s3"># X = X1 repeated n1 times, X2 repeated n2 times and so forth</span>
    <span class="s1">random_state = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">random_weight = random_state.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s1">n_samples)</span>
    <span class="s1">lsvr_unflat = svm.LinearSVR(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s4">1e3</span><span class="s2">, </span><span class="s1">tol=</span><span class="s4">1e-12</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">10000</span><span class="s1">).fit(</span>
        <span class="s1">diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target</span><span class="s2">, </span><span class="s1">sample_weight=random_weight</span>
    <span class="s1">)</span>
    <span class="s1">score3 = lsvr_unflat.score(</span>
        <span class="s1">diabetes.data</span><span class="s2">, </span><span class="s1">diabetes.target</span><span class="s2">, </span><span class="s1">sample_weight=random_weight</span>
    <span class="s1">)</span>

    <span class="s1">X_flat = np.repeat(diabetes.data</span><span class="s2">, </span><span class="s1">random_weight</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">y_flat = np.repeat(diabetes.target</span><span class="s2">, </span><span class="s1">random_weight</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">lsvr_flat = svm.LinearSVR(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s4">1e3</span><span class="s2">, </span><span class="s1">tol=</span><span class="s4">1e-12</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">10000</span><span class="s1">).fit(</span>
        <span class="s1">X_flat</span><span class="s2">, </span><span class="s1">y_flat</span>
    <span class="s1">)</span>
    <span class="s1">score4 = lsvr_flat.score(X_flat</span><span class="s2">, </span><span class="s1">y_flat)</span>

    <span class="s1">assert_almost_equal(score3</span><span class="s2">, </span><span class="s1">score4</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_svr_errors():</span>
    <span class="s1">X = [[</span><span class="s4">0.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1.0</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">]</span>

    <span class="s3"># Bad kernel</span>
    <span class="s1">clf = svm.SVR(kernel=</span><span class="s2">lambda </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: np.array([[</span><span class="s4">1.0</span><span class="s1">]]))</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.predict(X)</span>


<span class="s2">def </span><span class="s1">test_oneclass():</span>
    <span class="s3"># Test OneClassSVM</span>
    <span class="s1">clf = svm.OneClassSVM()</span>
    <span class="s1">clf.fit(X)</span>
    <span class="s1">pred = clf.predict(T)</span>

    <span class="s1">assert_array_equal(pred</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s2">assert </span><span class="s1">pred.dtype == np.dtype(</span><span class="s5">&quot;intp&quot;</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(clf.intercept_</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1.218</span><span class="s1">]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(clf.dual_coef_</span><span class="s2">, </span><span class="s1">[[</span><span class="s4">0.750</span><span class="s2">, </span><span class="s4">0.750</span><span class="s2">, </span><span class="s4">0.750</span><span class="s2">, </span><span class="s4">0.750</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(AttributeError):</span>
        <span class="s1">(</span><span class="s2">lambda</span><span class="s1">: clf.coef_)()</span>


<span class="s2">def </span><span class="s1">test_oneclass_decision_function():</span>
    <span class="s3"># Test OneClassSVM decision function</span>
    <span class="s1">clf = svm.OneClassSVM()</span>
    <span class="s1">rnd = check_random_state(</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s3"># Generate train data</span>
    <span class="s1">X = </span><span class="s4">0.3 </span><span class="s1">* rnd.randn(</span><span class="s4">100</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">X_train = np.r_[X + </span><span class="s4">2</span><span class="s2">, </span><span class="s1">X - </span><span class="s4">2</span><span class="s1">]</span>

    <span class="s3"># Generate some regular novel observations</span>
    <span class="s1">X = </span><span class="s4">0.3 </span><span class="s1">* rnd.randn(</span><span class="s4">20</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">X_test = np.r_[X + </span><span class="s4">2</span><span class="s2">, </span><span class="s1">X - </span><span class="s4">2</span><span class="s1">]</span>
    <span class="s3"># Generate some abnormal novel observations</span>
    <span class="s1">X_outliers = rnd.uniform(low=-</span><span class="s4">4</span><span class="s2">, </span><span class="s1">high=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">size=(</span><span class="s4">20</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))</span>

    <span class="s3"># fit the model</span>
    <span class="s1">clf = svm.OneClassSVM(nu=</span><span class="s4">0.1</span><span class="s2">, </span><span class="s1">kernel=</span><span class="s5">&quot;rbf&quot;</span><span class="s2">, </span><span class="s1">gamma=</span><span class="s4">0.1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_train)</span>

    <span class="s3"># predict things</span>
    <span class="s1">y_pred_test = clf.predict(X_test)</span>
    <span class="s2">assert </span><span class="s1">np.mean(y_pred_test == </span><span class="s4">1</span><span class="s1">) &gt; </span><span class="s4">0.9</span>
    <span class="s1">y_pred_outliers = clf.predict(X_outliers)</span>
    <span class="s2">assert </span><span class="s1">np.mean(y_pred_outliers == -</span><span class="s4">1</span><span class="s1">) &gt; </span><span class="s4">0.9</span>
    <span class="s1">dec_func_test = clf.decision_function(X_test)</span>
    <span class="s1">assert_array_equal((dec_func_test &gt; </span><span class="s4">0</span><span class="s1">).ravel()</span><span class="s2">, </span><span class="s1">y_pred_test == </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">dec_func_outliers = clf.decision_function(X_outliers)</span>
    <span class="s1">assert_array_equal((dec_func_outliers &gt; </span><span class="s4">0</span><span class="s1">).ravel()</span><span class="s2">, </span><span class="s1">y_pred_outliers == </span><span class="s4">1</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_oneclass_score_samples():</span>
    <span class="s1">X_train = [[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]]</span>
    <span class="s1">clf = svm.OneClassSVM(gamma=</span><span class="s4">1</span><span class="s1">).fit(X_train)</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">clf.score_samples([[</span><span class="s4">2.0</span><span class="s2">, </span><span class="s4">2.0</span><span class="s1">]])</span><span class="s2">,</span>
        <span class="s1">clf.decision_function([[</span><span class="s4">2.0</span><span class="s2">, </span><span class="s4">2.0</span><span class="s1">]]) + clf.offset_</span><span class="s2">,</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_tweak_params():</span>
    <span class="s3"># Make sure some tweaking of parameters works.</span>
    <span class="s3"># We change clf.dual_coef_ at run time and expect .predict() to change</span>
    <span class="s3"># accordingly. Notice that this is not trivial since it involves a lot</span>
    <span class="s3"># of C/Python copying in the libsvm bindings.</span>
    <span class="s3"># The success of this test ensures that the mapping between libsvm and</span>
    <span class="s3"># the python classifier is complete.</span>
    <span class="s1">clf = svm.SVC(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s4">1.0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">assert_array_equal(clf.dual_coef_</span><span class="s2">, </span><span class="s1">[[-</span><span class="s4">0.25</span><span class="s2">, </span><span class="s4">0.25</span><span class="s1">]])</span>
    <span class="s1">assert_array_equal(clf.predict([[-</span><span class="s4">0.1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.1</span><span class="s1">]])</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">clf._dual_coef_ = np.array([[</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]])</span>
    <span class="s1">assert_array_equal(clf.predict([[-</span><span class="s4">0.1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.1</span><span class="s1">]])</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_probability():</span>
    <span class="s3"># Predict probabilities using SVC</span>
    <span class="s3"># This uses cross validation, so we use a slightly bigger testing set.</span>

    <span class="s2">for </span><span class="s1">clf </span><span class="s2">in </span><span class="s1">(</span>
        <span class="s1">svm.SVC(probability=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">C=</span><span class="s4">1.0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">svm.NuSVC(probability=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">clf.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>

        <span class="s1">prob_predict = clf.predict_proba(iris.data)</span>
        <span class="s1">assert_array_almost_equal(np.sum(prob_predict</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.ones(iris.data.shape[</span><span class="s4">0</span><span class="s1">]))</span>
        <span class="s2">assert </span><span class="s1">np.mean(np.argmax(prob_predict</span><span class="s2">, </span><span class="s4">1</span><span class="s1">) == clf.predict(iris.data)) &gt; </span><span class="s4">0.9</span>

        <span class="s1">assert_almost_equal(</span>
            <span class="s1">clf.predict_proba(iris.data)</span><span class="s2">, </span><span class="s1">np.exp(clf.predict_log_proba(iris.data))</span><span class="s2">, </span><span class="s4">8</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_decision_function():</span>
    <span class="s3"># Test decision_function</span>
    <span class="s3"># Sanity check, test that decision_function implemented in python</span>
    <span class="s3"># returns the same as the one in libsvm</span>
    <span class="s3"># multi class:</span>
    <span class="s1">clf = svm.SVC(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s4">0.1</span><span class="s2">, </span><span class="s1">decision_function_shape=</span><span class="s5">&quot;ovo&quot;</span><span class="s1">).fit(</span>
        <span class="s1">iris.data</span><span class="s2">, </span><span class="s1">iris.target</span>
    <span class="s1">)</span>

    <span class="s1">dec = np.dot(iris.data</span><span class="s2">, </span><span class="s1">clf.coef_.T) + clf.intercept_</span>

    <span class="s1">assert_array_almost_equal(dec</span><span class="s2">, </span><span class="s1">clf.decision_function(iris.data))</span>

    <span class="s3"># binary:</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">dec = np.dot(X</span><span class="s2">, </span><span class="s1">clf.coef_.T) + clf.intercept_</span>
    <span class="s1">prediction = clf.predict(X)</span>
    <span class="s1">assert_array_almost_equal(dec.ravel()</span><span class="s2">, </span><span class="s1">clf.decision_function(X))</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">prediction</span><span class="s2">, </span><span class="s1">clf.classes_[(clf.decision_function(X) &gt; </span><span class="s4">0</span><span class="s1">).astype(int)]</span>
    <span class="s1">)</span>
    <span class="s1">expected = np.array([-</span><span class="s4">1.0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.66</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.66</span><span class="s2">, </span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(clf.decision_function(X)</span><span class="s2">, </span><span class="s1">expected</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>

    <span class="s3"># kernel binary:</span>
    <span class="s1">clf = svm.SVC(kernel=</span><span class="s5">&quot;rbf&quot;</span><span class="s2">, </span><span class="s1">gamma=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">decision_function_shape=</span><span class="s5">&quot;ovo&quot;</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>

    <span class="s1">rbfs = rbf_kernel(X</span><span class="s2">, </span><span class="s1">clf.support_vectors_</span><span class="s2">, </span><span class="s1">gamma=clf.gamma)</span>
    <span class="s1">dec = np.dot(rbfs</span><span class="s2">, </span><span class="s1">clf.dual_coef_.T) + clf.intercept_</span>
    <span class="s1">assert_array_almost_equal(dec.ravel()</span><span class="s2">, </span><span class="s1">clf.decision_function(X))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;SVM&quot;</span><span class="s2">, </span><span class="s1">(svm.SVC</span><span class="s2">, </span><span class="s1">svm.NuSVC))</span>
<span class="s2">def </span><span class="s1">test_decision_function_shape(SVM):</span>
    <span class="s3"># check that decision_function_shape='ovr' or 'ovo' gives</span>
    <span class="s3"># correct shape and is consistent with predict</span>

    <span class="s1">clf = SVM(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">decision_function_shape=</span><span class="s5">&quot;ovr&quot;</span><span class="s1">).fit(</span>
        <span class="s1">iris.data</span><span class="s2">, </span><span class="s1">iris.target</span>
    <span class="s1">)</span>
    <span class="s1">dec = clf.decision_function(iris.data)</span>
    <span class="s2">assert </span><span class="s1">dec.shape == (len(iris.data)</span><span class="s2">, </span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">assert_array_equal(clf.predict(iris.data)</span><span class="s2">, </span><span class="s1">np.argmax(dec</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">))</span>

    <span class="s3"># with five classes:</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(n_samples=</span><span class="s4">80</span><span class="s2">, </span><span class="s1">centers=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test = train_test_split(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">clf = SVM(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">decision_function_shape=</span><span class="s5">&quot;ovr&quot;</span><span class="s1">).fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s1">dec = clf.decision_function(X_test)</span>
    <span class="s2">assert </span><span class="s1">dec.shape == (len(X_test)</span><span class="s2">, </span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">assert_array_equal(clf.predict(X_test)</span><span class="s2">, </span><span class="s1">np.argmax(dec</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">))</span>

    <span class="s3"># check shape of ovo_decition_function=True</span>
    <span class="s1">clf = SVM(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">decision_function_shape=</span><span class="s5">&quot;ovo&quot;</span><span class="s1">).fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s1">dec = clf.decision_function(X_train)</span>
    <span class="s2">assert </span><span class="s1">dec.shape == (len(X_train)</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_svr_predict():</span>
    <span class="s3"># Test SVR's decision_function</span>
    <span class="s3"># Sanity check, test that predict implemented in python</span>
    <span class="s3"># returns the same as the one in libsvm</span>

    <span class="s1">X = iris.data</span>
    <span class="s1">y = iris.target</span>

    <span class="s3"># linear kernel</span>
    <span class="s1">reg = svm.SVR(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s4">0.1</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">dec = np.dot(X</span><span class="s2">, </span><span class="s1">reg.coef_.T) + reg.intercept_</span>
    <span class="s1">assert_array_almost_equal(dec.ravel()</span><span class="s2">, </span><span class="s1">reg.predict(X).ravel())</span>

    <span class="s3"># rbf kernel</span>
    <span class="s1">reg = svm.SVR(kernel=</span><span class="s5">&quot;rbf&quot;</span><span class="s2">, </span><span class="s1">gamma=</span><span class="s4">1</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">rbfs = rbf_kernel(X</span><span class="s2">, </span><span class="s1">reg.support_vectors_</span><span class="s2">, </span><span class="s1">gamma=reg.gamma)</span>
    <span class="s1">dec = np.dot(rbfs</span><span class="s2">, </span><span class="s1">reg.dual_coef_.T) + reg.intercept_</span>
    <span class="s1">assert_array_almost_equal(dec.ravel()</span><span class="s2">, </span><span class="s1">reg.predict(X).ravel())</span>


<span class="s2">def </span><span class="s1">test_weight():</span>
    <span class="s3"># Test class weights</span>
    <span class="s1">clf = svm.SVC(class_weight={</span><span class="s4">1</span><span class="s1">: </span><span class="s4">0.1</span><span class="s1">})</span>
    <span class="s3"># we give a small weights to class 1</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s3"># so all predicted values belong to class 2</span>
    <span class="s1">assert_array_almost_equal(clf.predict(X)</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s1">] * </span><span class="s4">6</span><span class="s1">)</span>

    <span class="s1">X_</span><span class="s2">, </span><span class="s1">y_ = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">weights=[</span><span class="s4">0.833</span><span class="s2">, </span><span class="s4">0.167</span><span class="s1">]</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">2</span>
    <span class="s1">)</span>

    <span class="s2">for </span><span class="s1">clf </span><span class="s2">in </span><span class="s1">(</span>
        <span class="s1">linear_model.LogisticRegression()</span><span class="s2">,</span>
        <span class="s1">svm.LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">svm.SVC()</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">clf.set_params(class_weight={</span><span class="s4">0</span><span class="s1">: </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">: </span><span class="s4">10</span><span class="s1">})</span>
        <span class="s1">clf.fit(X_[:</span><span class="s4">100</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y_[:</span><span class="s4">100</span><span class="s1">])</span>
        <span class="s1">y_pred = clf.predict(X_[</span><span class="s4">100</span><span class="s1">:])</span>
        <span class="s2">assert </span><span class="s1">f1_score(y_[</span><span class="s4">100</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">y_pred) &gt; </span><span class="s4">0.3</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;estimator&quot;</span><span class="s2">, </span><span class="s1">[svm.SVC(C=</span><span class="s4">1e-2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">svm.NuSVC()])</span>
<span class="s2">def </span><span class="s1">test_svm_classifier_sided_sample_weight(estimator):</span>
    <span class="s3"># fit a linear SVM and check that giving more weight to opposed samples</span>
    <span class="s3"># in the space will flip the decision toward these samples.</span>
    <span class="s1">X = [[-</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]]</span>
    <span class="s1">estimator.set_params(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span>

    <span class="s3"># check that with unit weights, a sample is supposed to be predicted on</span>
    <span class="s3"># the boundary</span>
    <span class="s1">sample_weight = [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">6</span>
    <span class="s1">estimator.fit(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">y_pred = estimator.decision_function([[-</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]])</span>
    <span class="s2">assert </span><span class="s1">y_pred == pytest.approx(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s3"># give more weights to opposed samples</span>
    <span class="s1">sample_weight = [</span><span class="s4">10.0</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span>
    <span class="s1">estimator.fit(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">y_pred = estimator.decision_function([[-</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]])</span>
    <span class="s2">assert </span><span class="s1">y_pred &lt; </span><span class="s4">0</span>

    <span class="s1">sample_weight = [</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">10.0</span><span class="s2">, </span><span class="s4">10.0</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.1</span><span class="s1">]</span>
    <span class="s1">estimator.fit(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">y_pred = estimator.decision_function([[-</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]])</span>
    <span class="s2">assert </span><span class="s1">y_pred &gt; </span><span class="s4">0</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;estimator&quot;</span><span class="s2">, </span><span class="s1">[svm.SVR(C=</span><span class="s4">1e-2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">svm.NuSVR(C=</span><span class="s4">1e-2</span><span class="s1">)])</span>
<span class="s2">def </span><span class="s1">test_svm_regressor_sided_sample_weight(estimator):</span>
    <span class="s3"># similar test to test_svm_classifier_sided_sample_weight but for</span>
    <span class="s3"># SVM regressors</span>
    <span class="s1">X = [[-</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]]</span>
    <span class="s1">estimator.set_params(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span>

    <span class="s3"># check that with unit weights, a sample is supposed to be predicted on</span>
    <span class="s3"># the boundary</span>
    <span class="s1">sample_weight = [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">6</span>
    <span class="s1">estimator.fit(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">y_pred = estimator.predict([[-</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]])</span>
    <span class="s2">assert </span><span class="s1">y_pred == pytest.approx(</span><span class="s4">1.5</span><span class="s1">)</span>

    <span class="s3"># give more weights to opposed samples</span>
    <span class="s1">sample_weight = [</span><span class="s4">10.0</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span>
    <span class="s1">estimator.fit(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">y_pred = estimator.predict([[-</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]])</span>
    <span class="s2">assert </span><span class="s1">y_pred &lt; </span><span class="s4">1.5</span>

    <span class="s1">sample_weight = [</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">10.0</span><span class="s2">, </span><span class="s4">10.0</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.1</span><span class="s1">]</span>
    <span class="s1">estimator.fit(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">y_pred = estimator.predict([[-</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]])</span>
    <span class="s2">assert </span><span class="s1">y_pred &gt; </span><span class="s4">1.5</span>


<span class="s2">def </span><span class="s1">test_svm_equivalence_sample_weight_C():</span>
    <span class="s3"># test that rescaling all samples is the same as changing C</span>
    <span class="s1">clf = svm.SVC()</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">dual_coef_no_weight = clf.dual_coef_</span>
    <span class="s1">clf.set_params(C=</span><span class="s4">100</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">sample_weight=np.repeat(</span><span class="s4">0.01</span><span class="s2">, </span><span class="s1">len(X)))</span>
    <span class="s1">assert_allclose(dual_coef_no_weight</span><span class="s2">, </span><span class="s1">clf.dual_coef_)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;Estimator, err_msg&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(svm.SVC</span><span class="s2">, </span><span class="s5">&quot;Invalid input - all samples have zero or negative weights.&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(svm.NuSVC</span><span class="s2">, </span><span class="s5">&quot;(negative dimensions are not allowed|nu is infeasible)&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(svm.SVR</span><span class="s2">, </span><span class="s5">&quot;Invalid input - all samples have zero or negative weights.&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(svm.NuSVR</span><span class="s2">, </span><span class="s5">&quot;Invalid input - all samples have zero or negative weights.&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(svm.OneClassSVM</span><span class="s2">, </span><span class="s5">&quot;Invalid input - all samples have zero or negative weights.&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
    <span class="s1">ids=[</span><span class="s5">&quot;SVC&quot;</span><span class="s2">, </span><span class="s5">&quot;NuSVC&quot;</span><span class="s2">, </span><span class="s5">&quot;SVR&quot;</span><span class="s2">, </span><span class="s5">&quot;NuSVR&quot;</span><span class="s2">, </span><span class="s5">&quot;OneClassSVM&quot;</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;sample_weight&quot;</span><span class="s2">,</span>
    <span class="s1">[[</span><span class="s4">0</span><span class="s1">] * len(Y)</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">0.3</span><span class="s1">] * len(Y)]</span><span class="s2">,</span>
    <span class="s1">ids=[</span><span class="s5">&quot;weights-are-zero&quot;</span><span class="s2">, </span><span class="s5">&quot;weights-are-negative&quot;</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_negative_sample_weights_mask_all_samples(Estimator</span><span class="s2">, </span><span class="s1">err_msg</span><span class="s2">, </span><span class="s1">sample_weight):</span>
    <span class="s1">est = Estimator(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;Classifier, err_msg&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">svm.SVC</span><span class="s2">,</span>
            <span class="s1">(</span>
                <span class="s5">&quot;Invalid input - all samples with positive weights belong to the same&quot;</span>
                <span class="s5">&quot; class&quot;</span>
            <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(svm.NuSVC</span><span class="s2">, </span><span class="s5">&quot;specified nu is infeasible&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
    <span class="s1">ids=[</span><span class="s5">&quot;SVC&quot;</span><span class="s2">, </span><span class="s5">&quot;NuSVC&quot;</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;sample_weight&quot;</span><span class="s2">,</span>
    <span class="s1">[[</span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.3</span><span class="s1">]]</span><span class="s2">,</span>
    <span class="s1">ids=[</span><span class="s5">&quot;mask-label-1&quot;</span><span class="s2">, </span><span class="s5">&quot;mask-label-2&quot;</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_negative_weights_svc_leave_just_one_label(Classifier</span><span class="s2">, </span><span class="s1">err_msg</span><span class="s2">, </span><span class="s1">sample_weight):</span>
    <span class="s1">clf = Classifier(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;Classifier, model&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(svm.SVC</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;when-left&quot;</span><span class="s1">: [</span><span class="s4">0.3998</span><span class="s2">, </span><span class="s4">0.4</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;when-right&quot;</span><span class="s1">: [</span><span class="s4">0.4</span><span class="s2">, </span><span class="s4">0.3999</span><span class="s1">]})</span><span class="s2">,</span>
        <span class="s1">(svm.NuSVC</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;when-left&quot;</span><span class="s1">: [</span><span class="s4">0.3333</span><span class="s2">, </span><span class="s4">0.3333</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;when-right&quot;</span><span class="s1">: [</span><span class="s4">0.3333</span><span class="s2">, </span><span class="s4">0.3333</span><span class="s1">]})</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
    <span class="s1">ids=[</span><span class="s5">&quot;SVC&quot;</span><span class="s2">, </span><span class="s5">&quot;NuSVC&quot;</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;sample_weight, mask_side&quot;</span><span class="s2">,</span>
    <span class="s1">[([</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;when-left&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;when-right&quot;</span><span class="s1">)]</span><span class="s2">,</span>
    <span class="s1">ids=[</span><span class="s5">&quot;partial-mask-label-1&quot;</span><span class="s2">, </span><span class="s5">&quot;partial-mask-label-2&quot;</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_negative_weights_svc_leave_two_labels(</span>
    <span class="s1">Classifier</span><span class="s2">, </span><span class="s1">model</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">mask_side</span>
<span class="s1">):</span>
    <span class="s1">clf = Classifier(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_allclose(clf.coef_</span><span class="s2">, </span><span class="s1">[model[mask_side]]</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-3</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[svm.SVC</span><span class="s2">, </span><span class="s1">svm.NuSVC</span><span class="s2">, </span><span class="s1">svm.NuSVR]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s5">&quot;SVC&quot;</span><span class="s2">, </span><span class="s5">&quot;NuSVC&quot;</span><span class="s2">, </span><span class="s5">&quot;NuSVR&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;sample_weight&quot;</span><span class="s2">,</span>
    <span class="s1">[[</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s2">,</span>
    <span class="s1">ids=[</span><span class="s5">&quot;partial-mask-label-1&quot;</span><span class="s2">, </span><span class="s5">&quot;partial-mask-label-2&quot;</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_negative_weight_equal_coeffs(Estimator</span><span class="s2">, </span><span class="s1">sample_weight):</span>
    <span class="s3"># model generates equal coefficients</span>
    <span class="s1">est = Estimator(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">coef = np.abs(est.coef_).ravel()</span>
    <span class="s2">assert </span><span class="s1">coef[</span><span class="s4">0</span><span class="s1">] == pytest.approx(coef[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rel=</span><span class="s4">1e-3</span><span class="s1">)</span>


<span class="s1">@ignore_warnings(category=UndefinedMetricWarning)</span>
<span class="s2">def </span><span class="s1">test_auto_weight():</span>
    <span class="s3"># Test class weights for imbalanced data</span>
    <span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">LogisticRegression</span>

    <span class="s3"># We take as dataset the two-dimensional projection of iris so</span>
    <span class="s3"># that it is not separable and remove half of predictors from</span>
    <span class="s3"># class 1.</span>
    <span class="s3"># We add one to the targets as a non-regression test:</span>
    <span class="s3"># class_weight=&quot;balanced&quot;</span>
    <span class="s3"># used to work only when the labels where a range [0..K).</span>
    <span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">compute_class_weight</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = iris.data[:</span><span class="s2">, </span><span class="s1">:</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">iris.target + </span><span class="s4">1</span>
    <span class="s1">unbalanced = np.delete(np.arange(y.size)</span><span class="s2">, </span><span class="s1">np.where(y &gt; </span><span class="s4">2</span><span class="s1">)[</span><span class="s4">0</span><span class="s1">][::</span><span class="s4">2</span><span class="s1">])</span>

    <span class="s1">classes = np.unique(y[unbalanced])</span>
    <span class="s1">class_weights = compute_class_weight(</span><span class="s5">&quot;balanced&quot;</span><span class="s2">, </span><span class="s1">classes=classes</span><span class="s2">, </span><span class="s1">y=y[unbalanced])</span>
    <span class="s2">assert </span><span class="s1">np.argmax(class_weights) == </span><span class="s4">2</span>

    <span class="s2">for </span><span class="s1">clf </span><span class="s2">in </span><span class="s1">(</span>
        <span class="s1">svm.SVC(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">svm.LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">LogisticRegression()</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s3"># check that score is better when class='balanced' is set.</span>
        <span class="s1">y_pred = clf.fit(X[unbalanced]</span><span class="s2">, </span><span class="s1">y[unbalanced]).predict(X)</span>
        <span class="s1">clf.set_params(class_weight=</span><span class="s5">&quot;balanced&quot;</span><span class="s1">)</span>
        <span class="s1">y_pred_balanced = clf.fit(</span>
            <span class="s1">X[unbalanced]</span><span class="s2">,</span>
            <span class="s1">y[unbalanced]</span><span class="s2">,</span>
        <span class="s1">).predict(X)</span>
        <span class="s2">assert </span><span class="s1">metrics.f1_score(y</span><span class="s2">, </span><span class="s1">y_pred</span><span class="s2">, </span><span class="s1">average=</span><span class="s5">&quot;macro&quot;</span><span class="s1">) &lt;= metrics.f1_score(</span>
            <span class="s1">y</span><span class="s2">, </span><span class="s1">y_pred_balanced</span><span class="s2">, </span><span class="s1">average=</span><span class="s5">&quot;macro&quot;</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_bad_input():</span>
    <span class="s3"># Test dimensions for labels</span>
    <span class="s1">Y2 = Y[:-</span><span class="s4">1</span><span class="s1">]  </span><span class="s3"># wrong dimensions for labels</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">svm.SVC().fit(X</span><span class="s2">, </span><span class="s1">Y2)</span>

    <span class="s3"># Test with arrays that are non-contiguous.</span>
    <span class="s2">for </span><span class="s1">clf </span><span class="s2">in </span><span class="s1">(svm.SVC()</span><span class="s2">, </span><span class="s1">svm.LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)):</span>
        <span class="s1">Xf = np.asfortranarray(X)</span>
        <span class="s2">assert not </span><span class="s1">Xf.flags[</span><span class="s5">&quot;C_CONTIGUOUS&quot;</span><span class="s1">]</span>
        <span class="s1">yf = np.ascontiguousarray(np.tile(Y</span><span class="s2">, </span><span class="s1">(</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)).T)</span>
        <span class="s1">yf = yf[:</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s2">assert not </span><span class="s1">yf.flags[</span><span class="s5">&quot;F_CONTIGUOUS&quot;</span><span class="s1">]</span>
        <span class="s2">assert not </span><span class="s1">yf.flags[</span><span class="s5">&quot;C_CONTIGUOUS&quot;</span><span class="s1">]</span>
        <span class="s1">clf.fit(Xf</span><span class="s2">, </span><span class="s1">yf)</span>
        <span class="s1">assert_array_equal(clf.predict(T)</span><span class="s2">, </span><span class="s1">true_result)</span>

    <span class="s3"># error for precomputed kernelsx</span>
    <span class="s1">clf = svm.SVC(kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>

    <span class="s3"># predict with sparse input when trained with dense</span>
    <span class="s1">clf = svm.SVC().fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.predict(sparse.lil_matrix(X))</span>

    <span class="s1">Xt = np.array(X).T</span>
    <span class="s1">clf.fit(np.dot(X</span><span class="s2">, </span><span class="s1">Xt)</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.predict(X)</span>

    <span class="s1">clf = svm.SVC()</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.predict(Xt)</span>


<span class="s2">def </span><span class="s1">test_svc_nonfinite_params():</span>
    <span class="s3"># Check SVC throws ValueError when dealing with non-finite parameter values</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s4">10</span>
    <span class="s1">fmax = np.finfo(np.float64).max</span>
    <span class="s1">X = fmax * rng.uniform(size=(n_samples</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">y = rng.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s1">size=n_samples)</span>

    <span class="s1">clf = svm.SVC()</span>
    <span class="s1">msg = </span><span class="s5">&quot;The dual coefficients or intercepts are not finite&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_unicode_kernel():</span>
    <span class="s3"># Test that a unicode kernel name does not cause a TypeError</span>
    <span class="s1">clf = svm.SVC(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">probability=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">clf.predict_proba(T)</span>
    <span class="s1">_libsvm.cross_validation(</span>
        <span class="s1">iris.data</span><span class="s2">, </span><span class="s1">iris.target.astype(np.float64)</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s1">kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">random_seed=</span><span class="s4">0</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_sparse_precomputed():</span>
    <span class="s1">clf = svm.SVC(kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s1">)</span>
    <span class="s1">sparse_gram = sparse.csr_matrix([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s2">with </span><span class="s1">pytest.raises(TypeError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;Sparse precomputed&quot;</span><span class="s1">):</span>
        <span class="s1">clf.fit(sparse_gram</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_sparse_fit_support_vectors_empty():</span>
    <span class="s3"># Regression test for #14893</span>
    <span class="s1">X_train = sparse.csr_matrix(</span>
        <span class="s1">[[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]]</span>
    <span class="s1">)</span>
    <span class="s1">y_train = np.array([</span><span class="s4">0.04</span><span class="s2">, </span><span class="s4">0.04</span><span class="s2">, </span><span class="s4">0.10</span><span class="s2">, </span><span class="s4">0.16</span><span class="s1">])</span>
    <span class="s1">model = svm.SVR(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span>
    <span class="s1">model.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
    <span class="s2">assert not </span><span class="s1">model.support_vectors_.data.size</span>
    <span class="s2">assert not </span><span class="s1">model.dual_coef_.data.size</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;loss&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">&quot;hinge&quot;</span><span class="s2">, </span><span class="s5">&quot;squared_hinge&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;penalty&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">&quot;l1&quot;</span><span class="s2">, </span><span class="s5">&quot;l2&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;dual&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_linearsvc_parameters(loss</span><span class="s2">, </span><span class="s1">penalty</span><span class="s2">, </span><span class="s1">dual):</span>
    <span class="s3"># Test possible parameter combinations in LinearSVC</span>
    <span class="s3"># Generate list of possible parameter combinations</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">clf = svm.LinearSVC(penalty=penalty</span><span class="s2">, </span><span class="s1">loss=loss</span><span class="s2">, </span><span class="s1">dual=dual</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">(</span>
        <span class="s1">(loss</span><span class="s2">, </span><span class="s1">penalty) == (</span><span class="s5">&quot;hinge&quot;</span><span class="s2">, </span><span class="s5">&quot;l1&quot;</span><span class="s1">)</span>
        <span class="s2">or </span><span class="s1">(loss</span><span class="s2">, </span><span class="s1">penalty</span><span class="s2">, </span><span class="s1">dual) == (</span><span class="s5">&quot;hinge&quot;</span><span class="s2">, </span><span class="s5">&quot;l2&quot;</span><span class="s2">, False</span><span class="s1">)</span>
        <span class="s2">or </span><span class="s1">(penalty</span><span class="s2">, </span><span class="s1">dual) == (</span><span class="s5">&quot;l1&quot;</span><span class="s2">, True</span><span class="s1">)</span>
    <span class="s1">):</span>
        <span class="s2">with </span><span class="s1">pytest.raises(</span>
            <span class="s1">ValueError</span><span class="s2">,</span>
            <span class="s1">match=</span><span class="s5">&quot;Unsupported set of arguments.*penalty='%s.*loss='%s.*dual=%s&quot;</span>
            <span class="s1">% (penalty</span><span class="s2">, </span><span class="s1">loss</span><span class="s2">, </span><span class="s1">dual)</span><span class="s2">,</span>
        <span class="s1">):</span>
            <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_linearsvc():</span>
    <span class="s3"># Test basic routines using LinearSVC</span>
    <span class="s1">clf = svm.LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">Y)</span>

    <span class="s3"># by default should have intercept</span>
    <span class="s2">assert </span><span class="s1">clf.fit_intercept</span>

    <span class="s1">assert_array_equal(clf.predict(T)</span><span class="s2">, </span><span class="s1">true_result)</span>
    <span class="s1">assert_array_almost_equal(clf.intercept_</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">3</span><span class="s1">)</span>

    <span class="s3"># the same with l1 penalty</span>
    <span class="s1">clf = svm.LinearSVC(</span>
        <span class="s1">penalty=</span><span class="s5">&quot;l1&quot;</span><span class="s2">, </span><span class="s1">loss=</span><span class="s5">&quot;squared_hinge&quot;</span><span class="s2">, </span><span class="s1">dual=</span><span class="s2">False, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">assert_array_equal(clf.predict(T)</span><span class="s2">, </span><span class="s1">true_result)</span>

    <span class="s3"># l2 penalty with dual formulation</span>
    <span class="s1">clf = svm.LinearSVC(penalty=</span><span class="s5">&quot;l2&quot;</span><span class="s2">, </span><span class="s1">dual=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">assert_array_equal(clf.predict(T)</span><span class="s2">, </span><span class="s1">true_result)</span>

    <span class="s3"># l2 penalty, l1 loss</span>
    <span class="s1">clf = svm.LinearSVC(penalty=</span><span class="s5">&quot;l2&quot;</span><span class="s2">, </span><span class="s1">loss=</span><span class="s5">&quot;hinge&quot;</span><span class="s2">, </span><span class="s1">dual=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">assert_array_equal(clf.predict(T)</span><span class="s2">, </span><span class="s1">true_result)</span>

    <span class="s3"># test also decision function</span>
    <span class="s1">dec = clf.decision_function(T)</span>
    <span class="s1">res = (dec &gt; </span><span class="s4">0</span><span class="s1">).astype(int) + </span><span class="s4">1</span>
    <span class="s1">assert_array_equal(res</span><span class="s2">, </span><span class="s1">true_result)</span>


<span class="s2">def </span><span class="s1">test_linearsvc_crammer_singer():</span>
    <span class="s3"># Test LinearSVC with crammer_singer multi-class svm</span>
    <span class="s1">ovr_clf = svm.LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">cs_clf = svm.LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">multi_class=</span><span class="s5">&quot;crammer_singer&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">cs_clf.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>

    <span class="s3"># similar prediction for ovr and crammer-singer:</span>
    <span class="s2">assert </span><span class="s1">(ovr_clf.predict(iris.data) == cs_clf.predict(iris.data)).mean() &gt; </span><span class="s4">0.9</span>

    <span class="s3"># classifiers shouldn't be the same</span>
    <span class="s2">assert </span><span class="s1">(ovr_clf.coef_ != cs_clf.coef_).all()</span>

    <span class="s3"># test decision function</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">cs_clf.predict(iris.data)</span><span class="s2">,</span>
        <span class="s1">np.argmax(cs_clf.decision_function(iris.data)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">dec_func = np.dot(iris.data</span><span class="s2">, </span><span class="s1">cs_clf.coef_.T) + cs_clf.intercept_</span>
    <span class="s1">assert_array_almost_equal(dec_func</span><span class="s2">, </span><span class="s1">cs_clf.decision_function(iris.data))</span>


<span class="s2">def </span><span class="s1">test_linearsvc_fit_sampleweight():</span>
    <span class="s3"># check correct result when sample_weight is 1</span>
    <span class="s1">n_samples = len(X)</span>
    <span class="s1">unit_weight = np.ones(n_samples)</span>
    <span class="s1">clf = svm.LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">clf_unitweight = svm.LinearSVC(</span>
        <span class="s1">dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">tol=</span><span class="s4">1e-12</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">1000</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">sample_weight=unit_weight)</span>

    <span class="s3"># check if same as sample_weight=None</span>
    <span class="s1">assert_array_equal(clf_unitweight.predict(T)</span><span class="s2">, </span><span class="s1">clf.predict(T))</span>
    <span class="s1">assert_allclose(clf.coef_</span><span class="s2">, </span><span class="s1">clf_unitweight.coef_</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0.0001</span><span class="s1">)</span>

    <span class="s3"># check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where</span>
    <span class="s3"># X = X1 repeated n1 times, X2 repeated n2 times and so forth</span>

    <span class="s1">random_state = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">random_weight = random_state.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s1">n_samples)</span>
    <span class="s1">lsvc_unflat = svm.LinearSVC(</span>
        <span class="s1">dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">tol=</span><span class="s4">1e-12</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">1000</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">sample_weight=random_weight)</span>

    <span class="s1">pred1 = lsvc_unflat.predict(T)</span>

    <span class="s1">X_flat = np.repeat(X</span><span class="s2">, </span><span class="s1">random_weight</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">y_flat = np.repeat(Y</span><span class="s2">, </span><span class="s1">random_weight</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">lsvc_flat = svm.LinearSVC(</span>
        <span class="s1">dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">tol=</span><span class="s4">1e-12</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">1000</span>
    <span class="s1">).fit(X_flat</span><span class="s2">, </span><span class="s1">y_flat)</span>
    <span class="s1">pred2 = lsvc_flat.predict(T)</span>

    <span class="s1">assert_array_equal(pred1</span><span class="s2">, </span><span class="s1">pred2)</span>
    <span class="s1">assert_allclose(lsvc_unflat.coef_</span><span class="s2">, </span><span class="s1">lsvc_flat.coef_</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0.0001</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_crammer_singer_binary():</span>
    <span class="s3"># Test Crammer-Singer formulation in the binary case</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_classes=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">fit_intercept </span><span class="s2">in </span><span class="s1">(</span><span class="s2">True, False</span><span class="s1">):</span>
        <span class="s1">acc = (</span>
            <span class="s1">svm.LinearSVC(</span>
                <span class="s1">dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">,</span>
                <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
                <span class="s1">multi_class=</span><span class="s5">&quot;crammer_singer&quot;</span><span class="s2">,</span>
                <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
            <span class="s1">.score(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">acc &gt; </span><span class="s4">0.9</span>


<span class="s2">def </span><span class="s1">test_linearsvc_iris():</span>
    <span class="s3"># Test that LinearSVC gives plausible predictions on the iris dataset</span>
    <span class="s3"># Also, test symbolic class names (classes_).</span>
    <span class="s1">target = iris.target_names[iris.target]</span>
    <span class="s1">clf = svm.LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(iris.data</span><span class="s2">, </span><span class="s1">target)</span>
    <span class="s2">assert </span><span class="s1">set(clf.classes_) == set(iris.target_names)</span>
    <span class="s2">assert </span><span class="s1">np.mean(clf.predict(iris.data) == target) &gt; </span><span class="s4">0.8</span>

    <span class="s1">dec = clf.decision_function(iris.data)</span>
    <span class="s1">pred = iris.target_names[np.argmax(dec</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)]</span>
    <span class="s1">assert_array_equal(pred</span><span class="s2">, </span><span class="s1">clf.predict(iris.data))</span>


<span class="s2">def </span><span class="s1">test_dense_liblinear_intercept_handling(classifier=svm.LinearSVC):</span>
    <span class="s3"># Test that dense liblinear honours intercept_scaling param</span>
    <span class="s1">X = [[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">clf = classifier(</span>
        <span class="s1">fit_intercept=</span><span class="s2">True,</span>
        <span class="s1">penalty=</span><span class="s5">&quot;l1&quot;</span><span class="s2">,</span>
        <span class="s1">loss=</span><span class="s5">&quot;squared_hinge&quot;</span><span class="s2">,</span>
        <span class="s1">dual=</span><span class="s2">False,</span>
        <span class="s1">C=</span><span class="s4">4</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s4">1e-7</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">clf.intercept_scaling == </span><span class="s4">1</span><span class="s2">, </span><span class="s1">clf.intercept_scaling</span>
    <span class="s2">assert </span><span class="s1">clf.fit_intercept</span>

    <span class="s3"># when intercept_scaling is low the intercept value is highly &quot;penalized&quot;</span>
    <span class="s3"># by regularization</span>
    <span class="s1">clf.intercept_scaling = </span><span class="s4">1</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(clf.intercept_</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">5</span><span class="s1">)</span>

    <span class="s3"># when intercept_scaling is sufficiently high, the intercept value</span>
    <span class="s3"># is not affected by regularization</span>
    <span class="s1">clf.intercept_scaling = </span><span class="s4">100</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">intercept1 = clf.intercept_</span>
    <span class="s2">assert </span><span class="s1">intercept1 &lt; -</span><span class="s4">1</span>

    <span class="s3"># when intercept_scaling is sufficiently high, the intercept value</span>
    <span class="s3"># doesn't depend on intercept_scaling value</span>
    <span class="s1">clf.intercept_scaling = </span><span class="s4">1000</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">intercept2 = clf.intercept_</span>
    <span class="s1">assert_array_almost_equal(intercept1</span><span class="s2">, </span><span class="s1">intercept2</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_liblinear_set_coef():</span>
    <span class="s3"># multi-class case</span>
    <span class="s1">clf = svm.LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s1">).fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">values = clf.decision_function(iris.data)</span>
    <span class="s1">clf.coef_ = clf.coef_.copy()</span>
    <span class="s1">clf.intercept_ = clf.intercept_.copy()</span>
    <span class="s1">values2 = clf.decision_function(iris.data)</span>
    <span class="s1">assert_array_almost_equal(values</span><span class="s2">, </span><span class="s1">values2)</span>

    <span class="s3"># binary-class case</span>
    <span class="s1">X = [[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>

    <span class="s1">clf = svm.LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">values = clf.decision_function(X)</span>
    <span class="s1">clf.coef_ = clf.coef_.copy()</span>
    <span class="s1">clf.intercept_ = clf.intercept_.copy()</span>
    <span class="s1">values2 = clf.decision_function(X)</span>
    <span class="s1">assert_array_equal(values</span><span class="s2">, </span><span class="s1">values2)</span>


<span class="s2">def </span><span class="s1">test_immutable_coef_property():</span>
    <span class="s3"># Check that primal coef modification are not silently ignored</span>
    <span class="s1">svms = [</span>
        <span class="s1">svm.SVC(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">).fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span><span class="s2">,</span>
        <span class="s1">svm.NuSVC(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">).fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span><span class="s2">,</span>
        <span class="s1">svm.SVR(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">).fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span><span class="s2">,</span>
        <span class="s1">svm.NuSVR(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">).fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span><span class="s2">,</span>
        <span class="s1">svm.OneClassSVM(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">).fit(iris.data)</span><span class="s2">,</span>
    <span class="s1">]</span>
    <span class="s2">for </span><span class="s1">clf </span><span class="s2">in </span><span class="s1">svms:</span>
        <span class="s2">with </span><span class="s1">pytest.raises(AttributeError):</span>
            <span class="s1">clf.__setattr__(</span><span class="s5">&quot;coef_&quot;</span><span class="s2">, </span><span class="s1">np.arange(</span><span class="s4">3</span><span class="s1">))</span>
        <span class="s2">with </span><span class="s1">pytest.raises((RuntimeError</span><span class="s2">, </span><span class="s1">ValueError)):</span>
            <span class="s1">clf.coef_.__setitem__((</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_linearsvc_verbose():</span>
    <span class="s3"># stdout: redirect</span>
    <span class="s2">import </span><span class="s1">os</span>

    <span class="s1">stdout = os.dup(</span><span class="s4">1</span><span class="s1">)  </span><span class="s3"># save original stdout</span>
    <span class="s1">os.dup2(os.pipe()[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)  </span><span class="s3"># replace it</span>

    <span class="s3"># actual call</span>
    <span class="s1">clf = svm.LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">verbose=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>

    <span class="s3"># stdout: restore</span>
    <span class="s1">os.dup2(stdout</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)  </span><span class="s3"># restore original stdout</span>


<span class="s2">def </span><span class="s1">test_svc_clone_with_callable_kernel():</span>
    <span class="s3"># create SVM with callable linear kernel, check that results are the same</span>
    <span class="s3"># as with built-in linear kernel</span>
    <span class="s1">svm_callable = svm.SVC(</span>
        <span class="s1">kernel=</span><span class="s2">lambda </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: np.dot(x</span><span class="s2">, </span><span class="s1">y.T)</span><span class="s2">,</span>
        <span class="s1">probability=</span><span class="s2">True,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">decision_function_shape=</span><span class="s5">&quot;ovr&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s3"># clone for checking clonability with lambda functions..</span>
    <span class="s1">svm_cloned = base.clone(svm_callable)</span>
    <span class="s1">svm_cloned.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>

    <span class="s1">svm_builtin = svm.SVC(</span>
        <span class="s1">kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">probability=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">decision_function_shape=</span><span class="s5">&quot;ovr&quot;</span>
    <span class="s1">)</span>
    <span class="s1">svm_builtin.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>

    <span class="s1">assert_array_almost_equal(svm_cloned.dual_coef_</span><span class="s2">, </span><span class="s1">svm_builtin.dual_coef_)</span>
    <span class="s1">assert_array_almost_equal(svm_cloned.intercept_</span><span class="s2">, </span><span class="s1">svm_builtin.intercept_)</span>
    <span class="s1">assert_array_equal(svm_cloned.predict(iris.data)</span><span class="s2">, </span><span class="s1">svm_builtin.predict(iris.data))</span>

    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">svm_cloned.predict_proba(iris.data)</span><span class="s2">,</span>
        <span class="s1">svm_builtin.predict_proba(iris.data)</span><span class="s2">,</span>
        <span class="s1">decimal=</span><span class="s4">4</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">svm_cloned.decision_function(iris.data)</span><span class="s2">,</span>
        <span class="s1">svm_builtin.decision_function(iris.data)</span><span class="s2">,</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_svc_bad_kernel():</span>
    <span class="s1">svc = svm.SVC(kernel=</span><span class="s2">lambda </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: x)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">svc.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>


<span class="s2">def </span><span class="s1">test_libsvm_convergence_warnings():</span>
    <span class="s1">a = svm.SVC(</span>
        <span class="s1">kernel=</span><span class="s2">lambda </span><span class="s1">x</span><span class="s2">, </span><span class="s1">y: np.dot(x</span><span class="s2">, </span><span class="s1">y.T)</span><span class="s2">, </span><span class="s1">probability=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">2</span>
    <span class="s1">)</span>
    <span class="s1">warning_msg = (</span>
        <span class="s5">r&quot;Solver terminated early \(max_iter=2\).  Consider pre-processing &quot;</span>
        <span class="s5">r&quot;your data with StandardScaler or MinMaxScaler.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s2">, </span><span class="s1">match=warning_msg):</span>
        <span class="s1">a.fit(np.array(X)</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s2">assert </span><span class="s1">np.all(a.n_iter_ == </span><span class="s4">2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_unfitted():</span>
    <span class="s1">X = </span><span class="s5">&quot;foo!&quot;  </span><span class="s3"># input validation not required when SVM not fitted</span>

    <span class="s1">clf = svm.SVC()</span>
    <span class="s2">with </span><span class="s1">pytest.raises(Exception</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">r&quot;.*\bSVC\b.*\bnot\b.*\bfitted\b&quot;</span><span class="s1">):</span>
        <span class="s1">clf.predict(X)</span>

    <span class="s1">clf = svm.NuSVR()</span>
    <span class="s2">with </span><span class="s1">pytest.raises(Exception</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">r&quot;.*\bNuSVR\b.*\bnot\b.*\bfitted\b&quot;</span><span class="s1">):</span>
        <span class="s1">clf.predict(X)</span>


<span class="s3"># ignore convergence warnings from max_iter=1</span>
<span class="s1">@ignore_warnings</span>
<span class="s2">def </span><span class="s1">test_consistent_proba():</span>
    <span class="s1">a = svm.SVC(probability=</span><span class="s2">True, </span><span class="s1">max_iter=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">proba_1 = a.fit(X</span><span class="s2">, </span><span class="s1">Y).predict_proba(X)</span>
    <span class="s1">a = svm.SVC(probability=</span><span class="s2">True, </span><span class="s1">max_iter=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">proba_2 = a.fit(X</span><span class="s2">, </span><span class="s1">Y).predict_proba(X)</span>
    <span class="s1">assert_array_almost_equal(proba_1</span><span class="s2">, </span><span class="s1">proba_2)</span>


<span class="s2">def </span><span class="s1">test_linear_svm_convergence_warnings():</span>
    <span class="s3"># Test that warnings are raised if model does not converge</span>

    <span class="s1">lsvc = svm.LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">warning_msg = </span><span class="s5">&quot;Liblinear failed to converge, increase the number of iterations.&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s2">, </span><span class="s1">match=warning_msg):</span>
        <span class="s1">lsvc.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s3"># Check that we have an n_iter_ attribute with int type as opposed to a</span>
    <span class="s3"># numpy array or an np.int32 so as to match the docstring.</span>
    <span class="s2">assert </span><span class="s1">isinstance(lsvc.n_iter_</span><span class="s2">, </span><span class="s1">int)</span>
    <span class="s2">assert </span><span class="s1">lsvc.n_iter_ == </span><span class="s4">2</span>

    <span class="s1">lsvr = svm.LinearSVR(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s2">, </span><span class="s1">match=warning_msg):</span>
        <span class="s1">lsvr.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s2">assert </span><span class="s1">isinstance(lsvr.n_iter_</span><span class="s2">, </span><span class="s1">int)</span>
    <span class="s2">assert </span><span class="s1">lsvr.n_iter_ == </span><span class="s4">2</span>


<span class="s2">def </span><span class="s1">test_svr_coef_sign():</span>
    <span class="s3"># Test that SVR(kernel=&quot;linear&quot;) has coef_ with the right sign.</span>
    <span class="s3"># Non-regression test for #2933.</span>
    <span class="s1">X = np.random.RandomState(</span><span class="s4">21</span><span class="s1">).randn(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">y = np.random.RandomState(</span><span class="s4">12</span><span class="s1">).randn(</span><span class="s4">10</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">svr </span><span class="s2">in </span><span class="s1">[</span>
        <span class="s1">svm.SVR(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">svm.NuSVR(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">svm.LinearSVR(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]:</span>
        <span class="s1">svr.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">svr.predict(X)</span><span class="s2">, </span><span class="s1">np.dot(X</span><span class="s2">, </span><span class="s1">svr.coef_.ravel()) + svr.intercept_</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_lsvc_intercept_scaling_zero():</span>
    <span class="s3"># Test that intercept_scaling is ignored when fit_intercept is False</span>

    <span class="s1">lsvc = svm.LinearSVC(dual=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">fit_intercept=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">lsvc.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s2">assert </span><span class="s1">lsvc.intercept_ == </span><span class="s4">0.0</span>


<span class="s2">def </span><span class="s1">test_hasattr_predict_proba():</span>
    <span class="s3"># Method must be (un)available before or after fit, switched by</span>
    <span class="s3"># `probability` param</span>

    <span class="s1">G = svm.SVC(probability=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">hasattr(G</span><span class="s2">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s1">)</span>
    <span class="s1">G.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s2">assert </span><span class="s1">hasattr(G</span><span class="s2">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s1">)</span>

    <span class="s1">G = svm.SVC(probability=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(G</span><span class="s2">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s1">)</span>
    <span class="s1">G.fit(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(G</span><span class="s2">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s1">)</span>

    <span class="s3"># Switching to `probability=True` after fitting should make</span>
    <span class="s3"># predict_proba available, but calling it must not work:</span>
    <span class="s1">G.probability = </span><span class="s2">True</span>
    <span class="s2">assert </span><span class="s1">hasattr(G</span><span class="s2">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s1">)</span>
    <span class="s1">msg = </span><span class="s5">&quot;predict_proba is not available when fitted with probability=False&quot;</span>

    <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">G.predict_proba(iris.data)</span>


<span class="s2">def </span><span class="s1">test_decision_function_shape_two_class():</span>
    <span class="s2">for </span><span class="s1">n_classes </span><span class="s2">in </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]:</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(centers=n_classes</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s2">for </span><span class="s1">estimator </span><span class="s2">in </span><span class="s1">[svm.SVC</span><span class="s2">, </span><span class="s1">svm.NuSVC]:</span>
            <span class="s1">clf = OneVsRestClassifier(estimator(decision_function_shape=</span><span class="s5">&quot;ovr&quot;</span><span class="s1">)).fit(</span>
                <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span>
            <span class="s1">)</span>
            <span class="s2">assert </span><span class="s1">len(clf.predict(X)) == len(y)</span>


<span class="s2">def </span><span class="s1">test_ovr_decision_function():</span>
    <span class="s3"># One point from each quadrant represents one class</span>
    <span class="s1">X_train = np.array([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">y_train = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span>

    <span class="s3"># First point is closer to the decision boundaries than the second point</span>
    <span class="s1">base_points = np.array([[</span><span class="s4">5</span><span class="s2">, </span><span class="s4">5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">10</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]])</span>

    <span class="s3"># For all the quadrants (classes)</span>
    <span class="s1">X_test = np.vstack(</span>
        <span class="s1">(</span>
            <span class="s1">base_points * [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,  </span><span class="s3"># Q1</span>
            <span class="s1">base_points * [-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,  </span><span class="s3"># Q2</span>
            <span class="s1">base_points * [-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,  </span><span class="s3"># Q3</span>
            <span class="s1">base_points * [</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,  </span><span class="s3"># Q4</span>
        <span class="s1">)</span>
    <span class="s1">)</span>

    <span class="s1">y_test = [</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">2 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">2 </span><span class="s1">+ [</span><span class="s4">2</span><span class="s1">] * </span><span class="s4">2 </span><span class="s1">+ [</span><span class="s4">3</span><span class="s1">] * </span><span class="s4">2</span>

    <span class="s1">clf = svm.SVC(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">decision_function_shape=</span><span class="s5">&quot;ovr&quot;</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>

    <span class="s1">y_pred = clf.predict(X_test)</span>

    <span class="s3"># Test if the prediction is the same as y</span>
    <span class="s1">assert_array_equal(y_pred</span><span class="s2">, </span><span class="s1">y_test)</span>

    <span class="s1">deci_val = clf.decision_function(X_test)</span>

    <span class="s3"># Assert that the predicted class has the maximum value</span>
    <span class="s1">assert_array_equal(np.argmax(deci_val</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">y_pred)</span>

    <span class="s3"># Get decision value at test points for the predicted class</span>
    <span class="s1">pred_class_deci_val = deci_val[range(</span><span class="s4">8</span><span class="s1">)</span><span class="s2">, </span><span class="s1">y_pred].reshape((</span><span class="s4">4</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))</span>

    <span class="s3"># Assert pred_class_deci_val &gt; 0 here</span>
    <span class="s2">assert </span><span class="s1">np.min(pred_class_deci_val) &gt; </span><span class="s4">0.0</span>

    <span class="s3"># Test if the first point has lower decision value on every quadrant</span>
    <span class="s3"># compared to the second point</span>
    <span class="s2">assert </span><span class="s1">np.all(pred_class_deci_val[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] &lt; pred_class_deci_val[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;SVCClass&quot;</span><span class="s2">, </span><span class="s1">[svm.SVC</span><span class="s2">, </span><span class="s1">svm.NuSVC])</span>
<span class="s2">def </span><span class="s1">test_svc_invalid_break_ties_param(SVCClass):</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(random_state=</span><span class="s4">42</span><span class="s1">)</span>

    <span class="s1">svm = SVCClass(</span>
        <span class="s1">kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s2">, </span><span class="s1">decision_function_shape=</span><span class="s5">&quot;ovo&quot;</span><span class="s2">, </span><span class="s1">break_ties=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s4">42</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;break_ties must be False&quot;</span><span class="s1">):</span>
        <span class="s1">svm.predict(y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;SVCClass&quot;</span><span class="s2">, </span><span class="s1">[svm.SVC</span><span class="s2">, </span><span class="s1">svm.NuSVC])</span>
<span class="s2">def </span><span class="s1">test_svc_ovr_tie_breaking(SVCClass):</span>
    <span class="s0">&quot;&quot;&quot;Test if predict breaks ties in OVR mode. 
    Related issue: https://github.com/scikit-learn/scikit-learn/issues/8277 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(random_state=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">n_samples=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s1">xs = np.linspace(X[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">].min()</span><span class="s2">, </span><span class="s1">X[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">].max()</span><span class="s2">, </span><span class="s4">100</span><span class="s1">)</span>
    <span class="s1">ys = np.linspace(X[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">].min()</span><span class="s2">, </span><span class="s1">X[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">].max()</span><span class="s2">, </span><span class="s4">100</span><span class="s1">)</span>
    <span class="s1">xx</span><span class="s2">, </span><span class="s1">yy = np.meshgrid(xs</span><span class="s2">, </span><span class="s1">ys)</span>

    <span class="s1">common_params = dict(</span>
        <span class="s1">kernel=</span><span class="s5">&quot;rbf&quot;</span><span class="s2">, </span><span class="s1">gamma=</span><span class="s4">1e6</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s2">, </span><span class="s1">decision_function_shape=</span><span class="s5">&quot;ovr&quot;</span>
    <span class="s1">)</span>
    <span class="s1">svm = SVCClass(</span>
        <span class="s1">break_ties=</span><span class="s2">False,</span>
        <span class="s1">**common_params</span><span class="s2">,</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">pred = svm.predict(np.c_[xx.ravel()</span><span class="s2">, </span><span class="s1">yy.ravel()])</span>
    <span class="s1">dv = svm.decision_function(np.c_[xx.ravel()</span><span class="s2">, </span><span class="s1">yy.ravel()])</span>
    <span class="s2">assert not </span><span class="s1">np.all(pred == np.argmax(dv</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">))</span>

    <span class="s1">svm = SVCClass(</span>
        <span class="s1">break_ties=</span><span class="s2">True,</span>
        <span class="s1">**common_params</span><span class="s2">,</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">pred = svm.predict(np.c_[xx.ravel()</span><span class="s2">, </span><span class="s1">yy.ravel()])</span>
    <span class="s1">dv = svm.decision_function(np.c_[xx.ravel()</span><span class="s2">, </span><span class="s1">yy.ravel()])</span>
    <span class="s2">assert </span><span class="s1">np.all(pred == np.argmax(dv</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">))</span>


<span class="s2">def </span><span class="s1">test_gamma_scale():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = [[</span><span class="s4">0.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1.0</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>

    <span class="s1">clf = svm.SVC()</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(clf._gamma</span><span class="s2">, </span><span class="s4">4</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;SVM, params&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(LinearSVC</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;penalty&quot;</span><span class="s1">: </span><span class="s5">&quot;l1&quot;</span><span class="s2">, </span><span class="s5">&quot;loss&quot;</span><span class="s1">: </span><span class="s5">&quot;squared_hinge&quot;</span><span class="s2">, </span><span class="s5">&quot;dual&quot;</span><span class="s1">: </span><span class="s2">False</span><span class="s1">})</span><span class="s2">,</span>
        <span class="s1">(LinearSVC</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;penalty&quot;</span><span class="s1">: </span><span class="s5">&quot;l2&quot;</span><span class="s2">, </span><span class="s5">&quot;loss&quot;</span><span class="s1">: </span><span class="s5">&quot;squared_hinge&quot;</span><span class="s2">, </span><span class="s5">&quot;dual&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">})</span><span class="s2">,</span>
        <span class="s1">(LinearSVC</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;penalty&quot;</span><span class="s1">: </span><span class="s5">&quot;l2&quot;</span><span class="s2">, </span><span class="s5">&quot;loss&quot;</span><span class="s1">: </span><span class="s5">&quot;squared_hinge&quot;</span><span class="s2">, </span><span class="s5">&quot;dual&quot;</span><span class="s1">: </span><span class="s2">False</span><span class="s1">})</span><span class="s2">,</span>
        <span class="s1">(LinearSVC</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;penalty&quot;</span><span class="s1">: </span><span class="s5">&quot;l2&quot;</span><span class="s2">, </span><span class="s5">&quot;loss&quot;</span><span class="s1">: </span><span class="s5">&quot;hinge&quot;</span><span class="s2">, </span><span class="s5">&quot;dual&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">})</span><span class="s2">,</span>
        <span class="s1">(LinearSVR</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;loss&quot;</span><span class="s1">: </span><span class="s5">&quot;epsilon_insensitive&quot;</span><span class="s2">, </span><span class="s5">&quot;dual&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">})</span><span class="s2">,</span>
        <span class="s1">(LinearSVR</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;loss&quot;</span><span class="s1">: </span><span class="s5">&quot;squared_epsilon_insensitive&quot;</span><span class="s2">, </span><span class="s5">&quot;dual&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">})</span><span class="s2">,</span>
        <span class="s1">(LinearSVR</span><span class="s2">, </span><span class="s1">{</span><span class="s5">&quot;loss&quot;</span><span class="s1">: </span><span class="s5">&quot;squared_epsilon_insensitive&quot;</span><span class="s2">, </span><span class="s5">&quot;dual&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">})</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_linearsvm_liblinear_sample_weight(SVM</span><span class="s2">, </span><span class="s1">params):</span>
    <span class="s1">X = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">4</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s1">dtype=np.dtype(</span><span class="s5">&quot;float&quot;</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">y = np.array(</span>
        <span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.dtype(</span><span class="s5">&quot;int&quot;</span><span class="s1">)</span>
    <span class="s1">)</span>

    <span class="s1">X2 = np.vstack([X</span><span class="s2">, </span><span class="s1">X])</span>
    <span class="s1">y2 = np.hstack([y</span><span class="s2">, </span><span class="s4">3 </span><span class="s1">- y])</span>
    <span class="s1">sample_weight = np.ones(shape=len(y) * </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">sample_weight[len(y) :] = </span><span class="s4">0</span>
    <span class="s1">X2</span><span class="s2">, </span><span class="s1">y2</span><span class="s2">, </span><span class="s1">sample_weight = shuffle(X2</span><span class="s2">, </span><span class="s1">y2</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">base_estimator = SVM(random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">base_estimator.set_params(**params)</span>
    <span class="s1">base_estimator.set_params(tol=</span><span class="s4">1e-12</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">1000</span><span class="s1">)</span>
    <span class="s1">est_no_weight = base.clone(base_estimator).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">est_with_weight = base.clone(base_estimator).fit(</span>
        <span class="s1">X2</span><span class="s2">, </span><span class="s1">y2</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight</span>
    <span class="s1">)</span>

    <span class="s2">for </span><span class="s1">method </span><span class="s2">in </span><span class="s1">(</span><span class="s5">&quot;predict&quot;</span><span class="s2">, </span><span class="s5">&quot;decision_function&quot;</span><span class="s1">):</span>
        <span class="s2">if </span><span class="s1">hasattr(base_estimator</span><span class="s2">, </span><span class="s1">method):</span>
            <span class="s1">X_est_no_weight = getattr(est_no_weight</span><span class="s2">, </span><span class="s1">method)(X)</span>
            <span class="s1">X_est_with_weight = getattr(est_with_weight</span><span class="s2">, </span><span class="s1">method)(X)</span>
            <span class="s1">assert_allclose(X_est_no_weight</span><span class="s2">, </span><span class="s1">X_est_with_weight)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;Klass&quot;</span><span class="s2">, </span><span class="s1">(OneClassSVM</span><span class="s2">, </span><span class="s1">SVR</span><span class="s2">, </span><span class="s1">NuSVR))</span>
<span class="s2">def </span><span class="s1">test_n_support(Klass):</span>
    <span class="s3"># Make n_support is correct for oneclass and SVR (used to be</span>
    <span class="s3"># non-initialized)</span>
    <span class="s3"># this is a non regression test for issue #14774</span>
    <span class="s1">X = np.array([[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.44</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.45</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.46</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">y = np.arange(X.shape[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">est = Klass()</span>
    <span class="s2">assert not </span><span class="s1">hasattr(est</span><span class="s2">, </span><span class="s5">&quot;n_support_&quot;</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">est.n_support_[</span><span class="s4">0</span><span class="s1">] == est.support_vectors_.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s2">assert </span><span class="s1">est.n_support_.size == </span><span class="s4">1</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[svm.SVC</span><span class="s2">, </span><span class="s1">svm.SVR])</span>
<span class="s2">def </span><span class="s1">test_custom_kernel_not_array_input(Estimator):</span>
    <span class="s0">&quot;&quot;&quot;Test using a custom kernel that is not fed with array-like for floats&quot;&quot;&quot;</span>
    <span class="s1">data = [</span><span class="s5">&quot;A A&quot;</span><span class="s2">, </span><span class="s5">&quot;A&quot;</span><span class="s2">, </span><span class="s5">&quot;B&quot;</span><span class="s2">, </span><span class="s5">&quot;B B&quot;</span><span class="s2">, </span><span class="s5">&quot;A B&quot;</span><span class="s1">]</span>
    <span class="s1">X = np.array([[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]])  </span><span class="s3"># count encoding</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>

    <span class="s2">def </span><span class="s1">string_kernel(X1</span><span class="s2">, </span><span class="s1">X2):</span>
        <span class="s2">assert </span><span class="s1">isinstance(X1[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">str)</span>
        <span class="s1">n_samples1 = _num_samples(X1)</span>
        <span class="s1">n_samples2 = _num_samples(X2)</span>
        <span class="s1">K = np.zeros((n_samples1</span><span class="s2">, </span><span class="s1">n_samples2))</span>
        <span class="s2">for </span><span class="s1">ii </span><span class="s2">in </span><span class="s1">range(n_samples1):</span>
            <span class="s2">for </span><span class="s1">jj </span><span class="s2">in </span><span class="s1">range(ii</span><span class="s2">, </span><span class="s1">n_samples2):</span>
                <span class="s1">K[ii</span><span class="s2">, </span><span class="s1">jj] = X1[ii].count(</span><span class="s5">&quot;A&quot;</span><span class="s1">) * X2[jj].count(</span><span class="s5">&quot;A&quot;</span><span class="s1">)</span>
                <span class="s1">K[ii</span><span class="s2">, </span><span class="s1">jj] += X1[ii].count(</span><span class="s5">&quot;B&quot;</span><span class="s1">) * X2[jj].count(</span><span class="s5">&quot;B&quot;</span><span class="s1">)</span>
                <span class="s1">K[jj</span><span class="s2">, </span><span class="s1">ii] = K[ii</span><span class="s2">, </span><span class="s1">jj]</span>
        <span class="s2">return </span><span class="s1">K</span>

    <span class="s1">K = string_kernel(data</span><span class="s2">, </span><span class="s1">data)</span>
    <span class="s1">assert_array_equal(np.dot(X</span><span class="s2">, </span><span class="s1">X.T)</span><span class="s2">, </span><span class="s1">K)</span>

    <span class="s1">svc1 = Estimator(kernel=string_kernel).fit(data</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">svc2 = Estimator(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">svc3 = Estimator(kernel=</span><span class="s5">&quot;precomputed&quot;</span><span class="s1">).fit(K</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">assert </span><span class="s1">svc1.score(data</span><span class="s2">, </span><span class="s1">y) == svc3.score(K</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">svc1.score(data</span><span class="s2">, </span><span class="s1">y) == svc2.score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">if </span><span class="s1">hasattr(svc1</span><span class="s2">, </span><span class="s5">&quot;decision_function&quot;</span><span class="s1">):  </span><span class="s3"># classifier</span>
        <span class="s1">assert_allclose(svc1.decision_function(data)</span><span class="s2">, </span><span class="s1">svc2.decision_function(X))</span>
        <span class="s1">assert_allclose(svc1.decision_function(data)</span><span class="s2">, </span><span class="s1">svc3.decision_function(K))</span>
        <span class="s1">assert_array_equal(svc1.predict(data)</span><span class="s2">, </span><span class="s1">svc2.predict(X))</span>
        <span class="s1">assert_array_equal(svc1.predict(data)</span><span class="s2">, </span><span class="s1">svc3.predict(K))</span>
    <span class="s2">else</span><span class="s1">:  </span><span class="s3"># regressor</span>
        <span class="s1">assert_allclose(svc1.predict(data)</span><span class="s2">, </span><span class="s1">svc2.predict(X))</span>
        <span class="s1">assert_allclose(svc1.predict(data)</span><span class="s2">, </span><span class="s1">svc3.predict(K))</span>


<span class="s2">def </span><span class="s1">test_svc_raises_error_internal_representation():</span>
    <span class="s0">&quot;&quot;&quot;Check that SVC raises error when internal representation is altered. 
 
    Non-regression test for #18891 and https://nvd.nist.gov/vuln/detail/CVE-2020-28975 
    &quot;&quot;&quot;</span>
    <span class="s1">clf = svm.SVC(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">clf._n_support[</span><span class="s4">0</span><span class="s1">] = </span><span class="s4">1000000</span>

    <span class="s1">msg = </span><span class="s5">&quot;The internal representation of SVC was altered&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.predict(X)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;estimator, expected_n_iter_type&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(svm.SVC</span><span class="s2">, </span><span class="s1">np.ndarray)</span><span class="s2">,</span>
        <span class="s1">(svm.NuSVC</span><span class="s2">, </span><span class="s1">np.ndarray)</span><span class="s2">,</span>
        <span class="s1">(svm.SVR</span><span class="s2">, </span><span class="s1">int)</span><span class="s2">,</span>
        <span class="s1">(svm.NuSVR</span><span class="s2">, </span><span class="s1">int)</span><span class="s2">,</span>
        <span class="s1">(svm.OneClassSVM</span><span class="s2">, </span><span class="s1">int)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;dataset&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">make_classification(n_classes=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">n_informative=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">make_classification(n_classes=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">n_informative=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">make_classification(n_classes=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">n_informative=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_n_iter_libsvm(estimator</span><span class="s2">, </span><span class="s1">expected_n_iter_type</span><span class="s2">, </span><span class="s1">dataset):</span>
    <span class="s3"># Check that the type of n_iter_ is correct for the classes that inherit</span>
    <span class="s3"># from BaseSVC.</span>
    <span class="s3"># Note that for SVC, and NuSVC this is an ndarray; while for SVR, NuSVR, and</span>
    <span class="s3"># OneClassSVM, it is an int.</span>
    <span class="s3"># For SVC and NuSVC also check the shape of n_iter_.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = dataset</span>
    <span class="s1">n_iter = estimator(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y).n_iter_</span>
    <span class="s2">assert </span><span class="s1">type(n_iter) == expected_n_iter_type</span>
    <span class="s2">if </span><span class="s1">estimator </span><span class="s2">in </span><span class="s1">[svm.SVC</span><span class="s2">, </span><span class="s1">svm.NuSVC]:</span>
        <span class="s1">n_classes = len(np.unique(y))</span>
        <span class="s2">assert </span><span class="s1">n_iter.shape == (n_classes * (n_classes - </span><span class="s4">1</span><span class="s1">) // </span><span class="s4">2</span><span class="s2">,</span><span class="s1">)</span>


<span class="s3"># TODO(1.4): Remove</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;Klass&quot;</span><span class="s2">, </span><span class="s1">[SVR</span><span class="s2">, </span><span class="s1">NuSVR</span><span class="s2">, </span><span class="s1">OneClassSVM])</span>
<span class="s2">def </span><span class="s1">test_svm_class_weights_deprecation(Klass):</span>
    <span class="s1">clf = Klass()</span>
    <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;error&quot;</span><span class="s2">, </span><span class="s1">FutureWarning)</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>
    <span class="s1">msg = (</span>
        <span class="s5">&quot;Attribute `class_weight_` was deprecated in version 1.2 and will be removed&quot;</span>
        <span class="s5">&quot; in 1.4&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s2">, </span><span class="s1">match=re.escape(msg)):</span>
        <span class="s1">getattr(clf</span><span class="s2">, </span><span class="s5">&quot;class_weight_&quot;</span><span class="s1">)</span>


<span class="s3"># TODO(1.5): Remove</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;Estimator&quot;</span><span class="s2">, </span><span class="s1">[LinearSVR</span><span class="s2">, </span><span class="s1">LinearSVC])</span>
<span class="s2">def </span><span class="s1">test_dual_auto_deprecation_warning(Estimator):</span>
    <span class="s1">svm = Estimator()</span>
    <span class="s1">msg = (</span>
        <span class="s5">&quot;The default value of `dual` will change from `True` to `'auto'` in&quot;</span>
        <span class="s5">&quot; 1.5. Set the value of `dual` explicitly to suppress the warning.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s2">, </span><span class="s1">match=re.escape(msg)):</span>
        <span class="s1">svm.fit(X</span><span class="s2">, </span><span class="s1">Y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;loss&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">&quot;squared_hinge&quot;</span><span class="s2">, </span><span class="s5">&quot;squared_epsilon_insensitive&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_dual_auto(loss):</span>
    <span class="s3"># OvR, L2, N &gt; M (6,2)</span>
    <span class="s1">dual = _validate_dual_parameter(</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">loss</span><span class="s2">, </span><span class="s5">&quot;l2&quot;</span><span class="s2">, </span><span class="s5">&quot;ovr&quot;</span><span class="s2">, </span><span class="s1">np.asarray(X))</span>
    <span class="s2">assert </span><span class="s1">dual </span><span class="s2">is False</span>
    <span class="s3"># OvR, L2, N &lt; M (2,6)</span>
    <span class="s1">dual = _validate_dual_parameter(</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">loss</span><span class="s2">, </span><span class="s5">&quot;l2&quot;</span><span class="s2">, </span><span class="s5">&quot;ovr&quot;</span><span class="s2">, </span><span class="s1">np.asarray(X).T)</span>
    <span class="s2">assert </span><span class="s1">dual </span><span class="s2">is True</span>


<span class="s2">def </span><span class="s1">test_dual_auto_edge_cases():</span>
    <span class="s3"># Hinge, OvR, L2, N &gt; M (6,2)</span>
    <span class="s1">dual = _validate_dual_parameter(</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s5">&quot;hinge&quot;</span><span class="s2">, </span><span class="s5">&quot;l2&quot;</span><span class="s2">, </span><span class="s5">&quot;ovr&quot;</span><span class="s2">, </span><span class="s1">np.asarray(X))</span>
    <span class="s2">assert </span><span class="s1">dual </span><span class="s2">is True  </span><span class="s3"># only supports True</span>
    <span class="s1">dual = _validate_dual_parameter(</span>
        <span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s5">&quot;epsilon_insensitive&quot;</span><span class="s2">, </span><span class="s5">&quot;l2&quot;</span><span class="s2">, </span><span class="s5">&quot;ovr&quot;</span><span class="s2">, </span><span class="s1">np.asarray(X)</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">dual </span><span class="s2">is True  </span><span class="s3"># only supports True</span>
    <span class="s3"># SqHinge, OvR, L1, N &lt; M (2,6)</span>
    <span class="s1">dual = _validate_dual_parameter(</span>
        <span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s5">&quot;squared_hinge&quot;</span><span class="s2">, </span><span class="s5">&quot;l1&quot;</span><span class="s2">, </span><span class="s5">&quot;ovr&quot;</span><span class="s2">, </span><span class="s1">np.asarray(X).T</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">dual </span><span class="s2">is False  </span><span class="s3"># only supports False</span>
</pre>
</body>
</html>