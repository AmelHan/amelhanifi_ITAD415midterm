<html>
<head>
<title>test_online_lda.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_online_lda.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">sys</span>
<span class="s0">from </span><span class="s1">io </span><span class="s0">import </span><span class="s1">StringIO</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">assert_array_equal</span>
<span class="s0">from </span><span class="s1">scipy.linalg </span><span class="s0">import </span><span class="s1">block_diag</span>
<span class="s0">from </span><span class="s1">scipy.sparse </span><span class="s0">import </span><span class="s1">csr_matrix</span>
<span class="s0">from </span><span class="s1">scipy.special </span><span class="s0">import </span><span class="s1">psi</span>

<span class="s0">from </span><span class="s1">sklearn.decomposition </span><span class="s0">import </span><span class="s1">LatentDirichletAllocation</span>
<span class="s0">from </span><span class="s1">sklearn.decomposition._online_lda_fast </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">_dirichlet_expectation_1d</span><span class="s0">,</span>
    <span class="s1">_dirichlet_expectation_2d</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.exceptions </span><span class="s0">import </span><span class="s1">NotFittedError</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s0">,</span>
    <span class="s1">assert_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s0">,</span>
    <span class="s1">if_safe_multiprocessing_with_blas</span><span class="s0">,</span>
<span class="s1">)</span>


<span class="s0">def </span><span class="s1">_build_sparse_mtx():</span>
    <span class="s2"># Create 3 topics and each topic has 3 distinct words.</span>
    <span class="s2"># (Each word only belongs to a single topic.)</span>
    <span class="s1">n_components = </span><span class="s3">3</span>
    <span class="s1">block = np.full((</span><span class="s3">3</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">n_components</span><span class="s0">, </span><span class="s1">dtype=int)</span>
    <span class="s1">blocks = [block] * n_components</span>
    <span class="s1">X = block_diag(*blocks)</span>
    <span class="s1">X = csr_matrix(X)</span>
    <span class="s0">return </span><span class="s1">(n_components</span><span class="s0">, </span><span class="s1">X)</span>


<span class="s0">def </span><span class="s1">test_lda_default_prior_params():</span>
    <span class="s2"># default prior parameter should be `1 / topics`</span>
    <span class="s2"># and verbose params should not affect result</span>
    <span class="s1">n_components</span><span class="s0">, </span><span class="s1">X = _build_sparse_mtx()</span>
    <span class="s1">prior = </span><span class="s3">1.0 </span><span class="s1">/ n_components</span>
    <span class="s1">lda_1 = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">doc_topic_prior=prior</span><span class="s0">,</span>
        <span class="s1">topic_word_prior=prior</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">lda_2 = LatentDirichletAllocation(n_components=n_components</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">topic_distr_1 = lda_1.fit_transform(X)</span>
    <span class="s1">topic_distr_2 = lda_2.fit_transform(X)</span>
    <span class="s1">assert_almost_equal(topic_distr_1</span><span class="s0">, </span><span class="s1">topic_distr_2)</span>


<span class="s0">def </span><span class="s1">test_lda_fit_batch():</span>
    <span class="s2"># Test LDA batch learning_offset (`fit` method with 'batch' learning)</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_components</span><span class="s0">, </span><span class="s1">X = _build_sparse_mtx()</span>
    <span class="s1">lda = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">evaluate_every=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">learning_method=</span><span class="s4">&quot;batch&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=rng</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">lda.fit(X)</span>

    <span class="s1">correct_idx_grps = [(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">3</span><span class="s0">, </span><span class="s3">4</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">6</span><span class="s0">, </span><span class="s3">7</span><span class="s0">, </span><span class="s3">8</span><span class="s1">)]</span>
    <span class="s0">for </span><span class="s1">component </span><span class="s0">in </span><span class="s1">lda.components_:</span>
        <span class="s2"># Find top 3 words in each LDA component</span>
        <span class="s1">top_idx = set(component.argsort()[-</span><span class="s3">3</span><span class="s1">:][::-</span><span class="s3">1</span><span class="s1">])</span>
        <span class="s0">assert </span><span class="s1">tuple(sorted(top_idx)) </span><span class="s0">in </span><span class="s1">correct_idx_grps</span>


<span class="s0">def </span><span class="s1">test_lda_fit_online():</span>
    <span class="s2"># Test LDA online learning (`fit` method with 'online' learning)</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_components</span><span class="s0">, </span><span class="s1">X = _build_sparse_mtx()</span>
    <span class="s1">lda = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">learning_offset=</span><span class="s3">10.0</span><span class="s0">,</span>
        <span class="s1">evaluate_every=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">learning_method=</span><span class="s4">&quot;online&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=rng</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">lda.fit(X)</span>

    <span class="s1">correct_idx_grps = [(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">3</span><span class="s0">, </span><span class="s3">4</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">6</span><span class="s0">, </span><span class="s3">7</span><span class="s0">, </span><span class="s3">8</span><span class="s1">)]</span>
    <span class="s0">for </span><span class="s1">component </span><span class="s0">in </span><span class="s1">lda.components_:</span>
        <span class="s2"># Find top 3 words in each LDA component</span>
        <span class="s1">top_idx = set(component.argsort()[-</span><span class="s3">3</span><span class="s1">:][::-</span><span class="s3">1</span><span class="s1">])</span>
        <span class="s0">assert </span><span class="s1">tuple(sorted(top_idx)) </span><span class="s0">in </span><span class="s1">correct_idx_grps</span>


<span class="s0">def </span><span class="s1">test_lda_partial_fit():</span>
    <span class="s2"># Test LDA online learning (`partial_fit` method)</span>
    <span class="s2"># (same as test_lda_batch)</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_components</span><span class="s0">, </span><span class="s1">X = _build_sparse_mtx()</span>
    <span class="s1">lda = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">learning_offset=</span><span class="s3">10.0</span><span class="s0">,</span>
        <span class="s1">total_samples=</span><span class="s3">100</span><span class="s0">,</span>
        <span class="s1">random_state=rng</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">3</span><span class="s1">):</span>
        <span class="s1">lda.partial_fit(X)</span>

    <span class="s1">correct_idx_grps = [(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">3</span><span class="s0">, </span><span class="s3">4</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">6</span><span class="s0">, </span><span class="s3">7</span><span class="s0">, </span><span class="s3">8</span><span class="s1">)]</span>
    <span class="s0">for </span><span class="s1">c </span><span class="s0">in </span><span class="s1">lda.components_:</span>
        <span class="s1">top_idx = set(c.argsort()[-</span><span class="s3">3</span><span class="s1">:][::-</span><span class="s3">1</span><span class="s1">])</span>
        <span class="s0">assert </span><span class="s1">tuple(sorted(top_idx)) </span><span class="s0">in </span><span class="s1">correct_idx_grps</span>


<span class="s0">def </span><span class="s1">test_lda_dense_input():</span>
    <span class="s2"># Test LDA with dense input.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_components</span><span class="s0">, </span><span class="s1">X = _build_sparse_mtx()</span>
    <span class="s1">lda = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">, </span><span class="s1">learning_method=</span><span class="s4">&quot;batch&quot;</span><span class="s0">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">lda.fit(X.toarray())</span>

    <span class="s1">correct_idx_grps = [(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">3</span><span class="s0">, </span><span class="s3">4</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">6</span><span class="s0">, </span><span class="s3">7</span><span class="s0">, </span><span class="s3">8</span><span class="s1">)]</span>
    <span class="s0">for </span><span class="s1">component </span><span class="s0">in </span><span class="s1">lda.components_:</span>
        <span class="s2"># Find top 3 words in each LDA component</span>
        <span class="s1">top_idx = set(component.argsort()[-</span><span class="s3">3</span><span class="s1">:][::-</span><span class="s3">1</span><span class="s1">])</span>
        <span class="s0">assert </span><span class="s1">tuple(sorted(top_idx)) </span><span class="s0">in </span><span class="s1">correct_idx_grps</span>


<span class="s0">def </span><span class="s1">test_lda_transform():</span>
    <span class="s2"># Test LDA transform.</span>
    <span class="s2"># Transform result cannot be negative and should be normalized</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randint(</span><span class="s3">5</span><span class="s0">, </span><span class="s1">size=(</span><span class="s3">20</span><span class="s0">, </span><span class="s3">10</span><span class="s1">))</span>
    <span class="s1">n_components = </span><span class="s3">3</span>
    <span class="s1">lda = LatentDirichletAllocation(n_components=n_components</span><span class="s0">, </span><span class="s1">random_state=rng)</span>
    <span class="s1">X_trans = lda.fit_transform(X)</span>
    <span class="s0">assert </span><span class="s1">(X_trans &gt; </span><span class="s3">0.0</span><span class="s1">).any()</span>
    <span class="s1">assert_array_almost_equal(np.sum(X_trans</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.ones(X_trans.shape[</span><span class="s3">0</span><span class="s1">]))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">&quot;online&quot;</span><span class="s0">, </span><span class="s4">&quot;batch&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_lda_fit_transform(method):</span>
    <span class="s2"># Test LDA fit_transform &amp; transform</span>
    <span class="s2"># fit_transform and transform result should be the same</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randint(</span><span class="s3">10</span><span class="s0">, </span><span class="s1">size=(</span><span class="s3">50</span><span class="s0">, </span><span class="s3">20</span><span class="s1">))</span>
    <span class="s1">lda = LatentDirichletAllocation(</span>
        <span class="s1">n_components=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">learning_method=method</span><span class="s0">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">X_fit = lda.fit_transform(X)</span>
    <span class="s1">X_trans = lda.transform(X)</span>
    <span class="s1">assert_array_almost_equal(X_fit</span><span class="s0">, </span><span class="s1">X_trans</span><span class="s0">, </span><span class="s3">4</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_lda_negative_input():</span>
    <span class="s2"># test pass dense matrix with sparse negative input.</span>
    <span class="s1">X = np.full((</span><span class="s3">5</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">)</span>
    <span class="s1">lda = LatentDirichletAllocation()</span>
    <span class="s1">regex = </span><span class="s4">r&quot;^Negative values in data passed&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=regex):</span>
        <span class="s1">lda.fit(X)</span>


<span class="s0">def </span><span class="s1">test_lda_no_component_error():</span>
    <span class="s2"># test `perplexity` before `fit`</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randint(</span><span class="s3">4</span><span class="s0">, </span><span class="s1">size=(</span><span class="s3">20</span><span class="s0">, </span><span class="s3">10</span><span class="s1">))</span>
    <span class="s1">lda = LatentDirichletAllocation()</span>
    <span class="s1">regex = (</span>
        <span class="s4">&quot;This LatentDirichletAllocation instance is not fitted yet. &quot;</span>
        <span class="s4">&quot;Call 'fit' with appropriate arguments before using this &quot;</span>
        <span class="s4">&quot;estimator.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s0">, </span><span class="s1">match=regex):</span>
        <span class="s1">lda.perplexity(X)</span>


<span class="s1">@if_safe_multiprocessing_with_blas</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">&quot;online&quot;</span><span class="s0">, </span><span class="s4">&quot;batch&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_lda_multi_jobs(method):</span>
    <span class="s1">n_components</span><span class="s0">, </span><span class="s1">X = _build_sparse_mtx()</span>
    <span class="s2"># Test LDA batch training with multi CPU</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">lda = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">n_jobs=</span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">learning_method=method</span><span class="s0">,</span>
        <span class="s1">evaluate_every=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">random_state=rng</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">lda.fit(X)</span>

    <span class="s1">correct_idx_grps = [(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">3</span><span class="s0">, </span><span class="s3">4</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">6</span><span class="s0">, </span><span class="s3">7</span><span class="s0">, </span><span class="s3">8</span><span class="s1">)]</span>
    <span class="s0">for </span><span class="s1">c </span><span class="s0">in </span><span class="s1">lda.components_:</span>
        <span class="s1">top_idx = set(c.argsort()[-</span><span class="s3">3</span><span class="s1">:][::-</span><span class="s3">1</span><span class="s1">])</span>
        <span class="s0">assert </span><span class="s1">tuple(sorted(top_idx)) </span><span class="s0">in </span><span class="s1">correct_idx_grps</span>


<span class="s1">@if_safe_multiprocessing_with_blas</span>
<span class="s0">def </span><span class="s1">test_lda_partial_fit_multi_jobs():</span>
    <span class="s2"># Test LDA online training with multi CPU</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_components</span><span class="s0">, </span><span class="s1">X = _build_sparse_mtx()</span>
    <span class="s1">lda = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">n_jobs=</span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">learning_offset=</span><span class="s3">5.0</span><span class="s0">,</span>
        <span class="s1">total_samples=</span><span class="s3">30</span><span class="s0">,</span>
        <span class="s1">random_state=rng</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">2</span><span class="s1">):</span>
        <span class="s1">lda.partial_fit(X)</span>

    <span class="s1">correct_idx_grps = [(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">3</span><span class="s0">, </span><span class="s3">4</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">6</span><span class="s0">, </span><span class="s3">7</span><span class="s0">, </span><span class="s3">8</span><span class="s1">)]</span>
    <span class="s0">for </span><span class="s1">c </span><span class="s0">in </span><span class="s1">lda.components_:</span>
        <span class="s1">top_idx = set(c.argsort()[-</span><span class="s3">3</span><span class="s1">:][::-</span><span class="s3">1</span><span class="s1">])</span>
        <span class="s0">assert </span><span class="s1">tuple(sorted(top_idx)) </span><span class="s0">in </span><span class="s1">correct_idx_grps</span>


<span class="s0">def </span><span class="s1">test_lda_preplexity_mismatch():</span>
    <span class="s2"># test dimension mismatch in `perplexity` method</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_components = rng.randint(</span><span class="s3">3</span><span class="s0">, </span><span class="s3">6</span><span class="s1">)</span>
    <span class="s1">n_samples = rng.randint(</span><span class="s3">6</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">X = np.random.randint(</span><span class="s3">4</span><span class="s0">, </span><span class="s1">size=(n_samples</span><span class="s0">, </span><span class="s3">10</span><span class="s1">))</span>
    <span class="s1">lda = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">learning_offset=</span><span class="s3">5.0</span><span class="s0">,</span>
        <span class="s1">total_samples=</span><span class="s3">20</span><span class="s0">,</span>
        <span class="s1">random_state=rng</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">lda.fit(X)</span>
    <span class="s2"># invalid samples</span>
    <span class="s1">invalid_n_samples = rng.randint(</span><span class="s3">4</span><span class="s0">, </span><span class="s1">size=(n_samples + </span><span class="s3">1</span><span class="s0">, </span><span class="s1">n_components))</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">r&quot;Number of samples&quot;</span><span class="s1">):</span>
        <span class="s1">lda._perplexity_precomp_distr(X</span><span class="s0">, </span><span class="s1">invalid_n_samples)</span>
    <span class="s2"># invalid topic number</span>
    <span class="s1">invalid_n_components = rng.randint(</span><span class="s3">4</span><span class="s0">, </span><span class="s1">size=(n_samples</span><span class="s0">, </span><span class="s1">n_components + </span><span class="s3">1</span><span class="s1">))</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">r&quot;Number of topics&quot;</span><span class="s1">):</span>
        <span class="s1">lda._perplexity_precomp_distr(X</span><span class="s0">, </span><span class="s1">invalid_n_components)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">&quot;online&quot;</span><span class="s0">, </span><span class="s4">&quot;batch&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_lda_perplexity(method):</span>
    <span class="s2"># Test LDA perplexity for batch training</span>
    <span class="s2"># perplexity should be lower after each iteration</span>
    <span class="s1">n_components</span><span class="s0">, </span><span class="s1">X = _build_sparse_mtx()</span>
    <span class="s1">lda_1 = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">learning_method=method</span><span class="s0">,</span>
        <span class="s1">total_samples=</span><span class="s3">100</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">lda_2 = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">10</span><span class="s0">,</span>
        <span class="s1">learning_method=method</span><span class="s0">,</span>
        <span class="s1">total_samples=</span><span class="s3">100</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">lda_1.fit(X)</span>
    <span class="s1">perp_1 = lda_1.perplexity(X</span><span class="s0">, </span><span class="s1">sub_sampling=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s1">lda_2.fit(X)</span>
    <span class="s1">perp_2 = lda_2.perplexity(X</span><span class="s0">, </span><span class="s1">sub_sampling=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">perp_1 &gt;= perp_2</span>

    <span class="s1">perp_1_subsampling = lda_1.perplexity(X</span><span class="s0">, </span><span class="s1">sub_sampling=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">perp_2_subsampling = lda_2.perplexity(X</span><span class="s0">, </span><span class="s1">sub_sampling=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">perp_1_subsampling &gt;= perp_2_subsampling</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">&quot;online&quot;</span><span class="s0">, </span><span class="s4">&quot;batch&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_lda_score(method):</span>
    <span class="s2"># Test LDA score for batch training</span>
    <span class="s2"># score should be higher after each iteration</span>
    <span class="s1">n_components</span><span class="s0">, </span><span class="s1">X = _build_sparse_mtx()</span>
    <span class="s1">lda_1 = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">learning_method=method</span><span class="s0">,</span>
        <span class="s1">total_samples=</span><span class="s3">100</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">lda_2 = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">10</span><span class="s0">,</span>
        <span class="s1">learning_method=method</span><span class="s0">,</span>
        <span class="s1">total_samples=</span><span class="s3">100</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">lda_1.fit_transform(X)</span>
    <span class="s1">score_1 = lda_1.score(X)</span>

    <span class="s1">lda_2.fit_transform(X)</span>
    <span class="s1">score_2 = lda_2.score(X)</span>
    <span class="s0">assert </span><span class="s1">score_2 &gt;= score_1</span>


<span class="s0">def </span><span class="s1">test_perplexity_input_format():</span>
    <span class="s2"># Test LDA perplexity for sparse and dense input</span>
    <span class="s2"># score should be the same for both dense and sparse input</span>
    <span class="s1">n_components</span><span class="s0">, </span><span class="s1">X = _build_sparse_mtx()</span>
    <span class="s1">lda = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">learning_method=</span><span class="s4">&quot;batch&quot;</span><span class="s0">,</span>
        <span class="s1">total_samples=</span><span class="s3">100</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">lda.fit(X)</span>
    <span class="s1">perp_1 = lda.perplexity(X)</span>
    <span class="s1">perp_2 = lda.perplexity(X.toarray())</span>
    <span class="s1">assert_almost_equal(perp_1</span><span class="s0">, </span><span class="s1">perp_2)</span>


<span class="s0">def </span><span class="s1">test_lda_score_perplexity():</span>
    <span class="s2"># Test the relationship between LDA score and perplexity</span>
    <span class="s1">n_components</span><span class="s0">, </span><span class="s1">X = _build_sparse_mtx()</span>
    <span class="s1">lda = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">10</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span>
    <span class="s1">)</span>
    <span class="s1">lda.fit(X)</span>
    <span class="s1">perplexity_1 = lda.perplexity(X</span><span class="s0">, </span><span class="s1">sub_sampling=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s1">score = lda.score(X)</span>
    <span class="s1">perplexity_2 = np.exp(-</span><span class="s3">1.0 </span><span class="s1">* (score / np.sum(X.data)))</span>
    <span class="s1">assert_almost_equal(perplexity_1</span><span class="s0">, </span><span class="s1">perplexity_2)</span>


<span class="s0">def </span><span class="s1">test_lda_fit_perplexity():</span>
    <span class="s2"># Test that the perplexity computed during fit is consistent with what is</span>
    <span class="s2"># returned by the perplexity method</span>
    <span class="s1">n_components</span><span class="s0">, </span><span class="s1">X = _build_sparse_mtx()</span>
    <span class="s1">lda = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">learning_method=</span><span class="s4">&quot;batch&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">evaluate_every=</span><span class="s3">1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">lda.fit(X)</span>

    <span class="s2"># Perplexity computed at end of fit method</span>
    <span class="s1">perplexity1 = lda.bound_</span>

    <span class="s2"># Result of perplexity method on the train set</span>
    <span class="s1">perplexity2 = lda.perplexity(X)</span>

    <span class="s1">assert_almost_equal(perplexity1</span><span class="s0">, </span><span class="s1">perplexity2)</span>


<span class="s0">def </span><span class="s1">test_lda_empty_docs():</span>
    <span class="s5">&quot;&quot;&quot;Test LDA on empty document (all-zero rows).&quot;&quot;&quot;</span>
    <span class="s1">Z = np.zeros((</span><span class="s3">5</span><span class="s0">, </span><span class="s3">4</span><span class="s1">))</span>
    <span class="s0">for </span><span class="s1">X </span><span class="s0">in </span><span class="s1">[Z</span><span class="s0">, </span><span class="s1">csr_matrix(Z)]:</span>
        <span class="s1">lda = LatentDirichletAllocation(max_iter=</span><span class="s3">750</span><span class="s1">).fit(X)</span>
        <span class="s1">assert_almost_equal(</span>
            <span class="s1">lda.components_.sum(axis=</span><span class="s3">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.ones(lda.components_.shape[</span><span class="s3">1</span><span class="s1">])</span>
        <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_dirichlet_expectation():</span>
    <span class="s5">&quot;&quot;&quot;Test Cython version of Dirichlet expectation calculation.&quot;&quot;&quot;</span>
    <span class="s1">x = np.logspace(-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">10</span><span class="s0">, </span><span class="s3">10000</span><span class="s1">)</span>
    <span class="s1">expectation = np.empty_like(x)</span>
    <span class="s1">_dirichlet_expectation_1d(x</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s1">expectation)</span>
    <span class="s1">assert_allclose(expectation</span><span class="s0">, </span><span class="s1">np.exp(psi(x) - psi(np.sum(x)))</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-19</span><span class="s1">)</span>

    <span class="s1">x = x.reshape(</span><span class="s3">100</span><span class="s0">, </span><span class="s3">100</span><span class="s1">)</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">_dirichlet_expectation_2d(x)</span><span class="s0">,</span>
        <span class="s1">psi(x) - psi(np.sum(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)[:</span><span class="s0">, </span><span class="s1">np.newaxis])</span><span class="s0">,</span>
        <span class="s1">rtol=</span><span class="s3">1e-11</span><span class="s0">,</span>
        <span class="s1">atol=</span><span class="s3">3e-9</span><span class="s0">,</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">check_verbosity(verbose</span><span class="s0">, </span><span class="s1">evaluate_every</span><span class="s0">, </span><span class="s1">expected_lines</span><span class="s0">, </span><span class="s1">expected_perplexities):</span>
    <span class="s1">n_components</span><span class="s0">, </span><span class="s1">X = _build_sparse_mtx()</span>
    <span class="s1">lda = LatentDirichletAllocation(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">3</span><span class="s0">,</span>
        <span class="s1">learning_method=</span><span class="s4">&quot;batch&quot;</span><span class="s0">,</span>
        <span class="s1">verbose=verbose</span><span class="s0">,</span>
        <span class="s1">evaluate_every=evaluate_every</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">out = StringIO()</span>
    <span class="s1">old_out</span><span class="s0">, </span><span class="s1">sys.stdout = sys.stdout</span><span class="s0">, </span><span class="s1">out</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">lda.fit(X)</span>
    <span class="s0">finally</span><span class="s1">:</span>
        <span class="s1">sys.stdout = old_out</span>

    <span class="s1">n_lines = out.getvalue().count(</span><span class="s4">&quot;</span><span class="s0">\n</span><span class="s4">&quot;</span><span class="s1">)</span>
    <span class="s1">n_perplexity = out.getvalue().count(</span><span class="s4">&quot;perplexity&quot;</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">expected_lines == n_lines</span>
    <span class="s0">assert </span><span class="s1">expected_perplexities == n_perplexity</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;verbose,evaluate_every,expected_lines,expected_perplexities&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s0">False, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s0">False, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s0">True, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s0">True, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s0">True, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_verbosity(verbose</span><span class="s0">, </span><span class="s1">evaluate_every</span><span class="s0">, </span><span class="s1">expected_lines</span><span class="s0">, </span><span class="s1">expected_perplexities):</span>
    <span class="s1">check_verbosity(verbose</span><span class="s0">, </span><span class="s1">evaluate_every</span><span class="s0">, </span><span class="s1">expected_lines</span><span class="s0">, </span><span class="s1">expected_perplexities)</span>


<span class="s0">def </span><span class="s1">test_lda_feature_names_out():</span>
    <span class="s5">&quot;&quot;&quot;Check feature names out for LatentDirichletAllocation.&quot;&quot;&quot;</span>
    <span class="s1">n_components</span><span class="s0">, </span><span class="s1">X = _build_sparse_mtx()</span>
    <span class="s1">lda = LatentDirichletAllocation(n_components=n_components).fit(X)</span>

    <span class="s1">names = lda.get_feature_names_out()</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">[</span><span class="s4">f&quot;latentdirichletallocation</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s4">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(n_components)]</span><span class="s0">, </span><span class="s1">names</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;learning_method&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">&quot;batch&quot;</span><span class="s0">, </span><span class="s4">&quot;online&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_lda_dtype_match(learning_method</span><span class="s0">, </span><span class="s1">global_dtype):</span>
    <span class="s5">&quot;&quot;&quot;Check data type preservation of fitted attributes.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.uniform(size=(</span><span class="s3">20</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)).astype(global_dtype</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s1">lda = LatentDirichletAllocation(</span>
        <span class="s1">n_components=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">learning_method=learning_method</span>
    <span class="s1">)</span>
    <span class="s1">lda.fit(X)</span>
    <span class="s0">assert </span><span class="s1">lda.components_.dtype == global_dtype</span>
    <span class="s0">assert </span><span class="s1">lda.exp_dirichlet_component_.dtype == global_dtype</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;learning_method&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">&quot;batch&quot;</span><span class="s0">, </span><span class="s4">&quot;online&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_lda_numerical_consistency(learning_method</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s5">&quot;&quot;&quot;Check numerical consistency between np.float32 and np.float64.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">X64 = rng.uniform(size=(</span><span class="s3">20</span><span class="s0">, </span><span class="s3">10</span><span class="s1">))</span>
    <span class="s1">X32 = X64.astype(np.float32)</span>

    <span class="s1">lda_64 = LatentDirichletAllocation(</span>
        <span class="s1">n_components=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">random_state=global_random_seed</span><span class="s0">, </span><span class="s1">learning_method=learning_method</span>
    <span class="s1">).fit(X64)</span>
    <span class="s1">lda_32 = LatentDirichletAllocation(</span>
        <span class="s1">n_components=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">random_state=global_random_seed</span><span class="s0">, </span><span class="s1">learning_method=learning_method</span>
    <span class="s1">).fit(X32)</span>

    <span class="s1">assert_allclose(lda_32.components_</span><span class="s0">, </span><span class="s1">lda_64.components_)</span>
    <span class="s1">assert_allclose(lda_32.transform(X32)</span><span class="s0">, </span><span class="s1">lda_64.transform(X64))</span>
</pre>
</body>
</html>