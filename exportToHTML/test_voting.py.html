<html>
<head>
<title>test_voting.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_voting.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Testing for the VotingClassifier and VotingRegressor&quot;&quot;&quot;</span>

<span class="s2">import </span><span class="s1">re</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>

<span class="s2">from </span><span class="s1">sklearn </span><span class="s2">import </span><span class="s1">datasets</span>
<span class="s2">from </span><span class="s1">sklearn.base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">ClassifierMixin</span><span class="s2">, </span><span class="s1">clone</span>
<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">make_multilabel_classification</span>
<span class="s2">from </span><span class="s1">sklearn.dummy </span><span class="s2">import </span><span class="s1">DummyRegressor</span>
<span class="s2">from </span><span class="s1">sklearn.ensemble </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">RandomForestClassifier</span><span class="s2">,</span>
    <span class="s1">RandomForestRegressor</span><span class="s2">,</span>
    <span class="s1">VotingClassifier</span><span class="s2">,</span>
    <span class="s1">VotingRegressor</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.exceptions </span><span class="s2">import </span><span class="s1">NotFittedError</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">LinearRegression</span><span class="s2">, </span><span class="s1">LogisticRegression</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">GridSearchCV</span><span class="s2">, </span><span class="s1">cross_val_score</span><span class="s2">, </span><span class="s1">train_test_split</span>
<span class="s2">from </span><span class="s1">sklearn.multiclass </span><span class="s2">import </span><span class="s1">OneVsRestClassifier</span>
<span class="s2">from </span><span class="s1">sklearn.naive_bayes </span><span class="s2">import </span><span class="s1">GaussianNB</span>
<span class="s2">from </span><span class="s1">sklearn.neighbors </span><span class="s2">import </span><span class="s1">KNeighborsClassifier</span>
<span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">StandardScaler</span>
<span class="s2">from </span><span class="s1">sklearn.svm </span><span class="s2">import </span><span class="s1">SVC</span>
<span class="s2">from </span><span class="s1">sklearn.tree </span><span class="s2">import </span><span class="s1">DecisionTreeClassifier</span><span class="s2">, </span><span class="s1">DecisionTreeRegressor</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">assert_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_equal</span><span class="s2">,</span>
<span class="s1">)</span>

<span class="s3"># Load datasets</span>
<span class="s1">iris = datasets.load_iris()</span>
<span class="s1">X</span><span class="s2">, </span><span class="s1">y = iris.data[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">:</span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">iris.target</span>
<span class="s3"># Scaled to solve ConvergenceWarning throw by Logistic Regression</span>
<span class="s1">X_scaled = StandardScaler().fit_transform(X)</span>

<span class="s1">X_r</span><span class="s2">, </span><span class="s1">y_r = datasets.load_diabetes(return_X_y=</span><span class="s2">True</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;params, err_msg&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s5">&quot;estimators&quot;</span><span class="s1">: []}</span><span class="s2">,</span>
            <span class="s5">&quot;Invalid 'estimators' attribute, 'estimators' should be a non-empty list&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s5">&quot;estimators&quot;</span><span class="s1">: [(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">LogisticRegression())]</span><span class="s2">, </span><span class="s5">&quot;weights&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]}</span><span class="s2">,</span>
            <span class="s5">&quot;Number of `estimators` and weights must be equal&quot;</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_voting_classifier_estimator_init(params</span><span class="s2">, </span><span class="s1">err_msg):</span>
    <span class="s1">ensemble = VotingClassifier(**params)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">ensemble.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_predictproba_hardvoting():</span>
    <span class="s1">eclf = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr1&quot;</span><span class="s2">, </span><span class="s1">LogisticRegression())</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;lr2&quot;</span><span class="s2">, </span><span class="s1">LogisticRegression())]</span><span class="s2">,</span>
        <span class="s1">voting=</span><span class="s5">&quot;hard&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">msg = </span><span class="s5">&quot;predict_proba is not available when voting='hard'&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(AttributeError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">eclf.predict_proba</span>

    <span class="s2">assert not </span><span class="s1">hasattr(eclf</span><span class="s2">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s1">)</span>
    <span class="s1">eclf.fit(X_scaled</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(eclf</span><span class="s2">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_notfitted():</span>
    <span class="s1">eclf = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr1&quot;</span><span class="s2">, </span><span class="s1">LogisticRegression())</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;lr2&quot;</span><span class="s2">, </span><span class="s1">LogisticRegression())]</span><span class="s2">,</span>
        <span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">ereg = VotingRegressor([(</span><span class="s5">&quot;dr&quot;</span><span class="s2">, </span><span class="s1">DummyRegressor())])</span>
    <span class="s1">msg = (</span>
        <span class="s5">&quot;This %s instance is not fitted yet. Call 'fit'&quot;</span>
        <span class="s5">&quot; with appropriate arguments before using this estimator.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s2">, </span><span class="s1">match=msg % </span><span class="s5">&quot;VotingClassifier&quot;</span><span class="s1">):</span>
        <span class="s1">eclf.predict(X)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s2">, </span><span class="s1">match=msg % </span><span class="s5">&quot;VotingClassifier&quot;</span><span class="s1">):</span>
        <span class="s1">eclf.predict_proba(X)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s2">, </span><span class="s1">match=msg % </span><span class="s5">&quot;VotingClassifier&quot;</span><span class="s1">):</span>
        <span class="s1">eclf.transform(X)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s2">, </span><span class="s1">match=msg % </span><span class="s5">&quot;VotingRegressor&quot;</span><span class="s1">):</span>
        <span class="s1">ereg.predict(X_r)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s2">, </span><span class="s1">match=msg % </span><span class="s5">&quot;VotingRegressor&quot;</span><span class="s1">):</span>
        <span class="s1">ereg.transform(X_r)</span>


<span class="s2">def </span><span class="s1">test_majority_label_iris(global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Check classification by majority label on dataset iris.&quot;&quot;&quot;</span>
    <span class="s1">clf1 = LogisticRegression(solver=</span><span class="s5">&quot;liblinear&quot;</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s1">clf2 = RandomForestClassifier(n_estimators=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s1">clf3 = GaussianNB()</span>
    <span class="s1">eclf = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;gnb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;hard&quot;</span>
    <span class="s1">)</span>
    <span class="s1">scores = cross_val_score(eclf</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s5">&quot;accuracy&quot;</span><span class="s1">)</span>

    <span class="s2">assert </span><span class="s1">scores.mean() &gt;= </span><span class="s4">0.9</span>


<span class="s2">def </span><span class="s1">test_tie_situation():</span>
    <span class="s0">&quot;&quot;&quot;Check voting classifier selects smaller class label in tie situation.&quot;&quot;&quot;</span>
    <span class="s1">clf1 = LogisticRegression(random_state=</span><span class="s4">123</span><span class="s2">, </span><span class="s1">solver=</span><span class="s5">&quot;liblinear&quot;</span><span class="s1">)</span>
    <span class="s1">clf2 = RandomForestClassifier(random_state=</span><span class="s4">123</span><span class="s1">)</span>
    <span class="s1">eclf = VotingClassifier(estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;hard&quot;</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)[</span><span class="s4">73</span><span class="s1">] == </span><span class="s4">2</span>
    <span class="s2">assert </span><span class="s1">clf2.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)[</span><span class="s4">73</span><span class="s1">] == </span><span class="s4">1</span>
    <span class="s2">assert </span><span class="s1">eclf.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)[</span><span class="s4">73</span><span class="s1">] == </span><span class="s4">1</span>


<span class="s2">def </span><span class="s1">test_weights_iris(global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Check classification by average probabilities on dataset iris.&quot;&quot;&quot;</span>
    <span class="s1">clf1 = LogisticRegression(random_state=global_random_seed)</span>
    <span class="s1">clf2 = RandomForestClassifier(n_estimators=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s1">clf3 = GaussianNB()</span>
    <span class="s1">eclf = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;gnb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">,</span>
        <span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s2">,</span>
        <span class="s1">weights=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">scores = cross_val_score(eclf</span><span class="s2">, </span><span class="s1">X_scaled</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">scoring=</span><span class="s5">&quot;accuracy&quot;</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">scores.mean() &gt;= </span><span class="s4">0.9</span>


<span class="s2">def </span><span class="s1">test_weights_regressor():</span>
    <span class="s0">&quot;&quot;&quot;Check weighted average regression prediction on diabetes dataset.&quot;&quot;&quot;</span>
    <span class="s1">reg1 = DummyRegressor(strategy=</span><span class="s5">&quot;mean&quot;</span><span class="s1">)</span>
    <span class="s1">reg2 = DummyRegressor(strategy=</span><span class="s5">&quot;median&quot;</span><span class="s1">)</span>
    <span class="s1">reg3 = DummyRegressor(strategy=</span><span class="s5">&quot;quantile&quot;</span><span class="s2">, </span><span class="s1">quantile=</span><span class="s4">0.2</span><span class="s1">)</span>
    <span class="s1">ereg = VotingRegressor(</span>
        <span class="s1">[(</span><span class="s5">&quot;mean&quot;</span><span class="s2">, </span><span class="s1">reg1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;median&quot;</span><span class="s2">, </span><span class="s1">reg2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;quantile&quot;</span><span class="s2">, </span><span class="s1">reg3)]</span><span class="s2">, </span><span class="s1">weights=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">X_r_train</span><span class="s2">, </span><span class="s1">X_r_test</span><span class="s2">, </span><span class="s1">y_r_train</span><span class="s2">, </span><span class="s1">y_r_test = train_test_split(</span>
        <span class="s1">X_r</span><span class="s2">, </span><span class="s1">y_r</span><span class="s2">, </span><span class="s1">test_size=</span><span class="s4">0.25</span>
    <span class="s1">)</span>

    <span class="s1">reg1_pred = reg1.fit(X_r_train</span><span class="s2">, </span><span class="s1">y_r_train).predict(X_r_test)</span>
    <span class="s1">reg2_pred = reg2.fit(X_r_train</span><span class="s2">, </span><span class="s1">y_r_train).predict(X_r_test)</span>
    <span class="s1">reg3_pred = reg3.fit(X_r_train</span><span class="s2">, </span><span class="s1">y_r_train).predict(X_r_test)</span>
    <span class="s1">ereg_pred = ereg.fit(X_r_train</span><span class="s2">, </span><span class="s1">y_r_train).predict(X_r_test)</span>

    <span class="s1">avg = np.average(</span>
        <span class="s1">np.asarray([reg1_pred</span><span class="s2">, </span><span class="s1">reg2_pred</span><span class="s2">, </span><span class="s1">reg3_pred])</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">weights=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">assert_almost_equal(ereg_pred</span><span class="s2">, </span><span class="s1">avg</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">2</span><span class="s1">)</span>

    <span class="s1">ereg_weights_none = VotingRegressor(</span>
        <span class="s1">[(</span><span class="s5">&quot;mean&quot;</span><span class="s2">, </span><span class="s1">reg1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;median&quot;</span><span class="s2">, </span><span class="s1">reg2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;quantile&quot;</span><span class="s2">, </span><span class="s1">reg3)]</span><span class="s2">, </span><span class="s1">weights=</span><span class="s2">None</span>
    <span class="s1">)</span>
    <span class="s1">ereg_weights_equal = VotingRegressor(</span>
        <span class="s1">[(</span><span class="s5">&quot;mean&quot;</span><span class="s2">, </span><span class="s1">reg1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;median&quot;</span><span class="s2">, </span><span class="s1">reg2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;quantile&quot;</span><span class="s2">, </span><span class="s1">reg3)]</span><span class="s2">, </span><span class="s1">weights=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">ereg_weights_none.fit(X_r_train</span><span class="s2">, </span><span class="s1">y_r_train)</span>
    <span class="s1">ereg_weights_equal.fit(X_r_train</span><span class="s2">, </span><span class="s1">y_r_train)</span>
    <span class="s1">ereg_none_pred = ereg_weights_none.predict(X_r_test)</span>
    <span class="s1">ereg_equal_pred = ereg_weights_equal.predict(X_r_test)</span>
    <span class="s1">assert_almost_equal(ereg_none_pred</span><span class="s2">, </span><span class="s1">ereg_equal_pred</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_predict_on_toy_problem(global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Manually check predicted class labels for toy dataset.&quot;&quot;&quot;</span>
    <span class="s1">clf1 = LogisticRegression(random_state=global_random_seed)</span>
    <span class="s1">clf2 = RandomForestClassifier(n_estimators=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s1">clf3 = GaussianNB()</span>

    <span class="s1">X = np.array(</span>
        <span class="s1">[[-</span><span class="s4">1.1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1.2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">3.4</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2.2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1.1</span><span class="s2">, </span><span class="s4">1.2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2.1</span><span class="s2">, </span><span class="s4">1.4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3.1</span><span class="s2">, </span><span class="s4">2.3</span><span class="s1">]]</span>
    <span class="s1">)</span>

    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>

    <span class="s1">assert_array_equal(clf1.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(clf2.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(clf3.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>

    <span class="s1">eclf = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;gnb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">,</span>
        <span class="s1">voting=</span><span class="s5">&quot;hard&quot;</span><span class="s2">,</span>
        <span class="s1">weights=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(eclf.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>

    <span class="s1">eclf = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;gnb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">,</span>
        <span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s2">,</span>
        <span class="s1">weights=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(eclf.fit(X</span><span class="s2">, </span><span class="s1">y).predict(X)</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_predict_proba_on_toy_problem():</span>
    <span class="s0">&quot;&quot;&quot;Calculate predicted probabilities on toy dataset.&quot;&quot;&quot;</span>
    <span class="s1">clf1 = LogisticRegression(random_state=</span><span class="s4">123</span><span class="s1">)</span>
    <span class="s1">clf2 = RandomForestClassifier(random_state=</span><span class="s4">123</span><span class="s1">)</span>
    <span class="s1">clf3 = GaussianNB()</span>
    <span class="s1">X = np.array([[-</span><span class="s4">1.1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1.2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">3.4</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2.2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1.1</span><span class="s2">, </span><span class="s4">1.2</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>

    <span class="s1">clf1_res = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">0.59790391</span><span class="s2">, </span><span class="s4">0.40209609</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">0.57622162</span><span class="s2">, </span><span class="s4">0.42377838</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">0.50728456</span><span class="s2">, </span><span class="s4">0.49271544</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">0.40241774</span><span class="s2">, </span><span class="s4">0.59758226</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">clf2_res = np.array([[</span><span class="s4">0.8</span><span class="s2">, </span><span class="s4">0.2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.8</span><span class="s2">, </span><span class="s4">0.2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.2</span><span class="s2">, </span><span class="s4">0.8</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.3</span><span class="s2">, </span><span class="s4">0.7</span><span class="s1">]])</span>

    <span class="s1">clf3_res = np.array(</span>
        <span class="s1">[[</span><span class="s4">0.9985082</span><span class="s2">, </span><span class="s4">0.0014918</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.99845843</span><span class="s2">, </span><span class="s4">0.00154157</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]]</span>
    <span class="s1">)</span>

    <span class="s1">t00 = (</span><span class="s4">2 </span><span class="s1">* clf1_res[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">] + clf2_res[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">] + clf3_res[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]) / </span><span class="s4">4</span>
    <span class="s1">t11 = (</span><span class="s4">2 </span><span class="s1">* clf1_res[</span><span class="s4">1</span><span class="s1">][</span><span class="s4">1</span><span class="s1">] + clf2_res[</span><span class="s4">1</span><span class="s1">][</span><span class="s4">1</span><span class="s1">] + clf3_res[</span><span class="s4">1</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]) / </span><span class="s4">4</span>
    <span class="s1">t21 = (</span><span class="s4">2 </span><span class="s1">* clf1_res[</span><span class="s4">2</span><span class="s1">][</span><span class="s4">1</span><span class="s1">] + clf2_res[</span><span class="s4">2</span><span class="s1">][</span><span class="s4">1</span><span class="s1">] + clf3_res[</span><span class="s4">2</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]) / </span><span class="s4">4</span>
    <span class="s1">t31 = (</span><span class="s4">2 </span><span class="s1">* clf1_res[</span><span class="s4">3</span><span class="s1">][</span><span class="s4">1</span><span class="s1">] + clf2_res[</span><span class="s4">3</span><span class="s1">][</span><span class="s4">1</span><span class="s1">] + clf3_res[</span><span class="s4">3</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]) / </span><span class="s4">4</span>

    <span class="s1">eclf = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;gnb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">,</span>
        <span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s2">,</span>
        <span class="s1">weights=[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">eclf_res = eclf.fit(X</span><span class="s2">, </span><span class="s1">y).predict_proba(X)</span>

    <span class="s1">assert_almost_equal(t00</span><span class="s2">, </span><span class="s1">eclf_res[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(t11</span><span class="s2">, </span><span class="s1">eclf_res[</span><span class="s4">1</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(t21</span><span class="s2">, </span><span class="s1">eclf_res[</span><span class="s4">2</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(t31</span><span class="s2">, </span><span class="s1">eclf_res[</span><span class="s4">3</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">1</span><span class="s1">)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">AttributeError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;predict_proba is not available when voting='hard'&quot;</span>
    <span class="s1">):</span>
        <span class="s1">eclf = VotingClassifier(</span>
            <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;gnb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;hard&quot;</span>
        <span class="s1">)</span>
        <span class="s1">eclf.fit(X</span><span class="s2">, </span><span class="s1">y).predict_proba(X)</span>


<span class="s2">def </span><span class="s1">test_multilabel():</span>
    <span class="s0">&quot;&quot;&quot;Check if error is raised for multilabel classification.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_multilabel_classification(</span>
        <span class="s1">n_classes=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">n_labels=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">allow_unlabeled=</span><span class="s2">False, </span><span class="s1">random_state=</span><span class="s4">123</span>
    <span class="s1">)</span>
    <span class="s1">clf = OneVsRestClassifier(SVC(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s1">))</span>

    <span class="s1">eclf = VotingClassifier(estimators=[(</span><span class="s5">&quot;ovr&quot;</span><span class="s2">, </span><span class="s1">clf)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;hard&quot;</span><span class="s1">)</span>

    <span class="s2">try</span><span class="s1">:</span>
        <span class="s1">eclf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">except </span><span class="s1">NotImplementedError:</span>
        <span class="s2">return</span>


<span class="s2">def </span><span class="s1">test_gridsearch():</span>
    <span class="s0">&quot;&quot;&quot;Check GridSearch support.&quot;&quot;&quot;</span>
    <span class="s1">clf1 = LogisticRegression(random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf2 = RandomForestClassifier(random_state=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">n_estimators=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">clf3 = GaussianNB()</span>
    <span class="s1">eclf = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;gnb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span>
    <span class="s1">)</span>

    <span class="s1">params = {</span>
        <span class="s5">&quot;lr__C&quot;</span><span class="s1">: [</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">100.0</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;voting&quot;</span><span class="s1">: [</span><span class="s5">&quot;soft&quot;</span><span class="s2">, </span><span class="s5">&quot;hard&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;weights&quot;</span><span class="s1">: [[</span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">]]</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s1">grid = GridSearchCV(estimator=eclf</span><span class="s2">, </span><span class="s1">param_grid=params</span><span class="s2">, </span><span class="s1">cv=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">grid.fit(X_scaled</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_parallel_fit(global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Check parallel backend of VotingClassifier on toy dataset.&quot;&quot;&quot;</span>
    <span class="s1">clf1 = LogisticRegression(random_state=global_random_seed)</span>
    <span class="s1">clf2 = RandomForestClassifier(n_estimators=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s1">clf3 = GaussianNB()</span>
    <span class="s1">X = np.array([[-</span><span class="s4">1.1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1.2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">3.4</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2.2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1.1</span><span class="s2">, </span><span class="s4">1.2</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>

    <span class="s1">eclf1 = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;gnb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s2">, </span><span class="s1">n_jobs=</span><span class="s4">1</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">eclf2 = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;gnb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s2">, </span><span class="s1">n_jobs=</span><span class="s4">2</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">assert_array_equal(eclf1.predict(X)</span><span class="s2">, </span><span class="s1">eclf2.predict(X))</span>
    <span class="s1">assert_array_almost_equal(eclf1.predict_proba(X)</span><span class="s2">, </span><span class="s1">eclf2.predict_proba(X))</span>


<span class="s2">def </span><span class="s1">test_sample_weight(global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Tests sample_weight parameter of VotingClassifier&quot;&quot;&quot;</span>
    <span class="s1">clf1 = LogisticRegression(random_state=global_random_seed)</span>
    <span class="s1">clf2 = RandomForestClassifier(n_estimators=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s1">clf3 = SVC(probability=</span><span class="s2">True, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s1">eclf1 = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;svc&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span>
    <span class="s1">).fit(X_scaled</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=np.ones((len(y)</span><span class="s2">,</span><span class="s1">)))</span>
    <span class="s1">eclf2 = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;svc&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span>
    <span class="s1">).fit(X_scaled</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(eclf1.predict(X_scaled)</span><span class="s2">, </span><span class="s1">eclf2.predict(X_scaled))</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">eclf1.predict_proba(X_scaled)</span><span class="s2">, </span><span class="s1">eclf2.predict_proba(X_scaled)</span>
    <span class="s1">)</span>
    <span class="s1">sample_weight = np.random.RandomState(global_random_seed).uniform(size=(len(y)</span><span class="s2">,</span><span class="s1">))</span>
    <span class="s1">eclf3 = VotingClassifier(estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s1">)</span>
    <span class="s1">eclf3.fit(X_scaled</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight)</span>
    <span class="s1">clf1.fit(X_scaled</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight)</span>
    <span class="s1">assert_array_equal(eclf3.predict(X_scaled)</span><span class="s2">, </span><span class="s1">clf1.predict(X_scaled))</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">eclf3.predict_proba(X_scaled)</span><span class="s2">, </span><span class="s1">clf1.predict_proba(X_scaled)</span>
    <span class="s1">)</span>

    <span class="s3"># check that an error is raised and indicative if sample_weight is not</span>
    <span class="s3"># supported.</span>
    <span class="s1">clf4 = KNeighborsClassifier()</span>
    <span class="s1">eclf3 = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;svc&quot;</span><span class="s2">, </span><span class="s1">clf3)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;knn&quot;</span><span class="s2">, </span><span class="s1">clf4)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span>
    <span class="s1">)</span>
    <span class="s1">msg = </span><span class="s5">&quot;Underlying estimator KNeighborsClassifier does not support sample weights.&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(TypeError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">eclf3.fit(X_scaled</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight)</span>

    <span class="s3"># check that _fit_single_estimator will raise the right error</span>
    <span class="s3"># it should raise the original error if this is not linked to sample_weight</span>
    <span class="s2">class </span><span class="s1">ClassifierErrorFit(ClassifierMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X_scaled</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight):</span>
            <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;Error unrelated to sample_weight.&quot;</span><span class="s1">)</span>

    <span class="s1">clf = ClassifierErrorFit()</span>
    <span class="s2">with </span><span class="s1">pytest.raises(TypeError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;Error unrelated to sample_weight&quot;</span><span class="s1">):</span>
        <span class="s1">clf.fit(X_scaled</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>


<span class="s2">def </span><span class="s1">test_sample_weight_kwargs():</span>
    <span class="s0">&quot;&quot;&quot;Check that VotingClassifier passes sample_weight as kwargs&quot;&quot;&quot;</span>

    <span class="s2">class </span><span class="s1">MockClassifier(ClassifierMixin</span><span class="s2">, </span><span class="s1">BaseEstimator):</span>
        <span class="s0">&quot;&quot;&quot;Mock Classifier to check that sample_weight is received as kwargs&quot;&quot;&quot;</span>

        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">**sample_weight):</span>
            <span class="s2">assert </span><span class="s5">&quot;sample_weight&quot; </span><span class="s2">in </span><span class="s1">sample_weight</span>

    <span class="s1">clf = MockClassifier()</span>
    <span class="s1">eclf = VotingClassifier(estimators=[(</span><span class="s5">&quot;mock&quot;</span><span class="s2">, </span><span class="s1">clf)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s1">)</span>

    <span class="s3"># Should not raise an error.</span>
    <span class="s1">eclf.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=np.ones((len(y)</span><span class="s2">,</span><span class="s1">)))</span>


<span class="s2">def </span><span class="s1">test_voting_classifier_set_params(global_random_seed):</span>
    <span class="s3"># check equivalence in the output when setting underlying estimators</span>
    <span class="s1">clf1 = LogisticRegression(random_state=global_random_seed)</span>
    <span class="s1">clf2 = RandomForestClassifier(</span>
        <span class="s1">n_estimators=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s2">None</span>
    <span class="s1">)</span>
    <span class="s1">clf3 = GaussianNB()</span>

    <span class="s1">eclf1 = VotingClassifier(</span>
        <span class="s1">[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s2">, </span><span class="s1">weights=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span>
    <span class="s1">).fit(X_scaled</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">eclf2 = VotingClassifier(</span>
        <span class="s1">[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;nb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s2">, </span><span class="s1">weights=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">eclf2.set_params(nb=clf2).fit(X_scaled</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">assert_array_equal(eclf1.predict(X_scaled)</span><span class="s2">, </span><span class="s1">eclf2.predict(X_scaled))</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">eclf1.predict_proba(X_scaled)</span><span class="s2">, </span><span class="s1">eclf2.predict_proba(X_scaled)</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">eclf2.estimators[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">1</span><span class="s1">].get_params() == clf1.get_params()</span>
    <span class="s2">assert </span><span class="s1">eclf2.estimators[</span><span class="s4">1</span><span class="s1">][</span><span class="s4">1</span><span class="s1">].get_params() == clf2.get_params()</span>


<span class="s2">def </span><span class="s1">test_set_estimator_drop():</span>
    <span class="s3"># VotingClassifier set_params should be able to set estimators as drop</span>
    <span class="s3"># Test predict</span>
    <span class="s1">clf1 = LogisticRegression(random_state=</span><span class="s4">123</span><span class="s1">)</span>
    <span class="s1">clf2 = RandomForestClassifier(n_estimators=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">123</span><span class="s1">)</span>
    <span class="s1">clf3 = GaussianNB()</span>
    <span class="s1">eclf1 = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;nb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">,</span>
        <span class="s1">voting=</span><span class="s5">&quot;hard&quot;</span><span class="s2">,</span>
        <span class="s1">weights=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">eclf2 = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;nb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">,</span>
        <span class="s1">voting=</span><span class="s5">&quot;hard&quot;</span><span class="s2">,</span>
        <span class="s1">weights=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">eclf2.set_params(rf=</span><span class="s5">&quot;drop&quot;</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">assert_array_equal(eclf1.predict(X)</span><span class="s2">, </span><span class="s1">eclf2.predict(X))</span>

    <span class="s2">assert </span><span class="s1">dict(eclf2.estimators)[</span><span class="s5">&quot;rf&quot;</span><span class="s1">] == </span><span class="s5">&quot;drop&quot;</span>
    <span class="s2">assert </span><span class="s1">len(eclf2.estimators_) == </span><span class="s4">2</span>
    <span class="s2">assert </span><span class="s1">all(</span>
        <span class="s1">isinstance(est</span><span class="s2">, </span><span class="s1">(LogisticRegression</span><span class="s2">, </span><span class="s1">GaussianNB)) </span><span class="s2">for </span><span class="s1">est </span><span class="s2">in </span><span class="s1">eclf2.estimators_</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">eclf2.get_params()[</span><span class="s5">&quot;rf&quot;</span><span class="s1">] == </span><span class="s5">&quot;drop&quot;</span>

    <span class="s1">eclf1.set_params(voting=</span><span class="s5">&quot;soft&quot;</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">eclf2.set_params(voting=</span><span class="s5">&quot;soft&quot;</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">assert_array_equal(eclf1.predict(X)</span><span class="s2">, </span><span class="s1">eclf2.predict(X))</span>
    <span class="s1">assert_array_almost_equal(eclf1.predict_proba(X)</span><span class="s2">, </span><span class="s1">eclf2.predict_proba(X))</span>
    <span class="s1">msg = </span><span class="s5">&quot;All estimators are dropped. At least one is required&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">eclf2.set_params(lr=</span><span class="s5">&quot;drop&quot;</span><span class="s2">, </span><span class="s1">rf=</span><span class="s5">&quot;drop&quot;</span><span class="s2">, </span><span class="s1">nb=</span><span class="s5">&quot;drop&quot;</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s3"># Test soft voting transform</span>
    <span class="s1">X1 = np.array([[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s1">]])</span>
    <span class="s1">y1 = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">eclf1 = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;nb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">,</span>
        <span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s2">,</span>
        <span class="s1">weights=[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">flatten_transform=</span><span class="s2">False,</span>
    <span class="s1">).fit(X1</span><span class="s2">, </span><span class="s1">y1)</span>

    <span class="s1">eclf2 = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;nb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">,</span>
        <span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s2">,</span>
        <span class="s1">weights=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">flatten_transform=</span><span class="s2">False,</span>
    <span class="s1">)</span>
    <span class="s1">eclf2.set_params(rf=</span><span class="s5">&quot;drop&quot;</span><span class="s1">).fit(X1</span><span class="s2">, </span><span class="s1">y1)</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">eclf1.transform(X1)</span><span class="s2">,</span>
        <span class="s1">np.array([[[</span><span class="s4">0.7</span><span class="s2">, </span><span class="s4">0.3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.3</span><span class="s2">, </span><span class="s4">0.7</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">[[</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]]])</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(eclf2.transform(X1)</span><span class="s2">, </span><span class="s1">np.array([[[</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">]]]))</span>
    <span class="s1">eclf1.set_params(voting=</span><span class="s5">&quot;hard&quot;</span><span class="s1">)</span>
    <span class="s1">eclf2.set_params(voting=</span><span class="s5">&quot;hard&quot;</span><span class="s1">)</span>
    <span class="s1">assert_array_equal(eclf1.transform(X1)</span><span class="s2">, </span><span class="s1">np.array([[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]]))</span>
    <span class="s1">assert_array_equal(eclf2.transform(X1)</span><span class="s2">, </span><span class="s1">np.array([[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">]]))</span>


<span class="s2">def </span><span class="s1">test_estimator_weights_format(global_random_seed):</span>
    <span class="s3"># Test estimator weights inputs as list and array</span>
    <span class="s1">clf1 = LogisticRegression(random_state=global_random_seed)</span>
    <span class="s1">clf2 = RandomForestClassifier(n_estimators=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s1">eclf1 = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)]</span><span class="s2">, </span><span class="s1">weights=[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span>
    <span class="s1">)</span>
    <span class="s1">eclf2 = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)]</span><span class="s2">, </span><span class="s1">weights=np.array((</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span>
    <span class="s1">)</span>
    <span class="s1">eclf1.fit(X_scaled</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">eclf2.fit(X_scaled</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">eclf1.predict_proba(X_scaled)</span><span class="s2">, </span><span class="s1">eclf2.predict_proba(X_scaled)</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_transform(global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Check transform method of VotingClassifier on toy dataset.&quot;&quot;&quot;</span>
    <span class="s1">clf1 = LogisticRegression(random_state=global_random_seed)</span>
    <span class="s1">clf2 = RandomForestClassifier(n_estimators=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s1">clf3 = GaussianNB()</span>
    <span class="s1">X = np.array([[-</span><span class="s4">1.1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1.2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">3.4</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2.2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1.1</span><span class="s2">, </span><span class="s4">1.2</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>

    <span class="s1">eclf1 = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;gnb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">, </span><span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">eclf2 = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;gnb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">,</span>
        <span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s2">,</span>
        <span class="s1">flatten_transform=</span><span class="s2">True,</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">eclf3 = VotingClassifier(</span>
        <span class="s1">estimators=[(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">clf1)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">clf2)</span><span class="s2">, </span><span class="s1">(</span><span class="s5">&quot;gnb&quot;</span><span class="s2">, </span><span class="s1">clf3)]</span><span class="s2">,</span>
        <span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s2">,</span>
        <span class="s1">flatten_transform=</span><span class="s2">False,</span>
    <span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">assert_array_equal(eclf1.transform(X).shape</span><span class="s2">, </span><span class="s1">(</span><span class="s4">4</span><span class="s2">, </span><span class="s4">6</span><span class="s1">))</span>
    <span class="s1">assert_array_equal(eclf2.transform(X).shape</span><span class="s2">, </span><span class="s1">(</span><span class="s4">4</span><span class="s2">, </span><span class="s4">6</span><span class="s1">))</span>
    <span class="s1">assert_array_equal(eclf3.transform(X).shape</span><span class="s2">, </span><span class="s1">(</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">assert_array_almost_equal(eclf1.transform(X)</span><span class="s2">, </span><span class="s1">eclf2.transform(X))</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">eclf3.transform(X).swapaxes(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">).reshape((</span><span class="s4">4</span><span class="s2">, </span><span class="s4">6</span><span class="s1">))</span><span class="s2">, </span><span class="s1">eclf2.transform(X)</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;X, y, voter&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">y</span><span class="s2">,</span>
            <span class="s1">VotingClassifier(</span>
                <span class="s1">[</span>
                    <span class="s1">(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">LogisticRegression())</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">RandomForestClassifier(n_estimators=</span><span class="s4">5</span><span class="s1">))</span><span class="s2">,</span>
                <span class="s1">]</span>
            <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">X_r</span><span class="s2">,</span>
            <span class="s1">y_r</span><span class="s2">,</span>
            <span class="s1">VotingRegressor(</span>
                <span class="s1">[</span>
                    <span class="s1">(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">LinearRegression())</span><span class="s2">,</span>
                    <span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">RandomForestRegressor(n_estimators=</span><span class="s4">5</span><span class="s1">))</span><span class="s2">,</span>
                <span class="s1">]</span>
            <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_none_estimator_with_weights(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">voter):</span>
    <span class="s3"># check that an estimator can be set to 'drop' and passing some weight</span>
    <span class="s3"># regression test for</span>
    <span class="s3"># https://github.com/scikit-learn/scikit-learn/issues/13777</span>
    <span class="s1">voter = clone(voter)</span>
    <span class="s3"># Scaled to solve ConvergenceWarning throw by Logistic Regression</span>
    <span class="s1">X_scaled = StandardScaler().fit_transform(X)</span>
    <span class="s1">voter.fit(X_scaled</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=np.ones(y.shape))</span>
    <span class="s1">voter.set_params(lr=</span><span class="s5">&quot;drop&quot;</span><span class="s1">)</span>
    <span class="s1">voter.fit(X_scaled</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=np.ones(y.shape))</span>
    <span class="s1">y_pred = voter.predict(X_scaled)</span>
    <span class="s2">assert </span><span class="s1">y_pred.shape == y.shape</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;est&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">VotingRegressor(</span>
            <span class="s1">estimators=[</span>
                <span class="s1">(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">LinearRegression())</span><span class="s2">,</span>
                <span class="s1">(</span><span class="s5">&quot;tree&quot;</span><span class="s2">, </span><span class="s1">DecisionTreeRegressor(random_state=</span><span class="s4">0</span><span class="s1">))</span><span class="s2">,</span>
            <span class="s1">]</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">VotingClassifier(</span>
            <span class="s1">estimators=[</span>
                <span class="s1">(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">LogisticRegression(random_state=</span><span class="s4">0</span><span class="s1">))</span><span class="s2">,</span>
                <span class="s1">(</span><span class="s5">&quot;tree&quot;</span><span class="s2">, </span><span class="s1">DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">))</span><span class="s2">,</span>
            <span class="s1">]</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
    <span class="s1">ids=[</span><span class="s5">&quot;VotingRegressor&quot;</span><span class="s2">, </span><span class="s5">&quot;VotingClassifier&quot;</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_n_features_in(est):</span>
    <span class="s1">X = [[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s2">, </span><span class="s4">6</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span>

    <span class="s2">assert not </span><span class="s1">hasattr(est</span><span class="s2">, </span><span class="s5">&quot;n_features_in_&quot;</span><span class="s1">)</span>
    <span class="s1">est.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">est.n_features_in_ == </span><span class="s4">2</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;estimator&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">VotingRegressor(</span>
            <span class="s1">estimators=[</span>
                <span class="s1">(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">LinearRegression())</span><span class="s2">,</span>
                <span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">RandomForestRegressor(random_state=</span><span class="s4">123</span><span class="s1">))</span><span class="s2">,</span>
            <span class="s1">]</span><span class="s2">,</span>
            <span class="s1">verbose=</span><span class="s2">True,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">VotingClassifier(</span>
            <span class="s1">estimators=[</span>
                <span class="s1">(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">LogisticRegression(random_state=</span><span class="s4">123</span><span class="s1">))</span><span class="s2">,</span>
                <span class="s1">(</span><span class="s5">&quot;rf&quot;</span><span class="s2">, </span><span class="s1">RandomForestClassifier(random_state=</span><span class="s4">123</span><span class="s1">))</span><span class="s2">,</span>
            <span class="s1">]</span><span class="s2">,</span>
            <span class="s1">verbose=</span><span class="s2">True,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_voting_verbose(estimator</span><span class="s2">, </span><span class="s1">capsys):</span>
    <span class="s1">X = np.array([[-</span><span class="s4">1.1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">1.2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">3.4</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2.2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1.1</span><span class="s2">, </span><span class="s4">1.2</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">])</span>

    <span class="s1">pattern = (</span>
        <span class="s5">r&quot;\[Voting\].*\(1 of 2\) Processing lr, total=.*\n&quot;</span>
        <span class="s5">r&quot;\[Voting\].*\(2 of 2\) Processing rf, total=.*\n$&quot;</span>
    <span class="s1">)</span>

    <span class="s1">estimator.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">re.match(pattern</span><span class="s2">, </span><span class="s1">capsys.readouterr()[</span><span class="s4">0</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_get_features_names_out_regressor():</span>
    <span class="s0">&quot;&quot;&quot;Check get_feature_names_out output for regressor.&quot;&quot;&quot;</span>

    <span class="s1">X = [[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s2">, </span><span class="s4">6</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span>

    <span class="s1">voting = VotingRegressor(</span>
        <span class="s1">estimators=[</span>
            <span class="s1">(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">LinearRegression())</span><span class="s2">,</span>
            <span class="s1">(</span><span class="s5">&quot;tree&quot;</span><span class="s2">, </span><span class="s1">DecisionTreeRegressor(random_state=</span><span class="s4">0</span><span class="s1">))</span><span class="s2">,</span>
            <span class="s1">(</span><span class="s5">&quot;ignore&quot;</span><span class="s2">, </span><span class="s5">&quot;drop&quot;</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">voting.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">names_out = voting.get_feature_names_out()</span>
    <span class="s1">expected_names = [</span><span class="s5">&quot;votingregressor_lr&quot;</span><span class="s2">, </span><span class="s5">&quot;votingregressor_tree&quot;</span><span class="s1">]</span>
    <span class="s1">assert_array_equal(names_out</span><span class="s2">, </span><span class="s1">expected_names)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;kwargs, expected_names&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s5">&quot;voting&quot;</span><span class="s1">: </span><span class="s5">&quot;soft&quot;</span><span class="s2">, </span><span class="s5">&quot;flatten_transform&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span><span class="s2">,</span>
            <span class="s1">[</span>
                <span class="s5">&quot;votingclassifier_lr0&quot;</span><span class="s2">,</span>
                <span class="s5">&quot;votingclassifier_lr1&quot;</span><span class="s2">,</span>
                <span class="s5">&quot;votingclassifier_lr2&quot;</span><span class="s2">,</span>
                <span class="s5">&quot;votingclassifier_tree0&quot;</span><span class="s2">,</span>
                <span class="s5">&quot;votingclassifier_tree1&quot;</span><span class="s2">,</span>
                <span class="s5">&quot;votingclassifier_tree2&quot;</span><span class="s2">,</span>
            <span class="s1">]</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">({</span><span class="s5">&quot;voting&quot;</span><span class="s1">: </span><span class="s5">&quot;hard&quot;</span><span class="s1">}</span><span class="s2">, </span><span class="s1">[</span><span class="s5">&quot;votingclassifier_lr&quot;</span><span class="s2">, </span><span class="s5">&quot;votingclassifier_tree&quot;</span><span class="s1">])</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_get_features_names_out_classifier(kwargs</span><span class="s2">, </span><span class="s1">expected_names):</span>
    <span class="s0">&quot;&quot;&quot;Check get_feature_names_out for classifier for different settings.&quot;&quot;&quot;</span>
    <span class="s1">X = [[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s2">, </span><span class="s4">6</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1.2</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>

    <span class="s1">voting = VotingClassifier(</span>
        <span class="s1">estimators=[</span>
            <span class="s1">(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">LogisticRegression(random_state=</span><span class="s4">0</span><span class="s1">))</span><span class="s2">,</span>
            <span class="s1">(</span><span class="s5">&quot;tree&quot;</span><span class="s2">, </span><span class="s1">DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s1">**kwargs</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">voting.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">X_trans = voting.transform(X)</span>
    <span class="s1">names_out = voting.get_feature_names_out()</span>

    <span class="s2">assert </span><span class="s1">X_trans.shape[</span><span class="s4">1</span><span class="s1">] == len(expected_names)</span>
    <span class="s1">assert_array_equal(names_out</span><span class="s2">, </span><span class="s1">expected_names)</span>


<span class="s2">def </span><span class="s1">test_get_features_names_out_classifier_error():</span>
    <span class="s0">&quot;&quot;&quot;Check that error is raised when voting=&quot;soft&quot; and flatten_transform=False.&quot;&quot;&quot;</span>
    <span class="s1">X = [[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">5</span><span class="s2">, </span><span class="s4">6</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span>

    <span class="s1">voting = VotingClassifier(</span>
        <span class="s1">estimators=[</span>
            <span class="s1">(</span><span class="s5">&quot;lr&quot;</span><span class="s2">, </span><span class="s1">LogisticRegression(random_state=</span><span class="s4">0</span><span class="s1">))</span><span class="s2">,</span>
            <span class="s1">(</span><span class="s5">&quot;tree&quot;</span><span class="s2">, </span><span class="s1">DecisionTreeClassifier(random_state=</span><span class="s4">0</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s1">]</span><span class="s2">,</span>
        <span class="s1">voting=</span><span class="s5">&quot;soft&quot;</span><span class="s2">,</span>
        <span class="s1">flatten_transform=</span><span class="s2">False,</span>
    <span class="s1">)</span>
    <span class="s1">voting.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">msg = (</span>
        <span class="s5">&quot;get_feature_names_out is not supported when `voting='soft'` and &quot;</span>
        <span class="s5">&quot;`flatten_transform=False`&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">voting.get_feature_names_out()</span>
</pre>
</body>
</html>