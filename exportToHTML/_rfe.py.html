<html>
<head>
<title>_rfe.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_rfe.py</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr&gt;</span>
<span class="s0">#          Vincent Michel &lt;vincent.michel@inria.fr&gt;</span>
<span class="s0">#          Gilles Louppe &lt;g.louppe@gmail.com&gt;</span>
<span class="s0">#</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">&quot;&quot;&quot;Recursive feature elimination for feature ranking&quot;&quot;&quot;</span>

<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">joblib </span><span class="s3">import </span><span class="s1">effective_n_jobs</span>

<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">BaseEstimator</span><span class="s3">, </span><span class="s1">MetaEstimatorMixin</span><span class="s3">, </span><span class="s1">_fit_context</span><span class="s3">, </span><span class="s1">clone</span><span class="s3">, </span><span class="s1">is_classifier</span>
<span class="s3">from </span><span class="s1">..metrics </span><span class="s3">import </span><span class="s1">check_scoring</span>
<span class="s3">from </span><span class="s1">..model_selection </span><span class="s3">import </span><span class="s1">check_cv</span>
<span class="s3">from </span><span class="s1">..model_selection._validation </span><span class="s3">import </span><span class="s1">_score</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">HasMethods</span><span class="s3">, </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">RealNotInt</span>
<span class="s3">from </span><span class="s1">..utils._tags </span><span class="s3">import </span><span class="s1">_safe_tags</span>
<span class="s3">from </span><span class="s1">..utils.metaestimators </span><span class="s3">import </span><span class="s1">_safe_split</span><span class="s3">, </span><span class="s1">available_if</span>
<span class="s3">from </span><span class="s1">..utils.parallel </span><span class="s3">import </span><span class="s1">Parallel</span><span class="s3">, </span><span class="s1">delayed</span>
<span class="s3">from </span><span class="s1">..utils.validation </span><span class="s3">import </span><span class="s1">check_is_fitted</span>
<span class="s3">from </span><span class="s1">._base </span><span class="s3">import </span><span class="s1">SelectorMixin</span><span class="s3">, </span><span class="s1">_get_feature_importances</span>


<span class="s3">def </span><span class="s1">_rfe_single_fit(rfe</span><span class="s3">, </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">train</span><span class="s3">, </span><span class="s1">test</span><span class="s3">, </span><span class="s1">scorer):</span>
    <span class="s2">&quot;&quot;&quot; 
    Return the score for a fit across one fold. 
    &quot;&quot;&quot;</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">y_train = _safe_split(estimator</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">train)</span>
    <span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_test = _safe_split(estimator</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">test</span><span class="s3">, </span><span class="s1">train)</span>
    <span class="s3">return </span><span class="s1">rfe._fit(</span>
        <span class="s1">X_train</span><span class="s3">,</span>
        <span class="s1">y_train</span><span class="s3">,</span>
        <span class="s3">lambda </span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">features: _score(</span>
            <span class="s1">estimator</span><span class="s3">, </span><span class="s1">X_test[:</span><span class="s3">, </span><span class="s1">features]</span><span class="s3">, </span><span class="s1">y_test</span><span class="s3">, </span><span class="s1">scorer</span>
        <span class="s1">)</span><span class="s3">,</span>
    <span class="s1">).scores_</span>


<span class="s3">def </span><span class="s1">_estimator_has(attr):</span>
    <span class="s2">&quot;&quot;&quot;Check if we can delegate a method to the underlying estimator. 
 
    First, we check the first fitted estimator if available, otherwise we 
    check the unfitted estimator. 
    &quot;&quot;&quot;</span>
    <span class="s3">return lambda </span><span class="s1">self: (</span>
        <span class="s1">hasattr(self.estimator_</span><span class="s3">, </span><span class="s1">attr)</span>
        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;estimator_&quot;</span><span class="s1">)</span>
        <span class="s3">else </span><span class="s1">hasattr(self.estimator</span><span class="s3">, </span><span class="s1">attr)</span>
    <span class="s1">)</span>


<span class="s3">class </span><span class="s1">RFE(SelectorMixin</span><span class="s3">, </span><span class="s1">MetaEstimatorMixin</span><span class="s3">, </span><span class="s1">BaseEstimator):</span>
    <span class="s2">&quot;&quot;&quot;Feature ranking with recursive feature elimination. 
 
    Given an external estimator that assigns weights to features (e.g., the 
    coefficients of a linear model), the goal of recursive feature elimination 
    (RFE) is to select features by recursively considering smaller and smaller 
    sets of features. First, the estimator is trained on the initial set of 
    features and the importance of each feature is obtained either through 
    any specific attribute or callable. 
    Then, the least important features are pruned from current set of features. 
    That procedure is recursively repeated on the pruned set until the desired 
    number of features to select is eventually reached. 
 
    Read more in the :ref:`User Guide &lt;rfe&gt;`. 
 
    Parameters 
    ---------- 
    estimator : ``Estimator`` instance 
        A supervised learning estimator with a ``fit`` method that provides 
        information about feature importance 
        (e.g. `coef_`, `feature_importances_`). 
 
    n_features_to_select : int or float, default=None 
        The number of features to select. If `None`, half of the features are 
        selected. If integer, the parameter is the absolute number of features 
        to select. If float between 0 and 1, it is the fraction of features to 
        select. 
 
        .. versionchanged:: 0.24 
           Added float values for fractions. 
 
    step : int or float, default=1 
        If greater than or equal to 1, then ``step`` corresponds to the 
        (integer) number of features to remove at each iteration. 
        If within (0.0, 1.0), then ``step`` corresponds to the percentage 
        (rounded down) of features to remove at each iteration. 
 
    verbose : int, default=0 
        Controls verbosity of output. 
 
    importance_getter : str or callable, default='auto' 
        If 'auto', uses the feature importance either through a `coef_` 
        or `feature_importances_` attributes of estimator. 
 
        Also accepts a string that specifies an attribute name/path 
        for extracting feature importance (implemented with `attrgetter`). 
        For example, give `regressor_.coef_` in case of 
        :class:`~sklearn.compose.TransformedTargetRegressor`  or 
        `named_steps.clf.feature_importances_` in case of 
        class:`~sklearn.pipeline.Pipeline` with its last step named `clf`. 
 
        If `callable`, overrides the default feature importance getter. 
        The callable is passed with the fitted estimator and it should 
        return importance for each feature. 
 
        .. versionadded:: 0.24 
 
    Attributes 
    ---------- 
    classes_ : ndarray of shape (n_classes,) 
        The classes labels. Only available when `estimator` is a classifier. 
 
    estimator_ : ``Estimator`` instance 
        The fitted estimator used to select features. 
 
    n_features_ : int 
        The number of selected features. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. Only defined if the 
        underlying estimator exposes such an attribute when fit. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    ranking_ : ndarray of shape (n_features,) 
        The feature ranking, such that ``ranking_[i]`` corresponds to the 
        ranking position of the i-th feature. Selected (i.e., estimated 
        best) features are assigned rank 1. 
 
    support_ : ndarray of shape (n_features,) 
        The mask of selected features. 
 
    See Also 
    -------- 
    RFECV : Recursive feature elimination with built-in cross-validated 
        selection of the best number of features. 
    SelectFromModel : Feature selection based on thresholds of importance 
        weights. 
    SequentialFeatureSelector : Sequential cross-validation based feature 
        selection. Does not rely on importance weights. 
 
    Notes 
    ----- 
    Allows NaN/Inf in the input if the underlying estimator does as well. 
 
    References 
    ---------- 
 
    .. [1] Guyon, I., Weston, J., Barnhill, S., &amp; Vapnik, V., &quot;Gene selection 
           for cancer classification using support vector machines&quot;, 
           Mach. Learn., 46(1-3), 389--422, 2002. 
 
    Examples 
    -------- 
    The following example shows how to retrieve the 5 most informative 
    features in the Friedman #1 dataset. 
 
    &gt;&gt;&gt; from sklearn.datasets import make_friedman1 
    &gt;&gt;&gt; from sklearn.feature_selection import RFE 
    &gt;&gt;&gt; from sklearn.svm import SVR 
    &gt;&gt;&gt; X, y = make_friedman1(n_samples=50, n_features=10, random_state=0) 
    &gt;&gt;&gt; estimator = SVR(kernel=&quot;linear&quot;) 
    &gt;&gt;&gt; selector = RFE(estimator, n_features_to_select=5, step=1) 
    &gt;&gt;&gt; selector = selector.fit(X, y) 
    &gt;&gt;&gt; selector.support_ 
    array([ True,  True,  True,  True,  True, False, False, False, False, 
           False]) 
    &gt;&gt;&gt; selector.ranking_ 
    array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;estimator&quot;</span><span class="s1">: [HasMethods([</span><span class="s4">&quot;fit&quot;</span><span class="s1">])]</span><span class="s3">,</span>
        <span class="s4">&quot;n_features_to_select&quot;</span><span class="s1">: [</span>
            <span class="s3">None,</span>
            <span class="s1">Interval(RealNotInt</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">closed=</span><span class="s4">&quot;right&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;step&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">Interval(RealNotInt</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s4">&quot;verbose&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;importance_getter&quot;</span><span class="s1">: [str</span><span class="s3">, </span><span class="s1">callable]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">estimator</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">n_features_to_select=</span><span class="s3">None,</span>
        <span class="s1">step=</span><span class="s5">1</span><span class="s3">,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">importance_getter=</span><span class="s4">&quot;auto&quot;</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">self.estimator = estimator</span>
        <span class="s1">self.n_features_to_select = n_features_to_select</span>
        <span class="s1">self.step = step</span>
        <span class="s1">self.importance_getter = importance_getter</span>
        <span class="s1">self.verbose = verbose</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">_estimator_type(self):</span>
        <span class="s3">return </span><span class="s1">self.estimator._estimator_type</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">classes_(self):</span>
        <span class="s2">&quot;&quot;&quot;Classes labels available when `estimator` is a classifier. 
 
        Returns 
        ------- 
        ndarray of shape (n_classes,) 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.estimator_.classes_</span>

    <span class="s1">@_fit_context(</span>
        <span class="s0"># RFE.estimator is not validated yet</span>
        <span class="s1">prefer_skip_nested_validation=</span><span class="s3">False</span>
    <span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">**fit_params):</span>
        <span class="s2">&quot;&quot;&quot;Fit the RFE model and then the underlying estimator on the selected features. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The training input samples. 
 
        y : array-like of shape (n_samples,) 
            The target values. 
 
        **fit_params : dict 
            Additional parameters passed to the `fit` method of the underlying 
            estimator. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self._fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">**fit_params)</span>

    <span class="s3">def </span><span class="s1">_fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">step_score=</span><span class="s3">None, </span><span class="s1">**fit_params):</span>
        <span class="s0"># Parameter step_score controls the calculation of self.scores_</span>
        <span class="s0"># step_score is not exposed to users</span>
        <span class="s0"># and is used when implementing RFECV</span>
        <span class="s0"># self.scores_ will not be calculated when calling _fit through fit</span>

        <span class="s1">tags = self._get_tags()</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">accept_sparse=</span><span class="s4">&quot;csc&quot;</span><span class="s3">,</span>
            <span class="s1">ensure_min_features=</span><span class="s5">2</span><span class="s3">,</span>
            <span class="s1">force_all_finite=</span><span class="s3">not </span><span class="s1">tags.get(</span><span class="s4">&quot;allow_nan&quot;</span><span class="s3">, True</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">multi_output=</span><span class="s3">True,</span>
        <span class="s1">)</span>

        <span class="s0"># Initialization</span>
        <span class="s1">n_features = X.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">if </span><span class="s1">self.n_features_to_select </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">n_features_to_select = n_features // </span><span class="s5">2</span>
        <span class="s3">elif </span><span class="s1">isinstance(self.n_features_to_select</span><span class="s3">, </span><span class="s1">Integral):  </span><span class="s0"># int</span>
            <span class="s1">n_features_to_select = self.n_features_to_select</span>
        <span class="s3">else</span><span class="s1">:  </span><span class="s0"># float</span>
            <span class="s1">n_features_to_select = int(n_features * self.n_features_to_select)</span>

        <span class="s3">if </span><span class="s5">0.0 </span><span class="s1">&lt; self.step &lt; </span><span class="s5">1.0</span><span class="s1">:</span>
            <span class="s1">step = int(max(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">self.step * n_features))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">step = int(self.step)</span>

        <span class="s1">support_ = np.ones(n_features</span><span class="s3">, </span><span class="s1">dtype=bool)</span>
        <span class="s1">ranking_ = np.ones(n_features</span><span class="s3">, </span><span class="s1">dtype=int)</span>

        <span class="s3">if </span><span class="s1">step_score:</span>
            <span class="s1">self.scores_ = []</span>

        <span class="s0"># Elimination</span>
        <span class="s3">while </span><span class="s1">np.sum(support_) &gt; n_features_to_select:</span>
            <span class="s0"># Remaining features</span>
            <span class="s1">features = np.arange(n_features)[support_]</span>

            <span class="s0"># Rank the remaining features</span>
            <span class="s1">estimator = clone(self.estimator)</span>
            <span class="s3">if </span><span class="s1">self.verbose &gt; </span><span class="s5">0</span><span class="s1">:</span>
                <span class="s1">print(</span><span class="s4">&quot;Fitting estimator with %d features.&quot; </span><span class="s1">% np.sum(support_))</span>

            <span class="s1">estimator.fit(X[:</span><span class="s3">, </span><span class="s1">features]</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">**fit_params)</span>

            <span class="s0"># Get importance and rank them</span>
            <span class="s1">importances = _get_feature_importances(</span>
                <span class="s1">estimator</span><span class="s3">,</span>
                <span class="s1">self.importance_getter</span><span class="s3">,</span>
                <span class="s1">transform_func=</span><span class="s4">&quot;square&quot;</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">ranks = np.argsort(importances)</span>

            <span class="s0"># for sparse case ranks is matrix</span>
            <span class="s1">ranks = np.ravel(ranks)</span>

            <span class="s0"># Eliminate the worse features</span>
            <span class="s1">threshold = min(step</span><span class="s3">, </span><span class="s1">np.sum(support_) - n_features_to_select)</span>

            <span class="s0"># Compute step score on the previous selection iteration</span>
            <span class="s0"># because 'estimator' must use features</span>
            <span class="s0"># that have not been eliminated yet</span>
            <span class="s3">if </span><span class="s1">step_score:</span>
                <span class="s1">self.scores_.append(step_score(estimator</span><span class="s3">, </span><span class="s1">features))</span>
            <span class="s1">support_[features[ranks][:threshold]] = </span><span class="s3">False</span>
            <span class="s1">ranking_[np.logical_not(support_)] += </span><span class="s5">1</span>

        <span class="s0"># Set final attributes</span>
        <span class="s1">features = np.arange(n_features)[support_]</span>
        <span class="s1">self.estimator_ = clone(self.estimator)</span>
        <span class="s1">self.estimator_.fit(X[:</span><span class="s3">, </span><span class="s1">features]</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">**fit_params)</span>

        <span class="s0"># Compute step score when only n_features_to_select features left</span>
        <span class="s3">if </span><span class="s1">step_score:</span>
            <span class="s1">self.scores_.append(step_score(self.estimator_</span><span class="s3">, </span><span class="s1">features))</span>
        <span class="s1">self.n_features_ = support_.sum()</span>
        <span class="s1">self.support_ = support_</span>
        <span class="s1">self.ranking_ = ranking_</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s1">@available_if(_estimator_has(</span><span class="s4">&quot;predict&quot;</span><span class="s1">))</span>
    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s2">&quot;&quot;&quot;Reduce X to the selected features and predict using the estimator. 
 
        Parameters 
        ---------- 
        X : array of shape [n_samples, n_features] 
            The input samples. 
 
        Returns 
        ------- 
        y : array of shape [n_samples] 
            The predicted target values. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">return </span><span class="s1">self.estimator_.predict(self.transform(X))</span>

    <span class="s1">@available_if(_estimator_has(</span><span class="s4">&quot;score&quot;</span><span class="s1">))</span>
    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">**fit_params):</span>
        <span class="s2">&quot;&quot;&quot;Reduce X to the selected features and return the score of the estimator. 
 
        Parameters 
        ---------- 
        X : array of shape [n_samples, n_features] 
            The input samples. 
 
        y : array of shape [n_samples] 
            The target values. 
 
        **fit_params : dict 
            Parameters to pass to the `score` method of the underlying 
            estimator. 
 
            .. versionadded:: 1.0 
 
        Returns 
        ------- 
        score : float 
            Score of the underlying base estimator computed with the selected 
            features returned by `rfe.transform(X)` and `y`. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">return </span><span class="s1">self.estimator_.score(self.transform(X)</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">**fit_params)</span>

    <span class="s3">def </span><span class="s1">_get_support_mask(self):</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">return </span><span class="s1">self.support_</span>

    <span class="s1">@available_if(_estimator_has(</span><span class="s4">&quot;decision_function&quot;</span><span class="s1">))</span>
    <span class="s3">def </span><span class="s1">decision_function(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s2">&quot;&quot;&quot;Compute the decision function of ``X``. 
 
        Parameters 
        ---------- 
        X : {array-like or sparse matrix} of shape (n_samples, n_features) 
            The input samples. Internally, it will be converted to 
            ``dtype=np.float32`` and if a sparse matrix is provided 
            to a sparse ``csr_matrix``. 
 
        Returns 
        ------- 
        score : array, shape = [n_samples, n_classes] or [n_samples] 
            The decision function of the input samples. The order of the 
            classes corresponds to that in the attribute :term:`classes_`. 
            Regression and binary classification produce an array of shape 
            [n_samples]. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">return </span><span class="s1">self.estimator_.decision_function(self.transform(X))</span>

    <span class="s1">@available_if(_estimator_has(</span><span class="s4">&quot;predict_proba&quot;</span><span class="s1">))</span>
    <span class="s3">def </span><span class="s1">predict_proba(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s2">&quot;&quot;&quot;Predict class probabilities for X. 
 
        Parameters 
        ---------- 
        X : {array-like or sparse matrix} of shape (n_samples, n_features) 
            The input samples. Internally, it will be converted to 
            ``dtype=np.float32`` and if a sparse matrix is provided 
            to a sparse ``csr_matrix``. 
 
        Returns 
        ------- 
        p : array of shape (n_samples, n_classes) 
            The class probabilities of the input samples. The order of the 
            classes corresponds to that in the attribute :term:`classes_`. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">return </span><span class="s1">self.estimator_.predict_proba(self.transform(X))</span>

    <span class="s1">@available_if(_estimator_has(</span><span class="s4">&quot;predict_log_proba&quot;</span><span class="s1">))</span>
    <span class="s3">def </span><span class="s1">predict_log_proba(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s2">&quot;&quot;&quot;Predict class log-probabilities for X. 
 
        Parameters 
        ---------- 
        X : array of shape [n_samples, n_features] 
            The input samples. 
 
        Returns 
        ------- 
        p : array of shape (n_samples, n_classes) 
            The class log-probabilities of the input samples. The order of the 
            classes corresponds to that in the attribute :term:`classes_`. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">return </span><span class="s1">self.estimator_.predict_log_proba(self.transform(X))</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span>
            <span class="s4">&quot;poor_score&quot;</span><span class="s1">: </span><span class="s3">True,</span>
            <span class="s4">&quot;allow_nan&quot;</span><span class="s1">: _safe_tags(self.estimator</span><span class="s3">, </span><span class="s1">key=</span><span class="s4">&quot;allow_nan&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s4">&quot;requires_y&quot;</span><span class="s1">: </span><span class="s3">True,</span>
        <span class="s1">}</span>


<span class="s3">class </span><span class="s1">RFECV(RFE):</span>
    <span class="s2">&quot;&quot;&quot;Recursive feature elimination with cross-validation to select features. 
 
    See glossary entry for :term:`cross-validation estimator`. 
 
    Read more in the :ref:`User Guide &lt;rfe&gt;`. 
 
    Parameters 
    ---------- 
    estimator : ``Estimator`` instance 
        A supervised learning estimator with a ``fit`` method that provides 
        information about feature importance either through a ``coef_`` 
        attribute or through a ``feature_importances_`` attribute. 
 
    step : int or float, default=1 
        If greater than or equal to 1, then ``step`` corresponds to the 
        (integer) number of features to remove at each iteration. 
        If within (0.0, 1.0), then ``step`` corresponds to the percentage 
        (rounded down) of features to remove at each iteration. 
        Note that the last iteration may remove fewer than ``step`` features in 
        order to reach ``min_features_to_select``. 
 
    min_features_to_select : int, default=1 
        The minimum number of features to be selected. This number of features 
        will always be scored, even if the difference between the original 
        feature count and ``min_features_to_select`` isn't divisible by 
        ``step``. 
 
        .. versionadded:: 0.20 
 
    cv : int, cross-validation generator or an iterable, default=None 
        Determines the cross-validation splitting strategy. 
        Possible inputs for cv are: 
 
        - None, to use the default 5-fold cross-validation, 
        - integer, to specify the number of folds. 
        - :term:`CV splitter`, 
        - An iterable yielding (train, test) splits as arrays of indices. 
 
        For integer/None inputs, if ``y`` is binary or multiclass, 
        :class:`~sklearn.model_selection.StratifiedKFold` is used. If the 
        estimator is a classifier or if ``y`` is neither binary nor multiclass, 
        :class:`~sklearn.model_selection.KFold` is used. 
 
        Refer :ref:`User Guide &lt;cross_validation&gt;` for the various 
        cross-validation strategies that can be used here. 
 
        .. versionchanged:: 0.22 
            ``cv`` default value of None changed from 3-fold to 5-fold. 
 
    scoring : str, callable or None, default=None 
        A string (see model evaluation documentation) or 
        a scorer callable object / function with signature 
        ``scorer(estimator, X, y)``. 
 
    verbose : int, default=0 
        Controls verbosity of output. 
 
    n_jobs : int or None, default=None 
        Number of cores to run in parallel while fitting across folds. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
        .. versionadded:: 0.18 
 
    importance_getter : str or callable, default='auto' 
        If 'auto', uses the feature importance either through a `coef_` 
        or `feature_importances_` attributes of estimator. 
 
        Also accepts a string that specifies an attribute name/path 
        for extracting feature importance. 
        For example, give `regressor_.coef_` in case of 
        :class:`~sklearn.compose.TransformedTargetRegressor`  or 
        `named_steps.clf.feature_importances_` in case of 
        :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`. 
 
        If `callable`, overrides the default feature importance getter. 
        The callable is passed with the fitted estimator and it should 
        return importance for each feature. 
 
        .. versionadded:: 0.24 
 
    Attributes 
    ---------- 
    classes_ : ndarray of shape (n_classes,) 
        The classes labels. Only available when `estimator` is a classifier. 
 
    estimator_ : ``Estimator`` instance 
        The fitted estimator used to select features. 
 
    cv_results_ : dict of ndarrays 
        A dict with keys: 
 
        split(k)_test_score : ndarray of shape (n_subsets_of_features,) 
            The cross-validation scores across (k)th fold. 
 
        mean_test_score : ndarray of shape (n_subsets_of_features,) 
            Mean of scores over the folds. 
 
        std_test_score : ndarray of shape (n_subsets_of_features,) 
            Standard deviation of scores over the folds. 
 
        .. versionadded:: 1.0 
 
    n_features_ : int 
        The number of selected features with cross-validation. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. Only defined if the 
        underlying estimator exposes such an attribute when fit. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    ranking_ : narray of shape (n_features,) 
        The feature ranking, such that `ranking_[i]` 
        corresponds to the ranking 
        position of the i-th feature. 
        Selected (i.e., estimated best) 
        features are assigned rank 1. 
 
    support_ : ndarray of shape (n_features,) 
        The mask of selected features. 
 
    See Also 
    -------- 
    RFE : Recursive feature elimination. 
 
    Notes 
    ----- 
    The size of all values in ``cv_results_`` is equal to 
    ``ceil((n_features - min_features_to_select) / step) + 1``, 
    where step is the number of features removed at each iteration. 
 
    Allows NaN/Inf in the input if the underlying estimator does as well. 
 
    References 
    ---------- 
 
    .. [1] Guyon, I., Weston, J., Barnhill, S., &amp; Vapnik, V., &quot;Gene selection 
           for cancer classification using support vector machines&quot;, 
           Mach. Learn., 46(1-3), 389--422, 2002. 
 
    Examples 
    -------- 
    The following example shows how to retrieve the a-priori not known 5 
    informative features in the Friedman #1 dataset. 
 
    &gt;&gt;&gt; from sklearn.datasets import make_friedman1 
    &gt;&gt;&gt; from sklearn.feature_selection import RFECV 
    &gt;&gt;&gt; from sklearn.svm import SVR 
    &gt;&gt;&gt; X, y = make_friedman1(n_samples=50, n_features=10, random_state=0) 
    &gt;&gt;&gt; estimator = SVR(kernel=&quot;linear&quot;) 
    &gt;&gt;&gt; selector = RFECV(estimator, step=1, cv=5) 
    &gt;&gt;&gt; selector = selector.fit(X, y) 
    &gt;&gt;&gt; selector.support_ 
    array([ True,  True,  True,  True,  True, False, False, False, False, 
           False]) 
    &gt;&gt;&gt; selector.ranking_ 
    array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**RFE._parameter_constraints</span><span class="s3">,</span>
        <span class="s4">&quot;min_features_to_select&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;cv&quot;</span><span class="s1">: [</span><span class="s4">&quot;cv_object&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;scoring&quot;</span><span class="s1">: [</span><span class="s3">None, </span><span class="s1">str</span><span class="s3">, </span><span class="s1">callable]</span><span class="s3">,</span>
        <span class="s4">&quot;n_jobs&quot;</span><span class="s1">: [</span><span class="s3">None, </span><span class="s1">Integral]</span><span class="s3">,</span>
    <span class="s1">}</span>
    <span class="s1">_parameter_constraints.pop(</span><span class="s4">&quot;n_features_to_select&quot;</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">estimator</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">step=</span><span class="s5">1</span><span class="s3">,</span>
        <span class="s1">min_features_to_select=</span><span class="s5">1</span><span class="s3">,</span>
        <span class="s1">cv=</span><span class="s3">None,</span>
        <span class="s1">scoring=</span><span class="s3">None,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">n_jobs=</span><span class="s3">None,</span>
        <span class="s1">importance_getter=</span><span class="s4">&quot;auto&quot;</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">self.estimator = estimator</span>
        <span class="s1">self.step = step</span>
        <span class="s1">self.importance_getter = importance_getter</span>
        <span class="s1">self.cv = cv</span>
        <span class="s1">self.scoring = scoring</span>
        <span class="s1">self.verbose = verbose</span>
        <span class="s1">self.n_jobs = n_jobs</span>
        <span class="s1">self.min_features_to_select = min_features_to_select</span>

    <span class="s1">@_fit_context(</span>
        <span class="s0"># RFECV.estimator is not validated yet</span>
        <span class="s1">prefer_skip_nested_validation=</span><span class="s3">False</span>
    <span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">groups=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;Fit the RFE model and automatically tune the number of selected features. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training vector, where `n_samples` is the number of samples and 
            `n_features` is the total number of features. 
 
        y : array-like of shape (n_samples,) 
            Target values (integers for classification, real numbers for 
            regression). 
 
        groups : array-like of shape (n_samples,) or None, default=None 
            Group labels for the samples used while splitting the dataset into 
            train/test set. Only used in conjunction with a &quot;Group&quot; :term:`cv` 
            instance (e.g., :class:`~sklearn.model_selection.GroupKFold`). 
 
            .. versionadded:: 0.20 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">tags = self._get_tags()</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">,</span>
            <span class="s1">ensure_min_features=</span><span class="s5">2</span><span class="s3">,</span>
            <span class="s1">force_all_finite=</span><span class="s3">not </span><span class="s1">tags.get(</span><span class="s4">&quot;allow_nan&quot;</span><span class="s3">, True</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">multi_output=</span><span class="s3">True,</span>
        <span class="s1">)</span>

        <span class="s0"># Initialization</span>
        <span class="s1">cv = check_cv(self.cv</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">classifier=is_classifier(self.estimator))</span>
        <span class="s1">scorer = check_scoring(self.estimator</span><span class="s3">, </span><span class="s1">scoring=self.scoring)</span>
        <span class="s1">n_features = X.shape[</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s3">if </span><span class="s5">0.0 </span><span class="s1">&lt; self.step &lt; </span><span class="s5">1.0</span><span class="s1">:</span>
            <span class="s1">step = int(max(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">self.step * n_features))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">step = int(self.step)</span>

        <span class="s0"># Build an RFE object, which will evaluate and score each possible</span>
        <span class="s0"># feature count, down to self.min_features_to_select</span>
        <span class="s1">rfe = RFE(</span>
            <span class="s1">estimator=self.estimator</span><span class="s3">,</span>
            <span class="s1">n_features_to_select=self.min_features_to_select</span><span class="s3">,</span>
            <span class="s1">importance_getter=self.importance_getter</span><span class="s3">,</span>
            <span class="s1">step=self.step</span><span class="s3">,</span>
            <span class="s1">verbose=self.verbose</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s0"># Determine the number of subsets of features by fitting across</span>
        <span class="s0"># the train folds and choosing the &quot;features_to_select&quot; parameter</span>
        <span class="s0"># that gives the least averaged error across all folds.</span>

        <span class="s0"># Note that joblib raises a non-picklable error for bound methods</span>
        <span class="s0"># even if n_jobs is set to 1 with the default multiprocessing</span>
        <span class="s0"># backend.</span>
        <span class="s0"># This branching is done so that to</span>
        <span class="s0"># make sure that user code that sets n_jobs to 1</span>
        <span class="s0"># and provides bound methods as scorers is not broken with the</span>
        <span class="s0"># addition of n_jobs parameter in version 0.18.</span>

        <span class="s3">if </span><span class="s1">effective_n_jobs(self.n_jobs) == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">parallel</span><span class="s3">, </span><span class="s1">func = list</span><span class="s3">, </span><span class="s1">_rfe_single_fit</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">parallel = Parallel(n_jobs=self.n_jobs)</span>
            <span class="s1">func = delayed(_rfe_single_fit)</span>

        <span class="s1">scores = parallel(</span>
            <span class="s1">func(rfe</span><span class="s3">, </span><span class="s1">self.estimator</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">train</span><span class="s3">, </span><span class="s1">test</span><span class="s3">, </span><span class="s1">scorer)</span>
            <span class="s3">for </span><span class="s1">train</span><span class="s3">, </span><span class="s1">test </span><span class="s3">in </span><span class="s1">cv.split(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">groups)</span>
        <span class="s1">)</span>

        <span class="s1">scores = np.array(scores)</span>
        <span class="s1">scores_sum = np.sum(scores</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">scores_sum_rev = scores_sum[::-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">argmax_idx = len(scores_sum) - np.argmax(scores_sum_rev) - </span><span class="s5">1</span>
        <span class="s1">n_features_to_select = max(</span>
            <span class="s1">n_features - (argmax_idx * step)</span><span class="s3">, </span><span class="s1">self.min_features_to_select</span>
        <span class="s1">)</span>

        <span class="s0"># Re-execute an elimination with best_k over the whole set</span>
        <span class="s1">rfe = RFE(</span>
            <span class="s1">estimator=self.estimator</span><span class="s3">,</span>
            <span class="s1">n_features_to_select=n_features_to_select</span><span class="s3">,</span>
            <span class="s1">step=self.step</span><span class="s3">,</span>
            <span class="s1">importance_getter=self.importance_getter</span><span class="s3">,</span>
            <span class="s1">verbose=self.verbose</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s1">rfe.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s0"># Set final attributes</span>
        <span class="s1">self.support_ = rfe.support_</span>
        <span class="s1">self.n_features_ = rfe.n_features_</span>
        <span class="s1">self.ranking_ = rfe.ranking_</span>
        <span class="s1">self.estimator_ = clone(self.estimator)</span>
        <span class="s1">self.estimator_.fit(self._transform(X)</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s0"># reverse to stay consistent with before</span>
        <span class="s1">scores_rev = scores[:</span><span class="s3">, </span><span class="s1">::-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">self.cv_results_ = {}</span>
        <span class="s1">self.cv_results_[</span><span class="s4">&quot;mean_test_score&quot;</span><span class="s1">] = np.mean(scores_rev</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">self.cv_results_[</span><span class="s4">&quot;std_test_score&quot;</span><span class="s1">] = np.std(scores_rev</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>

        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(scores.shape[</span><span class="s5">0</span><span class="s1">]):</span>
            <span class="s1">self.cv_results_[</span><span class="s4">f&quot;split</span><span class="s3">{</span><span class="s1">i</span><span class="s3">}</span><span class="s4">_test_score&quot;</span><span class="s1">] = scores_rev[i]</span>

        <span class="s3">return </span><span class="s1">self</span>
</pre>
</body>
</html>