<html>
<head>
<title>_differentiable_functions.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #808080;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_differentiable_functions.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">scipy.sparse </span><span class="s0">as </span><span class="s1">sps</span>
<span class="s0">from </span><span class="s1">._numdiff </span><span class="s0">import </span><span class="s1">approx_derivative</span><span class="s0">, </span><span class="s1">group_columns</span>
<span class="s0">from </span><span class="s1">._hessian_update_strategy </span><span class="s0">import </span><span class="s1">HessianUpdateStrategy</span>
<span class="s0">from </span><span class="s1">scipy.sparse.linalg </span><span class="s0">import </span><span class="s1">LinearOperator</span>


<span class="s1">FD_METHODS = (</span><span class="s2">'2-point'</span><span class="s0">, </span><span class="s2">'3-point'</span><span class="s0">, </span><span class="s2">'cs'</span><span class="s1">)</span>


<span class="s0">class </span><span class="s1">ScalarFunction:</span>
    <span class="s3">&quot;&quot;&quot;Scalar function and its derivatives. 
 
    This class defines a scalar function F: R^n-&gt;R and methods for 
    computing or approximating its first and second derivatives. 
 
    Parameters 
    ---------- 
    fun : callable 
        evaluates the scalar function. Must be of the form ``fun(x, *args)``, 
        where ``x`` is the argument in the form of a 1-D array and ``args`` is 
        a tuple of any additional fixed parameters needed to completely specify 
        the function. Should return a scalar. 
    x0 : array-like 
        Provides an initial set of variables for evaluating fun. Array of real 
        elements of size (n,), where 'n' is the number of independent 
        variables. 
    args : tuple, optional 
        Any additional fixed parameters needed to completely specify the scalar 
        function. 
    grad : {callable, '2-point', '3-point', 'cs'} 
        Method for computing the gradient vector. 
        If it is a callable, it should be a function that returns the gradient 
        vector: 
 
            ``grad(x, *args) -&gt; array_like, shape (n,)`` 
 
        where ``x`` is an array with shape (n,) and ``args`` is a tuple with 
        the fixed parameters. 
        Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used 
        to select a finite difference scheme for numerical estimation of the 
        gradient with a relative step size. These finite difference schemes 
        obey any specified `bounds`. 
    hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy} 
        Method for computing the Hessian matrix. If it is callable, it should 
        return the  Hessian matrix: 
 
            ``hess(x, *args) -&gt; {LinearOperator, spmatrix, array}, (n, n)`` 
 
        where x is a (n,) ndarray and `args` is a tuple with the fixed 
        parameters. Alternatively, the keywords {'2-point', '3-point', 'cs'} 
        select a finite difference scheme for numerical estimation. Or, objects 
        implementing `HessianUpdateStrategy` interface can be used to 
        approximate the Hessian. 
        Whenever the gradient is estimated via finite-differences, the Hessian 
        cannot be estimated with options {'2-point', '3-point', 'cs'} and needs 
        to be estimated using one of the quasi-Newton strategies. 
    finite_diff_rel_step : None or array_like 
        Relative step size to use. The absolute step size is computed as 
        ``h = finite_diff_rel_step * sign(x0) * max(1, abs(x0))``, possibly 
        adjusted to fit into the bounds. For ``method='3-point'`` the sign 
        of `h` is ignored. If None then finite_diff_rel_step is selected 
        automatically, 
    finite_diff_bounds : tuple of array_like 
        Lower and upper bounds on independent variables. Defaults to no bounds, 
        (-np.inf, np.inf). Each bound must match the size of `x0` or be a 
        scalar, in the latter case the bound will be the same for all 
        variables. Use it to limit the range of function evaluation. 
    epsilon : None or array_like, optional 
        Absolute step size to use, possibly adjusted to fit into the bounds. 
        For ``method='3-point'`` the sign of `epsilon` is ignored. By default 
        relative steps are used, only if ``epsilon is not None`` are absolute 
        steps used. 
 
    Notes 
    ----- 
    This class implements a memoization logic. There are methods `fun`, 
    `grad`, hess` and corresponding attributes `f`, `g` and `H`. The following 
    things should be considered: 
 
        1. Use only public methods `fun`, `grad` and `hess`. 
        2. After one of the methods is called, the corresponding attribute 
           will be set. However, a subsequent call with a different argument 
           of *any* of the methods may overwrite the attribute. 
    &quot;&quot;&quot;</span>
    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">fun</span><span class="s0">, </span><span class="s1">x0</span><span class="s0">, </span><span class="s1">args</span><span class="s0">, </span><span class="s1">grad</span><span class="s0">, </span><span class="s1">hess</span><span class="s0">, </span><span class="s1">finite_diff_rel_step</span><span class="s0">,</span>
                 <span class="s1">finite_diff_bounds</span><span class="s0">, </span><span class="s1">epsilon=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s0">if not </span><span class="s1">callable(grad) </span><span class="s0">and </span><span class="s1">grad </span><span class="s0">not in </span><span class="s1">FD_METHODS:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span>
                <span class="s2">f&quot;`grad` must be either callable or one of </span><span class="s0">{</span><span class="s1">FD_METHODS</span><span class="s0">}</span><span class="s2">.&quot;</span>
            <span class="s1">)</span>

        <span class="s0">if not </span><span class="s1">(callable(hess) </span><span class="s0">or </span><span class="s1">hess </span><span class="s0">in </span><span class="s1">FD_METHODS</span>
                <span class="s0">or </span><span class="s1">isinstance(hess</span><span class="s0">, </span><span class="s1">HessianUpdateStrategy)):</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span>
                <span class="s2">f&quot;`hess` must be either callable, HessianUpdateStrategy&quot;</span>
                <span class="s2">f&quot; or one of </span><span class="s0">{</span><span class="s1">FD_METHODS</span><span class="s0">}</span><span class="s2">.&quot;</span>
            <span class="s1">)</span>

        <span class="s0">if </span><span class="s1">grad </span><span class="s0">in </span><span class="s1">FD_METHODS </span><span class="s0">and </span><span class="s1">hess </span><span class="s0">in </span><span class="s1">FD_METHODS:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;Whenever the gradient is estimated via &quot;</span>
                             <span class="s2">&quot;finite-differences, we require the Hessian &quot;</span>
                             <span class="s2">&quot;to be estimated using one of the &quot;</span>
                             <span class="s2">&quot;quasi-Newton strategies.&quot;</span><span class="s1">)</span>

        <span class="s4"># the astype call ensures that self.x is a copy of x0</span>
        <span class="s1">self.x = np.atleast_1d(x0).astype(float)</span>
        <span class="s1">self.n = self.x.size</span>
        <span class="s1">self.nfev = </span><span class="s5">0</span>
        <span class="s1">self.ngev = </span><span class="s5">0</span>
        <span class="s1">self.nhev = </span><span class="s5">0</span>
        <span class="s1">self.f_updated = </span><span class="s0">False</span>
        <span class="s1">self.g_updated = </span><span class="s0">False</span>
        <span class="s1">self.H_updated = </span><span class="s0">False</span>

        <span class="s1">self._lowest_x = </span><span class="s0">None</span>
        <span class="s1">self._lowest_f = np.inf</span>

        <span class="s1">finite_diff_options = {}</span>
        <span class="s0">if </span><span class="s1">grad </span><span class="s0">in </span><span class="s1">FD_METHODS:</span>
            <span class="s1">finite_diff_options[</span><span class="s2">&quot;method&quot;</span><span class="s1">] = grad</span>
            <span class="s1">finite_diff_options[</span><span class="s2">&quot;rel_step&quot;</span><span class="s1">] = finite_diff_rel_step</span>
            <span class="s1">finite_diff_options[</span><span class="s2">&quot;abs_step&quot;</span><span class="s1">] = epsilon</span>
            <span class="s1">finite_diff_options[</span><span class="s2">&quot;bounds&quot;</span><span class="s1">] = finite_diff_bounds</span>
        <span class="s0">if </span><span class="s1">hess </span><span class="s0">in </span><span class="s1">FD_METHODS:</span>
            <span class="s1">finite_diff_options[</span><span class="s2">&quot;method&quot;</span><span class="s1">] = hess</span>
            <span class="s1">finite_diff_options[</span><span class="s2">&quot;rel_step&quot;</span><span class="s1">] = finite_diff_rel_step</span>
            <span class="s1">finite_diff_options[</span><span class="s2">&quot;abs_step&quot;</span><span class="s1">] = epsilon</span>
            <span class="s1">finite_diff_options[</span><span class="s2">&quot;as_linear_operator&quot;</span><span class="s1">] = </span><span class="s0">True</span>

        <span class="s4"># Function evaluation</span>
        <span class="s0">def </span><span class="s1">fun_wrapped(x):</span>
            <span class="s1">self.nfev += </span><span class="s5">1</span>
            <span class="s4"># Send a copy because the user may overwrite it.</span>
            <span class="s4"># Overwriting results in undefined behaviour because</span>
            <span class="s4"># fun(self.x) will change self.x, with the two no longer linked.</span>
            <span class="s1">fx = fun(np.copy(x)</span><span class="s0">, </span><span class="s1">*args)</span>
            <span class="s4"># Make sure the function returns a true scalar</span>
            <span class="s0">if not </span><span class="s1">np.isscalar(fx):</span>
                <span class="s0">try</span><span class="s1">:</span>
                    <span class="s1">fx = np.asarray(fx).item()</span>
                <span class="s0">except </span><span class="s1">(TypeError</span><span class="s0">, </span><span class="s1">ValueError) </span><span class="s0">as </span><span class="s1">e:</span>
                    <span class="s0">raise </span><span class="s1">ValueError(</span>
                        <span class="s2">&quot;The user-provided objective function &quot;</span>
                        <span class="s2">&quot;must return a scalar value.&quot;</span>
                    <span class="s1">) </span><span class="s0">from </span><span class="s1">e</span>

            <span class="s0">if </span><span class="s1">fx &lt; self._lowest_f:</span>
                <span class="s1">self._lowest_x = x</span>
                <span class="s1">self._lowest_f = fx</span>

            <span class="s0">return </span><span class="s1">fx</span>

        <span class="s0">def </span><span class="s1">update_fun():</span>
            <span class="s1">self.f = fun_wrapped(self.x)</span>

        <span class="s1">self._update_fun_impl = update_fun</span>
        <span class="s1">self._update_fun()</span>

        <span class="s4"># Gradient evaluation</span>
        <span class="s0">if </span><span class="s1">callable(grad):</span>
            <span class="s0">def </span><span class="s1">grad_wrapped(x):</span>
                <span class="s1">self.ngev += </span><span class="s5">1</span>
                <span class="s0">return </span><span class="s1">np.atleast_1d(grad(np.copy(x)</span><span class="s0">, </span><span class="s1">*args))</span>

            <span class="s0">def </span><span class="s1">update_grad():</span>
                <span class="s1">self.g = grad_wrapped(self.x)</span>

        <span class="s0">elif </span><span class="s1">grad </span><span class="s0">in </span><span class="s1">FD_METHODS:</span>
            <span class="s0">def </span><span class="s1">update_grad():</span>
                <span class="s1">self._update_fun()</span>
                <span class="s1">self.ngev += </span><span class="s5">1</span>
                <span class="s1">self.g = approx_derivative(fun_wrapped</span><span class="s0">, </span><span class="s1">self.x</span><span class="s0">, </span><span class="s1">f0=self.f</span><span class="s0">,</span>
                                           <span class="s1">**finite_diff_options)</span>

        <span class="s1">self._update_grad_impl = update_grad</span>
        <span class="s1">self._update_grad()</span>

        <span class="s4"># Hessian Evaluation</span>
        <span class="s0">if </span><span class="s1">callable(hess):</span>
            <span class="s1">self.H = hess(np.copy(x0)</span><span class="s0">, </span><span class="s1">*args)</span>
            <span class="s1">self.H_updated = </span><span class="s0">True</span>
            <span class="s1">self.nhev += </span><span class="s5">1</span>

            <span class="s0">if </span><span class="s1">sps.issparse(self.H):</span>
                <span class="s0">def </span><span class="s1">hess_wrapped(x):</span>
                    <span class="s1">self.nhev += </span><span class="s5">1</span>
                    <span class="s0">return </span><span class="s1">sps.csr_matrix(hess(np.copy(x)</span><span class="s0">, </span><span class="s1">*args))</span>
                <span class="s1">self.H = sps.csr_matrix(self.H)</span>

            <span class="s0">elif </span><span class="s1">isinstance(self.H</span><span class="s0">, </span><span class="s1">LinearOperator):</span>
                <span class="s0">def </span><span class="s1">hess_wrapped(x):</span>
                    <span class="s1">self.nhev += </span><span class="s5">1</span>
                    <span class="s0">return </span><span class="s1">hess(np.copy(x)</span><span class="s0">, </span><span class="s1">*args)</span>

            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">def </span><span class="s1">hess_wrapped(x):</span>
                    <span class="s1">self.nhev += </span><span class="s5">1</span>
                    <span class="s0">return </span><span class="s1">np.atleast_2d(np.asarray(hess(np.copy(x)</span><span class="s0">, </span><span class="s1">*args)))</span>
                <span class="s1">self.H = np.atleast_2d(np.asarray(self.H))</span>

            <span class="s0">def </span><span class="s1">update_hess():</span>
                <span class="s1">self.H = hess_wrapped(self.x)</span>

        <span class="s0">elif </span><span class="s1">hess </span><span class="s0">in </span><span class="s1">FD_METHODS:</span>
            <span class="s0">def </span><span class="s1">update_hess():</span>
                <span class="s1">self._update_grad()</span>
                <span class="s1">self.H = approx_derivative(grad_wrapped</span><span class="s0">, </span><span class="s1">self.x</span><span class="s0">, </span><span class="s1">f0=self.g</span><span class="s0">,</span>
                                           <span class="s1">**finite_diff_options)</span>
                <span class="s0">return </span><span class="s1">self.H</span>

            <span class="s1">update_hess()</span>
            <span class="s1">self.H_updated = </span><span class="s0">True</span>
        <span class="s0">elif </span><span class="s1">isinstance(hess</span><span class="s0">, </span><span class="s1">HessianUpdateStrategy):</span>
            <span class="s1">self.H = hess</span>
            <span class="s1">self.H.initialize(self.n</span><span class="s0">, </span><span class="s2">'hess'</span><span class="s1">)</span>
            <span class="s1">self.H_updated = </span><span class="s0">True</span>
            <span class="s1">self.x_prev = </span><span class="s0">None</span>
            <span class="s1">self.g_prev = </span><span class="s0">None</span>

            <span class="s0">def </span><span class="s1">update_hess():</span>
                <span class="s1">self._update_grad()</span>
                <span class="s1">self.H.update(self.x - self.x_prev</span><span class="s0">, </span><span class="s1">self.g - self.g_prev)</span>

        <span class="s1">self._update_hess_impl = update_hess</span>

        <span class="s0">if </span><span class="s1">isinstance(hess</span><span class="s0">, </span><span class="s1">HessianUpdateStrategy):</span>
            <span class="s0">def </span><span class="s1">update_x(x):</span>
                <span class="s1">self._update_grad()</span>
                <span class="s1">self.x_prev = self.x</span>
                <span class="s1">self.g_prev = self.g</span>
                <span class="s4"># ensure that self.x is a copy of x. Don't store a reference</span>
                <span class="s4"># otherwise the memoization doesn't work properly.</span>
                <span class="s1">self.x = np.atleast_1d(x).astype(float)</span>
                <span class="s1">self.f_updated = </span><span class="s0">False</span>
                <span class="s1">self.g_updated = </span><span class="s0">False</span>
                <span class="s1">self.H_updated = </span><span class="s0">False</span>
                <span class="s1">self._update_hess()</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">def </span><span class="s1">update_x(x):</span>
                <span class="s4"># ensure that self.x is a copy of x. Don't store a reference</span>
                <span class="s4"># otherwise the memoization doesn't work properly.</span>
                <span class="s1">self.x = np.atleast_1d(x).astype(float)</span>
                <span class="s1">self.f_updated = </span><span class="s0">False</span>
                <span class="s1">self.g_updated = </span><span class="s0">False</span>
                <span class="s1">self.H_updated = </span><span class="s0">False</span>
        <span class="s1">self._update_x_impl = update_x</span>

    <span class="s0">def </span><span class="s1">_update_fun(self):</span>
        <span class="s0">if not </span><span class="s1">self.f_updated:</span>
            <span class="s1">self._update_fun_impl()</span>
            <span class="s1">self.f_updated = </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">_update_grad(self):</span>
        <span class="s0">if not </span><span class="s1">self.g_updated:</span>
            <span class="s1">self._update_grad_impl()</span>
            <span class="s1">self.g_updated = </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">_update_hess(self):</span>
        <span class="s0">if not </span><span class="s1">self.H_updated:</span>
            <span class="s1">self._update_hess_impl()</span>
            <span class="s1">self.H_updated = </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">fun(self</span><span class="s0">, </span><span class="s1">x):</span>
        <span class="s0">if not </span><span class="s1">np.array_equal(x</span><span class="s0">, </span><span class="s1">self.x):</span>
            <span class="s1">self._update_x_impl(x)</span>
        <span class="s1">self._update_fun()</span>
        <span class="s0">return </span><span class="s1">self.f</span>

    <span class="s0">def </span><span class="s1">grad(self</span><span class="s0">, </span><span class="s1">x):</span>
        <span class="s0">if not </span><span class="s1">np.array_equal(x</span><span class="s0">, </span><span class="s1">self.x):</span>
            <span class="s1">self._update_x_impl(x)</span>
        <span class="s1">self._update_grad()</span>
        <span class="s0">return </span><span class="s1">self.g</span>

    <span class="s0">def </span><span class="s1">hess(self</span><span class="s0">, </span><span class="s1">x):</span>
        <span class="s0">if not </span><span class="s1">np.array_equal(x</span><span class="s0">, </span><span class="s1">self.x):</span>
            <span class="s1">self._update_x_impl(x)</span>
        <span class="s1">self._update_hess()</span>
        <span class="s0">return </span><span class="s1">self.H</span>

    <span class="s0">def </span><span class="s1">fun_and_grad(self</span><span class="s0">, </span><span class="s1">x):</span>
        <span class="s0">if not </span><span class="s1">np.array_equal(x</span><span class="s0">, </span><span class="s1">self.x):</span>
            <span class="s1">self._update_x_impl(x)</span>
        <span class="s1">self._update_fun()</span>
        <span class="s1">self._update_grad()</span>
        <span class="s0">return </span><span class="s1">self.f</span><span class="s0">, </span><span class="s1">self.g</span>


<span class="s0">class </span><span class="s1">VectorFunction:</span>
    <span class="s3">&quot;&quot;&quot;Vector function and its derivatives. 
 
    This class defines a vector function F: R^n-&gt;R^m and methods for 
    computing or approximating its first and second derivatives. 
 
    Notes 
    ----- 
    This class implements a memoization logic. There are methods `fun`, 
    `jac`, hess` and corresponding attributes `f`, `J` and `H`. The following 
    things should be considered: 
 
        1. Use only public methods `fun`, `jac` and `hess`. 
        2. After one of the methods is called, the corresponding attribute 
           will be set. However, a subsequent call with a different argument 
           of *any* of the methods may overwrite the attribute. 
    &quot;&quot;&quot;</span>
    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">fun</span><span class="s0">, </span><span class="s1">x0</span><span class="s0">, </span><span class="s1">jac</span><span class="s0">, </span><span class="s1">hess</span><span class="s0">,</span>
                 <span class="s1">finite_diff_rel_step</span><span class="s0">, </span><span class="s1">finite_diff_jac_sparsity</span><span class="s0">,</span>
                 <span class="s1">finite_diff_bounds</span><span class="s0">, </span><span class="s1">sparse_jacobian):</span>
        <span class="s0">if not </span><span class="s1">callable(jac) </span><span class="s0">and </span><span class="s1">jac </span><span class="s0">not in </span><span class="s1">FD_METHODS:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;`jac` must be either callable or one of {}.&quot;</span>
                             <span class="s1">.format(FD_METHODS))</span>

        <span class="s0">if not </span><span class="s1">(callable(hess) </span><span class="s0">or </span><span class="s1">hess </span><span class="s0">in </span><span class="s1">FD_METHODS</span>
                <span class="s0">or </span><span class="s1">isinstance(hess</span><span class="s0">, </span><span class="s1">HessianUpdateStrategy)):</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;`hess` must be either callable,&quot;</span>
                             <span class="s2">&quot;HessianUpdateStrategy or one of {}.&quot;</span>
                             <span class="s1">.format(FD_METHODS))</span>

        <span class="s0">if </span><span class="s1">jac </span><span class="s0">in </span><span class="s1">FD_METHODS </span><span class="s0">and </span><span class="s1">hess </span><span class="s0">in </span><span class="s1">FD_METHODS:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;Whenever the Jacobian is estimated via &quot;</span>
                             <span class="s2">&quot;finite-differences, we require the Hessian to &quot;</span>
                             <span class="s2">&quot;be estimated using one of the quasi-Newton &quot;</span>
                             <span class="s2">&quot;strategies.&quot;</span><span class="s1">)</span>

        <span class="s1">self.x = np.atleast_1d(x0).astype(float)</span>
        <span class="s1">self.n = self.x.size</span>
        <span class="s1">self.nfev = </span><span class="s5">0</span>
        <span class="s1">self.njev = </span><span class="s5">0</span>
        <span class="s1">self.nhev = </span><span class="s5">0</span>
        <span class="s1">self.f_updated = </span><span class="s0">False</span>
        <span class="s1">self.J_updated = </span><span class="s0">False</span>
        <span class="s1">self.H_updated = </span><span class="s0">False</span>

        <span class="s1">finite_diff_options = {}</span>
        <span class="s0">if </span><span class="s1">jac </span><span class="s0">in </span><span class="s1">FD_METHODS:</span>
            <span class="s1">finite_diff_options[</span><span class="s2">&quot;method&quot;</span><span class="s1">] = jac</span>
            <span class="s1">finite_diff_options[</span><span class="s2">&quot;rel_step&quot;</span><span class="s1">] = finite_diff_rel_step</span>
            <span class="s0">if </span><span class="s1">finite_diff_jac_sparsity </span><span class="s0">is not None</span><span class="s1">:</span>
                <span class="s1">sparsity_groups = group_columns(finite_diff_jac_sparsity)</span>
                <span class="s1">finite_diff_options[</span><span class="s2">&quot;sparsity&quot;</span><span class="s1">] = (finite_diff_jac_sparsity</span><span class="s0">,</span>
                                                   <span class="s1">sparsity_groups)</span>
            <span class="s1">finite_diff_options[</span><span class="s2">&quot;bounds&quot;</span><span class="s1">] = finite_diff_bounds</span>
            <span class="s1">self.x_diff = np.copy(self.x)</span>
        <span class="s0">if </span><span class="s1">hess </span><span class="s0">in </span><span class="s1">FD_METHODS:</span>
            <span class="s1">finite_diff_options[</span><span class="s2">&quot;method&quot;</span><span class="s1">] = hess</span>
            <span class="s1">finite_diff_options[</span><span class="s2">&quot;rel_step&quot;</span><span class="s1">] = finite_diff_rel_step</span>
            <span class="s1">finite_diff_options[</span><span class="s2">&quot;as_linear_operator&quot;</span><span class="s1">] = </span><span class="s0">True</span>
            <span class="s1">self.x_diff = np.copy(self.x)</span>
        <span class="s0">if </span><span class="s1">jac </span><span class="s0">in </span><span class="s1">FD_METHODS </span><span class="s0">and </span><span class="s1">hess </span><span class="s0">in </span><span class="s1">FD_METHODS:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;Whenever the Jacobian is estimated via &quot;</span>
                             <span class="s2">&quot;finite-differences, we require the Hessian to &quot;</span>
                             <span class="s2">&quot;be estimated using one of the quasi-Newton &quot;</span>
                             <span class="s2">&quot;strategies.&quot;</span><span class="s1">)</span>

        <span class="s4"># Function evaluation</span>
        <span class="s0">def </span><span class="s1">fun_wrapped(x):</span>
            <span class="s1">self.nfev += </span><span class="s5">1</span>
            <span class="s0">return </span><span class="s1">np.atleast_1d(fun(x))</span>

        <span class="s0">def </span><span class="s1">update_fun():</span>
            <span class="s1">self.f = fun_wrapped(self.x)</span>

        <span class="s1">self._update_fun_impl = update_fun</span>
        <span class="s1">update_fun()</span>

        <span class="s1">self.v = np.zeros_like(self.f)</span>
        <span class="s1">self.m = self.v.size</span>

        <span class="s4"># Jacobian Evaluation</span>
        <span class="s0">if </span><span class="s1">callable(jac):</span>
            <span class="s1">self.J = jac(self.x)</span>
            <span class="s1">self.J_updated = </span><span class="s0">True</span>
            <span class="s1">self.njev += </span><span class="s5">1</span>

            <span class="s0">if </span><span class="s1">(sparse_jacobian </span><span class="s0">or</span>
                    <span class="s1">sparse_jacobian </span><span class="s0">is None and </span><span class="s1">sps.issparse(self.J)):</span>
                <span class="s0">def </span><span class="s1">jac_wrapped(x):</span>
                    <span class="s1">self.njev += </span><span class="s5">1</span>
                    <span class="s0">return </span><span class="s1">sps.csr_matrix(jac(x))</span>
                <span class="s1">self.J = sps.csr_matrix(self.J)</span>
                <span class="s1">self.sparse_jacobian = </span><span class="s0">True</span>

            <span class="s0">elif </span><span class="s1">sps.issparse(self.J):</span>
                <span class="s0">def </span><span class="s1">jac_wrapped(x):</span>
                    <span class="s1">self.njev += </span><span class="s5">1</span>
                    <span class="s0">return </span><span class="s1">jac(x).toarray()</span>
                <span class="s1">self.J = self.J.toarray()</span>
                <span class="s1">self.sparse_jacobian = </span><span class="s0">False</span>

            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">def </span><span class="s1">jac_wrapped(x):</span>
                    <span class="s1">self.njev += </span><span class="s5">1</span>
                    <span class="s0">return </span><span class="s1">np.atleast_2d(jac(x))</span>
                <span class="s1">self.J = np.atleast_2d(self.J)</span>
                <span class="s1">self.sparse_jacobian = </span><span class="s0">False</span>

            <span class="s0">def </span><span class="s1">update_jac():</span>
                <span class="s1">self.J = jac_wrapped(self.x)</span>

        <span class="s0">elif </span><span class="s1">jac </span><span class="s0">in </span><span class="s1">FD_METHODS:</span>
            <span class="s1">self.J = approx_derivative(fun_wrapped</span><span class="s0">, </span><span class="s1">self.x</span><span class="s0">, </span><span class="s1">f0=self.f</span><span class="s0">,</span>
                                       <span class="s1">**finite_diff_options)</span>
            <span class="s1">self.J_updated = </span><span class="s0">True</span>

            <span class="s0">if </span><span class="s1">(sparse_jacobian </span><span class="s0">or</span>
                    <span class="s1">sparse_jacobian </span><span class="s0">is None and </span><span class="s1">sps.issparse(self.J)):</span>
                <span class="s0">def </span><span class="s1">update_jac():</span>
                    <span class="s1">self._update_fun()</span>
                    <span class="s1">self.J = sps.csr_matrix(</span>
                        <span class="s1">approx_derivative(fun_wrapped</span><span class="s0">, </span><span class="s1">self.x</span><span class="s0">, </span><span class="s1">f0=self.f</span><span class="s0">,</span>
                                          <span class="s1">**finite_diff_options))</span>
                <span class="s1">self.J = sps.csr_matrix(self.J)</span>
                <span class="s1">self.sparse_jacobian = </span><span class="s0">True</span>

            <span class="s0">elif </span><span class="s1">sps.issparse(self.J):</span>
                <span class="s0">def </span><span class="s1">update_jac():</span>
                    <span class="s1">self._update_fun()</span>
                    <span class="s1">self.J = approx_derivative(fun_wrapped</span><span class="s0">, </span><span class="s1">self.x</span><span class="s0">, </span><span class="s1">f0=self.f</span><span class="s0">,</span>
                                               <span class="s1">**finite_diff_options).toarray()</span>
                <span class="s1">self.J = self.J.toarray()</span>
                <span class="s1">self.sparse_jacobian = </span><span class="s0">False</span>

            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">def </span><span class="s1">update_jac():</span>
                    <span class="s1">self._update_fun()</span>
                    <span class="s1">self.J = np.atleast_2d(</span>
                        <span class="s1">approx_derivative(fun_wrapped</span><span class="s0">, </span><span class="s1">self.x</span><span class="s0">, </span><span class="s1">f0=self.f</span><span class="s0">,</span>
                                          <span class="s1">**finite_diff_options))</span>
                <span class="s1">self.J = np.atleast_2d(self.J)</span>
                <span class="s1">self.sparse_jacobian = </span><span class="s0">False</span>

        <span class="s1">self._update_jac_impl = update_jac</span>

        <span class="s4"># Define Hessian</span>
        <span class="s0">if </span><span class="s1">callable(hess):</span>
            <span class="s1">self.H = hess(self.x</span><span class="s0">, </span><span class="s1">self.v)</span>
            <span class="s1">self.H_updated = </span><span class="s0">True</span>
            <span class="s1">self.nhev += </span><span class="s5">1</span>

            <span class="s0">if </span><span class="s1">sps.issparse(self.H):</span>
                <span class="s0">def </span><span class="s1">hess_wrapped(x</span><span class="s0">, </span><span class="s1">v):</span>
                    <span class="s1">self.nhev += </span><span class="s5">1</span>
                    <span class="s0">return </span><span class="s1">sps.csr_matrix(hess(x</span><span class="s0">, </span><span class="s1">v))</span>
                <span class="s1">self.H = sps.csr_matrix(self.H)</span>

            <span class="s0">elif </span><span class="s1">isinstance(self.H</span><span class="s0">, </span><span class="s1">LinearOperator):</span>
                <span class="s0">def </span><span class="s1">hess_wrapped(x</span><span class="s0">, </span><span class="s1">v):</span>
                    <span class="s1">self.nhev += </span><span class="s5">1</span>
                    <span class="s0">return </span><span class="s1">hess(x</span><span class="s0">, </span><span class="s1">v)</span>

            <span class="s0">else</span><span class="s1">:</span>
                <span class="s0">def </span><span class="s1">hess_wrapped(x</span><span class="s0">, </span><span class="s1">v):</span>
                    <span class="s1">self.nhev += </span><span class="s5">1</span>
                    <span class="s0">return </span><span class="s1">np.atleast_2d(np.asarray(hess(x</span><span class="s0">, </span><span class="s1">v)))</span>
                <span class="s1">self.H = np.atleast_2d(np.asarray(self.H))</span>

            <span class="s0">def </span><span class="s1">update_hess():</span>
                <span class="s1">self.H = hess_wrapped(self.x</span><span class="s0">, </span><span class="s1">self.v)</span>
        <span class="s0">elif </span><span class="s1">hess </span><span class="s0">in </span><span class="s1">FD_METHODS:</span>
            <span class="s0">def </span><span class="s1">jac_dot_v(x</span><span class="s0">, </span><span class="s1">v):</span>
                <span class="s0">return </span><span class="s1">jac_wrapped(x).T.dot(v)</span>

            <span class="s0">def </span><span class="s1">update_hess():</span>
                <span class="s1">self._update_jac()</span>
                <span class="s1">self.H = approx_derivative(jac_dot_v</span><span class="s0">, </span><span class="s1">self.x</span><span class="s0">,</span>
                                           <span class="s1">f0=self.J.T.dot(self.v)</span><span class="s0">,</span>
                                           <span class="s1">args=(self.v</span><span class="s0">,</span><span class="s1">)</span><span class="s0">,</span>
                                           <span class="s1">**finite_diff_options)</span>
            <span class="s1">update_hess()</span>
            <span class="s1">self.H_updated = </span><span class="s0">True</span>
        <span class="s0">elif </span><span class="s1">isinstance(hess</span><span class="s0">, </span><span class="s1">HessianUpdateStrategy):</span>
            <span class="s1">self.H = hess</span>
            <span class="s1">self.H.initialize(self.n</span><span class="s0">, </span><span class="s2">'hess'</span><span class="s1">)</span>
            <span class="s1">self.H_updated = </span><span class="s0">True</span>
            <span class="s1">self.x_prev = </span><span class="s0">None</span>
            <span class="s1">self.J_prev = </span><span class="s0">None</span>

            <span class="s0">def </span><span class="s1">update_hess():</span>
                <span class="s1">self._update_jac()</span>
                <span class="s4"># When v is updated before x was updated, then x_prev and</span>
                <span class="s4"># J_prev are None and we need this check.</span>
                <span class="s0">if </span><span class="s1">self.x_prev </span><span class="s0">is not None and </span><span class="s1">self.J_prev </span><span class="s0">is not None</span><span class="s1">:</span>
                    <span class="s1">delta_x = self.x - self.x_prev</span>
                    <span class="s1">delta_g = self.J.T.dot(self.v) - self.J_prev.T.dot(self.v)</span>
                    <span class="s1">self.H.update(delta_x</span><span class="s0">, </span><span class="s1">delta_g)</span>

        <span class="s1">self._update_hess_impl = update_hess</span>

        <span class="s0">if </span><span class="s1">isinstance(hess</span><span class="s0">, </span><span class="s1">HessianUpdateStrategy):</span>
            <span class="s0">def </span><span class="s1">update_x(x):</span>
                <span class="s1">self._update_jac()</span>
                <span class="s1">self.x_prev = self.x</span>
                <span class="s1">self.J_prev = self.J</span>
                <span class="s1">self.x = np.atleast_1d(x).astype(float)</span>
                <span class="s1">self.f_updated = </span><span class="s0">False</span>
                <span class="s1">self.J_updated = </span><span class="s0">False</span>
                <span class="s1">self.H_updated = </span><span class="s0">False</span>
                <span class="s1">self._update_hess()</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">def </span><span class="s1">update_x(x):</span>
                <span class="s1">self.x = np.atleast_1d(x).astype(float)</span>
                <span class="s1">self.f_updated = </span><span class="s0">False</span>
                <span class="s1">self.J_updated = </span><span class="s0">False</span>
                <span class="s1">self.H_updated = </span><span class="s0">False</span>

        <span class="s1">self._update_x_impl = update_x</span>

    <span class="s0">def </span><span class="s1">_update_v(self</span><span class="s0">, </span><span class="s1">v):</span>
        <span class="s0">if not </span><span class="s1">np.array_equal(v</span><span class="s0">, </span><span class="s1">self.v):</span>
            <span class="s1">self.v = v</span>
            <span class="s1">self.H_updated = </span><span class="s0">False</span>

    <span class="s0">def </span><span class="s1">_update_x(self</span><span class="s0">, </span><span class="s1">x):</span>
        <span class="s0">if not </span><span class="s1">np.array_equal(x</span><span class="s0">, </span><span class="s1">self.x):</span>
            <span class="s1">self._update_x_impl(x)</span>

    <span class="s0">def </span><span class="s1">_update_fun(self):</span>
        <span class="s0">if not </span><span class="s1">self.f_updated:</span>
            <span class="s1">self._update_fun_impl()</span>
            <span class="s1">self.f_updated = </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">_update_jac(self):</span>
        <span class="s0">if not </span><span class="s1">self.J_updated:</span>
            <span class="s1">self._update_jac_impl()</span>
            <span class="s1">self.J_updated = </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">_update_hess(self):</span>
        <span class="s0">if not </span><span class="s1">self.H_updated:</span>
            <span class="s1">self._update_hess_impl()</span>
            <span class="s1">self.H_updated = </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">fun(self</span><span class="s0">, </span><span class="s1">x):</span>
        <span class="s1">self._update_x(x)</span>
        <span class="s1">self._update_fun()</span>
        <span class="s0">return </span><span class="s1">self.f</span>

    <span class="s0">def </span><span class="s1">jac(self</span><span class="s0">, </span><span class="s1">x):</span>
        <span class="s1">self._update_x(x)</span>
        <span class="s1">self._update_jac()</span>
        <span class="s0">return </span><span class="s1">self.J</span>

    <span class="s0">def </span><span class="s1">hess(self</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">v):</span>
        <span class="s4"># v should be updated before x.</span>
        <span class="s1">self._update_v(v)</span>
        <span class="s1">self._update_x(x)</span>
        <span class="s1">self._update_hess()</span>
        <span class="s0">return </span><span class="s1">self.H</span>


<span class="s0">class </span><span class="s1">LinearVectorFunction:</span>
    <span class="s3">&quot;&quot;&quot;Linear vector function and its derivatives. 
 
    Defines a linear function F = A x, where x is N-D vector and 
    A is m-by-n matrix. The Jacobian is constant and equals to A. The Hessian 
    is identically zero and it is returned as a csr matrix. 
    &quot;&quot;&quot;</span>
    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">A</span><span class="s0">, </span><span class="s1">x0</span><span class="s0">, </span><span class="s1">sparse_jacobian):</span>
        <span class="s0">if </span><span class="s1">sparse_jacobian </span><span class="s0">or </span><span class="s1">sparse_jacobian </span><span class="s0">is None and </span><span class="s1">sps.issparse(A):</span>
            <span class="s1">self.J = sps.csr_matrix(A)</span>
            <span class="s1">self.sparse_jacobian = </span><span class="s0">True</span>
        <span class="s0">elif </span><span class="s1">sps.issparse(A):</span>
            <span class="s1">self.J = A.toarray()</span>
            <span class="s1">self.sparse_jacobian = </span><span class="s0">False</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s4"># np.asarray makes sure A is ndarray and not matrix</span>
            <span class="s1">self.J = np.atleast_2d(np.asarray(A))</span>
            <span class="s1">self.sparse_jacobian = </span><span class="s0">False</span>

        <span class="s1">self.m</span><span class="s0">, </span><span class="s1">self.n = self.J.shape</span>

        <span class="s1">self.x = np.atleast_1d(x0).astype(float)</span>
        <span class="s1">self.f = self.J.dot(self.x)</span>
        <span class="s1">self.f_updated = </span><span class="s0">True</span>

        <span class="s1">self.v = np.zeros(self.m</span><span class="s0">, </span><span class="s1">dtype=float)</span>
        <span class="s1">self.H = sps.csr_matrix((self.n</span><span class="s0">, </span><span class="s1">self.n))</span>

    <span class="s0">def </span><span class="s1">_update_x(self</span><span class="s0">, </span><span class="s1">x):</span>
        <span class="s0">if not </span><span class="s1">np.array_equal(x</span><span class="s0">, </span><span class="s1">self.x):</span>
            <span class="s1">self.x = np.atleast_1d(x).astype(float)</span>
            <span class="s1">self.f_updated = </span><span class="s0">False</span>

    <span class="s0">def </span><span class="s1">fun(self</span><span class="s0">, </span><span class="s1">x):</span>
        <span class="s1">self._update_x(x)</span>
        <span class="s0">if not </span><span class="s1">self.f_updated:</span>
            <span class="s1">self.f = self.J.dot(x)</span>
            <span class="s1">self.f_updated = </span><span class="s0">True</span>
        <span class="s0">return </span><span class="s1">self.f</span>

    <span class="s0">def </span><span class="s1">jac(self</span><span class="s0">, </span><span class="s1">x):</span>
        <span class="s1">self._update_x(x)</span>
        <span class="s0">return </span><span class="s1">self.J</span>

    <span class="s0">def </span><span class="s1">hess(self</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">v):</span>
        <span class="s1">self._update_x(x)</span>
        <span class="s1">self.v = v</span>
        <span class="s0">return </span><span class="s1">self.H</span>


<span class="s0">class </span><span class="s1">IdentityVectorFunction(LinearVectorFunction):</span>
    <span class="s3">&quot;&quot;&quot;Identity vector function and its derivatives. 
 
    The Jacobian is the identity matrix, returned as a dense array when 
    `sparse_jacobian=False` and as a csr matrix otherwise. The Hessian is 
    identically zero and it is returned as a csr matrix. 
    &quot;&quot;&quot;</span>
    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">x0</span><span class="s0">, </span><span class="s1">sparse_jacobian):</span>
        <span class="s1">n = len(x0)</span>
        <span class="s0">if </span><span class="s1">sparse_jacobian </span><span class="s0">or </span><span class="s1">sparse_jacobian </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">A = sps.eye(n</span><span class="s0">, </span><span class="s1">format=</span><span class="s2">'csr'</span><span class="s1">)</span>
            <span class="s1">sparse_jacobian = </span><span class="s0">True</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">A = np.eye(n)</span>
            <span class="s1">sparse_jacobian = </span><span class="s0">False</span>
        <span class="s1">super().__init__(A</span><span class="s0">, </span><span class="s1">x0</span><span class="s0">, </span><span class="s1">sparse_jacobian)</span>
</pre>
</body>
</html>