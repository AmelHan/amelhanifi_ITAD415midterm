<html>
<head>
<title>test_huber.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_huber.py</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: Manoj Kumar mks542@nyu.edu</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">optimize</span><span class="s2">, </span><span class="s1">sparse</span>

<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">make_regression</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">HuberRegressor</span><span class="s2">, </span><span class="s1">LinearRegression</span><span class="s2">, </span><span class="s1">Ridge</span><span class="s2">, </span><span class="s1">SGDRegressor</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model._huber </span><span class="s2">import </span><span class="s1">_huber_loss_and_gradient</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">assert_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_equal</span><span class="s2">,</span>
<span class="s1">)</span>


<span class="s2">def </span><span class="s1">make_regression_with_outliers(n_samples=</span><span class="s3">50</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">20</span><span class="s1">):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s0"># Generate data with outliers by replacing 10% of the samples with noise.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">n_samples=n_samples</span><span class="s2">, </span><span class="s1">n_features=n_features</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">noise=</span><span class="s3">0.05</span>
    <span class="s1">)</span>

    <span class="s0"># Replace 10% of the sample with noise.</span>
    <span class="s1">num_noise = int(</span><span class="s3">0.1 </span><span class="s1">* n_samples)</span>
    <span class="s1">random_samples = rng.randint(</span><span class="s3">0</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">num_noise)</span>
    <span class="s1">X[random_samples</span><span class="s2">, </span><span class="s1">:] = </span><span class="s3">2.0 </span><span class="s1">* rng.normal(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">(num_noise</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s3">1</span><span class="s1">]))</span>
    <span class="s2">return </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span>


<span class="s2">def </span><span class="s1">test_huber_equals_lr_for_high_epsilon():</span>
    <span class="s0"># Test that Ridge matches LinearRegression for large epsilon</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression_with_outliers()</span>
    <span class="s1">lr = LinearRegression()</span>
    <span class="s1">lr.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">huber = HuberRegressor(epsilon=</span><span class="s3">1e3</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s3">0.0</span><span class="s1">)</span>
    <span class="s1">huber.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(huber.coef_</span><span class="s2">, </span><span class="s1">lr.coef_</span><span class="s2">, </span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(huber.intercept_</span><span class="s2">, </span><span class="s1">lr.intercept_</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_huber_max_iter():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression_with_outliers()</span>
    <span class="s1">huber = HuberRegressor(max_iter=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">huber.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">huber.n_iter_ == huber.max_iter</span>


<span class="s2">def </span><span class="s1">test_huber_gradient():</span>
    <span class="s0"># Test that the gradient calculated by _huber_loss_and_gradient is correct</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression_with_outliers()</span>
    <span class="s1">sample_weight = rng.randint(</span><span class="s3">1</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s1">(y.shape[</span><span class="s3">0</span><span class="s1">]))</span>

    <span class="s2">def </span><span class="s1">loss_func(x</span><span class="s2">, </span><span class="s1">*args):</span>
        <span class="s2">return </span><span class="s1">_huber_loss_and_gradient(x</span><span class="s2">, </span><span class="s1">*args)[</span><span class="s3">0</span><span class="s1">]</span>

    <span class="s2">def </span><span class="s1">grad_func(x</span><span class="s2">, </span><span class="s1">*args):</span>
        <span class="s2">return </span><span class="s1">_huber_loss_and_gradient(x</span><span class="s2">, </span><span class="s1">*args)[</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s0"># Check using optimize.check_grad that the gradients are equal.</span>
    <span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(</span><span class="s3">5</span><span class="s1">):</span>
        <span class="s0"># Check for both fit_intercept and otherwise.</span>
        <span class="s2">for </span><span class="s1">n_features </span><span class="s2">in </span><span class="s1">[X.shape[</span><span class="s3">1</span><span class="s1">] + </span><span class="s3">1</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s3">1</span><span class="s1">] + </span><span class="s3">2</span><span class="s1">]:</span>
            <span class="s1">w = rng.randn(n_features)</span>
            <span class="s1">w[-</span><span class="s3">1</span><span class="s1">] = np.abs(w[-</span><span class="s3">1</span><span class="s1">])</span>
            <span class="s1">grad_same = optimize.check_grad(</span>
                <span class="s1">loss_func</span><span class="s2">, </span><span class="s1">grad_func</span><span class="s2">, </span><span class="s1">w</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s3">0.01</span><span class="s2">, </span><span class="s3">0.1</span><span class="s2">, </span><span class="s1">sample_weight</span>
            <span class="s1">)</span>
            <span class="s1">assert_almost_equal(grad_same</span><span class="s2">, </span><span class="s3">1e-6</span><span class="s2">, </span><span class="s3">4</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_huber_sample_weights():</span>
    <span class="s0"># Test sample_weights implementation in HuberRegressor&quot;&quot;&quot;</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression_with_outliers()</span>
    <span class="s1">huber = HuberRegressor()</span>
    <span class="s1">huber.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">huber_coef = huber.coef_</span>
    <span class="s1">huber_intercept = huber.intercept_</span>

    <span class="s0"># Rescale coefs before comparing with assert_array_almost_equal to make</span>
    <span class="s0"># sure that the number of decimal places used is somewhat insensitive to</span>
    <span class="s0"># the amplitude of the coefficients and therefore to the scale of the</span>
    <span class="s0"># data and the regularization parameter</span>
    <span class="s1">scale = max(np.mean(np.abs(huber.coef_))</span><span class="s2">, </span><span class="s1">np.mean(np.abs(huber.intercept_)))</span>

    <span class="s1">huber.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=np.ones(y.shape[</span><span class="s3">0</span><span class="s1">]))</span>
    <span class="s1">assert_array_almost_equal(huber.coef_ / scale</span><span class="s2">, </span><span class="s1">huber_coef / scale)</span>
    <span class="s1">assert_array_almost_equal(huber.intercept_ / scale</span><span class="s2">, </span><span class="s1">huber_intercept / scale)</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression_with_outliers(n_samples=</span><span class="s3">5</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">20</span><span class="s1">)</span>
    <span class="s1">X_new = np.vstack((X</span><span class="s2">, </span><span class="s1">np.vstack((X[</span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X[</span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X[</span><span class="s3">3</span><span class="s1">]))))</span>
    <span class="s1">y_new = np.concatenate((y</span><span class="s2">, </span><span class="s1">[y[</span><span class="s3">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">[y[</span><span class="s3">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">[y[</span><span class="s3">3</span><span class="s1">]]))</span>
    <span class="s1">huber.fit(X_new</span><span class="s2">, </span><span class="s1">y_new)</span>
    <span class="s1">huber_coef = huber.coef_</span>
    <span class="s1">huber_intercept = huber.intercept_</span>
    <span class="s1">sample_weight = np.ones(X.shape[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">sample_weight[</span><span class="s3">1</span><span class="s1">] = </span><span class="s3">3</span>
    <span class="s1">sample_weight[</span><span class="s3">3</span><span class="s1">] = </span><span class="s3">2</span>
    <span class="s1">huber.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">assert_array_almost_equal(huber.coef_ / scale</span><span class="s2">, </span><span class="s1">huber_coef / scale)</span>
    <span class="s1">assert_array_almost_equal(huber.intercept_ / scale</span><span class="s2">, </span><span class="s1">huber_intercept / scale)</span>

    <span class="s0"># Test sparse implementation with sample weights.</span>
    <span class="s1">X_csr = sparse.csr_matrix(X)</span>
    <span class="s1">huber_sparse = HuberRegressor()</span>
    <span class="s1">huber_sparse.fit(X_csr</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">assert_array_almost_equal(huber_sparse.coef_ / scale</span><span class="s2">, </span><span class="s1">huber_coef / scale)</span>


<span class="s2">def </span><span class="s1">test_huber_sparse():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression_with_outliers()</span>
    <span class="s1">huber = HuberRegressor(alpha=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">huber.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">X_csr = sparse.csr_matrix(X)</span>
    <span class="s1">huber_sparse = HuberRegressor(alpha=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">huber_sparse.fit(X_csr</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(huber_sparse.coef_</span><span class="s2">, </span><span class="s1">huber.coef_)</span>
    <span class="s1">assert_array_equal(huber.outliers_</span><span class="s2">, </span><span class="s1">huber_sparse.outliers_)</span>


<span class="s2">def </span><span class="s1">test_huber_scaling_invariant():</span>
    <span class="s0"># Test that outliers filtering is scaling independent.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression_with_outliers()</span>
    <span class="s1">huber = HuberRegressor(fit_intercept=</span><span class="s2">False, </span><span class="s1">alpha=</span><span class="s3">0.0</span><span class="s1">)</span>
    <span class="s1">huber.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">n_outliers_mask_1 = huber.outliers_</span>
    <span class="s2">assert not </span><span class="s1">np.all(n_outliers_mask_1)</span>

    <span class="s1">huber.fit(X</span><span class="s2">, </span><span class="s3">2.0 </span><span class="s1">* y)</span>
    <span class="s1">n_outliers_mask_2 = huber.outliers_</span>
    <span class="s1">assert_array_equal(n_outliers_mask_2</span><span class="s2">, </span><span class="s1">n_outliers_mask_1)</span>

    <span class="s1">huber.fit(</span><span class="s3">2.0 </span><span class="s1">* X</span><span class="s2">, </span><span class="s3">2.0 </span><span class="s1">* y)</span>
    <span class="s1">n_outliers_mask_3 = huber.outliers_</span>
    <span class="s1">assert_array_equal(n_outliers_mask_3</span><span class="s2">, </span><span class="s1">n_outliers_mask_1)</span>


<span class="s2">def </span><span class="s1">test_huber_and_sgd_same_results():</span>
    <span class="s0"># Test they should converge to same coefficients for same parameters</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression_with_outliers(n_samples=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">2</span><span class="s1">)</span>

    <span class="s0"># Fit once to find out the scale parameter. Scale down X and y by scale</span>
    <span class="s0"># so that the scale parameter is optimized to 1.0</span>
    <span class="s1">huber = HuberRegressor(fit_intercept=</span><span class="s2">False, </span><span class="s1">alpha=</span><span class="s3">0.0</span><span class="s2">, </span><span class="s1">epsilon=</span><span class="s3">1.35</span><span class="s1">)</span>
    <span class="s1">huber.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">X_scale = X / huber.scale_</span>
    <span class="s1">y_scale = y / huber.scale_</span>
    <span class="s1">huber.fit(X_scale</span><span class="s2">, </span><span class="s1">y_scale)</span>
    <span class="s1">assert_almost_equal(huber.scale_</span><span class="s2">, </span><span class="s3">1.0</span><span class="s2">, </span><span class="s3">3</span><span class="s1">)</span>

    <span class="s1">sgdreg = SGDRegressor(</span>
        <span class="s1">alpha=</span><span class="s3">0.0</span><span class="s2">,</span>
        <span class="s1">loss=</span><span class="s4">&quot;huber&quot;</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">True,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s2">,</span>
        <span class="s1">max_iter=</span><span class="s3">10000</span><span class="s2">,</span>
        <span class="s1">fit_intercept=</span><span class="s2">False,</span>
        <span class="s1">epsilon=</span><span class="s3">1.35</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s2">None,</span>
    <span class="s1">)</span>
    <span class="s1">sgdreg.fit(X_scale</span><span class="s2">, </span><span class="s1">y_scale)</span>
    <span class="s1">assert_array_almost_equal(huber.coef_</span><span class="s2">, </span><span class="s1">sgdreg.coef_</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_huber_warm_start():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression_with_outliers()</span>
    <span class="s1">huber_warm = HuberRegressor(alpha=</span><span class="s3">1.0</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s3">10000</span><span class="s2">, </span><span class="s1">warm_start=</span><span class="s2">True, </span><span class="s1">tol=</span><span class="s3">1e-1</span><span class="s1">)</span>

    <span class="s1">huber_warm.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">huber_warm_coef = huber_warm.coef_.copy()</span>
    <span class="s1">huber_warm.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s0"># SciPy performs the tol check after doing the coef updates, so</span>
    <span class="s0"># these would be almost same but not equal.</span>
    <span class="s1">assert_array_almost_equal(huber_warm.coef_</span><span class="s2">, </span><span class="s1">huber_warm_coef</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>

    <span class="s2">assert </span><span class="s1">huber_warm.n_iter_ == </span><span class="s3">0</span>


<span class="s2">def </span><span class="s1">test_huber_better_r2_score():</span>
    <span class="s0"># Test that huber returns a better r2 score than non-outliers&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression_with_outliers()</span>
    <span class="s1">huber = HuberRegressor(alpha=</span><span class="s3">0.01</span><span class="s1">)</span>
    <span class="s1">huber.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">linear_loss = np.dot(X</span><span class="s2">, </span><span class="s1">huber.coef_) + huber.intercept_ - y</span>
    <span class="s1">mask = np.abs(linear_loss) &lt; huber.epsilon * huber.scale_</span>
    <span class="s1">huber_score = huber.score(X[mask]</span><span class="s2">, </span><span class="s1">y[mask])</span>
    <span class="s1">huber_outlier_score = huber.score(X[~mask]</span><span class="s2">, </span><span class="s1">y[~mask])</span>

    <span class="s0"># The Ridge regressor should be influenced by the outliers and hence</span>
    <span class="s0"># give a worse score on the non-outliers as compared to the huber</span>
    <span class="s0"># regressor.</span>
    <span class="s1">ridge = Ridge(alpha=</span><span class="s3">0.01</span><span class="s1">)</span>
    <span class="s1">ridge.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">ridge_score = ridge.score(X[mask]</span><span class="s2">, </span><span class="s1">y[mask])</span>
    <span class="s1">ridge_outlier_score = ridge.score(X[~mask]</span><span class="s2">, </span><span class="s1">y[~mask])</span>
    <span class="s2">assert </span><span class="s1">huber_score &gt; ridge_score</span>

    <span class="s0"># The huber model should also fit poorly on the outliers.</span>
    <span class="s2">assert </span><span class="s1">ridge_outlier_score &gt; huber_outlier_score</span>


<span class="s2">def </span><span class="s1">test_huber_bool():</span>
    <span class="s0"># Test that it does not crash with bool data</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(n_samples=</span><span class="s3">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">noise=</span><span class="s3">4.0</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X_bool = X &gt; </span><span class="s3">0</span>
    <span class="s1">HuberRegressor().fit(X_bool</span><span class="s2">, </span><span class="s1">y)</span>
</pre>
</body>
</html>