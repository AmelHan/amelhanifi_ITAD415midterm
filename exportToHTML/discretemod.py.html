<html>
<head>
<title>discretemod.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #cc7832;}
.s4 { color: #808080;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
discretemod.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Limited dependent variable and qualitative variables. 
 
Includes binary outcomes, count data, (ordered) ordinal data and limited  
dependent variables. 
 
General References 
-------------------- 
 
A.C. Cameron and P.K. Trivedi.  `Regression Analysis of Count Data`.  Cambridge, 
    1998 
 
G.S. Madalla. `Limited-Dependent and Qualitative Variables in Econometrics`.  
    Cambridge, 1983. 
 
W. Greene. `Econometric Analysis`. Prentice Hall, 5th. edition. 2003.  
&quot;&quot;&quot;</span>

<span class="s1">__all__ = [</span><span class="s2">&quot;Poisson&quot;</span><span class="s3">,</span><span class="s2">&quot;Logit&quot;</span><span class="s3">,</span><span class="s2">&quot;Probit&quot;</span><span class="s3">,</span><span class="s2">&quot;MNLogit&quot;</span><span class="s1">]</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">model </span><span class="s3">import </span><span class="s1">LikelihoodModel</span><span class="s3">, </span><span class="s1">LikelihoodModelResults</span>
<span class="s3">import </span><span class="s1">tools</span>
<span class="s3">from </span><span class="s1">decorators </span><span class="s3">import </span><span class="s1">*</span>
<span class="s3">from </span><span class="s1">regression </span><span class="s3">import </span><span class="s1">OLS</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">stats</span><span class="s3">, </span><span class="s1">factorial</span><span class="s3">, </span><span class="s1">special</span><span class="s3">, </span><span class="s1">optimize </span><span class="s4"># opt just for nbin</span>
<span class="s4">#import numdifftools as nd #This will be removed when all have analytic hessians</span>

<span class="s4">#TODO: add options for the parameter covariance/variance</span>
<span class="s4"># ie., OIM, EIM, and BHHH see Green 21.4</span>

<span class="s3">def </span><span class="s1">_isdummy(X):</span>
    <span class="s0">&quot;&quot;&quot; 
    Given an array X, returns a boolean column index for the dummy variables. 
 
    Parameters 
    ---------- 
    X : array-like 
        A 1d or 2d array of numbers 
 
    Examples 
    -------- 
    &gt;&gt;&gt; X = np.random.randint(0, 2, size=(15,5)).astype(float) 
    &gt;&gt;&gt; X[:,1:3] = np.random.randn(15,2) 
    &gt;&gt;&gt; ind = _isdummy(X) 
    &gt;&gt;&gt; ind 
    array([ True, False, False,  True,  True], dtype=bool) 
    &quot;&quot;&quot;</span>
    <span class="s1">X = np.asarray(X)</span>
    <span class="s3">if </span><span class="s1">X.ndim &gt; </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">ind = np.zeros(X.shape[</span><span class="s5">1</span><span class="s1">]).astype(bool)</span>
    <span class="s1">max = (np.max(X</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">) == </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">min = (np.min(X</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">) == </span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">remainder = np.all(X % </span><span class="s5">1. </span><span class="s1">== </span><span class="s5">0</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">ind = min &amp; max &amp; remainder</span>
    <span class="s3">if </span><span class="s1">X.ndim == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">ind = np.asarray([ind])</span>
    <span class="s3">return </span><span class="s1">ind</span>

<span class="s3">def </span><span class="s1">_iscount(X):</span>
    <span class="s0">&quot;&quot;&quot; 
    Given an array X, returns a boolean column index for count variables. 
 
    Parameters 
    ---------- 
    X : array-like 
        A 1d or 2d array of numbers 
 
    Examples 
    -------- 
    &gt;&gt;&gt; X = np.random.randint(0, 10, size=(15,5)).astype(float) 
    &gt;&gt;&gt; X[:,1:3] = np.random.randn(15,2) 
    &gt;&gt;&gt; ind = _iscount(X) 
    &gt;&gt;&gt; ind 
    array([ True, False, False,  True,  True], dtype=bool) 
    &quot;&quot;&quot;</span>
    <span class="s1">X = np.asarray(X)</span>
    <span class="s1">remainder = np.all(X % </span><span class="s5">1. </span><span class="s1">== </span><span class="s5">0</span><span class="s3">, </span><span class="s1">axis = </span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">dummy = _isdummy(X)</span>
    <span class="s1">remainder -= dummy</span>
    <span class="s3">return </span><span class="s1">remainder</span>

<span class="s3">class </span><span class="s1">DiscreteModel(LikelihoodModel):</span>
    <span class="s0">&quot;&quot;&quot; 
    Abstract class for discrete choice models. 
 
    This class does not do anything itself but lays out the methods and  
    call signature expected of child classes in addition to those of 
    scikits.statsmodels.model.LikelihoodModel. 
    &quot;&quot;&quot;</span>
    <span class="s3">def </span><span class="s1">__init___(endog</span><span class="s3">, </span><span class="s1">exog):</span>
        <span class="s1">super(DiscreteModel</span><span class="s3">, </span><span class="s1">self).__init__(endog</span><span class="s3">, </span><span class="s1">exog)</span>

    <span class="s3">def </span><span class="s1">initialize(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Initialize is called by  
        scikits.statsmodels.model.LikelihoodModel.__init__ 
        and should contain any preprocessing that needs to be done for a model. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.df_model = float(tools.rank(self.exog) - </span><span class="s5">1</span><span class="s1">) </span><span class="s4"># assumes constant</span>
        <span class="s1">self.df_resid = float(self.exog.shape[</span><span class="s5">0</span><span class="s1">] - tools.rank(self.exog))</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        The cumulative distribution function of the model. 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        The probability density (mass) function of the model. 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>


<span class="s3">class </span><span class="s1">Poisson(DiscreteModel):</span>
    <span class="s0">&quot;&quot;&quot; 
    Poisson model for count data 
 
    Parameters 
    ---------- 
    endog : array-like 
        1-d array of the response variable. 
    exog : array-like 
        `exog` is an n x p array where n is the number of observations and p 
        is the number of regressors including the intercept if one is included 
        in the data. 
     
    Attributes 
    ----------- 
    endog : array 
        A reference to the endogenous response variable 
    exog : array 
        A reference to the exogenous design. 
    nobs : float 
        The number of observations of the model. 
 
    Methods 
    ------- 
    cdf 
    fit 
    hessian 
    information 
    initialize 
    loglike 
    pdf 
    predict 
    score 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        Poisson model cumulative distribution function 
 
        Parameters 
        ----------- 
        X : array-like 
            `X` is the linear predictor of the model.  See notes. 
 
        Returns 
        ------- 
        The value of the Poisson CDF at each point. 
 
        Notes 
        ----- 
        The CDF is defined as 
 
        .. math:: \\exp\left(-\\lambda\\right)\\sum_{i=0}^{y}\\frac{\\lambda^{i}}{i!} 
 
        where :math:`\\lambda` assumes the loglinear model. I.e., 
 
        .. math:: \\ln\\lambda_{i}=X\\beta 
 
        The parameter `X` is :math:`X\\beta` in the above formula. 
        &quot;&quot;&quot;</span>
        <span class="s1">y = self.endog</span>
<span class="s4">#        xb = np.dot(self.exog, params)</span>
        <span class="s3">return </span><span class="s1">stats.poisson.cdf(y</span><span class="s3">, </span><span class="s1">np.exp(X))</span>
    
    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        Poisson model probability mass function 
 
        Parameters 
        ----------- 
        X : array-like 
            `X` is the linear predictor of the model.  See notes. 
 
        Returns 
        ------- 
        The value of the Poisson PMF at each point. 
 
        Notes 
        -------- 
        The PMF is defined as 
 
        .. math:: \\frac{e^{-\\lambda_{i}}\\lambda_{i}^{y_{i}}}{y_{i}!} 
 
        where :math:`\\lambda` assumes the loglinear model. I.e., 
 
        .. math:: \\ln\\lambda_{i}=X\\beta 
 
        The parameter `X` is :math:`X\\beta` in the above formula. 
        &quot;&quot;&quot;</span>
        <span class="s1">y = self.endog</span>
<span class="s4">#        xb = np.dot(self.exog,params)</span>
        <span class="s3">return </span><span class="s1">stats.poisson.pmf(y</span><span class="s3">, </span><span class="s1">np.exp(X))</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Loglikelihood of Poisson model 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model. 
 
        Returns 
        ------- 
        The log likelihood of the model evaluated at `params` 
 
        Notes 
        -------- 
        .. math :: \\ln L=\\sum_{i=1}^{n}\\left[-\\lambda_{i}+y_{i}x_{i}^{\\prime}\\beta-\\ln y_{i}!\\right] 
        &quot;&quot;&quot;</span>
        <span class="s1">XB = np.dot(self.exog</span><span class="s3">, </span><span class="s1">params)</span>
        <span class="s1">endog = self.endog</span>
        <span class="s3">return </span><span class="s1">np.sum(-np.exp(XB) +  endog*XB - np.log(factorial(endog)))</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Poisson model score (gradient) vector of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        The score vector of the model evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L}{\\partial\\beta}=\\sum_{i=1}^{n}\\left(y_{i}-\\lambda_{i}\\right)x_{i} 
 
        where the loglinear model is assumed 
 
        .. math:: \\ln\\lambda_{i}=X\\beta 
        &quot;&quot;&quot;</span>

        <span class="s1">X = self.exog</span>
        <span class="s1">L = np.exp(np.dot(X</span><span class="s3">,</span><span class="s1">params))</span>
        <span class="s3">return </span><span class="s1">np.dot(self.endog - L</span><span class="s3">,</span><span class="s1">X)</span>

    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Poisson model Hessian matrix of the loglikelihood 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        The Hessian matrix evaluated at params 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial^{2}\\ln L}{\\partial\\beta\\partial\\beta^{\\prime}}=-\\sum_{i=1}^{n}\\lambda_{i}x_{i}x_{i}^{\\prime} 
 
        where the loglinear model is assumed 
 
        .. math:: \\ln\\lambda_{i}=X\\beta 
 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">L = np.exp(np.dot(X</span><span class="s3">,</span><span class="s1">params))</span>
        <span class="s3">return </span><span class="s1">-np.dot(L*X.T</span><span class="s3">, </span><span class="s1">X)</span>
    
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">maxiter=</span><span class="s5">35</span><span class="s3">, </span><span class="s1">method=</span><span class="s2">'newton'</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s5">1e-08</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fits the Poisson model. 
 
        Parameters 
        ---------- 
        start_params : array-like, optional 
            The default is a 0 vector. 
        maxiter : int, optional 
            Maximum number of iterations.  The default is 35. 
        method : str, optional 
            `method` can be 'newton', 'ncg', 'bfgs'. The default is 'newton'. 
        tol : float, optional 
            The convergence tolerance for the solver.  The default is  
            1e-08. 
 
        Returns 
        -------- 
        DiscreteResults object 
 
        See also 
        -------- 
        scikits.statsmodels.model.LikelihoodModel 
        scikits.statsmodels.sandbox.discretemod.DiscreteResults 
        scipy.optimize 
        &quot;&quot;&quot;</span>

        <span class="s1">mlefit = super(Poisson</span><span class="s3">, </span><span class="s1">self).fit(start_params=start_params</span><span class="s3">,</span>
            <span class="s1">maxiter=maxiter</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">tol=tol)</span>
        <span class="s1">params = mlefit.params</span>
        <span class="s1">mlefit = DiscreteResults(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">self.hessian(params))</span>
        <span class="s3">return </span><span class="s1">mlefit</span>

<span class="s3">class </span><span class="s1">NbReg(DiscreteModel):</span>
    <span class="s3">pass</span>

<span class="s3">class </span><span class="s1">Logit(DiscreteModel):</span>
    <span class="s0">&quot;&quot;&quot; 
    Binary choice logit model 
     
    Parameters 
    ---------- 
    endog : array-like 
        1-d array of the response variable. 
    exog : array-like 
        `exog` is an n x p array where n is the number of observations and p 
        is the number of regressors including the intercept if one is included 
        in the data. 
     
    Attributes 
    ----------- 
    endog : array 
        A reference to the endogenous response variable 
    exog : array 
        A reference to the exogenous design. 
    nobs : float 
        The number of observations of the model. 
 
    Methods 
    -------- 
    cdf 
    fit 
    hessian 
    information 
    initialize 
    loglike 
    pdf 
    predict 
    score 
    &quot;&quot;&quot;</span>
    
    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        The logistic cumulative distribution function 
     
        Parameters 
        ---------- 
        X : array-like 
            `X` is the linear predictor of the logit model.  See notes. 
         
        Returns 
        ------- 
        1/(1 + exp(-X)) 
 
        Notes 
        ------ 
        In the logit model,  
 
        .. math:: \\Lambda\\left(x^{\\prime}\\beta\\right)=\\text{Prob}\\left(Y=1|x\\right)=\\frac{e^{x^{\\prime}\\beta}}{1+e^{x^{\\prime}\\beta}} 
        &quot;&quot;&quot;</span>
        <span class="s1">X = np.asarray(X)</span>
        <span class="s3">return </span><span class="s5">1</span><span class="s1">/(</span><span class="s5">1</span><span class="s1">+np.exp(-X))</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        The logistic probability density function 
 
        Parameters  
        ----------- 
        X : array-like 
            `X` is the linear predictor of the logit model.  See notes. 
 
        Returns 
        ------- 
        np.exp(-x)/(1+np.exp(-X))**2 
 
        Notes 
        ----- 
        In the logit model, 
 
        .. math:: \\lambda\\left(x^{\\prime}\\beta\\right)=\\frac{e^{-x^{\\prime}\\beta}}{\\left(1+e^{-x^{\\prime}\\beta}\\right)^{2}} 
        &quot;&quot;&quot;</span>
        <span class="s1">X = np.asarray(X)</span>
        <span class="s3">return </span><span class="s1">np.exp(-X)/(</span><span class="s5">1</span><span class="s1">+np.exp(-X))**</span><span class="s5">2</span>
    
    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Log-likelihood of logit model. 
 
        Parameters 
        ----------- 
        params : array-like 
            The parameters of the logit model. 
 
        Returns 
        ------- 
        The log-likelihood function of the logit model.  See notes. 
 
        Notes 
        ------ 
        .. math:: \\ln L=\\sum_{i}\\ln\\Lambda\\left(q_{i}x_{i}^{\\prime}\\beta\\right) 
 
        Where :math:`q=2y-1`. This simplification comes from the fact that the  
        logistic distribution is symmetric. 
        &quot;&quot;&quot;</span>
        <span class="s1">q = </span><span class="s5">2</span><span class="s1">*self.endog - </span><span class="s5">1</span>
        <span class="s1">X = self.exog</span>
        <span class="s3">return </span><span class="s1">np.sum(np.log(self.cdf(q*np.dot(X</span><span class="s3">,</span><span class="s1">params))))</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Logit model score (gradient) vector of the log-likelihood 
 
        Parameters 
        ---------- 
        params: array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        The score vector of the model evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L}{\\partial\\beta}=\\sum_{i=1}^{n}\\left(y_{i}-\\Lambda_{i}\\right)x_{i} 
        &quot;&quot;&quot;</span>

        <span class="s1">y = self.endog</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">L = self.cdf(np.dot(X</span><span class="s3">,</span><span class="s1">params))</span>
        <span class="s3">return </span><span class="s1">np.dot(y - L</span><span class="s3">,</span><span class="s1">X)</span>

    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Logit model Hessian matrix of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        The Hessian evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial^{2}\\ln L}{\\partial\\beta\\partial\\beta^{\\prime}}=-\\sum_{i}\\Lambda_{i}\\left(1-\\Lambda_{i}\\right)x_{i}x_{i}^{\\prime} 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">L = self.cdf(np.dot(X</span><span class="s3">,</span><span class="s1">params))</span>
        <span class="s3">return </span><span class="s1">-np.dot(L*(</span><span class="s5">1</span><span class="s1">-L)*X.T</span><span class="s3">,</span><span class="s1">X)</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">maxiter=</span><span class="s5">35</span><span class="s3">, </span><span class="s1">method=</span><span class="s2">'newton'</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s5">1e-08</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fits the binary logit model. 
 
        Parameters 
        ---------- 
        start_params : array-like, optional 
            The default is a 0 vector. 
        maxiter : int, optional 
            Maximum number of iterations.  The default is 35. 
        method : str, optional 
            `method` can be 'newton', 'ncg', 'bfgs'. The default is 'newton'. 
        tol : float, optional 
            The convergence tolerance for the solver.  The default is  
            1e-08. 
 
        Returns 
        -------- 
        DiscreteResults object 
 
        See also 
        -------- 
        scikits.statsmodels.model.LikelihoodModel 
        scikits.statsmodels.sandbox.discretemod.DiscreteResults 
        scipy.optimize 
        &quot;&quot;&quot;</span>
        <span class="s1">mlefit = super(Logit</span><span class="s3">, </span><span class="s1">self).fit(start_params=start_params</span><span class="s3">,</span>
            <span class="s1">maxiter=maxiter</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">tol=tol)</span>
        <span class="s1">params = mlefit.params</span>
        <span class="s1">mlefit = DiscreteResults(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">self.hessian(params))</span>
        <span class="s3">return </span><span class="s1">mlefit</span>


<span class="s3">class </span><span class="s1">Probit(DiscreteModel):</span>
    <span class="s0">&quot;&quot;&quot; 
    Binary choice Probit model 
 
    Parameters 
    ---------- 
    endog : array-like 
        1-d array of the response variable. 
    exog : array-like 
        `exog` is an n x p array where n is the number of observations and p 
        is the number of regressors including the intercept if one is included 
        in the data. 
     
    Attributes 
    ----------- 
    endog : array 
        A reference to the endogenous response variable 
    exog : array 
        A reference to the exogenous design. 
    nobs : float 
        The number of observations of the model. 
 
    Methods 
    -------- 
    cdf 
    fit 
    hessian 
    information 
    initialize 
    loglike 
    pdf 
    predict 
    score 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        Probit (Normal) cumulative distribution function 
 
        Parameters 
        ---------- 
        X : array-like 
            The linear predictor of the model (XB). 
 
        Returns 
        -------- 
        The cdf evaluated at `X`. 
 
        Notes 
        ----- 
        This function is just an alias for scipy.stats.norm.cdf 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">stats.norm.cdf(X)</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        Probit (Normal) probability density function 
 
        Parameters 
        ---------- 
        X : array-like 
            The linear predictor of the model (XB). 
 
        Returns 
        -------- 
        The pdf evaluated at X. 
 
        Notes 
        ----- 
        This function is just an alias for scipy.stats.norm.pdf 
 
        &quot;&quot;&quot;</span>
        <span class="s1">X = np.asarray(X)</span>
        <span class="s3">return </span><span class="s1">stats.norm.pdf(X)</span>


    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Log-likelihood of probit model (i.e., the normal distribution). 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model. 
 
        Returns 
        ------- 
        The log-likelihood evaluated at params 
 
        Notes 
        ----- 
        .. math:: \\ln L=\\sum_{i}\\ln\\Phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right) 
 
        Where :math:`q=2y-1`. This simplification comes from the fact that the  
        normal distribution is symmetric. 
        &quot;&quot;&quot;</span>

        <span class="s1">q = </span><span class="s5">2</span><span class="s1">*self.endog - </span><span class="s5">1</span>
        <span class="s1">X = self.exog</span>
        <span class="s3">return </span><span class="s1">np.sum(np.log(self.cdf(q*np.dot(X</span><span class="s3">,</span><span class="s1">params))))</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Probit model score (gradient) vector  
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        The score vector of the model evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L}{\\partial\\beta}=\\sum_{i=1}^{n}\\left[\\frac{q_{i}\\phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right)}{\\Phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right)}\\right]x_{i} 
 
        Where :math:`q=2y-1`. This simplification comes from the fact that the  
        normal distribution is symmetric. 
        &quot;&quot;&quot;</span>
        <span class="s1">y = self.endog</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">XB = np.dot(X</span><span class="s3">,</span><span class="s1">params)</span>
        <span class="s1">q = </span><span class="s5">2</span><span class="s1">*y - </span><span class="s5">1</span>
        <span class="s1">L = q*self.pdf(q*XB)/self.cdf(q*XB)</span>
        <span class="s3">return </span><span class="s1">np.dot(L</span><span class="s3">,</span><span class="s1">X)</span>

    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Probit model Hessian matrix of the log-likelihood 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        The Hessian evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial^{2}\\ln L}{\\partial\\beta\\partial\\beta^{\\prime}}=-\lambda_{i}\\left(\\lambda_{i}+x_{i}^{\\prime}\\beta\\right)x_{i}x_{i}^{\\prime} 
        where  
        .. math:: \\lambda_{i}=\\frac{q_{i}\\phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right)}{\\Phi\\left(q_{i}x_{i}^{\\prime}\\beta\\right)} 
        and :math:`q=2y-1` 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">XB = np.dot(X</span><span class="s3">,</span><span class="s1">params)</span>
        <span class="s1">q = </span><span class="s5">2</span><span class="s1">*self.endog - </span><span class="s5">1</span>
        <span class="s1">L = q*self.pdf(q*XB)/self.cdf(q*XB)</span>
        <span class="s3">return </span><span class="s1">np.dot(-L*(L+XB)*X.T</span><span class="s3">,</span><span class="s1">X)</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">maxiter=</span><span class="s5">35</span><span class="s3">, </span><span class="s1">method=</span><span class="s2">'newton'</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s5">1e-08</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fits the binary probit model. 
 
        Parameters 
        ---------- 
        start_params : array-like, optional 
            The default is a 0 vector. 
        maxiter : int, optional 
            Maximum number of iterations.  The default is 35. 
        method : str, optional 
            `method` can be 'newton', 'ncg', 'bfgs'. The default is 'newton'. 
        tol : float, optional 
            The convergence tolerance for the solver.  The default is  
            1e-08. 
 
        Returns 
        -------- 
        DiscreteResults object 
 
        See also 
        -------- 
        scikits.statsmodels.model.LikelihoodModel 
        scikits.statsmodels.sandbox.discretemod.DiscreteResults 
        scipy.optimize 
        &quot;&quot;&quot;</span>
        <span class="s1">mlefit = super(Probit</span><span class="s3">, </span><span class="s1">self).fit(start_params=start_params</span><span class="s3">,</span>
            <span class="s1">maxiter=maxiter</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">tol=tol)</span>
        <span class="s1">params = mlefit.params</span>
        <span class="s1">mlefit = DiscreteResults(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">self.hessian(params))</span>
        <span class="s3">return </span><span class="s1">mlefit</span>
            

<span class="s3">class </span><span class="s1">MNLogit(DiscreteModel):</span>
    <span class="s0">&quot;&quot;&quot; 
    Multinomial logit model 
 
    Parameters 
    ---------- 
    endog : array-like 
        `endog` is an 1-d vector of the endogenous response.  `endog` can 
        contain strings, ints, or floats.  Note that if it contains strings, 
        every distinct string will be a category.  No stripping of whitespace 
        is done. 
    exog : array-like  
        `exog` is an n x p array where n is the number of observations and p 
        is the number of regressors including the intercept if one is included 
        in the data. 
     
    Attributes 
    ---------- 
    J : float 
        The number of choices for the endogenous variable. Note that this 
        is zero-indexed. 
    K : float 
        The actual number of parameters for the exogenous design.  Includes  
        the constant if the design has one. 
    endog : array 
        A reference to the endogenous response variable 
    exog : array 
        A reference to the exogenous design. 
    names : dict 
        A dictionary mapping the column number in `wendog` to the variables  
        in `endog`. 
    nobs : float 
        The number of observations of the model. 
    wendog : array 
        An n x j array where j is the number of unique categories in `endog`. 
        Each column of j is a dummy variable indicating the category of 
        each observation. See `names` for a dictionary mapping each column to 
        its category. 
 
    Methods 
    -------- 
    cdf 
    fit 
    hessian 
    information 
    initialize 
    loglike 
    pdf 
    predict 
    score 
 
    Notes 
    ----- 
    See developer notes for further information on `MNLogit` internals. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">initialize(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Preprocesses the data for MNLogit. 
 
        Turns the endogenous variable into an array of dummies and assigns  
        J and K. 
        &quot;&quot;&quot;</span>
        <span class="s1">super(MNLogit</span><span class="s3">, </span><span class="s1">self).initialize()</span>
        <span class="s4">#This is also a &quot;whiten&quot; method as used in other models (eg regression)</span>
        <span class="s1">wendog</span><span class="s3">, </span><span class="s1">self.names = tools.categorical(self.endog</span><span class="s3">, </span><span class="s1">drop=</span><span class="s3">True, </span>
                <span class="s1">dictnames=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">self.wendog = wendog    </span><span class="s4"># don't drop first category</span>
        <span class="s1">self.J = float(wendog.shape[</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s1">self.K = float(self.exog.shape[</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s1">self.df_model *= (self.J-</span><span class="s5">1</span><span class="s1">) </span><span class="s4"># for each J - 1 equation.</span>
        <span class="s1">self.df_resid = self.nobs - self.df_model - (self.J-</span><span class="s5">1</span><span class="s1">)</span>


    <span class="s3">def </span><span class="s1">_eXB(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        A private method used by the cdf. 
 
        Returns 
        ------- 
        :math:`\exp(\beta_{j}^{\prime}x_{i})` 
 
        where :math:`j = 0,1,...,J` 
 
        Notes 
        ----- 
        A row of ones is appended for the dropped category. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">exog == </span><span class="s3">None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>
        <span class="s1">eXB = np.exp(np.dot(params.reshape(-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">exog.shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">exog.T))</span>
        <span class="s1">eXB = np.vstack((np.ones((</span><span class="s5">1</span><span class="s3">, </span><span class="s1">self.nobs))</span><span class="s3">, </span><span class="s1">eXB)) </span>
        <span class="s3">return </span><span class="s1">eXB</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">eXB):</span>
        <span class="s0">&quot;&quot;&quot; 
        NotImplemented 
        &quot;&quot;&quot;</span>
        <span class="s3">pass</span>

    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">eXB):</span>
        <span class="s0">&quot;&quot;&quot; 
        Multinomial logit cumulative distribution function. 
 
        Parameters 
        ---------- 
        eXB : array 
            The exponential predictor of the model exp(XB). 
 
        Returns 
        -------- 
        The cdf evaluated at `eXB`. 
 
        Notes 
        ----- 
        In the multinomial logit model. 
        .. math:: \\frac{\\exp\\left(\\beta_{j}^{\\prime}x_{i}\\right)}{\\sum_{k=0}^{J}\\exp\\left(\\beta_{k}^{\\prime}x_{i}\\right)}         
        &quot;&quot;&quot;</span>
        <span class="s1">num = eXB</span>
        <span class="s1">denom = eXB.sum(axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">num/denom[</span><span class="s3">None,</span><span class="s1">:]</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Log-likelihood of the multinomial logit model. 
 
        Parameters 
        ---------- 
        params : array-like 
            The parameters of the multinomial logit model. 
 
        Returns 
        ------- 
        The log-likelihood function of the logit model.  See notes. 
 
        Notes 
        ------ 
        .. math:: \\ln L=\\sum_{i=1}^{n}\\sum_{j=0}^{J}d_{ij}\\ln\\left(\\frac{\\exp\\left(\\beta_{j}^{\\prime}x_{i}\\right)}{\\sum_{k=0}^{J}\\exp\\left(\\beta_{k}^{\\prime}x_{i}\\right)}\\right) 
        where :math:`d_{ij}=1` if individual `i` chose alternative `j` and 0  
        if not. 
        &quot;&quot;&quot;</span>
        <span class="s1">d = self.wendog</span>
        <span class="s1">eXB = self._eXB(params)</span>
        <span class="s1">logprob = np.log(self.cdf(eXB))</span>
        <span class="s3">return </span><span class="s1">(d.T * logprob).sum()</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Score matrix for multinomial logit model log-likelihood 
 
        Parameters 
        ---------- 
        params : array 
            The parameters of the multinomial logit model. 
 
        Returns 
        -------- 
        The 2-d score vector of the multinomial logit model evaluated at  
        `params`. 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial\\ln L}{\\partial\\beta_{j}}=\\sum_{i}\\left(d_{ij}-\\frac{\\exp\\left(\\beta_{j}^{\\prime}x_{i}\\right)}{\\sum_{k=0}^{J}\\exp\\left(\\beta_{k}^{\\prime}x_{i}\\right)}\\right)x_{i} 
 
        for :math:`j=1,...,J` 
 
        In the multinomial model ths score matrix is K x J-1 but is returned  
        as a flattened array to work with the solvers. 
        &quot;&quot;&quot;</span>
        <span class="s1">eXB = self._eXB(params)</span>
        <span class="s1">firstterm = self.wendog[:</span><span class="s3">,</span><span class="s5">1</span><span class="s1">:].T - self.cdf(eXB)[</span><span class="s5">1</span><span class="s1">:</span><span class="s3">,</span><span class="s1">:]</span>
        <span class="s3">return </span><span class="s1">np.dot(firstterm</span><span class="s3">, </span><span class="s1">self.exog).flatten()</span>

    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Multinomial logit Hessian matrix of the log-likelihood 
 
        Parameters 
        ----------- 
        params : array-like 
            The parameters of the model 
 
        Returns 
        ------- 
        The Hessian evaluated at `params` 
 
        Notes 
        ----- 
        .. math:: \\frac{\\partial^{2}\\ln L}{\\partial\\beta_{j}\\partial\\beta_{l}}=-\\sum_{i=1}^{n}\\frac{\\exp\\left(\\beta_{j}^{\\prime}x_{i}\\right)}{\\sum_{k=0}^{J}\\exp\\left(\\beta_{k}^{\\prime}x_{i}\\right)}\\left[\\boldsymbol{1}\\left(j=l\\right)-\\frac{\\exp\\left(\\beta_{l}^{\\prime}x_{i}\\right)}{\\sum_{k=0}^{J}\\exp\\left(\\beta_{k}^{\\prime}x_{i}\\right)}\\right]x_{i}x_{l}^{\\prime} 
 
        where 
        :math:`\boldsymbol{1}\left(j=l\right)` equals 1 if `j` = `l` and 0 
        otherwise. 
 
        The actual Hessian matrix has J**2 * K x K elements. Our Hessian 
        is reshaped to be square (J*K, J*K) so that the solvers can use it. 
 
        This implementation does not take advantage of the symmetry of 
        the Hessian and could probably be refactored for speed. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">eXB = self._eXB(params)</span>
        <span class="s1">pr = self.cdf(eXB)</span>
        <span class="s1">partials = []</span>
        <span class="s1">J = self.wendog.shape[</span><span class="s5">1</span><span class="s1">] - </span><span class="s5">1</span>
        <span class="s1">K = self.exog.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(J):</span>
            <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(J): </span><span class="s4"># this loop assumes we drop the first col.</span>
                <span class="s3">if </span><span class="s1">i == j:</span>
                    <span class="s1">partials.append(\</span>
                        <span class="s1">-np.dot((pr[i+</span><span class="s5">1</span><span class="s3">,</span><span class="s1">:]*(</span><span class="s5">1</span><span class="s1">-pr[j+</span><span class="s5">1</span><span class="s3">,</span><span class="s1">:]))[</span><span class="s3">None,</span><span class="s1">:]*X.T</span><span class="s3">,</span><span class="s1">X))</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">partials.append(-np.dot(pr[i+</span><span class="s5">1</span><span class="s3">,</span><span class="s1">:]*-pr[j+</span><span class="s5">1</span><span class="s3">,</span><span class="s1">:][</span><span class="s3">None,</span><span class="s1">:]*X.T</span><span class="s3">,</span><span class="s1">X))</span>
        <span class="s1">H = np.array(partials)</span>
        <span class="s4"># the developer's notes on multinomial should clear this math up</span>
        <span class="s1">H = np.transpose(H.reshape(J</span><span class="s3">,</span><span class="s1">J</span><span class="s3">,</span><span class="s1">K</span><span class="s3">,</span><span class="s1">K)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">,</span><span class="s5">2</span><span class="s3">,</span><span class="s5">1</span><span class="s3">,</span><span class="s5">3</span><span class="s1">)).reshape(J*K</span><span class="s3">,</span><span class="s1">J*K)</span>
        <span class="s3">return </span><span class="s1">H</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">maxiter=</span><span class="s5">35</span><span class="s3">, </span><span class="s1">method=</span><span class="s2">'newton'</span><span class="s3">, </span>
            <span class="s1">tol=</span><span class="s5">1e-08</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fits the multinomial logit model. 
 
        Parameters 
        ---------- 
        start_params : array-like, optional 
            The default is a 0 vector. 
        maxiter : int, optional 
            Maximum number of iterations.  The default is 35. 
        method : str, optional 
            `method` can be 'newton', 'ncg', 'bfgs'. The default is 'newton'. 
        tol : float, optional 
            The convergence tolerance for the solver.  The default is  
            1e-08. 
 
        Notes 
        ----- 
        The reference category is always the first column of `wendog` for now. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">start_params == </span><span class="s3">None</span><span class="s1">:</span>
            <span class="s1">start_params = np.zeros((self.exog.shape[</span><span class="s5">1</span><span class="s1">]*\</span>
                    <span class="s1">(self.wendog.shape[</span><span class="s5">1</span><span class="s1">]-</span><span class="s5">1</span><span class="s1">)))</span>
        <span class="s1">mlefit = super(MNLogit</span><span class="s3">, </span><span class="s1">self).fit(start_params=start_params</span><span class="s3">,</span>
                <span class="s1">maxiter=maxiter</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">tol=tol)</span>
        <span class="s1">params = mlefit.params.reshape(-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">self.exog.shape[</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s1">mlefit = DiscreteResults(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">self.hessian(params))</span>
        <span class="s3">return </span><span class="s1">mlefit</span>
            
<span class="s4">#TODO: Weibull can replaced by a survival analsysis function</span>
<span class="s4"># like stat's streg (The cox model as well)</span>
<span class="s4">#class Weibull(DiscreteModel):</span>
<span class="s4">#    &quot;&quot;&quot;</span>
<span class="s4">#    Binary choice Weibull model</span>
<span class="s4">#</span>
<span class="s4">#    Notes</span>
<span class="s4">#    ------</span>
<span class="s4">#    This is unfinished and untested.</span>
<span class="s4">#    &quot;&quot;&quot;</span>
<span class="s4">##TODO: add analytic hessian for Weibull</span>
<span class="s4">#    def initialize(self):</span>
<span class="s4">#        pass</span>
<span class="s4">#</span>
<span class="s4">#    def cdf(self, X):</span>
<span class="s4">#        &quot;&quot;&quot;</span>
<span class="s4">#        Gumbell (Log Weibull) cumulative distribution function</span>
<span class="s4">#        &quot;&quot;&quot;</span>
<span class="s4">##        return np.exp(-np.exp(-X))</span>
<span class="s4">#        return stats.gumbel_r.cdf(X)</span>
<span class="s4">#        # these two are equivalent.</span>
<span class="s4">#        # Greene table and discussion is incorrect.</span>
<span class="s4">#</span>
<span class="s4">#    def pdf(self, X):</span>
<span class="s4">#        &quot;&quot;&quot;</span>
<span class="s4">#        Gumbell (LogWeibull) probability distribution function</span>
<span class="s4">#        &quot;&quot;&quot;</span>
<span class="s4">#        return stats.gumbel_r.pdf(X)</span>
<span class="s4">#</span>
<span class="s4">#    def loglike(self, params):</span>
<span class="s4">#        &quot;&quot;&quot;</span>
<span class="s4">#        Loglikelihood of Weibull distribution</span>
<span class="s4">#        &quot;&quot;&quot;</span>
<span class="s4">#        X = self.exog</span>
<span class="s4">#        cdf = self.cdf(np.dot(X,params))</span>
<span class="s4">#        y = self.endog</span>
<span class="s4">#        return np.sum(y*np.log(cdf) + (1-y)*np.log(1-cdf))</span>
<span class="s4">#</span>
<span class="s4">#    def score(self, params):</span>
<span class="s4">#        y = self.endog</span>
<span class="s4">#        X = self.exog</span>
<span class="s4">#        F = self.cdf(np.dot(X,params))</span>
<span class="s4">#        f = self.pdf(np.dot(X,params))</span>
<span class="s4">#        term = (y*f/F + (1 - y)*-f/(1-F))</span>
<span class="s4">#        return np.dot(term,X)</span>
<span class="s4">#</span>
<span class="s4">#    def hessian(self, params):</span>
<span class="s4">#        hess = nd.Jacobian(self.score)</span>
<span class="s4">#        return hess(params)</span>
<span class="s4">#</span>
<span class="s4">#    def fit(self, start_params=None, method='newton', maxiter=35, tol=1e-08):</span>
<span class="s4">## The example had problems with all zero start values, Hessian = 0</span>
<span class="s4">#        if start_params is None:</span>
<span class="s4">#            start_params = OLS(self.endog, self.exog).fit().params</span>
<span class="s4">#        mlefit = super(Weibull, self).fit(start_params=start_params, </span>
<span class="s4">#                method=method, maxiter=maxiter, tol=tol)</span>
<span class="s4">#        return mlefit</span>
<span class="s4">#</span>
<span class="s3">class </span><span class="s1">NegBinTwo(DiscreteModel):</span>
    <span class="s0">&quot;&quot;&quot; 
    NB2 Negative Binomial model. 
 
    Note: This is not working yet 
    &quot;&quot;&quot;</span>
<span class="s4">#NOTE: to use this with the solvers, the likelihood fit will probably</span>
<span class="s4"># need to be amended to have args, so that we can pass the ancillary param</span>
<span class="s4"># if not we can just stick the alpha param on the end of the beta params and</span>
<span class="s4"># amend all the methods to reflect this</span>
<span class="s4"># if we try to keep them separate I think we'd have to use a callback...</span>
<span class="s4"># need to check variance function, then derive score vector, and hessian</span>
<span class="s4"># loglike should be fine...</span>
<span class="s4"># also, alpha should maybe always be lnalpha to contrain it to be positive</span>

<span class="s4">#    def pdf(self, X, alpha):</span>
<span class="s4">#        a1 = alpha**-1</span>
<span class="s4">#        term1 = special.gamma(X + a1)/(special.agamma(X+1)*special.gamma(a1))</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Loglikelihood for NB2 model 
 
        Notes 
        ----- 
        The ancillary parameter is assumed to be the last element of 
        the params vector 
        &quot;&quot;&quot;</span>
        <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">a1 = alpha**-</span><span class="s5">1</span>
        <span class="s1">y = self.endog</span>
        <span class="s1">J = special.gammaln(y+a1) - special.gammaln(a1)</span>
<span class="s4"># See Cameron and Trivedi 1998 for a simplification of the above</span>
<span class="s4"># writing a convenience function using the log summation, *might*</span>
<span class="s4"># be more accurate</span>
        <span class="s1">XB = np.dot(self.exog</span><span class="s3">,</span><span class="s1">params)</span>
        <span class="s3">return </span><span class="s1">np.sum(J - np.log(factorial(y)) - \</span>
                <span class="s1">(y+a1)*np.log(</span><span class="s5">1</span><span class="s1">+alpha*np.exp(XB))+y*np.log(alpha)+y*XB)</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Score vector for NB2 model 
        &quot;&quot;&quot;</span>
        <span class="s3">import </span><span class="s1">numdifftools </span><span class="s3">as </span><span class="s1">nd</span>
        <span class="s1">y = self.endog</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">jfun = nd.Jacobian(self.loglike)</span>
        <span class="s3">return </span><span class="s1">jfun(params)[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">dLda2 = jfun(params)[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">alpha = params[-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">params = params[:-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">XB = np.dot(X</span><span class="s3">,</span><span class="s1">params)</span>
        <span class="s1">mu = np.exp(XB)</span>
        <span class="s1">a1 = alpha**-</span><span class="s5">1</span>
        <span class="s1">f1 = </span><span class="s3">lambda </span><span class="s1">x: </span><span class="s5">1.</span><span class="s1">/((x-</span><span class="s5">1</span><span class="s1">)*x/</span><span class="s5">2. </span><span class="s1">+ x*a1)</span>
        <span class="s1">cond = y&gt;</span><span class="s5">0</span>
        <span class="s1">dJ = np.piecewise(y</span><span class="s3">, </span><span class="s1">cond</span><span class="s3">, </span><span class="s1">[f1</span><span class="s3">,</span><span class="s5">1.</span><span class="s1">/a1])</span>
<span class="s4"># if y_i &lt; 1, this equals zero!  Not noted in C &amp;T</span>
        <span class="s1">dLdB = np.dot((y-mu)/(</span><span class="s5">1</span><span class="s1">+alpha*mu)</span><span class="s3">,</span><span class="s1">X)</span>
        <span class="s3">return </span><span class="s1">dLdB</span>
<span class="s4">#</span>
<span class="s4">#        dLda = np.sum(1/alpha**2 * (np.log(1+alpha*mu) - dJ) + \</span>
<span class="s4">#                (y-mu)/(alpha*(1+alpha*mu)))</span>
<span class="s4">#        scorevec = np.zeros((len(dLdB)+1))</span>
<span class="s4">#        scorevec[:-1] = dLdB</span>
<span class="s4">#        scorevec[-1] = dLda</span>
<span class="s4">#        scorevec[-1] = dLda2[-1]</span>
<span class="s4">#        return scorevec</span>

    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Hessian of NB2 model.  Currently uses numdifftools 
        &quot;&quot;&quot;</span>
<span class="s4">#        d2dBdB = </span>
<span class="s4">#        d2da2 =</span>
        <span class="s3">import </span><span class="s1">numdifftools </span><span class="s3">as </span><span class="s1">nd</span>
        <span class="s1">Hfun = nd.Jacobian(self.score)</span>
        <span class="s3">return </span><span class="s1">Hfun(params)[-</span><span class="s5">1</span><span class="s1">]</span>
<span class="s4"># is the numerical hessian block diagonal?  or is it block diagonal by assumption?</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">maxiter=</span><span class="s5">35</span><span class="s3">, </span><span class="s1">method=</span><span class="s2">'bfgs'</span><span class="s3">, </span><span class="s1">tol=</span><span class="s5">1e-08</span><span class="s1">):</span>
<span class="s4">#        start_params = [0]*(self.exog.shape[1])+[1]</span>
<span class="s4"># Use poisson fit as first guess.</span>
        <span class="s1">start_params = Poisson(self.endog</span><span class="s3">, </span><span class="s1">self.exog).fit().params</span>
        <span class="s1">start_params = np.roll(np.insert(start_params</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">mlefit = super(NegBinTwo</span><span class="s3">, </span><span class="s1">self).fit(start_params=start_params</span><span class="s3">, </span>
                <span class="s1">maxiter=maxiter</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">, </span><span class="s1">tol=tol)</span>
        <span class="s3">return </span><span class="s1">mlefit</span>


<span class="s4">### Results Class ###</span>

<span class="s4">#class DiscreteResults(object):</span>
<span class="s4">#TODO: these need to return z scores</span>
<span class="s3">class </span><span class="s1">DiscreteResults(LikelihoodModelResults):</span>
    <span class="s0">&quot;&quot;&quot; 
    A results class for the discrete dependent variable models. 
 
    Parameters 
    ---------- 
    model : A DiscreteModel instance 
    params : array-like 
        The parameters of a fitted model. 
    hessian : array-like 
        The hessian of the fitted model. 
    scale : float 
        A scale parameter for the covariance matrix. 
 
 
    Returns 
    ------- 
    *Attributes* 
 
    aic : float 
        Akaike information criterion.  -2*(`llf` - p) where p is the number 
        of regressors including the intercept. 
    bic : float 
        Bayesian information criterion. -2*`llf` + ln(`nobs`)*p where p is the 
        number of regressors including the intercept. 
    bse : array 
        The standard errors of the coefficients. 
    df_resid : float 
        See model definition. 
    df_model : float 
        See model definition. 
    fitted_values : array 
        Linear predictor XB. 
    llf : float 
        Value of the loglikelihood  
    llnull : float 
        Value of the constant-only loglikelihood 
    llr : float 
        Likelihood ratio chi-squared statistic; -2*(`llnull` - `llf`) 
    llr_pvalue : float 
        The chi-squared probability of getting a log-likelihood ratio  
        statistic greater than llr.  llr has a chi-squared distribution 
        with degrees of freedom `df_model`. 
    prsquared : float 
        McFadden's pseudo-R-squared. 1 - (`llf`/`llnull`) 
     
    Methods 
    ------- 
    margeff 
        Get marginal effects of the fitted model. 
    conf_int 
     
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">model</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">hessian</span><span class="s3">, </span><span class="s1">scale=</span><span class="s5">1.</span><span class="s1">):</span>
        <span class="s1">super(DiscreteResults</span><span class="s3">, </span><span class="s1">self).__init__(model</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span>
                <span class="s1">np.linalg.inv(-hessian)</span><span class="s3">, </span><span class="s1">scale=</span><span class="s5">1.</span><span class="s1">)</span>
        <span class="s1">self.df_model = model.df_model</span>
        <span class="s1">self.df_resid = model.df_resid</span>
        <span class="s1">self.nobs = model.nobs</span>
        <span class="s1">self._cache = resettable_cache()</span>
    
    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">bse(self):</span>
        <span class="s1">bse = np.sqrt(np.diag(self.cov_params()))</span>
        <span class="s3">if </span><span class="s1">self.params.ndim == </span><span class="s5">1 </span><span class="s3">or </span><span class="s1">self.params.shape[</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">bse</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">bse.reshape(self.params.shape)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">llf(self):</span>
        <span class="s1">model = self.model</span>
        <span class="s3">return </span><span class="s1">model.loglike(self.params)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">prsquared(self):</span>
        <span class="s3">return </span><span class="s5">1 </span><span class="s1">- self.llf/self.llnull</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">llr(self):</span>
        <span class="s3">return </span><span class="s1">-</span><span class="s5">2</span><span class="s1">*(self.llnull - self.llf)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">llr_pvalue(self):</span>
        <span class="s3">return </span><span class="s1">stats.chisqprob(self.llr</span><span class="s3">, </span><span class="s1">self.df_model)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">llnull(self):</span>
        <span class="s1">model = self.model </span><span class="s4"># will this use a new instance?</span>
        <span class="s1">null = model.__class__(model.endog</span><span class="s3">, </span><span class="s1">np.ones(model.nobs)).fit()</span>
        <span class="s3">return </span><span class="s1">null.llf</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">fittedvalues(self):</span>
        <span class="s3">return </span><span class="s1">np.dot(self.model.exog</span><span class="s3">, </span><span class="s1">self.params)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">aic(self):</span>
        <span class="s3">if </span><span class="s1">hasattr(self.model</span><span class="s3">, </span><span class="s2">&quot;J&quot;</span><span class="s1">):</span>
            <span class="s3">return </span><span class="s1">-</span><span class="s5">2</span><span class="s1">*(self.llf - (self.df_model+self.model.J-</span><span class="s5">1</span><span class="s1">))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">-</span><span class="s5">2</span><span class="s1">*(self.llf - (self.df_model+</span><span class="s5">1</span><span class="s1">))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">bic(self):</span>
        <span class="s3">if </span><span class="s1">hasattr(self.model</span><span class="s3">, </span><span class="s2">&quot;J&quot;</span><span class="s1">):</span>
            <span class="s3">return </span><span class="s1">-</span><span class="s5">2</span><span class="s1">*self.llf + np.log(self.nobs)*\</span>
                    <span class="s1">(self.df_model+self.model.J-</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">-</span><span class="s5">2</span><span class="s1">*self.llf + np.log(self.nobs)*(self.df_model+</span><span class="s5">1</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">conf_int(self</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s5">.05</span><span class="s3">, </span><span class="s1">cols=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s3">if </span><span class="s1">hasattr(self.model</span><span class="s3">, </span><span class="s2">&quot;J&quot;</span><span class="s1">):</span>
            <span class="s1">confint = super(DiscreteResults</span><span class="s3">, </span><span class="s1">self).conf_int(alpha=alpha</span><span class="s3">, </span>
                    <span class="s1">cols=cols)</span>
            <span class="s3">return </span><span class="s1">confint.transpose(</span><span class="s5">0</span><span class="s3">,</span><span class="s5">2</span><span class="s3">,</span><span class="s5">1</span><span class="s1">).reshape(self.model.J-</span><span class="s5">1</span><span class="s3">,</span><span class="s1">self.model.K</span><span class="s3">,</span><span class="s5">2</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">super(DiscreteResults</span><span class="s3">, </span><span class="s1">self).conf_int(alpha=alpha</span><span class="s3">, </span><span class="s1">cols=cols)</span>
    <span class="s1">conf_int.__doc__ = LikelihoodModelResults.conf_int.__doc__</span>
<span class="s4">#TODO: does the above work?</span>

<span class="s4">#TODO: the baove and the below will change if we merge the mixin branch</span>
    <span class="s3">def </span><span class="s1">t(self</span><span class="s3">, </span><span class="s1">column=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s3">if </span><span class="s1">hasattr(self.model</span><span class="s3">, </span><span class="s2">&quot;J&quot;</span><span class="s1">):</span>
            <span class="s4">#TODO: make this more robust once this is sorted</span>
            <span class="s3">if </span><span class="s1">column </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">column = range(int(self.model.K))</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">column = np.asarray(column)</span>
            <span class="s3">return </span><span class="s1">self.params/self.bse[:</span><span class="s3">,</span><span class="s1">column]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">super(DiscreteResults</span><span class="s3">, </span><span class="s1">self).t(column=column)</span>
    <span class="s1">t.__doc__ = LikelihoodModelResults.t.__doc__</span>


    <span class="s3">def </span><span class="s1">margeff(self</span><span class="s3">, </span><span class="s1">params=</span><span class="s3">None, </span><span class="s1">at=</span><span class="s2">'overall'</span><span class="s3">, </span><span class="s1">method=</span><span class="s2">'dydx'</span><span class="s3">, </span><span class="s1">atexog=</span><span class="s3">None, </span>
        <span class="s1">dummy=</span><span class="s3">False, </span><span class="s1">count=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Get marginal effects of the fitted model. 
 
        Parameters 
        ---------- 
        params : array-like, optional 
            The parameters. 
        at : str, optional 
            Options are: 
            'overall', The average of the marginal effects at each observation. 
            'mean', The marginal effects at the mean of each regressor. 
            'median', The marginal effects at the median of each regressor. 
            'zero', The marginal effects at zero for each regressor. 
            'all', The marginal effects at each observation. 
            Note that if `exog` is specified, then marginal effects for all 
            variables not specified by `exog` are calculated using the `at` 
            option. 
        method : str, optional 
            'dydx' - dy/dx - No transformation is made and marginal effects 
                are returned.  This is the default. 
            'eyex' - estimate elasticities of variables in `exog` -- 
                d(lny)/d(lnx) 
            'dyex' - estimate semielasticity -- dy/d(lnx)  
            'eydx' - estimate semeilasticity -- d(lny)/dx 
            Note that tranformations are done after each observation is  
            calculated.  Semi-elasticities for binary variables are computed 
            using the midpoint method. 'dyex' and 'eyex' do not make sense  
            for discrete variables. 
        atexog : array-like, optional 
            Optionally, you can provide the exogenous variables over which to  
            get the marginal effects.  This should be a dictionary with the key 
            as the zero-indexed column number and the value of the dictionary.   
            Default is None for all independent variables less the constant. 
        dummy : bool, optional 
            If False, treats binary variables (if present) as continuous.  This 
            is the default.  Else if True, treats binary variables as 
            changing from 0 to 1.  Note that any variable that is either 0 or 1 
            is treated as binary.  Each binary variable is treated separately 
            for now. 
        count : bool, optional 
            If False, treats count variables (if present) as continuous.  This  
            is the default.  Else if True, the marginal effect is the 
            change in probabilities when each observation is increased by one. 
 
        Returns 
        ------- 
        effects : ndarray 
            the marginal effect corresponding to the input options 
 
        Notes 
        ----- 
        When using after Poisson, returns the expected number of events 
        per period, assuming that the model is loglinear. 
        &quot;&quot;&quot;</span>
<span class="s4">#TODO:</span>
<span class="s4">#        factor : None or dictionary, optional</span>
<span class="s4">#            If a factor variable is present (it must be an integer, though</span>
<span class="s4">#            of type float), then `factor` may be a dict with the zero-indexed </span>
<span class="s4">#            column of the factor and the value should be the base-outcome.</span>

        <span class="s1">model = self.model</span>
        <span class="s1">method = method.lower()</span>
        <span class="s1">at = at.lower()</span>
        <span class="s3">if </span><span class="s1">params </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">params = self.params</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">params = np.asarray(params)</span>
            <span class="s4"># could prob use a shape check here (do we even need this option?)</span>
        <span class="s3">if not </span><span class="s1">at </span><span class="s3">in </span><span class="s1">[</span><span class="s2">'overall'</span><span class="s3">,</span><span class="s2">'mean'</span><span class="s3">,</span><span class="s2">'median'</span><span class="s3">,</span><span class="s2">'zero'</span><span class="s3">,</span><span class="s2">'all'</span><span class="s1">]:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s3">, </span><span class="s2">&quot;%s not a valid option for `at`.&quot; </span><span class="s1">% at</span>

        <span class="s1">exog = model.exog.copy() </span><span class="s4"># copy because values are changed</span>
        <span class="s1">ind = exog.var(</span><span class="s5">0</span><span class="s1">) != </span><span class="s5">0 </span><span class="s4"># index for non-constants</span>
        
        <span class="s4"># get user instructions</span>
        <span class="s3">if </span><span class="s1">dummy == </span><span class="s3">True or </span><span class="s1">count == </span><span class="s3">True</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">method </span><span class="s3">in </span><span class="s1">[</span><span class="s2">'dyex'</span><span class="s3">,</span><span class="s2">'eyex'</span><span class="s1">]:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s3">, </span><span class="s2">&quot;%s not allowed for discrete </span><span class="s3">\ 
</span><span class="s2">variables&quot; </span><span class="s1">% method</span>
            <span class="s3">if </span><span class="s1">at </span><span class="s3">in </span><span class="s1">[</span><span class="s2">'median'</span><span class="s3">, </span><span class="s2">'zero'</span><span class="s1">]:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s3">, </span><span class="s2">&quot;%s not allowed for discrete </span><span class="s3">\ 
</span><span class="s2">variables&quot; </span><span class="s1">% at</span>
            <span class="s3">if </span><span class="s1">dummy:</span>
                <span class="s1">dummy_ind = _isdummy(exog)</span>
            <span class="s3">if </span><span class="s1">count:</span>
                <span class="s1">count_ind = _iscount(exog)</span>
        <span class="s3">if </span><span class="s1">atexog </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s3">if not </span><span class="s1">isinstance(atexog</span><span class="s3">, </span><span class="s1">dict):</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s3">, </span><span class="s2">&quot;exog, if not None, should be a dict. </span><span class="s3">\ 
</span><span class="s2">Got %s&quot; </span><span class="s1">% type(atexog)</span>
            <span class="s3">for </span><span class="s1">key </span><span class="s3">in </span><span class="s1">atexog:</span>
                <span class="s1">exog[:</span><span class="s3">,</span><span class="s1">key] = atexog[key]</span>

        <span class="s3">if </span><span class="s1">at == </span><span class="s2">'mean'</span><span class="s1">:</span>
            <span class="s1">exog[:</span><span class="s3">,</span><span class="s1">ind] = exog.mean(</span><span class="s5">0</span><span class="s1">)[ind]</span>
        <span class="s3">elif </span><span class="s1">at == </span><span class="s2">'median'</span><span class="s1">:</span>
            <span class="s1">exog[:</span><span class="s3">,</span><span class="s1">ind] = np.median(exog</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)[ind]</span>
        <span class="s3">elif </span><span class="s1">at == </span><span class="s2">'zero'</span><span class="s1">:</span>
            <span class="s1">exog[:</span><span class="s3">,</span><span class="s1">ind] = </span><span class="s5">0</span>
        <span class="s3">if </span><span class="s1">method </span><span class="s3">not in </span><span class="s1">[</span><span class="s2">'dydx'</span><span class="s3">,</span><span class="s2">'eyex'</span><span class="s3">,</span><span class="s2">'dyex'</span><span class="s3">,</span><span class="s2">'eydx'</span><span class="s1">]:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s3">, </span><span class="s2">&quot;method is not understood.  Got %s&quot; </span><span class="s1">% method</span>
        <span class="s4"># group 1 probit, logit, logistic, cloglog, heckprob, xtprobit</span>
        <span class="s3">if </span><span class="s1">isinstance(model</span><span class="s3">, </span><span class="s1">(Probit</span><span class="s3">, </span><span class="s1">Logit)):</span>
            <span class="s1">effects = np.dot(model.pdf(np.dot(exog</span><span class="s3">,</span><span class="s1">params))[:</span><span class="s3">,None</span><span class="s1">]</span><span class="s3">,</span>
                    <span class="s1">params[</span><span class="s3">None,</span><span class="s1">:])</span>
        <span class="s4"># group 2 oprobit, ologit, gologit, mlogit, biprobit</span>
        <span class="s4">#TODO</span>
        <span class="s4"># group 3 poisson, nbreg, zip, zinb</span>
        <span class="s3">elif </span><span class="s1">isinstance(model</span><span class="s3">, </span><span class="s1">(Poisson)):</span>
            <span class="s1">effects = np.exp(np.dot(exog</span><span class="s3">, </span><span class="s1">params))[:</span><span class="s3">,None</span><span class="s1">]*params[</span><span class="s3">None,</span><span class="s1">:]</span>
        <span class="s1">fittedvalues = np.dot(exog</span><span class="s3">, </span><span class="s1">params) </span><span class="s4">#TODO: add a predict method</span>
                                            <span class="s4"># that takes an exog kwd</span>
        <span class="s3">if </span><span class="s2">'ex' </span><span class="s3">in </span><span class="s1">method:</span>
            <span class="s1">effects *= exog</span>
        <span class="s3">if </span><span class="s2">'dy' </span><span class="s3">in </span><span class="s1">method:</span>
            <span class="s3">if </span><span class="s1">at == </span><span class="s2">'all'</span><span class="s1">:</span>
                <span class="s1">effects = effects[:</span><span class="s3">,</span><span class="s1">ind]</span>
            <span class="s3">elif </span><span class="s1">at == </span><span class="s2">'overall'</span><span class="s1">:</span>
                <span class="s1">effects = effects.mean(</span><span class="s5">0</span><span class="s1">)[ind]</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">effects = effects[</span><span class="s5">0</span><span class="s3">,</span><span class="s1">ind]</span>
        <span class="s3">if </span><span class="s2">'ey' </span><span class="s3">in </span><span class="s1">method:</span>
            <span class="s1">effects /= model.cdf(fittedvalues[:</span><span class="s3">,None</span><span class="s1">])</span>
            <span class="s3">if </span><span class="s1">at == </span><span class="s2">'all'</span><span class="s1">:</span>
                <span class="s1">effects = effects[:</span><span class="s3">,</span><span class="s1">ind]</span>
            <span class="s3">elif </span><span class="s1">at == </span><span class="s2">'overall'</span><span class="s1">:</span>
                <span class="s1">effects = effects.mean(</span><span class="s5">0</span><span class="s1">)[ind]</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">effects = effects[</span><span class="s5">0</span><span class="s3">,</span><span class="s1">ind]</span>
        <span class="s3">if </span><span class="s1">dummy == </span><span class="s3">True</span><span class="s1">:</span>
            <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">tf </span><span class="s3">in </span><span class="s1">enumerate(dummy_ind):</span>
                <span class="s3">if </span><span class="s1">tf == </span><span class="s3">True</span><span class="s1">:</span>
                    <span class="s1">exog0 = exog.copy()</span>
                    <span class="s1">exog0[:</span><span class="s3">,</span><span class="s1">i] = </span><span class="s5">0</span>
                    <span class="s1">fittedvalues0 = np.dot(exog0</span><span class="s3">,</span><span class="s1">params)</span>
                    <span class="s1">exog1 = exog.copy()</span>
                    <span class="s1">exog1[:</span><span class="s3">,</span><span class="s1">i] = </span><span class="s5">1</span>
                    <span class="s1">fittedvalues1 = np.dot(exog1</span><span class="s3">, </span><span class="s1">params)</span>
                    <span class="s1">effect0 = model.cdf(np.dot(exog0</span><span class="s3">, </span><span class="s1">params))</span>
                    <span class="s1">effect1 = model.cdf(np.dot(exog1</span><span class="s3">, </span><span class="s1">params))</span>
                    <span class="s3">if </span><span class="s2">'ey' </span><span class="s3">in </span><span class="s1">method:</span>
                        <span class="s1">effect0 /= model.cdf(fittedvalues0)</span>
                        <span class="s1">effect1 /= model.cdf(fittedvalues1)</span>
                    <span class="s1">effects[i] = (effect1 - effect0).mean()</span>
        <span class="s3">if </span><span class="s1">count == </span><span class="s3">True</span><span class="s1">:</span>
            <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">tf </span><span class="s3">in </span><span class="s1">enumerate(count_ind):</span>
                <span class="s3">if </span><span class="s1">tf == </span><span class="s3">True</span><span class="s1">:</span>
                    <span class="s1">exog0 = exog.copy()</span>
                    <span class="s1">exog1 = exog.copy()</span>
                    <span class="s1">exog1[:</span><span class="s3">,</span><span class="s1">i] += </span><span class="s5">1</span>
                    <span class="s1">effect0 = model.cdf(np.dot(exog0</span><span class="s3">, </span><span class="s1">params))</span>
                    <span class="s1">effect1 = model.cdf(np.dot(exog1</span><span class="s3">, </span><span class="s1">params))</span>
<span class="s4">#TODO: compute discrete elasticity correctly</span>
<span class="s4">#Stata doesn't use the midpoint method or a weighted average.  </span>
<span class="s4">#Check elsewhere</span>
                    <span class="s3">if </span><span class="s2">'ey' </span><span class="s3">in </span><span class="s1">method:</span>
<span class="s4">#                        #TODO: don't know if this is theoretically correct</span>
                        <span class="s1">fittedvalues0 = np.dot(exog0</span><span class="s3">,</span><span class="s1">params)</span>
                        <span class="s1">fittedvalues1 = np.dot(exog1</span><span class="s3">,</span><span class="s1">params)</span>
<span class="s4">#                        weight1 = model.exog[:,i].mean()</span>
<span class="s4">#                        weight0 = 1 - weight1</span>
                        <span class="s1">wfv = (</span><span class="s5">.5</span><span class="s1">*model.cdf(fittedvalues1) + \</span>
                                <span class="s5">.5</span><span class="s1">*model.cdf(fittedvalues0))</span>
                        <span class="s1">effects[i] = ((effect1 - effect0)/wfv).mean()</span>
                    <span class="s1">effects[i] = (effect1 - effect0).mean() </span>
        <span class="s4"># Set standard error of the marginal effects by Delta method.</span>
        <span class="s1">self.margfx_se = </span><span class="s3">None</span>
        <span class="s1">self.margfx = effects</span>
        <span class="s3">return </span><span class="s1">effects</span>
        
<span class="s3">if </span><span class="s1">__name__==</span><span class="s2">&quot;__main__&quot;</span><span class="s1">:</span>
    <span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
    <span class="s3">import </span><span class="s1">scikits.statsmodels </span><span class="s3">as </span><span class="s1">sm</span>
<span class="s4"># Scratch work for negative binomial models</span>
<span class="s4"># dvisits was written using an R package, I can provide the dataset </span>
<span class="s4"># on request until the copyright is cleared up</span>
<span class="s4">#TODO: request permission to use dvisits</span>
    <span class="s1">data2 = np.genfromtxt(</span><span class="s2">'./dvisits.txt'</span><span class="s3">, </span><span class="s1">names=</span><span class="s3">True</span><span class="s1">)</span>
<span class="s4"># note that this has missing values for Accident</span>
    <span class="s1">endog = data2[</span><span class="s2">'doctorco'</span><span class="s1">]</span>
    <span class="s1">exog = data2[[</span><span class="s2">'sex'</span><span class="s3">,</span><span class="s2">'age'</span><span class="s3">,</span><span class="s2">'agesq'</span><span class="s3">,</span><span class="s2">'income'</span><span class="s3">,</span><span class="s2">'levyplus'</span><span class="s3">,</span><span class="s2">'freepoor'</span><span class="s3">,</span>
            <span class="s2">'freerepa'</span><span class="s3">,</span><span class="s2">'illness'</span><span class="s3">,</span><span class="s2">'actdays'</span><span class="s3">,</span><span class="s2">'hscore'</span><span class="s3">,</span><span class="s2">'chcond1'</span><span class="s3">,</span>
            <span class="s2">'chcond2'</span><span class="s1">]].view(float).reshape(len(data2)</span><span class="s3">,</span><span class="s1">-</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">exog = sm.add_constant(exog</span><span class="s3">, </span><span class="s1">prepend=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">poisson_mod = Poisson(endog</span><span class="s3">, </span><span class="s1">exog)</span>
    <span class="s1">poisson_res = poisson_mod.fit()</span>
<span class="s4">#    nb2_mod = NegBinTwo(endog, exog)</span>
<span class="s4">#    nb2_res = nb2_mod.fit()</span>
<span class="s4"># solvers hang (with no error and no maxiter warn...)</span>
<span class="s4"># haven't derived hessian (though it will be block diagonal) to check</span>
<span class="s4"># newton, note that Lawless (1987) has the derivations</span>
<span class="s4"># appear to be something wrong with the score?</span>
<span class="s4"># according to Lawless, traditionally the likelihood is maximized wrt to B</span>
<span class="s4"># and a gridsearch on a to determin ahat?</span>
<span class="s4"># or the Breslow approach, which is 2 step iterative.</span>
    <span class="s1">nb2_params = [-</span><span class="s5">2.190</span><span class="s3">,</span><span class="s5">.217</span><span class="s3">,</span><span class="s1">-</span><span class="s5">.216</span><span class="s3">,</span><span class="s5">.609</span><span class="s3">,</span><span class="s1">-</span><span class="s5">.142</span><span class="s3">,</span><span class="s5">.118</span><span class="s3">,</span><span class="s1">-</span><span class="s5">.497</span><span class="s3">,</span><span class="s5">.145</span><span class="s3">,</span><span class="s5">.214</span><span class="s3">,</span><span class="s5">.144</span><span class="s3">,</span>
            <span class="s5">.038</span><span class="s3">,</span><span class="s5">.099</span><span class="s3">,</span><span class="s5">.190</span><span class="s3">,</span><span class="s5">1.077</span><span class="s1">] </span><span class="s4"># alpha is last</span>
    <span class="s4"># taken from Cameron and Trivedi</span>
<span class="s4"># the below is from Cameron and Trivedi as well</span>
<span class="s4">#    endog2 = np.array(endog&gt;=1, dtype=float)</span>
<span class="s4"># skipped for now, binary poisson results look off?</span>


</pre>
</body>
</html>