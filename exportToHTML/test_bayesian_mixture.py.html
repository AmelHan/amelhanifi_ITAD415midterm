<html>
<head>
<title>test_bayesian_mixture.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_bayesian_mixture.py</font>
</center></td></tr></table>
<pre><span class="s0"># Author: Wei Xue &lt;xuewei4d@gmail.com&gt;</span>
<span class="s0">#         Thierry Guillemot &lt;thierry.guillemot.work@gmail.com&gt;</span>
<span class="s0"># License: BSD 3 clause</span>
<span class="s2">import </span><span class="s1">copy</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">scipy.special </span><span class="s2">import </span><span class="s1">gammaln</span>

<span class="s2">from </span><span class="s1">sklearn.exceptions </span><span class="s2">import </span><span class="s1">ConvergenceWarning</span><span class="s2">, </span><span class="s1">NotFittedError</span>
<span class="s2">from </span><span class="s1">sklearn.metrics.cluster </span><span class="s2">import </span><span class="s1">adjusted_rand_score</span>
<span class="s2">from </span><span class="s1">sklearn.mixture </span><span class="s2">import </span><span class="s1">BayesianGaussianMixture</span>
<span class="s2">from </span><span class="s1">sklearn.mixture._bayesian_mixture </span><span class="s2">import </span><span class="s1">_log_dirichlet_norm</span><span class="s2">, </span><span class="s1">_log_wishart_norm</span>
<span class="s2">from </span><span class="s1">sklearn.mixture.tests.test_gaussian_mixture </span><span class="s2">import </span><span class="s1">RandomData</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">assert_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_equal</span><span class="s2">,</span>
    <span class="s1">ignore_warnings</span><span class="s2">,</span>
<span class="s1">)</span>

<span class="s1">COVARIANCE_TYPE = [</span><span class="s3">&quot;full&quot;</span><span class="s2">, </span><span class="s3">&quot;tied&quot;</span><span class="s2">, </span><span class="s3">&quot;diag&quot;</span><span class="s2">, </span><span class="s3">&quot;spherical&quot;</span><span class="s1">]</span>
<span class="s1">PRIOR_TYPE = [</span><span class="s3">&quot;dirichlet_process&quot;</span><span class="s2">, </span><span class="s3">&quot;dirichlet_distribution&quot;</span><span class="s1">]</span>


<span class="s2">def </span><span class="s1">test_log_dirichlet_norm():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">weight_concentration = rng.rand(</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">expected_norm = gammaln(np.sum(weight_concentration)) - np.sum(</span>
        <span class="s1">gammaln(weight_concentration)</span>
    <span class="s1">)</span>
    <span class="s1">predected_norm = _log_dirichlet_norm(weight_concentration)</span>

    <span class="s1">assert_almost_equal(expected_norm</span><span class="s2">, </span><span class="s1">predected_norm)</span>


<span class="s2">def </span><span class="s1">test_log_wishart_norm():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">n_components</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">5</span><span class="s2">, </span><span class="s4">2</span>
    <span class="s1">degrees_of_freedom = np.abs(rng.rand(n_components)) + </span><span class="s4">1.0</span>
    <span class="s1">log_det_precisions_chol = n_features * np.log(range(</span><span class="s4">2</span><span class="s2">, </span><span class="s4">2 </span><span class="s1">+ n_components))</span>

    <span class="s1">expected_norm = np.empty(</span><span class="s4">5</span><span class="s1">)</span>
    <span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">(degrees_of_freedom_k</span><span class="s2">, </span><span class="s1">log_det_k) </span><span class="s2">in </span><span class="s1">enumerate(</span>
        <span class="s1">zip(degrees_of_freedom</span><span class="s2">, </span><span class="s1">log_det_precisions_chol)</span>
    <span class="s1">):</span>
        <span class="s1">expected_norm[k] = -(</span>
            <span class="s1">degrees_of_freedom_k * (log_det_k + </span><span class="s4">0.5 </span><span class="s1">* n_features * np.log(</span><span class="s4">2.0</span><span class="s1">))</span>
            <span class="s1">+ np.sum(</span>
                <span class="s1">gammaln(</span>
                    <span class="s4">0.5</span>
                    <span class="s1">* (degrees_of_freedom_k - np.arange(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">n_features)[:</span><span class="s2">, </span><span class="s1">np.newaxis])</span>
                <span class="s1">)</span><span class="s2">,</span>
                <span class="s4">0</span><span class="s2">,</span>
            <span class="s1">)</span>
        <span class="s1">).item()</span>
    <span class="s1">predected_norm = _log_wishart_norm(</span>
        <span class="s1">degrees_of_freedom</span><span class="s2">, </span><span class="s1">log_det_precisions_chol</span><span class="s2">, </span><span class="s1">n_features</span>
    <span class="s1">)</span>

    <span class="s1">assert_almost_equal(expected_norm</span><span class="s2">, </span><span class="s1">predected_norm)</span>


<span class="s2">def </span><span class="s1">test_bayesian_mixture_weights_prior_initialisation():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_components</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">10</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">2</span>
    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>

    <span class="s0"># Check correct init for a given value of weight_concentration_prior</span>
    <span class="s1">weight_concentration_prior = rng.rand()</span>
    <span class="s1">bgmm = BayesianGaussianMixture(</span>
        <span class="s1">weight_concentration_prior=weight_concentration_prior</span><span class="s2">, </span><span class="s1">random_state=rng</span>
    <span class="s1">).fit(X)</span>
    <span class="s1">assert_almost_equal(weight_concentration_prior</span><span class="s2">, </span><span class="s1">bgmm.weight_concentration_prior_)</span>

    <span class="s0"># Check correct init for the default value of weight_concentration_prior</span>
    <span class="s1">bgmm = BayesianGaussianMixture(n_components=n_components</span><span class="s2">, </span><span class="s1">random_state=rng).fit(X)</span>
    <span class="s1">assert_almost_equal(</span><span class="s4">1.0 </span><span class="s1">/ n_components</span><span class="s2">, </span><span class="s1">bgmm.weight_concentration_prior_)</span>


<span class="s2">def </span><span class="s1">test_bayesian_mixture_mean_prior_initialisation():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_components</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">10</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span>
    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>

    <span class="s0"># Check correct init for a given value of mean_precision_prior</span>
    <span class="s1">mean_precision_prior = rng.rand()</span>
    <span class="s1">bgmm = BayesianGaussianMixture(</span>
        <span class="s1">mean_precision_prior=mean_precision_prior</span><span class="s2">, </span><span class="s1">random_state=rng</span>
    <span class="s1">).fit(X)</span>
    <span class="s1">assert_almost_equal(mean_precision_prior</span><span class="s2">, </span><span class="s1">bgmm.mean_precision_prior_)</span>

    <span class="s0"># Check correct init for the default value of mean_precision_prior</span>
    <span class="s1">bgmm = BayesianGaussianMixture(random_state=rng).fit(X)</span>
    <span class="s1">assert_almost_equal(</span><span class="s4">1.0</span><span class="s2">, </span><span class="s1">bgmm.mean_precision_prior_)</span>

    <span class="s0"># Check correct init for a given value of mean_prior</span>
    <span class="s1">mean_prior = rng.rand(n_features)</span>
    <span class="s1">bgmm = BayesianGaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">, </span><span class="s1">mean_prior=mean_prior</span><span class="s2">, </span><span class="s1">random_state=rng</span>
    <span class="s1">).fit(X)</span>
    <span class="s1">assert_almost_equal(mean_prior</span><span class="s2">, </span><span class="s1">bgmm.mean_prior_)</span>

    <span class="s0"># Check correct init for the default value of bemean_priorta</span>
    <span class="s1">bgmm = BayesianGaussianMixture(n_components=n_components</span><span class="s2">, </span><span class="s1">random_state=rng).fit(X)</span>
    <span class="s1">assert_almost_equal(X.mean(axis=</span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">bgmm.mean_prior_)</span>


<span class="s2">def </span><span class="s1">test_bayesian_mixture_precisions_prior_initialisation():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">10</span><span class="s2">, </span><span class="s4">2</span>
    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>

    <span class="s0"># Check raise message for a bad value of degrees_of_freedom_prior</span>
    <span class="s1">bad_degrees_of_freedom_prior_ = n_features - </span><span class="s4">1.0</span>
    <span class="s1">bgmm = BayesianGaussianMixture(</span>
        <span class="s1">degrees_of_freedom_prior=bad_degrees_of_freedom_prior_</span><span class="s2">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">msg = (</span>
        <span class="s3">&quot;The parameter 'degrees_of_freedom_prior' should be greater than&quot;</span>
        <span class="s3">f&quot; </span><span class="s2">{</span><span class="s1">n_features -</span><span class="s4">1</span><span class="s2">}</span><span class="s3">, but got </span><span class="s2">{</span><span class="s1">bad_degrees_of_freedom_prior_</span><span class="s2">:</span><span class="s3">.3f</span><span class="s2">}</span><span class="s3">.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">bgmm.fit(X)</span>

    <span class="s0"># Check correct init for a given value of degrees_of_freedom_prior</span>
    <span class="s1">degrees_of_freedom_prior = rng.rand() + n_features - </span><span class="s4">1.0</span>
    <span class="s1">bgmm = BayesianGaussianMixture(</span>
        <span class="s1">degrees_of_freedom_prior=degrees_of_freedom_prior</span><span class="s2">, </span><span class="s1">random_state=rng</span>
    <span class="s1">).fit(X)</span>
    <span class="s1">assert_almost_equal(degrees_of_freedom_prior</span><span class="s2">, </span><span class="s1">bgmm.degrees_of_freedom_prior_)</span>

    <span class="s0"># Check correct init for the default value of degrees_of_freedom_prior</span>
    <span class="s1">degrees_of_freedom_prior_default = n_features</span>
    <span class="s1">bgmm = BayesianGaussianMixture(</span>
        <span class="s1">degrees_of_freedom_prior=degrees_of_freedom_prior_default</span><span class="s2">, </span><span class="s1">random_state=rng</span>
    <span class="s1">).fit(X)</span>
    <span class="s1">assert_almost_equal(</span>
        <span class="s1">degrees_of_freedom_prior_default</span><span class="s2">, </span><span class="s1">bgmm.degrees_of_freedom_prior_</span>
    <span class="s1">)</span>

    <span class="s0"># Check correct init for a given value of covariance_prior</span>
    <span class="s1">covariance_prior = {</span>
        <span class="s3">&quot;full&quot;</span><span class="s1">: np.cov(X.T</span><span class="s2">, </span><span class="s1">bias=</span><span class="s4">1</span><span class="s1">) + </span><span class="s4">10</span><span class="s2">,</span>
        <span class="s3">&quot;tied&quot;</span><span class="s1">: np.cov(X.T</span><span class="s2">, </span><span class="s1">bias=</span><span class="s4">1</span><span class="s1">) + </span><span class="s4">5</span><span class="s2">,</span>
        <span class="s3">&quot;diag&quot;</span><span class="s1">: np.diag(np.atleast_2d(np.cov(X.T</span><span class="s2">, </span><span class="s1">bias=</span><span class="s4">1</span><span class="s1">))) + </span><span class="s4">3</span><span class="s2">,</span>
        <span class="s3">&quot;spherical&quot;</span><span class="s1">: rng.rand()</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s1">bgmm = BayesianGaussianMixture(random_state=rng)</span>
    <span class="s2">for </span><span class="s1">cov_type </span><span class="s2">in </span><span class="s1">[</span><span class="s3">&quot;full&quot;</span><span class="s2">, </span><span class="s3">&quot;tied&quot;</span><span class="s2">, </span><span class="s3">&quot;diag&quot;</span><span class="s2">, </span><span class="s3">&quot;spherical&quot;</span><span class="s1">]:</span>
        <span class="s1">bgmm.covariance_type = cov_type</span>
        <span class="s1">bgmm.covariance_prior = covariance_prior[cov_type]</span>
        <span class="s1">bgmm.fit(X)</span>
        <span class="s1">assert_almost_equal(covariance_prior[cov_type]</span><span class="s2">, </span><span class="s1">bgmm.covariance_prior_)</span>

    <span class="s0"># Check correct init for the default value of covariance_prior</span>
    <span class="s1">covariance_prior_default = {</span>
        <span class="s3">&quot;full&quot;</span><span class="s1">: np.atleast_2d(np.cov(X.T))</span><span class="s2">,</span>
        <span class="s3">&quot;tied&quot;</span><span class="s1">: np.atleast_2d(np.cov(X.T))</span><span class="s2">,</span>
        <span class="s3">&quot;diag&quot;</span><span class="s1">: np.var(X</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">ddof=</span><span class="s4">1</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s3">&quot;spherical&quot;</span><span class="s1">: np.var(X</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">ddof=</span><span class="s4">1</span><span class="s1">).mean()</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s1">bgmm = BayesianGaussianMixture(random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s2">for </span><span class="s1">cov_type </span><span class="s2">in </span><span class="s1">[</span><span class="s3">&quot;full&quot;</span><span class="s2">, </span><span class="s3">&quot;tied&quot;</span><span class="s2">, </span><span class="s3">&quot;diag&quot;</span><span class="s2">, </span><span class="s3">&quot;spherical&quot;</span><span class="s1">]:</span>
        <span class="s1">bgmm.covariance_type = cov_type</span>
        <span class="s1">bgmm.fit(X)</span>
        <span class="s1">assert_almost_equal(covariance_prior_default[cov_type]</span><span class="s2">, </span><span class="s1">bgmm.covariance_prior_)</span>


<span class="s2">def </span><span class="s1">test_bayesian_mixture_check_is_fitted():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">10</span><span class="s2">, </span><span class="s4">2</span>

    <span class="s0"># Check raise message</span>
    <span class="s1">bgmm = BayesianGaussianMixture(random_state=rng)</span>
    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>

    <span class="s1">msg = </span><span class="s3">&quot;This BayesianGaussianMixture instance is not fitted yet.&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">bgmm.score(X)</span>


<span class="s2">def </span><span class="s1">test_bayesian_mixture_weights():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">10</span><span class="s2">, </span><span class="s4">2</span>

    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>

    <span class="s0"># Case Dirichlet distribution for the weight concentration prior type</span>
    <span class="s1">bgmm = BayesianGaussianMixture(</span>
        <span class="s1">weight_concentration_prior_type=</span><span class="s3">&quot;dirichlet_distribution&quot;</span><span class="s2">,</span>
        <span class="s1">n_components=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">random_state=rng</span><span class="s2">,</span>
    <span class="s1">).fit(X)</span>

    <span class="s1">expected_weights = bgmm.weight_concentration_ / np.sum(bgmm.weight_concentration_)</span>
    <span class="s1">assert_almost_equal(expected_weights</span><span class="s2">, </span><span class="s1">bgmm.weights_)</span>
    <span class="s1">assert_almost_equal(np.sum(bgmm.weights_)</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">)</span>

    <span class="s0"># Case Dirichlet process for the weight concentration prior type</span>
    <span class="s1">dpgmm = BayesianGaussianMixture(</span>
        <span class="s1">weight_concentration_prior_type=</span><span class="s3">&quot;dirichlet_process&quot;</span><span class="s2">,</span>
        <span class="s1">n_components=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">random_state=rng</span><span class="s2">,</span>
    <span class="s1">).fit(X)</span>
    <span class="s1">weight_dirichlet_sum = (</span>
        <span class="s1">dpgmm.weight_concentration_[</span><span class="s4">0</span><span class="s1">] + dpgmm.weight_concentration_[</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">tmp = dpgmm.weight_concentration_[</span><span class="s4">1</span><span class="s1">] / weight_dirichlet_sum</span>
    <span class="s1">expected_weights = (</span>
        <span class="s1">dpgmm.weight_concentration_[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">/ weight_dirichlet_sum</span>
        <span class="s1">* np.hstack((</span><span class="s4">1</span><span class="s2">, </span><span class="s1">np.cumprod(tmp[:-</span><span class="s4">1</span><span class="s1">])))</span>
    <span class="s1">)</span>
    <span class="s1">expected_weights /= np.sum(expected_weights)</span>
    <span class="s1">assert_almost_equal(expected_weights</span><span class="s2">, </span><span class="s1">dpgmm.weights_)</span>
    <span class="s1">assert_almost_equal(np.sum(dpgmm.weights_)</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">)</span>


<span class="s1">@ignore_warnings(category=ConvergenceWarning)</span>
<span class="s2">def </span><span class="s1">test_monotonic_likelihood():</span>
    <span class="s0"># We check that each step of the each step of variational inference without</span>
    <span class="s0"># regularization improve monotonically the training set of the bound</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">n_components = rand_data.n_components</span>

    <span class="s2">for </span><span class="s1">prior_type </span><span class="s2">in </span><span class="s1">PRIOR_TYPE:</span>
        <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
            <span class="s1">X = rand_data.X[covar_type]</span>
            <span class="s1">bgmm = BayesianGaussianMixture(</span>
                <span class="s1">weight_concentration_prior_type=prior_type</span><span class="s2">,</span>
                <span class="s1">n_components=</span><span class="s4">2 </span><span class="s1">* n_components</span><span class="s2">,</span>
                <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
                <span class="s1">warm_start=</span><span class="s2">True,</span>
                <span class="s1">max_iter=</span><span class="s4">1</span><span class="s2">,</span>
                <span class="s1">random_state=rng</span><span class="s2">,</span>
                <span class="s1">tol=</span><span class="s4">1e-3</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">current_lower_bound = -np.inf</span>
            <span class="s0"># Do one training iteration at a time so we can make sure that the</span>
            <span class="s0"># training log likelihood increases after each iteration.</span>
            <span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">600</span><span class="s1">):</span>
                <span class="s1">prev_lower_bound = current_lower_bound</span>
                <span class="s1">current_lower_bound = bgmm.fit(X).lower_bound_</span>
                <span class="s2">assert </span><span class="s1">current_lower_bound &gt;= prev_lower_bound</span>

                <span class="s2">if </span><span class="s1">bgmm.converged_:</span>
                    <span class="s2">break</span>
            <span class="s2">assert </span><span class="s1">bgmm.converged_</span>


<span class="s2">def </span><span class="s1">test_compare_covar_type():</span>
    <span class="s0"># We can compare the 'full' precision with the other cov_type if we apply</span>
    <span class="s0"># 1 iter of the M-step (done during _initialize_parameters).</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">7</span><span class="s1">)</span>
    <span class="s1">X = rand_data.X[</span><span class="s3">&quot;full&quot;</span><span class="s1">]</span>
    <span class="s1">n_components = rand_data.n_components</span>

    <span class="s2">for </span><span class="s1">prior_type </span><span class="s2">in </span><span class="s1">PRIOR_TYPE:</span>
        <span class="s0"># Computation of the full_covariance</span>
        <span class="s1">bgmm = BayesianGaussianMixture(</span>
            <span class="s1">weight_concentration_prior_type=prior_type</span><span class="s2">,</span>
            <span class="s1">n_components=</span><span class="s4">2 </span><span class="s1">* n_components</span><span class="s2">,</span>
            <span class="s1">covariance_type=</span><span class="s3">&quot;full&quot;</span><span class="s2">,</span>
            <span class="s1">max_iter=</span><span class="s4">1</span><span class="s2">,</span>
            <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">tol=</span><span class="s4">1e-7</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">bgmm._check_parameters(X)</span>
        <span class="s1">bgmm._initialize_parameters(X</span><span class="s2">, </span><span class="s1">np.random.RandomState(</span><span class="s4">0</span><span class="s1">))</span>
        <span class="s1">full_covariances = (</span>
            <span class="s1">bgmm.covariances_ * bgmm.degrees_of_freedom_[:</span><span class="s2">, </span><span class="s1">np.newaxis</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">)</span>

        <span class="s0"># Check tied_covariance = mean(full_covariances, 0)</span>
        <span class="s1">bgmm = BayesianGaussianMixture(</span>
            <span class="s1">weight_concentration_prior_type=prior_type</span><span class="s2">,</span>
            <span class="s1">n_components=</span><span class="s4">2 </span><span class="s1">* n_components</span><span class="s2">,</span>
            <span class="s1">covariance_type=</span><span class="s3">&quot;tied&quot;</span><span class="s2">,</span>
            <span class="s1">max_iter=</span><span class="s4">1</span><span class="s2">,</span>
            <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">tol=</span><span class="s4">1e-7</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">bgmm._check_parameters(X)</span>
        <span class="s1">bgmm._initialize_parameters(X</span><span class="s2">, </span><span class="s1">np.random.RandomState(</span><span class="s4">0</span><span class="s1">))</span>

        <span class="s1">tied_covariance = bgmm.covariances_ * bgmm.degrees_of_freedom_</span>
        <span class="s1">assert_almost_equal(tied_covariance</span><span class="s2">, </span><span class="s1">np.mean(full_covariances</span><span class="s2">, </span><span class="s4">0</span><span class="s1">))</span>

        <span class="s0"># Check diag_covariance = diag(full_covariances)</span>
        <span class="s1">bgmm = BayesianGaussianMixture(</span>
            <span class="s1">weight_concentration_prior_type=prior_type</span><span class="s2">,</span>
            <span class="s1">n_components=</span><span class="s4">2 </span><span class="s1">* n_components</span><span class="s2">,</span>
            <span class="s1">covariance_type=</span><span class="s3">&quot;diag&quot;</span><span class="s2">,</span>
            <span class="s1">max_iter=</span><span class="s4">1</span><span class="s2">,</span>
            <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">tol=</span><span class="s4">1e-7</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">bgmm._check_parameters(X)</span>
        <span class="s1">bgmm._initialize_parameters(X</span><span class="s2">, </span><span class="s1">np.random.RandomState(</span><span class="s4">0</span><span class="s1">))</span>

        <span class="s1">diag_covariances = bgmm.covariances_ * bgmm.degrees_of_freedom_[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">assert_almost_equal(</span>
            <span class="s1">diag_covariances</span><span class="s2">, </span><span class="s1">np.array([np.diag(cov) </span><span class="s2">for </span><span class="s1">cov </span><span class="s2">in </span><span class="s1">full_covariances])</span>
        <span class="s1">)</span>

        <span class="s0"># Check spherical_covariance = np.mean(diag_covariances, 0)</span>
        <span class="s1">bgmm = BayesianGaussianMixture(</span>
            <span class="s1">weight_concentration_prior_type=prior_type</span><span class="s2">,</span>
            <span class="s1">n_components=</span><span class="s4">2 </span><span class="s1">* n_components</span><span class="s2">,</span>
            <span class="s1">covariance_type=</span><span class="s3">&quot;spherical&quot;</span><span class="s2">,</span>
            <span class="s1">max_iter=</span><span class="s4">1</span><span class="s2">,</span>
            <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">tol=</span><span class="s4">1e-7</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">bgmm._check_parameters(X)</span>
        <span class="s1">bgmm._initialize_parameters(X</span><span class="s2">, </span><span class="s1">np.random.RandomState(</span><span class="s4">0</span><span class="s1">))</span>

        <span class="s1">spherical_covariances = bgmm.covariances_ * bgmm.degrees_of_freedom_</span>
        <span class="s1">assert_almost_equal(spherical_covariances</span><span class="s2">, </span><span class="s1">np.mean(diag_covariances</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>


<span class="s1">@ignore_warnings(category=ConvergenceWarning)</span>
<span class="s2">def </span><span class="s1">test_check_covariance_precision():</span>
    <span class="s0"># We check that the dot product of the covariance and the precision</span>
    <span class="s0"># matrices is identity.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">7</span><span class="s1">)</span>
    <span class="s1">n_components</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">2 </span><span class="s1">* rand_data.n_components</span><span class="s2">, </span><span class="s4">2</span>

    <span class="s0"># Computation of the full_covariance</span>
    <span class="s1">bgmm = BayesianGaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">100</span><span class="s2">, </span><span class="s1">random_state=rng</span><span class="s2">, </span><span class="s1">tol=</span><span class="s4">1e-3</span><span class="s2">, </span><span class="s1">reg_covar=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">bgmm.covariance_type = covar_type</span>
        <span class="s1">bgmm.fit(rand_data.X[covar_type])</span>

        <span class="s2">if </span><span class="s1">covar_type == </span><span class="s3">&quot;full&quot;</span><span class="s1">:</span>
            <span class="s2">for </span><span class="s1">covar</span><span class="s2">, </span><span class="s1">precision </span><span class="s2">in </span><span class="s1">zip(bgmm.covariances_</span><span class="s2">, </span><span class="s1">bgmm.precisions_):</span>
                <span class="s1">assert_almost_equal(np.dot(covar</span><span class="s2">, </span><span class="s1">precision)</span><span class="s2">, </span><span class="s1">np.eye(n_features))</span>
        <span class="s2">elif </span><span class="s1">covar_type == </span><span class="s3">&quot;tied&quot;</span><span class="s1">:</span>
            <span class="s1">assert_almost_equal(</span>
                <span class="s1">np.dot(bgmm.covariances_</span><span class="s2">, </span><span class="s1">bgmm.precisions_)</span><span class="s2">, </span><span class="s1">np.eye(n_features)</span>
            <span class="s1">)</span>

        <span class="s2">elif </span><span class="s1">covar_type == </span><span class="s3">&quot;diag&quot;</span><span class="s1">:</span>
            <span class="s1">assert_almost_equal(</span>
                <span class="s1">bgmm.covariances_ * bgmm.precisions_</span><span class="s2">,</span>
                <span class="s1">np.ones((n_components</span><span class="s2">, </span><span class="s1">n_features))</span><span class="s2">,</span>
            <span class="s1">)</span>

        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">assert_almost_equal(</span>
                <span class="s1">bgmm.covariances_ * bgmm.precisions_</span><span class="s2">, </span><span class="s1">np.ones(n_components)</span>
            <span class="s1">)</span>


<span class="s1">@ignore_warnings(category=ConvergenceWarning)</span>
<span class="s2">def </span><span class="s1">test_invariant_translation():</span>
    <span class="s0"># We check here that adding a constant in the data change correctly the</span>
    <span class="s0"># parameters of the mixture</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">100</span><span class="s1">)</span>
    <span class="s1">n_components = </span><span class="s4">2 </span><span class="s1">* rand_data.n_components</span>

    <span class="s2">for </span><span class="s1">prior_type </span><span class="s2">in </span><span class="s1">PRIOR_TYPE:</span>
        <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
            <span class="s1">X = rand_data.X[covar_type]</span>
            <span class="s1">bgmm1 = BayesianGaussianMixture(</span>
                <span class="s1">weight_concentration_prior_type=prior_type</span><span class="s2">,</span>
                <span class="s1">n_components=n_components</span><span class="s2">,</span>
                <span class="s1">max_iter=</span><span class="s4">100</span><span class="s2">,</span>
                <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
                <span class="s1">tol=</span><span class="s4">1e-3</span><span class="s2">,</span>
                <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">).fit(X)</span>
            <span class="s1">bgmm2 = BayesianGaussianMixture(</span>
                <span class="s1">weight_concentration_prior_type=prior_type</span><span class="s2">,</span>
                <span class="s1">n_components=n_components</span><span class="s2">,</span>
                <span class="s1">max_iter=</span><span class="s4">100</span><span class="s2">,</span>
                <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
                <span class="s1">tol=</span><span class="s4">1e-3</span><span class="s2">,</span>
                <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">).fit(X + </span><span class="s4">100</span><span class="s1">)</span>

            <span class="s1">assert_almost_equal(bgmm1.means_</span><span class="s2">, </span><span class="s1">bgmm2.means_ - </span><span class="s4">100</span><span class="s1">)</span>
            <span class="s1">assert_almost_equal(bgmm1.weights_</span><span class="s2">, </span><span class="s1">bgmm2.weights_)</span>
            <span class="s1">assert_almost_equal(bgmm1.covariances_</span><span class="s2">, </span><span class="s1">bgmm2.covariances_)</span>


<span class="s1">@pytest.mark.filterwarnings(</span><span class="s3">&quot;ignore:.*did not converge.*&quot;</span><span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;seed, max_iter, tol&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1e-7</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># strict non-convergence</span>
        <span class="s1">(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1e-1</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># loose non-convergence</span>
        <span class="s1">(</span><span class="s4">3</span><span class="s2">, </span><span class="s4">300</span><span class="s2">, </span><span class="s4">1e-7</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># strict convergence</span>
        <span class="s1">(</span><span class="s4">4</span><span class="s2">, </span><span class="s4">300</span><span class="s2">, </span><span class="s4">1e-1</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># loose convergence</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_bayesian_mixture_fit_predict(seed</span><span class="s2">, </span><span class="s1">max_iter</span><span class="s2">, </span><span class="s1">tol):</span>
    <span class="s1">rng = np.random.RandomState(seed)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">n_samples=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">7</span><span class="s1">)</span>
    <span class="s1">n_components = </span><span class="s4">2 </span><span class="s1">* rand_data.n_components</span>

    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">bgmm1 = BayesianGaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
            <span class="s1">tol=tol</span><span class="s2">,</span>
            <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">bgmm1.covariance_type = covar_type</span>
        <span class="s1">bgmm2 = copy.deepcopy(bgmm1)</span>
        <span class="s1">X = rand_data.X[covar_type]</span>

        <span class="s1">Y_pred1 = bgmm1.fit(X).predict(X)</span>
        <span class="s1">Y_pred2 = bgmm2.fit_predict(X)</span>
        <span class="s1">assert_array_equal(Y_pred1</span><span class="s2">, </span><span class="s1">Y_pred2)</span>


<span class="s2">def </span><span class="s1">test_bayesian_mixture_fit_predict_n_init():</span>
    <span class="s0"># Check that fit_predict is equivalent to fit.predict, when n_init &gt; 1</span>
    <span class="s1">X = np.random.RandomState(</span><span class="s4">0</span><span class="s1">).randn(</span><span class="s4">50</span><span class="s2">, </span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">gm = BayesianGaussianMixture(n_components=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">y_pred1 = gm.fit_predict(X)</span>
    <span class="s1">y_pred2 = gm.predict(X)</span>
    <span class="s1">assert_array_equal(y_pred1</span><span class="s2">, </span><span class="s1">y_pred2)</span>


<span class="s2">def </span><span class="s1">test_bayesian_mixture_predict_predict_proba():</span>
    <span class="s0"># this is the same test as test_gaussian_mixture_predict_predict_proba()</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng)</span>
    <span class="s2">for </span><span class="s1">prior_type </span><span class="s2">in </span><span class="s1">PRIOR_TYPE:</span>
        <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
            <span class="s1">X = rand_data.X[covar_type]</span>
            <span class="s1">Y = rand_data.Y</span>
            <span class="s1">bgmm = BayesianGaussianMixture(</span>
                <span class="s1">n_components=rand_data.n_components</span><span class="s2">,</span>
                <span class="s1">random_state=rng</span><span class="s2">,</span>
                <span class="s1">weight_concentration_prior_type=prior_type</span><span class="s2">,</span>
                <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
            <span class="s1">)</span>

            <span class="s0"># Check a warning message arrive if we don't do fit</span>
            <span class="s1">msg = (</span>
                <span class="s3">&quot;This BayesianGaussianMixture instance is not fitted yet. &quot;</span>
                <span class="s3">&quot;Call 'fit' with appropriate arguments before using this &quot;</span>
                <span class="s3">&quot;estimator.&quot;</span>
            <span class="s1">)</span>
            <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s2">, </span><span class="s1">match=msg):</span>
                <span class="s1">bgmm.predict(X)</span>

            <span class="s1">bgmm.fit(X)</span>
            <span class="s1">Y_pred = bgmm.predict(X)</span>
            <span class="s1">Y_pred_proba = bgmm.predict_proba(X).argmax(axis=</span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">assert_array_equal(Y_pred</span><span class="s2">, </span><span class="s1">Y_pred_proba)</span>
            <span class="s2">assert </span><span class="s1">adjusted_rand_score(Y</span><span class="s2">, </span><span class="s1">Y_pred) &gt;= </span><span class="s4">0.95</span>
</pre>
</body>
</html>