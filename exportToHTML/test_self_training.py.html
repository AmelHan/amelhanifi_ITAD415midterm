<html>
<head>
<title>test_self_training.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_self_training.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">math </span><span class="s0">import </span><span class="s1">ceil</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">assert_array_equal</span>

<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">load_iris</span><span class="s0">, </span><span class="s1">make_blobs</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble </span><span class="s0">import </span><span class="s1">StackingClassifier</span>
<span class="s0">from </span><span class="s1">sklearn.exceptions </span><span class="s0">import </span><span class="s1">NotFittedError</span>
<span class="s0">from </span><span class="s1">sklearn.metrics </span><span class="s0">import </span><span class="s1">accuracy_score</span>
<span class="s0">from </span><span class="s1">sklearn.model_selection </span><span class="s0">import </span><span class="s1">train_test_split</span>
<span class="s0">from </span><span class="s1">sklearn.neighbors </span><span class="s0">import </span><span class="s1">KNeighborsClassifier</span>
<span class="s0">from </span><span class="s1">sklearn.semi_supervised </span><span class="s0">import </span><span class="s1">SelfTrainingClassifier</span>
<span class="s0">from </span><span class="s1">sklearn.svm </span><span class="s0">import </span><span class="s1">SVC</span>

<span class="s2"># Author: Oliver Rausch &lt;rauscho@ethz.ch&gt;</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s2"># load the iris dataset and randomly permute it</span>
<span class="s1">iris = load_iris()</span>
<span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(</span>
    <span class="s1">iris.data</span><span class="s0">, </span><span class="s1">iris.target</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span>
<span class="s1">)</span>

<span class="s1">n_labeled_samples = </span><span class="s3">50</span>

<span class="s1">y_train_missing_labels = y_train.copy()</span>
<span class="s1">y_train_missing_labels[n_labeled_samples:] = -</span><span class="s3">1</span>
<span class="s1">mapping = {</span><span class="s3">0</span><span class="s1">: </span><span class="s4">&quot;A&quot;</span><span class="s0">, </span><span class="s3">1</span><span class="s1">: </span><span class="s4">&quot;B&quot;</span><span class="s0">, </span><span class="s3">2</span><span class="s1">: </span><span class="s4">&quot;C&quot;</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">: </span><span class="s4">&quot;-1&quot;</span><span class="s1">}</span>
<span class="s1">y_train_missing_strings = np.vectorize(mapping.get)(y_train_missing_labels).astype(</span>
    <span class="s1">object</span>
<span class="s1">)</span>
<span class="s1">y_train_missing_strings[y_train_missing_labels == -</span><span class="s3">1</span><span class="s1">] = -</span><span class="s3">1</span>


<span class="s0">def </span><span class="s1">test_warns_k_best():</span>
    <span class="s1">st = SelfTrainingClassifier(KNeighborsClassifier()</span><span class="s0">, </span><span class="s1">criterion=</span><span class="s4">&quot;k_best&quot;</span><span class="s0">, </span><span class="s1">k_best=</span><span class="s3">1000</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">&quot;k_best is larger than&quot;</span><span class="s1">):</span>
        <span class="s1">st.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_missing_labels)</span>

    <span class="s0">assert </span><span class="s1">st.termination_condition_ == </span><span class="s4">&quot;all_labeled&quot;</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;base_estimator&quot;</span><span class="s0">,</span>
    <span class="s1">[KNeighborsClassifier()</span><span class="s0">, </span><span class="s1">SVC(gamma=</span><span class="s4">&quot;scale&quot;</span><span class="s0">, </span><span class="s1">probability=</span><span class="s0">True, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;selection_crit&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;threshold&quot;</span><span class="s0">, </span><span class="s4">&quot;k_best&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_classification(base_estimator</span><span class="s0">, </span><span class="s1">selection_crit):</span>
    <span class="s2"># Check classification for various parameter settings.</span>
    <span class="s2"># Also assert that predictions for strings and numerical labels are equal.</span>
    <span class="s2"># Also test for multioutput classification</span>
    <span class="s1">threshold = </span><span class="s3">0.75</span>
    <span class="s1">max_iter = </span><span class="s3">10</span>
    <span class="s1">st = SelfTrainingClassifier(</span>
        <span class="s1">base_estimator</span><span class="s0">, </span><span class="s1">max_iter=max_iter</span><span class="s0">, </span><span class="s1">threshold=threshold</span><span class="s0">, </span><span class="s1">criterion=selection_crit</span>
    <span class="s1">)</span>
    <span class="s1">st.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_missing_labels)</span>
    <span class="s1">pred = st.predict(X_test)</span>
    <span class="s1">proba = st.predict_proba(X_test)</span>

    <span class="s1">st_string = SelfTrainingClassifier(</span>
        <span class="s1">base_estimator</span><span class="s0">, </span><span class="s1">max_iter=max_iter</span><span class="s0">, </span><span class="s1">criterion=selection_crit</span><span class="s0">, </span><span class="s1">threshold=threshold</span>
    <span class="s1">)</span>
    <span class="s1">st_string.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_missing_strings)</span>
    <span class="s1">pred_string = st_string.predict(X_test)</span>
    <span class="s1">proba_string = st_string.predict_proba(X_test)</span>

    <span class="s1">assert_array_equal(np.vectorize(mapping.get)(pred)</span><span class="s0">, </span><span class="s1">pred_string)</span>
    <span class="s1">assert_array_equal(proba</span><span class="s0">, </span><span class="s1">proba_string)</span>

    <span class="s0">assert </span><span class="s1">st.termination_condition_ == st_string.termination_condition_</span>
    <span class="s2"># Check consistency between labeled_iter, n_iter and max_iter</span>
    <span class="s1">labeled = y_train_missing_labels != -</span><span class="s3">1</span>
    <span class="s2"># assert that labeled samples have labeled_iter = 0</span>
    <span class="s1">assert_array_equal(st.labeled_iter_ == </span><span class="s3">0</span><span class="s0">, </span><span class="s1">labeled)</span>
    <span class="s2"># assert that labeled samples do not change label during training</span>
    <span class="s1">assert_array_equal(y_train_missing_labels[labeled]</span><span class="s0">, </span><span class="s1">st.transduction_[labeled])</span>

    <span class="s2"># assert that the max of the iterations is less than the total amount of</span>
    <span class="s2"># iterations</span>
    <span class="s0">assert </span><span class="s1">np.max(st.labeled_iter_) &lt;= st.n_iter_ &lt;= max_iter</span>
    <span class="s0">assert </span><span class="s1">np.max(st_string.labeled_iter_) &lt;= st_string.n_iter_ &lt;= max_iter</span>

    <span class="s2"># check shapes</span>
    <span class="s0">assert </span><span class="s1">st.labeled_iter_.shape == st.transduction_.shape</span>
    <span class="s0">assert </span><span class="s1">st_string.labeled_iter_.shape == st_string.transduction_.shape</span>


<span class="s0">def </span><span class="s1">test_k_best():</span>
    <span class="s1">st = SelfTrainingClassifier(</span>
        <span class="s1">KNeighborsClassifier(n_neighbors=</span><span class="s3">1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">criterion=</span><span class="s4">&quot;k_best&quot;</span><span class="s0">,</span>
        <span class="s1">k_best=</span><span class="s3">10</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s0">None,</span>
    <span class="s1">)</span>
    <span class="s1">y_train_only_one_label = np.copy(y_train)</span>
    <span class="s1">y_train_only_one_label[</span><span class="s3">1</span><span class="s1">:] = -</span><span class="s3">1</span>
    <span class="s1">n_samples = y_train.shape[</span><span class="s3">0</span><span class="s1">]</span>

    <span class="s1">n_expected_iter = ceil((n_samples - </span><span class="s3">1</span><span class="s1">) / </span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">st.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_only_one_label)</span>
    <span class="s0">assert </span><span class="s1">st.n_iter_ == n_expected_iter</span>

    <span class="s2"># Check labeled_iter_</span>
    <span class="s0">assert </span><span class="s1">np.sum(st.labeled_iter_ == </span><span class="s3">0</span><span class="s1">) == </span><span class="s3">1</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">n_expected_iter):</span>
        <span class="s0">assert </span><span class="s1">np.sum(st.labeled_iter_ == i) == </span><span class="s3">10</span>
    <span class="s0">assert </span><span class="s1">np.sum(st.labeled_iter_ == n_expected_iter) == (n_samples - </span><span class="s3">1</span><span class="s1">) % </span><span class="s3">10</span>
    <span class="s0">assert </span><span class="s1">st.termination_condition_ == </span><span class="s4">&quot;all_labeled&quot;</span>


<span class="s0">def </span><span class="s1">test_sanity_classification():</span>
    <span class="s1">base_estimator = SVC(gamma=</span><span class="s4">&quot;scale&quot;</span><span class="s0">, </span><span class="s1">probability=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">base_estimator.fit(X_train[n_labeled_samples:]</span><span class="s0">, </span><span class="s1">y_train[n_labeled_samples:])</span>

    <span class="s1">st = SelfTrainingClassifier(base_estimator)</span>
    <span class="s1">st.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_missing_labels)</span>

    <span class="s1">pred1</span><span class="s0">, </span><span class="s1">pred2 = base_estimator.predict(X_test)</span><span class="s0">, </span><span class="s1">st.predict(X_test)</span>
    <span class="s0">assert not </span><span class="s1">np.array_equal(pred1</span><span class="s0">, </span><span class="s1">pred2)</span>
    <span class="s1">score_supervised = accuracy_score(base_estimator.predict(X_test)</span><span class="s0">, </span><span class="s1">y_test)</span>
    <span class="s1">score_self_training = accuracy_score(st.predict(X_test)</span><span class="s0">, </span><span class="s1">y_test)</span>

    <span class="s0">assert </span><span class="s1">score_self_training &gt; score_supervised</span>


<span class="s0">def </span><span class="s1">test_none_iter():</span>
    <span class="s2"># Check that the all samples were labeled after a 'reasonable' number of</span>
    <span class="s2"># iterations.</span>
    <span class="s1">st = SelfTrainingClassifier(KNeighborsClassifier()</span><span class="s0">, </span><span class="s1">threshold=</span><span class="s3">0.55</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s0">None</span><span class="s1">)</span>
    <span class="s1">st.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_missing_labels)</span>

    <span class="s0">assert </span><span class="s1">st.n_iter_ &lt; </span><span class="s3">10</span>
    <span class="s0">assert </span><span class="s1">st.termination_condition_ == </span><span class="s4">&quot;all_labeled&quot;</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;base_estimator&quot;</span><span class="s0">,</span>
    <span class="s1">[KNeighborsClassifier()</span><span class="s0">, </span><span class="s1">SVC(gamma=</span><span class="s4">&quot;scale&quot;</span><span class="s0">, </span><span class="s1">probability=</span><span class="s0">True, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;y&quot;</span><span class="s0">, </span><span class="s1">[y_train_missing_labels</span><span class="s0">, </span><span class="s1">y_train_missing_strings])</span>
<span class="s0">def </span><span class="s1">test_zero_iterations(base_estimator</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s2"># Check classification for zero iterations.</span>
    <span class="s2"># Fitting a SelfTrainingClassifier with zero iterations should give the</span>
    <span class="s2"># same results as fitting a supervised classifier.</span>
    <span class="s2"># This also asserts that string arrays work as expected.</span>

    <span class="s1">clf1 = SelfTrainingClassifier(base_estimator</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">clf1.fit(X_train</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">clf2 = base_estimator.fit(X_train[:n_labeled_samples]</span><span class="s0">, </span><span class="s1">y[:n_labeled_samples])</span>

    <span class="s1">assert_array_equal(clf1.predict(X_test)</span><span class="s0">, </span><span class="s1">clf2.predict(X_test))</span>
    <span class="s0">assert </span><span class="s1">clf1.termination_condition_ == </span><span class="s4">&quot;max_iter&quot;</span>


<span class="s0">def </span><span class="s1">test_prefitted_throws_error():</span>
    <span class="s2"># Test that passing a pre-fitted classifier and calling predict throws an</span>
    <span class="s2"># error</span>
    <span class="s1">knn = KNeighborsClassifier()</span>
    <span class="s1">knn.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">st = SelfTrainingClassifier(knn)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">NotFittedError</span><span class="s0">,</span>
        <span class="s1">match=</span><span class="s4">&quot;This SelfTrainingClassifier instance is not fitted yet&quot;</span><span class="s0">,</span>
    <span class="s1">):</span>
        <span class="s1">st.predict(X_train)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;max_iter&quot;</span><span class="s0">, </span><span class="s1">range(</span><span class="s3">1</span><span class="s0">, </span><span class="s3">5</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_labeled_iter(max_iter):</span>
    <span class="s2"># Check that the amount of datapoints labeled in iteration 0 is equal to</span>
    <span class="s2"># the amount of labeled datapoints we passed.</span>
    <span class="s1">st = SelfTrainingClassifier(KNeighborsClassifier()</span><span class="s0">, </span><span class="s1">max_iter=max_iter)</span>

    <span class="s1">st.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_missing_labels)</span>
    <span class="s1">amount_iter_0 = len(st.labeled_iter_[st.labeled_iter_ == </span><span class="s3">0</span><span class="s1">])</span>
    <span class="s0">assert </span><span class="s1">amount_iter_0 == n_labeled_samples</span>
    <span class="s2"># Check that the max of the iterations is less than the total amount of</span>
    <span class="s2"># iterations</span>
    <span class="s0">assert </span><span class="s1">np.max(st.labeled_iter_) &lt;= st.n_iter_ &lt;= max_iter</span>


<span class="s0">def </span><span class="s1">test_no_unlabeled():</span>
    <span class="s2"># Test that training on a fully labeled dataset produces the same results</span>
    <span class="s2"># as training the classifier by itself.</span>
    <span class="s1">knn = KNeighborsClassifier()</span>
    <span class="s1">knn.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">st = SelfTrainingClassifier(knn)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">&quot;y contains no unlabeled samples&quot;</span><span class="s1">):</span>
        <span class="s1">st.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">assert_array_equal(knn.predict(X_test)</span><span class="s0">, </span><span class="s1">st.predict(X_test))</span>
    <span class="s2"># Assert that all samples were labeled in iteration 0 (since there were no</span>
    <span class="s2"># unlabeled samples).</span>
    <span class="s0">assert </span><span class="s1">np.all(st.labeled_iter_ == </span><span class="s3">0</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">st.termination_condition_ == </span><span class="s4">&quot;all_labeled&quot;</span>


<span class="s0">def </span><span class="s1">test_early_stopping():</span>
    <span class="s1">svc = SVC(gamma=</span><span class="s4">&quot;scale&quot;</span><span class="s0">, </span><span class="s1">probability=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">st = SelfTrainingClassifier(svc)</span>
    <span class="s1">X_train_easy = [[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.5</span><span class="s1">]]</span>
    <span class="s1">y_train_easy = [</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s2"># X = [[0.5]] cannot be predicted on with a high confidence, so training</span>
    <span class="s2"># stops early</span>
    <span class="s1">st.fit(X_train_easy</span><span class="s0">, </span><span class="s1">y_train_easy)</span>
    <span class="s0">assert </span><span class="s1">st.n_iter_ == </span><span class="s3">1</span>
    <span class="s0">assert </span><span class="s1">st.termination_condition_ == </span><span class="s4">&quot;no_change&quot;</span>


<span class="s0">def </span><span class="s1">test_strings_dtype():</span>
    <span class="s1">clf = SelfTrainingClassifier(KNeighborsClassifier())</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(n_samples=</span><span class="s3">30</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">cluster_std=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">labels_multiclass = [</span><span class="s4">&quot;one&quot;</span><span class="s0">, </span><span class="s4">&quot;two&quot;</span><span class="s0">, </span><span class="s4">&quot;three&quot;</span><span class="s1">]</span>

    <span class="s1">y_strings = np.take(labels_multiclass</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">&quot;dtype&quot;</span><span class="s1">):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y_strings)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;verbose&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_verbose(capsys</span><span class="s0">, </span><span class="s1">verbose):</span>
    <span class="s1">clf = SelfTrainingClassifier(KNeighborsClassifier()</span><span class="s0">, </span><span class="s1">verbose=verbose)</span>
    <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_missing_labels)</span>

    <span class="s1">captured = capsys.readouterr()</span>

    <span class="s0">if </span><span class="s1">verbose:</span>
        <span class="s0">assert </span><span class="s4">&quot;iteration&quot; </span><span class="s0">in </span><span class="s1">captured.out</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s4">&quot;iteration&quot; </span><span class="s0">not in </span><span class="s1">captured.out</span>


<span class="s0">def </span><span class="s1">test_verbose_k_best(capsys):</span>
    <span class="s1">st = SelfTrainingClassifier(</span>
        <span class="s1">KNeighborsClassifier(n_neighbors=</span><span class="s3">1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">criterion=</span><span class="s4">&quot;k_best&quot;</span><span class="s0">,</span>
        <span class="s1">k_best=</span><span class="s3">10</span><span class="s0">,</span>
        <span class="s1">verbose=</span><span class="s0">True,</span>
        <span class="s1">max_iter=</span><span class="s0">None,</span>
    <span class="s1">)</span>

    <span class="s1">y_train_only_one_label = np.copy(y_train)</span>
    <span class="s1">y_train_only_one_label[</span><span class="s3">1</span><span class="s1">:] = -</span><span class="s3">1</span>
    <span class="s1">n_samples = y_train.shape[</span><span class="s3">0</span><span class="s1">]</span>

    <span class="s1">n_expected_iter = ceil((n_samples - </span><span class="s3">1</span><span class="s1">) / </span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">st.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_only_one_label)</span>

    <span class="s1">captured = capsys.readouterr()</span>

    <span class="s1">msg = </span><span class="s4">&quot;End of iteration {}, added {} new labels.&quot;</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">n_expected_iter):</span>
        <span class="s0">assert </span><span class="s1">msg.format(i</span><span class="s0">, </span><span class="s3">10</span><span class="s1">) </span><span class="s0">in </span><span class="s1">captured.out</span>

    <span class="s0">assert </span><span class="s1">msg.format(n_expected_iter</span><span class="s0">, </span><span class="s1">(n_samples - </span><span class="s3">1</span><span class="s1">) % </span><span class="s3">10</span><span class="s1">) </span><span class="s0">in </span><span class="s1">captured.out</span>


<span class="s0">def </span><span class="s1">test_k_best_selects_best():</span>
    <span class="s2"># Tests that the labels added by st really are the 10 best labels.</span>
    <span class="s1">svc = SVC(gamma=</span><span class="s4">&quot;scale&quot;</span><span class="s0">, </span><span class="s1">probability=</span><span class="s0">True, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">st = SelfTrainingClassifier(svc</span><span class="s0">, </span><span class="s1">criterion=</span><span class="s4">&quot;k_best&quot;</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">k_best=</span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">has_label = y_train_missing_labels != -</span><span class="s3">1</span>
    <span class="s1">st.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_missing_labels)</span>

    <span class="s1">got_label = ~has_label &amp; (st.transduction_ != -</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">svc.fit(X_train[has_label]</span><span class="s0">, </span><span class="s1">y_train_missing_labels[has_label])</span>
    <span class="s1">pred = svc.predict_proba(X_train[~has_label])</span>
    <span class="s1">max_proba = np.max(pred</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">most_confident_svc = X_train[~has_label][np.argsort(max_proba)[-</span><span class="s3">10</span><span class="s1">:]]</span>
    <span class="s1">added_by_st = X_train[np.where(got_label)].tolist()</span>

    <span class="s0">for </span><span class="s1">row </span><span class="s0">in </span><span class="s1">most_confident_svc.tolist():</span>
        <span class="s0">assert </span><span class="s1">row </span><span class="s0">in </span><span class="s1">added_by_st</span>


<span class="s0">def </span><span class="s1">test_base_estimator_meta_estimator():</span>
    <span class="s2"># Check that a meta-estimator relying on an estimator implementing</span>
    <span class="s2"># `predict_proba` will work even if it does not expose this method before being</span>
    <span class="s2"># fitted.</span>
    <span class="s2"># Non-regression test for:</span>
    <span class="s2"># https://github.com/scikit-learn/scikit-learn/issues/19119</span>

    <span class="s1">base_estimator = StackingClassifier(</span>
        <span class="s1">estimators=[</span>
            <span class="s1">(</span><span class="s4">&quot;svc_1&quot;</span><span class="s0">, </span><span class="s1">SVC(probability=</span><span class="s0">True</span><span class="s1">))</span><span class="s0">,</span>
            <span class="s1">(</span><span class="s4">&quot;svc_2&quot;</span><span class="s0">, </span><span class="s1">SVC(probability=</span><span class="s0">True</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">]</span><span class="s0">,</span>
        <span class="s1">final_estimator=SVC(probability=</span><span class="s0">True</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">cv=</span><span class="s3">2</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">hasattr(base_estimator</span><span class="s0">, </span><span class="s4">&quot;predict_proba&quot;</span><span class="s1">)</span>
    <span class="s1">clf = SelfTrainingClassifier(base_estimator=base_estimator)</span>
    <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_missing_labels)</span>
    <span class="s1">clf.predict_proba(X_test)</span>

    <span class="s1">base_estimator = StackingClassifier(</span>
        <span class="s1">estimators=[</span>
            <span class="s1">(</span><span class="s4">&quot;svc_1&quot;</span><span class="s0">, </span><span class="s1">SVC(probability=</span><span class="s0">False</span><span class="s1">))</span><span class="s0">,</span>
            <span class="s1">(</span><span class="s4">&quot;svc_2&quot;</span><span class="s0">, </span><span class="s1">SVC(probability=</span><span class="s0">False</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">]</span><span class="s0">,</span>
        <span class="s1">final_estimator=SVC(probability=</span><span class="s0">False</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">cv=</span><span class="s3">2</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">assert not </span><span class="s1">hasattr(base_estimator</span><span class="s0">, </span><span class="s4">&quot;predict_proba&quot;</span><span class="s1">)</span>
    <span class="s1">clf = SelfTrainingClassifier(base_estimator=base_estimator)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(AttributeError):</span>
        <span class="s1">clf.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_missing_labels)</span>


<span class="s0">def </span><span class="s1">test_missing_predict_proba():</span>
    <span class="s2"># Check that an error is thrown if predict_proba is not implemented</span>
    <span class="s1">base_estimator = SVC(probability=</span><span class="s0">False, </span><span class="s1">gamma=</span><span class="s4">&quot;scale&quot;</span><span class="s1">)</span>
    <span class="s1">self_training = SelfTrainingClassifier(base_estimator)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(AttributeError</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">&quot;predict_proba is not available&quot;</span><span class="s1">):</span>
        <span class="s1">self_training.fit(X_train</span><span class="s0">, </span><span class="s1">y_train_missing_labels)</span>
</pre>
</body>
</html>