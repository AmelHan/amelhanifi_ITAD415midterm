<html>
<head>
<title>test_extmath.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_extmath.py</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span>
<span class="s0">#          Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span>
<span class="s0">#          Denis Engemann &lt;denis-alexander.engemann@inria.fr&gt;</span>
<span class="s0">#</span>
<span class="s0"># License: BSD 3 clause</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">linalg</span><span class="s2">, </span><span class="s1">sparse</span>
<span class="s2">from </span><span class="s1">scipy.linalg </span><span class="s2">import </span><span class="s1">eigh</span>
<span class="s2">from </span><span class="s1">scipy.sparse.linalg </span><span class="s2">import </span><span class="s1">eigsh</span>
<span class="s2">from </span><span class="s1">scipy.special </span><span class="s2">import </span><span class="s1">expit</span>

<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">make_low_rank_matrix</span><span class="s2">, </span><span class="s1">make_sparse_spd_matrix</span>
<span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">gen_batches</span>
<span class="s2">from </span><span class="s1">sklearn.utils._arpack </span><span class="s2">import </span><span class="s1">_init_arpack_v0</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s2">,</span>
    <span class="s1">assert_allclose_dense_sparse</span><span class="s2">,</span>
    <span class="s1">assert_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_equal</span><span class="s2">,</span>
    <span class="s1">skip_if_32bit</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.utils.extmath </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">_deterministic_vector_sign_flip</span><span class="s2">,</span>
    <span class="s1">_incremental_mean_and_var</span><span class="s2">,</span>
    <span class="s1">_randomized_eigsh</span><span class="s2">,</span>
    <span class="s1">_safe_accumulator_op</span><span class="s2">,</span>
    <span class="s1">cartesian</span><span class="s2">,</span>
    <span class="s1">density</span><span class="s2">,</span>
    <span class="s1">log_logistic</span><span class="s2">,</span>
    <span class="s1">randomized_svd</span><span class="s2">,</span>
    <span class="s1">row_norms</span><span class="s2">,</span>
    <span class="s1">safe_sparse_dot</span><span class="s2">,</span>
    <span class="s1">softmax</span><span class="s2">,</span>
    <span class="s1">stable_cumsum</span><span class="s2">,</span>
    <span class="s1">svd_flip</span><span class="s2">,</span>
    <span class="s1">weighted_mode</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.utils.fixes </span><span class="s2">import </span><span class="s1">_mode</span>


<span class="s2">def </span><span class="s1">test_density():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randint(</span><span class="s3">10</span><span class="s2">, </span><span class="s1">size=(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">5</span><span class="s1">))</span>
    <span class="s1">X[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s1">] = </span><span class="s3">0</span>
    <span class="s1">X[</span><span class="s3">5</span><span class="s2">, </span><span class="s3">3</span><span class="s1">] = </span><span class="s3">0</span>
    <span class="s1">X_csr = sparse.csr_matrix(X)</span>
    <span class="s1">X_csc = sparse.csc_matrix(X)</span>
    <span class="s1">X_coo = sparse.coo_matrix(X)</span>
    <span class="s1">X_lil = sparse.lil_matrix(X)</span>

    <span class="s2">for </span><span class="s1">X_ </span><span class="s2">in </span><span class="s1">(X_csr</span><span class="s2">, </span><span class="s1">X_csc</span><span class="s2">, </span><span class="s1">X_coo</span><span class="s2">, </span><span class="s1">X_lil):</span>
        <span class="s2">assert </span><span class="s1">density(X_) == density(X)</span>


<span class="s0"># TODO(1.4): Remove test</span>
<span class="s2">def </span><span class="s1">test_density_deprecated_kwargs():</span>
    <span class="s4">&quot;&quot;&quot;Check that future warning is raised when user enters keyword arguments.&quot;&quot;&quot;</span>
    <span class="s1">test_array = np.array([[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">4</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">6</span><span class="s1">]])</span>
    <span class="s2">with </span><span class="s1">pytest.warns(</span>
        <span class="s1">FutureWarning</span><span class="s2">,</span>
        <span class="s1">match=(</span>
            <span class="s5">&quot;Additional keyword arguments are deprecated in version 1.2 and will be&quot;</span>
            <span class="s5">&quot; removed in version 1.4.&quot;</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">density(test_array</span><span class="s2">, </span><span class="s1">a=</span><span class="s3">1</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_uniform_weights():</span>
    <span class="s0"># with uniform weights, results should be identical to stats.mode</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">x = rng.randint(</span><span class="s3">10</span><span class="s2">, </span><span class="s1">size=(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">5</span><span class="s1">))</span>
    <span class="s1">weights = np.ones(x.shape)</span>

    <span class="s2">for </span><span class="s1">axis </span><span class="s2">in </span><span class="s1">(</span><span class="s2">None, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s1">):</span>
        <span class="s1">mode</span><span class="s2">, </span><span class="s1">score = _mode(x</span><span class="s2">, </span><span class="s1">axis)</span>
        <span class="s1">mode2</span><span class="s2">, </span><span class="s1">score2 = weighted_mode(x</span><span class="s2">, </span><span class="s1">weights</span><span class="s2">, </span><span class="s1">axis=axis)</span>

        <span class="s1">assert_array_equal(mode</span><span class="s2">, </span><span class="s1">mode2)</span>
        <span class="s1">assert_array_equal(score</span><span class="s2">, </span><span class="s1">score2)</span>


<span class="s2">def </span><span class="s1">test_random_weights():</span>
    <span class="s0"># set this up so that each row should have a weighted mode of 6,</span>
    <span class="s0"># with a score that is easily reproduced</span>
    <span class="s1">mode_result = </span><span class="s3">6</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">x = rng.randint(mode_result</span><span class="s2">, </span><span class="s1">size=(</span><span class="s3">100</span><span class="s2">, </span><span class="s3">10</span><span class="s1">))</span>
    <span class="s1">w = rng.random_sample(x.shape)</span>

    <span class="s1">x[:</span><span class="s2">, </span><span class="s1">:</span><span class="s3">5</span><span class="s1">] = mode_result</span>
    <span class="s1">w[:</span><span class="s2">, </span><span class="s1">:</span><span class="s3">5</span><span class="s1">] += </span><span class="s3">1</span>

    <span class="s1">mode</span><span class="s2">, </span><span class="s1">score = weighted_mode(x</span><span class="s2">, </span><span class="s1">w</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">assert_array_equal(mode</span><span class="s2">, </span><span class="s1">mode_result)</span>
    <span class="s1">assert_array_almost_equal(score.ravel()</span><span class="s2">, </span><span class="s1">w[:</span><span class="s2">, </span><span class="s1">:</span><span class="s3">5</span><span class="s1">].sum(</span><span class="s3">1</span><span class="s1">))</span>


<span class="s2">def </span><span class="s1">check_randomized_svd_low_rank(dtype):</span>
    <span class="s0"># Check that extmath.randomized_svd is consistent with linalg.svd</span>
    <span class="s1">n_samples = </span><span class="s3">100</span>
    <span class="s1">n_features = </span><span class="s3">500</span>
    <span class="s1">rank = </span><span class="s3">5</span>
    <span class="s1">k = </span><span class="s3">10</span>
    <span class="s1">decimal = </span><span class="s3">5 </span><span class="s2">if </span><span class="s1">dtype == np.float32 </span><span class="s2">else </span><span class="s3">7</span>
    <span class="s1">dtype = np.dtype(dtype)</span>

    <span class="s0"># generate a matrix X of approximate effective rank `rank` and no noise</span>
    <span class="s0"># component (very structured signal):</span>
    <span class="s1">X = make_low_rank_matrix(</span>
        <span class="s1">n_samples=n_samples</span><span class="s2">,</span>
        <span class="s1">n_features=n_features</span><span class="s2">,</span>
        <span class="s1">effective_rank=rank</span><span class="s2">,</span>
        <span class="s1">tail_strength=</span><span class="s3">0.0</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s2">,</span>
    <span class="s1">).astype(dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">X.shape == (n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>

    <span class="s0"># compute the singular values of X using the slow exact method</span>
    <span class="s1">U</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">Vt = linalg.svd(X</span><span class="s2">, </span><span class="s1">full_matrices=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s0"># Convert the singular values to the specific dtype</span>
    <span class="s1">U = U.astype(dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">s = s.astype(dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">Vt = Vt.astype(dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">normalizer </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s5">&quot;LU&quot;</span><span class="s2">, </span><span class="s5">&quot;QR&quot;</span><span class="s1">]:  </span><span class="s0"># 'none' would not be stable</span>
        <span class="s0"># compute the singular values of X using the fast approximate method</span>
        <span class="s1">Ua</span><span class="s2">, </span><span class="s1">sa</span><span class="s2">, </span><span class="s1">Va = randomized_svd(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">power_iteration_normalizer=normalizer</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span>
        <span class="s1">)</span>

        <span class="s0"># If the input dtype is float, then the output dtype is float of the</span>
        <span class="s0"># same bit size (f32 is not upcast to f64)</span>
        <span class="s0"># But if the input dtype is int, the output dtype is float64</span>
        <span class="s2">if </span><span class="s1">dtype.kind == </span><span class="s5">&quot;f&quot;</span><span class="s1">:</span>
            <span class="s2">assert </span><span class="s1">Ua.dtype == dtype</span>
            <span class="s2">assert </span><span class="s1">sa.dtype == dtype</span>
            <span class="s2">assert </span><span class="s1">Va.dtype == dtype</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">assert </span><span class="s1">Ua.dtype == np.float64</span>
            <span class="s2">assert </span><span class="s1">sa.dtype == np.float64</span>
            <span class="s2">assert </span><span class="s1">Va.dtype == np.float64</span>

        <span class="s2">assert </span><span class="s1">Ua.shape == (n_samples</span><span class="s2">, </span><span class="s1">k)</span>
        <span class="s2">assert </span><span class="s1">sa.shape == (k</span><span class="s2">,</span><span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">Va.shape == (k</span><span class="s2">, </span><span class="s1">n_features)</span>

        <span class="s0"># ensure that the singular values of both methods are equal up to the</span>
        <span class="s0"># real rank of the matrix</span>
        <span class="s1">assert_almost_equal(s[:k]</span><span class="s2">, </span><span class="s1">sa</span><span class="s2">, </span><span class="s1">decimal=decimal)</span>

        <span class="s0"># check the singular vectors too (while not checking the sign)</span>
        <span class="s1">assert_almost_equal(</span>
            <span class="s1">np.dot(U[:</span><span class="s2">, </span><span class="s1">:k]</span><span class="s2">, </span><span class="s1">Vt[:k</span><span class="s2">, </span><span class="s1">:])</span><span class="s2">, </span><span class="s1">np.dot(Ua</span><span class="s2">, </span><span class="s1">Va)</span><span class="s2">, </span><span class="s1">decimal=decimal</span>
        <span class="s1">)</span>

        <span class="s0"># check the sparse matrix representation</span>
        <span class="s1">X = sparse.csr_matrix(X)</span>

        <span class="s0"># compute the singular values of X using the fast approximate method</span>
        <span class="s1">Ua</span><span class="s2">, </span><span class="s1">sa</span><span class="s2">, </span><span class="s1">Va = randomized_svd(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">power_iteration_normalizer=normalizer</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span>
        <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">dtype.kind == </span><span class="s5">&quot;f&quot;</span><span class="s1">:</span>
            <span class="s2">assert </span><span class="s1">Ua.dtype == dtype</span>
            <span class="s2">assert </span><span class="s1">sa.dtype == dtype</span>
            <span class="s2">assert </span><span class="s1">Va.dtype == dtype</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">assert </span><span class="s1">Ua.dtype.kind == </span><span class="s5">&quot;f&quot;</span>
            <span class="s2">assert </span><span class="s1">sa.dtype.kind == </span><span class="s5">&quot;f&quot;</span>
            <span class="s2">assert </span><span class="s1">Va.dtype.kind == </span><span class="s5">&quot;f&quot;</span>

        <span class="s1">assert_almost_equal(s[:rank]</span><span class="s2">, </span><span class="s1">sa[:rank]</span><span class="s2">, </span><span class="s1">decimal=decimal)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;dtype&quot;</span><span class="s2">, </span><span class="s1">(np.int32</span><span class="s2">, </span><span class="s1">np.int64</span><span class="s2">, </span><span class="s1">np.float32</span><span class="s2">, </span><span class="s1">np.float64))</span>
<span class="s2">def </span><span class="s1">test_randomized_svd_low_rank_all_dtypes(dtype):</span>
    <span class="s1">check_randomized_svd_low_rank(dtype)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;dtype&quot;</span><span class="s2">, </span><span class="s1">(np.int32</span><span class="s2">, </span><span class="s1">np.int64</span><span class="s2">, </span><span class="s1">np.float32</span><span class="s2">, </span><span class="s1">np.float64))</span>
<span class="s2">def </span><span class="s1">test_randomized_eigsh(dtype):</span>
    <span class="s4">&quot;&quot;&quot;Test that `_randomized_eigsh` returns the appropriate components&quot;&quot;&quot;</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">X = np.diag(np.array([</span><span class="s3">1.0</span><span class="s2">, </span><span class="s1">-</span><span class="s3">2.0</span><span class="s2">, </span><span class="s3">0.0</span><span class="s2">, </span><span class="s3">3.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=dtype))</span>
    <span class="s0"># random rotation that preserves the eigenvalues of X</span>
    <span class="s1">rand_rot = np.linalg.qr(rng.normal(size=X.shape))[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">X = rand_rot @ X @ rand_rot.T</span>

    <span class="s0"># with 'module' selection method, the negative eigenvalue shows up</span>
    <span class="s1">eigvals</span><span class="s2">, </span><span class="s1">eigvecs = _randomized_eigsh(X</span><span class="s2">, </span><span class="s1">n_components=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">selection=</span><span class="s5">&quot;module&quot;</span><span class="s1">)</span>
    <span class="s0"># eigenvalues</span>
    <span class="s2">assert </span><span class="s1">eigvals.shape == (</span><span class="s3">2</span><span class="s2">,</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(eigvals</span><span class="s2">, </span><span class="s1">[</span><span class="s3">3.0</span><span class="s2">, </span><span class="s1">-</span><span class="s3">2.0</span><span class="s1">])  </span><span class="s0"># negative eigenvalue here</span>
    <span class="s0"># eigenvectors</span>
    <span class="s2">assert </span><span class="s1">eigvecs.shape == (</span><span class="s3">4</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span>

    <span class="s0"># with 'value' selection method, the negative eigenvalue does not show up</span>
    <span class="s2">with </span><span class="s1">pytest.raises(NotImplementedError):</span>
        <span class="s1">_randomized_eigsh(X</span><span class="s2">, </span><span class="s1">n_components=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">selection=</span><span class="s5">&quot;value&quot;</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;k&quot;</span><span class="s2">, </span><span class="s1">(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">50</span><span class="s2">, </span><span class="s3">100</span><span class="s2">, </span><span class="s3">199</span><span class="s2">, </span><span class="s3">200</span><span class="s1">))</span>
<span class="s2">def </span><span class="s1">test_randomized_eigsh_compared_to_others(k):</span>
    <span class="s4">&quot;&quot;&quot;Check that `_randomized_eigsh` is similar to other `eigsh` 
 
    Tests that for a random PSD matrix, `_randomized_eigsh` provides results 
    comparable to LAPACK (scipy.linalg.eigh) and ARPACK 
    (scipy.sparse.linalg.eigsh). 
 
    Note: some versions of ARPACK do not support k=n_features. 
    &quot;&quot;&quot;</span>

    <span class="s0"># make a random PSD matrix</span>
    <span class="s1">n_features = </span><span class="s3">200</span>
    <span class="s1">X = make_sparse_spd_matrix(n_features</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s0"># compare two versions of randomized</span>
    <span class="s0"># rough and fast</span>
    <span class="s1">eigvals</span><span class="s2">, </span><span class="s1">eigvecs = _randomized_eigsh(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">n_components=k</span><span class="s2">, </span><span class="s1">selection=</span><span class="s5">&quot;module&quot;</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s3">25</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span>
    <span class="s1">)</span>
    <span class="s0"># more accurate but slow (TODO find realistic settings here)</span>
    <span class="s1">eigvals_qr</span><span class="s2">, </span><span class="s1">eigvecs_qr = _randomized_eigsh(</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">n_components=k</span><span class="s2">,</span>
        <span class="s1">n_iter=</span><span class="s3">25</span><span class="s2">,</span>
        <span class="s1">n_oversamples=</span><span class="s3">20</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s2">,</span>
        <span class="s1">power_iteration_normalizer=</span><span class="s5">&quot;QR&quot;</span><span class="s2">,</span>
        <span class="s1">selection=</span><span class="s5">&quot;module&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s0"># with LAPACK</span>
    <span class="s1">eigvals_lapack</span><span class="s2">, </span><span class="s1">eigvecs_lapack = eigh(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">subset_by_index=(n_features - k</span><span class="s2">, </span><span class="s1">n_features - </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">)</span>
    <span class="s1">indices = eigvals_lapack.argsort()[::-</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">eigvals_lapack = eigvals_lapack[indices]</span>
    <span class="s1">eigvecs_lapack = eigvecs_lapack[:</span><span class="s2">, </span><span class="s1">indices]</span>

    <span class="s0"># -- eigenvalues comparison</span>
    <span class="s2">assert </span><span class="s1">eigvals_lapack.shape == (k</span><span class="s2">,</span><span class="s1">)</span>
    <span class="s0"># comparison precision</span>
    <span class="s1">assert_array_almost_equal(eigvals</span><span class="s2">, </span><span class="s1">eigvals_lapack</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">6</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(eigvals_qr</span><span class="s2">, </span><span class="s1">eigvals_lapack</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">6</span><span class="s1">)</span>

    <span class="s0"># -- eigenvectors comparison</span>
    <span class="s2">assert </span><span class="s1">eigvecs_lapack.shape == (n_features</span><span class="s2">, </span><span class="s1">k)</span>
    <span class="s0"># flip eigenvectors' sign to enforce deterministic output</span>
    <span class="s1">dummy_vecs = np.zeros_like(eigvecs).T</span>
    <span class="s1">eigvecs</span><span class="s2">, </span><span class="s1">_ = svd_flip(eigvecs</span><span class="s2">, </span><span class="s1">dummy_vecs)</span>
    <span class="s1">eigvecs_qr</span><span class="s2">, </span><span class="s1">_ = svd_flip(eigvecs_qr</span><span class="s2">, </span><span class="s1">dummy_vecs)</span>
    <span class="s1">eigvecs_lapack</span><span class="s2">, </span><span class="s1">_ = svd_flip(eigvecs_lapack</span><span class="s2">, </span><span class="s1">dummy_vecs)</span>
    <span class="s1">assert_array_almost_equal(eigvecs</span><span class="s2">, </span><span class="s1">eigvecs_lapack</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">4</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(eigvecs_qr</span><span class="s2">, </span><span class="s1">eigvecs_lapack</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">6</span><span class="s1">)</span>

    <span class="s0"># comparison ARPACK ~ LAPACK (some ARPACK implems do not support k=n)</span>
    <span class="s2">if </span><span class="s1">k &lt; n_features:</span>
        <span class="s1">v0 = _init_arpack_v0(n_features</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s0"># &quot;LA&quot; largest algebraic &lt;=&gt; selection=&quot;value&quot; in randomized_eigsh</span>
        <span class="s1">eigvals_arpack</span><span class="s2">, </span><span class="s1">eigvecs_arpack = eigsh(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">which=</span><span class="s5">&quot;LA&quot;</span><span class="s2">, </span><span class="s1">tol=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s2">None, </span><span class="s1">v0=v0</span>
        <span class="s1">)</span>
        <span class="s1">indices = eigvals_arpack.argsort()[::-</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s0"># eigenvalues</span>
        <span class="s1">eigvals_arpack = eigvals_arpack[indices]</span>
        <span class="s1">assert_array_almost_equal(eigvals_lapack</span><span class="s2">, </span><span class="s1">eigvals_arpack</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">10</span><span class="s1">)</span>
        <span class="s0"># eigenvectors</span>
        <span class="s1">eigvecs_arpack = eigvecs_arpack[:</span><span class="s2">, </span><span class="s1">indices]</span>
        <span class="s1">eigvecs_arpack</span><span class="s2">, </span><span class="s1">_ = svd_flip(eigvecs_arpack</span><span class="s2">, </span><span class="s1">dummy_vecs)</span>
        <span class="s1">assert_array_almost_equal(eigvecs_arpack</span><span class="s2">, </span><span class="s1">eigvecs_lapack</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">8</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;n,rank&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">7</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s3">100</span><span class="s2">, </span><span class="s3">10</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s3">100</span><span class="s2">, </span><span class="s3">80</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s3">500</span><span class="s2">, </span><span class="s3">10</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s3">500</span><span class="s2">, </span><span class="s3">250</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s3">500</span><span class="s2">, </span><span class="s3">400</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_randomized_eigsh_reconst_low_rank(n</span><span class="s2">, </span><span class="s1">rank):</span>
    <span class="s4">&quot;&quot;&quot;Check that randomized_eigsh is able to reconstruct a low rank psd matrix 
 
    Tests that the decomposition provided by `_randomized_eigsh` leads to 
    orthonormal eigenvectors, and that a low rank PSD matrix can be effectively 
    reconstructed with good accuracy using it. 
    &quot;&quot;&quot;</span>
    <span class="s2">assert </span><span class="s1">rank &lt; n</span>

    <span class="s0"># create a low rank PSD</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">69</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(n</span><span class="s2">, </span><span class="s1">rank)</span>
    <span class="s1">A = X @ X.T</span>

    <span class="s0"># approximate A with the &quot;right&quot; number of components</span>
    <span class="s1">S</span><span class="s2">, </span><span class="s1">V = _randomized_eigsh(A</span><span class="s2">, </span><span class="s1">n_components=rank</span><span class="s2">, </span><span class="s1">random_state=rng)</span>
    <span class="s0"># orthonormality checks</span>
    <span class="s1">assert_array_almost_equal(np.linalg.norm(V</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.ones(S.shape))</span>
    <span class="s1">assert_array_almost_equal(V.T @ V</span><span class="s2">, </span><span class="s1">np.diag(np.ones(S.shape)))</span>
    <span class="s0"># reconstruction</span>
    <span class="s1">A_reconstruct = V @ np.diag(S) @ V.T</span>

    <span class="s0"># test that the approximation is good</span>
    <span class="s1">assert_array_almost_equal(A_reconstruct</span><span class="s2">, </span><span class="s1">A</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">6</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;dtype&quot;</span><span class="s2">, </span><span class="s1">(np.float32</span><span class="s2">, </span><span class="s1">np.float64))</span>
<span class="s2">def </span><span class="s1">test_row_norms(dtype):</span>
    <span class="s1">X = np.random.RandomState(</span><span class="s3">42</span><span class="s1">).randn(</span><span class="s3">100</span><span class="s2">, </span><span class="s3">100</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">dtype </span><span class="s2">is </span><span class="s1">np.float32:</span>
        <span class="s1">precision = </span><span class="s3">4</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">precision = </span><span class="s3">5</span>

    <span class="s1">X = X.astype(dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">sq_norm = (X**</span><span class="s3">2</span><span class="s1">).sum(axis=</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(sq_norm</span><span class="s2">, </span><span class="s1">row_norms(X</span><span class="s2">, </span><span class="s1">squared=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">, </span><span class="s1">precision)</span>
    <span class="s1">assert_array_almost_equal(np.sqrt(sq_norm)</span><span class="s2">, </span><span class="s1">row_norms(X)</span><span class="s2">, </span><span class="s1">precision)</span>

    <span class="s2">for </span><span class="s1">csr_index_dtype </span><span class="s2">in </span><span class="s1">[np.int32</span><span class="s2">, </span><span class="s1">np.int64]:</span>
        <span class="s1">Xcsr = sparse.csr_matrix(X</span><span class="s2">, </span><span class="s1">dtype=dtype)</span>
        <span class="s0"># csr_matrix will use int32 indices by default,</span>
        <span class="s0"># up-casting those to int64 when necessary</span>
        <span class="s2">if </span><span class="s1">csr_index_dtype </span><span class="s2">is </span><span class="s1">np.int64:</span>
            <span class="s1">Xcsr.indptr = Xcsr.indptr.astype(csr_index_dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
            <span class="s1">Xcsr.indices = Xcsr.indices.astype(csr_index_dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">Xcsr.indices.dtype == csr_index_dtype</span>
        <span class="s2">assert </span><span class="s1">Xcsr.indptr.dtype == csr_index_dtype</span>
        <span class="s1">assert_array_almost_equal(sq_norm</span><span class="s2">, </span><span class="s1">row_norms(Xcsr</span><span class="s2">, </span><span class="s1">squared=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">, </span><span class="s1">precision)</span>
        <span class="s1">assert_array_almost_equal(np.sqrt(sq_norm)</span><span class="s2">, </span><span class="s1">row_norms(Xcsr)</span><span class="s2">, </span><span class="s1">precision)</span>


<span class="s2">def </span><span class="s1">test_randomized_svd_low_rank_with_noise():</span>
    <span class="s0"># Check that extmath.randomized_svd can handle noisy matrices</span>
    <span class="s1">n_samples = </span><span class="s3">100</span>
    <span class="s1">n_features = </span><span class="s3">500</span>
    <span class="s1">rank = </span><span class="s3">5</span>
    <span class="s1">k = </span><span class="s3">10</span>

    <span class="s0"># generate a matrix X wity structure approximate rank `rank` and an</span>
    <span class="s0"># important noisy component</span>
    <span class="s1">X = make_low_rank_matrix(</span>
        <span class="s1">n_samples=n_samples</span><span class="s2">,</span>
        <span class="s1">n_features=n_features</span><span class="s2">,</span>
        <span class="s1">effective_rank=rank</span><span class="s2">,</span>
        <span class="s1">tail_strength=</span><span class="s3">0.1</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">X.shape == (n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>

    <span class="s0"># compute the singular values of X using the slow exact method</span>
    <span class="s1">_</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">_ = linalg.svd(X</span><span class="s2">, </span><span class="s1">full_matrices=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">normalizer </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s5">&quot;none&quot;</span><span class="s2">, </span><span class="s5">&quot;LU&quot;</span><span class="s2">, </span><span class="s5">&quot;QR&quot;</span><span class="s1">]:</span>
        <span class="s0"># compute the singular values of X using the fast approximate</span>
        <span class="s0"># method without the iterated power method</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">sa</span><span class="s2">, </span><span class="s1">_ = randomized_svd(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">power_iteration_normalizer=normalizer</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span>
        <span class="s1">)</span>

        <span class="s0"># the approximation does not tolerate the noise:</span>
        <span class="s2">assert </span><span class="s1">np.abs(s[:k] - sa).max() &gt; </span><span class="s3">0.01</span>

        <span class="s0"># compute the singular values of X using the fast approximate</span>
        <span class="s0"># method with iterated power method</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">sap</span><span class="s2">, </span><span class="s1">_ = randomized_svd(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">power_iteration_normalizer=normalizer</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span>
        <span class="s1">)</span>

        <span class="s0"># the iterated power method is helping getting rid of the noise:</span>
        <span class="s1">assert_almost_equal(s[:k]</span><span class="s2">, </span><span class="s1">sap</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">3</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_randomized_svd_infinite_rank():</span>
    <span class="s0"># Check that extmath.randomized_svd can handle noisy matrices</span>
    <span class="s1">n_samples = </span><span class="s3">100</span>
    <span class="s1">n_features = </span><span class="s3">500</span>
    <span class="s1">rank = </span><span class="s3">5</span>
    <span class="s1">k = </span><span class="s3">10</span>

    <span class="s0"># let us try again without 'low_rank component': just regularly but slowly</span>
    <span class="s0"># decreasing singular values: the rank of the data matrix is infinite</span>
    <span class="s1">X = make_low_rank_matrix(</span>
        <span class="s1">n_samples=n_samples</span><span class="s2">,</span>
        <span class="s1">n_features=n_features</span><span class="s2">,</span>
        <span class="s1">effective_rank=rank</span><span class="s2">,</span>
        <span class="s1">tail_strength=</span><span class="s3">1.0</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">X.shape == (n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>

    <span class="s0"># compute the singular values of X using the slow exact method</span>
    <span class="s1">_</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">_ = linalg.svd(X</span><span class="s2">, </span><span class="s1">full_matrices=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s2">for </span><span class="s1">normalizer </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s5">&quot;none&quot;</span><span class="s2">, </span><span class="s5">&quot;LU&quot;</span><span class="s2">, </span><span class="s5">&quot;QR&quot;</span><span class="s1">]:</span>
        <span class="s0"># compute the singular values of X using the fast approximate method</span>
        <span class="s0"># without the iterated power method</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">sa</span><span class="s2">, </span><span class="s1">_ = randomized_svd(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">power_iteration_normalizer=normalizer</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span>
        <span class="s1">)</span>

        <span class="s0"># the approximation does not tolerate the noise:</span>
        <span class="s2">assert </span><span class="s1">np.abs(s[:k] - sa).max() &gt; </span><span class="s3">0.1</span>

        <span class="s0"># compute the singular values of X using the fast approximate method</span>
        <span class="s0"># with iterated power method</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">sap</span><span class="s2">, </span><span class="s1">_ = randomized_svd(</span>
            <span class="s1">X</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s3">5</span><span class="s2">, </span><span class="s1">power_iteration_normalizer=normalizer</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span>
        <span class="s1">)</span>

        <span class="s0"># the iterated power method is still managing to get most of the</span>
        <span class="s0"># structure at the requested rank</span>
        <span class="s1">assert_almost_equal(s[:k]</span><span class="s2">, </span><span class="s1">sap</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">3</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_randomized_svd_transpose_consistency():</span>
    <span class="s0"># Check that transposing the design matrix has limited impact</span>
    <span class="s1">n_samples = </span><span class="s3">100</span>
    <span class="s1">n_features = </span><span class="s3">500</span>
    <span class="s1">rank = </span><span class="s3">4</span>
    <span class="s1">k = </span><span class="s3">10</span>

    <span class="s1">X = make_low_rank_matrix(</span>
        <span class="s1">n_samples=n_samples</span><span class="s2">,</span>
        <span class="s1">n_features=n_features</span><span class="s2">,</span>
        <span class="s1">effective_rank=rank</span><span class="s2">,</span>
        <span class="s1">tail_strength=</span><span class="s3">0.5</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">X.shape == (n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>

    <span class="s1">U1</span><span class="s2">, </span><span class="s1">s1</span><span class="s2">, </span><span class="s1">V1 = randomized_svd(X</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">transpose=</span><span class="s2">False, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">U2</span><span class="s2">, </span><span class="s1">s2</span><span class="s2">, </span><span class="s1">V2 = randomized_svd(X</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">transpose=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">U3</span><span class="s2">, </span><span class="s1">s3</span><span class="s2">, </span><span class="s1">V3 = randomized_svd(X</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">transpose=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">U4</span><span class="s2">, </span><span class="s1">s4</span><span class="s2">, </span><span class="s1">V4 = linalg.svd(X</span><span class="s2">, </span><span class="s1">full_matrices=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s1">assert_almost_equal(s1</span><span class="s2">, </span><span class="s1">s4[:k]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(s2</span><span class="s2">, </span><span class="s1">s4[:k]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(s3</span><span class="s2">, </span><span class="s1">s4[:k]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">3</span><span class="s1">)</span>

    <span class="s1">assert_almost_equal(np.dot(U1</span><span class="s2">, </span><span class="s1">V1)</span><span class="s2">, </span><span class="s1">np.dot(U4[:</span><span class="s2">, </span><span class="s1">:k]</span><span class="s2">, </span><span class="s1">V4[:k</span><span class="s2">, </span><span class="s1">:])</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(np.dot(U2</span><span class="s2">, </span><span class="s1">V2)</span><span class="s2">, </span><span class="s1">np.dot(U4[:</span><span class="s2">, </span><span class="s1">:k]</span><span class="s2">, </span><span class="s1">V4[:k</span><span class="s2">, </span><span class="s1">:])</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>

    <span class="s0"># in this case 'auto' is equivalent to transpose</span>
    <span class="s1">assert_almost_equal(s2</span><span class="s2">, </span><span class="s1">s3)</span>


<span class="s2">def </span><span class="s1">test_randomized_svd_power_iteration_normalizer():</span>
    <span class="s0"># randomized_svd with power_iteration_normalized='none' diverges for</span>
    <span class="s0"># large number of power iterations on this dataset</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">X = make_low_rank_matrix(</span><span class="s3">100</span><span class="s2">, </span><span class="s3">500</span><span class="s2">, </span><span class="s1">effective_rank=</span><span class="s3">50</span><span class="s2">, </span><span class="s1">random_state=rng)</span>
    <span class="s1">X += </span><span class="s3">3 </span><span class="s1">* rng.randint(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s1">size=X.shape)</span>
    <span class="s1">n_components = </span><span class="s3">50</span>

    <span class="s0"># Check that it diverges with many (non-normalized) power iterations</span>
    <span class="s1">U</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">Vt = randomized_svd(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">n_components</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">power_iteration_normalizer=</span><span class="s5">&quot;none&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span>
    <span class="s1">)</span>
    <span class="s1">A = X - U.dot(np.diag(s).dot(Vt))</span>
    <span class="s1">error_2 = linalg.norm(A</span><span class="s2">, </span><span class="s1">ord=</span><span class="s5">&quot;fro&quot;</span><span class="s1">)</span>
    <span class="s1">U</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">Vt = randomized_svd(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">n_components</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s3">20</span><span class="s2">, </span><span class="s1">power_iteration_normalizer=</span><span class="s5">&quot;none&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span>
    <span class="s1">)</span>
    <span class="s1">A = X - U.dot(np.diag(s).dot(Vt))</span>
    <span class="s1">error_20 = linalg.norm(A</span><span class="s2">, </span><span class="s1">ord=</span><span class="s5">&quot;fro&quot;</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">np.abs(error_2 - error_20) &gt; </span><span class="s3">100</span>

    <span class="s2">for </span><span class="s1">normalizer </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;LU&quot;</span><span class="s2">, </span><span class="s5">&quot;QR&quot;</span><span class="s2">, </span><span class="s5">&quot;auto&quot;</span><span class="s1">]:</span>
        <span class="s1">U</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">Vt = randomized_svd(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">n_components</span><span class="s2">,</span>
            <span class="s1">n_iter=</span><span class="s3">2</span><span class="s2">,</span>
            <span class="s1">power_iteration_normalizer=normalizer</span><span class="s2">,</span>
            <span class="s1">random_state=</span><span class="s3">0</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">A = X - U.dot(np.diag(s).dot(Vt))</span>
        <span class="s1">error_2 = linalg.norm(A</span><span class="s2">, </span><span class="s1">ord=</span><span class="s5">&quot;fro&quot;</span><span class="s1">)</span>

        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">[</span><span class="s3">5</span><span class="s2">, </span><span class="s3">10</span><span class="s2">, </span><span class="s3">50</span><span class="s1">]:</span>
            <span class="s1">U</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">Vt = randomized_svd(</span>
                <span class="s1">X</span><span class="s2">,</span>
                <span class="s1">n_components</span><span class="s2">,</span>
                <span class="s1">n_iter=i</span><span class="s2">,</span>
                <span class="s1">power_iteration_normalizer=normalizer</span><span class="s2">,</span>
                <span class="s1">random_state=</span><span class="s3">0</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">A = X - U.dot(np.diag(s).dot(Vt))</span>
            <span class="s1">error = linalg.norm(A</span><span class="s2">, </span><span class="s1">ord=</span><span class="s5">&quot;fro&quot;</span><span class="s1">)</span>
            <span class="s2">assert </span><span class="s3">15 </span><span class="s1">&gt; np.abs(error_2 - error)</span>


<span class="s2">def </span><span class="s1">test_randomized_svd_sparse_warnings():</span>
    <span class="s0"># randomized_svd throws a warning for lil and dok matrix</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">X = make_low_rank_matrix(</span><span class="s3">50</span><span class="s2">, </span><span class="s3">20</span><span class="s2">, </span><span class="s1">effective_rank=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">random_state=rng)</span>
    <span class="s1">n_components = </span><span class="s3">5</span>
    <span class="s2">for </span><span class="s1">cls </span><span class="s2">in </span><span class="s1">(sparse.lil_matrix</span><span class="s2">, </span><span class="s1">sparse.dok_matrix):</span>
        <span class="s1">X = cls(X)</span>
        <span class="s1">warn_msg = (</span>
            <span class="s5">&quot;Calculating SVD of a {} is expensive. &quot;</span>
            <span class="s5">&quot;csr_matrix is more efficient.&quot;</span><span class="s1">.format(cls.__name__)</span>
        <span class="s1">)</span>
        <span class="s2">with </span><span class="s1">pytest.warns(sparse.SparseEfficiencyWarning</span><span class="s2">, </span><span class="s1">match=warn_msg):</span>
            <span class="s1">randomized_svd(X</span><span class="s2">, </span><span class="s1">n_components</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">power_iteration_normalizer=</span><span class="s5">&quot;none&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_svd_flip():</span>
    <span class="s0"># Check that svd_flip works in both situations, and reconstructs input.</span>
    <span class="s1">rs = np.random.RandomState(</span><span class="s3">1999</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s3">20</span>
    <span class="s1">n_features = </span><span class="s3">10</span>
    <span class="s1">X = rs.randn(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>

    <span class="s0"># Check matrix reconstruction</span>
    <span class="s1">U</span><span class="s2">, </span><span class="s1">S</span><span class="s2">, </span><span class="s1">Vt = linalg.svd(X</span><span class="s2">, </span><span class="s1">full_matrices=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">U1</span><span class="s2">, </span><span class="s1">V1 = svd_flip(U</span><span class="s2">, </span><span class="s1">Vt</span><span class="s2">, </span><span class="s1">u_based_decision=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(np.dot(U1 * S</span><span class="s2">, </span><span class="s1">V1)</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">6</span><span class="s1">)</span>

    <span class="s0"># Check transposed matrix reconstruction</span>
    <span class="s1">XT = X.T</span>
    <span class="s1">U</span><span class="s2">, </span><span class="s1">S</span><span class="s2">, </span><span class="s1">Vt = linalg.svd(XT</span><span class="s2">, </span><span class="s1">full_matrices=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">U2</span><span class="s2">, </span><span class="s1">V2 = svd_flip(U</span><span class="s2">, </span><span class="s1">Vt</span><span class="s2">, </span><span class="s1">u_based_decision=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(np.dot(U2 * S</span><span class="s2">, </span><span class="s1">V2)</span><span class="s2">, </span><span class="s1">XT</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">6</span><span class="s1">)</span>

    <span class="s0"># Check that different flip methods are equivalent under reconstruction</span>
    <span class="s1">U_flip1</span><span class="s2">, </span><span class="s1">V_flip1 = svd_flip(U</span><span class="s2">, </span><span class="s1">Vt</span><span class="s2">, </span><span class="s1">u_based_decision=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(np.dot(U_flip1 * S</span><span class="s2">, </span><span class="s1">V_flip1)</span><span class="s2">, </span><span class="s1">XT</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">6</span><span class="s1">)</span>
    <span class="s1">U_flip2</span><span class="s2">, </span><span class="s1">V_flip2 = svd_flip(U</span><span class="s2">, </span><span class="s1">Vt</span><span class="s2">, </span><span class="s1">u_based_decision=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(np.dot(U_flip2 * S</span><span class="s2">, </span><span class="s1">V_flip2)</span><span class="s2">, </span><span class="s1">XT</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">6</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_randomized_svd_sign_flip():</span>
    <span class="s1">a = np.array([[</span><span class="s3">2.0</span><span class="s2">, </span><span class="s3">0.0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0.0</span><span class="s2">, </span><span class="s3">1.0</span><span class="s1">]])</span>
    <span class="s1">u1</span><span class="s2">, </span><span class="s1">s1</span><span class="s2">, </span><span class="s1">v1 = randomized_svd(a</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s1">flip_sign=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s3">41</span><span class="s1">)</span>
    <span class="s2">for </span><span class="s1">seed </span><span class="s2">in </span><span class="s1">range(</span><span class="s3">10</span><span class="s1">):</span>
        <span class="s1">u2</span><span class="s2">, </span><span class="s1">s2</span><span class="s2">, </span><span class="s1">v2 = randomized_svd(a</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s1">flip_sign=</span><span class="s2">True, </span><span class="s1">random_state=seed)</span>
        <span class="s1">assert_almost_equal(u1</span><span class="s2">, </span><span class="s1">u2)</span>
        <span class="s1">assert_almost_equal(v1</span><span class="s2">, </span><span class="s1">v2)</span>
        <span class="s1">assert_almost_equal(np.dot(u2 * s2</span><span class="s2">, </span><span class="s1">v2)</span><span class="s2">, </span><span class="s1">a)</span>
        <span class="s1">assert_almost_equal(np.dot(u2.T</span><span class="s2">, </span><span class="s1">u2)</span><span class="s2">, </span><span class="s1">np.eye(</span><span class="s3">2</span><span class="s1">))</span>
        <span class="s1">assert_almost_equal(np.dot(v2.T</span><span class="s2">, </span><span class="s1">v2)</span><span class="s2">, </span><span class="s1">np.eye(</span><span class="s3">2</span><span class="s1">))</span>


<span class="s2">def </span><span class="s1">test_randomized_svd_sign_flip_with_transpose():</span>
    <span class="s0"># Check if the randomized_svd sign flipping is always done based on u</span>
    <span class="s0"># irrespective of transpose.</span>
    <span class="s0"># See https://github.com/scikit-learn/scikit-learn/issues/5608</span>
    <span class="s0"># for more details.</span>
    <span class="s2">def </span><span class="s1">max_loading_is_positive(u</span><span class="s2">, </span><span class="s1">v):</span>
        <span class="s4">&quot;&quot;&quot; 
        returns bool tuple indicating if the values maximising np.abs 
        are positive across all rows for u and across all columns for v. 
        &quot;&quot;&quot;</span>
        <span class="s1">u_based = (np.abs(u).max(axis=</span><span class="s3">0</span><span class="s1">) == u.max(axis=</span><span class="s3">0</span><span class="s1">)).all()</span>
        <span class="s1">v_based = (np.abs(v).max(axis=</span><span class="s3">1</span><span class="s1">) == v.max(axis=</span><span class="s3">1</span><span class="s1">)).all()</span>
        <span class="s2">return </span><span class="s1">u_based</span><span class="s2">, </span><span class="s1">v_based</span>

    <span class="s1">mat = np.arange(</span><span class="s3">10 </span><span class="s1">* </span><span class="s3">8</span><span class="s1">).reshape(</span><span class="s3">10</span><span class="s2">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s0"># Without transpose</span>
    <span class="s1">u_flipped</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">v_flipped = randomized_svd(mat</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s1">flip_sign=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">u_based</span><span class="s2">, </span><span class="s1">v_based = max_loading_is_positive(u_flipped</span><span class="s2">, </span><span class="s1">v_flipped)</span>
    <span class="s2">assert </span><span class="s1">u_based</span>
    <span class="s2">assert not </span><span class="s1">v_based</span>

    <span class="s0"># With transpose</span>
    <span class="s1">u_flipped_with_transpose</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">v_flipped_with_transpose = randomized_svd(</span>
        <span class="s1">mat</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s1">flip_sign=</span><span class="s2">True, </span><span class="s1">transpose=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s3">0</span>
    <span class="s1">)</span>
    <span class="s1">u_based</span><span class="s2">, </span><span class="s1">v_based = max_loading_is_positive(</span>
        <span class="s1">u_flipped_with_transpose</span><span class="s2">, </span><span class="s1">v_flipped_with_transpose</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">u_based</span>
    <span class="s2">assert not </span><span class="s1">v_based</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;n&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">50</span><span class="s2">, </span><span class="s3">100</span><span class="s2">, </span><span class="s3">300</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;m&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">50</span><span class="s2">, </span><span class="s3">100</span><span class="s2">, </span><span class="s3">300</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;k&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">10</span><span class="s2">, </span><span class="s3">20</span><span class="s2">, </span><span class="s3">50</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;seed&quot;</span><span class="s2">, </span><span class="s1">range(</span><span class="s3">5</span><span class="s1">))</span>
<span class="s2">def </span><span class="s1">test_randomized_svd_lapack_driver(n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">seed):</span>
    <span class="s0"># Check that different SVD drivers provide consistent results</span>

    <span class="s0"># Matrix being compressed</span>
    <span class="s1">rng = np.random.RandomState(seed)</span>
    <span class="s1">X = rng.rand(n</span><span class="s2">, </span><span class="s1">m)</span>

    <span class="s0"># Number of components</span>
    <span class="s1">u1</span><span class="s2">, </span><span class="s1">s1</span><span class="s2">, </span><span class="s1">vt1 = randomized_svd(X</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">svd_lapack_driver=</span><span class="s5">&quot;gesdd&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">u2</span><span class="s2">, </span><span class="s1">s2</span><span class="s2">, </span><span class="s1">vt2 = randomized_svd(X</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">svd_lapack_driver=</span><span class="s5">&quot;gesvd&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s0"># Check shape and contents</span>
    <span class="s2">assert </span><span class="s1">u1.shape == u2.shape</span>
    <span class="s1">assert_allclose(u1</span><span class="s2">, </span><span class="s1">u2</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s3">1e-3</span><span class="s1">)</span>

    <span class="s2">assert </span><span class="s1">s1.shape == s2.shape</span>
    <span class="s1">assert_allclose(s1</span><span class="s2">, </span><span class="s1">s2</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s3">1e-3</span><span class="s1">)</span>

    <span class="s2">assert </span><span class="s1">vt1.shape == vt2.shape</span>
    <span class="s1">assert_allclose(vt1</span><span class="s2">, </span><span class="s1">vt2</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s3">1e-3</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_cartesian():</span>
    <span class="s0"># Check if cartesian product delivers the right results</span>

    <span class="s1">axes = (np.array([</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s1">])</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s3">4</span><span class="s2">, </span><span class="s3">5</span><span class="s1">])</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s3">6</span><span class="s2">, </span><span class="s3">7</span><span class="s1">]))</span>

    <span class="s1">true_out = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">6</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">7</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">6</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">7</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">2</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">6</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">2</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">7</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">2</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">6</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">2</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">7</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">3</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">6</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">3</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">7</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">3</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">6</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">3</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">7</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">out = cartesian(axes)</span>
    <span class="s1">assert_array_equal(true_out</span><span class="s2">, </span><span class="s1">out)</span>

    <span class="s0"># check single axis</span>
    <span class="s1">x = np.arange(</span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">assert_array_equal(x[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span><span class="s2">, </span><span class="s1">cartesian((x</span><span class="s2">,</span><span class="s1">)))</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;arrays, output_dtype&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">[np.array([</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s3">4</span><span class="s2">, </span><span class="s3">5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.int64)]</span><span class="s2">,</span>
            <span class="s1">np.dtype(np.int64)</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">[np.array([</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s3">4</span><span class="s2">, </span><span class="s3">5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.float64)]</span><span class="s2">,</span>
            <span class="s1">np.dtype(np.float64)</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s1">[np.array([</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s5">&quot;x&quot;</span><span class="s2">, </span><span class="s5">&quot;y&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=object)]</span><span class="s2">,</span>
            <span class="s1">np.dtype(object)</span><span class="s2">,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_cartesian_mix_types(arrays</span><span class="s2">, </span><span class="s1">output_dtype):</span>
    <span class="s4">&quot;&quot;&quot;Check that the cartesian product works with mixed types.&quot;&quot;&quot;</span>
    <span class="s1">output = cartesian(arrays)</span>

    <span class="s2">assert </span><span class="s1">output.dtype == output_dtype</span>


<span class="s2">def </span><span class="s1">test_logistic_sigmoid():</span>
    <span class="s0"># Check correctness and robustness of logistic sigmoid implementation</span>
    <span class="s2">def </span><span class="s1">naive_log_logistic(x):</span>
        <span class="s2">return </span><span class="s1">np.log(expit(x))</span>

    <span class="s1">x = np.linspace(-</span><span class="s3">2</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">50</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(log_logistic(x)</span><span class="s2">, </span><span class="s1">naive_log_logistic(x))</span>

    <span class="s1">extreme_x = np.array([-</span><span class="s3">100.0</span><span class="s2">, </span><span class="s3">100.0</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(log_logistic(extreme_x)</span><span class="s2">, </span><span class="s1">[-</span><span class="s3">100</span><span class="s2">, </span><span class="s3">0</span><span class="s1">])</span>


<span class="s1">@pytest.fixture()</span>
<span class="s2">def </span><span class="s1">rng():</span>
    <span class="s2">return </span><span class="s1">np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;dtype&quot;</span><span class="s2">, </span><span class="s1">[np.float32</span><span class="s2">, </span><span class="s1">np.float64])</span>
<span class="s2">def </span><span class="s1">test_incremental_weighted_mean_and_variance_simple(rng</span><span class="s2">, </span><span class="s1">dtype):</span>
    <span class="s1">mult = </span><span class="s3">10</span>
    <span class="s1">X = rng.rand(</span><span class="s3">1000</span><span class="s2">, </span><span class="s3">20</span><span class="s1">).astype(dtype) * mult</span>
    <span class="s1">sample_weight = rng.rand(X.shape[</span><span class="s3">0</span><span class="s1">]) * mult</span>
    <span class="s1">mean</span><span class="s2">, </span><span class="s1">var</span><span class="s2">, </span><span class="s1">_ = _incremental_mean_and_var(X</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s1">expected_mean = np.average(X</span><span class="s2">, </span><span class="s1">weights=sample_weight</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">expected_var = (</span>
        <span class="s1">np.average(X**</span><span class="s3">2</span><span class="s2">, </span><span class="s1">weights=sample_weight</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">) - expected_mean**</span><span class="s3">2</span>
    <span class="s1">)</span>
    <span class="s1">assert_almost_equal(mean</span><span class="s2">, </span><span class="s1">expected_mean)</span>
    <span class="s1">assert_almost_equal(var</span><span class="s2">, </span><span class="s1">expected_var)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;mean&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1e7</span><span class="s2">, </span><span class="s1">-</span><span class="s3">1e7</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;var&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1e-8</span><span class="s2">, </span><span class="s3">1e5</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;weight_loc, weight_scale&quot;</span><span class="s2">, </span><span class="s1">[(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1e-8</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1e-8</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(</span><span class="s3">1e7</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)]</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_incremental_weighted_mean_and_variance(</span>
    <span class="s1">mean</span><span class="s2">, </span><span class="s1">var</span><span class="s2">, </span><span class="s1">weight_loc</span><span class="s2">, </span><span class="s1">weight_scale</span><span class="s2">, </span><span class="s1">rng</span>
<span class="s1">):</span>
    <span class="s0"># Testing of correctness and numerical stability</span>
    <span class="s2">def </span><span class="s1">_assert(X</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">expected_mean</span><span class="s2">, </span><span class="s1">expected_var):</span>
        <span class="s1">n = X.shape[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s2">for </span><span class="s1">chunk_size </span><span class="s2">in </span><span class="s1">[</span><span class="s3">1</span><span class="s2">, </span><span class="s1">n // </span><span class="s3">10 </span><span class="s1">+ </span><span class="s3">1</span><span class="s2">, </span><span class="s1">n // </span><span class="s3">4 </span><span class="s1">+ </span><span class="s3">1</span><span class="s2">, </span><span class="s1">n // </span><span class="s3">2 </span><span class="s1">+ </span><span class="s3">1</span><span class="s2">, </span><span class="s1">n]:</span>
            <span class="s1">last_mean</span><span class="s2">, </span><span class="s1">last_weight_sum</span><span class="s2">, </span><span class="s1">last_var = </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span>
            <span class="s2">for </span><span class="s1">batch </span><span class="s2">in </span><span class="s1">gen_batches(n</span><span class="s2">, </span><span class="s1">chunk_size):</span>
                <span class="s1">last_mean</span><span class="s2">, </span><span class="s1">last_var</span><span class="s2">, </span><span class="s1">last_weight_sum = _incremental_mean_and_var(</span>
                    <span class="s1">X[batch]</span><span class="s2">,</span>
                    <span class="s1">last_mean</span><span class="s2">,</span>
                    <span class="s1">last_var</span><span class="s2">,</span>
                    <span class="s1">last_weight_sum</span><span class="s2">,</span>
                    <span class="s1">sample_weight=sample_weight[batch]</span><span class="s2">,</span>
                <span class="s1">)</span>
            <span class="s1">assert_allclose(last_mean</span><span class="s2">, </span><span class="s1">expected_mean)</span>
            <span class="s1">assert_allclose(last_var</span><span class="s2">, </span><span class="s1">expected_var</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">1e-6</span><span class="s1">)</span>

    <span class="s1">size = (</span><span class="s3">100</span><span class="s2">, </span><span class="s3">20</span><span class="s1">)</span>
    <span class="s1">weight = rng.normal(loc=weight_loc</span><span class="s2">, </span><span class="s1">scale=weight_scale</span><span class="s2">, </span><span class="s1">size=size[</span><span class="s3">0</span><span class="s1">])</span>

    <span class="s0"># Compare to weighted average: np.average</span>
    <span class="s1">X = rng.normal(loc=mean</span><span class="s2">, </span><span class="s1">scale=var</span><span class="s2">, </span><span class="s1">size=size)</span>
    <span class="s1">expected_mean = _safe_accumulator_op(np.average</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">weights=weight</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">expected_var = _safe_accumulator_op(</span>
        <span class="s1">np.average</span><span class="s2">, </span><span class="s1">(X - expected_mean) ** </span><span class="s3">2</span><span class="s2">, </span><span class="s1">weights=weight</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span>
    <span class="s1">)</span>
    <span class="s1">_assert(X</span><span class="s2">, </span><span class="s1">weight</span><span class="s2">, </span><span class="s1">expected_mean</span><span class="s2">, </span><span class="s1">expected_var)</span>

    <span class="s0"># Compare to unweighted mean: np.mean</span>
    <span class="s1">X = rng.normal(loc=mean</span><span class="s2">, </span><span class="s1">scale=var</span><span class="s2">, </span><span class="s1">size=size)</span>
    <span class="s1">ones_weight = np.ones(size[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">expected_mean = _safe_accumulator_op(np.mean</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">expected_var = _safe_accumulator_op(np.var</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">_assert(X</span><span class="s2">, </span><span class="s1">ones_weight</span><span class="s2">, </span><span class="s1">expected_mean</span><span class="s2">, </span><span class="s1">expected_var)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;dtype&quot;</span><span class="s2">, </span><span class="s1">[np.float32</span><span class="s2">, </span><span class="s1">np.float64])</span>
<span class="s2">def </span><span class="s1">test_incremental_weighted_mean_and_variance_ignore_nan(dtype):</span>
    <span class="s1">old_means = np.array([</span><span class="s3">535.0</span><span class="s2">, </span><span class="s3">535.0</span><span class="s2">, </span><span class="s3">535.0</span><span class="s2">, </span><span class="s3">535.0</span><span class="s1">])</span>
    <span class="s1">old_variances = np.array([</span><span class="s3">4225.0</span><span class="s2">, </span><span class="s3">4225.0</span><span class="s2">, </span><span class="s3">4225.0</span><span class="s2">, </span><span class="s3">4225.0</span><span class="s1">])</span>
    <span class="s1">old_weight_sum = np.array([</span><span class="s3">2</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span>
    <span class="s1">sample_weights_X = np.ones(</span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">sample_weights_X_nan = np.ones(</span><span class="s3">4</span><span class="s1">)</span>

    <span class="s1">X = np.array(</span>
        <span class="s1">[[</span><span class="s3">170</span><span class="s2">, </span><span class="s3">170</span><span class="s2">, </span><span class="s3">170</span><span class="s2">, </span><span class="s3">170</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">430</span><span class="s2">, </span><span class="s3">430</span><span class="s2">, </span><span class="s3">430</span><span class="s2">, </span><span class="s3">430</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">300</span><span class="s2">, </span><span class="s3">300</span><span class="s2">, </span><span class="s3">300</span><span class="s2">, </span><span class="s3">300</span><span class="s1">]]</span>
    <span class="s1">).astype(dtype)</span>

    <span class="s1">X_nan = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s3">170</span><span class="s2">, </span><span class="s1">np.nan</span><span class="s2">, </span><span class="s3">170</span><span class="s2">, </span><span class="s3">170</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[np.nan</span><span class="s2">, </span><span class="s3">170</span><span class="s2">, </span><span class="s3">430</span><span class="s2">, </span><span class="s3">430</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">430</span><span class="s2">, </span><span class="s3">430</span><span class="s2">, </span><span class="s1">np.nan</span><span class="s2">, </span><span class="s3">300</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">300</span><span class="s2">, </span><span class="s3">300</span><span class="s2">, </span><span class="s3">300</span><span class="s2">, </span><span class="s1">np.nan]</span><span class="s2">,</span>
        <span class="s1">]</span>
    <span class="s1">).astype(dtype)</span>

    <span class="s1">X_means</span><span class="s2">, </span><span class="s1">X_variances</span><span class="s2">, </span><span class="s1">X_count = _incremental_mean_and_var(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">old_means</span><span class="s2">, </span><span class="s1">old_variances</span><span class="s2">, </span><span class="s1">old_weight_sum</span><span class="s2">, </span><span class="s1">sample_weight=sample_weights_X</span>
    <span class="s1">)</span>
    <span class="s1">X_nan_means</span><span class="s2">, </span><span class="s1">X_nan_variances</span><span class="s2">, </span><span class="s1">X_nan_count = _incremental_mean_and_var(</span>
        <span class="s1">X_nan</span><span class="s2">,</span>
        <span class="s1">old_means</span><span class="s2">,</span>
        <span class="s1">old_variances</span><span class="s2">,</span>
        <span class="s1">old_weight_sum</span><span class="s2">,</span>
        <span class="s1">sample_weight=sample_weights_X_nan</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">assert_allclose(X_nan_means</span><span class="s2">, </span><span class="s1">X_means)</span>
    <span class="s1">assert_allclose(X_nan_variances</span><span class="s2">, </span><span class="s1">X_variances)</span>
    <span class="s1">assert_allclose(X_nan_count</span><span class="s2">, </span><span class="s1">X_count)</span>


<span class="s2">def </span><span class="s1">test_incremental_variance_update_formulas():</span>
    <span class="s0"># Test Youngs and Cramer incremental variance formulas.</span>
    <span class="s0"># Doggie data from https://www.mathsisfun.com/data/standard-deviation.html</span>
    <span class="s1">A = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s3">600</span><span class="s2">, </span><span class="s3">470</span><span class="s2">, </span><span class="s3">170</span><span class="s2">, </span><span class="s3">430</span><span class="s2">, </span><span class="s3">300</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">600</span><span class="s2">, </span><span class="s3">470</span><span class="s2">, </span><span class="s3">170</span><span class="s2">, </span><span class="s3">430</span><span class="s2">, </span><span class="s3">300</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">600</span><span class="s2">, </span><span class="s3">470</span><span class="s2">, </span><span class="s3">170</span><span class="s2">, </span><span class="s3">430</span><span class="s2">, </span><span class="s3">300</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">600</span><span class="s2">, </span><span class="s3">470</span><span class="s2">, </span><span class="s3">170</span><span class="s2">, </span><span class="s3">430</span><span class="s2">, </span><span class="s3">300</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">]</span>
    <span class="s1">).T</span>
    <span class="s1">idx = </span><span class="s3">2</span>
    <span class="s1">X1 = A[:idx</span><span class="s2">, </span><span class="s1">:]</span>
    <span class="s1">X2 = A[idx:</span><span class="s2">, </span><span class="s1">:]</span>

    <span class="s1">old_means = X1.mean(axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">old_variances = X1.var(axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">old_sample_count = np.full(X1.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X1.shape[</span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span>
    <span class="s1">final_means</span><span class="s2">, </span><span class="s1">final_variances</span><span class="s2">, </span><span class="s1">final_count = _incremental_mean_and_var(</span>
        <span class="s1">X2</span><span class="s2">, </span><span class="s1">old_means</span><span class="s2">, </span><span class="s1">old_variances</span><span class="s2">, </span><span class="s1">old_sample_count</span>
    <span class="s1">)</span>
    <span class="s1">assert_almost_equal(final_means</span><span class="s2">, </span><span class="s1">A.mean(axis=</span><span class="s3">0</span><span class="s1">)</span><span class="s2">, </span><span class="s3">6</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(final_variances</span><span class="s2">, </span><span class="s1">A.var(axis=</span><span class="s3">0</span><span class="s1">)</span><span class="s2">, </span><span class="s3">6</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(final_count</span><span class="s2">, </span><span class="s1">A.shape[</span><span class="s3">0</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_incremental_mean_and_variance_ignore_nan():</span>
    <span class="s1">old_means = np.array([</span><span class="s3">535.0</span><span class="s2">, </span><span class="s3">535.0</span><span class="s2">, </span><span class="s3">535.0</span><span class="s2">, </span><span class="s3">535.0</span><span class="s1">])</span>
    <span class="s1">old_variances = np.array([</span><span class="s3">4225.0</span><span class="s2">, </span><span class="s3">4225.0</span><span class="s2">, </span><span class="s3">4225.0</span><span class="s2">, </span><span class="s3">4225.0</span><span class="s1">])</span>
    <span class="s1">old_sample_count = np.array([</span><span class="s3">2</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span>

    <span class="s1">X = np.array([[</span><span class="s3">170</span><span class="s2">, </span><span class="s3">170</span><span class="s2">, </span><span class="s3">170</span><span class="s2">, </span><span class="s3">170</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">430</span><span class="s2">, </span><span class="s3">430</span><span class="s2">, </span><span class="s3">430</span><span class="s2">, </span><span class="s3">430</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">300</span><span class="s2">, </span><span class="s3">300</span><span class="s2">, </span><span class="s3">300</span><span class="s2">, </span><span class="s3">300</span><span class="s1">]])</span>

    <span class="s1">X_nan = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s3">170</span><span class="s2">, </span><span class="s1">np.nan</span><span class="s2">, </span><span class="s3">170</span><span class="s2">, </span><span class="s3">170</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[np.nan</span><span class="s2">, </span><span class="s3">170</span><span class="s2">, </span><span class="s3">430</span><span class="s2">, </span><span class="s3">430</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">430</span><span class="s2">, </span><span class="s3">430</span><span class="s2">, </span><span class="s1">np.nan</span><span class="s2">, </span><span class="s3">300</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s3">300</span><span class="s2">, </span><span class="s3">300</span><span class="s2">, </span><span class="s3">300</span><span class="s2">, </span><span class="s1">np.nan]</span><span class="s2">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">X_means</span><span class="s2">, </span><span class="s1">X_variances</span><span class="s2">, </span><span class="s1">X_count = _incremental_mean_and_var(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">old_means</span><span class="s2">, </span><span class="s1">old_variances</span><span class="s2">, </span><span class="s1">old_sample_count</span>
    <span class="s1">)</span>
    <span class="s1">X_nan_means</span><span class="s2">, </span><span class="s1">X_nan_variances</span><span class="s2">, </span><span class="s1">X_nan_count = _incremental_mean_and_var(</span>
        <span class="s1">X_nan</span><span class="s2">, </span><span class="s1">old_means</span><span class="s2">, </span><span class="s1">old_variances</span><span class="s2">, </span><span class="s1">old_sample_count</span>
    <span class="s1">)</span>

    <span class="s1">assert_allclose(X_nan_means</span><span class="s2">, </span><span class="s1">X_means)</span>
    <span class="s1">assert_allclose(X_nan_variances</span><span class="s2">, </span><span class="s1">X_variances)</span>
    <span class="s1">assert_allclose(X_nan_count</span><span class="s2">, </span><span class="s1">X_count)</span>


<span class="s1">@skip_if_32bit</span>
<span class="s2">def </span><span class="s1">test_incremental_variance_numerical_stability():</span>
    <span class="s0"># Test Youngs and Cramer incremental variance formulas.</span>

    <span class="s2">def </span><span class="s1">np_var(A):</span>
        <span class="s2">return </span><span class="s1">A.var(axis=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s0"># Naive one pass variance computation - not numerically stable</span>
    <span class="s0"># https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance</span>
    <span class="s2">def </span><span class="s1">one_pass_var(X):</span>
        <span class="s1">n = X.shape[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">exp_x2 = (X**</span><span class="s3">2</span><span class="s1">).sum(axis=</span><span class="s3">0</span><span class="s1">) / n</span>
        <span class="s1">expx_2 = (X.sum(axis=</span><span class="s3">0</span><span class="s1">) / n) ** </span><span class="s3">2</span>
        <span class="s2">return </span><span class="s1">exp_x2 - expx_2</span>

    <span class="s0"># Two-pass algorithm, stable.</span>
    <span class="s0"># We use it as a benchmark. It is not an online algorithm</span>
    <span class="s0"># https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Two-pass_algorithm</span>
    <span class="s2">def </span><span class="s1">two_pass_var(X):</span>
        <span class="s1">mean = X.mean(axis=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">Y = X.copy()</span>
        <span class="s2">return </span><span class="s1">np.mean((Y - mean) ** </span><span class="s3">2</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s0"># Naive online implementation</span>
    <span class="s0"># https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm</span>
    <span class="s0"># This works only for chunks for size 1</span>
    <span class="s2">def </span><span class="s1">naive_mean_variance_update(x</span><span class="s2">, </span><span class="s1">last_mean</span><span class="s2">, </span><span class="s1">last_variance</span><span class="s2">, </span><span class="s1">last_sample_count):</span>
        <span class="s1">updated_sample_count = last_sample_count + </span><span class="s3">1</span>
        <span class="s1">samples_ratio = last_sample_count / float(updated_sample_count)</span>
        <span class="s1">updated_mean = x / updated_sample_count + last_mean * samples_ratio</span>
        <span class="s1">updated_variance = (</span>
            <span class="s1">last_variance * samples_ratio</span>
            <span class="s1">+ (x - last_mean) * (x - updated_mean) / updated_sample_count</span>
        <span class="s1">)</span>
        <span class="s2">return </span><span class="s1">updated_mean</span><span class="s2">, </span><span class="s1">updated_variance</span><span class="s2">, </span><span class="s1">updated_sample_count</span>

    <span class="s0"># We want to show a case when one_pass_var has error &gt; 1e-3 while</span>
    <span class="s0"># _batch_mean_variance_update has less.</span>
    <span class="s1">tol = </span><span class="s3">200</span>
    <span class="s1">n_features = </span><span class="s3">2</span>
    <span class="s1">n_samples = </span><span class="s3">10000</span>
    <span class="s1">x1 = np.array(</span><span class="s3">1e8</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s1">x2 = np.log(</span><span class="s3">1e-5</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s1">A0 = np.full((n_samples // </span><span class="s3">2</span><span class="s2">, </span><span class="s1">n_features)</span><span class="s2">, </span><span class="s1">x1</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s1">A1 = np.full((n_samples // </span><span class="s3">2</span><span class="s2">, </span><span class="s1">n_features)</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s1">A = np.vstack((A0</span><span class="s2">, </span><span class="s1">A1))</span>

    <span class="s0"># Naive one pass var: &gt;tol (=1063)</span>
    <span class="s2">assert </span><span class="s1">np.abs(np_var(A) - one_pass_var(A)).max() &gt; tol</span>

    <span class="s0"># Starting point for online algorithms: after A0</span>

    <span class="s0"># Naive implementation: &gt;tol (436)</span>
    <span class="s1">mean</span><span class="s2">, </span><span class="s1">var</span><span class="s2">, </span><span class="s1">n = A0[</span><span class="s3">0</span><span class="s2">, </span><span class="s1">:]</span><span class="s2">, </span><span class="s1">np.zeros(n_features)</span><span class="s2">, </span><span class="s1">n_samples // </span><span class="s3">2</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(A1.shape[</span><span class="s3">0</span><span class="s1">]):</span>
        <span class="s1">mean</span><span class="s2">, </span><span class="s1">var</span><span class="s2">, </span><span class="s1">n = naive_mean_variance_update(A1[i</span><span class="s2">, </span><span class="s1">:]</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">var</span><span class="s2">, </span><span class="s1">n)</span>
    <span class="s2">assert </span><span class="s1">n == A.shape[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s0"># the mean is also slightly unstable</span>
    <span class="s2">assert </span><span class="s1">np.abs(A.mean(axis=</span><span class="s3">0</span><span class="s1">) - mean).max() &gt; </span><span class="s3">1e-6</span>
    <span class="s2">assert </span><span class="s1">np.abs(np_var(A) - var).max() &gt; tol</span>

    <span class="s0"># Robust implementation: &lt;tol (177)</span>
    <span class="s1">mean</span><span class="s2">, </span><span class="s1">var = A0[</span><span class="s3">0</span><span class="s2">, </span><span class="s1">:]</span><span class="s2">, </span><span class="s1">np.zeros(n_features)</span>
    <span class="s1">n = np.full(n_features</span><span class="s2">, </span><span class="s1">n_samples // </span><span class="s3">2</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(A1.shape[</span><span class="s3">0</span><span class="s1">]):</span>
        <span class="s1">mean</span><span class="s2">, </span><span class="s1">var</span><span class="s2">, </span><span class="s1">n = _incremental_mean_and_var(</span>
            <span class="s1">A1[i</span><span class="s2">, </span><span class="s1">:].reshape((</span><span class="s3">1</span><span class="s2">, </span><span class="s1">A1.shape[</span><span class="s3">1</span><span class="s1">]))</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">var</span><span class="s2">, </span><span class="s1">n</span>
        <span class="s1">)</span>
    <span class="s1">assert_array_equal(n</span><span class="s2">, </span><span class="s1">A.shape[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">assert_array_almost_equal(A.mean(axis=</span><span class="s3">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">mean)</span>
    <span class="s2">assert </span><span class="s1">tol &gt; np.abs(np_var(A) - var).max()</span>


<span class="s2">def </span><span class="s1">test_incremental_variance_ddof():</span>
    <span class="s0"># Test that degrees of freedom parameter for calculations are correct.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">1999</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(</span><span class="s3">50</span><span class="s2">, </span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X.shape</span>
    <span class="s2">for </span><span class="s1">batch_size </span><span class="s2">in </span><span class="s1">[</span><span class="s3">11</span><span class="s2">, </span><span class="s3">20</span><span class="s2">, </span><span class="s3">37</span><span class="s1">]:</span>
        <span class="s1">steps = np.arange(</span><span class="s3">0</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">batch_size)</span>
        <span class="s2">if </span><span class="s1">steps[-</span><span class="s3">1</span><span class="s1">] != X.shape[</span><span class="s3">0</span><span class="s1">]:</span>
            <span class="s1">steps = np.hstack([steps</span><span class="s2">, </span><span class="s1">n_samples])</span>

        <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">j </span><span class="s2">in </span><span class="s1">zip(steps[:-</span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">steps[</span><span class="s3">1</span><span class="s1">:]):</span>
            <span class="s1">batch = X[i:j</span><span class="s2">, </span><span class="s1">:]</span>
            <span class="s2">if </span><span class="s1">i == </span><span class="s3">0</span><span class="s1">:</span>
                <span class="s1">incremental_means = batch.mean(axis=</span><span class="s3">0</span><span class="s1">)</span>
                <span class="s1">incremental_variances = batch.var(axis=</span><span class="s3">0</span><span class="s1">)</span>
                <span class="s0"># Assign this twice so that the test logic is consistent</span>
                <span class="s1">incremental_count = batch.shape[</span><span class="s3">0</span><span class="s1">]</span>
                <span class="s1">sample_count = np.full(batch.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">batch.shape[</span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.int32)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">result = _incremental_mean_and_var(</span>
                    <span class="s1">batch</span><span class="s2">, </span><span class="s1">incremental_means</span><span class="s2">, </span><span class="s1">incremental_variances</span><span class="s2">, </span><span class="s1">sample_count</span>
                <span class="s1">)</span>
                <span class="s1">(incremental_means</span><span class="s2">, </span><span class="s1">incremental_variances</span><span class="s2">, </span><span class="s1">incremental_count) = result</span>
                <span class="s1">sample_count += batch.shape[</span><span class="s3">0</span><span class="s1">]</span>

            <span class="s1">calculated_means = np.mean(X[:j]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
            <span class="s1">calculated_variances = np.var(X[:j]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
            <span class="s1">assert_almost_equal(incremental_means</span><span class="s2">, </span><span class="s1">calculated_means</span><span class="s2">, </span><span class="s3">6</span><span class="s1">)</span>
            <span class="s1">assert_almost_equal(incremental_variances</span><span class="s2">, </span><span class="s1">calculated_variances</span><span class="s2">, </span><span class="s3">6</span><span class="s1">)</span>
            <span class="s1">assert_array_equal(incremental_count</span><span class="s2">, </span><span class="s1">sample_count)</span>


<span class="s2">def </span><span class="s1">test_vector_sign_flip():</span>
    <span class="s0"># Testing that sign flip is working &amp; largest value has positive sign</span>
    <span class="s1">data = np.random.RandomState(</span><span class="s3">36</span><span class="s1">).randn(</span><span class="s3">5</span><span class="s2">, </span><span class="s3">5</span><span class="s1">)</span>
    <span class="s1">max_abs_rows = np.argmax(np.abs(data)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">data_flipped = _deterministic_vector_sign_flip(data)</span>
    <span class="s1">max_rows = np.argmax(data_flipped</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">assert_array_equal(max_abs_rows</span><span class="s2">, </span><span class="s1">max_rows)</span>
    <span class="s1">signs = np.sign(data[range(data.shape[</span><span class="s3">0</span><span class="s1">])</span><span class="s2">, </span><span class="s1">max_abs_rows])</span>
    <span class="s1">assert_array_equal(data</span><span class="s2">, </span><span class="s1">data_flipped * signs[:</span><span class="s2">, </span><span class="s1">np.newaxis])</span>


<span class="s2">def </span><span class="s1">test_softmax():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(</span><span class="s3">3</span><span class="s2">, </span><span class="s3">5</span><span class="s1">)</span>
    <span class="s1">exp_X = np.exp(X)</span>
    <span class="s1">sum_exp_X = np.sum(exp_X</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">).reshape((-</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">))</span>
    <span class="s1">assert_array_almost_equal(softmax(X)</span><span class="s2">, </span><span class="s1">exp_X / sum_exp_X)</span>


<span class="s2">def </span><span class="s1">test_stable_cumsum():</span>
    <span class="s1">assert_array_equal(stable_cumsum([</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s1">])</span><span class="s2">, </span><span class="s1">np.cumsum([</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s1">]))</span>
    <span class="s1">r = np.random.RandomState(</span><span class="s3">0</span><span class="s1">).rand(</span><span class="s3">100000</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(RuntimeWarning):</span>
        <span class="s1">stable_cumsum(r</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s0"># test axis parameter</span>
    <span class="s1">A = np.random.RandomState(</span><span class="s3">36</span><span class="s1">).randint(</span><span class="s3">1000</span><span class="s2">, </span><span class="s1">size=(</span><span class="s3">5</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">5</span><span class="s1">))</span>
    <span class="s1">assert_array_equal(stable_cumsum(A</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.cumsum(A</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">))</span>
    <span class="s1">assert_array_equal(stable_cumsum(A</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.cumsum(A</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">))</span>
    <span class="s1">assert_array_equal(stable_cumsum(A</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.cumsum(A</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">2</span><span class="s1">))</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;A_array_constr&quot;</span><span class="s2">, </span><span class="s1">[np.array</span><span class="s2">, </span><span class="s1">sparse.csr_matrix]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s5">&quot;dense&quot;</span><span class="s2">, </span><span class="s5">&quot;sparse&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;B_array_constr&quot;</span><span class="s2">, </span><span class="s1">[np.array</span><span class="s2">, </span><span class="s1">sparse.csr_matrix]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s5">&quot;dense&quot;</span><span class="s2">, </span><span class="s5">&quot;sparse&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_safe_sparse_dot_2d(A_array_constr</span><span class="s2">, </span><span class="s1">B_array_constr):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">A = rng.random_sample((</span><span class="s3">30</span><span class="s2">, </span><span class="s3">10</span><span class="s1">))</span>
    <span class="s1">B = rng.random_sample((</span><span class="s3">10</span><span class="s2">, </span><span class="s3">20</span><span class="s1">))</span>
    <span class="s1">expected = np.dot(A</span><span class="s2">, </span><span class="s1">B)</span>

    <span class="s1">A = A_array_constr(A)</span>
    <span class="s1">B = B_array_constr(B)</span>
    <span class="s1">actual = safe_sparse_dot(A</span><span class="s2">, </span><span class="s1">B</span><span class="s2">, </span><span class="s1">dense_output=</span><span class="s2">True</span><span class="s1">)</span>

    <span class="s1">assert_allclose(actual</span><span class="s2">, </span><span class="s1">expected)</span>


<span class="s2">def </span><span class="s1">test_safe_sparse_dot_nd():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s0"># dense ND / sparse</span>
    <span class="s1">A = rng.random_sample((</span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">6</span><span class="s1">))</span>
    <span class="s1">B = rng.random_sample((</span><span class="s3">6</span><span class="s2">, </span><span class="s3">7</span><span class="s1">))</span>
    <span class="s1">expected = np.dot(A</span><span class="s2">, </span><span class="s1">B)</span>
    <span class="s1">B = sparse.csr_matrix(B)</span>
    <span class="s1">actual = safe_sparse_dot(A</span><span class="s2">, </span><span class="s1">B)</span>
    <span class="s1">assert_allclose(actual</span><span class="s2">, </span><span class="s1">expected)</span>

    <span class="s0"># sparse / dense ND</span>
    <span class="s1">A = rng.random_sample((</span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s1">))</span>
    <span class="s1">B = rng.random_sample((</span><span class="s3">4</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s3">6</span><span class="s1">))</span>
    <span class="s1">expected = np.dot(A</span><span class="s2">, </span><span class="s1">B)</span>
    <span class="s1">A = sparse.csr_matrix(A)</span>
    <span class="s1">actual = safe_sparse_dot(A</span><span class="s2">, </span><span class="s1">B)</span>
    <span class="s1">assert_allclose(actual</span><span class="s2">, </span><span class="s1">expected)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;A_array_constr&quot;</span><span class="s2">, </span><span class="s1">[np.array</span><span class="s2">, </span><span class="s1">sparse.csr_matrix]</span><span class="s2">, </span><span class="s1">ids=[</span><span class="s5">&quot;dense&quot;</span><span class="s2">, </span><span class="s5">&quot;sparse&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_safe_sparse_dot_2d_1d(A_array_constr):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">B = rng.random_sample((</span><span class="s3">10</span><span class="s1">))</span>

    <span class="s0"># 2D @ 1D</span>
    <span class="s1">A = rng.random_sample((</span><span class="s3">30</span><span class="s2">, </span><span class="s3">10</span><span class="s1">))</span>
    <span class="s1">expected = np.dot(A</span><span class="s2">, </span><span class="s1">B)</span>
    <span class="s1">A = A_array_constr(A)</span>
    <span class="s1">actual = safe_sparse_dot(A</span><span class="s2">, </span><span class="s1">B)</span>
    <span class="s1">assert_allclose(actual</span><span class="s2">, </span><span class="s1">expected)</span>

    <span class="s0"># 1D @ 2D</span>
    <span class="s1">A = rng.random_sample((</span><span class="s3">10</span><span class="s2">, </span><span class="s3">30</span><span class="s1">))</span>
    <span class="s1">expected = np.dot(B</span><span class="s2">, </span><span class="s1">A)</span>
    <span class="s1">A = A_array_constr(A)</span>
    <span class="s1">actual = safe_sparse_dot(B</span><span class="s2">, </span><span class="s1">A)</span>
    <span class="s1">assert_allclose(actual</span><span class="s2">, </span><span class="s1">expected)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;dense_output&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_safe_sparse_dot_dense_output(dense_output):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">A = sparse.random(</span><span class="s3">30</span><span class="s2">, </span><span class="s3">10</span><span class="s2">, </span><span class="s1">density=</span><span class="s3">0.1</span><span class="s2">, </span><span class="s1">random_state=rng)</span>
    <span class="s1">B = sparse.random(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">20</span><span class="s2">, </span><span class="s1">density=</span><span class="s3">0.1</span><span class="s2">, </span><span class="s1">random_state=rng)</span>

    <span class="s1">expected = A.dot(B)</span>
    <span class="s1">actual = safe_sparse_dot(A</span><span class="s2">, </span><span class="s1">B</span><span class="s2">, </span><span class="s1">dense_output=dense_output)</span>

    <span class="s2">assert </span><span class="s1">sparse.issparse(actual) == (</span><span class="s2">not </span><span class="s1">dense_output)</span>

    <span class="s2">if </span><span class="s1">dense_output:</span>
        <span class="s1">expected = expected.toarray()</span>
    <span class="s1">assert_allclose_dense_sparse(actual</span><span class="s2">, </span><span class="s1">expected)</span>
</pre>
</body>
</html>