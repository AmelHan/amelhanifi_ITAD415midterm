<html>
<head>
<title>kernel_approximation.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
kernel_approximation.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
The :mod:`sklearn.kernel_approximation` module implements several 
approximate kernel feature maps based on Fourier transforms and Count Sketches. 
&quot;&quot;&quot;</span>

<span class="s2"># Author: Andreas Mueller &lt;amueller@ais.uni-bonn.de&gt;</span>
<span class="s2">#         Daniel Lopez-Sanchez (TensorSketch) &lt;lope@usal.es&gt;</span>

<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">scipy.sparse </span><span class="s3">as </span><span class="s1">sp</span>
<span class="s3">from </span><span class="s1">scipy.linalg </span><span class="s3">import </span><span class="s1">svd</span>

<span class="s3">try</span><span class="s1">:</span>
    <span class="s3">from </span><span class="s1">scipy.fft </span><span class="s3">import </span><span class="s1">fft</span><span class="s3">, </span><span class="s1">ifft</span>
<span class="s3">except </span><span class="s1">ImportError:  </span><span class="s2"># scipy &lt; 1.4</span>
    <span class="s3">from </span><span class="s1">scipy.fftpack </span><span class="s3">import </span><span class="s1">fft</span><span class="s3">, </span><span class="s1">ifft</span>

<span class="s3">from </span><span class="s1">.base </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">BaseEstimator</span><span class="s3">,</span>
    <span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s3">,</span>
    <span class="s1">TransformerMixin</span><span class="s3">,</span>
    <span class="s1">_fit_context</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">.metrics.pairwise </span><span class="s3">import </span><span class="s1">KERNEL_PARAMS</span><span class="s3">, </span><span class="s1">PAIRWISE_KERNEL_FUNCTIONS</span><span class="s3">, </span><span class="s1">pairwise_kernels</span>
<span class="s3">from </span><span class="s1">.utils </span><span class="s3">import </span><span class="s1">check_random_state</span><span class="s3">, </span><span class="s1">deprecated</span>
<span class="s3">from </span><span class="s1">.utils._param_validation </span><span class="s3">import </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">StrOptions</span>
<span class="s3">from </span><span class="s1">.utils.extmath </span><span class="s3">import </span><span class="s1">safe_sparse_dot</span>
<span class="s3">from </span><span class="s1">.utils.validation </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">_check_feature_names_in</span><span class="s3">,</span>
    <span class="s1">check_is_fitted</span><span class="s3">,</span>
    <span class="s1">check_non_negative</span><span class="s3">,</span>
<span class="s1">)</span>


<span class="s3">class </span><span class="s1">PolynomialCountSketch(</span>
    <span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s3">, </span><span class="s1">TransformerMixin</span><span class="s3">, </span><span class="s1">BaseEstimator</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Polynomial kernel approximation via Tensor Sketch. 
 
    Implements Tensor Sketch, which approximates the feature map 
    of the polynomial kernel:: 
 
        K(X, Y) = (gamma * &lt;X, Y&gt; + coef0)^degree 
 
    by efficiently computing a Count Sketch of the outer product of a 
    vector with itself using Fast Fourier Transforms (FFT). Read more in the 
    :ref:`User Guide &lt;polynomial_kernel_approx&gt;`. 
 
    .. versionadded:: 0.24 
 
    Parameters 
    ---------- 
    gamma : float, default=1.0 
        Parameter of the polynomial kernel whose feature map 
        will be approximated. 
 
    degree : int, default=2 
        Degree of the polynomial kernel whose feature map 
        will be approximated. 
 
    coef0 : int, default=0 
        Constant term of the polynomial kernel whose feature map 
        will be approximated. 
 
    n_components : int, default=100 
        Dimensionality of the output feature space. Usually, `n_components` 
        should be greater than the number of features in input samples in 
        order to achieve good performance. The optimal score / run time 
        balance is typically achieved around `n_components` = 10 * `n_features`, 
        but this depends on the specific dataset being used. 
 
    random_state : int, RandomState instance, default=None 
        Determines random number generation for indexHash and bitHash 
        initialization. Pass an int for reproducible results across multiple 
        function calls. See :term:`Glossary &lt;random_state&gt;`. 
 
    Attributes 
    ---------- 
    indexHash_ : ndarray of shape (degree, n_features), dtype=int64 
        Array of indexes in range [0, n_components) used to represent 
        the 2-wise independent hash functions for Count Sketch computation. 
 
    bitHash_ : ndarray of shape (degree, n_features), dtype=float32 
        Array with random entries in {+1, -1}, used to represent 
        the 2-wise independent hash functions for Count Sketch computation. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel. 
    Nystroem : Approximate a kernel map using a subset of the training data. 
    RBFSampler : Approximate a RBF kernel feature map using random Fourier 
        features. 
    SkewedChi2Sampler : Approximate feature map for &quot;skewed chi-squared&quot; kernel. 
    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.kernel_approximation import PolynomialCountSketch 
    &gt;&gt;&gt; from sklearn.linear_model import SGDClassifier 
    &gt;&gt;&gt; X = [[0, 0], [1, 1], [1, 0], [0, 1]] 
    &gt;&gt;&gt; y = [0, 0, 1, 1] 
    &gt;&gt;&gt; ps = PolynomialCountSketch(degree=3, random_state=1) 
    &gt;&gt;&gt; X_features = ps.fit_transform(X) 
    &gt;&gt;&gt; clf = SGDClassifier(max_iter=10, tol=1e-3) 
    &gt;&gt;&gt; clf.fit(X_features, y) 
    SGDClassifier(max_iter=10) 
    &gt;&gt;&gt; clf.score(X_features, y) 
    1.0 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;gamma&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;degree&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;coef0&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, None, None, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;n_components&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">gamma=</span><span class="s5">1.0</span><span class="s3">, </span><span class="s1">degree=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">coef0=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">n_components=</span><span class="s5">100</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s3">None</span>
    <span class="s1">):</span>
        <span class="s1">self.gamma = gamma</span>
        <span class="s1">self.degree = degree</span>
        <span class="s1">self.coef0 = coef0</span>
        <span class="s1">self.n_components = n_components</span>
        <span class="s1">self.random_state = random_state</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model with X. 
 
        Initializes the internal variables. The method needs no information 
        about the distribution of data, so we only care about n_features in X. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training data, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_outputs), \ 
                default=None 
            Target values (None for unsupervised transformations). 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">accept_sparse=</span><span class="s4">&quot;csc&quot;</span><span class="s1">)</span>
        <span class="s1">random_state = check_random_state(self.random_state)</span>

        <span class="s1">n_features = X.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">if </span><span class="s1">self.coef0 != </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">n_features += </span><span class="s5">1</span>

        <span class="s1">self.indexHash_ = random_state.randint(</span>
            <span class="s5">0</span><span class="s3">, </span><span class="s1">high=self.n_components</span><span class="s3">, </span><span class="s1">size=(self.degree</span><span class="s3">, </span><span class="s1">n_features)</span>
        <span class="s1">)</span>

        <span class="s1">self.bitHash_ = random_state.choice(a=[-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">size=(self.degree</span><span class="s3">, </span><span class="s1">n_features))</span>
        <span class="s1">self._n_features_out = self.n_components</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">transform(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Generate the feature map approximation for X. 
 
        Parameters 
        ---------- 
        X : {array-like}, shape (n_samples, n_features) 
            New data, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        Returns 
        ------- 
        X_new : array-like, shape (n_samples, n_components) 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>

        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">accept_sparse=</span><span class="s4">&quot;csc&quot;</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False</span><span class="s1">)</span>

        <span class="s1">X_gamma = np.sqrt(self.gamma) * X</span>

        <span class="s3">if </span><span class="s1">sp.issparse(X_gamma) </span><span class="s3">and </span><span class="s1">self.coef0 != </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">X_gamma = sp.hstack(</span>
                <span class="s1">[X_gamma</span><span class="s3">, </span><span class="s1">np.sqrt(self.coef0) * np.ones((X_gamma.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))]</span><span class="s3">,</span>
                <span class="s1">format=</span><span class="s4">&quot;csc&quot;</span><span class="s3">,</span>
            <span class="s1">)</span>

        <span class="s3">elif not </span><span class="s1">sp.issparse(X_gamma) </span><span class="s3">and </span><span class="s1">self.coef0 != </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">X_gamma = np.hstack(</span>
                <span class="s1">[X_gamma</span><span class="s3">, </span><span class="s1">np.sqrt(self.coef0) * np.ones((X_gamma.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))]</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">X_gamma.shape[</span><span class="s5">1</span><span class="s1">] != self.indexHash_.shape[</span><span class="s5">1</span><span class="s1">]:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;Number of features of test samples does not&quot;</span>
                <span class="s4">&quot; match that of training samples.&quot;</span>
            <span class="s1">)</span>

        <span class="s1">count_sketches = np.zeros((X_gamma.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">self.degree</span><span class="s3">, </span><span class="s1">self.n_components))</span>

        <span class="s3">if </span><span class="s1">sp.issparse(X_gamma):</span>
            <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(X_gamma.shape[</span><span class="s5">1</span><span class="s1">]):</span>
                <span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">range(self.degree):</span>
                    <span class="s1">iHashIndex = self.indexHash_[d</span><span class="s3">, </span><span class="s1">j]</span>
                    <span class="s1">iHashBit = self.bitHash_[d</span><span class="s3">, </span><span class="s1">j]</span>
                    <span class="s1">count_sketches[:</span><span class="s3">, </span><span class="s1">d</span><span class="s3">, </span><span class="s1">iHashIndex] += (</span>
                        <span class="s1">(iHashBit * X_gamma[:</span><span class="s3">, </span><span class="s1">j]).toarray().ravel()</span>
                    <span class="s1">)</span>

        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(X_gamma.shape[</span><span class="s5">1</span><span class="s1">]):</span>
                <span class="s3">for </span><span class="s1">d </span><span class="s3">in </span><span class="s1">range(self.degree):</span>
                    <span class="s1">iHashIndex = self.indexHash_[d</span><span class="s3">, </span><span class="s1">j]</span>
                    <span class="s1">iHashBit = self.bitHash_[d</span><span class="s3">, </span><span class="s1">j]</span>
                    <span class="s1">count_sketches[:</span><span class="s3">, </span><span class="s1">d</span><span class="s3">, </span><span class="s1">iHashIndex] += iHashBit * X_gamma[:</span><span class="s3">, </span><span class="s1">j]</span>

        <span class="s2"># For each same, compute a count sketch of phi(x) using the polynomial</span>
        <span class="s2"># multiplication (via FFT) of p count sketches of x.</span>
        <span class="s1">count_sketches_fft = fft(count_sketches</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">overwrite_x=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">count_sketches_fft_prod = np.prod(count_sketches_fft</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">data_sketch = np.real(ifft(count_sketches_fft_prod</span><span class="s3">, </span><span class="s1">overwrite_x=</span><span class="s3">True</span><span class="s1">))</span>

        <span class="s3">return </span><span class="s1">data_sketch</span>


<span class="s3">class </span><span class="s1">RBFSampler(ClassNamePrefixFeaturesOutMixin</span><span class="s3">, </span><span class="s1">TransformerMixin</span><span class="s3">, </span><span class="s1">BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Approximate a RBF kernel feature map using random Fourier features. 
 
    It implements a variant of Random Kitchen Sinks.[1] 
 
    Read more in the :ref:`User Guide &lt;rbf_kernel_approx&gt;`. 
 
    Parameters 
    ---------- 
    gamma : 'scale' or float, default=1.0 
        Parameter of RBF kernel: exp(-gamma * x^2). 
        If ``gamma='scale'`` is passed then it uses 
        1 / (n_features * X.var()) as value of gamma. 
 
        .. versionadded:: 1.2 
           The option `&quot;scale&quot;` was added in 1.2. 
 
    n_components : int, default=100 
        Number of Monte Carlo samples per original feature. 
        Equals the dimensionality of the computed feature space. 
 
    random_state : int, RandomState instance or None, default=None 
        Pseudo-random number generator to control the generation of the random 
        weights and random offset when fitting the training data. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    Attributes 
    ---------- 
    random_offset_ : ndarray of shape (n_components,), dtype={np.float64, np.float32} 
        Random offset used to compute the projection in the `n_components` 
        dimensions of the feature space. 
 
    random_weights_ : ndarray of shape (n_features, n_components),\ 
        dtype={np.float64, np.float32} 
        Random projection directions drawn from the Fourier transform 
        of the RBF kernel. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel. 
    Nystroem : Approximate a kernel map using a subset of the training data. 
    PolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch. 
    SkewedChi2Sampler : Approximate feature map for 
        &quot;skewed chi-squared&quot; kernel. 
    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels. 
 
    Notes 
    ----- 
    See &quot;Random Features for Large-Scale Kernel Machines&quot; by A. Rahimi and 
    Benjamin Recht. 
 
    [1] &quot;Weighted Sums of Random Kitchen Sinks: Replacing 
    minimization with randomization in learning&quot; by A. Rahimi and 
    Benjamin Recht. 
    (https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf) 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.kernel_approximation import RBFSampler 
    &gt;&gt;&gt; from sklearn.linear_model import SGDClassifier 
    &gt;&gt;&gt; X = [[0, 0], [1, 1], [1, 0], [0, 1]] 
    &gt;&gt;&gt; y = [0, 0, 1, 1] 
    &gt;&gt;&gt; rbf_feature = RBFSampler(gamma=1, random_state=1) 
    &gt;&gt;&gt; X_features = rbf_feature.fit_transform(X) 
    &gt;&gt;&gt; clf = SGDClassifier(max_iter=5, tol=1e-3) 
    &gt;&gt;&gt; clf.fit(X_features, y) 
    SGDClassifier(max_iter=5) 
    &gt;&gt;&gt; clf.score(X_features, y) 
    1.0 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;gamma&quot;</span><span class="s1">: [</span>
            <span class="s1">StrOptions({</span><span class="s4">&quot;scale&quot;</span><span class="s1">})</span><span class="s3">,</span>
            <span class="s1">Interval(Real</span><span class="s3">, </span><span class="s5">0.0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_components&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">gamma=</span><span class="s5">1.0</span><span class="s3">, </span><span class="s1">n_components=</span><span class="s5">100</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">self.gamma = gamma</span>
        <span class="s1">self.n_components = n_components</span>
        <span class="s1">self.random_state = random_state</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model with X. 
 
        Samples random projection according to n_features. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix}, shape (n_samples, n_features) 
            Training data, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        y : array-like, shape (n_samples,) or (n_samples, n_outputs), \ 
                default=None 
            Target values (None for unsupervised transformations). 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s1">)</span>
        <span class="s1">random_state = check_random_state(self.random_state)</span>
        <span class="s1">n_features = X.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">sparse = sp.issparse(X)</span>
        <span class="s3">if </span><span class="s1">self.gamma == </span><span class="s4">&quot;scale&quot;</span><span class="s1">:</span>
            <span class="s2"># var = E[X^2] - E[X]^2 if sparse</span>
            <span class="s1">X_var = (X.multiply(X)).mean() - (X.mean()) ** </span><span class="s5">2 </span><span class="s3">if </span><span class="s1">sparse </span><span class="s3">else </span><span class="s1">X.var()</span>
            <span class="s1">self._gamma = </span><span class="s5">1.0 </span><span class="s1">/ (n_features * X_var) </span><span class="s3">if </span><span class="s1">X_var != </span><span class="s5">0 </span><span class="s3">else </span><span class="s5">1.0</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._gamma = self.gamma</span>
        <span class="s1">self.random_weights_ = (</span><span class="s5">2.0 </span><span class="s1">* self._gamma) ** </span><span class="s5">0.5 </span><span class="s1">* random_state.normal(</span>
            <span class="s1">size=(n_features</span><span class="s3">, </span><span class="s1">self.n_components)</span>
        <span class="s1">)</span>

        <span class="s1">self.random_offset_ = random_state.uniform(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2 </span><span class="s1">* np.pi</span><span class="s3">, </span><span class="s1">size=self.n_components)</span>

        <span class="s3">if </span><span class="s1">X.dtype == np.float32:</span>
            <span class="s2"># Setting the data type of the fitted attribute will ensure the</span>
            <span class="s2"># output data type during `transform`.</span>
            <span class="s1">self.random_weights_ = self.random_weights_.astype(X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>
            <span class="s1">self.random_offset_ = self.random_offset_.astype(X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>

        <span class="s1">self._n_features_out = self.n_components</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">transform(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Apply the approximate feature map to X. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix}, shape (n_samples, n_features) 
            New data, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        Returns 
        ------- 
        X_new : array-like, shape (n_samples, n_components) 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">projection = safe_sparse_dot(X</span><span class="s3">, </span><span class="s1">self.random_weights_)</span>
        <span class="s1">projection += self.random_offset_</span>
        <span class="s1">np.cos(projection</span><span class="s3">, </span><span class="s1">projection)</span>
        <span class="s1">projection *= (</span><span class="s5">2.0 </span><span class="s1">/ self.n_components) ** </span><span class="s5">0.5</span>
        <span class="s3">return </span><span class="s1">projection</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span><span class="s4">&quot;preserves_dtype&quot;</span><span class="s1">: [np.float64</span><span class="s3">, </span><span class="s1">np.float32]}</span>


<span class="s3">class </span><span class="s1">SkewedChi2Sampler(</span>
    <span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s3">, </span><span class="s1">TransformerMixin</span><span class="s3">, </span><span class="s1">BaseEstimator</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Approximate feature map for &quot;skewed chi-squared&quot; kernel. 
 
    Read more in the :ref:`User Guide &lt;skewed_chi_kernel_approx&gt;`. 
 
    Parameters 
    ---------- 
    skewedness : float, default=1.0 
        &quot;skewedness&quot; parameter of the kernel. Needs to be cross-validated. 
 
    n_components : int, default=100 
        Number of Monte Carlo samples per original feature. 
        Equals the dimensionality of the computed feature space. 
 
    random_state : int, RandomState instance or None, default=None 
        Pseudo-random number generator to control the generation of the random 
        weights and random offset when fitting the training data. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    Attributes 
    ---------- 
    random_weights_ : ndarray of shape (n_features, n_components) 
        Weight array, sampled from a secant hyperbolic distribution, which will 
        be used to linearly transform the log of the data. 
 
    random_offset_ : ndarray of shape (n_features, n_components) 
        Bias term, which will be added to the data. It is uniformly distributed 
        between 0 and 2*pi. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel. 
    Nystroem : Approximate a kernel map using a subset of the training data. 
    RBFSampler : Approximate a RBF kernel feature map using random Fourier 
        features. 
    SkewedChi2Sampler : Approximate feature map for &quot;skewed chi-squared&quot; kernel. 
    sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel. 
    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels. 
 
    References 
    ---------- 
    See &quot;Random Fourier Approximations for Skewed Multiplicative Histogram 
    Kernels&quot; by Fuxin Li, Catalin Ionescu and Cristian Sminchisescu. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.kernel_approximation import SkewedChi2Sampler 
    &gt;&gt;&gt; from sklearn.linear_model import SGDClassifier 
    &gt;&gt;&gt; X = [[0, 0], [1, 1], [1, 0], [0, 1]] 
    &gt;&gt;&gt; y = [0, 0, 1, 1] 
    &gt;&gt;&gt; chi2_feature = SkewedChi2Sampler(skewedness=.01, 
    ...                                  n_components=10, 
    ...                                  random_state=0) 
    &gt;&gt;&gt; X_features = chi2_feature.fit_transform(X, y) 
    &gt;&gt;&gt; clf = SGDClassifier(max_iter=10, tol=1e-3) 
    &gt;&gt;&gt; clf.fit(X_features, y) 
    SGDClassifier(max_iter=10) 
    &gt;&gt;&gt; clf.score(X_features, y) 
    1.0 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;skewedness&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, None, None, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;n_components&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">skewedness=</span><span class="s5">1.0</span><span class="s3">, </span><span class="s1">n_components=</span><span class="s5">100</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">self.skewedness = skewedness</span>
        <span class="s1">self.n_components = n_components</span>
        <span class="s1">self.random_state = random_state</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model with X. 
 
        Samples random projection according to n_features. 
 
        Parameters 
        ---------- 
        X : array-like, shape (n_samples, n_features) 
            Training data, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        y : array-like, shape (n_samples,) or (n_samples, n_outputs), \ 
                default=None 
            Target values (None for unsupervised transformations). 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X)</span>
        <span class="s1">random_state = check_random_state(self.random_state)</span>
        <span class="s1">n_features = X.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">uniform = random_state.uniform(size=(n_features</span><span class="s3">, </span><span class="s1">self.n_components))</span>
        <span class="s2"># transform by inverse CDF of sech</span>
        <span class="s1">self.random_weights_ = </span><span class="s5">1.0 </span><span class="s1">/ np.pi * np.log(np.tan(np.pi / </span><span class="s5">2.0 </span><span class="s1">* uniform))</span>
        <span class="s1">self.random_offset_ = random_state.uniform(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2 </span><span class="s1">* np.pi</span><span class="s3">, </span><span class="s1">size=self.n_components)</span>

        <span class="s3">if </span><span class="s1">X.dtype == np.float32:</span>
            <span class="s2"># Setting the data type of the fitted attribute will ensure the</span>
            <span class="s2"># output data type during `transform`.</span>
            <span class="s1">self.random_weights_ = self.random_weights_.astype(X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>
            <span class="s1">self.random_offset_ = self.random_offset_.astype(X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>

        <span class="s1">self._n_features_out = self.n_components</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">transform(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Apply the approximate feature map to X. 
 
        Parameters 
        ---------- 
        X : array-like, shape (n_samples, n_features) 
            New data, where `n_samples` is the number of samples 
            and `n_features` is the number of features. All values of X must be 
            strictly greater than &quot;-skewedness&quot;. 
 
        Returns 
        ------- 
        X_new : array-like, shape (n_samples, n_components) 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">X = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">True, </span><span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False</span>
        <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">(X &lt;= -self.skewedness).any():</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;X may not contain entries smaller than -skewedness.&quot;</span><span class="s1">)</span>

        <span class="s1">X += self.skewedness</span>
        <span class="s1">np.log(X</span><span class="s3">, </span><span class="s1">X)</span>
        <span class="s1">projection = safe_sparse_dot(X</span><span class="s3">, </span><span class="s1">self.random_weights_)</span>
        <span class="s1">projection += self.random_offset_</span>
        <span class="s1">np.cos(projection</span><span class="s3">, </span><span class="s1">projection)</span>
        <span class="s1">projection *= np.sqrt(</span><span class="s5">2.0</span><span class="s1">) / np.sqrt(self.n_components)</span>
        <span class="s3">return </span><span class="s1">projection</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span><span class="s4">&quot;preserves_dtype&quot;</span><span class="s1">: [np.float64</span><span class="s3">, </span><span class="s1">np.float32]}</span>


<span class="s3">class </span><span class="s1">AdditiveChi2Sampler(TransformerMixin</span><span class="s3">, </span><span class="s1">BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Approximate feature map for additive chi2 kernel. 
 
    Uses sampling the fourier transform of the kernel characteristic 
    at regular intervals. 
 
    Since the kernel that is to be approximated is additive, the components of 
    the input vectors can be treated separately.  Each entry in the original 
    space is transformed into 2*sample_steps-1 features, where sample_steps is 
    a parameter of the method. Typical values of sample_steps include 1, 2 and 
    3. 
 
    Optimal choices for the sampling interval for certain data ranges can be 
    computed (see the reference). The default values should be reasonable. 
 
    Read more in the :ref:`User Guide &lt;additive_chi_kernel_approx&gt;`. 
 
    Parameters 
    ---------- 
    sample_steps : int, default=2 
        Gives the number of (complex) sampling points. 
 
    sample_interval : float, default=None 
        Sampling interval. Must be specified when sample_steps not in {1,2,3}. 
 
    Attributes 
    ---------- 
    sample_interval_ : float 
        Stored sampling interval. Specified as a parameter if `sample_steps` 
        not in {1,2,3}. 
 
        .. deprecated:: 1.3 
           `sample_interval_` serves internal purposes only and will be removed in 1.5. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    SkewedChi2Sampler : A Fourier-approximation to a non-additive variant of 
        the chi squared kernel. 
 
    sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel. 
 
    sklearn.metrics.pairwise.additive_chi2_kernel : The exact additive chi 
        squared kernel. 
 
    Notes 
    ----- 
    This estimator approximates a slightly different version of the additive 
    chi squared kernel then ``metric.additive_chi2`` computes. 
 
    This estimator is stateless and does not need to be fitted. However, we 
    recommend to call :meth:`fit_transform` instead of :meth:`transform`, as 
    parameter validation is only performed in :meth:`fit`. 
 
    References 
    ---------- 
    See `&quot;Efficient additive kernels via explicit feature maps&quot; 
    &lt;http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf&gt;`_ 
    A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence, 
    2011 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import load_digits 
    &gt;&gt;&gt; from sklearn.linear_model import SGDClassifier 
    &gt;&gt;&gt; from sklearn.kernel_approximation import AdditiveChi2Sampler 
    &gt;&gt;&gt; X, y = load_digits(return_X_y=True) 
    &gt;&gt;&gt; chi2sampler = AdditiveChi2Sampler(sample_steps=2) 
    &gt;&gt;&gt; X_transformed = chi2sampler.fit_transform(X, y) 
    &gt;&gt;&gt; clf = SGDClassifier(max_iter=5, random_state=0, tol=1e-3) 
    &gt;&gt;&gt; clf.fit(X_transformed, y) 
    SGDClassifier(max_iter=5, random_state=0) 
    &gt;&gt;&gt; clf.score(X_transformed, y) 
    0.9499... 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;sample_steps&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;sample_interval&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">*</span><span class="s3">, </span><span class="s1">sample_steps=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">sample_interval=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">self.sample_steps = sample_steps</span>
        <span class="s1">self.sample_interval = sample_interval</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Only validates estimator's parameters. 
 
        This method allows to: (i) validate the estimator's parameters and 
        (ii) be consistent with the scikit-learn transformer API. 
 
        Parameters 
        ---------- 
        X : array-like, shape (n_samples, n_features) 
            Training data, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        y : array-like, shape (n_samples,) or (n_samples, n_outputs), \ 
                default=None 
            Target values (None for unsupervised transformations). 
 
        Returns 
        ------- 
        self : object 
            Returns the transformer. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s1">)</span>
        <span class="s1">check_non_negative(X</span><span class="s3">, </span><span class="s4">&quot;X in AdditiveChi2Sampler.fit&quot;</span><span class="s1">)</span>

        <span class="s2"># TODO(1.5): remove the setting of _sample_interval from fit</span>
        <span class="s3">if </span><span class="s1">self.sample_interval </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s2"># See figure 2 c) of &quot;Efficient additive kernels via explicit feature maps&quot;</span>
            <span class="s2"># &lt;http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf&gt;</span>
            <span class="s2"># A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence,</span>
            <span class="s2"># 2011</span>
            <span class="s3">if </span><span class="s1">self.sample_steps == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">self._sample_interval = </span><span class="s5">0.8</span>
            <span class="s3">elif </span><span class="s1">self.sample_steps == </span><span class="s5">2</span><span class="s1">:</span>
                <span class="s1">self._sample_interval = </span><span class="s5">0.5</span>
            <span class="s3">elif </span><span class="s1">self.sample_steps == </span><span class="s5">3</span><span class="s1">:</span>
                <span class="s1">self._sample_interval = </span><span class="s5">0.4</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;If sample_steps is not in [1, 2, 3],&quot;</span>
                    <span class="s4">&quot; you need to provide sample_interval&quot;</span>
                <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._sample_interval = self.sample_interval</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s2"># TODO(1.5): remove</span>
    <span class="s1">@deprecated(  </span><span class="s2"># type: ignore</span>
        <span class="s4">&quot;The ``sample_interval_`` attribute was deprecated in version 1.3 and &quot;</span>
        <span class="s4">&quot;will be removed 1.5.&quot;</span>
    <span class="s1">)</span>
    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">sample_interval_(self):</span>
        <span class="s3">return </span><span class="s1">self._sample_interval</span>

    <span class="s3">def </span><span class="s1">transform(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Apply approximate feature map to X. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix}, shape (n_samples, n_features) 
            Training data, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        Returns 
        ------- 
        X_new : {ndarray, sparse matrix}, \ 
               shape = (n_samples, n_features * (2*sample_steps - 1)) 
            Whether the return value is an array or sparse matrix depends on 
            the type of the input X. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">check_non_negative(X</span><span class="s3">, </span><span class="s4">&quot;X in AdditiveChi2Sampler.transform&quot;</span><span class="s1">)</span>
        <span class="s1">sparse = sp.issparse(X)</span>

        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;_sample_interval&quot;</span><span class="s1">):</span>
            <span class="s2"># TODO(1.5): remove this branch</span>
            <span class="s1">sample_interval = self._sample_interval</span>

        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self.sample_interval </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s2"># See figure 2 c) of &quot;Efficient additive kernels via explicit feature maps&quot; # noqa</span>
                <span class="s2"># &lt;http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf&gt;</span>
                <span class="s2"># A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence, # noqa</span>
                <span class="s2"># 2011</span>
                <span class="s3">if </span><span class="s1">self.sample_steps == </span><span class="s5">1</span><span class="s1">:</span>
                    <span class="s1">sample_interval = </span><span class="s5">0.8</span>
                <span class="s3">elif </span><span class="s1">self.sample_steps == </span><span class="s5">2</span><span class="s1">:</span>
                    <span class="s1">sample_interval = </span><span class="s5">0.5</span>
                <span class="s3">elif </span><span class="s1">self.sample_steps == </span><span class="s5">3</span><span class="s1">:</span>
                    <span class="s1">sample_interval = </span><span class="s5">0.4</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s3">raise </span><span class="s1">ValueError(</span>
                        <span class="s4">&quot;If sample_steps is not in [1, 2, 3],&quot;</span>
                        <span class="s4">&quot; you need to provide sample_interval&quot;</span>
                    <span class="s1">)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">sample_interval = self.sample_interval</span>

        <span class="s2"># zeroth component</span>
        <span class="s2"># 1/cosh = sech</span>
        <span class="s2"># cosh(0) = 1.0</span>
        <span class="s1">transf = self._transform_sparse </span><span class="s3">if </span><span class="s1">sparse </span><span class="s3">else </span><span class="s1">self._transform_dense</span>
        <span class="s3">return </span><span class="s1">transf(X</span><span class="s3">, </span><span class="s1">self.sample_steps</span><span class="s3">, </span><span class="s1">sample_interval)</span>

    <span class="s3">def </span><span class="s1">get_feature_names_out(self</span><span class="s3">, </span><span class="s1">input_features=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Get output feature names for transformation. 
 
        Parameters 
        ---------- 
        input_features : array-like of str or None, default=None 
            Only used to validate feature names with the names seen in :meth:`fit`. 
 
        Returns 
        ------- 
        feature_names_out : ndarray of str objects 
            Transformed feature names. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self</span><span class="s3">, </span><span class="s4">&quot;n_features_in_&quot;</span><span class="s1">)</span>
        <span class="s1">input_features = _check_feature_names_in(</span>
            <span class="s1">self</span><span class="s3">, </span><span class="s1">input_features</span><span class="s3">, </span><span class="s1">generate_names=</span><span class="s3">True</span>
        <span class="s1">)</span>
        <span class="s1">est_name = self.__class__.__name__.lower()</span>

        <span class="s1">names_list = [</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">est_name</span><span class="s3">}</span><span class="s4">_</span><span class="s3">{</span><span class="s1">name</span><span class="s3">}</span><span class="s4">_sqrt&quot; </span><span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">input_features]</span>

        <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">self.sample_steps):</span>
            <span class="s1">cos_names = [</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">est_name</span><span class="s3">}</span><span class="s4">_</span><span class="s3">{</span><span class="s1">name</span><span class="s3">}</span><span class="s4">_cos</span><span class="s3">{</span><span class="s1">j</span><span class="s3">}</span><span class="s4">&quot; </span><span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">input_features]</span>
            <span class="s1">sin_names = [</span><span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">est_name</span><span class="s3">}</span><span class="s4">_</span><span class="s3">{</span><span class="s1">name</span><span class="s3">}</span><span class="s4">_sin</span><span class="s3">{</span><span class="s1">j</span><span class="s3">}</span><span class="s4">&quot; </span><span class="s3">for </span><span class="s1">name </span><span class="s3">in </span><span class="s1">input_features]</span>
            <span class="s1">names_list.extend(cos_names + sin_names)</span>

        <span class="s3">return </span><span class="s1">np.asarray(names_list</span><span class="s3">, </span><span class="s1">dtype=object)</span>

    <span class="s1">@staticmethod</span>
    <span class="s3">def </span><span class="s1">_transform_dense(X</span><span class="s3">, </span><span class="s1">sample_steps</span><span class="s3">, </span><span class="s1">sample_interval):</span>
        <span class="s1">non_zero = X != </span><span class="s5">0.0</span>
        <span class="s1">X_nz = X[non_zero]</span>

        <span class="s1">X_step = np.zeros_like(X)</span>
        <span class="s1">X_step[non_zero] = np.sqrt(X_nz * sample_interval)</span>

        <span class="s1">X_new = [X_step]</span>

        <span class="s1">log_step_nz = sample_interval * np.log(X_nz)</span>
        <span class="s1">step_nz = </span><span class="s5">2 </span><span class="s1">* X_nz * sample_interval</span>

        <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">sample_steps):</span>
            <span class="s1">factor_nz = np.sqrt(step_nz / np.cosh(np.pi * j * sample_interval))</span>

            <span class="s1">X_step = np.zeros_like(X)</span>
            <span class="s1">X_step[non_zero] = factor_nz * np.cos(j * log_step_nz)</span>
            <span class="s1">X_new.append(X_step)</span>

            <span class="s1">X_step = np.zeros_like(X)</span>
            <span class="s1">X_step[non_zero] = factor_nz * np.sin(j * log_step_nz)</span>
            <span class="s1">X_new.append(X_step)</span>

        <span class="s3">return </span><span class="s1">np.hstack(X_new)</span>

    <span class="s1">@staticmethod</span>
    <span class="s3">def </span><span class="s1">_transform_sparse(X</span><span class="s3">, </span><span class="s1">sample_steps</span><span class="s3">, </span><span class="s1">sample_interval):</span>
        <span class="s1">indices = X.indices.copy()</span>
        <span class="s1">indptr = X.indptr.copy()</span>

        <span class="s1">data_step = np.sqrt(X.data * sample_interval)</span>
        <span class="s1">X_step = sp.csr_matrix(</span>
            <span class="s1">(data_step</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr)</span><span class="s3">, </span><span class="s1">shape=X.shape</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span>
        <span class="s1">)</span>
        <span class="s1">X_new = [X_step]</span>

        <span class="s1">log_step_nz = sample_interval * np.log(X.data)</span>
        <span class="s1">step_nz = </span><span class="s5">2 </span><span class="s1">* X.data * sample_interval</span>

        <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">sample_steps):</span>
            <span class="s1">factor_nz = np.sqrt(step_nz / np.cosh(np.pi * j * sample_interval))</span>

            <span class="s1">data_step = factor_nz * np.cos(j * log_step_nz)</span>
            <span class="s1">X_step = sp.csr_matrix(</span>
                <span class="s1">(data_step</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr)</span><span class="s3">, </span><span class="s1">shape=X.shape</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span>
            <span class="s1">)</span>
            <span class="s1">X_new.append(X_step)</span>

            <span class="s1">data_step = factor_nz * np.sin(j * log_step_nz)</span>
            <span class="s1">X_step = sp.csr_matrix(</span>
                <span class="s1">(data_step</span><span class="s3">, </span><span class="s1">indices</span><span class="s3">, </span><span class="s1">indptr)</span><span class="s3">, </span><span class="s1">shape=X.shape</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span>
            <span class="s1">)</span>
            <span class="s1">X_new.append(X_step)</span>

        <span class="s3">return </span><span class="s1">sp.hstack(X_new)</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span><span class="s4">&quot;stateless&quot;</span><span class="s1">: </span><span class="s3">True, </span><span class="s4">&quot;requires_positive_X&quot;</span><span class="s1">: </span><span class="s3">True</span><span class="s1">}</span>


<span class="s3">class </span><span class="s1">Nystroem(ClassNamePrefixFeaturesOutMixin</span><span class="s3">, </span><span class="s1">TransformerMixin</span><span class="s3">, </span><span class="s1">BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Approximate a kernel map using a subset of the training data. 
 
    Constructs an approximate feature map for an arbitrary kernel 
    using a subset of the data as basis. 
 
    Read more in the :ref:`User Guide &lt;nystroem_kernel_approx&gt;`. 
 
    .. versionadded:: 0.13 
 
    Parameters 
    ---------- 
    kernel : str or callable, default='rbf' 
        Kernel map to be approximated. A callable should accept two arguments 
        and the keyword arguments passed to this object as `kernel_params`, and 
        should return a floating point number. 
 
    gamma : float, default=None 
        Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 
        and sigmoid kernels. Interpretation of the default value is left to 
        the kernel; see the documentation for sklearn.metrics.pairwise. 
        Ignored by other kernels. 
 
    coef0 : float, default=None 
        Zero coefficient for polynomial and sigmoid kernels. 
        Ignored by other kernels. 
 
    degree : float, default=None 
        Degree of the polynomial kernel. Ignored by other kernels. 
 
    kernel_params : dict, default=None 
        Additional parameters (keyword arguments) for kernel function passed 
        as callable object. 
 
    n_components : int, default=100 
        Number of features to construct. 
        How many data points will be used to construct the mapping. 
 
    random_state : int, RandomState instance or None, default=None 
        Pseudo-random number generator to control the uniform sampling without 
        replacement of `n_components` of the training data to construct the 
        basis kernel. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    n_jobs : int, default=None 
        The number of jobs to use for the computation. This works by breaking 
        down the kernel matrix into `n_jobs` even slices and computing them in 
        parallel. 
 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
        .. versionadded:: 0.24 
 
    Attributes 
    ---------- 
    components_ : ndarray of shape (n_components, n_features) 
        Subset of training points used to construct the feature map. 
 
    component_indices_ : ndarray of shape (n_components) 
        Indices of ``components_`` in the training set. 
 
    normalization_ : ndarray of shape (n_components, n_components) 
        Normalization matrix needed for embedding. 
        Square root of the kernel matrix on ``components_``. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel. 
    PolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch. 
    RBFSampler : Approximate a RBF kernel feature map using random Fourier 
        features. 
    SkewedChi2Sampler : Approximate feature map for &quot;skewed chi-squared&quot; kernel. 
    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels. 
 
    References 
    ---------- 
    * Williams, C.K.I. and Seeger, M. 
      &quot;Using the Nystroem method to speed up kernel machines&quot;, 
      Advances in neural information processing systems 2001 
 
    * T. Yang, Y. Li, M. Mahdavi, R. Jin and Z. Zhou 
      &quot;Nystroem Method vs Random Fourier Features: A Theoretical and Empirical 
      Comparison&quot;, 
      Advances in Neural Information Processing Systems 2012 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn import datasets, svm 
    &gt;&gt;&gt; from sklearn.kernel_approximation import Nystroem 
    &gt;&gt;&gt; X, y = datasets.load_digits(n_class=9, return_X_y=True) 
    &gt;&gt;&gt; data = X / 16. 
    &gt;&gt;&gt; clf = svm.LinearSVC(dual=&quot;auto&quot;) 
    &gt;&gt;&gt; feature_map_nystroem = Nystroem(gamma=.2, 
    ...                                 random_state=1, 
    ...                                 n_components=300) 
    &gt;&gt;&gt; data_transformed = feature_map_nystroem.fit_transform(data) 
    &gt;&gt;&gt; clf.fit(data_transformed, y) 
    LinearSVC(dual='auto') 
    &gt;&gt;&gt; clf.score(data_transformed, y) 
    0.9987... 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;kernel&quot;</span><span class="s1">: [</span>
            <span class="s1">StrOptions(set(PAIRWISE_KERNEL_FUNCTIONS.keys()) | {</span><span class="s4">&quot;precomputed&quot;</span><span class="s1">})</span><span class="s3">,</span>
            <span class="s1">callable</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;gamma&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;coef0&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, None, None, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;degree&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;kernel_params&quot;</span><span class="s1">: [dict</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_components&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_jobs&quot;</span><span class="s1">: [Integral</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">kernel=</span><span class="s4">&quot;rbf&quot;</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">gamma=</span><span class="s3">None,</span>
        <span class="s1">coef0=</span><span class="s3">None,</span>
        <span class="s1">degree=</span><span class="s3">None,</span>
        <span class="s1">kernel_params=</span><span class="s3">None,</span>
        <span class="s1">n_components=</span><span class="s5">100</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
        <span class="s1">n_jobs=</span><span class="s3">None,</span>
    <span class="s1">):</span>
        <span class="s1">self.kernel = kernel</span>
        <span class="s1">self.gamma = gamma</span>
        <span class="s1">self.coef0 = coef0</span>
        <span class="s1">self.degree = degree</span>
        <span class="s1">self.kernel_params = kernel_params</span>
        <span class="s1">self.n_components = n_components</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.n_jobs = n_jobs</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit estimator to data. 
 
        Samples a subset of training points, computes kernel 
        on these and computes normalization matrix. 
 
        Parameters 
        ---------- 
        X : array-like, shape (n_samples, n_features) 
            Training data, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        y : array-like, shape (n_samples,) or (n_samples, n_outputs), \ 
                default=None 
            Target values (None for unsupervised transformations). 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s1">)</span>
        <span class="s1">rnd = check_random_state(self.random_state)</span>
        <span class="s1">n_samples = X.shape[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s2"># get basis vectors</span>
        <span class="s3">if </span><span class="s1">self.n_components &gt; n_samples:</span>
            <span class="s2"># XXX should we just bail?</span>
            <span class="s1">n_components = n_samples</span>
            <span class="s1">warnings.warn(</span>
                <span class="s4">&quot;n_components &gt; n_samples. This is not possible.</span><span class="s3">\n</span><span class="s4">&quot;</span>
                <span class="s4">&quot;n_components was set to n_samples, which results&quot;</span>
                <span class="s4">&quot; in inefficient evaluation of the full kernel.&quot;</span>
            <span class="s1">)</span>

        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">n_components = self.n_components</span>
        <span class="s1">n_components = min(n_samples</span><span class="s3">, </span><span class="s1">n_components)</span>
        <span class="s1">inds = rnd.permutation(n_samples)</span>
        <span class="s1">basis_inds = inds[:n_components]</span>
        <span class="s1">basis = X[basis_inds]</span>

        <span class="s1">basis_kernel = pairwise_kernels(</span>
            <span class="s1">basis</span><span class="s3">,</span>
            <span class="s1">metric=self.kernel</span><span class="s3">,</span>
            <span class="s1">filter_params=</span><span class="s3">True,</span>
            <span class="s1">n_jobs=self.n_jobs</span><span class="s3">,</span>
            <span class="s1">**self._get_kernel_params()</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s2"># sqrt of kernel matrix on basis vectors</span>
        <span class="s1">U</span><span class="s3">, </span><span class="s1">S</span><span class="s3">, </span><span class="s1">V = svd(basis_kernel)</span>
        <span class="s1">S = np.maximum(S</span><span class="s3">, </span><span class="s5">1e-12</span><span class="s1">)</span>
        <span class="s1">self.normalization_ = np.dot(U / np.sqrt(S)</span><span class="s3">, </span><span class="s1">V)</span>
        <span class="s1">self.components_ = basis</span>
        <span class="s1">self.component_indices_ = basis_inds</span>
        <span class="s1">self._n_features_out = n_components</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">transform(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Apply feature map to X. 
 
        Computes an approximate feature map using the kernel 
        between some training points and X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Data to transform. 
 
        Returns 
        ------- 
        X_transformed : ndarray of shape (n_samples, n_components) 
            Transformed data. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False</span><span class="s1">)</span>

        <span class="s1">kernel_params = self._get_kernel_params()</span>
        <span class="s1">embedded = pairwise_kernels(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">self.components_</span><span class="s3">,</span>
            <span class="s1">metric=self.kernel</span><span class="s3">,</span>
            <span class="s1">filter_params=</span><span class="s3">True,</span>
            <span class="s1">n_jobs=self.n_jobs</span><span class="s3">,</span>
            <span class="s1">**kernel_params</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">np.dot(embedded</span><span class="s3">, </span><span class="s1">self.normalization_.T)</span>

    <span class="s3">def </span><span class="s1">_get_kernel_params(self):</span>
        <span class="s1">params = self.kernel_params</span>
        <span class="s3">if </span><span class="s1">params </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">params = {}</span>
        <span class="s3">if not </span><span class="s1">callable(self.kernel) </span><span class="s3">and </span><span class="s1">self.kernel != </span><span class="s4">&quot;precomputed&quot;</span><span class="s1">:</span>
            <span class="s3">for </span><span class="s1">param </span><span class="s3">in </span><span class="s1">KERNEL_PARAMS[self.kernel]:</span>
                <span class="s3">if </span><span class="s1">getattr(self</span><span class="s3">, </span><span class="s1">param) </span><span class="s3">is not None</span><span class="s1">:</span>
                    <span class="s1">params[param] = getattr(self</span><span class="s3">, </span><span class="s1">param)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">(</span>
                <span class="s1">self.gamma </span><span class="s3">is not None</span>
                <span class="s3">or </span><span class="s1">self.coef0 </span><span class="s3">is not None</span>
                <span class="s3">or </span><span class="s1">self.degree </span><span class="s3">is not None</span>
            <span class="s1">):</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;Don't pass gamma, coef0 or degree to &quot;</span>
                    <span class="s4">&quot;Nystroem if using a callable &quot;</span>
                    <span class="s4">&quot;or precomputed kernel&quot;</span>
                <span class="s1">)</span>

        <span class="s3">return </span><span class="s1">params</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span>
            <span class="s4">&quot;_xfail_checks&quot;</span><span class="s1">: {</span>
                <span class="s4">&quot;check_transformer_preserve_dtypes&quot;</span><span class="s1">: (</span>
                    <span class="s4">&quot;dtypes are preserved but not at a close enough precision&quot;</span>
                <span class="s1">)</span>
            <span class="s1">}</span><span class="s3">,</span>
            <span class="s4">&quot;preserves_dtype&quot;</span><span class="s1">: [np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">,</span>
        <span class="s1">}</span>
</pre>
</body>
</html>