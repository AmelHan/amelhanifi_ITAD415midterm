<html>
<head>
<title>_prediction.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_prediction.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot; 
Created on Fri Dec 19 11:29:18 2014 
 
Author: Josef Perktold 
License: BSD-3 
 
&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">stats</span>
<span class="s3">import </span><span class="s1">pandas </span><span class="s3">as </span><span class="s1">pd</span>


<span class="s0"># this is similar to ContrastResults after t_test, copied and adjusted</span>
<span class="s3">class </span><span class="s1">PredictionResults:</span>
    <span class="s2">&quot;&quot;&quot; 
    Results class for predictions. 
 
    Parameters 
    ---------- 
    predicted_mean : ndarray 
        The array containing the prediction means. 
    var_pred_mean : ndarray 
        The array of the variance of the prediction means. 
    var_resid : ndarray 
        The array of residual variances. 
    df : int 
        The degree of freedom used if dist is 't'. 
    dist : {'norm', 't', object} 
        Either a string for the normal or t distribution or another object 
        that exposes a `ppf` method. 
    row_labels : list[str] 
        Row labels used in summary frame. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">predicted_mean</span><span class="s3">, </span><span class="s1">var_pred_mean</span><span class="s3">, </span><span class="s1">var_resid</span><span class="s3">,</span>
                 <span class="s1">df=</span><span class="s3">None, </span><span class="s1">dist=</span><span class="s3">None, </span><span class="s1">row_labels=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">self.predicted = predicted_mean</span>
        <span class="s1">self.var_pred = var_pred_mean</span>
        <span class="s1">self.df = df</span>
        <span class="s1">self.var_resid = var_resid</span>
        <span class="s1">self.row_labels = row_labels</span>

        <span class="s3">if </span><span class="s1">dist </span><span class="s3">is None or </span><span class="s1">dist == </span><span class="s4">'norm'</span><span class="s1">:</span>
            <span class="s1">self.dist = stats.norm</span>
            <span class="s1">self.dist_args = ()</span>
        <span class="s3">elif </span><span class="s1">dist == </span><span class="s4">'t'</span><span class="s1">:</span>
            <span class="s1">self.dist = stats.t</span>
            <span class="s1">self.dist_args = (self.df</span><span class="s3">,</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.dist = dist</span>
            <span class="s1">self.dist_args = ()</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">se_obs(self):</span>
        <span class="s3">return </span><span class="s1">np.sqrt(self.var_pred_mean + self.var_resid)</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">se_mean(self):</span>
        <span class="s3">return </span><span class="s1">self.se</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">predicted_mean(self):</span>
        <span class="s0"># alias for backwards compatibility</span>
        <span class="s3">return </span><span class="s1">self.predicted</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">var_pred_mean(self):</span>
        <span class="s0"># alias for backwards compatibility</span>
        <span class="s3">return </span><span class="s1">self.var_pred</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">se(self):</span>
        <span class="s0"># alias for backwards compatibility</span>
        <span class="s3">return </span><span class="s1">np.sqrt(self.var_pred_mean)</span>

    <span class="s3">def </span><span class="s1">conf_int(self</span><span class="s3">, </span><span class="s1">obs=</span><span class="s3">False, </span><span class="s1">alpha=</span><span class="s5">0.05</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Returns the confidence interval of the value, `effect` of the 
        constraint. 
 
        This is currently only available for t and z tests. 
 
        Parameters 
        ---------- 
        alpha : float, optional 
            The significance level for the confidence interval. 
            ie., The default `alpha` = .05 returns a 95% confidence interval. 
 
        Returns 
        ------- 
        ci : ndarray, (k_constraints, 2) 
            The array has the lower and the upper limit of the confidence 
            interval in the columns. 
        &quot;&quot;&quot;</span>

        <span class="s1">se = self.se_obs </span><span class="s3">if </span><span class="s1">obs </span><span class="s3">else </span><span class="s1">self.se_mean</span>

        <span class="s1">q = self.dist.ppf(</span><span class="s5">1 </span><span class="s1">- alpha / </span><span class="s5">2.</span><span class="s3">, </span><span class="s1">*self.dist_args)</span>
        <span class="s1">lower = self.predicted_mean - q * se</span>
        <span class="s1">upper = self.predicted_mean + q * se</span>
        <span class="s3">return </span><span class="s1">np.column_stack((lower</span><span class="s3">, </span><span class="s1">upper))</span>

    <span class="s3">def </span><span class="s1">summary_frame(self</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s5">0.05</span><span class="s1">):</span>
        <span class="s0"># TODO: finish and cleanup</span>
        <span class="s1">ci_obs = self.conf_int(alpha=alpha</span><span class="s3">, </span><span class="s1">obs=</span><span class="s3">True</span><span class="s1">)  </span><span class="s0"># need to split</span>
        <span class="s1">ci_mean = self.conf_int(alpha=alpha</span><span class="s3">, </span><span class="s1">obs=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">to_include = {}</span>
        <span class="s1">to_include[</span><span class="s4">'mean'</span><span class="s1">] = self.predicted_mean</span>
        <span class="s1">to_include[</span><span class="s4">'mean_se'</span><span class="s1">] = self.se_mean</span>
        <span class="s1">to_include[</span><span class="s4">'mean_ci_lower'</span><span class="s1">] = ci_mean[:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">to_include[</span><span class="s4">'mean_ci_upper'</span><span class="s1">] = ci_mean[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">to_include[</span><span class="s4">'obs_ci_lower'</span><span class="s1">] = ci_obs[:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">to_include[</span><span class="s4">'obs_ci_upper'</span><span class="s1">] = ci_obs[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">self.table = to_include</span>
        <span class="s0"># pandas dict does not handle 2d_array</span>
        <span class="s0"># data = np.column_stack(list(to_include.values()))</span>
        <span class="s0"># names = ....</span>
        <span class="s1">res = pd.DataFrame(to_include</span><span class="s3">, </span><span class="s1">index=self.row_labels</span><span class="s3">,</span>
                           <span class="s1">columns=to_include.keys())</span>
        <span class="s3">return </span><span class="s1">res</span>


<span class="s3">def </span><span class="s1">get_prediction(self</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">transform=</span><span class="s3">True, </span><span class="s1">weights=</span><span class="s3">None,</span>
                   <span class="s1">row_labels=</span><span class="s3">None, </span><span class="s1">pred_kwds=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot; 
    Compute prediction results. 
 
    Parameters 
    ---------- 
    exog : array_like, optional 
        The values for which you want to predict. 
    transform : bool, optional 
        If the model was fit via a formula, do you want to pass 
        exog through the formula. Default is True. E.g., if you fit 
        a model y ~ log(x1) + log(x2), and transform is True, then 
        you can pass a data structure that contains x1 and x2 in 
        their original form. Otherwise, you'd need to log the data 
        first. 
    weights : array_like, optional 
        Weights interpreted as in WLS, used for the variance of the predicted 
        residual. 
    row_labels : list 
        A list of row labels to use.  If not provided, read `exog` is 
        available. 
    **kwargs 
        Some models can take additional keyword arguments, see the predict 
        method of the model for the details. 
 
    Returns 
    ------- 
    linear_model.PredictionResults 
        The prediction results instance contains prediction and prediction 
        variance and can on demand calculate confidence intervals and summary 
        tables for the prediction of the mean and of new observations. 
    &quot;&quot;&quot;</span>

    <span class="s0"># prepare exog and row_labels, based on base Results.predict</span>
    <span class="s3">if </span><span class="s1">transform </span><span class="s3">and </span><span class="s1">hasattr(self.model</span><span class="s3">, </span><span class="s4">'formula'</span><span class="s1">) </span><span class="s3">and </span><span class="s1">exog </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s3">from </span><span class="s1">patsy </span><span class="s3">import </span><span class="s1">dmatrix</span>
        <span class="s3">if </span><span class="s1">isinstance(exog</span><span class="s3">, </span><span class="s1">pd.Series):</span>
            <span class="s0"># GH-6509</span>
            <span class="s1">exog = pd.DataFrame(exog)</span>
        <span class="s1">exog = dmatrix(self.model.data.design_info</span><span class="s3">, </span><span class="s1">exog)</span>

    <span class="s3">if </span><span class="s1">exog </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">row_labels </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">row_labels = getattr(exog</span><span class="s3">, </span><span class="s4">'index'</span><span class="s3">, None</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">callable(row_labels):</span>
                <span class="s1">row_labels = </span><span class="s3">None</span>

        <span class="s1">exog = np.asarray(exog)</span>
        <span class="s3">if </span><span class="s1">exog.ndim == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s0"># Params informs whether a row or column vector</span>
            <span class="s3">if </span><span class="s1">self.params.shape[</span><span class="s5">0</span><span class="s1">] &gt; </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">exog = exog[</span><span class="s3">None, </span><span class="s1">:]</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">exog = exog[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s1">exog = np.atleast_2d(exog)  </span><span class="s0"># needed in count model shape[1]</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">exog = self.model.exog</span>
        <span class="s3">if </span><span class="s1">weights </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">weights = getattr(self.model</span><span class="s3">, </span><span class="s4">'weights'</span><span class="s3">, None</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">row_labels </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">row_labels = getattr(self.model.data</span><span class="s3">, </span><span class="s4">'row_labels'</span><span class="s3">, None</span><span class="s1">)</span>

    <span class="s0"># need to handle other arrays, TODO: is delegating to model possible ?</span>
    <span class="s3">if </span><span class="s1">weights </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">weights = np.asarray(weights)</span>
        <span class="s3">if </span><span class="s1">(weights.size &gt; </span><span class="s5">1 </span><span class="s3">and</span>
                <span class="s1">(weights.ndim != </span><span class="s5">1 </span><span class="s3">or </span><span class="s1">weights.shape[</span><span class="s5">0</span><span class="s1">] == exog.shape[</span><span class="s5">1</span><span class="s1">])):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'weights has wrong shape'</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">pred_kwds </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">pred_kwds = {}</span>
    <span class="s1">predicted_mean = self.model.predict(self.params</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">**pred_kwds)</span>

    <span class="s1">covb = self.cov_params()</span>
    <span class="s1">var_pred_mean = (exog * np.dot(covb</span><span class="s3">, </span><span class="s1">exog.T).T).sum(</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">var_resid = self.scale  </span><span class="s0"># self.mse_resid / weights</span>

    <span class="s0"># TODO: check that we have correct scale, Refactor scale #???</span>
    <span class="s0"># special case for now:</span>
    <span class="s3">if </span><span class="s1">self.cov_type == </span><span class="s4">'fixed scale'</span><span class="s1">:</span>
        <span class="s1">var_resid = self.cov_kwds[</span><span class="s4">'scale'</span><span class="s1">]</span>

    <span class="s3">if </span><span class="s1">weights </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">var_resid /= weights</span>

    <span class="s1">dist = [</span><span class="s4">'norm'</span><span class="s3">, </span><span class="s4">'t'</span><span class="s1">][self.use_t]</span>
    <span class="s3">return </span><span class="s1">PredictionResults(predicted_mean</span><span class="s3">, </span><span class="s1">var_pred_mean</span><span class="s3">, </span><span class="s1">var_resid</span><span class="s3">,</span>
                             <span class="s1">df=self.df_resid</span><span class="s3">, </span><span class="s1">dist=dist</span><span class="s3">,</span>
                             <span class="s1">row_labels=row_labels)</span>
</pre>
</body>
</html>