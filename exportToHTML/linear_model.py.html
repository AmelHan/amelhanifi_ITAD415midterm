<html>
<head>
<title>linear_model.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
linear_model.py</font>
</center></td></tr></table>
<pre><span class="s0"># TODO: Determine which tests are valid for GLSAR, and under what conditions</span>
<span class="s0"># TODO: Fix issue with constant and GLS</span>
<span class="s0"># TODO: GLS: add options Iterative GLS, for iterative fgls if sigma is None</span>
<span class="s0"># TODO: GLS: default if sigma is none should be two-step GLS</span>
<span class="s0"># TODO: Check nesting when performing model based tests, lr, wald, lm</span>
<span class="s2">&quot;&quot;&quot; 
This module implements standard regression models: 
 
Generalized Least Squares (GLS) 
Ordinary Least Squares (OLS) 
Weighted Least Squares (WLS) 
Generalized Least Squares with autoregressive error terms GLSAR(p) 
 
Models are specified with an endogenous response variable and an 
exogenous design matrix and are fit using their `fit` method. 
 
Subclasses that have more complicated covariance matrices 
should write over the 'whiten' method as the fit method 
prewhitens the response by calling 'whiten'. 
 
General reference for regression models: 
 
D. C. Montgomery and E.A. Peck. &quot;Introduction to Linear Regression 
    Analysis.&quot; 2nd. Ed., Wiley, 1992. 
 
Econometrics references for regression models: 
 
R. Davidson and J.G. MacKinnon.  &quot;Econometric Theory and Methods,&quot; Oxford, 
    2004. 
 
W. Green.  &quot;Econometric Analysis,&quot; 5th ed., Pearson, 2003. 
&quot;&quot;&quot;</span>
<span class="s3">from </span><span class="s1">__future__ </span><span class="s3">import </span><span class="s1">annotations</span>

<span class="s3">from </span><span class="s1">statsmodels.compat.pandas </span><span class="s3">import </span><span class="s1">Appender</span>
<span class="s3">from </span><span class="s1">statsmodels.compat.python </span><span class="s3">import </span><span class="s1">lrange</span><span class="s3">, </span><span class="s1">lzip</span>

<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Literal</span><span class="s3">, </span><span class="s1">Sequence</span>
<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">optimize</span><span class="s3">, </span><span class="s1">stats</span>
<span class="s3">from </span><span class="s1">scipy.linalg </span><span class="s3">import </span><span class="s1">cholesky</span><span class="s3">, </span><span class="s1">toeplitz</span>
<span class="s3">from </span><span class="s1">scipy.linalg.lapack </span><span class="s3">import </span><span class="s1">dtrtri</span>

<span class="s3">import </span><span class="s1">statsmodels.base.model </span><span class="s3">as </span><span class="s1">base</span>
<span class="s3">import </span><span class="s1">statsmodels.base.wrapper </span><span class="s3">as </span><span class="s1">wrap</span>
<span class="s3">from </span><span class="s1">statsmodels.emplike.elregress </span><span class="s3">import </span><span class="s1">_ELRegOpts</span>
<span class="s0"># need import in module instead of lazily to copy `__doc__`</span>
<span class="s3">from </span><span class="s1">statsmodels.regression._prediction </span><span class="s3">import </span><span class="s1">PredictionResults</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.decorators </span><span class="s3">import </span><span class="s1">cache_readonly</span><span class="s3">, </span><span class="s1">cache_writable</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">InvalidTestWarning</span><span class="s3">,</span>
    <span class="s1">ValueWarning</span><span class="s3">,</span>
    <span class="s1">)</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.tools </span><span class="s3">import </span><span class="s1">pinv_extended</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.typing </span><span class="s3">import </span><span class="s1">Float64Array</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.validation </span><span class="s3">import </span><span class="s1">bool_like</span><span class="s3">, </span><span class="s1">float_like</span><span class="s3">, </span><span class="s1">string_like</span>

<span class="s3">from </span><span class="s1">. </span><span class="s3">import </span><span class="s1">_prediction </span><span class="s3">as </span><span class="s1">pred</span>

<span class="s1">__docformat__ = </span><span class="s4">'restructuredtext en'</span>

<span class="s1">__all__ = [</span><span class="s4">'GLS'</span><span class="s3">, </span><span class="s4">'WLS'</span><span class="s3">, </span><span class="s4">'OLS'</span><span class="s3">, </span><span class="s4">'GLSAR'</span><span class="s3">, </span><span class="s4">'PredictionResults'</span><span class="s3">,</span>
           <span class="s4">'RegressionResultsWrapper'</span><span class="s1">]</span>


<span class="s1">_fit_regularized_doc =\</span>
        <span class="s4">r&quot;&quot;&quot; 
        Return a regularized fit to a linear regression model. 
 
        Parameters 
        ---------- 
        method : str 
            Either 'elastic_net' or 'sqrt_lasso'. 
        alpha : scalar or array_like 
            The penalty weight.  If a scalar, the same penalty weight 
            applies to all variables in the model.  If a vector, it 
            must have the same length as `params`, and contains a 
            penalty weight for each coefficient. 
        L1_wt : scalar 
            The fraction of the penalty given to the L1 penalty term. 
            Must be between 0 and 1 (inclusive).  If 0, the fit is a 
            ridge fit, if 1 it is a lasso fit. 
        start_params : array_like 
            Starting values for ``params``. 
        profile_scale : bool 
            If True the penalized fit is computed using the profile 
            (concentrated) log-likelihood for the Gaussian model. 
            Otherwise the fit uses the residual sum of squares. 
        refit : bool 
            If True, the model is refit using only the variables that 
            have non-zero coefficients in the regularized fit.  The 
            refitted model is not regularized. 
        **kwargs 
            Additional keyword arguments that contain information used when 
            constructing a model using the formula interface. 
 
        Returns 
        ------- 
        statsmodels.base.elastic_net.RegularizedResults 
            The regularized results. 
 
        Notes 
        ----- 
        The elastic net uses a combination of L1 and L2 penalties. 
        The implementation closely follows the glmnet package in R. 
 
        The function that is minimized is: 
 
        .. math:: 
 
            0.5*RSS/n + alpha*((1-L1\_wt)*|params|_2^2/2 + L1\_wt*|params|_1) 
 
        where RSS is the usual regression sum of squares, n is the 
        sample size, and :math:`|*|_1` and :math:`|*|_2` are the L1 and L2 
        norms. 
 
        For WLS and GLS, the RSS is calculated using the whitened endog and 
        exog data. 
 
        Post-estimation results are based on the same data used to 
        select variables, hence may be subject to overfitting biases. 
 
        The elastic_net method uses the following keyword arguments: 
 
        maxiter : int 
            Maximum number of iterations 
        cnvrg_tol : float 
            Convergence threshold for line searches 
        zero_tol : float 
            Coefficients below this threshold are treated as zero. 
 
        The square root lasso approach is a variation of the Lasso 
        that is largely self-tuning (the optimal tuning parameter 
        does not depend on the standard deviation of the regression 
        errors).  If the errors are Gaussian, the tuning parameter 
        can be taken to be 
 
        alpha = 1.1 * np.sqrt(n) * norm.ppf(1 - 0.05 / (2 * p)) 
 
        where n is the sample size and p is the number of predictors. 
 
        The square root lasso uses the following keyword arguments: 
 
        zero_tol : float 
            Coefficients below this threshold are treated as zero. 
 
        The cvxopt module is required to estimate model using the square root 
        lasso. 
 
        References 
        ---------- 
        .. [*] Friedman, Hastie, Tibshirani (2008).  Regularization paths for 
           generalized linear models via coordinate descent.  Journal of 
           Statistical Software 33(1), 1-22 Feb 2010. 
 
        .. [*] A Belloni, V Chernozhukov, L Wang (2011).  Square-root Lasso: 
           pivotal recovery of sparse signals via conic programming. 
           Biometrika 98(4), 791-806. https://arxiv.org/pdf/1009.5689.pdf 
        &quot;&quot;&quot;</span>


<span class="s3">def </span><span class="s1">_get_sigma(sigma</span><span class="s3">, </span><span class="s1">nobs):</span>
    <span class="s2">&quot;&quot;&quot; 
    Returns sigma (matrix, nobs by nobs) for GLS and the inverse of its 
    Cholesky decomposition.  Handles dimensions and checks integrity. 
    If sigma is None, returns None, None. Otherwise returns sigma, 
    cholsigmainv. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">sigma </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s3">return None, None</span>
    <span class="s1">sigma = np.asarray(sigma).squeeze()</span>
    <span class="s3">if </span><span class="s1">sigma.ndim == </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s1">sigma = np.repeat(sigma</span><span class="s3">, </span><span class="s1">nobs)</span>
    <span class="s3">if </span><span class="s1">sigma.ndim == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">sigma.shape != (nobs</span><span class="s3">,</span><span class="s1">):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Sigma must be a scalar, 1d of length %s or a 2d &quot;</span>
                             <span class="s4">&quot;array of shape %s x %s&quot; </span><span class="s1">% (nobs</span><span class="s3">, </span><span class="s1">nobs</span><span class="s3">, </span><span class="s1">nobs))</span>
        <span class="s1">cholsigmainv = </span><span class="s5">1</span><span class="s1">/np.sqrt(sigma)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">sigma.shape != (nobs</span><span class="s3">, </span><span class="s1">nobs):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Sigma must be a scalar, 1d of length %s or a 2d &quot;</span>
                             <span class="s4">&quot;array of shape %s x %s&quot; </span><span class="s1">% (nobs</span><span class="s3">, </span><span class="s1">nobs</span><span class="s3">, </span><span class="s1">nobs))</span>
        <span class="s1">cholsigmainv</span><span class="s3">, </span><span class="s1">info = dtrtri(cholesky(sigma</span><span class="s3">, </span><span class="s1">lower=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
                                    <span class="s1">lower=</span><span class="s3">True, </span><span class="s1">overwrite_c=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">info &gt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">np.linalg.LinAlgError(</span><span class="s4">'Cholesky decomposition of sigma '</span>
                                        <span class="s4">'yields a singular matrix'</span><span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">info &lt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'Invalid input to dtrtri (info = %d)' </span><span class="s1">% info)</span>
    <span class="s3">return </span><span class="s1">sigma</span><span class="s3">, </span><span class="s1">cholsigmainv</span>


<span class="s3">class </span><span class="s1">RegressionModel(base.LikelihoodModel):</span>
    <span class="s2">&quot;&quot;&quot; 
    Base class for linear regression models. Should not be directly called. 
 
    Intended for subclassing. 
    &quot;&quot;&quot;</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s1">super(RegressionModel</span><span class="s3">, </span><span class="s1">self).__init__(endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s1">self.pinv_wexog: Float64Array | </span><span class="s3">None </span><span class="s1">= </span><span class="s3">None</span>
        <span class="s1">self._data_attr.extend([</span><span class="s4">'pinv_wexog'</span><span class="s3">, </span><span class="s4">'wendog'</span><span class="s3">, </span><span class="s4">'wexog'</span><span class="s3">, </span><span class="s4">'weights'</span><span class="s1">])</span>

    <span class="s3">def </span><span class="s1">initialize(self):</span>
        <span class="s2">&quot;&quot;&quot;Initialize model components.&quot;&quot;&quot;</span>
        <span class="s1">self.wexog = self.whiten(self.exog)</span>
        <span class="s1">self.wendog = self.whiten(self.endog)</span>
        <span class="s0"># overwrite nobs from class Model:</span>
        <span class="s1">self.nobs = float(self.wexog.shape[</span><span class="s5">0</span><span class="s1">])</span>

        <span class="s1">self._df_model = </span><span class="s3">None</span>
        <span class="s1">self._df_resid = </span><span class="s3">None</span>
        <span class="s1">self.rank = </span><span class="s3">None</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">df_model(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        The model degree of freedom. 
 
        The dof is defined as the rank of the regressor matrix minus 1 if a 
        constant is included. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._df_model </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self.rank </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">self.rank = np.linalg.matrix_rank(self.exog)</span>
            <span class="s1">self._df_model = float(self.rank - self.k_constant)</span>
        <span class="s3">return </span><span class="s1">self._df_model</span>

    <span class="s1">@df_model.setter</span>
    <span class="s3">def </span><span class="s1">df_model(self</span><span class="s3">, </span><span class="s1">value):</span>
        <span class="s1">self._df_model = value</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">df_resid(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        The residual degree of freedom. 
 
        The dof is defined as the number of observations minus the rank of 
        the regressor matrix. 
        &quot;&quot;&quot;</span>

        <span class="s3">if </span><span class="s1">self._df_resid </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self.rank </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">self.rank = np.linalg.matrix_rank(self.exog)</span>
            <span class="s1">self._df_resid = self.nobs - self.rank</span>
        <span class="s3">return </span><span class="s1">self._df_resid</span>

    <span class="s1">@df_resid.setter</span>
    <span class="s3">def </span><span class="s1">df_resid(self</span><span class="s3">, </span><span class="s1">value):</span>
        <span class="s1">self._df_resid = value</span>

    <span class="s3">def </span><span class="s1">whiten(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">&quot;&quot;&quot; 
        Whiten method that must be overwritten by individual models. 
 
        Parameters 
        ---------- 
        x : array_like 
            Data to be whitened. 
        &quot;&quot;&quot;</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s4">&quot;Subclasses must implement.&quot;</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">fit(</span>
            <span class="s1">self</span><span class="s3">,</span>
            <span class="s1">method: Literal[</span><span class="s4">&quot;pinv&quot;</span><span class="s3">, </span><span class="s4">&quot;qr&quot;</span><span class="s1">] = </span><span class="s4">&quot;pinv&quot;</span><span class="s3">,</span>
            <span class="s1">cov_type: Literal[</span>
                <span class="s4">&quot;nonrobust&quot;</span><span class="s3">,</span>
                <span class="s4">&quot;fixed scale&quot;</span><span class="s3">,</span>
                <span class="s4">&quot;HC0&quot;</span><span class="s3">,</span>
                <span class="s4">&quot;HC1&quot;</span><span class="s3">,</span>
                <span class="s4">&quot;HC2&quot;</span><span class="s3">,</span>
                <span class="s4">&quot;HC3&quot;</span><span class="s3">,</span>
                <span class="s4">&quot;HAC&quot;</span><span class="s3">,</span>
                <span class="s4">&quot;hac-panel&quot;</span><span class="s3">,</span>
                <span class="s4">&quot;hac-groupsum&quot;</span><span class="s3">,</span>
                <span class="s4">&quot;cluster&quot;</span><span class="s3">,</span>
            <span class="s1">] = </span><span class="s4">&quot;nonrobust&quot;</span><span class="s3">,</span>
            <span class="s1">cov_kwds=</span><span class="s3">None,</span>
            <span class="s1">use_t: bool | </span><span class="s3">None </span><span class="s1">= </span><span class="s3">None,</span>
            <span class="s1">**kwargs</span>
    <span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Full fit of the model. 
 
        The results include an estimate of covariance matrix, (whitened) 
        residuals and an estimate of scale. 
 
        Parameters 
        ---------- 
        method : str, optional 
            Can be &quot;pinv&quot;, &quot;qr&quot;.  &quot;pinv&quot; uses the Moore-Penrose pseudoinverse 
            to solve the least squares problem. &quot;qr&quot; uses the QR 
            factorization. 
        cov_type : str, optional 
            See `regression.linear_model.RegressionResults` for a description 
            of the available covariance estimators. 
        cov_kwds : list or None, optional 
            See `linear_model.RegressionResults.get_robustcov_results` for a 
            description required keywords for alternative covariance 
            estimators. 
        use_t : bool, optional 
            Flag indicating to use the Student's t distribution when computing 
            p-values.  Default behavior depends on cov_type. See 
            `linear_model.RegressionResults.get_robustcov_results` for 
            implementation details. 
        **kwargs 
            Additional keyword arguments that contain information used when 
            constructing a model using the formula interface. 
 
        Returns 
        ------- 
        RegressionResults 
            The model estimation results. 
 
        See Also 
        -------- 
        RegressionResults 
            The results container. 
        RegressionResults.get_robustcov_results 
            A method to change the covariance estimator used when fitting the 
            model. 
 
        Notes 
        ----- 
        The fit method uses the pseudoinverse of the design/exogenous variables 
        to solve the least squares minimization. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">method == </span><span class="s4">&quot;pinv&quot;</span><span class="s1">:</span>
            <span class="s3">if not </span><span class="s1">(hasattr(self</span><span class="s3">, </span><span class="s4">'pinv_wexog'</span><span class="s1">) </span><span class="s3">and</span>
                    <span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'normalized_cov_params'</span><span class="s1">) </span><span class="s3">and</span>
                    <span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'rank'</span><span class="s1">)):</span>

                <span class="s1">self.pinv_wexog</span><span class="s3">, </span><span class="s1">singular_values = pinv_extended(self.wexog)</span>
                <span class="s1">self.normalized_cov_params = np.dot(</span>
                    <span class="s1">self.pinv_wexog</span><span class="s3">, </span><span class="s1">np.transpose(self.pinv_wexog))</span>

                <span class="s0"># Cache these singular values for use later.</span>
                <span class="s1">self.wexog_singular_values = singular_values</span>
                <span class="s1">self.rank = np.linalg.matrix_rank(np.diag(singular_values))</span>

            <span class="s1">beta = np.dot(self.pinv_wexog</span><span class="s3">, </span><span class="s1">self.wendog)</span>

        <span class="s3">elif </span><span class="s1">method == </span><span class="s4">&quot;qr&quot;</span><span class="s1">:</span>
            <span class="s3">if not </span><span class="s1">(hasattr(self</span><span class="s3">, </span><span class="s4">'exog_Q'</span><span class="s1">) </span><span class="s3">and</span>
                    <span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'exog_R'</span><span class="s1">) </span><span class="s3">and</span>
                    <span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'normalized_cov_params'</span><span class="s1">) </span><span class="s3">and</span>
                    <span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'rank'</span><span class="s1">)):</span>
                <span class="s1">Q</span><span class="s3">, </span><span class="s1">R = np.linalg.qr(self.wexog)</span>
                <span class="s1">self.exog_Q</span><span class="s3">, </span><span class="s1">self.exog_R = Q</span><span class="s3">, </span><span class="s1">R</span>
                <span class="s1">self.normalized_cov_params = np.linalg.inv(np.dot(R.T</span><span class="s3">, </span><span class="s1">R))</span>

                <span class="s0"># Cache singular values from R.</span>
                <span class="s1">self.wexog_singular_values = np.linalg.svd(R</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
                <span class="s1">self.rank = np.linalg.matrix_rank(R)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">Q</span><span class="s3">, </span><span class="s1">R = self.exog_Q</span><span class="s3">, </span><span class="s1">self.exog_R</span>
            <span class="s0"># Needed for some covariance estimators, see GH #8157</span>
            <span class="s1">self.pinv_wexog = np.linalg.pinv(self.wexog)</span>
            <span class="s0"># used in ANOVA</span>
            <span class="s1">self.effects = effects = np.dot(Q.T</span><span class="s3">, </span><span class="s1">self.wendog)</span>
            <span class="s1">beta = np.linalg.solve(R</span><span class="s3">, </span><span class="s1">effects)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'method has to be &quot;pinv&quot; or &quot;qr&quot;'</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self._df_model </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self._df_model = float(self.rank - self.k_constant)</span>
        <span class="s3">if </span><span class="s1">self._df_resid </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self.df_resid = self.nobs - self.rank</span>

        <span class="s3">if </span><span class="s1">isinstance(self</span><span class="s3">, </span><span class="s1">OLS):</span>
            <span class="s1">lfit = OLSResults(</span>
                <span class="s1">self</span><span class="s3">, </span><span class="s1">beta</span><span class="s3">,</span>
                <span class="s1">normalized_cov_params=self.normalized_cov_params</span><span class="s3">,</span>
                <span class="s1">cov_type=cov_type</span><span class="s3">, </span><span class="s1">cov_kwds=cov_kwds</span><span class="s3">, </span><span class="s1">use_t=use_t)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">lfit = RegressionResults(</span>
                <span class="s1">self</span><span class="s3">, </span><span class="s1">beta</span><span class="s3">,</span>
                <span class="s1">normalized_cov_params=self.normalized_cov_params</span><span class="s3">,</span>
                <span class="s1">cov_type=cov_type</span><span class="s3">, </span><span class="s1">cov_kwds=cov_kwds</span><span class="s3">, </span><span class="s1">use_t=use_t</span><span class="s3">,</span>
                <span class="s1">**kwargs)</span>
        <span class="s3">return </span><span class="s1">RegressionResultsWrapper(lfit)</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Return linear predicted values from a design matrix. 
 
        Parameters 
        ---------- 
        params : array_like 
            Parameters of a linear model. 
        exog : array_like, optional 
            Design / exogenous data. Model exog is used if None. 
 
        Returns 
        ------- 
        array_like 
            An array of fitted values. 
 
        Notes 
        ----- 
        If the model has not yet been fit, params is not optional. 
        &quot;&quot;&quot;</span>
        <span class="s0"># JP: this does not look correct for GLMAR</span>
        <span class="s0"># SS: it needs its own predict method</span>

        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>

        <span class="s3">return </span><span class="s1">np.dot(exog</span><span class="s3">, </span><span class="s1">params)</span>

    <span class="s3">def </span><span class="s1">get_distribution(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">scale</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">dist_class=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Construct a random number generator for the predictive distribution. 
 
        Parameters 
        ---------- 
        params : array_like 
            The model parameters (regression coefficients). 
        scale : scalar 
            The variance parameter. 
        exog : array_like 
            The predictor variable matrix. 
        dist_class : class 
            A random number generator class.  Must take 'loc' and 'scale' 
            as arguments and return a random number generator implementing 
            an ``rvs`` method for simulating random values. Defaults to normal. 
 
        Returns 
        ------- 
        gen 
            Frozen random number generator object with mean and variance 
            determined by the fitted linear model.  Use the ``rvs`` method 
            to generate random values. 
 
        Notes 
        ----- 
        Due to the behavior of ``scipy.stats.distributions objects``, 
        the returned random number generator must be called with 
        ``gen.rvs(n)`` where ``n`` is the number of observations in 
        the data set used to fit the model.  If any other value is 
        used for ``n``, misleading results will be produced. 
        &quot;&quot;&quot;</span>
        <span class="s1">fit = self.predict(params</span><span class="s3">, </span><span class="s1">exog)</span>
        <span class="s3">if </span><span class="s1">dist_class </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">from </span><span class="s1">scipy.stats.distributions </span><span class="s3">import </span><span class="s1">norm</span>
            <span class="s1">dist_class = norm</span>
        <span class="s1">gen = dist_class(loc=fit</span><span class="s3">, </span><span class="s1">scale=np.sqrt(scale))</span>
        <span class="s3">return </span><span class="s1">gen</span>


<span class="s3">class </span><span class="s1">GLS(RegressionModel):</span>
    <span class="s1">__doc__ = </span><span class="s4">r&quot;&quot;&quot; 
    Generalized Least Squares 
 
    %(params)s 
    sigma : scalar or array 
        The array or scalar `sigma` is the weighting matrix of the covariance. 
        The default is None for no scaling.  If `sigma` is a scalar, it is 
        assumed that `sigma` is an n x n diagonal matrix with the given 
        scalar, `sigma` as the value of each diagonal element.  If `sigma` 
        is an n-length vector, then `sigma` is assumed to be a diagonal 
        matrix with the given `sigma` on the diagonal.  This should be the 
        same as WLS. 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    pinv_wexog : ndarray 
        `pinv_wexog` is the p x n Moore-Penrose pseudoinverse of `wexog`. 
    cholsimgainv : ndarray 
        The transpose of the Cholesky decomposition of the pseudoinverse. 
    df_model : float 
        p - 1, where p is the number of regressors including the intercept. 
        of freedom. 
    df_resid : float 
        Number of observations n less the number of parameters p. 
    llf : float 
        The value of the likelihood function of the fitted model. 
    nobs : float 
        The number of observations n. 
    normalized_cov_params : ndarray 
        p x p array :math:`(X^{T}\Sigma^{-1}X)^{-1}` 
    results : RegressionResults instance 
        A property that returns the RegressionResults class if fit. 
    sigma : ndarray 
        `sigma` is the n x n covariance structure of the error terms. 
    wexog : ndarray 
        Design matrix whitened by `cholsigmainv` 
    wendog : ndarray 
        Response variable whitened by `cholsigmainv` 
 
    See Also 
    -------- 
    WLS : Fit a linear model using Weighted Least Squares. 
    OLS : Fit a linear model using Ordinary Least Squares. 
 
    Notes 
    ----- 
    If sigma is a function of the data making one of the regressors 
    a constant, then the current postestimation statistics will not be correct. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import statsmodels.api as sm 
    &gt;&gt;&gt; data = sm.datasets.longley.load() 
    &gt;&gt;&gt; data.exog = sm.add_constant(data.exog) 
    &gt;&gt;&gt; ols_resid = sm.OLS(data.endog, data.exog).fit().resid 
    &gt;&gt;&gt; res_fit = sm.OLS(ols_resid[1:], ols_resid[:-1]).fit() 
    &gt;&gt;&gt; rho = res_fit.params 
 
    `rho` is a consistent estimator of the correlation of the residuals from 
    an OLS fit of the longley data.  It is assumed that this is the true rho 
    of the AR process data. 
 
    &gt;&gt;&gt; from scipy.linalg import toeplitz 
    &gt;&gt;&gt; order = toeplitz(np.arange(16)) 
    &gt;&gt;&gt; sigma = rho**order 
 
    `sigma` is an n x n matrix of the autocorrelation structure of the 
    data. 
 
    &gt;&gt;&gt; gls_model = sm.GLS(data.endog, data.exog, sigma=sigma) 
    &gt;&gt;&gt; gls_results = gls_model.fit() 
    &gt;&gt;&gt; print(gls_results.summary()) 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s4">'params'</span><span class="s1">: base._model_params_doc</span><span class="s3">,</span>
           <span class="s4">'extra_params'</span><span class="s1">: base._missing_param_doc + base._extra_param_doc}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">sigma=</span><span class="s3">None, </span><span class="s1">missing=</span><span class="s4">'none'</span><span class="s3">, </span><span class="s1">hasconst=</span><span class="s3">None,</span>
                 <span class="s1">**kwargs):</span>
        <span class="s3">if </span><span class="s1">type(self) </span><span class="s3">is </span><span class="s1">GLS:</span>
            <span class="s1">self._check_kwargs(kwargs)</span>
        <span class="s0"># TODO: add options igls, for iterative fgls if sigma is None</span>
        <span class="s0"># TODO: default if sigma is none should be two-step GLS</span>
        <span class="s1">sigma</span><span class="s3">, </span><span class="s1">cholsigmainv = _get_sigma(sigma</span><span class="s3">, </span><span class="s1">len(endog))</span>

        <span class="s1">super(GLS</span><span class="s3">, </span><span class="s1">self).__init__(endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">missing=missing</span><span class="s3">,</span>
                                  <span class="s1">hasconst=hasconst</span><span class="s3">, </span><span class="s1">sigma=sigma</span><span class="s3">,</span>
                                  <span class="s1">cholsigmainv=cholsigmainv</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s0"># store attribute names for data arrays</span>
        <span class="s1">self._data_attr.extend([</span><span class="s4">'sigma'</span><span class="s3">, </span><span class="s4">'cholsigmainv'</span><span class="s1">])</span>

    <span class="s3">def </span><span class="s1">whiten(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">&quot;&quot;&quot; 
        GLS whiten method. 
 
        Parameters 
        ---------- 
        x : array_like 
            Data to be whitened. 
 
        Returns 
        ------- 
        ndarray 
            The value np.dot(cholsigmainv,X). 
 
        See Also 
        -------- 
        GLS : Fit a linear model using Generalized Least Squares. 
        &quot;&quot;&quot;</span>
        <span class="s1">x = np.asarray(x)</span>
        <span class="s3">if </span><span class="s1">self.sigma </span><span class="s3">is None or </span><span class="s1">self.sigma.shape == ():</span>
            <span class="s3">return </span><span class="s1">x</span>
        <span class="s3">elif </span><span class="s1">self.sigma.ndim == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">x.ndim == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s3">return </span><span class="s1">x * self.cholsigmainv</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s3">return </span><span class="s1">x * self.cholsigmainv[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">np.dot(self.cholsigmainv</span><span class="s3">, </span><span class="s1">x)</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">r&quot;&quot;&quot; 
        Compute the value of the Gaussian log-likelihood function at params. 
 
        Given the whitened design matrix, the log-likelihood is evaluated 
        at the parameter vector `params` for the dependent variable `endog`. 
 
        Parameters 
        ---------- 
        params : array_like 
            The model parameters. 
 
        Returns 
        ------- 
        float 
            The value of the log-likelihood function for a GLS Model. 
 
        Notes 
        ----- 
        The log-likelihood function for the normal distribution is 
 
        .. math:: -\frac{n}{2}\log\left(\left(Y-\hat{Y}\right)^{\prime} 
                   \left(Y-\hat{Y}\right)\right) 
                  -\frac{n}{2}\left(1+\log\left(\frac{2\pi}{n}\right)\right) 
                  -\frac{1}{2}\log\left(\left|\Sigma\right|\right) 
 
        Y and Y-hat are whitened. 
        &quot;&quot;&quot;</span>
        <span class="s0"># TODO: combine this with OLS/WLS loglike and add _det_sigma argument</span>
        <span class="s1">nobs2 = self.nobs / </span><span class="s5">2.0</span>
        <span class="s1">SSR = np.sum((self.wendog - np.dot(self.wexog</span><span class="s3">, </span><span class="s1">params))**</span><span class="s5">2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">llf = -np.log(SSR) * nobs2      </span><span class="s0"># concentrated likelihood</span>
        <span class="s1">llf -= (</span><span class="s5">1</span><span class="s1">+np.log(np.pi/nobs2))*nobs2  </span><span class="s0"># with likelihood constant</span>
        <span class="s3">if </span><span class="s1">np.any(self.sigma):</span>
            <span class="s0"># FIXME: robust-enough check? unneeded if _det_sigma gets defined</span>
            <span class="s3">if </span><span class="s1">self.sigma.ndim == </span><span class="s5">2</span><span class="s1">:</span>
                <span class="s1">det = np.linalg.slogdet(self.sigma)</span>
                <span class="s1">llf -= </span><span class="s5">.5</span><span class="s1">*det[</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">llf -= </span><span class="s5">0.5</span><span class="s1">*np.sum(np.log(self.sigma))</span>
            <span class="s0"># with error covariance matrix</span>
        <span class="s3">return </span><span class="s1">llf</span>

    <span class="s3">def </span><span class="s1">hessian_factor(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">scale=</span><span class="s3">None, </span><span class="s1">observed=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Compute weights for calculating Hessian. 
 
        Parameters 
        ---------- 
        params : ndarray 
            The parameter at which Hessian is evaluated. 
        scale : None or float 
            If scale is None, then the default scale will be calculated. 
            Default scale is defined by `self.scaletype` and set in fit. 
            If scale is not None, then it is used as a fixed scale. 
        observed : bool 
            If True, then the observed Hessian is returned. If false then the 
            expected information matrix is returned. 
 
        Returns 
        ------- 
        ndarray 
            A 1d weight vector used in the calculation of the Hessian. 
            The hessian is obtained by `(exog.T * hessian_factor).dot(exog)`. 
        &quot;&quot;&quot;</span>

        <span class="s3">if </span><span class="s1">self.sigma </span><span class="s3">is None or </span><span class="s1">self.sigma.shape == ():</span>
            <span class="s3">return </span><span class="s1">np.ones(self.exog.shape[</span><span class="s5">0</span><span class="s1">])</span>
        <span class="s3">elif </span><span class="s1">self.sigma.ndim == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">self.cholsigmainv</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">np.diag(self.cholsigmainv)</span>

    <span class="s1">@Appender(_fit_regularized_doc)</span>
    <span class="s3">def </span><span class="s1">fit_regularized(self</span><span class="s3">, </span><span class="s1">method=</span><span class="s4">&quot;elastic_net&quot;</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s5">0.</span><span class="s3">,</span>
                        <span class="s1">L1_wt=</span><span class="s5">1.</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">profile_scale=</span><span class="s3">False,</span>
                        <span class="s1">refit=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s3">if not </span><span class="s1">np.isscalar(alpha):</span>
            <span class="s1">alpha = np.asarray(alpha)</span>
        <span class="s0"># Need to adjust since RSS/n term in elastic net uses nominal</span>
        <span class="s0"># n in denominator</span>
        <span class="s3">if </span><span class="s1">self.sigma </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self.sigma.ndim == </span><span class="s5">2</span><span class="s1">:</span>
                <span class="s1">var_obs = np.diag(self.sigma)</span>
            <span class="s3">elif </span><span class="s1">self.sigma.ndim == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">var_obs = self.sigma</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;sigma should be 1-dim or 2-dim&quot;</span><span class="s1">)</span>

            <span class="s1">alpha = alpha * np.sum(</span><span class="s5">1 </span><span class="s1">/ var_obs) / len(self.endog)</span>

        <span class="s1">rslt = OLS(self.wendog</span><span class="s3">, </span><span class="s1">self.wexog).fit_regularized(</span>
            <span class="s1">method=method</span><span class="s3">, </span><span class="s1">alpha=alpha</span><span class="s3">,</span>
            <span class="s1">L1_wt=L1_wt</span><span class="s3">,</span>
            <span class="s1">start_params=start_params</span><span class="s3">,</span>
            <span class="s1">profile_scale=profile_scale</span><span class="s3">,</span>
            <span class="s1">refit=refit</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s3">from </span><span class="s1">statsmodels.base.elastic_net </span><span class="s3">import </span><span class="s1">(</span>
            <span class="s1">RegularizedResults</span><span class="s3">,</span>
            <span class="s1">RegularizedResultsWrapper</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">rrslt = RegularizedResults(self</span><span class="s3">, </span><span class="s1">rslt.params)</span>
        <span class="s3">return </span><span class="s1">RegularizedResultsWrapper(rrslt)</span>


<span class="s3">class </span><span class="s1">WLS(RegressionModel):</span>
    <span class="s1">__doc__ = </span><span class="s4">&quot;&quot;&quot; 
    Weighted Least Squares 
 
    The weights are presumed to be (proportional to) the inverse of 
    the variance of the observations.  That is, if the variables are 
    to be transformed by 1/sqrt(W) you must supply weights = 1/W. 
 
    %(params)s 
    weights : array_like, optional 
        A 1d array of weights.  If you supply 1/W then the variables are 
        pre- multiplied by 1/sqrt(W).  If no weights are supplied the 
        default value is 1 and WLS results are the same as OLS. 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    weights : ndarray 
        The stored weights supplied as an argument. 
 
    See Also 
    -------- 
    GLS : Fit a linear model using Generalized Least Squares. 
    OLS : Fit a linear model using Ordinary Least Squares. 
 
    Notes 
    ----- 
    If the weights are a function of the data, then the post estimation 
    statistics such as fvalue and mse_model might not be correct, as the 
    package does not yet support no-constant regression. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import statsmodels.api as sm 
    &gt;&gt;&gt; Y = [1,3,4,5,2,3,4] 
    &gt;&gt;&gt; X = range(1,8) 
    &gt;&gt;&gt; X = sm.add_constant(X) 
    &gt;&gt;&gt; wls_model = sm.WLS(Y,X, weights=list(range(1,8))) 
    &gt;&gt;&gt; results = wls_model.fit() 
    &gt;&gt;&gt; results.params 
    array([ 2.91666667,  0.0952381 ]) 
    &gt;&gt;&gt; results.tvalues 
    array([ 2.0652652 ,  0.35684428]) 
    &gt;&gt;&gt; print(results.t_test([1, 0])) 
    &lt;T test: effect=array([ 2.91666667]), sd=array([[ 1.41224801]]), 
     t=array([[ 2.0652652]]), p=array([[ 0.04690139]]), df_denom=5&gt; 
    &gt;&gt;&gt; print(results.f_test([0, 1])) 
    &lt;F test: F=array([[ 0.12733784]]), p=[[ 0.73577409]], df_denom=5, df_num=1&gt; 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s4">'params'</span><span class="s1">: base._model_params_doc</span><span class="s3">,</span>
           <span class="s4">'extra_params'</span><span class="s1">: base._missing_param_doc + base._extra_param_doc}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">weights=</span><span class="s5">1.</span><span class="s3">, </span><span class="s1">missing=</span><span class="s4">'none'</span><span class="s3">, </span><span class="s1">hasconst=</span><span class="s3">None,</span>
                 <span class="s1">**kwargs):</span>
        <span class="s3">if </span><span class="s1">type(self) </span><span class="s3">is </span><span class="s1">WLS:</span>
            <span class="s1">self._check_kwargs(kwargs)</span>
        <span class="s1">weights = np.array(weights)</span>
        <span class="s3">if </span><span class="s1">weights.shape == ():</span>
            <span class="s3">if </span><span class="s1">(missing == </span><span class="s4">'drop' </span><span class="s3">and </span><span class="s4">'missing_idx' </span><span class="s3">in </span><span class="s1">kwargs </span><span class="s3">and</span>
                    <span class="s1">kwargs[</span><span class="s4">'missing_idx'</span><span class="s1">] </span><span class="s3">is not None</span><span class="s1">):</span>
                <span class="s0"># patsy may have truncated endog</span>
                <span class="s1">weights = np.repeat(weights</span><span class="s3">, </span><span class="s1">len(kwargs[</span><span class="s4">'missing_idx'</span><span class="s1">]))</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">weights = np.repeat(weights</span><span class="s3">, </span><span class="s1">len(endog))</span>
        <span class="s0"># handle case that endog might be of len == 1</span>
        <span class="s3">if </span><span class="s1">len(weights) == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">weights = np.array([weights.squeeze()])</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">weights = weights.squeeze()</span>
        <span class="s1">super(WLS</span><span class="s3">, </span><span class="s1">self).__init__(endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">missing=missing</span><span class="s3">,</span>
                                  <span class="s1">weights=weights</span><span class="s3">, </span><span class="s1">hasconst=hasconst</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s1">nobs = self.exog.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">weights = self.weights</span>
        <span class="s3">if </span><span class="s1">weights.size != nobs </span><span class="s3">and </span><span class="s1">weights.shape[</span><span class="s5">0</span><span class="s1">] != nobs:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'Weights must be scalar or same length as design'</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">whiten(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">&quot;&quot;&quot; 
        Whitener for WLS model, multiplies each column by sqrt(self.weights). 
 
        Parameters 
        ---------- 
        x : array_like 
            Data to be whitened. 
 
        Returns 
        ------- 
        array_like 
            The whitened values sqrt(weights)*X. 
        &quot;&quot;&quot;</span>

        <span class="s1">x = np.asarray(x)</span>
        <span class="s3">if </span><span class="s1">x.ndim == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">x * np.sqrt(self.weights)</span>
        <span class="s3">elif </span><span class="s1">x.ndim == </span><span class="s5">2</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">np.sqrt(self.weights)[:</span><span class="s3">, None</span><span class="s1">] * x</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">r&quot;&quot;&quot; 
        Compute the value of the gaussian log-likelihood function at params. 
 
        Given the whitened design matrix, the log-likelihood is evaluated 
        at the parameter vector `params` for the dependent variable `Y`. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameter estimates. 
 
        Returns 
        ------- 
        float 
            The value of the log-likelihood function for a WLS Model. 
 
        Notes 
        ----- 
        .. math:: -\frac{n}{2}\log SSR 
                  -\frac{n}{2}\left(1+\log\left(\frac{2\pi}{n}\right)\right) 
                  -\frac{1}{2}\log\left(\left|W\right|\right) 
 
        where :math:`W` is a diagonal weight matrix matrix and 
        :math:`SSR=\left(Y-\hat{Y}\right)^\prime W \left(Y-\hat{Y}\right)` is 
        the sum of the squared weighted residuals. 
        &quot;&quot;&quot;</span>
        <span class="s1">nobs2 = self.nobs / </span><span class="s5">2.0</span>
        <span class="s1">SSR = np.sum((self.wendog - np.dot(self.wexog</span><span class="s3">, </span><span class="s1">params))**</span><span class="s5">2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">llf = -np.log(SSR) * nobs2      </span><span class="s0"># concentrated likelihood</span>
        <span class="s1">llf -= (</span><span class="s5">1</span><span class="s1">+np.log(np.pi/nobs2))*nobs2  </span><span class="s0"># with constant</span>
        <span class="s1">llf += </span><span class="s5">0.5 </span><span class="s1">* np.sum(np.log(self.weights))</span>
        <span class="s3">return </span><span class="s1">llf</span>

    <span class="s3">def </span><span class="s1">hessian_factor(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">scale=</span><span class="s3">None, </span><span class="s1">observed=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Compute the weights for calculating the Hessian. 
 
        Parameters 
        ---------- 
        params : ndarray 
            The parameter at which Hessian is evaluated. 
        scale : None or float 
            If scale is None, then the default scale will be calculated. 
            Default scale is defined by `self.scaletype` and set in fit. 
            If scale is not None, then it is used as a fixed scale. 
        observed : bool 
            If True, then the observed Hessian is returned. If false then the 
            expected information matrix is returned. 
 
        Returns 
        ------- 
        ndarray 
            A 1d weight vector used in the calculation of the Hessian. 
            The hessian is obtained by `(exog.T * hessian_factor).dot(exog)`. 
        &quot;&quot;&quot;</span>

        <span class="s3">return </span><span class="s1">self.weights</span>

    <span class="s1">@Appender(_fit_regularized_doc)</span>
    <span class="s3">def </span><span class="s1">fit_regularized(self</span><span class="s3">, </span><span class="s1">method=</span><span class="s4">&quot;elastic_net&quot;</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s5">0.</span><span class="s3">,</span>
                        <span class="s1">L1_wt=</span><span class="s5">1.</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">profile_scale=</span><span class="s3">False,</span>
                        <span class="s1">refit=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>
        <span class="s0"># Docstring attached below</span>
        <span class="s3">if not </span><span class="s1">np.isscalar(alpha):</span>
            <span class="s1">alpha = np.asarray(alpha)</span>
        <span class="s0"># Need to adjust since RSS/n in elastic net uses nominal n in</span>
        <span class="s0"># denominator</span>
        <span class="s1">alpha = alpha * np.sum(self.weights) / len(self.weights)</span>

        <span class="s1">rslt = OLS(self.wendog</span><span class="s3">, </span><span class="s1">self.wexog).fit_regularized(</span>
            <span class="s1">method=method</span><span class="s3">, </span><span class="s1">alpha=alpha</span><span class="s3">,</span>
            <span class="s1">L1_wt=L1_wt</span><span class="s3">,</span>
            <span class="s1">start_params=start_params</span><span class="s3">,</span>
            <span class="s1">profile_scale=profile_scale</span><span class="s3">,</span>
            <span class="s1">refit=refit</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s3">from </span><span class="s1">statsmodels.base.elastic_net </span><span class="s3">import </span><span class="s1">(</span>
            <span class="s1">RegularizedResults</span><span class="s3">,</span>
            <span class="s1">RegularizedResultsWrapper</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">rrslt = RegularizedResults(self</span><span class="s3">, </span><span class="s1">rslt.params)</span>
        <span class="s3">return </span><span class="s1">RegularizedResultsWrapper(rrslt)</span>


<span class="s3">class </span><span class="s1">OLS(WLS):</span>
    <span class="s1">__doc__ = </span><span class="s4">&quot;&quot;&quot; 
    Ordinary Least Squares 
 
    %(params)s 
    %(extra_params)s 
 
    Attributes 
    ---------- 
    weights : scalar 
        Has an attribute weights = array(1.0) due to inheritance from WLS. 
 
    See Also 
    -------- 
    WLS : Fit a linear model using Weighted Least Squares. 
    GLS : Fit a linear model using Generalized Least Squares. 
 
    Notes 
    ----- 
    No constant is added by the model unless you are using formulas. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import statsmodels.api as sm 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; duncan_prestige = sm.datasets.get_rdataset(&quot;Duncan&quot;, &quot;carData&quot;) 
    &gt;&gt;&gt; Y = duncan_prestige.data['income'] 
    &gt;&gt;&gt; X = duncan_prestige.data['education'] 
    &gt;&gt;&gt; X = sm.add_constant(X) 
    &gt;&gt;&gt; model = sm.OLS(Y,X) 
    &gt;&gt;&gt; results = model.fit() 
    &gt;&gt;&gt; results.params 
    const        10.603498 
    education     0.594859 
    dtype: float64 
 
    &gt;&gt;&gt; results.tvalues 
    const        2.039813 
    education    6.892802 
    dtype: float64 
 
    &gt;&gt;&gt; print(results.t_test([1, 0])) 
                                 Test for Constraints 
    ============================================================================== 
                     coef    std err          t      P&gt;|t|      [0.025      0.975] 
    ------------------------------------------------------------------------------ 
    c0            10.6035      5.198      2.040      0.048       0.120      21.087 
    ============================================================================== 
 
    &gt;&gt;&gt; print(results.f_test(np.identity(2))) 
    &lt;F test: F=array([[159.63031026]]), p=1.2607168903696672e-20, 
     df_denom=43, df_num=2&gt; 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s4">'params'</span><span class="s1">: base._model_params_doc</span><span class="s3">,</span>
           <span class="s4">'extra_params'</span><span class="s1">: base._missing_param_doc + base._extra_param_doc}</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">missing=</span><span class="s4">'none'</span><span class="s3">, </span><span class="s1">hasconst=</span><span class="s3">None,</span>
                 <span class="s1">**kwargs):</span>
        <span class="s3">if </span><span class="s4">&quot;weights&quot; </span><span class="s3">in </span><span class="s1">kwargs:</span>
            <span class="s1">msg = (</span><span class="s4">&quot;Weights are not supported in OLS and will be ignored&quot;</span>
                   <span class="s4">&quot;An exception will be raised in the next version.&quot;</span><span class="s1">)</span>
            <span class="s1">warnings.warn(msg</span><span class="s3">, </span><span class="s1">ValueWarning)</span>
        <span class="s1">super(OLS</span><span class="s3">, </span><span class="s1">self).__init__(endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">missing=missing</span><span class="s3">,</span>
                                  <span class="s1">hasconst=hasconst</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s3">if </span><span class="s4">&quot;weights&quot; </span><span class="s3">in </span><span class="s1">self._init_keys:</span>
            <span class="s1">self._init_keys.remove(</span><span class="s4">&quot;weights&quot;</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">type(self) </span><span class="s3">is </span><span class="s1">OLS:</span>
            <span class="s1">self._check_kwargs(kwargs</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;offset&quot;</span><span class="s1">])</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">scale=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        The likelihood function for the OLS model. 
 
        Parameters 
        ---------- 
        params : array_like 
            The coefficients with which to estimate the log-likelihood. 
        scale : float or None 
            If None, return the profile (concentrated) log likelihood 
            (profiled over the scale parameter), else return the 
            log-likelihood using the given scale value. 
 
        Returns 
        ------- 
        float 
            The likelihood function evaluated at params. 
        &quot;&quot;&quot;</span>
        <span class="s1">nobs2 = self.nobs / </span><span class="s5">2.0</span>
        <span class="s1">nobs = float(self.nobs)</span>
        <span class="s1">resid = self.endog - np.dot(self.exog</span><span class="s3">, </span><span class="s1">params)</span>
        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'offset'</span><span class="s1">):</span>
            <span class="s1">resid -= self.offset</span>
        <span class="s1">ssr = np.sum(resid**</span><span class="s5">2</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">scale </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s0"># profile log likelihood</span>
            <span class="s1">llf = -nobs2*np.log(</span><span class="s5">2</span><span class="s1">*np.pi) - nobs2*np.log(ssr / nobs) - nobs2</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s0"># log-likelihood</span>
            <span class="s1">llf = -nobs2 * np.log(</span><span class="s5">2 </span><span class="s1">* np.pi * scale) - ssr / (</span><span class="s5">2</span><span class="s1">*scale)</span>
        <span class="s3">return </span><span class="s1">llf</span>

    <span class="s3">def </span><span class="s1">whiten(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">&quot;&quot;&quot; 
        OLS model whitener does nothing. 
 
        Parameters 
        ---------- 
        x : array_like 
            Data to be whitened. 
 
        Returns 
        ------- 
        array_like 
            The input array unmodified. 
 
        See Also 
        -------- 
        OLS : Fit a linear model using Ordinary Least Squares. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">x</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">scale=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Evaluate the score function at a given point. 
 
        The score corresponds to the profile (concentrated) 
        log-likelihood in which the scale parameter has been profiled 
        out. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameter vector at which the score function is 
            computed. 
        scale : float or None 
            If None, return the profile (concentrated) log likelihood 
            (profiled over the scale parameter), else return the 
            log-likelihood using the given scale value. 
 
        Returns 
        ------- 
        ndarray 
            The score vector. 
        &quot;&quot;&quot;</span>

        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;_wexog_xprod&quot;</span><span class="s1">):</span>
            <span class="s1">self._setup_score_hess()</span>

        <span class="s1">xtxb = np.dot(self._wexog_xprod</span><span class="s3">, </span><span class="s1">params)</span>
        <span class="s1">sdr = -self._wexog_x_wendog + xtxb</span>

        <span class="s3">if </span><span class="s1">scale </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">ssr = self._wendog_xprod - </span><span class="s5">2 </span><span class="s1">* np.dot(self._wexog_x_wendog.T</span><span class="s3">,</span>
                                                  <span class="s1">params)</span>
            <span class="s1">ssr += np.dot(params</span><span class="s3">, </span><span class="s1">xtxb)</span>
            <span class="s3">return </span><span class="s1">-self.nobs * sdr / ssr</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">-sdr / scale</span>

    <span class="s3">def </span><span class="s1">_setup_score_hess(self):</span>
        <span class="s1">y = self.wendog</span>
        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'offset'</span><span class="s1">):</span>
            <span class="s1">y = y - self.offset</span>
        <span class="s1">self._wendog_xprod = np.sum(y * y)</span>
        <span class="s1">self._wexog_xprod = np.dot(self.wexog.T</span><span class="s3">, </span><span class="s1">self.wexog)</span>
        <span class="s1">self._wexog_x_wendog = np.dot(self.wexog.T</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">scale=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Evaluate the Hessian function at a given point. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameter vector at which the Hessian is computed. 
        scale : float or None 
            If None, return the profile (concentrated) log likelihood 
            (profiled over the scale parameter), else return the 
            log-likelihood using the given scale value. 
 
        Returns 
        ------- 
        ndarray 
            The Hessian matrix. 
        &quot;&quot;&quot;</span>

        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;_wexog_xprod&quot;</span><span class="s1">):</span>
            <span class="s1">self._setup_score_hess()</span>

        <span class="s1">xtxb = np.dot(self._wexog_xprod</span><span class="s3">, </span><span class="s1">params)</span>

        <span class="s3">if </span><span class="s1">scale </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">ssr = self._wendog_xprod - </span><span class="s5">2 </span><span class="s1">* np.dot(self._wexog_x_wendog.T</span><span class="s3">,</span>
                                                  <span class="s1">params)</span>
            <span class="s1">ssr += np.dot(params</span><span class="s3">, </span><span class="s1">xtxb)</span>
            <span class="s1">ssrp = -</span><span class="s5">2</span><span class="s1">*self._wexog_x_wendog + </span><span class="s5">2</span><span class="s1">*xtxb</span>
            <span class="s1">hm = self._wexog_xprod / ssr - np.outer(ssrp</span><span class="s3">, </span><span class="s1">ssrp) / ssr**</span><span class="s5">2</span>
            <span class="s3">return </span><span class="s1">-self.nobs * hm / </span><span class="s5">2</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">-self._wexog_xprod / scale</span>

    <span class="s3">def </span><span class="s1">hessian_factor(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">scale=</span><span class="s3">None, </span><span class="s1">observed=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Calculate the weights for the Hessian. 
 
        Parameters 
        ---------- 
        params : ndarray 
            The parameter at which Hessian is evaluated. 
        scale : None or float 
            If scale is None, then the default scale will be calculated. 
            Default scale is defined by `self.scaletype` and set in fit. 
            If scale is not None, then it is used as a fixed scale. 
        observed : bool 
            If True, then the observed Hessian is returned. If false then the 
            expected information matrix is returned. 
 
        Returns 
        ------- 
        ndarray 
            A 1d weight vector used in the calculation of the Hessian. 
            The hessian is obtained by `(exog.T * hessian_factor).dot(exog)`. 
        &quot;&quot;&quot;</span>

        <span class="s3">return </span><span class="s1">np.ones(self.exog.shape[</span><span class="s5">0</span><span class="s1">])</span>

    <span class="s1">@Appender(_fit_regularized_doc)</span>
    <span class="s3">def </span><span class="s1">fit_regularized(self</span><span class="s3">, </span><span class="s1">method=</span><span class="s4">&quot;elastic_net&quot;</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s5">0.</span><span class="s3">,</span>
                        <span class="s1">L1_wt=</span><span class="s5">1.</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">profile_scale=</span><span class="s3">False,</span>
                        <span class="s1">refit=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>

        <span class="s0"># In the future we could add support for other penalties, e.g. SCAD.</span>
        <span class="s3">if </span><span class="s1">method </span><span class="s3">not in </span><span class="s1">(</span><span class="s4">&quot;elastic_net&quot;</span><span class="s3">, </span><span class="s4">&quot;sqrt_lasso&quot;</span><span class="s1">):</span>
            <span class="s1">msg = </span><span class="s4">&quot;Unknown method '%s' for fit_regularized&quot; </span><span class="s1">% method</span>
            <span class="s3">raise </span><span class="s1">ValueError(msg)</span>

        <span class="s0"># Set default parameters.</span>
        <span class="s1">defaults = {</span><span class="s4">&quot;maxiter&quot;</span><span class="s1">:  </span><span class="s5">50</span><span class="s3">, </span><span class="s4">&quot;cnvrg_tol&quot;</span><span class="s1">: </span><span class="s5">1e-10</span><span class="s3">,</span>
                    <span class="s4">&quot;zero_tol&quot;</span><span class="s1">: </span><span class="s5">1e-8</span><span class="s1">}</span>
        <span class="s1">defaults.update(kwargs)</span>

        <span class="s3">if </span><span class="s1">method == </span><span class="s4">&quot;sqrt_lasso&quot;</span><span class="s1">:</span>
            <span class="s3">from </span><span class="s1">statsmodels.base.elastic_net </span><span class="s3">import </span><span class="s1">(</span>
                <span class="s1">RegularizedResults</span><span class="s3">,</span>
                <span class="s1">RegularizedResultsWrapper</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">params = self._sqrt_lasso(alpha</span><span class="s3">, </span><span class="s1">refit</span><span class="s3">, </span><span class="s1">defaults[</span><span class="s4">&quot;zero_tol&quot;</span><span class="s1">])</span>
            <span class="s1">results = RegularizedResults(self</span><span class="s3">, </span><span class="s1">params)</span>
            <span class="s3">return </span><span class="s1">RegularizedResultsWrapper(results)</span>

        <span class="s3">from </span><span class="s1">statsmodels.base.elastic_net </span><span class="s3">import </span><span class="s1">fit_elasticnet</span>

        <span class="s3">if </span><span class="s1">L1_wt == </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">self._fit_ridge(alpha)</span>

        <span class="s0"># If a scale parameter is passed in, the non-profile</span>
        <span class="s0"># likelihood (residual sum of squares divided by -2) is used,</span>
        <span class="s0"># otherwise the profile likelihood is used.</span>
        <span class="s3">if </span><span class="s1">profile_scale:</span>
            <span class="s1">loglike_kwds = {}</span>
            <span class="s1">score_kwds = {}</span>
            <span class="s1">hess_kwds = {}</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">loglike_kwds = {</span><span class="s4">&quot;scale&quot;</span><span class="s1">: </span><span class="s5">1</span><span class="s1">}</span>
            <span class="s1">score_kwds = {</span><span class="s4">&quot;scale&quot;</span><span class="s1">: </span><span class="s5">1</span><span class="s1">}</span>
            <span class="s1">hess_kwds = {</span><span class="s4">&quot;scale&quot;</span><span class="s1">: </span><span class="s5">1</span><span class="s1">}</span>

        <span class="s3">return </span><span class="s1">fit_elasticnet(self</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">,</span>
                              <span class="s1">alpha=alpha</span><span class="s3">,</span>
                              <span class="s1">L1_wt=L1_wt</span><span class="s3">,</span>
                              <span class="s1">start_params=start_params</span><span class="s3">,</span>
                              <span class="s1">loglike_kwds=loglike_kwds</span><span class="s3">,</span>
                              <span class="s1">score_kwds=score_kwds</span><span class="s3">,</span>
                              <span class="s1">hess_kwds=hess_kwds</span><span class="s3">,</span>
                              <span class="s1">refit=refit</span><span class="s3">,</span>
                              <span class="s1">check_step=</span><span class="s3">False,</span>
                              <span class="s1">**defaults)</span>

    <span class="s3">def </span><span class="s1">_sqrt_lasso(self</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">refit</span><span class="s3">, </span><span class="s1">zero_tol):</span>

        <span class="s3">try</span><span class="s1">:</span>
            <span class="s3">import </span><span class="s1">cvxopt</span>
        <span class="s3">except </span><span class="s1">ImportError:</span>
            <span class="s1">msg = </span><span class="s4">'sqrt_lasso fitting requires the cvxopt module'</span>
            <span class="s3">raise </span><span class="s1">ValueError(msg)</span>

        <span class="s1">n = len(self.endog)</span>
        <span class="s1">p = self.exog.shape[</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">h0 = cvxopt.matrix(</span><span class="s5">0.</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s1">*p+</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">h1 = cvxopt.matrix(</span><span class="s5">0.</span><span class="s3">, </span><span class="s1">(n+</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">h1[</span><span class="s5">1</span><span class="s1">:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = cvxopt.matrix(self.endog</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>

        <span class="s1">G0 = cvxopt.spmatrix([]</span><span class="s3">, </span><span class="s1">[]</span><span class="s3">, </span><span class="s1">[]</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s1">*p+</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">*p+</span><span class="s5">1</span><span class="s1">))</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">*p+</span><span class="s5">1</span><span class="s1">):</span>
            <span class="s1">G0[i</span><span class="s3">, </span><span class="s1">i] = -</span><span class="s5">1</span>
        <span class="s1">G1 = cvxopt.matrix(</span><span class="s5">0.</span><span class="s3">, </span><span class="s1">(n+</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s1">*p+</span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">G1[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = -</span><span class="s5">1</span>
        <span class="s1">G1[</span><span class="s5">1</span><span class="s1">:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">:p+</span><span class="s5">1</span><span class="s1">] = self.exog</span>
        <span class="s1">G1[</span><span class="s5">1</span><span class="s1">:</span><span class="s3">, </span><span class="s1">p+</span><span class="s5">1</span><span class="s1">:] = -self.exog</span>

        <span class="s1">c = cvxopt.matrix(alpha / n</span><span class="s3">, </span><span class="s1">(</span><span class="s5">2</span><span class="s1">*p + </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">c[</span><span class="s5">0</span><span class="s1">] = </span><span class="s5">1 </span><span class="s1">/ np.sqrt(n)</span>

        <span class="s3">from </span><span class="s1">cvxopt </span><span class="s3">import </span><span class="s1">solvers</span>
        <span class="s1">solvers.options[</span><span class="s4">&quot;show_progress&quot;</span><span class="s1">] = </span><span class="s3">False</span>

        <span class="s1">rslt = solvers.socp(c</span><span class="s3">, </span><span class="s1">Gl=G0</span><span class="s3">, </span><span class="s1">hl=h0</span><span class="s3">, </span><span class="s1">Gq=[G1]</span><span class="s3">, </span><span class="s1">hq=[h1])</span>
        <span class="s1">x = np.asarray(rslt[</span><span class="s4">'x'</span><span class="s1">]).flat</span>
        <span class="s1">bp = x[</span><span class="s5">1</span><span class="s1">:p+</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">bn = x[p+</span><span class="s5">1</span><span class="s1">:]</span>
        <span class="s1">params = bp - bn</span>

        <span class="s3">if not </span><span class="s1">refit:</span>
            <span class="s3">return </span><span class="s1">params</span>

        <span class="s1">ii = np.flatnonzero(np.abs(params) &gt; zero_tol)</span>
        <span class="s1">rfr = OLS(self.endog</span><span class="s3">, </span><span class="s1">self.exog[:</span><span class="s3">, </span><span class="s1">ii]).fit()</span>
        <span class="s1">params *= </span><span class="s5">0</span>
        <span class="s1">params[ii] = rfr.params</span>

        <span class="s3">return </span><span class="s1">params</span>

    <span class="s3">def </span><span class="s1">_fit_ridge(self</span><span class="s3">, </span><span class="s1">alpha):</span>
        <span class="s2">&quot;&quot;&quot; 
        Fit a linear model using ridge regression. 
 
        Parameters 
        ---------- 
        alpha : scalar or array_like 
            The penalty weight.  If a scalar, the same penalty weight 
            applies to all variables in the model.  If a vector, it 
            must have the same length as `params`, and contains a 
            penalty weight for each coefficient. 
 
        Notes 
        ----- 
        Equivalent to fit_regularized with L1_wt = 0 (but implemented 
        more efficiently). 
        &quot;&quot;&quot;</span>

        <span class="s1">u</span><span class="s3">, </span><span class="s1">s</span><span class="s3">, </span><span class="s1">vt = np.linalg.svd(self.exog</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">v = vt.T</span>
        <span class="s1">q = np.dot(u.T</span><span class="s3">, </span><span class="s1">self.endog) * s</span>
        <span class="s1">s2 = s * s</span>
        <span class="s3">if </span><span class="s1">np.isscalar(alpha):</span>
            <span class="s1">sd = s2 + alpha * self.nobs</span>
            <span class="s1">params = q / sd</span>
            <span class="s1">params = np.dot(v</span><span class="s3">, </span><span class="s1">params)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">alpha = np.asarray(alpha)</span>
            <span class="s1">vtav = self.nobs * np.dot(vt</span><span class="s3">, </span><span class="s1">alpha[:</span><span class="s3">, None</span><span class="s1">] * v)</span>
            <span class="s1">d = np.diag(vtav) + s2</span>
            <span class="s1">np.fill_diagonal(vtav</span><span class="s3">, </span><span class="s1">d)</span>
            <span class="s1">r = np.linalg.solve(vtav</span><span class="s3">, </span><span class="s1">q)</span>
            <span class="s1">params = np.dot(v</span><span class="s3">, </span><span class="s1">r)</span>

        <span class="s3">from </span><span class="s1">statsmodels.base.elastic_net </span><span class="s3">import </span><span class="s1">RegularizedResults</span>
        <span class="s3">return </span><span class="s1">RegularizedResults(self</span><span class="s3">, </span><span class="s1">params)</span>


<span class="s3">class </span><span class="s1">GLSAR(GLS):</span>
    <span class="s1">__doc__ = </span><span class="s4">&quot;&quot;&quot; 
    Generalized Least Squares with AR covariance structure 
 
    %(params)s 
    rho : int 
        The order of the autoregressive covariance. 
    %(extra_params)s 
 
    Notes 
    ----- 
    GLSAR is considered to be experimental. 
    The linear autoregressive process of order p--AR(p)--is defined as: 
    TODO 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import statsmodels.api as sm 
    &gt;&gt;&gt; X = range(1,8) 
    &gt;&gt;&gt; X = sm.add_constant(X) 
    &gt;&gt;&gt; Y = [1,3,4,5,8,10,9] 
    &gt;&gt;&gt; model = sm.GLSAR(Y, X, rho=2) 
    &gt;&gt;&gt; for i in range(6): 
    ...     results = model.fit() 
    ...     print(&quot;AR coefficients: {0}&quot;.format(model.rho)) 
    ...     rho, sigma = sm.regression.yule_walker(results.resid, 
    ...                                            order=model.order) 
    ...     model = sm.GLSAR(Y, X, rho) 
    ... 
    AR coefficients: [ 0.  0.] 
    AR coefficients: [-0.52571491 -0.84496178] 
    AR coefficients: [-0.6104153  -0.86656458] 
    AR coefficients: [-0.60439494 -0.857867  ] 
    AR coefficients: [-0.6048218  -0.85846157] 
    AR coefficients: [-0.60479146 -0.85841922] 
    &gt;&gt;&gt; results.params 
    array([-0.66661205,  1.60850853]) 
    &gt;&gt;&gt; results.tvalues 
    array([ -2.10304127,  21.8047269 ]) 
    &gt;&gt;&gt; print(results.t_test([1, 0])) 
    &lt;T test: effect=array([-0.66661205]), sd=array([[ 0.31697526]]), 
     t=array([[-2.10304127]]), p=array([[ 0.06309969]]), df_denom=3&gt; 
    &gt;&gt;&gt; print(results.f_test(np.identity(2))) 
    &lt;F test: F=array([[ 1815.23061844]]), p=[[ 0.00002372]], 
     df_denom=3, df_num=2&gt; 
 
    Or, equivalently 
 
    &gt;&gt;&gt; model2 = sm.GLSAR(Y, X, rho=2) 
    &gt;&gt;&gt; res = model2.iterative_fit(maxiter=6) 
    &gt;&gt;&gt; model2.rho 
    array([-0.60479146, -0.85841922]) 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s4">'params'</span><span class="s1">: base._model_params_doc</span><span class="s3">,</span>
           <span class="s4">'extra_params'</span><span class="s1">: base._missing_param_doc + base._extra_param_doc}</span>
    <span class="s0"># TODO: Complete docstring</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">rho=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">missing=</span><span class="s4">'none'</span><span class="s3">, </span><span class="s1">hasconst=</span><span class="s3">None,</span>
                 <span class="s1">**kwargs):</span>
        <span class="s0"># this looks strange, interpreting rho as order if it is int</span>
        <span class="s3">if </span><span class="s1">isinstance(rho</span><span class="s3">, </span><span class="s1">(int</span><span class="s3">, </span><span class="s1">np.integer)):</span>
            <span class="s1">self.order = int(rho)</span>
            <span class="s1">self.rho = np.zeros(self.order</span><span class="s3">, </span><span class="s1">np.float64)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.rho = np.squeeze(np.asarray(rho))</span>
            <span class="s3">if </span><span class="s1">len(self.rho.shape) </span><span class="s3">not in </span><span class="s1">[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;AR parameters must be a scalar or a vector&quot;</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">self.rho.shape == ():</span>
                <span class="s1">self.rho.shape = (</span><span class="s5">1</span><span class="s3">,</span><span class="s1">)</span>
            <span class="s1">self.order = self.rho.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s0"># JP this looks wrong, should be a regression on constant</span>
            <span class="s0"># results for rho estimate now identical to yule-walker on y</span>
            <span class="s0"># super(AR, self).__init__(endog, add_constant(endog))</span>
            <span class="s1">super(GLSAR</span><span class="s3">, </span><span class="s1">self).__init__(endog</span><span class="s3">, </span><span class="s1">np.ones((endog.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span><span class="s3">,</span>
                                        <span class="s1">missing=missing</span><span class="s3">, </span><span class="s1">hasconst=</span><span class="s3">None,</span>
                                        <span class="s1">**kwargs)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">super(GLSAR</span><span class="s3">, </span><span class="s1">self).__init__(endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">missing=missing</span><span class="s3">,</span>
                                        <span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">iterative_fit(self</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s5">3</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s5">1e-4</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Perform an iterative two-stage procedure to estimate a GLS model. 
 
        The model is assumed to have AR(p) errors, AR(p) parameters and 
        regression coefficients are estimated iteratively. 
 
        Parameters 
        ---------- 
        maxiter : int, optional 
            The number of iterations. 
        rtol : float, optional 
            Relative tolerance between estimated coefficients to stop the 
            estimation.  Stops if max(abs(last - current) / abs(last)) &lt; rtol. 
        **kwargs 
            Additional keyword arguments passed to `fit`. 
 
        Returns 
        ------- 
        RegressionResults 
            The results computed using an iterative fit. 
        &quot;&quot;&quot;</span>
        <span class="s0"># TODO: update this after going through example.</span>
        <span class="s1">converged = </span><span class="s3">False</span>
        <span class="s1">i = -</span><span class="s5">1  </span><span class="s0"># need to initialize for maxiter &lt; 1 (skip loop)</span>
        <span class="s1">history = {</span><span class="s4">'params'</span><span class="s1">: []</span><span class="s3">, </span><span class="s4">'rho'</span><span class="s1">: [self.rho]}</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(maxiter - </span><span class="s5">1</span><span class="s1">):</span>
            <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'pinv_wexog'</span><span class="s1">):</span>
                <span class="s3">del </span><span class="s1">self.pinv_wexog</span>
            <span class="s1">self.initialize()</span>
            <span class="s1">results = self.fit()</span>
            <span class="s1">history[</span><span class="s4">'params'</span><span class="s1">].append(results.params)</span>
            <span class="s3">if </span><span class="s1">i == </span><span class="s5">0</span><span class="s1">:</span>
                <span class="s1">last = results.params</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">diff = np.max(np.abs(last - results.params) / np.abs(last))</span>
                <span class="s3">if </span><span class="s1">diff &lt; rtol:</span>
                    <span class="s1">converged = </span><span class="s3">True</span>
                    <span class="s3">break</span>
                <span class="s1">last = results.params</span>
            <span class="s1">self.rho</span><span class="s3">, </span><span class="s1">_ = yule_walker(results.resid</span><span class="s3">,</span>
                                      <span class="s1">order=self.order</span><span class="s3">, </span><span class="s1">df=</span><span class="s3">None</span><span class="s1">)</span>
            <span class="s1">history[</span><span class="s4">'rho'</span><span class="s1">].append(self.rho)</span>

        <span class="s0"># why not another call to self.initialize</span>
        <span class="s0"># Use kwarg to insert history</span>
        <span class="s3">if not </span><span class="s1">converged </span><span class="s3">and </span><span class="s1">maxiter &gt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s0"># maxiter &lt;= 0 just does OLS</span>
            <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'pinv_wexog'</span><span class="s1">):</span>
                <span class="s3">del </span><span class="s1">self.pinv_wexog</span>
            <span class="s1">self.initialize()</span>

        <span class="s0"># if converged then this is a duplicate fit, because we did not</span>
        <span class="s0"># update rho</span>
        <span class="s1">results = self.fit(history=history</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s1">results.iter = i + </span><span class="s5">1</span>
        <span class="s0"># add last fit to history, not if duplicate fit</span>
        <span class="s3">if not </span><span class="s1">converged:</span>
            <span class="s1">results.history[</span><span class="s4">'params'</span><span class="s1">].append(results.params)</span>
            <span class="s1">results.iter += </span><span class="s5">1</span>

        <span class="s1">results.converged = converged</span>

        <span class="s3">return </span><span class="s1">results</span>

    <span class="s3">def </span><span class="s1">whiten(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">&quot;&quot;&quot; 
        Whiten a series of columns according to an AR(p) covariance structure. 
 
        Whitening using this method drops the initial p observations. 
 
        Parameters 
        ---------- 
        x : array_like 
            The data to be whitened. 
 
        Returns 
        ------- 
        ndarray 
            The whitened data. 
        &quot;&quot;&quot;</span>
        <span class="s0"># TODO: notation for AR process</span>
        <span class="s1">x = np.asarray(x</span><span class="s3">, </span><span class="s1">np.float64)</span>
        <span class="s1">_x = x.copy()</span>

        <span class="s0"># the following loops over the first axis,  works for 1d and nd</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.order):</span>
            <span class="s1">_x[(i + </span><span class="s5">1</span><span class="s1">):] = _x[(i + </span><span class="s5">1</span><span class="s1">):] - self.rho[i] * x[</span><span class="s5">0</span><span class="s1">:-(i + </span><span class="s5">1</span><span class="s1">)]</span>
        <span class="s3">return </span><span class="s1">_x[self.order:]</span>


<span class="s3">def </span><span class="s1">yule_walker(x</span><span class="s3">, </span><span class="s1">order=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">method=</span><span class="s4">&quot;adjusted&quot;</span><span class="s3">, </span><span class="s1">df=</span><span class="s3">None, </span><span class="s1">inv=</span><span class="s3">False,</span>
                <span class="s1">demean=</span><span class="s3">True</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot; 
    Estimate AR(p) parameters from a sequence using the Yule-Walker equations. 
 
    Adjusted or maximum-likelihood estimator (mle) 
 
    Parameters 
    ---------- 
    x : array_like 
        A 1d array. 
    order : int, optional 
        The order of the autoregressive process.  Default is 1. 
    method : str, optional 
       Method can be 'adjusted' or 'mle' and this determines 
       denominator in estimate of autocorrelation function (ACF) at 
       lag k. If 'mle', the denominator is n=X.shape[0], if 'adjusted' 
       the denominator is n-k.  The default is adjusted. 
    df : int, optional 
       Specifies the degrees of freedom. If `df` is supplied, then it 
       is assumed the X has `df` degrees of freedom rather than `n`. 
       Default is None. 
    inv : bool 
        If inv is True the inverse of R is also returned.  Default is 
        False. 
    demean : bool 
        True, the mean is subtracted from `X` before estimation. 
 
    Returns 
    ------- 
    rho : ndarray 
        AR(p) coefficients computed using the Yule-Walker method. 
    sigma : float 
        The estimate of the residual standard deviation. 
 
    See Also 
    -------- 
    burg : Burg's AR estimator. 
 
    Notes 
    ----- 
    See https://en.wikipedia.org/wiki/Autoregressive_moving_average_model for 
    further details. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import statsmodels.api as sm 
    &gt;&gt;&gt; from statsmodels.datasets.sunspots import load 
    &gt;&gt;&gt; data = load() 
    &gt;&gt;&gt; rho, sigma = sm.regression.yule_walker(data.endog, order=4, 
    ...                                        method=&quot;mle&quot;) 
 
    &gt;&gt;&gt; rho 
    array([ 1.28310031, -0.45240924, -0.20770299,  0.04794365]) 
    &gt;&gt;&gt; sigma 
    16.808022730464351 
    &quot;&quot;&quot;</span>
    <span class="s0"># TODO: define R better, look back at notes and technical notes on YW.</span>
    <span class="s0"># First link here is useful</span>
    <span class="s0"># http://www-stat.wharton.upenn.edu/~steele/Courses/956/ResourceDetails/YuleWalkerAndMore.htm</span>

    <span class="s1">method = string_like(</span>
        <span class="s1">method</span><span class="s3">, </span><span class="s4">&quot;method&quot;</span><span class="s3">, </span><span class="s1">options=(</span><span class="s4">&quot;adjusted&quot;</span><span class="s3">, </span><span class="s4">&quot;unbiased&quot;</span><span class="s3">, </span><span class="s4">&quot;mle&quot;</span><span class="s1">)</span>
    <span class="s1">)</span>
    <span class="s3">if </span><span class="s1">method == </span><span class="s4">&quot;unbiased&quot;</span><span class="s1">:</span>
        <span class="s1">warnings.warn(</span>
            <span class="s4">&quot;unbiased is deprecated in factor of adjusted to reflect that the &quot;</span>
            <span class="s4">&quot;term is adjusting the sample size used in the autocovariance &quot;</span>
            <span class="s4">&quot;calculation rather than estimating an unbiased autocovariance. &quot;</span>
            <span class="s4">&quot;After release 0.13, using 'unbiased' will raise.&quot;</span><span class="s3">,</span>
            <span class="s1">FutureWarning</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">method = </span><span class="s4">&quot;adjusted&quot;</span>

    <span class="s3">if </span><span class="s1">method </span><span class="s3">not in </span><span class="s1">(</span><span class="s4">&quot;adjusted&quot;</span><span class="s3">, </span><span class="s4">&quot;mle&quot;</span><span class="s1">):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;ACF estimation method must be 'adjusted' or 'MLE'&quot;</span><span class="s1">)</span>
    <span class="s1">x = np.array(x</span><span class="s3">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s3">if </span><span class="s1">demean:</span>
        <span class="s1">x -= x.mean()</span>
    <span class="s1">n = df </span><span class="s3">or </span><span class="s1">x.shape[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s0"># this handles df_resid ie., n - p</span>
    <span class="s1">adj_needed = method == </span><span class="s4">&quot;adjusted&quot;</span>

    <span class="s3">if </span><span class="s1">x.ndim &gt; </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">x.shape[</span><span class="s5">1</span><span class="s1">] != </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;expecting a vector to estimate AR parameters&quot;</span><span class="s1">)</span>
    <span class="s1">r = np.zeros(order+</span><span class="s5">1</span><span class="s3">, </span><span class="s1">np.float64)</span>
    <span class="s1">r[</span><span class="s5">0</span><span class="s1">] = (x ** </span><span class="s5">2</span><span class="s1">).sum() / n</span>
    <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">order+</span><span class="s5">1</span><span class="s1">):</span>
        <span class="s1">r[k] = (x[</span><span class="s5">0</span><span class="s1">:-k] * x[k:]).sum() / (n - k * adj_needed)</span>
    <span class="s1">R = toeplitz(r[:-</span><span class="s5">1</span><span class="s1">])</span>

    <span class="s3">try</span><span class="s1">:</span>
        <span class="s1">rho = np.linalg.solve(R</span><span class="s3">, </span><span class="s1">r[</span><span class="s5">1</span><span class="s1">:])</span>
    <span class="s3">except </span><span class="s1">np.linalg.LinAlgError </span><span class="s3">as </span><span class="s1">err:</span>
        <span class="s3">if </span><span class="s4">'Singular matrix' </span><span class="s3">in </span><span class="s1">str(err):</span>
            <span class="s1">warnings.warn(</span><span class="s4">&quot;Matrix is singular. Using pinv.&quot;</span><span class="s3">, </span><span class="s1">ValueWarning)</span>
            <span class="s1">rho = np.linalg.pinv(R) @ r[</span><span class="s5">1</span><span class="s1">:]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise</span>

    <span class="s1">sigmasq = r[</span><span class="s5">0</span><span class="s1">] - (r[</span><span class="s5">1</span><span class="s1">:]*rho).sum()</span>
    <span class="s3">if not </span><span class="s1">np.isnan(sigmasq) </span><span class="s3">and </span><span class="s1">sigmasq &gt; </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s1">sigma = np.sqrt(sigmasq)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">sigma = np.nan</span>
    <span class="s3">if </span><span class="s1">inv:</span>
        <span class="s3">return </span><span class="s1">rho</span><span class="s3">, </span><span class="s1">sigma</span><span class="s3">, </span><span class="s1">np.linalg.inv(R)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">rho</span><span class="s3">, </span><span class="s1">sigma</span>


<span class="s3">def </span><span class="s1">burg(endog</span><span class="s3">, </span><span class="s1">order=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">demean=</span><span class="s3">True</span><span class="s1">):</span>
    <span class="s2">&quot;&quot;&quot; 
    Compute Burg's AP(p) parameter estimator. 
 
    Parameters 
    ---------- 
    endog : array_like 
        The endogenous variable. 
    order : int, optional 
        Order of the AR.  Default is 1. 
    demean : bool, optional 
        Flag indicating to subtract the mean from endog before estimation. 
 
    Returns 
    ------- 
    rho : ndarray 
        The AR(p) coefficients computed using Burg's algorithm. 
    sigma2 : float 
        The estimate of the residual variance. 
 
    See Also 
    -------- 
    yule_walker : Estimate AR parameters using the Yule-Walker method. 
 
    Notes 
    ----- 
    AR model estimated includes a constant that is estimated using the sample 
    mean (see [1]_). This value is not reported. 
 
    References 
    ---------- 
    .. [1] Brockwell, P.J. and Davis, R.A., 2016. Introduction to time series 
        and forecasting. Springer. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import statsmodels.api as sm 
    &gt;&gt;&gt; from statsmodels.datasets.sunspots import load 
    &gt;&gt;&gt; data = load() 
    &gt;&gt;&gt; rho, sigma2 = sm.regression.linear_model.burg(data.endog, order=4) 
 
    &gt;&gt;&gt; rho 
    array([ 1.30934186, -0.48086633, -0.20185982,  0.05501941]) 
    &gt;&gt;&gt; sigma2 
    271.2467306963966 
    &quot;&quot;&quot;</span>
    <span class="s0"># Avoid circular imports</span>
    <span class="s3">from </span><span class="s1">statsmodels.tsa.stattools </span><span class="s3">import </span><span class="s1">levinson_durbin_pacf</span><span class="s3">, </span><span class="s1">pacf_burg</span>

    <span class="s1">endog = np.squeeze(np.asarray(endog))</span>
    <span class="s3">if </span><span class="s1">endog.ndim != </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'endog must be 1-d or squeezable to 1-d.'</span><span class="s1">)</span>
    <span class="s1">order = int(order)</span>
    <span class="s3">if </span><span class="s1">order &lt; </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'order must be an integer larger than 1'</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">demean:</span>
        <span class="s1">endog = endog - endog.mean()</span>
    <span class="s1">pacf</span><span class="s3">, </span><span class="s1">sigma = pacf_burg(endog</span><span class="s3">, </span><span class="s1">order</span><span class="s3">, </span><span class="s1">demean=demean)</span>
    <span class="s1">ar</span><span class="s3">, </span><span class="s1">_ = levinson_durbin_pacf(pacf)</span>
    <span class="s3">return </span><span class="s1">ar</span><span class="s3">, </span><span class="s1">sigma[-</span><span class="s5">1</span><span class="s1">]</span>


<span class="s3">class </span><span class="s1">RegressionResults(base.LikelihoodModelResults):</span>
    <span class="s2">r&quot;&quot;&quot; 
    This class summarizes the fit of a linear regression model. 
 
    It handles the output of contrasts, estimates of covariance, etc. 
 
    Parameters 
    ---------- 
    model : RegressionModel 
        The regression model instance. 
    params : ndarray 
        The estimated parameters. 
    normalized_cov_params : ndarray 
        The normalized covariance parameters. 
    scale : float 
        The estimated scale of the residuals. 
    cov_type : str 
        The covariance estimator used in the results. 
    cov_kwds : dict 
        Additional keywords used in the covariance specification. 
    use_t : bool 
        Flag indicating to use the Student's t in inference. 
    **kwargs 
        Additional keyword arguments used to initialize the results. 
 
    Attributes 
    ---------- 
    pinv_wexog 
        See model class docstring for implementation details. 
    cov_type 
        Parameter covariance estimator used for standard errors and t-stats. 
    df_model 
        Model degrees of freedom. The number of regressors `p`. Does not 
        include the constant if one is present. 
    df_resid 
        Residual degrees of freedom. `n - p - 1`, if a constant is present. 
        `n - p` if a constant is not included. 
    het_scale 
        adjusted squared residuals for heteroscedasticity robust standard 
        errors. Is only available after `HC#_se` or `cov_HC#` is called. 
        See HC#_se for more information. 
    history 
        Estimation history for iterative estimators. 
    model 
        A pointer to the model instance that called fit() or results. 
    params 
        The linear coefficients that minimize the least squares 
        criterion.  This is usually called Beta for the classical 
        linear model. 
    &quot;&quot;&quot;</span>

    <span class="s1">_cache = {}  </span><span class="s0"># needs to be a class attribute for scale setter?</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">model</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">normalized_cov_params=</span><span class="s3">None, </span><span class="s1">scale=</span><span class="s5">1.</span><span class="s3">,</span>
                 <span class="s1">cov_type=</span><span class="s4">'nonrobust'</span><span class="s3">, </span><span class="s1">cov_kwds=</span><span class="s3">None, </span><span class="s1">use_t=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s1">super(RegressionResults</span><span class="s3">, </span><span class="s1">self).__init__(</span>
            <span class="s1">model</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">normalized_cov_params</span><span class="s3">, </span><span class="s1">scale)</span>

        <span class="s1">self._cache = {}</span>
        <span class="s3">if </span><span class="s1">hasattr(model</span><span class="s3">, </span><span class="s4">'wexog_singular_values'</span><span class="s1">):</span>
            <span class="s1">self._wexog_singular_values = model.wexog_singular_values</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._wexog_singular_values = </span><span class="s3">None</span>

        <span class="s1">self.df_model = model.df_model</span>
        <span class="s1">self.df_resid = model.df_resid</span>

        <span class="s3">if </span><span class="s1">cov_type == </span><span class="s4">'nonrobust'</span><span class="s1">:</span>
            <span class="s1">self.cov_type = </span><span class="s4">'nonrobust'</span>
            <span class="s1">self.cov_kwds = {</span>
                <span class="s4">'description'</span><span class="s1">: </span><span class="s4">'Standard Errors assume that the ' </span><span class="s1">+</span>
                <span class="s4">'covariance matrix of the errors is correctly ' </span><span class="s1">+</span>
                <span class="s4">'specified.'</span><span class="s1">}</span>
            <span class="s3">if </span><span class="s1">use_t </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">use_t = </span><span class="s3">True  </span><span class="s0"># TODO: class default</span>
            <span class="s1">self.use_t = use_t</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">cov_kwds </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">cov_kwds = {}</span>
            <span class="s3">if </span><span class="s4">'use_t' </span><span class="s3">in </span><span class="s1">cov_kwds:</span>
                <span class="s0"># TODO: we want to get rid of 'use_t' in cov_kwds</span>
                <span class="s1">use_t_2 = cov_kwds.pop(</span><span class="s4">'use_t'</span><span class="s1">)</span>
                <span class="s3">if </span><span class="s1">use_t </span><span class="s3">is None</span><span class="s1">:</span>
                    <span class="s1">use_t = use_t_2</span>
                <span class="s0"># TODO: warn or not?</span>
            <span class="s1">self.get_robustcov_results(cov_type=cov_type</span><span class="s3">, </span><span class="s1">use_self=</span><span class="s3">True,</span>
                                       <span class="s1">use_t=use_t</span><span class="s3">, </span><span class="s1">**cov_kwds)</span>
        <span class="s3">for </span><span class="s1">key </span><span class="s3">in </span><span class="s1">kwargs:</span>
            <span class="s1">setattr(self</span><span class="s3">, </span><span class="s1">key</span><span class="s3">, </span><span class="s1">kwargs[key])</span>

    <span class="s3">def </span><span class="s1">conf_int(self</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s5">.05</span><span class="s3">, </span><span class="s1">cols=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Compute the confidence interval of the fitted parameters. 
 
        Parameters 
        ---------- 
        alpha : float, optional 
            The `alpha` level for the confidence interval. The default 
            `alpha` = .05 returns a 95% confidence interval. 
        cols : array_like, optional 
            Columns to include in returned confidence intervals. 
 
        Returns 
        ------- 
        array_like 
            The confidence intervals. 
 
        Notes 
        ----- 
        The confidence interval is based on Student's t-distribution. 
        &quot;&quot;&quot;</span>
        <span class="s0"># keep method for docstring for now</span>
        <span class="s1">ci = super(RegressionResults</span><span class="s3">, </span><span class="s1">self).conf_int(alpha=alpha</span><span class="s3">, </span><span class="s1">cols=cols)</span>
        <span class="s3">return </span><span class="s1">ci</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">nobs(self):</span>
        <span class="s2">&quot;&quot;&quot;Number of observations n.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">float(self.model.wexog.shape[</span><span class="s5">0</span><span class="s1">])</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">fittedvalues(self):</span>
        <span class="s2">&quot;&quot;&quot;The predicted values for the original (unwhitened) design.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.model.predict(self.params</span><span class="s3">, </span><span class="s1">self.model.exog)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">wresid(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        The residuals of the transformed/whitened regressand and regressor(s). 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.model.wendog - self.model.predict(</span>
            <span class="s1">self.params</span><span class="s3">, </span><span class="s1">self.model.wexog)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid(self):</span>
        <span class="s2">&quot;&quot;&quot;The residuals of the model.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.model.endog - self.model.predict(</span>
            <span class="s1">self.params</span><span class="s3">, </span><span class="s1">self.model.exog)</span>

    <span class="s0"># TODO: fix writable example</span>
    <span class="s1">@cache_writable()</span>
    <span class="s3">def </span><span class="s1">scale(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        A scale factor for the covariance matrix. 
 
        The Default value is ssr/(n-p).  Note that the square root of `scale` 
        is often called the standard error of the regression. 
        &quot;&quot;&quot;</span>
        <span class="s1">wresid = self.wresid</span>
        <span class="s3">return </span><span class="s1">np.dot(wresid</span><span class="s3">, </span><span class="s1">wresid) / self.df_resid</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">ssr(self):</span>
        <span class="s2">&quot;&quot;&quot;Sum of squared (whitened) residuals.&quot;&quot;&quot;</span>
        <span class="s1">wresid = self.wresid</span>
        <span class="s3">return </span><span class="s1">np.dot(wresid</span><span class="s3">, </span><span class="s1">wresid)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">centered_tss(self):</span>
        <span class="s2">&quot;&quot;&quot;The total (weighted) sum of squares centered about the mean.&quot;&quot;&quot;</span>
        <span class="s1">model = self.model</span>
        <span class="s1">weights = getattr(model</span><span class="s3">, </span><span class="s4">'weights'</span><span class="s3">, None</span><span class="s1">)</span>
        <span class="s1">sigma = getattr(model</span><span class="s3">, </span><span class="s4">'sigma'</span><span class="s3">, None</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">weights </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">mean = np.average(model.endog</span><span class="s3">, </span><span class="s1">weights=weights)</span>
            <span class="s3">return </span><span class="s1">np.sum(weights * (model.endog - mean)**</span><span class="s5">2</span><span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">sigma </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s0"># Exactly matches WLS when sigma is diagonal</span>
            <span class="s1">iota = np.ones_like(model.endog)</span>
            <span class="s1">iota = model.whiten(iota)</span>
            <span class="s1">mean = model.wendog.dot(iota) / iota.dot(iota)</span>
            <span class="s1">err = model.endog - mean</span>
            <span class="s1">err = model.whiten(err)</span>
            <span class="s3">return </span><span class="s1">np.sum(err**</span><span class="s5">2</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">centered_endog = model.wendog - model.wendog.mean()</span>
            <span class="s3">return </span><span class="s1">np.dot(centered_endog</span><span class="s3">, </span><span class="s1">centered_endog)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">uncentered_tss(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Uncentered sum of squares. 
 
        The sum of the squared values of the (whitened) endogenous response 
        variable. 
        &quot;&quot;&quot;</span>
        <span class="s1">wendog = self.model.wendog</span>
        <span class="s3">return </span><span class="s1">np.dot(wendog</span><span class="s3">, </span><span class="s1">wendog)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">ess(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        The explained sum of squares. 
 
        If a constant is present, the centered total sum of squares minus the 
        sum of squared residuals. If there is no constant, the uncentered total 
        sum of squares is used. 
        &quot;&quot;&quot;</span>

        <span class="s3">if </span><span class="s1">self.k_constant:</span>
            <span class="s3">return </span><span class="s1">self.centered_tss - self.ssr</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">self.uncentered_tss - self.ssr</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">rsquared(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        R-squared of the model. 
 
        This is defined here as 1 - `ssr`/`centered_tss` if the constant is 
        included in the model and 1 - `ssr`/`uncentered_tss` if the constant is 
        omitted. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self.k_constant:</span>
            <span class="s3">return </span><span class="s5">1 </span><span class="s1">- self.ssr/self.centered_tss</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s5">1 </span><span class="s1">- self.ssr/self.uncentered_tss</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">rsquared_adj(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Adjusted R-squared. 
 
        This is defined here as 1 - (`nobs`-1)/`df_resid` * (1-`rsquared`) 
        if a constant is included and 1 - `nobs`/`df_resid` * (1-`rsquared`) if 
        no constant is included. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s5">1 </span><span class="s1">- (np.divide(self.nobs - self.k_constant</span><span class="s3">, </span><span class="s1">self.df_resid)</span>
                    <span class="s1">* (</span><span class="s5">1 </span><span class="s1">- self.rsquared))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">mse_model(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Mean squared error the model. 
 
        The explained sum of squares divided by the model degrees of freedom. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">np.all(self.df_model == </span><span class="s5">0.0</span><span class="s1">):</span>
            <span class="s3">return </span><span class="s1">np.full_like(self.ess</span><span class="s3">, </span><span class="s1">np.nan)</span>
        <span class="s3">return </span><span class="s1">self.ess/self.df_model</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">mse_resid(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Mean squared error of the residuals. 
 
        The sum of squared residuals divided by the residual degrees of 
        freedom. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">np.all(self.df_resid == </span><span class="s5">0.0</span><span class="s1">):</span>
            <span class="s3">return </span><span class="s1">np.full_like(self.ssr</span><span class="s3">, </span><span class="s1">np.nan)</span>
        <span class="s3">return </span><span class="s1">self.ssr/self.df_resid</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">mse_total(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Total mean squared error. 
 
        The uncentered total sum of squares divided by the number of 
        observations. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">np.all(self.df_resid + self.df_model == </span><span class="s5">0.0</span><span class="s1">):</span>
            <span class="s3">return </span><span class="s1">np.full_like(self.centered_tss</span><span class="s3">, </span><span class="s1">np.nan)</span>
        <span class="s3">if </span><span class="s1">self.k_constant:</span>
            <span class="s3">return </span><span class="s1">self.centered_tss / (self.df_resid + self.df_model)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">self.uncentered_tss / (self.df_resid + self.df_model)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">fvalue(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        F-statistic of the fully specified model. 
 
        Calculated as the mean squared error of the model divided by the mean 
        squared error of the residuals if the nonrobust covariance is used. 
        Otherwise computed using a Wald-like quadratic form that tests whether 
        all coefficients (excluding the constant) are zero. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'cov_type'</span><span class="s1">) </span><span class="s3">and </span><span class="s1">self.cov_type != </span><span class="s4">'nonrobust'</span><span class="s1">:</span>
            <span class="s0"># with heteroscedasticity or correlation robustness</span>
            <span class="s1">k_params = self.normalized_cov_params.shape[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">mat = np.eye(k_params)</span>
            <span class="s1">const_idx = self.model.data.const_idx</span>
            <span class="s0"># TODO: What if model includes implicit constant, e.g. all</span>
            <span class="s0">#       dummies but no constant regressor?</span>
            <span class="s0"># TODO: Restats as LM test by projecting orthogonalizing</span>
            <span class="s0">#       to constant?</span>
            <span class="s3">if </span><span class="s1">self.model.data.k_constant == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s0"># if constant is implicit, return nan see #2444</span>
                <span class="s3">if </span><span class="s1">const_idx </span><span class="s3">is None</span><span class="s1">:</span>
                    <span class="s3">return </span><span class="s1">np.nan</span>

                <span class="s1">idx = lrange(k_params)</span>
                <span class="s1">idx.pop(const_idx)</span>
                <span class="s1">mat = mat[idx]  </span><span class="s0"># remove constant</span>
                <span class="s3">if </span><span class="s1">mat.size == </span><span class="s5">0</span><span class="s1">:  </span><span class="s0"># see  #3642</span>
                    <span class="s3">return </span><span class="s1">np.nan</span>
            <span class="s1">ft = self.f_test(mat)</span>
            <span class="s0"># using backdoor to set another attribute that we already have</span>
            <span class="s1">self._cache[</span><span class="s4">'f_pvalue'</span><span class="s1">] = float(ft.pvalue)</span>
            <span class="s3">return </span><span class="s1">float(ft.fvalue)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s0"># for standard homoscedastic case</span>
            <span class="s3">return </span><span class="s1">self.mse_model/self.mse_resid</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">f_pvalue(self):</span>
        <span class="s2">&quot;&quot;&quot;The p-value of the F-statistic.&quot;&quot;&quot;</span>
        <span class="s0"># Special case for df_model 0</span>
        <span class="s3">if </span><span class="s1">self.df_model == </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">np.full_like(self.fvalue</span><span class="s3">, </span><span class="s1">np.nan)</span>
        <span class="s3">return </span><span class="s1">stats.f.sf(self.fvalue</span><span class="s3">, </span><span class="s1">self.df_model</span><span class="s3">, </span><span class="s1">self.df_resid)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">bse(self):</span>
        <span class="s2">&quot;&quot;&quot;The standard errors of the parameter estimates.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.sqrt(np.diag(self.cov_params()))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">aic(self):</span>
        <span class="s2">r&quot;&quot;&quot; 
        Akaike's information criteria. 
 
        For a model with a constant :math:`-2llf + 2(df\_model + 1)`. For a 
        model without a constant :math:`-2llf + 2(df\_model)`. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.info_criteria(</span><span class="s4">&quot;aic&quot;</span><span class="s1">)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">bic(self):</span>
        <span class="s2">r&quot;&quot;&quot; 
        Bayes' information criteria. 
 
        For a model with a constant :math:`-2llf + \log(n)(df\_model+1)`. 
        For a model without a constant :math:`-2llf + \log(n)(df\_model)`. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.info_criteria(</span><span class="s4">&quot;bic&quot;</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">info_criteria(self</span><span class="s3">, </span><span class="s1">crit</span><span class="s3">, </span><span class="s1">dk_params=</span><span class="s5">0</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;Return an information criterion for the model. 
 
        Parameters 
        ---------- 
        crit : string 
            One of 'aic', 'bic', 'aicc' or 'hqic'. 
        dk_params : int or float 
            Correction to the number of parameters used in the information 
            criterion. By default, only mean parameters are included, the 
            scale parameter is not included in the parameter count. 
            Use ``dk_params=1`` to include scale in the parameter count. 
 
        Returns 
        ------- 
        Value of information criterion. 
 
        References 
        ---------- 
        Burnham KP, Anderson KR (2002). Model Selection and Multimodel 
        Inference; Springer New York. 
        &quot;&quot;&quot;</span>
        <span class="s1">crit = crit.lower()</span>
        <span class="s1">k_params = self.df_model + self.k_constant + dk_params</span>

        <span class="s3">if </span><span class="s1">crit == </span><span class="s4">&quot;aic&quot;</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">-</span><span class="s5">2 </span><span class="s1">* self.llf + </span><span class="s5">2 </span><span class="s1">* k_params</span>
        <span class="s3">elif </span><span class="s1">crit == </span><span class="s4">&quot;bic&quot;</span><span class="s1">:</span>
            <span class="s1">bic = -</span><span class="s5">2</span><span class="s1">*self.llf + np.log(self.nobs) * k_params</span>
            <span class="s3">return </span><span class="s1">bic</span>
        <span class="s3">elif </span><span class="s1">crit == </span><span class="s4">&quot;aicc&quot;</span><span class="s1">:</span>
            <span class="s3">from </span><span class="s1">statsmodels.tools.eval_measures </span><span class="s3">import </span><span class="s1">aicc</span>
            <span class="s3">return </span><span class="s1">aicc(self.llf</span><span class="s3">, </span><span class="s1">self.nobs</span><span class="s3">, </span><span class="s1">k_params)</span>
        <span class="s3">elif </span><span class="s1">crit == </span><span class="s4">&quot;hqic&quot;</span><span class="s1">:</span>
            <span class="s3">from </span><span class="s1">statsmodels.tools.eval_measures </span><span class="s3">import </span><span class="s1">hqic</span>
            <span class="s3">return </span><span class="s1">hqic(self.llf</span><span class="s3">, </span><span class="s1">self.nobs</span><span class="s3">, </span><span class="s1">k_params)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">eigenvals(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Return eigenvalues sorted in decreasing order. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self._wexog_singular_values </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">eigvals = self._wexog_singular_values ** </span><span class="s5">2</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">wx = self.model.wexog</span>
            <span class="s1">eigvals = np.linalg.linalg.eigvalsh(wx.T @ wx)</span>
        <span class="s3">return </span><span class="s1">np.sort(eigvals)[::-</span><span class="s5">1</span><span class="s1">]</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">condition_number(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Return condition number of exogenous matrix. 
 
        Calculated as ratio of largest to smallest singular value of the 
        exogenous variables. This value is the same as the square root of 
        the ratio of the largest to smallest eigenvalue of the inner-product 
        of the exogenous variables. 
        &quot;&quot;&quot;</span>
        <span class="s1">eigvals = self.eigenvals</span>
        <span class="s3">return </span><span class="s1">np.sqrt(eigvals[</span><span class="s5">0</span><span class="s1">]/eigvals[-</span><span class="s5">1</span><span class="s1">])</span>

    <span class="s0"># TODO: make these properties reset bse</span>
    <span class="s3">def </span><span class="s1">_HCCM(self</span><span class="s3">, </span><span class="s1">scale):</span>
        <span class="s1">H = np.dot(self.model.pinv_wexog</span><span class="s3">,</span>
                   <span class="s1">scale[:</span><span class="s3">, None</span><span class="s1">] * self.model.pinv_wexog.T)</span>
        <span class="s3">return </span><span class="s1">H</span>

    <span class="s3">def </span><span class="s1">_abat_diagonal(self</span><span class="s3">, </span><span class="s1">a</span><span class="s3">, </span><span class="s1">b):</span>
        <span class="s0"># equivalent to np.diag(a @ b @ a.T)</span>
        <span class="s3">return </span><span class="s1">np.einsum(</span><span class="s4">'ij,ik,kj-&gt;i'</span><span class="s3">, </span><span class="s1">a</span><span class="s3">, </span><span class="s1">a</span><span class="s3">, </span><span class="s1">b)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">cov_HC0(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Heteroscedasticity robust covariance matrix. See HC0_se. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.het_scale = self.wresid**</span><span class="s5">2</span>
        <span class="s1">cov_HC0 = self._HCCM(self.het_scale)</span>
        <span class="s3">return </span><span class="s1">cov_HC0</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">cov_HC1(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Heteroscedasticity robust covariance matrix. See HC1_se. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.het_scale = self.nobs/(self.df_resid)*(self.wresid**</span><span class="s5">2</span><span class="s1">)</span>
        <span class="s1">cov_HC1 = self._HCCM(self.het_scale)</span>
        <span class="s3">return </span><span class="s1">cov_HC1</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">cov_HC2(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Heteroscedasticity robust covariance matrix. See HC2_se. 
        &quot;&quot;&quot;</span>
        <span class="s1">wexog = self.model.wexog</span>
        <span class="s1">h = self._abat_diagonal(wexog</span><span class="s3">, </span><span class="s1">self.normalized_cov_params)</span>
        <span class="s1">self.het_scale = self.wresid**</span><span class="s5">2</span><span class="s1">/(</span><span class="s5">1</span><span class="s1">-h)</span>
        <span class="s1">cov_HC2 = self._HCCM(self.het_scale)</span>
        <span class="s3">return </span><span class="s1">cov_HC2</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">cov_HC3(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Heteroscedasticity robust covariance matrix. See HC3_se. 
        &quot;&quot;&quot;</span>
        <span class="s1">wexog = self.model.wexog</span>
        <span class="s1">h = self._abat_diagonal(wexog</span><span class="s3">, </span><span class="s1">self.normalized_cov_params)</span>
        <span class="s1">self.het_scale = (self.wresid / (</span><span class="s5">1 </span><span class="s1">- h))**</span><span class="s5">2</span>
        <span class="s1">cov_HC3 = self._HCCM(self.het_scale)</span>
        <span class="s3">return </span><span class="s1">cov_HC3</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">HC0_se(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        White's (1980) heteroskedasticity robust standard errors. 
 
        Notes 
        ----- 
        Defined as sqrt(diag(X.T X)^(-1)X.T diag(e_i^(2)) X(X.T X)^(-1) 
        where e_i = resid[i]. 
 
        When HC0_se or cov_HC0 is called the RegressionResults instance will 
        then have another attribute `het_scale`, which is in this case is just 
        resid**2. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.sqrt(np.diag(self.cov_HC0))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">HC1_se(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        MacKinnon and White's (1985) heteroskedasticity robust standard errors. 
 
        Notes 
        ----- 
        Defined as sqrt(diag(n/(n-p)*HC_0). 
 
        When HC1_se or cov_HC1 is called the RegressionResults instance will 
        then have another attribute `het_scale`, which is in this case is 
        n/(n-p)*resid**2. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.sqrt(np.diag(self.cov_HC1))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">HC2_se(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        MacKinnon and White's (1985) heteroskedasticity robust standard errors. 
 
        Notes 
        ----- 
        Defined as (X.T X)^(-1)X.T diag(e_i^(2)/(1-h_ii)) X(X.T X)^(-1) 
        where h_ii = x_i(X.T X)^(-1)x_i.T 
 
        When HC2_se or cov_HC2 is called the RegressionResults instance will 
        then have another attribute `het_scale`, which is in this case is 
        resid^(2)/(1-h_ii). 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.sqrt(np.diag(self.cov_HC2))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">HC3_se(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        MacKinnon and White's (1985) heteroskedasticity robust standard errors. 
 
        Notes 
        ----- 
        Defined as (X.T X)^(-1)X.T diag(e_i^(2)/(1-h_ii)^(2)) X(X.T X)^(-1) 
        where h_ii = x_i(X.T X)^(-1)x_i.T. 
 
        When HC3_se or cov_HC3 is called the RegressionResults instance will 
        then have another attribute `het_scale`, which is in this case is 
        resid^(2)/(1-h_ii)^(2). 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.sqrt(np.diag(self.cov_HC3))</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid_pearson(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Residuals, normalized to have unit variance. 
 
        Returns 
        ------- 
        array_like 
            The array `wresid` normalized by the sqrt of the scale to have 
            unit variance. 
        &quot;&quot;&quot;</span>

        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'resid'</span><span class="s1">):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'Method requires residuals.'</span><span class="s1">)</span>
        <span class="s1">eps = np.finfo(self.wresid.dtype).eps</span>
        <span class="s3">if </span><span class="s1">np.sqrt(self.scale) &lt; </span><span class="s5">10 </span><span class="s1">* eps * self.model.endog.mean():</span>
            <span class="s0"># do not divide if scale is zero close to numerical precision</span>
            <span class="s1">warnings.warn(</span>
                <span class="s4">&quot;All residuals are 0, cannot compute normed residuals.&quot;</span><span class="s3">,</span>
                <span class="s1">RuntimeWarning</span>
            <span class="s1">)</span>
            <span class="s3">return </span><span class="s1">self.wresid</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">self.wresid / np.sqrt(self.scale)</span>

    <span class="s3">def </span><span class="s1">_is_nested(self</span><span class="s3">, </span><span class="s1">restricted):</span>
        <span class="s2">&quot;&quot;&quot; 
        Parameters 
        ---------- 
        restricted : Result instance 
            The restricted model is assumed to be nested in the current 
            model. The result instance of the restricted model is required to 
            have two attributes, residual sum of squares, `ssr`, residual 
            degrees of freedom, `df_resid`. 
 
        Returns 
        ------- 
        nested : bool 
            True if nested, otherwise false 
 
        Notes 
        ----- 
        A most nests another model if the regressors in the smaller 
        model are spanned by the regressors in the larger model and 
        the regressand is identical. 
        &quot;&quot;&quot;</span>

        <span class="s3">if </span><span class="s1">self.model.nobs != restricted.model.nobs:</span>
            <span class="s3">return False</span>

        <span class="s1">full_rank = self.model.rank</span>
        <span class="s1">restricted_rank = restricted.model.rank</span>
        <span class="s3">if </span><span class="s1">full_rank &lt;= restricted_rank:</span>
            <span class="s3">return False</span>

        <span class="s1">restricted_exog = restricted.model.wexog</span>
        <span class="s1">full_wresid = self.wresid</span>

        <span class="s1">scores = restricted_exog * full_wresid[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s1">score_l2 = np.sqrt(np.mean(scores.mean(</span><span class="s5">0</span><span class="s1">) ** </span><span class="s5">2</span><span class="s1">))</span>
        <span class="s0"># TODO: Could be improved, and may fail depending on scale of</span>
        <span class="s0"># regressors</span>
        <span class="s3">return </span><span class="s1">np.allclose(score_l2</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">compare_lm_test(self</span><span class="s3">, </span><span class="s1">restricted</span><span class="s3">, </span><span class="s1">demean=</span><span class="s3">True, </span><span class="s1">use_lr=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Use Lagrange Multiplier test to test a set of linear restrictions. 
 
        Parameters 
        ---------- 
        restricted : Result instance 
            The restricted model is assumed to be nested in the 
            current model. The result instance of the restricted model 
            is required to have two attributes, residual sum of 
            squares, `ssr`, residual degrees of freedom, `df_resid`. 
        demean : bool 
            Flag indicating whether the demean the scores based on the 
            residuals from the restricted model.  If True, the covariance of 
            the scores are used and the LM test is identical to the large 
            sample version of the LR test. 
        use_lr : bool 
            A flag indicating whether to estimate the covariance of the model 
            scores using the unrestricted model. Setting the to True improves 
            the power of the test. 
 
        Returns 
        ------- 
        lm_value : float 
            The test statistic which has a chi2 distributed. 
        p_value : float 
            The p-value of the test statistic. 
        df_diff : int 
            The degrees of freedom of the restriction, i.e. difference in df 
            between models. 
 
        Notes 
        ----- 
        The LM test examines whether the scores from the restricted model are 
        0. If the null is true, and the restrictions are valid, then the 
        parameters of the restricted model should be close to the minimum of 
        the sum of squared errors, and so the scores should be close to zero, 
        on average. 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">numpy.linalg </span><span class="s3">import </span><span class="s1">inv</span>

        <span class="s3">import </span><span class="s1">statsmodels.stats.sandwich_covariance </span><span class="s3">as </span><span class="s1">sw</span>

        <span class="s3">if not </span><span class="s1">self._is_nested(restricted):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Restricted model is not nested by full model.&quot;</span><span class="s1">)</span>

        <span class="s1">wresid = restricted.wresid</span>
        <span class="s1">wexog = self.model.wexog</span>
        <span class="s1">scores = wexog * wresid[:</span><span class="s3">, None</span><span class="s1">]</span>

        <span class="s1">n = self.nobs</span>
        <span class="s1">df_full = self.df_resid</span>
        <span class="s1">df_restr = restricted.df_resid</span>
        <span class="s1">df_diff = (df_restr - df_full)</span>

        <span class="s1">s = scores.mean(axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">use_lr:</span>
            <span class="s1">scores = wexog * self.wresid[:</span><span class="s3">, None</span><span class="s1">]</span>
            <span class="s1">demean = </span><span class="s3">False</span>

        <span class="s3">if </span><span class="s1">demean:</span>
            <span class="s1">scores = scores - scores.mean(</span><span class="s5">0</span><span class="s1">)[</span><span class="s3">None, </span><span class="s1">:]</span>
        <span class="s0"># Form matters here.  If homoskedastics can be sigma^2 (X'X)^-1</span>
        <span class="s0"># If Heteroskedastic then the form below is fine</span>
        <span class="s0"># If HAC then need to use HAC</span>
        <span class="s0"># If Cluster, should use cluster</span>

        <span class="s1">cov_type = getattr(self</span><span class="s3">, </span><span class="s4">'cov_type'</span><span class="s3">, </span><span class="s4">'nonrobust'</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">cov_type == </span><span class="s4">'nonrobust'</span><span class="s1">:</span>
            <span class="s1">sigma2 = np.mean(wresid**</span><span class="s5">2</span><span class="s1">)</span>
            <span class="s1">xpx = np.dot(wexog.T</span><span class="s3">, </span><span class="s1">wexog) / n</span>
            <span class="s1">s_inv = inv(sigma2 * xpx)</span>
        <span class="s3">elif </span><span class="s1">cov_type </span><span class="s3">in </span><span class="s1">(</span><span class="s4">'HC0'</span><span class="s3">, </span><span class="s4">'HC1'</span><span class="s3">, </span><span class="s4">'HC2'</span><span class="s3">, </span><span class="s4">'HC3'</span><span class="s1">):</span>
            <span class="s1">s_inv = inv(np.dot(scores.T</span><span class="s3">, </span><span class="s1">scores) / n)</span>
        <span class="s3">elif </span><span class="s1">cov_type == </span><span class="s4">'HAC'</span><span class="s1">:</span>
            <span class="s1">maxlags = self.cov_kwds[</span><span class="s4">'maxlags'</span><span class="s1">]</span>
            <span class="s1">s_inv = inv(sw.S_hac_simple(scores</span><span class="s3">, </span><span class="s1">maxlags) / n)</span>
        <span class="s3">elif </span><span class="s1">cov_type == </span><span class="s4">'cluster'</span><span class="s1">:</span>
            <span class="s0"># cluster robust standard errors</span>
            <span class="s1">groups = self.cov_kwds[</span><span class="s4">'groups'</span><span class="s1">]</span>
            <span class="s0"># TODO: Might need demean option in S_crosssection by group?</span>
            <span class="s1">s_inv = inv(sw.S_crosssection(scores</span><span class="s3">, </span><span class="s1">groups))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'Only nonrobust, HC, HAC and cluster are ' </span><span class="s1">+</span>
                             <span class="s4">'currently connected'</span><span class="s1">)</span>

        <span class="s1">lm_value = n * (s @ s_inv @ s.T)</span>
        <span class="s1">p_value = stats.chi2.sf(lm_value</span><span class="s3">, </span><span class="s1">df_diff)</span>
        <span class="s3">return </span><span class="s1">lm_value</span><span class="s3">, </span><span class="s1">p_value</span><span class="s3">, </span><span class="s1">df_diff</span>

    <span class="s3">def </span><span class="s1">compare_f_test(self</span><span class="s3">, </span><span class="s1">restricted):</span>
        <span class="s2">&quot;&quot;&quot; 
        Use F test to test whether restricted model is correct. 
 
        Parameters 
        ---------- 
        restricted : Result instance 
            The restricted model is assumed to be nested in the 
            current model. The result instance of the restricted model 
            is required to have two attributes, residual sum of 
            squares, `ssr`, residual degrees of freedom, `df_resid`. 
 
        Returns 
        ------- 
        f_value : float 
            The test statistic which has an F distribution. 
        p_value : float 
            The p-value of the test statistic. 
        df_diff : int 
            The degrees of freedom of the restriction, i.e. difference in 
            df between models. 
 
        Notes 
        ----- 
        See mailing list discussion October 17, 
 
        This test compares the residual sum of squares of the two 
        models.  This is not a valid test, if there is unspecified 
        heteroscedasticity or correlation. This method will issue a 
        warning if this is detected but still return the results under 
        the assumption of homoscedasticity and no autocorrelation 
        (sphericity). 
        &quot;&quot;&quot;</span>

        <span class="s1">has_robust1 = getattr(self</span><span class="s3">, </span><span class="s4">'cov_type'</span><span class="s3">, </span><span class="s4">'nonrobust'</span><span class="s1">) != </span><span class="s4">'nonrobust'</span>
        <span class="s1">has_robust2 = (getattr(restricted</span><span class="s3">, </span><span class="s4">'cov_type'</span><span class="s3">, </span><span class="s4">'nonrobust'</span><span class="s1">) !=</span>
                       <span class="s4">'nonrobust'</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">has_robust1 </span><span class="s3">or </span><span class="s1">has_robust2:</span>
            <span class="s1">warnings.warn(</span><span class="s4">'F test for comparison is likely invalid with ' </span><span class="s1">+</span>
                          <span class="s4">'robust covariance, proceeding anyway'</span><span class="s3">,</span>
                          <span class="s1">InvalidTestWarning)</span>

        <span class="s1">ssr_full = self.ssr</span>
        <span class="s1">ssr_restr = restricted.ssr</span>
        <span class="s1">df_full = self.df_resid</span>
        <span class="s1">df_restr = restricted.df_resid</span>

        <span class="s1">df_diff = (df_restr - df_full)</span>
        <span class="s1">f_value = (ssr_restr - ssr_full) / df_diff / ssr_full * df_full</span>
        <span class="s1">p_value = stats.f.sf(f_value</span><span class="s3">, </span><span class="s1">df_diff</span><span class="s3">, </span><span class="s1">df_full)</span>
        <span class="s3">return </span><span class="s1">f_value</span><span class="s3">, </span><span class="s1">p_value</span><span class="s3">, </span><span class="s1">df_diff</span>

    <span class="s3">def </span><span class="s1">compare_lr_test(self</span><span class="s3">, </span><span class="s1">restricted</span><span class="s3">, </span><span class="s1">large_sample=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Likelihood ratio test to test whether restricted model is correct. 
 
        Parameters 
        ---------- 
        restricted : Result instance 
            The restricted model is assumed to be nested in the current model. 
            The result instance of the restricted model is required to have two 
            attributes, residual sum of squares, `ssr`, residual degrees of 
            freedom, `df_resid`. 
 
        large_sample : bool 
            Flag indicating whether to use a heteroskedasticity robust version 
            of the LR test, which is a modified LM test. 
 
        Returns 
        ------- 
        lr_stat : float 
            The likelihood ratio which is chisquare distributed with df_diff 
            degrees of freedom. 
        p_value : float 
            The p-value of the test statistic. 
        df_diff : int 
            The degrees of freedom of the restriction, i.e. difference in df 
            between models. 
 
        Notes 
        ----- 
        The exact likelihood ratio is valid for homoskedastic data, 
        and is defined as 
 
        .. math:: D=-2\\log\\left(\\frac{\\mathcal{L}_{null}} 
           {\\mathcal{L}_{alternative}}\\right) 
 
        where :math:`\\mathcal{L}` is the likelihood of the 
        model. With :math:`D` distributed as chisquare with df equal 
        to difference in number of parameters or equivalently 
        difference in residual degrees of freedom. 
 
        The large sample version of the likelihood ratio is defined as 
 
        .. math:: D=n s^{\\prime}S^{-1}s 
 
        where :math:`s=n^{-1}\\sum_{i=1}^{n} s_{i}` 
 
        .. math:: s_{i} = x_{i,alternative} \\epsilon_{i,null} 
 
        is the average score of the model evaluated using the 
        residuals from null model and the regressors from the 
        alternative model and :math:`S` is the covariance of the 
        scores, :math:`s_{i}`.  The covariance of the scores is 
        estimated using the same estimator as in the alternative 
        model. 
 
        This test compares the loglikelihood of the two models.  This 
        may not be a valid test, if there is unspecified 
        heteroscedasticity or correlation. This method will issue a 
        warning if this is detected but still return the results 
        without taking unspecified heteroscedasticity or correlation 
        into account. 
 
        This test compares the loglikelihood of the two models.  This 
        may not be a valid test, if there is unspecified 
        heteroscedasticity or correlation. This method will issue a 
        warning if this is detected but still return the results 
        without taking unspecified heteroscedasticity or correlation 
        into account. 
 
        is the average score of the model evaluated using the 
        residuals from null model and the regressors from the 
        alternative model and :math:`S` is the covariance of the 
        scores, :math:`s_{i}`.  The covariance of the scores is 
        estimated using the same estimator as in the alternative 
        model. 
        &quot;&quot;&quot;</span>
        <span class="s0"># TODO: put into separate function, needs tests</span>

        <span class="s0"># See mailing list discussion October 17,</span>

        <span class="s3">if </span><span class="s1">large_sample:</span>
            <span class="s3">return </span><span class="s1">self.compare_lm_test(restricted</span><span class="s3">, </span><span class="s1">use_lr=</span><span class="s3">True</span><span class="s1">)</span>

        <span class="s1">has_robust1 = (getattr(self</span><span class="s3">, </span><span class="s4">'cov_type'</span><span class="s3">, </span><span class="s4">'nonrobust'</span><span class="s1">) != </span><span class="s4">'nonrobust'</span><span class="s1">)</span>
        <span class="s1">has_robust2 = (</span>
            <span class="s1">getattr(restricted</span><span class="s3">, </span><span class="s4">'cov_type'</span><span class="s3">, </span><span class="s4">'nonrobust'</span><span class="s1">) != </span><span class="s4">'nonrobust'</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">has_robust1 </span><span class="s3">or </span><span class="s1">has_robust2:</span>
            <span class="s1">warnings.warn(</span><span class="s4">'Likelihood Ratio test is likely invalid with ' </span><span class="s1">+</span>
                          <span class="s4">'robust covariance, proceeding anyway'</span><span class="s3">,</span>
                          <span class="s1">InvalidTestWarning)</span>

        <span class="s1">llf_full = self.llf</span>
        <span class="s1">llf_restr = restricted.llf</span>
        <span class="s1">df_full = self.df_resid</span>
        <span class="s1">df_restr = restricted.df_resid</span>

        <span class="s1">lrdf = (df_restr - df_full)</span>
        <span class="s1">lrstat = -</span><span class="s5">2</span><span class="s1">*(llf_restr - llf_full)</span>
        <span class="s1">lr_pvalue = stats.chi2.sf(lrstat</span><span class="s3">, </span><span class="s1">lrdf)</span>

        <span class="s3">return </span><span class="s1">lrstat</span><span class="s3">, </span><span class="s1">lr_pvalue</span><span class="s3">, </span><span class="s1">lrdf</span>

    <span class="s3">def </span><span class="s1">get_robustcov_results(self</span><span class="s3">, </span><span class="s1">cov_type=</span><span class="s4">'HC1'</span><span class="s3">, </span><span class="s1">use_t=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Create new results instance with robust covariance as default. 
 
        Parameters 
        ---------- 
        cov_type : str 
            The type of robust sandwich estimator to use. See Notes below. 
        use_t : bool 
            If true, then the t distribution is used for inference. 
            If false, then the normal distribution is used. 
            If `use_t` is None, then an appropriate default is used, which is 
            `True` if the cov_type is nonrobust, and `False` in all other 
            cases. 
        **kwargs 
            Required or optional arguments for robust covariance calculation. 
            See Notes below. 
 
        Returns 
        ------- 
        RegressionResults 
            This method creates a new results instance with the 
            requested robust covariance as the default covariance of 
            the parameters.  Inferential statistics like p-values and 
            hypothesis tests will be based on this covariance matrix. 
 
        Notes 
        ----- 
        The following covariance types and required or optional arguments are 
        currently available: 
 
        - 'fixed scale' uses a predefined scale 
 
          ``scale``: float, optional 
            Argument to set the scale. Default is 1. 
 
        - 'HC0', 'HC1', 'HC2', 'HC3': heteroscedasticity robust covariance 
 
          - no keyword arguments 
 
        - 'HAC': heteroskedasticity-autocorrelation robust covariance 
 
          ``maxlags`` :  integer, required 
            number of lags to use 
 
          ``kernel`` : {callable, str}, optional 
            kernels currently available kernels are ['bartlett', 'uniform'], 
            default is Bartlett 
 
          ``use_correction``: bool, optional 
            If true, use small sample correction 
 
        - 'cluster': clustered covariance estimator 
 
          ``groups`` : array_like[int], required : 
            Integer-valued index of clusters or groups. 
 
          ``use_correction``: bool, optional 
            If True the sandwich covariance is calculated with a small 
            sample correction. 
            If False the sandwich covariance is calculated without 
            small sample correction. 
 
          ``df_correction``: bool, optional 
            If True (default), then the degrees of freedom for the 
            inferential statistics and hypothesis tests, such as 
            pvalues, f_pvalue, conf_int, and t_test and f_test, are 
            based on the number of groups minus one instead of the 
            total number of observations minus the number of explanatory 
            variables. `df_resid` of the results instance is also 
            adjusted. When `use_t` is also True, then pvalues are 
            computed using the Student's t distribution using the 
            corrected values. These may differ substantially from 
            p-values based on the normal is the number of groups is 
            small. 
            If False, then `df_resid` of the results instance is not 
            adjusted. 
 
        - 'hac-groupsum': Driscoll and Kraay, heteroscedasticity and 
          autocorrelation robust covariance for panel data 
          # TODO: more options needed here 
 
          ``time`` : array_like, required 
            index of time periods 
          ``maxlags`` : integer, required 
            number of lags to use 
          ``kernel`` : {callable, str}, optional 
            The available kernels are ['bartlett', 'uniform']. The default is 
            Bartlett. 
          ``use_correction`` : {False, 'hac', 'cluster'}, optional 
            If False the the sandwich covariance is calculated without small 
            sample correction. If `use_correction = 'cluster'` (default), 
            then the same small sample correction as in the case of 
            `covtype='cluster'` is used. 
          ``df_correction`` : bool, optional 
            The adjustment to df_resid, see cov_type 'cluster' above 
 
        - 'hac-panel': heteroscedasticity and autocorrelation robust standard 
          errors in panel data. The data needs to be sorted in this case, the 
          time series for each panel unit or cluster need to be stacked. The 
          membership to a time series of an individual or group can be either 
          specified by group indicators or by increasing time periods. One of 
          ``groups`` or ``time`` is required. # TODO: we need more options here 
 
          ``groups`` : array_like[int] 
            indicator for groups 
          ``time`` : array_like[int] 
            index of time periods 
          ``maxlags`` : int, required 
            number of lags to use 
          ``kernel`` : {callable, str}, optional 
            Available kernels are ['bartlett', 'uniform'], default 
            is Bartlett 
          ``use_correction`` : {False, 'hac', 'cluster'}, optional 
            If False the sandwich covariance is calculated without 
            small sample correction. 
          ``df_correction`` : bool, optional 
            Adjustment to df_resid, see cov_type 'cluster' above 
 
        **Reminder**: ``use_correction`` in &quot;hac-groupsum&quot; and &quot;hac-panel&quot; is 
        not bool, needs to be in {False, 'hac', 'cluster'}. 
 
        .. todo:: Currently there is no check for extra or misspelled keywords, 
             except in the case of cov_type `HCx` 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.base.covtype </span><span class="s3">import </span><span class="s1">descriptions</span><span class="s3">, </span><span class="s1">normalize_cov_type</span>
        <span class="s3">import </span><span class="s1">statsmodels.stats.sandwich_covariance </span><span class="s3">as </span><span class="s1">sw</span>

        <span class="s1">cov_type = normalize_cov_type(cov_type)</span>

        <span class="s3">if </span><span class="s4">'kernel' </span><span class="s3">in </span><span class="s1">kwargs:</span>
            <span class="s1">kwargs[</span><span class="s4">'weights_func'</span><span class="s1">] = kwargs.pop(</span><span class="s4">'kernel'</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s4">'weights_func' </span><span class="s3">in </span><span class="s1">kwargs </span><span class="s3">and not </span><span class="s1">callable(kwargs[</span><span class="s4">'weights_func'</span><span class="s1">]):</span>
            <span class="s1">kwargs[</span><span class="s4">'weights_func'</span><span class="s1">] = sw.kernel_dict[kwargs[</span><span class="s4">'weights_func'</span><span class="s1">]]</span>

        <span class="s0"># TODO: make separate function that returns a robust cov plus info</span>
        <span class="s1">use_self = kwargs.pop(</span><span class="s4">'use_self'</span><span class="s3">, False</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">use_self:</span>
            <span class="s1">res = self</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">res = self.__class__(</span>
                <span class="s1">self.model</span><span class="s3">, </span><span class="s1">self.params</span><span class="s3">,</span>
                <span class="s1">normalized_cov_params=self.normalized_cov_params</span><span class="s3">,</span>
                <span class="s1">scale=self.scale)</span>

        <span class="s1">res.cov_type = cov_type</span>
        <span class="s0"># use_t might already be defined by the class, and already set</span>
        <span class="s3">if </span><span class="s1">use_t </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">use_t = self.use_t</span>
        <span class="s1">res.cov_kwds = {</span><span class="s4">'use_t'</span><span class="s1">: use_t}  </span><span class="s0"># store for information</span>
        <span class="s1">res.use_t = use_t</span>

        <span class="s1">adjust_df = </span><span class="s3">False</span>
        <span class="s3">if </span><span class="s1">cov_type </span><span class="s3">in </span><span class="s1">[</span><span class="s4">'cluster'</span><span class="s3">, </span><span class="s4">'hac-panel'</span><span class="s3">, </span><span class="s4">'hac-groupsum'</span><span class="s1">]:</span>
            <span class="s1">df_correction = kwargs.get(</span><span class="s4">'df_correction'</span><span class="s3">, None</span><span class="s1">)</span>
            <span class="s0"># TODO: check also use_correction, do I need all combinations?</span>
            <span class="s3">if </span><span class="s1">df_correction </span><span class="s3">is not False</span><span class="s1">:  </span><span class="s0"># i.e. in [None, True]:</span>
                <span class="s0"># user did not explicitely set it to False</span>
                <span class="s1">adjust_df = </span><span class="s3">True</span>

        <span class="s1">res.cov_kwds[</span><span class="s4">'adjust_df'</span><span class="s1">] = adjust_df</span>

        <span class="s0"># verify and set kwargs, and calculate cov</span>
        <span class="s0"># TODO: this should be outsourced in a function so we can reuse it in</span>
        <span class="s0">#       other models</span>
        <span class="s0"># TODO: make it DRYer   repeated code for checking kwargs</span>
        <span class="s3">if </span><span class="s1">cov_type </span><span class="s3">in </span><span class="s1">[</span><span class="s4">'fixed scale'</span><span class="s3">, </span><span class="s4">'fixed_scale'</span><span class="s1">]:</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'description'</span><span class="s1">] = descriptions[</span><span class="s4">'fixed_scale'</span><span class="s1">]</span>

            <span class="s1">res.cov_kwds[</span><span class="s4">'scale'</span><span class="s1">] = scale = kwargs.get(</span><span class="s4">'scale'</span><span class="s3">, </span><span class="s5">1.</span><span class="s1">)</span>
            <span class="s1">res.cov_params_default = scale * res.normalized_cov_params</span>
        <span class="s3">elif </span><span class="s1">cov_type.upper() </span><span class="s3">in </span><span class="s1">(</span><span class="s4">'HC0'</span><span class="s3">, </span><span class="s4">'HC1'</span><span class="s3">, </span><span class="s4">'HC2'</span><span class="s3">, </span><span class="s4">'HC3'</span><span class="s1">):</span>
            <span class="s3">if </span><span class="s1">kwargs:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'heteroscedasticity robust covariance '</span>
                                 <span class="s4">'does not use keywords'</span><span class="s1">)</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'description'</span><span class="s1">] = descriptions[cov_type.upper()]</span>
            <span class="s1">res.cov_params_default = getattr(self</span><span class="s3">, </span><span class="s4">'cov_' </span><span class="s1">+ cov_type.upper())</span>
        <span class="s3">elif </span><span class="s1">cov_type.lower() == </span><span class="s4">'hac'</span><span class="s1">:</span>
            <span class="s0"># TODO: check if required, default in cov_hac_simple</span>
            <span class="s1">maxlags = kwargs[</span><span class="s4">'maxlags'</span><span class="s1">]</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'maxlags'</span><span class="s1">] = maxlags</span>
            <span class="s1">weights_func = kwargs.get(</span><span class="s4">'weights_func'</span><span class="s3">, </span><span class="s1">sw.weights_bartlett)</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'weights_func'</span><span class="s1">] = weights_func</span>
            <span class="s1">use_correction = kwargs.get(</span><span class="s4">'use_correction'</span><span class="s3">, False</span><span class="s1">)</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'use_correction'</span><span class="s1">] = use_correction</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'description'</span><span class="s1">] = descriptions[</span><span class="s4">'HAC'</span><span class="s1">].format(</span>
                <span class="s1">maxlags=maxlags</span><span class="s3">,</span>
                <span class="s1">correction=[</span><span class="s4">'without'</span><span class="s3">, </span><span class="s4">'with'</span><span class="s1">][use_correction])</span>

            <span class="s1">res.cov_params_default = sw.cov_hac_simple(</span>
                <span class="s1">self</span><span class="s3">, </span><span class="s1">nlags=maxlags</span><span class="s3">, </span><span class="s1">weights_func=weights_func</span><span class="s3">,</span>
                <span class="s1">use_correction=use_correction)</span>
        <span class="s3">elif </span><span class="s1">cov_type.lower() == </span><span class="s4">'cluster'</span><span class="s1">:</span>
            <span class="s0"># cluster robust standard errors, one- or two-way</span>
            <span class="s1">groups = kwargs[</span><span class="s4">'groups'</span><span class="s1">]</span>
            <span class="s3">if not </span><span class="s1">hasattr(groups</span><span class="s3">, </span><span class="s4">'shape'</span><span class="s1">):</span>
                <span class="s1">groups = np.asarray(groups).T</span>

            <span class="s3">if </span><span class="s1">groups.ndim &gt;= </span><span class="s5">2</span><span class="s1">:</span>
                <span class="s1">groups = groups.squeeze()</span>

            <span class="s1">res.cov_kwds[</span><span class="s4">'groups'</span><span class="s1">] = groups</span>
            <span class="s1">use_correction = kwargs.get(</span><span class="s4">'use_correction'</span><span class="s3">, True</span><span class="s1">)</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'use_correction'</span><span class="s1">] = use_correction</span>
            <span class="s3">if </span><span class="s1">groups.ndim == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s3">if </span><span class="s1">adjust_df:</span>
                    <span class="s0"># need to find number of groups</span>
                    <span class="s0"># duplicate work</span>
                    <span class="s1">self.n_groups = n_groups = len(np.unique(groups))</span>
                <span class="s1">res.cov_params_default = sw.cov_cluster(</span>
                    <span class="s1">self</span><span class="s3">, </span><span class="s1">groups</span><span class="s3">, </span><span class="s1">use_correction=use_correction)</span>

            <span class="s3">elif </span><span class="s1">groups.ndim == </span><span class="s5">2</span><span class="s1">:</span>
                <span class="s3">if </span><span class="s1">hasattr(groups</span><span class="s3">, </span><span class="s4">'values'</span><span class="s1">):</span>
                    <span class="s1">groups = groups.values</span>

                <span class="s3">if </span><span class="s1">adjust_df:</span>
                    <span class="s0"># need to find number of groups</span>
                    <span class="s0"># duplicate work</span>
                    <span class="s1">n_groups0 = len(np.unique(groups[:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]))</span>
                    <span class="s1">n_groups1 = len(np.unique(groups[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]))</span>
                    <span class="s1">self.n_groups = (n_groups0</span><span class="s3">, </span><span class="s1">n_groups1)</span>
                    <span class="s1">n_groups = min(n_groups0</span><span class="s3">, </span><span class="s1">n_groups1)  </span><span class="s0"># use for adjust_df</span>

                <span class="s0"># Note: sw.cov_cluster_2groups has 3 returns</span>
                <span class="s1">res.cov_params_default = sw.cov_cluster_2groups(</span>
                    <span class="s1">self</span><span class="s3">, </span><span class="s1">groups</span><span class="s3">, </span><span class="s1">use_correction=use_correction)[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'only two groups are supported'</span><span class="s1">)</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'description'</span><span class="s1">] = descriptions[</span><span class="s4">'cluster'</span><span class="s1">]</span>

        <span class="s3">elif </span><span class="s1">cov_type.lower() == </span><span class="s4">'hac-panel'</span><span class="s1">:</span>
            <span class="s0"># cluster robust standard errors</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'time'</span><span class="s1">] = time = kwargs.get(</span><span class="s4">'time'</span><span class="s3">, None</span><span class="s1">)</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'groups'</span><span class="s1">] = groups = kwargs.get(</span><span class="s4">'groups'</span><span class="s3">, None</span><span class="s1">)</span>
            <span class="s0"># TODO: nlags is currently required</span>
            <span class="s0"># nlags = kwargs.get('nlags', True)</span>
            <span class="s0"># res.cov_kwds['nlags'] = nlags</span>
            <span class="s0"># TODO: `nlags` or `maxlags`</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'maxlags'</span><span class="s1">] = maxlags = kwargs[</span><span class="s4">'maxlags'</span><span class="s1">]</span>
            <span class="s1">use_correction = kwargs.get(</span><span class="s4">'use_correction'</span><span class="s3">, </span><span class="s4">'hac'</span><span class="s1">)</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'use_correction'</span><span class="s1">] = use_correction</span>
            <span class="s1">weights_func = kwargs.get(</span><span class="s4">'weights_func'</span><span class="s3">, </span><span class="s1">sw.weights_bartlett)</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'weights_func'</span><span class="s1">] = weights_func</span>
            <span class="s3">if </span><span class="s1">groups </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">groups = np.asarray(groups)</span>
                <span class="s1">tt = (np.nonzero(groups[:-</span><span class="s5">1</span><span class="s1">] != groups[</span><span class="s5">1</span><span class="s1">:])[</span><span class="s5">0</span><span class="s1">] + </span><span class="s5">1</span><span class="s1">).tolist()</span>
                <span class="s1">nobs_ = len(groups)</span>
            <span class="s3">elif </span><span class="s1">time </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">time = np.asarray(time)</span>
                <span class="s0"># TODO: clumsy time index in cov_nw_panel</span>
                <span class="s1">tt = (np.nonzero(time[</span><span class="s5">1</span><span class="s1">:] &lt; time[:-</span><span class="s5">1</span><span class="s1">])[</span><span class="s5">0</span><span class="s1">] + </span><span class="s5">1</span><span class="s1">).tolist()</span>
                <span class="s1">nobs_ = len(time)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'either time or groups needs to be given'</span><span class="s1">)</span>
            <span class="s1">groupidx = lzip([</span><span class="s5">0</span><span class="s1">] + tt</span><span class="s3">, </span><span class="s1">tt + [nobs_])</span>
            <span class="s1">self.n_groups = n_groups = len(groupidx)</span>
            <span class="s1">res.cov_params_default = sw.cov_nw_panel(</span>
                <span class="s1">self</span><span class="s3">,</span>
                <span class="s1">maxlags</span><span class="s3">,</span>
                <span class="s1">groupidx</span><span class="s3">,</span>
                <span class="s1">weights_func=weights_func</span><span class="s3">,</span>
                <span class="s1">use_correction=use_correction</span>
            <span class="s1">)</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'description'</span><span class="s1">] = descriptions[</span><span class="s4">'HAC-Panel'</span><span class="s1">]</span>

        <span class="s3">elif </span><span class="s1">cov_type.lower() == </span><span class="s4">'hac-groupsum'</span><span class="s1">:</span>
            <span class="s0"># Driscoll-Kraay standard errors</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'time'</span><span class="s1">] = time = kwargs[</span><span class="s4">'time'</span><span class="s1">]</span>
            <span class="s0"># TODO: nlags is currently required</span>
            <span class="s0"># nlags = kwargs.get('nlags', True)</span>
            <span class="s0"># res.cov_kwds['nlags'] = nlags</span>
            <span class="s0"># TODO: `nlags` or `maxlags`</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'maxlags'</span><span class="s1">] = maxlags = kwargs[</span><span class="s4">'maxlags'</span><span class="s1">]</span>
            <span class="s1">use_correction = kwargs.get(</span><span class="s4">'use_correction'</span><span class="s3">, </span><span class="s4">'cluster'</span><span class="s1">)</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'use_correction'</span><span class="s1">] = use_correction</span>
            <span class="s1">weights_func = kwargs.get(</span><span class="s4">'weights_func'</span><span class="s3">, </span><span class="s1">sw.weights_bartlett)</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'weights_func'</span><span class="s1">] = weights_func</span>
            <span class="s3">if </span><span class="s1">adjust_df:</span>
                <span class="s0"># need to find number of groups</span>
                <span class="s1">tt = (np.nonzero(time[</span><span class="s5">1</span><span class="s1">:] &lt; time[:-</span><span class="s5">1</span><span class="s1">])[</span><span class="s5">0</span><span class="s1">] + </span><span class="s5">1</span><span class="s1">)</span>
                <span class="s1">self.n_groups = n_groups = len(tt) + </span><span class="s5">1</span>
            <span class="s1">res.cov_params_default = sw.cov_nw_groupsum(</span>
                <span class="s1">self</span><span class="s3">, </span><span class="s1">maxlags</span><span class="s3">, </span><span class="s1">time</span><span class="s3">, </span><span class="s1">weights_func=weights_func</span><span class="s3">,</span>
                <span class="s1">use_correction=use_correction)</span>
            <span class="s1">res.cov_kwds[</span><span class="s4">'description'</span><span class="s1">] = descriptions[</span><span class="s4">'HAC-Groupsum'</span><span class="s1">]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'cov_type not recognized. See docstring for ' </span><span class="s1">+</span>
                             <span class="s4">'available options and spelling'</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">adjust_df:</span>
            <span class="s0"># Note: df_resid is used for scale and others, add new attribute</span>
            <span class="s1">res.df_resid_inference = n_groups - </span><span class="s5">1</span>

        <span class="s3">return </span><span class="s1">res</span>

    <span class="s1">@Appender(pred.get_prediction.__doc__)</span>
    <span class="s3">def </span><span class="s1">get_prediction(self</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">transform=</span><span class="s3">True, </span><span class="s1">weights=</span><span class="s3">None,</span>
                       <span class="s1">row_labels=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>

        <span class="s3">return </span><span class="s1">pred.get_prediction(</span>
            <span class="s1">self</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">transform=transform</span><span class="s3">, </span><span class="s1">weights=weights</span><span class="s3">,</span>
            <span class="s1">row_labels=row_labels</span><span class="s3">, </span><span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">summary(</span>
            <span class="s1">self</span><span class="s3">,</span>
            <span class="s1">yname: str | </span><span class="s3">None </span><span class="s1">= </span><span class="s3">None,</span>
            <span class="s1">xname: Sequence[str] | </span><span class="s3">None </span><span class="s1">= </span><span class="s3">None,</span>
            <span class="s1">title: str | </span><span class="s3">None </span><span class="s1">= </span><span class="s3">None,</span>
            <span class="s1">alpha: float = </span><span class="s5">0.05</span><span class="s3">,</span>
            <span class="s1">slim: bool = </span><span class="s3">False,</span>
    <span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Summarize the Regression Results. 
 
        Parameters 
        ---------- 
        yname : str, optional 
            Name of endogenous (response) variable. The Default is `y`. 
        xname : list[str], optional 
            Names for the exogenous variables. Default is `var_##` for ## in 
            the number of regressors. Must match the number of parameters 
            in the model. 
        title : str, optional 
            Title for the top table. If not None, then this replaces the 
            default title. 
        alpha : float, optional 
            The significance level for the confidence intervals. 
        slim : bool, optional 
            Flag indicating to produce reduced set or diagnostic information. 
            Default is False. 
 
        Returns 
        ------- 
        Summary 
            Instance holding the summary tables and text, which can be printed 
            or converted to various output formats. 
 
        See Also 
        -------- 
        statsmodels.iolib.summary.Summary : A class that holds summary results. 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.stats.stattools </span><span class="s3">import </span><span class="s1">(</span>
            <span class="s1">durbin_watson</span><span class="s3">,</span>
            <span class="s1">jarque_bera</span><span class="s3">,</span>
            <span class="s1">omni_normtest</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">alpha = float_like(alpha</span><span class="s3">, </span><span class="s4">&quot;alpha&quot;</span><span class="s3">, </span><span class="s1">optional=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">slim = bool_like(slim</span><span class="s3">, </span><span class="s4">&quot;slim&quot;</span><span class="s3">, </span><span class="s1">optional=</span><span class="s3">False, </span><span class="s1">strict=</span><span class="s3">True</span><span class="s1">)</span>

        <span class="s1">jb</span><span class="s3">, </span><span class="s1">jbpv</span><span class="s3">, </span><span class="s1">skew</span><span class="s3">, </span><span class="s1">kurtosis = jarque_bera(self.wresid)</span>
        <span class="s1">omni</span><span class="s3">, </span><span class="s1">omnipv = omni_normtest(self.wresid)</span>

        <span class="s1">eigvals = self.eigenvals</span>
        <span class="s1">condno = self.condition_number</span>

        <span class="s0"># TODO: Avoid adding attributes in non-__init__</span>
        <span class="s1">self.diagn = dict(jb=jb</span><span class="s3">, </span><span class="s1">jbpv=jbpv</span><span class="s3">, </span><span class="s1">skew=skew</span><span class="s3">, </span><span class="s1">kurtosis=kurtosis</span><span class="s3">,</span>
                          <span class="s1">omni=omni</span><span class="s3">, </span><span class="s1">omnipv=omnipv</span><span class="s3">, </span><span class="s1">condno=condno</span><span class="s3">,</span>
                          <span class="s1">mineigval=eigvals[-</span><span class="s5">1</span><span class="s1">])</span>

        <span class="s0"># TODO not used yet</span>
        <span class="s0"># diagn_left_header = ['Models stats']</span>
        <span class="s0"># diagn_right_header = ['Residual stats']</span>

        <span class="s0"># TODO: requiring list/iterable is a bit annoying</span>
        <span class="s0">#   need more control over formatting</span>
        <span class="s0"># TODO: default do not work if it's not identically spelled</span>

        <span class="s1">top_left = [(</span><span class="s4">'Dep. Variable:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">(</span><span class="s4">'Model:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">(</span><span class="s4">'Method:'</span><span class="s3">, </span><span class="s1">[</span><span class="s4">'Least Squares'</span><span class="s1">])</span><span class="s3">,</span>
                    <span class="s1">(</span><span class="s4">'Date:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">(</span><span class="s4">'Time:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">(</span><span class="s4">'No. Observations:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">(</span><span class="s4">'Df Residuals:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">(</span><span class="s4">'Df Model:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">]</span>

        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'cov_type'</span><span class="s1">):</span>
            <span class="s1">top_left.append((</span><span class="s4">'Covariance Type:'</span><span class="s3">, </span><span class="s1">[self.cov_type]))</span>

        <span class="s1">rsquared_type = </span><span class="s4">'' </span><span class="s3">if </span><span class="s1">self.k_constant </span><span class="s3">else </span><span class="s4">' (uncentered)'</span>
        <span class="s1">top_right = [(</span><span class="s4">'R-squared' </span><span class="s1">+ rsquared_type + </span><span class="s4">':'</span><span class="s3">,</span>
                      <span class="s1">[</span><span class="s4">&quot;%#8.3f&quot; </span><span class="s1">% self.rsquared])</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s4">'Adj. R-squared' </span><span class="s1">+ rsquared_type + </span><span class="s4">':'</span><span class="s3">,</span>
                      <span class="s1">[</span><span class="s4">&quot;%#8.3f&quot; </span><span class="s1">% self.rsquared_adj])</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s4">'F-statistic:'</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;%#8.4g&quot; </span><span class="s1">% self.fvalue])</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s4">'Prob (F-statistic):'</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;%#6.3g&quot; </span><span class="s1">% self.f_pvalue])</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s4">'Log-Likelihood:'</span><span class="s3">, None</span><span class="s1">)</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s4">'AIC:'</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;%#8.4g&quot; </span><span class="s1">% self.aic])</span><span class="s3">,</span>
                     <span class="s1">(</span><span class="s4">'BIC:'</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;%#8.4g&quot; </span><span class="s1">% self.bic])</span>
                     <span class="s1">]</span>

        <span class="s3">if </span><span class="s1">slim:</span>
            <span class="s1">slimlist = [</span><span class="s4">'Dep. Variable:'</span><span class="s3">, </span><span class="s4">'Model:'</span><span class="s3">, </span><span class="s4">'No. Observations:'</span><span class="s3">,</span>
                        <span class="s4">'Covariance Type:'</span><span class="s3">, </span><span class="s4">'R-squared:'</span><span class="s3">, </span><span class="s4">'Adj. R-squared:'</span><span class="s3">,</span>
                        <span class="s4">'F-statistic:'</span><span class="s3">, </span><span class="s4">'Prob (F-statistic):'</span><span class="s1">]</span>
            <span class="s1">diagn_left = diagn_right = []</span>
            <span class="s1">top_left = [elem </span><span class="s3">for </span><span class="s1">elem </span><span class="s3">in </span><span class="s1">top_left </span><span class="s3">if </span><span class="s1">elem[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">in </span><span class="s1">slimlist]</span>
            <span class="s1">top_right = [elem </span><span class="s3">for </span><span class="s1">elem </span><span class="s3">in </span><span class="s1">top_right </span><span class="s3">if </span><span class="s1">elem[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">in </span><span class="s1">slimlist]</span>
            <span class="s1">top_right = top_right + \</span>
                <span class="s1">[(</span><span class="s4">&quot;&quot;</span><span class="s3">, </span><span class="s1">[])] * (len(top_left) - len(top_right))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">diagn_left = [(</span><span class="s4">'Omnibus:'</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;%#6.3f&quot; </span><span class="s1">% omni])</span><span class="s3">,</span>
                          <span class="s1">(</span><span class="s4">'Prob(Omnibus):'</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;%#6.3f&quot; </span><span class="s1">% omnipv])</span><span class="s3">,</span>
                          <span class="s1">(</span><span class="s4">'Skew:'</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;%#6.3f&quot; </span><span class="s1">% skew])</span><span class="s3">,</span>
                          <span class="s1">(</span><span class="s4">'Kurtosis:'</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;%#6.3f&quot; </span><span class="s1">% kurtosis])</span>
                          <span class="s1">]</span>

            <span class="s1">diagn_right = [(</span><span class="s4">'Durbin-Watson:'</span><span class="s3">,</span>
                            <span class="s1">[</span><span class="s4">&quot;%#8.3f&quot; </span><span class="s1">% durbin_watson(self.wresid)]</span>
                            <span class="s1">)</span><span class="s3">,</span>
                           <span class="s1">(</span><span class="s4">'Jarque-Bera (JB):'</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;%#8.3f&quot; </span><span class="s1">% jb])</span><span class="s3">,</span>
                           <span class="s1">(</span><span class="s4">'Prob(JB):'</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;%#8.3g&quot; </span><span class="s1">% jbpv])</span><span class="s3">,</span>
                           <span class="s1">(</span><span class="s4">'Cond. No.'</span><span class="s3">, </span><span class="s1">[</span><span class="s4">&quot;%#8.3g&quot; </span><span class="s1">% condno])</span>
                           <span class="s1">]</span>

        <span class="s3">if </span><span class="s1">title </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">title = self.model.__class__.__name__ + </span><span class="s4">' ' </span><span class="s1">+ </span><span class="s4">&quot;Regression Results&quot;</span>

        <span class="s0"># create summary table instance</span>
        <span class="s3">from </span><span class="s1">statsmodels.iolib.summary </span><span class="s3">import </span><span class="s1">Summary</span>
        <span class="s1">smry = Summary()</span>
        <span class="s1">smry.add_table_2cols(self</span><span class="s3">, </span><span class="s1">gleft=top_left</span><span class="s3">, </span><span class="s1">gright=top_right</span><span class="s3">,</span>
                             <span class="s1">yname=yname</span><span class="s3">, </span><span class="s1">xname=xname</span><span class="s3">, </span><span class="s1">title=title)</span>
        <span class="s1">smry.add_table_params(self</span><span class="s3">, </span><span class="s1">yname=yname</span><span class="s3">, </span><span class="s1">xname=xname</span><span class="s3">, </span><span class="s1">alpha=alpha</span><span class="s3">,</span>
                              <span class="s1">use_t=self.use_t)</span>
        <span class="s3">if not </span><span class="s1">slim:</span>
            <span class="s1">smry.add_table_2cols(self</span><span class="s3">, </span><span class="s1">gleft=diagn_left</span><span class="s3">, </span><span class="s1">gright=diagn_right</span><span class="s3">,</span>
                                 <span class="s1">yname=yname</span><span class="s3">, </span><span class="s1">xname=xname</span><span class="s3">,</span>
                                 <span class="s1">title=</span><span class="s4">&quot;&quot;</span><span class="s1">)</span>

        <span class="s0"># add warnings/notes, added to text format only</span>
        <span class="s1">etext = []</span>
        <span class="s3">if not </span><span class="s1">self.k_constant:</span>
            <span class="s1">etext.append(</span>
                <span class="s4">&quot;R² is computed without centering (uncentered) since the &quot;</span>
                <span class="s4">&quot;model does not contain a constant.&quot;</span>
            <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'cov_type'</span><span class="s1">):</span>
            <span class="s1">etext.append(self.cov_kwds[</span><span class="s4">'description'</span><span class="s1">])</span>
        <span class="s3">if </span><span class="s1">self.model.exog.shape[</span><span class="s5">0</span><span class="s1">] &lt; self.model.exog.shape[</span><span class="s5">1</span><span class="s1">]:</span>
            <span class="s1">wstr = </span><span class="s4">&quot;The input rank is higher than the number of observations.&quot;</span>
            <span class="s1">etext.append(wstr)</span>
        <span class="s3">if </span><span class="s1">eigvals[-</span><span class="s5">1</span><span class="s1">] &lt; </span><span class="s5">1e-10</span><span class="s1">:</span>
            <span class="s1">wstr = </span><span class="s4">&quot;The smallest eigenvalue is %6.3g. This might indicate &quot;</span>
            <span class="s1">wstr += </span><span class="s4">&quot;that there are</span><span class="s3">\n</span><span class="s4">&quot;</span>
            <span class="s1">wstr += </span><span class="s4">&quot;strong multicollinearity problems or that the design &quot;</span>
            <span class="s1">wstr += </span><span class="s4">&quot;matrix is singular.&quot;</span>
            <span class="s1">wstr = wstr % eigvals[-</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s1">etext.append(wstr)</span>
        <span class="s3">elif </span><span class="s1">condno &gt; </span><span class="s5">1000</span><span class="s1">:  </span><span class="s0"># TODO: what is recommended?</span>
            <span class="s1">wstr = </span><span class="s4">&quot;The condition number is large, %6.3g. This might &quot;</span>
            <span class="s1">wstr += </span><span class="s4">&quot;indicate that there are</span><span class="s3">\n</span><span class="s4">&quot;</span>
            <span class="s1">wstr += </span><span class="s4">&quot;strong multicollinearity or other numerical &quot;</span>
            <span class="s1">wstr += </span><span class="s4">&quot;problems.&quot;</span>
            <span class="s1">wstr = wstr % condno</span>
            <span class="s1">etext.append(wstr)</span>

        <span class="s3">if </span><span class="s1">etext:</span>
            <span class="s1">etext = [</span><span class="s4">&quot;[{0}] {1}&quot;</span><span class="s1">.format(i + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">text)</span>
                     <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">text </span><span class="s3">in </span><span class="s1">enumerate(etext)]</span>
            <span class="s1">etext.insert(</span><span class="s5">0</span><span class="s3">, </span><span class="s4">&quot;Notes:&quot;</span><span class="s1">)</span>
            <span class="s1">smry.add_extra_txt(etext)</span>

        <span class="s3">return </span><span class="s1">smry</span>

    <span class="s3">def </span><span class="s1">summary2(</span>
            <span class="s1">self</span><span class="s3">,</span>
            <span class="s1">yname: str | </span><span class="s3">None </span><span class="s1">= </span><span class="s3">None,</span>
            <span class="s1">xname: Sequence[str] | </span><span class="s3">None </span><span class="s1">= </span><span class="s3">None,</span>
            <span class="s1">title: str | </span><span class="s3">None </span><span class="s1">= </span><span class="s3">None,</span>
            <span class="s1">alpha: float = </span><span class="s5">0.05</span><span class="s3">,</span>
            <span class="s1">float_format: str = </span><span class="s4">&quot;%.4f&quot;</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Experimental summary function to summarize the regression results. 
 
        Parameters 
        ---------- 
        yname : str 
            The name of the dependent variable (optional). 
        xname : list[str], optional 
            Names for the exogenous variables. Default is `var_##` for ## in 
            the number of regressors. Must match the number of parameters 
            in the model. 
        title : str, optional 
            Title for the top table. If not None, then this replaces the 
            default title. 
        alpha : float 
            The significance level for the confidence intervals. 
        float_format : str 
            The format for floats in parameters summary. 
 
        Returns 
        ------- 
        Summary 
            Instance holding the summary tables and text, which can be printed 
            or converted to various output formats. 
 
        See Also 
        -------- 
        statsmodels.iolib.summary2.Summary 
            A class that holds summary results. 
        &quot;&quot;&quot;</span>
        <span class="s0"># Diagnostics</span>
        <span class="s3">from </span><span class="s1">statsmodels.stats.stattools </span><span class="s3">import </span><span class="s1">(</span>
            <span class="s1">durbin_watson</span><span class="s3">,</span>
            <span class="s1">jarque_bera</span><span class="s3">,</span>
            <span class="s1">omni_normtest</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s1">jb</span><span class="s3">, </span><span class="s1">jbpv</span><span class="s3">, </span><span class="s1">skew</span><span class="s3">, </span><span class="s1">kurtosis = jarque_bera(self.wresid)</span>
        <span class="s1">omni</span><span class="s3">, </span><span class="s1">omnipv = omni_normtest(self.wresid)</span>
        <span class="s1">dw = durbin_watson(self.wresid)</span>
        <span class="s1">eigvals = self.eigenvals</span>
        <span class="s1">condno = self.condition_number</span>
        <span class="s1">diagnostic = dict([</span>
            <span class="s1">(</span><span class="s4">'Omnibus:'</span><span class="s3">,  </span><span class="s4">&quot;%.3f&quot; </span><span class="s1">% omni)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s4">'Prob(Omnibus):'</span><span class="s3">, </span><span class="s4">&quot;%.3f&quot; </span><span class="s1">% omnipv)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s4">'Skew:'</span><span class="s3">, </span><span class="s4">&quot;%.3f&quot; </span><span class="s1">% skew)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s4">'Kurtosis:'</span><span class="s3">, </span><span class="s4">&quot;%.3f&quot; </span><span class="s1">% kurtosis)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s4">'Durbin-Watson:'</span><span class="s3">, </span><span class="s4">&quot;%.3f&quot; </span><span class="s1">% dw)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s4">'Jarque-Bera (JB):'</span><span class="s3">, </span><span class="s4">&quot;%.3f&quot; </span><span class="s1">% jb)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s4">'Prob(JB):'</span><span class="s3">, </span><span class="s4">&quot;%.3f&quot; </span><span class="s1">% jbpv)</span><span class="s3">,</span>
            <span class="s1">(</span><span class="s4">'Condition No.:'</span><span class="s3">, </span><span class="s4">&quot;%.0f&quot; </span><span class="s1">% condno)</span>
            <span class="s1">])</span>

        <span class="s0"># Summary</span>
        <span class="s3">from </span><span class="s1">statsmodels.iolib </span><span class="s3">import </span><span class="s1">summary2</span>
        <span class="s1">smry = summary2.Summary()</span>
        <span class="s1">smry.add_base(results=self</span><span class="s3">, </span><span class="s1">alpha=alpha</span><span class="s3">, </span><span class="s1">float_format=float_format</span><span class="s3">,</span>
                      <span class="s1">xname=xname</span><span class="s3">, </span><span class="s1">yname=yname</span><span class="s3">, </span><span class="s1">title=title)</span>
        <span class="s1">smry.add_dict(diagnostic)</span>

        <span class="s1">etext = []</span>

        <span class="s3">if not </span><span class="s1">self.k_constant:</span>
            <span class="s1">etext.append(</span>
                <span class="s4">&quot;R² is computed without centering (uncentered) since the </span><span class="s3">\ 
                </span><span class="s4">model does not contain a constant.&quot;</span>
            <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'cov_type'</span><span class="s1">):</span>
            <span class="s1">etext.append(self.cov_kwds[</span><span class="s4">'description'</span><span class="s1">])</span>
        <span class="s3">if </span><span class="s1">self.model.exog.shape[</span><span class="s5">0</span><span class="s1">] &lt; self.model.exog.shape[</span><span class="s5">1</span><span class="s1">]:</span>
            <span class="s1">wstr = </span><span class="s4">&quot;The input rank is higher than the number of observations.&quot;</span>
            <span class="s1">etext.append(wstr)</span>

        <span class="s0"># Warnings</span>
        <span class="s3">if </span><span class="s1">eigvals[-</span><span class="s5">1</span><span class="s1">] &lt; </span><span class="s5">1e-10</span><span class="s1">:</span>
            <span class="s1">warn = </span><span class="s4">&quot;The smallest eigenvalue is %6.3g. This might indicate that</span><span class="s3">\ 
                </span><span class="s4">there are strong multicollinearity problems or that the design</span><span class="s3">\ 
                </span><span class="s4">matrix is singular.&quot; </span><span class="s1">% eigvals[-</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s1">etext.append(warn)</span>
        <span class="s3">elif </span><span class="s1">condno &gt; </span><span class="s5">1000</span><span class="s1">:</span>
            <span class="s1">warn = </span><span class="s4">&quot;The condition number is large, %6.3g. This might indicate</span><span class="s3">\ 
                </span><span class="s4">that there are strong multicollinearity or other numerical</span><span class="s3">\ 
                </span><span class="s4">problems.&quot; </span><span class="s1">% condno</span>
            <span class="s1">etext.append(warn)</span>

        <span class="s3">if </span><span class="s1">etext:</span>
            <span class="s1">etext = [</span><span class="s4">&quot;[{0}] {1}&quot;</span><span class="s1">.format(i + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">text)</span>
                     <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">text </span><span class="s3">in </span><span class="s1">enumerate(etext)]</span>
            <span class="s1">etext.insert(</span><span class="s5">0</span><span class="s3">, </span><span class="s4">&quot;Notes:&quot;</span><span class="s1">)</span>

        <span class="s3">for </span><span class="s1">line </span><span class="s3">in </span><span class="s1">etext:</span>
            <span class="s1">smry.add_text(line)</span>

        <span class="s3">return </span><span class="s1">smry</span>


<span class="s3">class </span><span class="s1">OLSResults(RegressionResults):</span>
    <span class="s2">&quot;&quot;&quot; 
    Results class for for an OLS model. 
 
    Parameters 
    ---------- 
    model : RegressionModel 
        The regression model instance. 
    params : ndarray 
        The estimated parameters. 
    normalized_cov_params : ndarray 
        The normalized covariance parameters. 
    scale : float 
        The estimated scale of the residuals. 
    cov_type : str 
        The covariance estimator used in the results. 
    cov_kwds : dict 
        Additional keywords used in the covariance specification. 
    use_t : bool 
        Flag indicating to use the Student's t in inference. 
    **kwargs 
        Additional keyword arguments used to initialize the results. 
 
    See Also 
    -------- 
    RegressionResults 
        Results store for WLS and GLW models. 
 
    Notes 
    ----- 
    Most of the methods and attributes are inherited from RegressionResults. 
    The special methods that are only available for OLS are: 
 
    - get_influence 
    - outlier_test 
    - el_test 
    - conf_int_el 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">get_influence(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Calculate influence and outlier measures. 
 
        Returns 
        ------- 
        OLSInfluence 
            The instance containing methods to calculate the main influence and 
            outlier measures for the OLS regression. 
 
        See Also 
        -------- 
        statsmodels.stats.outliers_influence.OLSInfluence 
            A class that exposes methods to examine observation influence. 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.stats.outliers_influence </span><span class="s3">import </span><span class="s1">OLSInfluence</span>
        <span class="s3">return </span><span class="s1">OLSInfluence(self)</span>

    <span class="s3">def </span><span class="s1">outlier_test(self</span><span class="s3">, </span><span class="s1">method=</span><span class="s4">'bonf'</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s5">.05</span><span class="s3">, </span><span class="s1">labels=</span><span class="s3">None,</span>
                     <span class="s1">order=</span><span class="s3">False, </span><span class="s1">cutoff=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Test observations for outliers according to method. 
 
        Parameters 
        ---------- 
        method : str 
            The method to use in the outlier test.  Must be one of: 
 
            - `bonferroni` : one-step correction 
            - `sidak` : one-step correction 
            - `holm-sidak` : 
            - `holm` : 
            - `simes-hochberg` : 
            - `hommel` : 
            - `fdr_bh` : Benjamini/Hochberg 
            - `fdr_by` : Benjamini/Yekutieli 
 
            See `statsmodels.stats.multitest.multipletests` for details. 
        alpha : float 
            The familywise error rate (FWER). 
        labels : None or array_like 
            If `labels` is not None, then it will be used as index to the 
            returned pandas DataFrame. See also Returns below. 
        order : bool 
            Whether or not to order the results by the absolute value of the 
            studentized residuals. If labels are provided they will also be 
            sorted. 
        cutoff : None or float in [0, 1] 
            If cutoff is not None, then the return only includes observations 
            with multiple testing corrected p-values strictly below the cutoff. 
            The returned array or dataframe can be empty if t. 
 
        Returns 
        ------- 
        array_like 
            Returns either an ndarray or a DataFrame if labels is not None. 
            Will attempt to get labels from model_results if available. The 
            columns are the Studentized residuals, the unadjusted p-value, 
            and the corrected p-value according to method. 
 
        Notes 
        ----- 
        The unadjusted p-value is stats.t.sf(abs(resid), df) where 
        df = df_resid - 1. 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.stats.outliers_influence </span><span class="s3">import </span><span class="s1">outlier_test</span>
        <span class="s3">return </span><span class="s1">outlier_test(self</span><span class="s3">, </span><span class="s1">method</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">labels=labels</span><span class="s3">,</span>
                            <span class="s1">order=order</span><span class="s3">, </span><span class="s1">cutoff=cutoff)</span>

    <span class="s3">def </span><span class="s1">el_test(self</span><span class="s3">, </span><span class="s1">b0_vals</span><span class="s3">, </span><span class="s1">param_nums</span><span class="s3">, </span><span class="s1">return_weights=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">ret_params=</span><span class="s5">0</span><span class="s3">,</span>
                <span class="s1">method=</span><span class="s4">'nm'</span><span class="s3">, </span><span class="s1">stochastic_exog=</span><span class="s5">1</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Test single or joint hypotheses using Empirical Likelihood. 
 
        Parameters 
        ---------- 
        b0_vals : 1darray 
            The hypothesized value of the parameter to be tested. 
        param_nums : 1darray 
            The parameter number to be tested. 
        return_weights : bool 
            If true, returns the weights that optimize the likelihood 
            ratio at b0_vals. The default is False. 
        ret_params : bool 
            If true, returns the parameter vector that maximizes the likelihood 
            ratio at b0_vals.  Also returns the weights.  The default is False. 
        method : str 
            Can either be 'nm' for Nelder-Mead or 'powell' for Powell.  The 
            optimization method that optimizes over nuisance parameters. 
            The default is 'nm'. 
        stochastic_exog : bool 
            When True, the exogenous variables are assumed to be stochastic. 
            When the regressors are nonstochastic, moment conditions are 
            placed on the exogenous variables.  Confidence intervals for 
            stochastic regressors are at least as large as non-stochastic 
            regressors. The default is True. 
 
        Returns 
        ------- 
        tuple 
            The p-value and -2 times the log-likelihood ratio for the 
            hypothesized values. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; import statsmodels.api as sm 
        &gt;&gt;&gt; data = sm.datasets.stackloss.load() 
        &gt;&gt;&gt; endog = data.endog 
        &gt;&gt;&gt; exog = sm.add_constant(data.exog) 
        &gt;&gt;&gt; model = sm.OLS(endog, exog) 
        &gt;&gt;&gt; fitted = model.fit() 
        &gt;&gt;&gt; fitted.params 
        &gt;&gt;&gt; array([-39.91967442,   0.7156402 ,   1.29528612,  -0.15212252]) 
        &gt;&gt;&gt; fitted.rsquared 
        &gt;&gt;&gt; 0.91357690446068196 
        &gt;&gt;&gt; # Test that the slope on the first variable is 0 
        &gt;&gt;&gt; fitted.el_test([0], [1]) 
        &gt;&gt;&gt; (27.248146353888796, 1.7894660442330235e-07) 
        &quot;&quot;&quot;</span>
        <span class="s1">params = np.copy(self.params)</span>
        <span class="s1">opt_fun_inst = _ELRegOpts()  </span><span class="s0"># to store weights</span>
        <span class="s3">if </span><span class="s1">len(param_nums) == len(params):</span>
            <span class="s1">llr = opt_fun_inst._opt_nuis_regress(</span>
                <span class="s1">[]</span><span class="s3">,</span>
                <span class="s1">param_nums=param_nums</span><span class="s3">,</span>
                <span class="s1">endog=self.model.endog</span><span class="s3">,</span>
                <span class="s1">exog=self.model.exog</span><span class="s3">,</span>
                <span class="s1">nobs=self.model.nobs</span><span class="s3">,</span>
                <span class="s1">nvar=self.model.exog.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">params=params</span><span class="s3">,</span>
                <span class="s1">b0_vals=b0_vals</span><span class="s3">,</span>
                <span class="s1">stochastic_exog=stochastic_exog)</span>
            <span class="s1">pval = </span><span class="s5">1 </span><span class="s1">- stats.chi2.cdf(llr</span><span class="s3">, </span><span class="s1">len(param_nums))</span>
            <span class="s3">if </span><span class="s1">return_weights:</span>
                <span class="s3">return </span><span class="s1">llr</span><span class="s3">, </span><span class="s1">pval</span><span class="s3">, </span><span class="s1">opt_fun_inst.new_weights</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s3">return </span><span class="s1">llr</span><span class="s3">, </span><span class="s1">pval</span>
        <span class="s1">x0 = np.delete(params</span><span class="s3">, </span><span class="s1">param_nums)</span>
        <span class="s1">args = (param_nums</span><span class="s3">, </span><span class="s1">self.model.endog</span><span class="s3">, </span><span class="s1">self.model.exog</span><span class="s3">,</span>
                <span class="s1">self.model.nobs</span><span class="s3">, </span><span class="s1">self.model.exog.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">params</span><span class="s3">,</span>
                <span class="s1">b0_vals</span><span class="s3">, </span><span class="s1">stochastic_exog)</span>
        <span class="s3">if </span><span class="s1">method == </span><span class="s4">'nm'</span><span class="s1">:</span>
            <span class="s1">llr = optimize.fmin(opt_fun_inst._opt_nuis_regress</span><span class="s3">, </span><span class="s1">x0</span><span class="s3">,</span>
                                <span class="s1">maxfun=</span><span class="s5">10000</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s5">10000</span><span class="s3">, </span><span class="s1">full_output=</span><span class="s5">1</span><span class="s3">,</span>
                                <span class="s1">disp=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">args=args)[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">if </span><span class="s1">method == </span><span class="s4">'powell'</span><span class="s1">:</span>
            <span class="s1">llr = optimize.fmin_powell(opt_fun_inst._opt_nuis_regress</span><span class="s3">, </span><span class="s1">x0</span><span class="s3">,</span>
                                       <span class="s1">full_output=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">0</span><span class="s3">,</span>
                                       <span class="s1">args=args)[</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">pval = </span><span class="s5">1 </span><span class="s1">- stats.chi2.cdf(llr</span><span class="s3">, </span><span class="s1">len(param_nums))</span>
        <span class="s3">if </span><span class="s1">ret_params:</span>
            <span class="s3">return </span><span class="s1">llr</span><span class="s3">, </span><span class="s1">pval</span><span class="s3">, </span><span class="s1">opt_fun_inst.new_weights</span><span class="s3">, </span><span class="s1">opt_fun_inst.new_params</span>
        <span class="s3">elif </span><span class="s1">return_weights:</span>
            <span class="s3">return </span><span class="s1">llr</span><span class="s3">, </span><span class="s1">pval</span><span class="s3">, </span><span class="s1">opt_fun_inst.new_weights</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">llr</span><span class="s3">, </span><span class="s1">pval</span>

    <span class="s3">def </span><span class="s1">conf_int_el(self</span><span class="s3">, </span><span class="s1">param_num</span><span class="s3">, </span><span class="s1">sig=</span><span class="s5">.05</span><span class="s3">, </span><span class="s1">upper_bound=</span><span class="s3">None,</span>
                    <span class="s1">lower_bound=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s4">'nm'</span><span class="s3">, </span><span class="s1">stochastic_exog=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Compute the confidence interval using Empirical Likelihood. 
 
        Parameters 
        ---------- 
        param_num : float 
            The parameter for which the confidence interval is desired. 
        sig : float 
            The significance level.  Default is 0.05. 
        upper_bound : float 
            The maximum value the upper limit can be.  Default is the 
            99.9% confidence value under OLS assumptions. 
        lower_bound : float 
            The minimum value the lower limit can be.  Default is the 99.9% 
            confidence value under OLS assumptions. 
        method : str 
            Can either be 'nm' for Nelder-Mead or 'powell' for Powell.  The 
            optimization method that optimizes over nuisance parameters. 
            The default is 'nm'. 
        stochastic_exog : bool 
            When True, the exogenous variables are assumed to be stochastic. 
            When the regressors are nonstochastic, moment conditions are 
            placed on the exogenous variables.  Confidence intervals for 
            stochastic regressors are at least as large as non-stochastic 
            regressors.  The default is True. 
 
        Returns 
        ------- 
        lowerl : float 
            The lower bound of the confidence interval. 
        upperl : float 
            The upper bound of the confidence interval. 
 
        See Also 
        -------- 
        el_test : Test parameters using Empirical Likelihood. 
 
        Notes 
        ----- 
        This function uses brentq to find the value of beta where 
        test_beta([beta], param_num)[1] is equal to the critical value. 
 
        The function returns the results of each iteration of brentq at each 
        value of beta. 
 
        The current function value of the last printed optimization should be 
        the critical value at the desired significance level. For alpha=.05, 
        the value is 3.841459. 
 
        To ensure optimization terminated successfully, it is suggested to do 
        el_test([lower_limit], [param_num]). 
 
        If the optimization does not terminate successfully, consider switching 
        optimization algorithms. 
 
        If optimization is still not successful, try changing the values of 
        start_int_params.  If the current function value repeatedly jumps 
        from a number between 0 and the critical value and a very large number 
        (&gt;50), the starting parameters of the interior minimization need 
        to be changed. 
        &quot;&quot;&quot;</span>
        <span class="s1">r0 = stats.chi2.ppf(</span><span class="s5">1 </span><span class="s1">- sig</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">upper_bound </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">upper_bound = self.conf_int(</span><span class="s5">.01</span><span class="s1">)[param_num][</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">if </span><span class="s1">lower_bound </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">lower_bound = self.conf_int(</span><span class="s5">.01</span><span class="s1">)[param_num][</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s3">def </span><span class="s1">f(b0):</span>
            <span class="s3">return </span><span class="s1">self.el_test(np.array([b0])</span><span class="s3">, </span><span class="s1">np.array([param_num])</span><span class="s3">,</span>
                                <span class="s1">method=method</span><span class="s3">,</span>
                                <span class="s1">stochastic_exog=stochastic_exog)[</span><span class="s5">0</span><span class="s1">] - r0</span>

        <span class="s1">lowerl = optimize.brenth(f</span><span class="s3">, </span><span class="s1">lower_bound</span><span class="s3">,</span>
                                 <span class="s1">self.params[param_num])</span>
        <span class="s1">upperl = optimize.brenth(f</span><span class="s3">, </span><span class="s1">self.params[param_num]</span><span class="s3">,</span>
                                 <span class="s1">upper_bound)</span>
        <span class="s0">#  ^ Seems to be faster than brentq in most cases</span>
        <span class="s3">return </span><span class="s1">(lowerl</span><span class="s3">, </span><span class="s1">upperl)</span>


<span class="s3">class </span><span class="s1">RegressionResultsWrapper(wrap.ResultsWrapper):</span>

    <span class="s1">_attrs = {</span>
        <span class="s4">'chisq'</span><span class="s1">: </span><span class="s4">'columns'</span><span class="s3">,</span>
        <span class="s4">'sresid'</span><span class="s1">: </span><span class="s4">'rows'</span><span class="s3">,</span>
        <span class="s4">'weights'</span><span class="s1">: </span><span class="s4">'rows'</span><span class="s3">,</span>
        <span class="s4">'wresid'</span><span class="s1">: </span><span class="s4">'rows'</span><span class="s3">,</span>
        <span class="s4">'bcov_unscaled'</span><span class="s1">: </span><span class="s4">'cov'</span><span class="s3">,</span>
        <span class="s4">'bcov_scaled'</span><span class="s1">: </span><span class="s4">'cov'</span><span class="s3">,</span>
        <span class="s4">'HC0_se'</span><span class="s1">: </span><span class="s4">'columns'</span><span class="s3">,</span>
        <span class="s4">'HC1_se'</span><span class="s1">: </span><span class="s4">'columns'</span><span class="s3">,</span>
        <span class="s4">'HC2_se'</span><span class="s1">: </span><span class="s4">'columns'</span><span class="s3">,</span>
        <span class="s4">'HC3_se'</span><span class="s1">: </span><span class="s4">'columns'</span><span class="s3">,</span>
        <span class="s4">'norm_resid'</span><span class="s1">: </span><span class="s4">'rows'</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s1">_wrap_attrs = wrap.union_dicts(base.LikelihoodResultsWrapper._attrs</span><span class="s3">,</span>
                                   <span class="s1">_attrs)</span>

    <span class="s1">_methods = {}</span>

    <span class="s1">_wrap_methods = wrap.union_dicts(</span>
                        <span class="s1">base.LikelihoodResultsWrapper._wrap_methods</span><span class="s3">,</span>
                        <span class="s1">_methods)</span>


<span class="s1">wrap.populate_wrapper(RegressionResultsWrapper</span><span class="s3">,</span>
                      <span class="s1">RegressionResults)</span>
</pre>
</body>
</html>