<html>
<head>
<title>aft_el.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
aft_el.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
 
Accelerated Failure Time (AFT) Model with empirical likelihood inference. 
 
AFT regression analysis is applicable when the researcher has access 
to a randomly right censored dependent variable, a matrix of exogenous 
variables and an indicatior variable (delta) that takes a value of 0 if the 
observation is censored and 1 otherwise. 
 
AFT References 
-------------- 
 
Stute, W. (1993). &quot;Consistent Estimation Under Random Censorship when 
Covariables are Present.&quot; Journal of Multivariate Analysis. 
Vol. 45. Iss. 1. 89-103 
 
EL and AFT References 
--------------------- 
 
Zhou, Kim And Bathke. &quot;Empirical Likelihood Analysis for the Heteroskedastic 
Accelerated Failure Time Model.&quot; Manuscript: 
URL: www.ms.uky.edu/~mai/research/CasewiseEL20080724.pdf 
 
Zhou, M. (2005). Empirical Likelihood Ratio with Arbitrarily Censored/ 
Truncated Data by EM Algorithm.  Journal of Computational and Graphical 
Statistics. 14:3, 643-656. 
 
 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">warnings</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s3">#from elregress import ElReg</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">optimize</span>
<span class="s2">from </span><span class="s1">scipy.stats </span><span class="s2">import </span><span class="s1">chi2</span>

<span class="s2">from </span><span class="s1">statsmodels.regression.linear_model </span><span class="s2">import </span><span class="s1">OLS</span><span class="s2">, </span><span class="s1">WLS</span>
<span class="s2">from </span><span class="s1">statsmodels.tools </span><span class="s2">import </span><span class="s1">add_constant</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s2">import </span><span class="s1">IterationLimitWarning</span>

<span class="s2">from </span><span class="s1">.descriptive </span><span class="s2">import </span><span class="s1">_OptFuncts</span>


<span class="s2">class </span><span class="s1">OptAFT(_OptFuncts):</span>
    <span class="s0">&quot;&quot;&quot; 
    Provides optimization functions used in estimating and conducting 
    inference in an AFT model. 
 
    Methods 
    ------ 
 
    _opt_wtd_nuis_regress: 
        Function optimized over nuisance parameters to compute 
        the profile likelihood 
 
    _EM_test: 
        Uses the modified Em algorithm of Zhou 2005 to maximize the 
        likelihood of a parameter vector. 
    &quot;&quot;&quot;</span>
    <span class="s2">def </span><span class="s1">__init__(self):</span>
        <span class="s2">pass</span>

    <span class="s2">def </span><span class="s1">_opt_wtd_nuis_regress(self</span><span class="s2">, </span><span class="s1">test_vals):</span>
        <span class="s0">&quot;&quot;&quot; 
        A function that is optimized over nuisance parameters to conduct a 
        hypothesis test for the parameters of interest 
 
        Parameters 
        ---------- 
 
        params: 1d array 
            The regression coefficients of the model.  This includes the 
            nuisance and parameters of interests. 
 
        Returns 
        ------- 
        llr : float 
            -2 times the log likelihood of the nuisance parameters and the 
            hypothesized value of the parameter(s) of interest. 
        &quot;&quot;&quot;</span>
        <span class="s1">test_params = test_vals.reshape(self.model.nvar</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">est_vect = self.model.uncens_exog * (self.model.uncens_endog -</span>
                                            <span class="s1">np.dot(self.model.uncens_exog</span><span class="s2">,</span>
                                                         <span class="s1">test_params))</span>
        <span class="s1">eta_star = self._modif_newton(np.zeros(self.model.nvar)</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">,</span>
                                         <span class="s1">self.model._fit_weights)</span>
        <span class="s1">denom = np.sum(self.model._fit_weights) + np.dot(eta_star</span><span class="s2">, </span><span class="s1">est_vect.T)</span>
        <span class="s1">self.new_weights = self.model._fit_weights / denom</span>
        <span class="s2">return </span><span class="s1">-</span><span class="s4">1 </span><span class="s1">* np.sum(np.log(self.new_weights))</span>

    <span class="s2">def </span><span class="s1">_EM_test(self</span><span class="s2">, </span><span class="s1">nuisance_params</span><span class="s2">, </span><span class="s1">params=</span><span class="s2">None, </span><span class="s1">param_nums=</span><span class="s2">None,</span>
                 <span class="s1">b0_vals=</span><span class="s2">None, </span><span class="s1">F=</span><span class="s2">None, </span><span class="s1">survidx=</span><span class="s2">None, </span><span class="s1">uncens_nobs=</span><span class="s2">None,</span>
                <span class="s1">numcensbelow=</span><span class="s2">None, </span><span class="s1">km=</span><span class="s2">None, </span><span class="s1">uncensored=</span><span class="s2">None, </span><span class="s1">censored=</span><span class="s2">None,</span>
                <span class="s1">maxiter=</span><span class="s2">None, </span><span class="s1">ftol=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Uses EM algorithm to compute the maximum likelihood of a test 
 
        Parameters 
        ---------- 
 
        nuisance_params : ndarray 
            Vector of values to be used as nuisance params. 
 
        maxiter : int 
            Number of iterations in the EM algorithm for a parameter vector 
 
        Returns 
        ------- 
        -2 ''*'' log likelihood ratio at hypothesized values and 
        nuisance params 
 
        Notes 
        ----- 
        Optional parameters are provided by the test_beta function. 
        &quot;&quot;&quot;</span>
        <span class="s1">iters = </span><span class="s4">0</span>
        <span class="s1">params[param_nums] = b0_vals</span>

        <span class="s1">nuis_param_index = np.int_(np.delete(np.arange(self.model.nvar)</span><span class="s2">,</span>
                                           <span class="s1">param_nums))</span>
        <span class="s1">params[nuis_param_index] = nuisance_params</span>
        <span class="s1">to_test = params.reshape(self.model.nvar</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">opt_res = np.inf</span>
        <span class="s1">diff = np.inf</span>
        <span class="s2">while </span><span class="s1">iters &lt; maxiter </span><span class="s2">and </span><span class="s1">diff &gt; ftol:</span>
            <span class="s1">F = F.flatten()</span>
            <span class="s1">death = np.cumsum(F[::-</span><span class="s4">1</span><span class="s1">])</span>
            <span class="s1">survivalprob = death[::-</span><span class="s4">1</span><span class="s1">]</span>
            <span class="s1">surv_point_mat = np.dot(F.reshape(-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span><span class="s2">,</span>
                                <span class="s4">1. </span><span class="s1">/ survivalprob[survidx].reshape(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">- </span><span class="s4">1</span><span class="s1">))</span>
            <span class="s1">surv_point_mat = add_constant(surv_point_mat)</span>
            <span class="s1">summed_wts = np.cumsum(surv_point_mat</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">wts = summed_wts[np.int_(np.arange(uncens_nobs))</span><span class="s2">,</span>
                             <span class="s1">numcensbelow[uncensored]]</span>
            <span class="s3"># ^E step</span>
            <span class="s3"># See Zhou 2005, section 3.</span>
            <span class="s1">self.model._fit_weights = wts</span>
            <span class="s1">new_opt_res = self._opt_wtd_nuis_regress(to_test)</span>
                <span class="s3"># ^ Uncensored weights' contribution to likelihood value.</span>
            <span class="s1">F = self.new_weights</span>
                <span class="s3"># ^ M step</span>
            <span class="s1">diff = np.abs(new_opt_res - opt_res)</span>
            <span class="s1">opt_res = new_opt_res</span>
            <span class="s1">iters = iters + </span><span class="s4">1</span>
        <span class="s1">death = np.cumsum(F.flatten()[::-</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s1">survivalprob = death[::-</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">llike = -opt_res + np.sum(np.log(survivalprob[survidx]))</span>
        <span class="s1">wtd_km = km.flatten() / np.sum(km)</span>
        <span class="s1">survivalmax = np.cumsum(wtd_km[::-</span><span class="s4">1</span><span class="s1">])[::-</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">llikemax = np.sum(np.log(wtd_km[uncensored])) + \</span>
          <span class="s1">np.sum(np.log(survivalmax[censored]))</span>
        <span class="s2">if </span><span class="s1">iters == maxiter:</span>
            <span class="s1">warnings.warn(</span><span class="s5">'The EM reached the maximum number of iterations'</span><span class="s2">,</span>
                          <span class="s1">IterationLimitWarning)</span>
        <span class="s2">return </span><span class="s1">-</span><span class="s4">2 </span><span class="s1">* (llike - llikemax)</span>

    <span class="s2">def </span><span class="s1">_ci_limits_beta(self</span><span class="s2">, </span><span class="s1">b0</span><span class="s2">, </span><span class="s1">param_num=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the difference between the log likelihood for a 
        parameter and some critical value. 
 
        Parameters 
        ---------- 
        b0: float 
            Value of a regression parameter 
        param_num : int 
            Parameter index of b0 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.test_beta([b0]</span><span class="s2">, </span><span class="s1">[param_num])[</span><span class="s4">0</span><span class="s1">] - self.r0</span>


<span class="s2">class </span><span class="s1">emplikeAFT:</span>
    <span class="s0">&quot;&quot;&quot; 
 
    Class for estimating and conducting inference in an AFT model. 
 
    Parameters 
    ---------- 
 
    endog: nx1 array 
        Response variables that are subject to random censoring 
 
    exog: nxk array 
        Matrix of covariates 
 
    censors: nx1 array 
        array with entries 0 or 1.  0 indicates a response was 
        censored. 
 
    Attributes 
    ---------- 
    nobs : float 
        Number of observations 
    endog : ndarray 
        Endog attay 
    exog : ndarray 
        Exogenous variable matrix 
    censors 
        Censors array but sets the max(endog) to uncensored 
    nvar : float 
        Number of exogenous variables 
    uncens_nobs : float 
        Number of uncensored observations 
    uncens_endog : ndarray 
        Uncensored response variables 
    uncens_exog : ndarray 
        Exogenous variables of the uncensored observations 
 
    Methods 
    ------- 
 
    params: 
        Fits model parameters 
 
    test_beta: 
        Tests if beta = b0 for any vector b0. 
 
    Notes 
    ----- 
 
    The data is immediately sorted in order of increasing endogenous 
    variables 
 
    The last observation is assumed to be uncensored which makes 
    estimation and inference possible. 
    &quot;&quot;&quot;</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">censors):</span>
        <span class="s1">self.nobs = np.shape(exog)[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">self.endog = endog.reshape(self.nobs</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">self.exog = exog.reshape(self.nobs</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">self.censors = np.asarray(censors).reshape(self.nobs</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">self.nvar = self.exog.shape[</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">idx = np.lexsort((-self.censors[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">self.endog[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]))</span>
        <span class="s1">self.endog = self.endog[idx]</span>
        <span class="s1">self.exog = self.exog[idx]</span>
        <span class="s1">self.censors = self.censors[idx]</span>
        <span class="s1">self.censors[-</span><span class="s4">1</span><span class="s1">] = </span><span class="s4">1  </span><span class="s3"># Sort in init, not in function</span>
        <span class="s1">self.uncens_nobs = int(np.sum(self.censors))</span>
        <span class="s1">mask = self.censors.ravel().astype(bool)</span>
        <span class="s1">self.uncens_endog = self.endog[mask</span><span class="s2">, </span><span class="s1">:].reshape(-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">self.uncens_exog = self.exog[mask</span><span class="s2">, </span><span class="s1">:]</span>


    <span class="s2">def </span><span class="s1">_is_tied(self</span><span class="s2">, </span><span class="s1">endog</span><span class="s2">, </span><span class="s1">censors):</span>
        <span class="s0">&quot;&quot;&quot; 
        Indicated if an observation takes the same value as the next 
        ordered observation. 
 
        Parameters 
        ---------- 
        endog : ndarray 
            Models endogenous variable 
        censors : ndarray 
            arrat indicating a censored array 
 
        Returns 
        ------- 
        indic_ties : ndarray 
            ties[i]=1 if endog[i]==endog[i+1] and 
            censors[i]=censors[i+1] 
        &quot;&quot;&quot;</span>
        <span class="s1">nobs = int(self.nobs)</span>
        <span class="s1">endog_idx = endog[np.arange(nobs - </span><span class="s4">1</span><span class="s1">)] == (</span>
            <span class="s1">endog[np.arange(nobs - </span><span class="s4">1</span><span class="s1">) + </span><span class="s4">1</span><span class="s1">])</span>
        <span class="s1">censors_idx = censors[np.arange(nobs - </span><span class="s4">1</span><span class="s1">)] == (</span>
            <span class="s1">censors[np.arange(nobs - </span><span class="s4">1</span><span class="s1">) + </span><span class="s4">1</span><span class="s1">])</span>
        <span class="s1">indic_ties = endog_idx * censors_idx  </span><span class="s3"># Both true</span>
        <span class="s2">return </span><span class="s1">np.int_(indic_ties)</span>

    <span class="s2">def </span><span class="s1">_km_w_ties(self</span><span class="s2">, </span><span class="s1">tie_indic</span><span class="s2">, </span><span class="s1">untied_km):</span>
        <span class="s0">&quot;&quot;&quot; 
        Computes KM estimator value at each observation, taking into acocunt 
        ties in the data. 
 
        Parameters 
        ---------- 
        tie_indic: 1d array 
            Indicates if the i'th observation is the same as the ith +1 
        untied_km: 1d array 
            Km estimates at each observation assuming no ties. 
        &quot;&quot;&quot;</span>
        <span class="s3"># TODO: Vectorize, even though it is only 1 pass through for any</span>
        <span class="s3"># function call</span>
        <span class="s1">num_same = </span><span class="s4">1</span>
        <span class="s1">idx_nums = []</span>
        <span class="s2">for </span><span class="s1">obs_num </span><span class="s2">in </span><span class="s1">np.arange(int(self.nobs - </span><span class="s4">1</span><span class="s1">))[::-</span><span class="s4">1</span><span class="s1">]:</span>
            <span class="s2">if </span><span class="s1">tie_indic[obs_num] == </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">idx_nums.append(obs_num)</span>
                <span class="s1">num_same = num_same + </span><span class="s4">1</span>
                <span class="s1">untied_km[obs_num] = untied_km[obs_num + </span><span class="s4">1</span><span class="s1">]</span>
            <span class="s2">elif </span><span class="s1">tie_indic[obs_num] == </span><span class="s4">0 </span><span class="s2">and </span><span class="s1">num_same &gt; </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">idx_nums.append(max(idx_nums) + </span><span class="s4">1</span><span class="s1">)</span>
                <span class="s1">idx_nums = np.asarray(idx_nums)</span>
                <span class="s1">untied_km[idx_nums] = untied_km[idx_nums]</span>
                <span class="s1">num_same = </span><span class="s4">1</span>
                <span class="s1">idx_nums = []</span>
        <span class="s2">return </span><span class="s1">untied_km.reshape(self.nobs</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">_make_km(self</span><span class="s2">, </span><span class="s1">endog</span><span class="s2">, </span><span class="s1">censors):</span>
        <span class="s0">&quot;&quot;&quot; 
 
        Computes the Kaplan-Meier estimate for the weights in the AFT model 
 
        Parameters 
        ---------- 
        endog: nx1 array 
            Array of response variables 
        censors: nx1 array 
            Censor-indicating variable 
 
        Returns 
        ------- 
        Kaplan Meier estimate for each observation 
 
        Notes 
        ----- 
 
        This function makes calls to _is_tied and km_w_ties to handle ties in 
        the data.If a censored observation and an uncensored observation has 
        the same value, it is assumed that the uncensored happened first. 
        &quot;&quot;&quot;</span>
        <span class="s1">nobs = self.nobs</span>
        <span class="s1">num = (nobs - (np.arange(nobs) + </span><span class="s4">1.</span><span class="s1">))</span>
        <span class="s1">denom = ((nobs - (np.arange(nobs) + </span><span class="s4">1.</span><span class="s1">) + </span><span class="s4">1.</span><span class="s1">))</span>
        <span class="s1">km = (num / denom).reshape(nobs</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">km = km ** np.abs(censors - </span><span class="s4">1.</span><span class="s1">)</span>
        <span class="s1">km = np.cumprod(km)  </span><span class="s3"># If no ties, this is kaplan-meier</span>
        <span class="s1">tied = self._is_tied(endog</span><span class="s2">, </span><span class="s1">censors)</span>
        <span class="s1">wtd_km = self._km_w_ties(tied</span><span class="s2">, </span><span class="s1">km)</span>
        <span class="s2">return </span><span class="s1">(censors / wtd_km).reshape(nobs</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">fit(self):</span>
        <span class="s0">&quot;&quot;&quot; 
 
        Fits an AFT model and returns results instance 
 
        Parameters 
        ---------- 
        None 
 
 
        Returns 
        ------- 
        Results instance. 
 
        Notes 
        ----- 
        To avoid dividing by zero, max(endog) is assumed to be uncensored. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">AFTResults(self)</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">endog=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">if </span><span class="s1">endog </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">endog = self.endog</span>
        <span class="s2">return </span><span class="s1">np.dot(endog</span><span class="s2">, </span><span class="s1">params)</span>


<span class="s2">class </span><span class="s1">AFTResults(OptAFT):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">model):</span>
        <span class="s1">self.model = model</span>

    <span class="s2">def </span><span class="s1">params(self):</span>
        <span class="s0">&quot;&quot;&quot; 
 
        Fits an AFT model and returns parameters. 
 
        Parameters 
        ---------- 
        None 
 
 
        Returns 
        ------- 
        Fitted params 
 
        Notes 
        ----- 
        To avoid dividing by zero, max(endog) is assumed to be uncensored. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.model.modif_censors = np.copy(self.model.censors)</span>
        <span class="s1">self.model.modif_censors[-</span><span class="s4">1</span><span class="s1">] = </span><span class="s4">1</span>
        <span class="s1">wts = self.model._make_km(self.model.endog</span><span class="s2">, </span><span class="s1">self.model.modif_censors)</span>
        <span class="s1">res = WLS(self.model.endog</span><span class="s2">, </span><span class="s1">self.model.exog</span><span class="s2">, </span><span class="s1">wts).fit()</span>
        <span class="s1">params = res.params</span>
        <span class="s2">return </span><span class="s1">params</span>

    <span class="s2">def </span><span class="s1">test_beta(self</span><span class="s2">, </span><span class="s1">b0_vals</span><span class="s2">, </span><span class="s1">param_nums</span><span class="s2">, </span><span class="s1">ftol=</span><span class="s4">10 </span><span class="s1">** - </span><span class="s4">5</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s4">30</span><span class="s2">,</span>
                  <span class="s1">print_weights=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the profile log likelihood for regression parameters 
        'param_num' at 'b0_vals.' 
 
        Parameters 
        ---------- 
        b0_vals : list 
            The value of parameters to be tested 
        param_num : list 
            Which parameters to be tested 
        maxiter : int, optional 
            How many iterations to use in the EM algorithm.  Default is 30 
        ftol : float, optional 
            The function tolerance for the EM optimization. 
            Default is 10''**''-5 
        print_weights : bool 
            If true, returns the weights tate maximize the profile 
            log likelihood. Default is False 
 
        Returns 
        ------- 
 
        test_results : tuple 
            The log-likelihood and p-pvalue of the test. 
 
        Notes 
        ----- 
 
        The function will warn if the EM reaches the maxiter.  However, when 
        optimizing over nuisance parameters, it is possible to reach a 
        maximum number of inner iterations for a specific value for the 
        nuisance parameters while the resultsof the function are still valid. 
        This usually occurs when the optimization over the nuisance parameters 
        selects parameter values that yield a log-likihood ratio close to 
        infinity. 
 
        Examples 
        -------- 
 
        &gt;&gt;&gt; import statsmodels.api as sm 
        &gt;&gt;&gt; import numpy as np 
 
        # Test parameter is .05 in one regressor no intercept model 
        &gt;&gt;&gt; data=sm.datasets.heart.load() 
        &gt;&gt;&gt; y = np.log10(data.endog) 
        &gt;&gt;&gt; x = data.exog 
        &gt;&gt;&gt; cens = data.censors 
        &gt;&gt;&gt; model = sm.emplike.emplikeAFT(y, x, cens) 
        &gt;&gt;&gt; res=model.test_beta([0], [0]) 
        &gt;&gt;&gt; res 
        (1.4657739632606308, 0.22601365256959183) 
 
        #Test slope is 0 in  model with intercept 
 
        &gt;&gt;&gt; data=sm.datasets.heart.load() 
        &gt;&gt;&gt; y = np.log10(data.endog) 
        &gt;&gt;&gt; x = data.exog 
        &gt;&gt;&gt; cens = data.censors 
        &gt;&gt;&gt; model = sm.emplike.emplikeAFT(y, sm.add_constant(x), cens) 
        &gt;&gt;&gt; res = model.test_beta([0], [1]) 
        &gt;&gt;&gt; res 
        (4.623487775078047, 0.031537049752572731) 
        &quot;&quot;&quot;</span>
        <span class="s1">censors = self.model.censors</span>
        <span class="s1">endog = self.model.endog</span>
        <span class="s1">exog = self.model.exog</span>
        <span class="s1">uncensored = (censors == </span><span class="s4">1</span><span class="s1">).flatten()</span>
        <span class="s1">censored = (censors == </span><span class="s4">0</span><span class="s1">).flatten()</span>
        <span class="s1">uncens_endog = endog[uncensored]</span>
        <span class="s1">uncens_exog = exog[uncensored</span><span class="s2">, </span><span class="s1">:]</span>
        <span class="s1">reg_model = OLS(uncens_endog</span><span class="s2">, </span><span class="s1">uncens_exog).fit()</span>
        <span class="s1">llr</span><span class="s2">, </span><span class="s1">pval</span><span class="s2">, </span><span class="s1">new_weights = reg_model.el_test(b0_vals</span><span class="s2">, </span><span class="s1">param_nums</span><span class="s2">,</span>
                                      <span class="s1">return_weights=</span><span class="s2">True</span><span class="s1">)  </span><span class="s3"># Needs to be changed</span>
        <span class="s1">km = self.model._make_km(endog</span><span class="s2">, </span><span class="s1">censors).flatten()  </span><span class="s3"># when merged</span>
        <span class="s1">uncens_nobs = self.model.uncens_nobs</span>
        <span class="s1">F = np.asarray(new_weights).reshape(uncens_nobs)</span>
        <span class="s3"># Step 0 ^</span>
        <span class="s1">params = self.params()</span>
        <span class="s1">survidx = np.where(censors == </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">survidx = survidx[</span><span class="s4">0</span><span class="s1">] - np.arange(len(survidx[</span><span class="s4">0</span><span class="s1">]))</span>
        <span class="s1">numcensbelow = np.int_(np.cumsum(</span><span class="s4">1 </span><span class="s1">- censors))</span>
        <span class="s2">if </span><span class="s1">len(param_nums) == len(params):</span>
            <span class="s1">llr = self._EM_test([]</span><span class="s2">, </span><span class="s1">F=F</span><span class="s2">, </span><span class="s1">params=params</span><span class="s2">,</span>
                                      <span class="s1">param_nums=param_nums</span><span class="s2">,</span>
                                <span class="s1">b0_vals=b0_vals</span><span class="s2">, </span><span class="s1">survidx=survidx</span><span class="s2">,</span>
                             <span class="s1">uncens_nobs=uncens_nobs</span><span class="s2">,</span>
                             <span class="s1">numcensbelow=numcensbelow</span><span class="s2">, </span><span class="s1">km=km</span><span class="s2">,</span>
                             <span class="s1">uncensored=uncensored</span><span class="s2">, </span><span class="s1">censored=censored</span><span class="s2">,</span>
                             <span class="s1">ftol=ftol</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s4">25</span><span class="s1">)</span>
            <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">chi2.sf(llr</span><span class="s2">, </span><span class="s1">self.model.nvar)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">x0 = np.delete(params</span><span class="s2">, </span><span class="s1">param_nums)</span>
            <span class="s2">try</span><span class="s1">:</span>
                <span class="s1">res = optimize.fmin(self._EM_test</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">,</span>
                                   <span class="s1">(params</span><span class="s2">, </span><span class="s1">param_nums</span><span class="s2">, </span><span class="s1">b0_vals</span><span class="s2">, </span><span class="s1">F</span><span class="s2">, </span><span class="s1">survidx</span><span class="s2">,</span>
                                    <span class="s1">uncens_nobs</span><span class="s2">, </span><span class="s1">numcensbelow</span><span class="s2">, </span><span class="s1">km</span><span class="s2">, </span><span class="s1">uncensored</span><span class="s2">,</span>
                                    <span class="s1">censored</span><span class="s2">, </span><span class="s1">maxiter</span><span class="s2">, </span><span class="s1">ftol)</span><span class="s2">, </span><span class="s1">full_output=</span><span class="s4">1</span><span class="s2">,</span>
                                    <span class="s1">disp=</span><span class="s4">0</span><span class="s1">)</span>

                <span class="s1">llr = res[</span><span class="s4">1</span><span class="s1">]</span>
                <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">chi2.sf(llr</span><span class="s2">, </span><span class="s1">len(param_nums))</span>
            <span class="s2">except </span><span class="s1">np.linalg.linalg.LinAlgError:</span>
                <span class="s2">return </span><span class="s1">np.inf</span><span class="s2">, </span><span class="s4">0</span>

    <span class="s2">def </span><span class="s1">ci_beta(self</span><span class="s2">, </span><span class="s1">param_num</span><span class="s2">, </span><span class="s1">beta_high</span><span class="s2">, </span><span class="s1">beta_low</span><span class="s2">, </span><span class="s1">sig=</span><span class="s4">.05</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the confidence interval for a regression 
        parameter in the AFT model. 
 
        Parameters 
        ---------- 
        param_num : int 
            Parameter number of interest 
        beta_high : float 
            Upper bound for the confidence interval 
        beta_low : float 
            Lower bound for the confidence interval 
        sig : float, optional 
            Significance level.  Default is .05 
 
        Notes 
        ----- 
        If the function returns f(a) and f(b) must have different signs, 
        consider widening the search area by adjusting beta_low and 
        beta_high. 
 
        Also note that this process is computational intensive.  There 
        are 4 levels of optimization/solving.  From outer to inner: 
 
        1) Solving so that llr-critical value = 0 
        2) maximizing over nuisance parameters 
        3) Using  EM at each value of nuisamce parameters 
        4) Using the _modified_Newton optimizer at each iteration 
           of the EM algorithm. 
 
        Also, for very unlikely nuisance parameters, it is possible for 
        the EM algorithm to not converge.  This is not an indicator 
        that the solver did not find the correct solution.  It just means 
        for a specific iteration of the nuisance parameters, the optimizer 
        was unable to converge. 
 
        If the user desires to verify the success of the optimization, 
        it is recommended to test the limits using test_beta. 
        &quot;&quot;&quot;</span>
        <span class="s1">params = self.params()</span>
        <span class="s1">self.r0 = chi2.ppf(</span><span class="s4">1 </span><span class="s1">- sig</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">ll = optimize.brentq(self._ci_limits_beta</span><span class="s2">, </span><span class="s1">beta_low</span><span class="s2">,</span>
                             <span class="s1">params[param_num]</span><span class="s2">, </span><span class="s1">(param_num))</span>
        <span class="s1">ul = optimize.brentq(self._ci_limits_beta</span><span class="s2">,</span>
                             <span class="s1">params[param_num]</span><span class="s2">, </span><span class="s1">beta_high</span><span class="s2">, </span><span class="s1">(param_num))</span>
        <span class="s2">return </span><span class="s1">ll</span><span class="s2">, </span><span class="s1">ul</span>
</pre>
</body>
</html>