<html>
<head>
<title>betareg.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
betareg.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>

<span class="s2">u&quot;&quot;&quot; 
Beta regression for modeling rates and proportions. 
 
References 
---------- 
Gr√ºn, Bettina, Ioannis Kosmidis, and Achim Zeileis. Extended beta regression 
in R: Shaken, stirred, mixed, and partitioned. No. 2011-22. Working Papers in 
Economics and Statistics, 2011. 
 
Smithson, Michael, and Jay Verkuilen. &quot;A better lemon squeezer? 
Maximum-likelihood regression with beta-distributed dependent variables.&quot; 
Psychological methods 11.1 (2006): 54. 
&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy.special </span><span class="s3">import </span><span class="s1">gammaln </span><span class="s3">as </span><span class="s1">lgamma</span>
<span class="s3">import </span><span class="s1">patsy</span>

<span class="s3">import </span><span class="s1">statsmodels.base.wrapper </span><span class="s3">as </span><span class="s1">wrap</span>
<span class="s3">import </span><span class="s1">statsmodels.regression.linear_model </span><span class="s3">as </span><span class="s1">lm</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.decorators </span><span class="s3">import </span><span class="s1">cache_readonly</span>
<span class="s3">from </span><span class="s1">statsmodels.base.model </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">GenericLikelihoodModel</span><span class="s3">, </span><span class="s1">GenericLikelihoodModelResults</span><span class="s3">, </span><span class="s1">_LLRMixin)</span>
<span class="s3">from </span><span class="s1">statsmodels.genmod </span><span class="s3">import </span><span class="s1">families</span>


<span class="s1">_init_example = </span><span class="s4">&quot;&quot;&quot; 
 
    Beta regression with default of logit-link for exog and log-link 
    for precision. 
 
    &gt;&gt;&gt; mod = BetaModel(endog, exog) 
    &gt;&gt;&gt; rslt = mod.fit() 
    &gt;&gt;&gt; print(rslt.summary()) 
 
    We can also specify a formula and a specific structure and use the 
    identity-link for precision. 
 
    &gt;&gt;&gt; from sm.families.links import identity 
    &gt;&gt;&gt; Z = patsy.dmatrix('~ temp', dat, return_type='dataframe') 
    &gt;&gt;&gt; mod = BetaModel.from_formula('iyield ~ C(batch, Treatment(10)) + temp', 
    ...                              dat, exog_precision=Z, 
    ...                              link_precision=identity()) 
 
    In the case of proportion-data, we may think that the precision depends on 
    the number of measurements. E.g for sequence data, on the number of 
    sequence reads covering a site: 
 
    &gt;&gt;&gt; Z = patsy.dmatrix('~ coverage', df) 
    &gt;&gt;&gt; formula = 'methylation ~ disease + age + gender + coverage' 
    &gt;&gt;&gt; mod = BetaModel.from_formula(formula, df, Z) 
    &gt;&gt;&gt; rslt = mod.fit() 
 
&quot;&quot;&quot;</span>


<span class="s3">class </span><span class="s1">BetaModel(GenericLikelihoodModel):</span>
    <span class="s1">__doc__ = </span><span class="s4">&quot;&quot;&quot;Beta Regression. 
 
    The Model is parameterized by mean and precision. Both can depend on 
    explanatory variables through link functions. 
 
    Parameters 
    ---------- 
    endog : array_like 
        1d array of endogenous response variable. 
    exog : array_like 
        A nobs x k array where `nobs` is the number of observations and `k` 
        is the number of regressors. An intercept is not included by default 
        and should be added by the user (models specified using a formula 
        include an intercept by default). See `statsmodels.tools.add_constant`. 
    exog_precision : array_like 
        2d array of variables for the precision. 
    link : link 
        Any link in sm.families.links for mean, should have range in 
        interval [0, 1]. Default is logit-link. 
    link_precision : link 
        Any link in sm.families.links for precision, should have 
        range in positive line. Default is log-link. 
    **kwds : extra keywords 
        Keyword options that will be handled by super classes. 
        Not all general keywords will be supported in this class. 
 
    Notes 
    ----- 
    Status: experimental, new in 0.13. 
    Core results are verified, but api can change and some extra results 
    specific to Beta regression are missing. 
 
    Examples 
    -------- 
    {example} 
 
    See Also 
    -------- 
    :ref:`links` 
 
    &quot;&quot;&quot;</span><span class="s1">.format(example=_init_example)</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">exog_precision=</span><span class="s3">None,</span>
                 <span class="s1">link=families.links.Logit()</span><span class="s3">,</span>
                 <span class="s1">link_precision=families.links.Log()</span><span class="s3">, </span><span class="s1">**kwds):</span>

        <span class="s1">etmp = np.array(endog)</span>
        <span class="s3">assert </span><span class="s1">np.all((</span><span class="s5">0 </span><span class="s1">&lt; etmp) &amp; (etmp &lt; </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s3">if </span><span class="s1">exog_precision </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">extra_names = [</span><span class="s4">'precision'</span><span class="s1">]</span>
            <span class="s1">exog_precision = np.ones((len(endog)</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=</span><span class="s4">'f'</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">extra_names = [</span><span class="s4">'precision-%s' </span><span class="s1">% zc </span><span class="s3">for </span><span class="s1">zc </span><span class="s3">in</span>
                           <span class="s1">(exog_precision.columns</span>
                            <span class="s3">if </span><span class="s1">hasattr(exog_precision</span><span class="s3">, </span><span class="s4">'columns'</span><span class="s1">)</span>
                            <span class="s3">else </span><span class="s1">range(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">exog_precision.shape[</span><span class="s5">1</span><span class="s1">] + </span><span class="s5">1</span><span class="s1">))]</span>

        <span class="s1">kwds[</span><span class="s4">'extra_params_names'</span><span class="s1">] = extra_names</span>

        <span class="s1">super(BetaModel</span><span class="s3">, </span><span class="s1">self).__init__(endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">,</span>
                                        <span class="s1">exog_precision=exog_precision</span><span class="s3">,</span>
                                        <span class="s1">**kwds)</span>
        <span class="s1">self.link = link</span>
        <span class="s1">self.link_precision = link_precision</span>
        <span class="s0"># not needed, handled by super:</span>
        <span class="s0"># self.exog_precision = exog_precision</span>
        <span class="s0"># inherited df do not account for precision params</span>
        <span class="s1">self.nobs = self.endog.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">self.k_extra = </span><span class="s5">1</span>
        <span class="s1">self.df_model = self.nparams - </span><span class="s5">2</span>
        <span class="s1">self.df_resid = self.nobs - self.nparams</span>
        <span class="s3">assert </span><span class="s1">len(self.exog_precision) == len(self.endog)</span>
        <span class="s1">self.hess_type = </span><span class="s4">&quot;oim&quot;</span>
        <span class="s3">if </span><span class="s4">'exog_precision' </span><span class="s3">not in </span><span class="s1">self._init_keys:</span>
            <span class="s1">self._init_keys.extend([</span><span class="s4">'exog_precision'</span><span class="s1">])</span>
        <span class="s1">self._init_keys.extend([</span><span class="s4">'link'</span><span class="s3">, </span><span class="s4">'link_precision'</span><span class="s1">])</span>
        <span class="s1">self._null_drop_keys = [</span><span class="s4">'exog_precision'</span><span class="s1">]</span>
        <span class="s3">del </span><span class="s1">kwds[</span><span class="s4">'extra_params_names'</span><span class="s1">]</span>
        <span class="s1">self._check_kwargs(kwds)</span>
        <span class="s1">self.results_class = BetaResults</span>
        <span class="s1">self.results_class_wrapper = BetaResultsWrapper</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">from_formula(cls</span><span class="s3">, </span><span class="s1">formula</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">exog_precision_formula=</span><span class="s3">None,</span>
                     <span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s3">if </span><span class="s1">exog_precision_formula </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s4">'subset' </span><span class="s3">in </span><span class="s1">kwargs:</span>
                <span class="s1">d = data.ix[kwargs[</span><span class="s4">'subset'</span><span class="s1">]]</span>
                <span class="s1">Z = patsy.dmatrix(exog_precision_formula</span><span class="s3">, </span><span class="s1">d)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">Z = patsy.dmatrix(exog_precision_formula</span><span class="s3">, </span><span class="s1">data)</span>
            <span class="s1">kwargs[</span><span class="s4">'exog_precision'</span><span class="s1">] = Z</span>

        <span class="s3">return </span><span class="s1">super(BetaModel</span><span class="s3">, </span><span class="s1">cls).from_formula(formula</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">,</span>
                                                  <span class="s1">**kwargs)</span>

    <span class="s3">def </span><span class="s1">_get_exogs(self):</span>
        <span class="s3">return </span><span class="s1">(self.exog</span><span class="s3">, </span><span class="s1">self.exog_precision)</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exog_precision=</span><span class="s3">None, </span><span class="s1">which=</span><span class="s4">&quot;mean&quot;</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;Predict values for mean or precision 
 
        Parameters 
        ---------- 
        params : array_like 
            The model parameters. 
        exog : array_like 
            Array of predictor variables for mean. 
        exog_precision : array_like 
            Array of predictor variables for precision parameter. 
        which : str 
 
            - &quot;mean&quot; : mean, conditional expectation E(endog | exog) 
            - &quot;precision&quot; : predicted precision 
            - &quot;linear&quot; : linear predictor for the mean function 
            - &quot;linear-precision&quot; : linear predictor for the precision parameter 
 
        Returns 
        ------- 
        ndarray, predicted values 
        &quot;&quot;&quot;</span>
        <span class="s0"># compatibility with old names and misspelling</span>
        <span class="s3">if </span><span class="s1">which == </span><span class="s4">&quot;linpred&quot;</span><span class="s1">:</span>
            <span class="s1">which = </span><span class="s4">&quot;linear&quot;</span>
        <span class="s3">if </span><span class="s1">which </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;linpred_precision&quot;</span><span class="s3">, </span><span class="s4">&quot;linear_precision&quot;</span><span class="s1">]:</span>
            <span class="s1">which = </span><span class="s4">&quot;linear-precision&quot;</span>

        <span class="s1">k_mean = self.exog.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">if </span><span class="s1">which </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;mean&quot;</span><span class="s3">,  </span><span class="s4">&quot;linear&quot;</span><span class="s1">]:</span>
            <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">exog = self.exog</span>
            <span class="s1">params_mean = params[:k_mean]</span>
            <span class="s0"># Zparams = params[k_mean:]</span>
            <span class="s1">linpred = np.dot(exog</span><span class="s3">, </span><span class="s1">params_mean)</span>
            <span class="s3">if </span><span class="s1">which == </span><span class="s4">&quot;mean&quot;</span><span class="s1">:</span>
                <span class="s1">mu = self.link.inverse(linpred)</span>
                <span class="s1">res = mu</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">res = linpred</span>

        <span class="s3">elif </span><span class="s1">which </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;precision&quot;</span><span class="s3">, </span><span class="s4">&quot;linear-precision&quot;</span><span class="s1">]:</span>
            <span class="s3">if </span><span class="s1">exog_precision </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">exog_precision = self.exog_precision</span>
            <span class="s1">params_prec = params[k_mean:]</span>
            <span class="s1">linpred_prec = np.dot(exog_precision</span><span class="s3">, </span><span class="s1">params_prec)</span>

            <span class="s3">if </span><span class="s1">which == </span><span class="s4">&quot;precision&quot;</span><span class="s1">:</span>
                <span class="s1">phi = self.link_precision.inverse(linpred_prec)</span>
                <span class="s1">res = phi</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">res = linpred_prec</span>

        <span class="s3">elif </span><span class="s1">which == </span><span class="s4">&quot;var&quot;</span><span class="s1">:</span>
            <span class="s1">res = self._predict_var(</span>
                <span class="s1">params</span><span class="s3">,</span>
                <span class="s1">exog=exog</span><span class="s3">,</span>
                <span class="s1">exog_precision=exog_precision</span>
                <span class="s1">)</span>

        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'which = %s is not available' </span><span class="s1">% which)</span>

        <span class="s3">return </span><span class="s1">res</span>

    <span class="s3">def </span><span class="s1">_predict_precision(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog_precision=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;Predict values for precision function for given exog_precision. 
 
        Parameters 
        ---------- 
        params : array_like 
            The model parameters. 
        exog_precision : array_like 
            Array of predictor variables for precision. 
 
        Returns 
        ------- 
        Predicted precision. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">exog_precision </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog_precision = self.exog_precision</span>

        <span class="s1">k_mean = self.exog.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">params_precision = params[k_mean:]</span>
        <span class="s1">linpred_prec = np.dot(exog_precision</span><span class="s3">, </span><span class="s1">params_precision)</span>
        <span class="s1">phi = self.link_precision.inverse(linpred_prec)</span>

        <span class="s3">return </span><span class="s1">phi</span>

    <span class="s3">def </span><span class="s1">_predict_var(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exog_precision=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;predict values for conditional variance V(endog | exog) 
 
        Parameters 
        ---------- 
        params : array_like 
            The model parameters. 
        exog : array_like 
            Array of predictor variables for mean. 
        exog_precision : array_like 
            Array of predictor variables for precision. 
 
        Returns 
        ------- 
        Predicted conditional variance. 
        &quot;&quot;&quot;</span>
        <span class="s1">mean = self.predict(params</span><span class="s3">, </span><span class="s1">exog=exog)</span>
        <span class="s1">precision = self._predict_precision(params</span><span class="s3">,</span>
                                            <span class="s1">exog_precision=exog_precision)</span>

        <span class="s1">var_endog = mean * (</span><span class="s5">1 </span><span class="s1">- mean) / (</span><span class="s5">1 </span><span class="s1">+ precision)</span>
        <span class="s3">return </span><span class="s1">var_endog</span>

    <span class="s3">def </span><span class="s1">loglikeobs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot; 
        Loglikelihood for observations of the Beta regressionmodel. 
 
        Parameters 
        ---------- 
        params : ndarray 
            The parameters of the model, coefficients for linear predictors 
            of the mean and of the precision function. 
 
        Returns 
        ------- 
        loglike : ndarray 
            The log likelihood for each observation of the model evaluated 
            at `params`. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self._llobs(self.endog</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">, </span><span class="s1">self.exog_precision</span><span class="s3">, </span><span class="s1">params)</span>

    <span class="s3">def </span><span class="s1">_llobs(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">exog_precision</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot; 
        Loglikelihood for observations with data arguments. 
 
        Parameters 
        ---------- 
        endog : ndarray 
            1d array of endogenous variable. 
        exog : ndarray 
            2d array of explanatory variables. 
        exog_precision : ndarray 
            2d array of explanatory variables for precision. 
        params : ndarray 
            The parameters of the model, coefficients for linear predictors 
            of the mean and of the precision function. 
 
        Returns 
        ------- 
        loglike : ndarray 
            The log likelihood for each observation of the model evaluated 
            at `params`. 
        &quot;&quot;&quot;</span>
        <span class="s1">y</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">Z = endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">exog_precision</span>
        <span class="s1">nz = Z.shape[</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">params_mean = params[:-nz]</span>
        <span class="s1">params_prec = params[-nz:]</span>
        <span class="s1">linpred = np.dot(X</span><span class="s3">, </span><span class="s1">params_mean)</span>
        <span class="s1">linpred_prec = np.dot(Z</span><span class="s3">, </span><span class="s1">params_prec)</span>

        <span class="s1">mu = self.link.inverse(linpred)</span>
        <span class="s1">phi = self.link_precision.inverse(linpred_prec)</span>

        <span class="s1">eps_lb = </span><span class="s5">1e-200</span>
        <span class="s1">alpha = np.clip(mu * phi</span><span class="s3">, </span><span class="s1">eps_lb</span><span class="s3">, </span><span class="s1">np.inf)</span>
        <span class="s1">beta = np.clip((</span><span class="s5">1 </span><span class="s1">- mu) * phi</span><span class="s3">, </span><span class="s1">eps_lb</span><span class="s3">, </span><span class="s1">np.inf)</span>

        <span class="s1">ll = (lgamma(phi) - lgamma(alpha)</span>
              <span class="s1">- lgamma(beta)</span>
              <span class="s1">+ (mu * phi - </span><span class="s5">1</span><span class="s1">) * np.log(y)</span>
              <span class="s1">+ (((</span><span class="s5">1 </span><span class="s1">- mu) * phi) - </span><span class="s5">1</span><span class="s1">) * np.log(</span><span class="s5">1 </span><span class="s1">- y))</span>

        <span class="s3">return </span><span class="s1">ll</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot; 
        Returns the score vector of the log-likelihood. 
 
        http://www.tandfonline.com/doi/pdf/10.1080/00949650903389993 
 
        Parameters 
        ---------- 
        params : ndarray 
            Parameter at which score is evaluated. 
 
        Returns 
        ------- 
        score : ndarray 
            First derivative of loglikelihood function. 
        &quot;&quot;&quot;</span>
        <span class="s1">sf1</span><span class="s3">, </span><span class="s1">sf2 = self.score_factor(params)</span>

        <span class="s1">d1 = np.dot(sf1</span><span class="s3">, </span><span class="s1">self.exog)</span>
        <span class="s1">d2 = np.dot(sf2</span><span class="s3">, </span><span class="s1">self.exog_precision)</span>
        <span class="s3">return </span><span class="s1">np.concatenate((d1</span><span class="s3">, </span><span class="s1">d2))</span>

    <span class="s3">def </span><span class="s1">_score_check(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot;Inherited score with finite differences 
 
        Parameters 
        ---------- 
        params : ndarray 
            Parameter at which score is evaluated. 
 
        Returns 
        ------- 
        score based on numerical derivatives 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">super(BetaModel</span><span class="s3">, </span><span class="s1">self).score(params)</span>

    <span class="s3">def </span><span class="s1">score_factor(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">endog=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;Derivative of loglikelihood function w.r.t. linear predictors. 
 
        This needs to be multiplied with the exog to obtain the score_obs. 
 
        Parameters 
        ---------- 
        params : ndarray 
            Parameter at which score is evaluated. 
 
        Returns 
        ------- 
        score_factor : ndarray, 2-D 
            A 2d weight vector used in the calculation of the score_obs. 
 
        Notes 
        ----- 
        The score_obs can be obtained from score_factor ``sf`` using 
 
            - d1 = sf[:, :1] * exog 
            - d2 = sf[:, 1:2] * exog_precision 
 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">special</span>
        <span class="s1">digamma = special.psi</span>

        <span class="s1">y = self.endog </span><span class="s3">if </span><span class="s1">endog </span><span class="s3">is None else </span><span class="s1">endog</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">Z = self.exog</span><span class="s3">, </span><span class="s1">self.exog_precision</span>
        <span class="s1">nz = Z.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">Xparams = params[:-nz]</span>
        <span class="s1">Zparams = params[-nz:]</span>

        <span class="s0"># NO LINKS</span>
        <span class="s1">mu = self.link.inverse(np.dot(X</span><span class="s3">, </span><span class="s1">Xparams))</span>
        <span class="s1">phi = self.link_precision.inverse(np.dot(Z</span><span class="s3">, </span><span class="s1">Zparams))</span>

        <span class="s1">eps_lb = </span><span class="s5">1e-200  </span><span class="s0"># lower bound for evaluating digamma, avoids -inf</span>
        <span class="s1">alpha = np.clip(mu * phi</span><span class="s3">, </span><span class="s1">eps_lb</span><span class="s3">, </span><span class="s1">np.inf)</span>
        <span class="s1">beta = np.clip((</span><span class="s5">1 </span><span class="s1">- mu) * phi</span><span class="s3">, </span><span class="s1">eps_lb</span><span class="s3">, </span><span class="s1">np.inf)</span>

        <span class="s1">ystar = np.log(y / (</span><span class="s5">1. </span><span class="s1">- y))</span>
        <span class="s1">dig_beta = digamma(beta)</span>
        <span class="s1">mustar = digamma(alpha) - dig_beta</span>
        <span class="s1">yt = np.log(</span><span class="s5">1 </span><span class="s1">- y)</span>
        <span class="s1">mut = dig_beta - digamma(phi)</span>

        <span class="s1">t = </span><span class="s5">1. </span><span class="s1">/ self.link.deriv(mu)</span>
        <span class="s1">h = </span><span class="s5">1. </span><span class="s1">/ self.link_precision.deriv(phi)</span>
        <span class="s0">#</span>
        <span class="s1">sf1 = phi * t * (ystar - mustar)</span>
        <span class="s1">sf2 = h * (mu * (ystar - mustar) + yt - mut)</span>

        <span class="s3">return </span><span class="s1">(sf1</span><span class="s3">, </span><span class="s1">sf2)</span>

    <span class="s3">def </span><span class="s1">score_hessian_factor(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">return_hessian=</span><span class="s3">False,</span>
                             <span class="s1">observed=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;Derivatives of loglikelihood function w.r.t. linear predictors. 
 
        This calculates score and hessian factors at the same time, because 
        there is a large overlap in calculations. 
 
        Parameters 
        ---------- 
        params : ndarray 
            Parameter at which score is evaluated. 
        return_hessian : bool 
            If False, then only score_factors are returned 
            If True, the both score and hessian factors are returned 
        observed : bool 
            If True, then the observed Hessian is returned (default). 
            If False, then the expected information matrix is returned. 
 
        Returns 
        ------- 
        score_factor : ndarray, 2-D 
            A 2d weight vector used in the calculation of the score_obs. 
        (-jbb, -jbg, -jgg) : tuple 
            A tuple with 3 hessian factors, corresponding to the upper 
            triangle of the Hessian matrix. 
            TODO: check why there are minus 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">special</span>
        <span class="s1">digamma = special.psi</span>

        <span class="s1">y</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">Z = self.endog</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">, </span><span class="s1">self.exog_precision</span>
        <span class="s1">nz = Z.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">Xparams = params[:-nz]</span>
        <span class="s1">Zparams = params[-nz:]</span>

        <span class="s0"># NO LINKS</span>
        <span class="s1">mu = self.link.inverse(np.dot(X</span><span class="s3">, </span><span class="s1">Xparams))</span>
        <span class="s1">phi = self.link_precision.inverse(np.dot(Z</span><span class="s3">, </span><span class="s1">Zparams))</span>

        <span class="s0"># We need to prevent mu = 0 and (1-mu) = 0 in digamma call</span>
        <span class="s1">eps_lb = </span><span class="s5">1e-200  </span><span class="s0"># lower bound for evaluating digamma, avoids -inf</span>
        <span class="s1">alpha = np.clip(mu * phi</span><span class="s3">, </span><span class="s1">eps_lb</span><span class="s3">, </span><span class="s1">np.inf)</span>
        <span class="s1">beta = np.clip((</span><span class="s5">1 </span><span class="s1">- mu) * phi</span><span class="s3">, </span><span class="s1">eps_lb</span><span class="s3">, </span><span class="s1">np.inf)</span>

        <span class="s1">ystar = np.log(y / (</span><span class="s5">1. </span><span class="s1">- y))</span>
        <span class="s1">dig_beta = digamma(beta)</span>
        <span class="s1">mustar = digamma(alpha) - dig_beta</span>
        <span class="s1">yt = np.log(</span><span class="s5">1 </span><span class="s1">- y)</span>
        <span class="s1">mut = dig_beta - digamma(phi)</span>

        <span class="s1">t = </span><span class="s5">1. </span><span class="s1">/ self.link.deriv(mu)</span>
        <span class="s1">h = </span><span class="s5">1. </span><span class="s1">/ self.link_precision.deriv(phi)</span>

        <span class="s1">ymu_star = (ystar - mustar)</span>
        <span class="s1">sf1 = phi * t * ymu_star</span>
        <span class="s1">sf2 = h * (mu * ymu_star + yt - mut)</span>

        <span class="s3">if </span><span class="s1">return_hessian:</span>
            <span class="s1">trigamma = </span><span class="s3">lambda </span><span class="s1">x: special.polygamma(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">x)  </span><span class="s0"># noqa</span>
            <span class="s1">trig_beta = trigamma(beta)</span>
            <span class="s1">var_star = trigamma(alpha) + trig_beta</span>
            <span class="s1">var_t = trig_beta - trigamma(phi)</span>

            <span class="s1">c = - trig_beta</span>
            <span class="s1">s = self.link.deriv2(mu)</span>
            <span class="s1">q = self.link_precision.deriv2(phi)</span>

            <span class="s1">jbb = (phi * t) * var_star</span>
            <span class="s3">if </span><span class="s1">observed:</span>
                <span class="s1">jbb += s * t**</span><span class="s5">2 </span><span class="s1">* ymu_star</span>

            <span class="s1">jbb *= t * phi</span>

            <span class="s1">jbg = phi * t * h * (mu * var_star + c)</span>
            <span class="s3">if </span><span class="s1">observed:</span>
                <span class="s1">jbg -= ymu_star * t * h</span>

            <span class="s1">jgg = h**</span><span class="s5">2 </span><span class="s1">* (mu**</span><span class="s5">2 </span><span class="s1">* var_star + </span><span class="s5">2 </span><span class="s1">* mu * c + var_t)</span>
            <span class="s3">if </span><span class="s1">observed:</span>
                <span class="s1">jgg += (mu * ymu_star + yt - mut) * q * h**</span><span class="s5">3    </span><span class="s0"># **3 ?</span>

            <span class="s3">return </span><span class="s1">(sf1</span><span class="s3">, </span><span class="s1">sf2)</span><span class="s3">, </span><span class="s1">(-jbb</span><span class="s3">, </span><span class="s1">-jbg</span><span class="s3">, </span><span class="s1">-jgg)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">(sf1</span><span class="s3">, </span><span class="s1">sf2)</span>

    <span class="s3">def </span><span class="s1">score_obs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot; 
        Score, first derivative of the loglikelihood for each observation. 
 
        Parameters 
        ---------- 
        params : ndarray 
            Parameter at which score is evaluated. 
 
        Returns 
        ------- 
        score_obs : ndarray, 2d 
            The first derivative of the loglikelihood function evaluated at 
            params for each observation. 
        &quot;&quot;&quot;</span>
        <span class="s1">sf1</span><span class="s3">, </span><span class="s1">sf2 = self.score_factor(params)</span>

        <span class="s0"># elementwise product for each row (observation)</span>
        <span class="s1">d1 = sf1[:</span><span class="s3">, None</span><span class="s1">] * self.exog</span>
        <span class="s1">d2 = sf2[:</span><span class="s3">, None</span><span class="s1">] * self.exog_precision</span>
        <span class="s3">return </span><span class="s1">np.column_stack((d1</span><span class="s3">, </span><span class="s1">d2))</span>

    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">observed=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;Hessian, second derivative of loglikelihood function 
 
        Parameters 
        ---------- 
        params : ndarray 
            Parameter at which Hessian is evaluated. 
        observed : bool 
            If True, then the observed Hessian is returned (default). 
            If False, then the expected information matrix is returned. 
 
        Returns 
        ------- 
        hessian : ndarray 
            Hessian, i.e. observed information, or expected information matrix. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self.hess_type == </span><span class="s4">&quot;eim&quot;</span><span class="s1">:</span>
            <span class="s1">observed = </span><span class="s3">False</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">observed = </span><span class="s3">True</span>
        <span class="s1">_</span><span class="s3">, </span><span class="s1">hf = self.score_hessian_factor(params</span><span class="s3">, </span><span class="s1">return_hessian=</span><span class="s3">True,</span>
                                          <span class="s1">observed=observed)</span>

        <span class="s1">hf11</span><span class="s3">, </span><span class="s1">hf12</span><span class="s3">, </span><span class="s1">hf22 = hf</span>

        <span class="s0"># elementwise product for each row (observation)</span>
        <span class="s1">d11 = (self.exog.T * hf11).dot(self.exog)</span>
        <span class="s1">d12 = (self.exog.T * hf12).dot(self.exog_precision)</span>
        <span class="s1">d22 = (self.exog_precision.T * hf22).dot(self.exog_precision)</span>
        <span class="s3">return </span><span class="s1">np.block([[d11</span><span class="s3">, </span><span class="s1">d12]</span><span class="s3">, </span><span class="s1">[d12.T</span><span class="s3">, </span><span class="s1">d22]])</span>

    <span class="s3">def </span><span class="s1">hessian_factor(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">observed=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;Derivatives of loglikelihood function w.r.t. linear predictors. 
        &quot;&quot;&quot;</span>
        <span class="s1">_</span><span class="s3">, </span><span class="s1">hf = self.score_hessian_factor(params</span><span class="s3">, </span><span class="s1">return_hessian=</span><span class="s3">True,</span>
                                          <span class="s1">observed=observed)</span>
        <span class="s3">return </span><span class="s1">hf</span>

    <span class="s3">def </span><span class="s1">_start_params(self</span><span class="s3">, </span><span class="s1">niter=</span><span class="s5">2</span><span class="s3">, </span><span class="s1">return_intermediate=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;find starting values 
 
        Parameters 
        ---------- 
        niter : int 
            Number of iterations of WLS approximation 
        return_intermediate : bool 
            If False (default), then only the preliminary parameter estimate 
            will be returned. 
            If True, then also the two results instances of the WLS estimate 
            for mean parameters and for the precision parameters will be 
            returned. 
 
        Returns 
        ------- 
        sp : ndarray 
            start parameters for the optimization 
        res_m2 : results instance (optional) 
            Results instance for the WLS regression of the mean function. 
        res_p2 : results instance (optional) 
            Results instance for the WLS regression of the precision function. 
 
        Notes 
        ----- 
        This calculates a few iteration of weighted least squares. This is not 
        a full scoring algorithm. 
        &quot;&quot;&quot;</span>
        <span class="s0"># WLS of the mean equation uses the implied weights (inverse variance),</span>
        <span class="s0"># WLS for the precision equations uses weights that only take</span>
        <span class="s0"># account of the link transformation of the precision endog.</span>
        <span class="s3">from </span><span class="s1">statsmodels.regression.linear_model </span><span class="s3">import </span><span class="s1">OLS</span><span class="s3">, </span><span class="s1">WLS</span>
        <span class="s1">res_m = OLS(self.link(self.endog)</span><span class="s3">, </span><span class="s1">self.exog).fit()</span>
        <span class="s1">fitted = self.link.inverse(res_m.fittedvalues)</span>
        <span class="s1">resid = self.endog - fitted</span>

        <span class="s1">prec_i = fitted * (</span><span class="s5">1 </span><span class="s1">- fitted) / np.maximum(np.abs(resid)</span><span class="s3">, </span><span class="s5">1e-2</span><span class="s1">)**</span><span class="s5">2 </span><span class="s1">- </span><span class="s5">1</span>
        <span class="s1">res_p = OLS(self.link_precision(prec_i)</span><span class="s3">, </span><span class="s1">self.exog_precision).fit()</span>
        <span class="s1">prec_fitted = self.link_precision.inverse(res_p.fittedvalues)</span>
        <span class="s0"># sp = np.concatenate((res_m.params, res_p.params))</span>

        <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(niter):</span>
            <span class="s1">y_var_inv = (</span><span class="s5">1 </span><span class="s1">+ prec_fitted) / (fitted * (</span><span class="s5">1 </span><span class="s1">- fitted))</span>
            <span class="s0"># y_var = fitted * (1 - fitted) / (1 + prec_fitted)</span>

            <span class="s1">ylink_var_inv = y_var_inv / self.link.deriv(fitted)**</span><span class="s5">2</span>
            <span class="s1">res_m2 = WLS(self.link(self.endog)</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">,</span>
                         <span class="s1">weights=ylink_var_inv).fit()</span>
            <span class="s1">fitted = self.link.inverse(res_m2.fittedvalues)</span>
            <span class="s1">resid2 = self.endog - fitted</span>

            <span class="s1">prec_i2 = (fitted * (</span><span class="s5">1 </span><span class="s1">- fitted) /</span>
                       <span class="s1">np.maximum(np.abs(resid2)</span><span class="s3">, </span><span class="s5">1e-2</span><span class="s1">)**</span><span class="s5">2 </span><span class="s1">- </span><span class="s5">1</span><span class="s1">)</span>
            <span class="s1">w_p = </span><span class="s5">1. </span><span class="s1">/ self.link_precision.deriv(prec_fitted)**</span><span class="s5">2</span>
            <span class="s1">res_p2 = WLS(self.link_precision(prec_i2)</span><span class="s3">, </span><span class="s1">self.exog_precision</span><span class="s3">,</span>
                         <span class="s1">weights=w_p).fit()</span>
            <span class="s1">prec_fitted = self.link_precision.inverse(res_p2.fittedvalues)</span>
            <span class="s1">sp2 = np.concatenate((res_m2.params</span><span class="s3">, </span><span class="s1">res_p2.params))</span>

        <span class="s3">if </span><span class="s1">return_intermediate:</span>
            <span class="s3">return </span><span class="s1">sp2</span><span class="s3">, </span><span class="s1">res_m2</span><span class="s3">, </span><span class="s1">res_p2</span>

        <span class="s3">return </span><span class="s1">sp2</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">maxiter=</span><span class="s5">1000</span><span class="s3">, </span><span class="s1">disp=</span><span class="s3">False,</span>
            <span class="s1">method=</span><span class="s4">'bfgs'</span><span class="s3">, </span><span class="s1">**kwds):</span>
        <span class="s2">&quot;&quot;&quot; 
        Fit the model by maximum likelihood. 
 
        Parameters 
        ---------- 
        start_params : array-like 
            A vector of starting values for the regression 
            coefficients.  If None, a default is chosen. 
        maxiter : integer 
            The maximum number of iterations 
        disp : bool 
            Show convergence stats. 
        method : str 
            The optimization method to use. 
        kwds : 
            Keyword arguments for the optimizer. 
 
        Returns 
        ------- 
        BetaResults instance. 
        &quot;&quot;&quot;</span>

        <span class="s3">if </span><span class="s1">start_params </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">start_params = self._start_params()</span>
<span class="s0">#           # http://www.ime.usp.br/~sferrari/beta.pdf suggests starting phi</span>
<span class="s0">#           # on page 8</span>

        <span class="s3">if </span><span class="s4">&quot;cov_type&quot; </span><span class="s3">in </span><span class="s1">kwds:</span>
            <span class="s0"># this is a workaround because we cannot tell super to use eim</span>
            <span class="s3">if </span><span class="s1">kwds[</span><span class="s4">&quot;cov_type&quot;</span><span class="s1">].lower() == </span><span class="s4">&quot;eim&quot;</span><span class="s1">:</span>
                <span class="s1">self.hess_type = </span><span class="s4">&quot;eim&quot;</span>
                <span class="s3">del </span><span class="s1">kwds[</span><span class="s4">&quot;cov_type&quot;</span><span class="s1">]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.hess_type = </span><span class="s4">&quot;oim&quot;</span>

        <span class="s1">res = super(BetaModel</span><span class="s3">, </span><span class="s1">self).fit(start_params=start_params</span><span class="s3">,</span>
                                         <span class="s1">maxiter=maxiter</span><span class="s3">, </span><span class="s1">method=method</span><span class="s3">,</span>
                                         <span class="s1">disp=disp</span><span class="s3">, </span><span class="s1">**kwds)</span>
        <span class="s3">if not </span><span class="s1">isinstance(res</span><span class="s3">, </span><span class="s1">BetaResultsWrapper):</span>
            <span class="s0"># currently GenericLikelihoodModel doe not add wrapper</span>
            <span class="s1">res = BetaResultsWrapper(res)</span>
        <span class="s3">return </span><span class="s1">res</span>

    <span class="s3">def </span><span class="s1">_deriv_mean_dparams(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot; 
        Derivative of the expected endog with respect to the parameters. 
 
        not verified yet 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which score is evaluated 
 
        Returns 
        ------- 
        The value of the derivative of the expected endog with respect 
        to the parameter vector. 
        &quot;&quot;&quot;</span>
        <span class="s1">link = self.link</span>
        <span class="s1">lin_pred = self.predict(params</span><span class="s3">, </span><span class="s1">which=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s1">idl = link.inverse_deriv(lin_pred)</span>
        <span class="s1">dmat = self.exog * idl[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s3">return </span><span class="s1">np.column_stack((dmat</span><span class="s3">, </span><span class="s1">np.zeros(self.exog_precision.shape)))</span>

    <span class="s3">def </span><span class="s1">_deriv_score_obs_dendog(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot;derivative of score_obs w.r.t. endog 
 
        Parameters 
        ---------- 
        params : ndarray 
            parameter at which score is evaluated 
 
        Returns 
        ------- 
        derivative : ndarray_2d 
            The derivative of the score_obs with respect to endog. 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.tools.numdiff </span><span class="s3">import </span><span class="s1">_approx_fprime_cs_scalar</span>

        <span class="s3">def </span><span class="s1">f(y):</span>
            <span class="s3">if </span><span class="s1">y.ndim == </span><span class="s5">2 </span><span class="s3">and </span><span class="s1">y.shape[</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">y = y[:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">sf = self.score_factor(params</span><span class="s3">, </span><span class="s1">endog=y)</span>
            <span class="s3">return </span><span class="s1">np.column_stack(sf)</span>

        <span class="s1">dsf = _approx_fprime_cs_scalar(self.endog[:</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">f)</span>
        <span class="s0"># deriv is 2d vector</span>
        <span class="s1">d1 = dsf[:</span><span class="s3">, </span><span class="s1">:</span><span class="s5">1</span><span class="s1">] * self.exog</span>
        <span class="s1">d2 = dsf[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">:</span><span class="s5">2</span><span class="s1">] * self.exog_precision</span>

        <span class="s3">return </span><span class="s1">np.column_stack((d1</span><span class="s3">, </span><span class="s1">d2))</span>

    <span class="s3">def </span><span class="s1">get_distribution_params(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exog_precision=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Return distribution parameters converted from model prediction. 
 
        Parameters 
        ---------- 
        params : array_like 
            The model parameters. 
        exog : array_like 
            Array of predictor variables for mean. 
        exog_precision : array_like 
            Array of predictor variables for mean. 
 
        Returns 
        ------- 
        (alpha, beta) : tuple of ndarrays 
            Parameters for the scipy distribution to evaluate predictive 
            distribution. 
        &quot;&quot;&quot;</span>
        <span class="s1">mean = self.predict(params</span><span class="s3">, </span><span class="s1">exog=exog)</span>
        <span class="s1">precision = self.predict(params</span><span class="s3">, </span><span class="s1">exog_precision=exog_precision</span><span class="s3">,</span>
                                 <span class="s1">which=</span><span class="s4">&quot;precision&quot;</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">precision * mean</span><span class="s3">, </span><span class="s1">precision * (</span><span class="s5">1 </span><span class="s1">- mean)</span>

    <span class="s3">def </span><span class="s1">get_distribution(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exog_precision=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Return a instance of the predictive distribution. 
 
        Parameters 
        ---------- 
        params : array_like 
            The model parameters. 
        exog : array_like 
            Array of predictor variables for mean. 
        exog_precision : array_like 
            Array of predictor variables for mean. 
 
        Returns 
        ------- 
        Instance of a scipy frozen distribution based on estimated 
        parameters. 
 
        See Also 
        -------- 
        predict 
 
        Notes 
        ----- 
        This function delegates to the predict method to handle exog and 
        exog_precision, which in turn makes any required transformations. 
 
        Due to the behavior of ``scipy.stats.distributions objects``, the 
        returned random number generator must be called with ``gen.rvs(n)`` 
        where ``n`` is the number of observations in the data set used 
        to fit the model.  If any other value is used for ``n``, misleading 
        results will be produced. 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">stats</span>
        <span class="s1">args = self.get_distribution_params(params</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">,</span>
                                            <span class="s1">exog_precision=exog_precision)</span>
        <span class="s1">distr = stats.beta(*args)</span>
        <span class="s3">return </span><span class="s1">distr</span>


<span class="s3">class </span><span class="s1">BetaResults(GenericLikelihoodModelResults</span><span class="s3">, </span><span class="s1">_LLRMixin):</span>
    <span class="s2">&quot;&quot;&quot;Results class for Beta regression 
 
    This class inherits from GenericLikelihoodModelResults and not all 
    inherited methods might be appropriate in this case. 
    &quot;&quot;&quot;</span>

    <span class="s0"># GenericLikeihoodmodel doesn't define fittedvalues, residuals and similar</span>
    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">fittedvalues(self):</span>
        <span class="s2">&quot;&quot;&quot;In-sample predicted mean, conditional expectation.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.model.predict(self.params)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">fitted_precision(self):</span>
        <span class="s2">&quot;&quot;&quot;In-sample predicted precision&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.model.predict(self.params</span><span class="s3">, </span><span class="s1">which=</span><span class="s4">&quot;precision&quot;</span><span class="s1">)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid(self):</span>
        <span class="s2">&quot;&quot;&quot;Response residual&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.model.endog - self.fittedvalues</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid_pearson(self):</span>
        <span class="s2">&quot;&quot;&quot;Pearson standardize residual&quot;&quot;&quot;</span>
        <span class="s1">std = np.sqrt(self.model.predict(self.params</span><span class="s3">, </span><span class="s1">which=</span><span class="s4">&quot;var&quot;</span><span class="s1">))</span>
        <span class="s3">return </span><span class="s1">self.resid / std</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">prsquared(self):</span>
        <span class="s2">&quot;&quot;&quot;Cox-Snell Likelihood-Ratio pseudo-R-squared. 
 
        1 - exp((llnull - .llf) * (2 / nobs)) 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.pseudo_rsquared(kind=</span><span class="s4">&quot;lr&quot;</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">get_distribution_params(self</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exog_precision=</span><span class="s3">None,</span>
                                <span class="s1">transform=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Return distribution parameters converted from model prediction. 
 
        Parameters 
        ---------- 
        params : array_like 
            The model parameters. 
        exog : array_like 
            Array of predictor variables for mean. 
        transform : bool 
            If transform is True and formulas have been used, then predictor 
            ``exog`` is passed through the formula processing. Default is True. 
 
        Returns 
        ------- 
        (alpha, beta) : tuple of ndarrays 
            Parameters for the scipy distribution to evaluate predictive 
            distribution. 
        &quot;&quot;&quot;</span>
        <span class="s1">mean = self.predict(exog=exog</span><span class="s3">, </span><span class="s1">transform=transform)</span>
        <span class="s1">precision = self.predict(exog_precision=exog_precision</span><span class="s3">,</span>
                                 <span class="s1">which=</span><span class="s4">&quot;precision&quot;</span><span class="s3">, </span><span class="s1">transform=transform)</span>
        <span class="s3">return </span><span class="s1">precision * mean</span><span class="s3">, </span><span class="s1">precision * (</span><span class="s5">1 </span><span class="s1">- mean)</span>

    <span class="s3">def </span><span class="s1">get_distribution(self</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">exog_precision=</span><span class="s3">None, </span><span class="s1">transform=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Return a instance of the predictive distribution. 
 
        Parameters 
        ---------- 
        exog : array_like 
            Array of predictor variables for mean. 
        exog_precision : array_like 
            Array of predictor variables for mean. 
        transform : bool 
            If transform is True and formulas have been used, then predictor 
            ``exog`` is passed through the formula processing. Default is True. 
 
        Returns 
        ------- 
        Instance of a scipy frozen distribution based on estimated 
        parameters. 
 
        See Also 
        -------- 
        predict 
 
        Notes 
        ----- 
        This function delegates to the predict method to handle exog and 
        exog_precision, which in turn makes any required transformations. 
 
        Due to the behavior of ``scipy.stats.distributions objects``, the 
        returned random number generator must be called with ``gen.rvs(n)`` 
        where ``n`` is the number of observations in the data set used 
        to fit the model.  If any other value is used for ``n``, misleading 
        results will be produced. 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">stats</span>
        <span class="s1">args = self.get_distribution_params(exog=exog</span><span class="s3">,</span>
                                            <span class="s1">exog_precision=exog_precision</span><span class="s3">,</span>
                                            <span class="s1">transform=transform)</span>
        <span class="s1">args = (np.asarray(arg) </span><span class="s3">for </span><span class="s1">arg </span><span class="s3">in </span><span class="s1">args)</span>
        <span class="s1">distr = stats.beta(*args)</span>
        <span class="s3">return </span><span class="s1">distr</span>

    <span class="s3">def </span><span class="s1">get_influence(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Get an instance of MLEInfluence with influence and outlier measures 
 
        Returns 
        ------- 
        infl : MLEInfluence instance 
            The instance has methods to calculate the main influence and 
            outlier measures as attributes. 
 
        See Also 
        -------- 
        statsmodels.stats.outliers_influence.MLEInfluence 
 
        Notes 
        ----- 
        Support for mutli-link and multi-exog models is still experimental 
        in MLEInfluence. Interface and some definitions might still change. 
 
        Note: Difference to R betareg: Betareg has the same general leverage 
        as this model. However, they use a linear approximation hat matrix 
        to scale and studentize influence and residual statistics. 
        MLEInfluence uses the generalized leverage as hat_matrix_diag. 
        Additionally, MLEInfluence uses pearson residuals for residual 
        analusis. 
 
        References 
        ---------- 
        todo 
 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.stats.outliers_influence </span><span class="s3">import </span><span class="s1">MLEInfluence</span>
        <span class="s3">return </span><span class="s1">MLEInfluence(self)</span>

    <span class="s3">def </span><span class="s1">bootstrap(self</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>


<span class="s3">class </span><span class="s1">BetaResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s3">pass</span>


<span class="s1">wrap.populate_wrapper(BetaResultsWrapper</span><span class="s3">,</span>
                      <span class="s1">BetaResults)</span>
</pre>
</body>
</html>