<html>
<head>
<title>kernels.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
kernels.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Module of kernels that are able to handle continuous as well as categorical 
variables (both ordered and unordered). 
 
This is a slight deviation from the current approach in 
statsmodels.nonparametric.kernels where each kernel is a class object. 
 
Having kernel functions rather than classes makes extension to a multivariate 
kernel density estimation much easier. 
 
NOTE: As it is, this module does not interact with the existing API 
&quot;&quot;&quot;</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy.special </span><span class="s2">import </span><span class="s1">erf</span>


<span class="s3">#TODO:</span>
<span class="s3"># - make sure we only receive int input for wang-ryzin and aitchison-aitken</span>
<span class="s3"># - Check for the scalar Xi case everywhere</span>


<span class="s2">def </span><span class="s1">aitchison_aitken(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">num_levels=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0">r&quot;&quot;&quot; 
    The Aitchison-Aitken kernel, used for unordered discrete random variables. 
 
    Parameters 
    ---------- 
    h : 1-D ndarray, shape (K,) 
        The bandwidths used to estimate the value of the kernel function. 
    Xi : 2-D ndarray of ints, shape (nobs, K) 
        The value of the training set. 
    x : 1-D ndarray, shape (K,) 
        The value at which the kernel density is being estimated. 
    num_levels : bool, optional 
        Gives the user the option to specify the number of levels for the 
        random variable.  If False, the number of levels is calculated from 
        the data. 
 
    Returns 
    ------- 
    kernel_value : ndarray, shape (nobs, K) 
        The value of the kernel function at each training point for each var. 
 
    Notes 
    ----- 
    See p.18 of [2]_ for details.  The value of the kernel L if :math:`X_{i}=x` 
    is :math:`1-\lambda`, otherwise it is :math:`\frac{\lambda}{c-1}`. 
    Here :math:`c` is the number of levels plus one of the RV. 
 
    References 
    ---------- 
    .. [*] J. Aitchison and C.G.G. Aitken, &quot;Multivariate binary discrimination 
           by the kernel method&quot;, Biometrika, vol. 63, pp. 413-420, 1976. 
    .. [*] Racine, Jeff. &quot;Nonparametric Econometrics: A Primer,&quot; Foundation 
           and Trends in Econometrics: Vol 3: No 1, pp1-88., 2008. 
    &quot;&quot;&quot;</span>
    <span class="s1">Xi = Xi.reshape(Xi.size)  </span><span class="s3"># seems needed in case Xi is scalar</span>
    <span class="s2">if </span><span class="s1">num_levels </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">num_levels = np.asarray(np.unique(Xi).size)</span>

    <span class="s1">kernel_value = np.ones(Xi.size) * h / (num_levels - </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">idx = Xi == x</span>
    <span class="s1">kernel_value[idx] = (idx * (</span><span class="s4">1 </span><span class="s1">- h))[idx]</span>
    <span class="s2">return </span><span class="s1">kernel_value</span>


<span class="s2">def </span><span class="s1">wang_ryzin(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x):</span>
    <span class="s0">r&quot;&quot;&quot; 
    The Wang-Ryzin kernel, used for ordered discrete random variables. 
 
    Parameters 
    ---------- 
    h : scalar or 1-D ndarray, shape (K,) 
        The bandwidths used to estimate the value of the kernel function. 
    Xi : ndarray of ints, shape (nobs, K) 
        The value of the training set. 
    x : scalar or 1-D ndarray of shape (K,) 
        The value at which the kernel density is being estimated. 
 
    Returns 
    ------- 
    kernel_value : ndarray, shape (nobs, K) 
        The value of the kernel function at each training point for each var. 
 
    Notes 
    ----- 
    See p. 19 in [1]_ for details.  The value of the kernel L if 
    :math:`X_{i}=x` is :math:`1-\lambda`, otherwise it is 
    :math:`\frac{1-\lambda}{2}\lambda^{|X_{i}-x|}`, where :math:`\lambda` is 
    the bandwidth. 
 
    References 
    ---------- 
    .. [*] Racine, Jeff. &quot;Nonparametric Econometrics: A Primer,&quot; Foundation 
           and Trends in Econometrics: Vol 3: No 1, pp1-88., 2008. 
           http://dx.doi.org/10.1561/0800000009 
    .. [*] M.-C. Wang and J. van Ryzin, &quot;A class of smooth estimators for 
           discrete distributions&quot;, Biometrika, vol. 68, pp. 301-309, 1981. 
    &quot;&quot;&quot;</span>
    <span class="s1">Xi = Xi.reshape(Xi.size)  </span><span class="s3"># seems needed in case Xi is scalar</span>
    <span class="s1">kernel_value = </span><span class="s4">0.5 </span><span class="s1">* (</span><span class="s4">1 </span><span class="s1">- h) * (h ** abs(Xi - x))</span>
    <span class="s1">idx = Xi == x</span>
    <span class="s1">kernel_value[idx] = (idx * (</span><span class="s4">1 </span><span class="s1">- h))[idx]</span>
    <span class="s2">return </span><span class="s1">kernel_value</span>


<span class="s2">def </span><span class="s1">gaussian(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x):</span>
    <span class="s0">&quot;&quot;&quot; 
    Gaussian Kernel for continuous variables 
    Parameters 
    ---------- 
    h : 1-D ndarray, shape (K,) 
        The bandwidths used to estimate the value of the kernel function. 
    Xi : 1-D ndarray, shape (K,) 
        The value of the training set. 
    x : 1-D ndarray, shape (K,) 
        The value at which the kernel density is being estimated. 
 
    Returns 
    ------- 
    kernel_value : ndarray, shape (nobs, K) 
        The value of the kernel function at each training point for each var. 
    &quot;&quot;&quot;</span>
    <span class="s2">return </span><span class="s1">(</span><span class="s4">1. </span><span class="s1">/ np.sqrt(</span><span class="s4">2 </span><span class="s1">* np.pi)) * np.exp(-(Xi - x)**</span><span class="s4">2 </span><span class="s1">/ (h**</span><span class="s4">2 </span><span class="s1">* </span><span class="s4">2.</span><span class="s1">))</span>


<span class="s2">def </span><span class="s1">tricube(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x):</span>
    <span class="s0">&quot;&quot;&quot; 
    Tricube Kernel for continuous variables 
    Parameters 
    ---------- 
    h : 1-D ndarray, shape (K,) 
        The bandwidths used to estimate the value of the kernel function. 
    Xi : 1-D ndarray, shape (K,) 
        The value of the training set. 
    x : 1-D ndarray, shape (K,) 
        The value at which the kernel density is being estimated. 
 
    Returns 
    ------- 
    kernel_value : ndarray, shape (nobs, K) 
        The value of the kernel function at each training point for each var. 
    &quot;&quot;&quot;</span>
    <span class="s1">u = (Xi - x) / h</span>
    <span class="s1">u[np.abs(u) &gt; </span><span class="s4">1</span><span class="s1">] = </span><span class="s4">0</span>
    <span class="s2">return </span><span class="s1">(</span><span class="s4">70. </span><span class="s1">/ </span><span class="s4">81</span><span class="s1">) * (</span><span class="s4">1 </span><span class="s1">- np.abs(u)**</span><span class="s4">3</span><span class="s1">)**</span><span class="s4">3</span>


<span class="s2">def </span><span class="s1">gaussian_convolution(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x):</span>
    <span class="s0">&quot;&quot;&quot; Calculates the Gaussian Convolution Kernel &quot;&quot;&quot;</span>
    <span class="s2">return </span><span class="s1">(</span><span class="s4">1. </span><span class="s1">/ np.sqrt(</span><span class="s4">4 </span><span class="s1">* np.pi)) * np.exp(- (Xi - x)**</span><span class="s4">2 </span><span class="s1">/ (h**</span><span class="s4">2 </span><span class="s1">* </span><span class="s4">4.</span><span class="s1">))</span>


<span class="s2">def </span><span class="s1">wang_ryzin_convolution(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">Xj):</span>
    <span class="s3"># This is the equivalent of the convolution case with the Gaussian Kernel</span>
    <span class="s3"># However it is not exactly convolution. Think of a better name</span>
    <span class="s3"># References</span>
    <span class="s1">ordered = np.zeros(Xi.size)</span>
    <span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">np.unique(Xi):</span>
        <span class="s1">ordered += wang_ryzin(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x) * wang_ryzin(h</span><span class="s2">, </span><span class="s1">Xj</span><span class="s2">, </span><span class="s1">x)</span>

    <span class="s2">return </span><span class="s1">ordered</span>


<span class="s2">def </span><span class="s1">aitchison_aitken_convolution(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">Xj):</span>
    <span class="s1">Xi_vals = np.unique(Xi)</span>
    <span class="s1">ordered = np.zeros(Xi.size)</span>
    <span class="s1">num_levels = Xi_vals.size</span>
    <span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">Xi_vals:</span>
        <span class="s1">ordered += aitchison_aitken(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">num_levels=num_levels) * \</span>
                   <span class="s1">aitchison_aitken(h</span><span class="s2">, </span><span class="s1">Xj</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">num_levels=num_levels)</span>

    <span class="s2">return </span><span class="s1">ordered</span>


<span class="s2">def </span><span class="s1">gaussian_cdf(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x):</span>
    <span class="s2">return </span><span class="s4">0.5 </span><span class="s1">* h * (</span><span class="s4">1 </span><span class="s1">+ erf((x - Xi) / (h * np.sqrt(</span><span class="s4">2</span><span class="s1">))))</span>


<span class="s2">def </span><span class="s1">aitchison_aitken_cdf(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x_u):</span>
    <span class="s1">x_u = int(x_u)</span>
    <span class="s1">Xi_vals = np.unique(Xi)</span>
    <span class="s1">ordered = np.zeros(Xi.size)</span>
    <span class="s1">num_levels = Xi_vals.size</span>
    <span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">Xi_vals:</span>
        <span class="s2">if </span><span class="s1">x &lt;= x_u:  </span><span class="s3">#FIXME: why a comparison for unordered variables?</span>
            <span class="s1">ordered += aitchison_aitken(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">num_levels=num_levels)</span>

    <span class="s2">return </span><span class="s1">ordered</span>


<span class="s2">def </span><span class="s1">wang_ryzin_cdf(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x_u):</span>
    <span class="s1">ordered = np.zeros(Xi.size)</span>
    <span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">np.unique(Xi):</span>
        <span class="s2">if </span><span class="s1">x &lt;= x_u:</span>
            <span class="s1">ordered += wang_ryzin(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x)</span>

    <span class="s2">return </span><span class="s1">ordered</span>


<span class="s2">def </span><span class="s1">d_gaussian(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x):</span>
    <span class="s3"># The derivative of the Gaussian Kernel</span>
    <span class="s2">return </span><span class="s4">2 </span><span class="s1">* (Xi - x) * gaussian(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x) / h**</span><span class="s4">2</span>


<span class="s2">def </span><span class="s1">aitchison_aitken_reg(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x):</span>
    <span class="s0">&quot;&quot;&quot; 
    A version for the Aitchison-Aitken kernel for nonparametric regression. 
 
    Suggested by Li and Racine. 
    &quot;&quot;&quot;</span>
    <span class="s1">kernel_value = np.ones(Xi.size)</span>
    <span class="s1">ix = Xi != x</span>
    <span class="s1">inDom = ix * h</span>
    <span class="s1">kernel_value[ix] = inDom[ix]</span>
    <span class="s2">return </span><span class="s1">kernel_value</span>


<span class="s2">def </span><span class="s1">wang_ryzin_reg(h</span><span class="s2">, </span><span class="s1">Xi</span><span class="s2">, </span><span class="s1">x):</span>
    <span class="s0">&quot;&quot;&quot; 
    A version for the Wang-Ryzin kernel for nonparametric regression. 
 
    Suggested by Li and Racine in [1] ch.4 
    &quot;&quot;&quot;</span>
    <span class="s2">return </span><span class="s1">h ** abs(Xi - x)</span>
</pre>
</body>
</html>