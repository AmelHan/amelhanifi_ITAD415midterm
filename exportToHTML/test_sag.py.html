<html>
<head>
<title>test_sag.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_sag.py</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: Danny Sullivan &lt;dbsullivan23@gmail.com&gt;</span>
<span class="s0">#          Tom Dupre la Tour &lt;tom.dupre-la-tour@m4x.org&gt;</span>
<span class="s0">#</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">import </span><span class="s1">math</span>
<span class="s2">import </span><span class="s1">re</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">import </span><span class="s1">scipy.sparse </span><span class="s2">as </span><span class="s1">sp</span>
<span class="s2">from </span><span class="s1">scipy.special </span><span class="s2">import </span><span class="s1">logsumexp</span>

<span class="s2">from </span><span class="s1">sklearn._loss.loss </span><span class="s2">import </span><span class="s1">HalfMultinomialLoss</span>
<span class="s2">from </span><span class="s1">sklearn.base </span><span class="s2">import </span><span class="s1">clone</span>
<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">load_iris</span><span class="s2">, </span><span class="s1">make_blobs</span><span class="s2">, </span><span class="s1">make_classification</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">LogisticRegression</span><span class="s2">, </span><span class="s1">Ridge</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model._base </span><span class="s2">import </span><span class="s1">make_dataset</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model._linear_loss </span><span class="s2">import </span><span class="s1">LinearModelLoss</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model._sag </span><span class="s2">import </span><span class="s1">get_auto_step_size</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model._sag_fast </span><span class="s2">import </span><span class="s1">_multinomial_grad_loss_all_samples</span>
<span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">LabelBinarizer</span><span class="s2">, </span><span class="s1">LabelEncoder</span>
<span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">check_random_state</span><span class="s2">, </span><span class="s1">compute_class_weight</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s2">,</span>
    <span class="s1">assert_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.utils.extmath </span><span class="s2">import </span><span class="s1">row_norms</span>

<span class="s1">iris = load_iris()</span>


<span class="s0"># this is used for sag classification</span>
<span class="s2">def </span><span class="s1">log_dloss(p</span><span class="s2">, </span><span class="s1">y):</span>
    <span class="s1">z = p * y</span>
    <span class="s0"># approximately equal and saves the computation of the log</span>
    <span class="s2">if </span><span class="s1">z &gt; </span><span class="s3">18.0</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">math.exp(-z) * -y</span>
    <span class="s2">if </span><span class="s1">z &lt; -</span><span class="s3">18.0</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">-y</span>
    <span class="s2">return </span><span class="s1">-y / (math.exp(z) + </span><span class="s3">1.0</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">log_loss(p</span><span class="s2">, </span><span class="s1">y):</span>
    <span class="s2">return </span><span class="s1">np.mean(np.log(</span><span class="s3">1.0 </span><span class="s1">+ np.exp(-y * p)))</span>


<span class="s0"># this is used for sag regression</span>
<span class="s2">def </span><span class="s1">squared_dloss(p</span><span class="s2">, </span><span class="s1">y):</span>
    <span class="s2">return </span><span class="s1">p - y</span>


<span class="s2">def </span><span class="s1">squared_loss(p</span><span class="s2">, </span><span class="s1">y):</span>
    <span class="s2">return </span><span class="s1">np.mean(</span><span class="s3">0.5 </span><span class="s1">* (p - y) * (p - y))</span>


<span class="s0"># function for measuring the log loss</span>
<span class="s2">def </span><span class="s1">get_pobj(w</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">myX</span><span class="s2">, </span><span class="s1">myy</span><span class="s2">, </span><span class="s1">loss):</span>
    <span class="s1">w = w.ravel()</span>
    <span class="s1">pred = np.dot(myX</span><span class="s2">, </span><span class="s1">w)</span>
    <span class="s1">p = loss(pred</span><span class="s2">, </span><span class="s1">myy)</span>
    <span class="s1">p += alpha * w.dot(w) / </span><span class="s3">2.0</span>
    <span class="s2">return </span><span class="s1">p</span>


<span class="s2">def </span><span class="s1">sag(</span>
    <span class="s1">X</span><span class="s2">,</span>
    <span class="s1">y</span><span class="s2">,</span>
    <span class="s1">step_size</span><span class="s2">,</span>
    <span class="s1">alpha</span><span class="s2">,</span>
    <span class="s1">n_iter=</span><span class="s3">1</span><span class="s2">,</span>
    <span class="s1">dloss=</span><span class="s2">None,</span>
    <span class="s1">sparse=</span><span class="s2">False,</span>
    <span class="s1">sample_weight=</span><span class="s2">None,</span>
    <span class="s1">fit_intercept=</span><span class="s2">True,</span>
    <span class="s1">saga=</span><span class="s2">False,</span>
<span class="s1">):</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X.shape[</span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">weights = np.zeros(X.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">sum_gradient = np.zeros(X.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">gradient_memory = np.zeros((n_samples</span><span class="s2">, </span><span class="s1">n_features))</span>

    <span class="s1">intercept = </span><span class="s3">0.0</span>
    <span class="s1">intercept_sum_gradient = </span><span class="s3">0.0</span>
    <span class="s1">intercept_gradient_memory = np.zeros(n_samples)</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s3">77</span><span class="s1">)</span>
    <span class="s1">decay = </span><span class="s3">1.0</span>
    <span class="s1">seen = set()</span>

    <span class="s0"># sparse data has a fixed decay of .01</span>
    <span class="s2">if </span><span class="s1">sparse:</span>
        <span class="s1">decay = </span><span class="s3">0.01</span>

    <span class="s2">for </span><span class="s1">epoch </span><span class="s2">in </span><span class="s1">range(n_iter):</span>
        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(n_samples):</span>
            <span class="s1">idx = int(rng.rand() * n_samples)</span>
            <span class="s0"># idx = k</span>
            <span class="s1">entry = X[idx]</span>
            <span class="s1">seen.add(idx)</span>
            <span class="s1">p = np.dot(entry</span><span class="s2">, </span><span class="s1">weights) + intercept</span>
            <span class="s1">gradient = dloss(p</span><span class="s2">, </span><span class="s1">y[idx])</span>
            <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">gradient *= sample_weight[idx]</span>
            <span class="s1">update = entry * gradient + alpha * weights</span>
            <span class="s1">gradient_correction = update - gradient_memory[idx]</span>
            <span class="s1">sum_gradient += gradient_correction</span>
            <span class="s1">gradient_memory[idx] = update</span>
            <span class="s2">if </span><span class="s1">saga:</span>
                <span class="s1">weights -= gradient_correction * step_size * (</span><span class="s3">1 </span><span class="s1">- </span><span class="s3">1.0 </span><span class="s1">/ len(seen))</span>

            <span class="s2">if </span><span class="s1">fit_intercept:</span>
                <span class="s1">gradient_correction = gradient - intercept_gradient_memory[idx]</span>
                <span class="s1">intercept_gradient_memory[idx] = gradient</span>
                <span class="s1">intercept_sum_gradient += gradient_correction</span>
                <span class="s1">gradient_correction *= step_size * (</span><span class="s3">1.0 </span><span class="s1">- </span><span class="s3">1.0 </span><span class="s1">/ len(seen))</span>
                <span class="s2">if </span><span class="s1">saga:</span>
                    <span class="s1">intercept -= (</span>
                        <span class="s1">step_size * intercept_sum_gradient / len(seen) * decay</span>
                    <span class="s1">) + gradient_correction</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">intercept -= step_size * intercept_sum_gradient / len(seen) * decay</span>

            <span class="s1">weights -= step_size * sum_gradient / len(seen)</span>

    <span class="s2">return </span><span class="s1">weights</span><span class="s2">, </span><span class="s1">intercept</span>


<span class="s2">def </span><span class="s1">sag_sparse(</span>
    <span class="s1">X</span><span class="s2">,</span>
    <span class="s1">y</span><span class="s2">,</span>
    <span class="s1">step_size</span><span class="s2">,</span>
    <span class="s1">alpha</span><span class="s2">,</span>
    <span class="s1">n_iter=</span><span class="s3">1</span><span class="s2">,</span>
    <span class="s1">dloss=</span><span class="s2">None,</span>
    <span class="s1">sample_weight=</span><span class="s2">None,</span>
    <span class="s1">sparse=</span><span class="s2">False,</span>
    <span class="s1">fit_intercept=</span><span class="s2">True,</span>
    <span class="s1">saga=</span><span class="s2">False,</span>
    <span class="s1">random_state=</span><span class="s3">0</span><span class="s2">,</span>
<span class="s1">):</span>
    <span class="s2">if </span><span class="s1">step_size * alpha == </span><span class="s3">1.0</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ZeroDivisionError(</span>
            <span class="s4">&quot;Sparse sag does not handle the case step_size * alpha == 1&quot;</span>
        <span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X.shape[</span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">weights = np.zeros(n_features)</span>
    <span class="s1">sum_gradient = np.zeros(n_features)</span>
    <span class="s1">last_updated = np.zeros(n_features</span><span class="s2">, </span><span class="s1">dtype=int)</span>
    <span class="s1">gradient_memory = np.zeros(n_samples)</span>
    <span class="s1">rng = check_random_state(random_state)</span>
    <span class="s1">intercept = </span><span class="s3">0.0</span>
    <span class="s1">intercept_sum_gradient = </span><span class="s3">0.0</span>
    <span class="s1">wscale = </span><span class="s3">1.0</span>
    <span class="s1">decay = </span><span class="s3">1.0</span>
    <span class="s1">seen = set()</span>

    <span class="s1">c_sum = np.zeros(n_iter * n_samples)</span>

    <span class="s0"># sparse data has a fixed decay of .01</span>
    <span class="s2">if </span><span class="s1">sparse:</span>
        <span class="s1">decay = </span><span class="s3">0.01</span>

    <span class="s1">counter = </span><span class="s3">0</span>
    <span class="s2">for </span><span class="s1">epoch </span><span class="s2">in </span><span class="s1">range(n_iter):</span>
        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(n_samples):</span>
            <span class="s0"># idx = k</span>
            <span class="s1">idx = int(rng.rand() * n_samples)</span>
            <span class="s1">entry = X[idx]</span>
            <span class="s1">seen.add(idx)</span>

            <span class="s2">if </span><span class="s1">counter &gt;= </span><span class="s3">1</span><span class="s1">:</span>
                <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(n_features):</span>
                    <span class="s2">if </span><span class="s1">last_updated[j] == </span><span class="s3">0</span><span class="s1">:</span>
                        <span class="s1">weights[j] -= c_sum[counter - </span><span class="s3">1</span><span class="s1">] * sum_gradient[j]</span>
                    <span class="s2">else</span><span class="s1">:</span>
                        <span class="s1">weights[j] -= (</span>
                            <span class="s1">c_sum[counter - </span><span class="s3">1</span><span class="s1">] - c_sum[last_updated[j] - </span><span class="s3">1</span><span class="s1">]</span>
                        <span class="s1">) * sum_gradient[j]</span>
                    <span class="s1">last_updated[j] = counter</span>

            <span class="s1">p = (wscale * np.dot(entry</span><span class="s2">, </span><span class="s1">weights)) + intercept</span>
            <span class="s1">gradient = dloss(p</span><span class="s2">, </span><span class="s1">y[idx])</span>

            <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">gradient *= sample_weight[idx]</span>

            <span class="s1">update = entry * gradient</span>
            <span class="s1">gradient_correction = update - (gradient_memory[idx] * entry)</span>
            <span class="s1">sum_gradient += gradient_correction</span>
            <span class="s2">if </span><span class="s1">saga:</span>
                <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(n_features):</span>
                    <span class="s1">weights[j] -= (</span>
                        <span class="s1">gradient_correction[j]</span>
                        <span class="s1">* step_size</span>
                        <span class="s1">* (</span><span class="s3">1 </span><span class="s1">- </span><span class="s3">1.0 </span><span class="s1">/ len(seen))</span>
                        <span class="s1">/ wscale</span>
                    <span class="s1">)</span>

            <span class="s2">if </span><span class="s1">fit_intercept:</span>
                <span class="s1">gradient_correction = gradient - gradient_memory[idx]</span>
                <span class="s1">intercept_sum_gradient += gradient_correction</span>
                <span class="s1">gradient_correction *= step_size * (</span><span class="s3">1.0 </span><span class="s1">- </span><span class="s3">1.0 </span><span class="s1">/ len(seen))</span>
                <span class="s2">if </span><span class="s1">saga:</span>
                    <span class="s1">intercept -= (</span>
                        <span class="s1">step_size * intercept_sum_gradient / len(seen) * decay</span>
                    <span class="s1">) + gradient_correction</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">intercept -= step_size * intercept_sum_gradient / len(seen) * decay</span>

            <span class="s1">gradient_memory[idx] = gradient</span>

            <span class="s1">wscale *= </span><span class="s3">1.0 </span><span class="s1">- alpha * step_size</span>
            <span class="s2">if </span><span class="s1">counter == </span><span class="s3">0</span><span class="s1">:</span>
                <span class="s1">c_sum[</span><span class="s3">0</span><span class="s1">] = step_size / (wscale * len(seen))</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">c_sum[counter] = c_sum[counter - </span><span class="s3">1</span><span class="s1">] + step_size / (wscale * len(seen))</span>

            <span class="s2">if </span><span class="s1">counter &gt;= </span><span class="s3">1 </span><span class="s2">and </span><span class="s1">wscale &lt; </span><span class="s3">1e-9</span><span class="s1">:</span>
                <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(n_features):</span>
                    <span class="s2">if </span><span class="s1">last_updated[j] == </span><span class="s3">0</span><span class="s1">:</span>
                        <span class="s1">weights[j] -= c_sum[counter] * sum_gradient[j]</span>
                    <span class="s2">else</span><span class="s1">:</span>
                        <span class="s1">weights[j] -= (</span>
                            <span class="s1">c_sum[counter] - c_sum[last_updated[j] - </span><span class="s3">1</span><span class="s1">]</span>
                        <span class="s1">) * sum_gradient[j]</span>
                    <span class="s1">last_updated[j] = counter + </span><span class="s3">1</span>
                <span class="s1">c_sum[counter] = </span><span class="s3">0</span>
                <span class="s1">weights *= wscale</span>
                <span class="s1">wscale = </span><span class="s3">1.0</span>

            <span class="s1">counter += </span><span class="s3">1</span>

    <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(n_features):</span>
        <span class="s2">if </span><span class="s1">last_updated[j] == </span><span class="s3">0</span><span class="s1">:</span>
            <span class="s1">weights[j] -= c_sum[counter - </span><span class="s3">1</span><span class="s1">] * sum_gradient[j]</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">weights[j] -= (</span>
                <span class="s1">c_sum[counter - </span><span class="s3">1</span><span class="s1">] - c_sum[last_updated[j] - </span><span class="s3">1</span><span class="s1">]</span>
            <span class="s1">) * sum_gradient[j]</span>
    <span class="s1">weights *= wscale</span>
    <span class="s2">return </span><span class="s1">weights</span><span class="s2">, </span><span class="s1">intercept</span>


<span class="s2">def </span><span class="s1">get_step_size(X</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">fit_intercept</span><span class="s2">, </span><span class="s1">classification=</span><span class="s2">True</span><span class="s1">):</span>
    <span class="s2">if </span><span class="s1">classification:</span>
        <span class="s2">return </span><span class="s3">4.0 </span><span class="s1">/ (np.max(np.sum(X * X</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)) + fit_intercept + </span><span class="s3">4.0 </span><span class="s1">* alpha)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s3">1.0 </span><span class="s1">/ (np.max(np.sum(X * X</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)) + fit_intercept + alpha)</span>


<span class="s2">def </span><span class="s1">test_classifier_matching():</span>
    <span class="s1">n_samples = </span><span class="s3">20</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(n_samples=n_samples</span><span class="s2">, </span><span class="s1">centers=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">cluster_std=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">y[y == </span><span class="s3">0</span><span class="s1">] = -</span><span class="s3">1</span>
    <span class="s1">alpha = </span><span class="s3">1.1</span>
    <span class="s1">fit_intercept = </span><span class="s2">True</span>
    <span class="s1">step_size = get_step_size(X</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">fit_intercept)</span>
    <span class="s2">for </span><span class="s1">solver </span><span class="s2">in </span><span class="s1">[</span><span class="s4">&quot;sag&quot;</span><span class="s2">, </span><span class="s4">&quot;saga&quot;</span><span class="s1">]:</span>
        <span class="s2">if </span><span class="s1">solver == </span><span class="s4">&quot;sag&quot;</span><span class="s1">:</span>
            <span class="s1">n_iter = </span><span class="s3">80</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s0"># SAGA variance w.r.t. stream order is higher</span>
            <span class="s1">n_iter = </span><span class="s3">300</span>
        <span class="s1">clf = LogisticRegression(</span>
            <span class="s1">solver=solver</span><span class="s2">,</span>
            <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
            <span class="s1">tol=</span><span class="s3">1e-11</span><span class="s2">,</span>
            <span class="s1">C=</span><span class="s3">1.0 </span><span class="s1">/ alpha / n_samples</span><span class="s2">,</span>
            <span class="s1">max_iter=n_iter</span><span class="s2">,</span>
            <span class="s1">random_state=</span><span class="s3">10</span><span class="s2">,</span>
            <span class="s1">multi_class=</span><span class="s4">&quot;ovr&quot;</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s1">weights</span><span class="s2">, </span><span class="s1">intercept = sag_sparse(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">y</span><span class="s2">,</span>
            <span class="s1">step_size</span><span class="s2">,</span>
            <span class="s1">alpha</span><span class="s2">,</span>
            <span class="s1">n_iter=n_iter</span><span class="s2">,</span>
            <span class="s1">dloss=log_dloss</span><span class="s2">,</span>
            <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
            <span class="s1">saga=solver == </span><span class="s4">&quot;saga&quot;</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">weights2</span><span class="s2">, </span><span class="s1">intercept2 = sag(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">y</span><span class="s2">,</span>
            <span class="s1">step_size</span><span class="s2">,</span>
            <span class="s1">alpha</span><span class="s2">,</span>
            <span class="s1">n_iter=n_iter</span><span class="s2">,</span>
            <span class="s1">dloss=log_dloss</span><span class="s2">,</span>
            <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
            <span class="s1">saga=solver == </span><span class="s4">&quot;saga&quot;</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">weights = np.atleast_2d(weights)</span>
        <span class="s1">intercept = np.atleast_1d(intercept)</span>
        <span class="s1">weights2 = np.atleast_2d(weights2)</span>
        <span class="s1">intercept2 = np.atleast_1d(intercept2)</span>

        <span class="s1">assert_array_almost_equal(weights</span><span class="s2">, </span><span class="s1">clf.coef_</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">9</span><span class="s1">)</span>
        <span class="s1">assert_array_almost_equal(intercept</span><span class="s2">, </span><span class="s1">clf.intercept_</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">9</span><span class="s1">)</span>
        <span class="s1">assert_array_almost_equal(weights2</span><span class="s2">, </span><span class="s1">clf.coef_</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">9</span><span class="s1">)</span>
        <span class="s1">assert_array_almost_equal(intercept2</span><span class="s2">, </span><span class="s1">clf.intercept_</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">9</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_regressor_matching():</span>
    <span class="s1">n_samples = </span><span class="s3">10</span>
    <span class="s1">n_features = </span><span class="s3">5</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s2">, </span><span class="s1">n_features))</span>
    <span class="s1">true_w = rng.normal(size=n_features)</span>
    <span class="s1">y = X.dot(true_w)</span>

    <span class="s1">alpha = </span><span class="s3">1.0</span>
    <span class="s1">n_iter = </span><span class="s3">100</span>
    <span class="s1">fit_intercept = </span><span class="s2">True</span>

    <span class="s1">step_size = get_step_size(X</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">fit_intercept</span><span class="s2">, </span><span class="s1">classification=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">clf = Ridge(</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s3">0.00000000001</span><span class="s2">,</span>
        <span class="s1">solver=</span><span class="s4">&quot;sag&quot;</span><span class="s2">,</span>
        <span class="s1">alpha=alpha * n_samples</span><span class="s2">,</span>
        <span class="s1">max_iter=n_iter</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">weights1</span><span class="s2">, </span><span class="s1">intercept1 = sag_sparse(</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">y</span><span class="s2">,</span>
        <span class="s1">step_size</span><span class="s2">,</span>
        <span class="s1">alpha</span><span class="s2">,</span>
        <span class="s1">n_iter=n_iter</span><span class="s2">,</span>
        <span class="s1">dloss=squared_dloss</span><span class="s2">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">weights2</span><span class="s2">, </span><span class="s1">intercept2 = sag(</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">y</span><span class="s2">,</span>
        <span class="s1">step_size</span><span class="s2">,</span>
        <span class="s1">alpha</span><span class="s2">,</span>
        <span class="s1">n_iter=n_iter</span><span class="s2">,</span>
        <span class="s1">dloss=squared_dloss</span><span class="s2">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">assert_allclose(weights1</span><span class="s2">, </span><span class="s1">clf.coef_)</span>
    <span class="s1">assert_allclose(intercept1</span><span class="s2">, </span><span class="s1">clf.intercept_)</span>
    <span class="s1">assert_allclose(weights2</span><span class="s2">, </span><span class="s1">clf.coef_)</span>
    <span class="s1">assert_allclose(intercept2</span><span class="s2">, </span><span class="s1">clf.intercept_)</span>


<span class="s1">@pytest.mark.filterwarnings(</span><span class="s4">&quot;ignore:The max_iter was reached&quot;</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_sag_pobj_matches_logistic_regression():</span>
    <span class="s5">&quot;&quot;&quot;tests if the sag pobj matches log reg&quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s3">100</span>
    <span class="s1">alpha = </span><span class="s3">1.0</span>
    <span class="s1">max_iter = </span><span class="s3">20</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(n_samples=n_samples</span><span class="s2">, </span><span class="s1">centers=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">cluster_std=</span><span class="s3">0.1</span><span class="s1">)</span>

    <span class="s1">clf1 = LogisticRegression(</span>
        <span class="s1">solver=</span><span class="s4">&quot;sag&quot;</span><span class="s2">,</span>
        <span class="s1">fit_intercept=</span><span class="s2">False,</span>
        <span class="s1">tol=</span><span class="s3">0.0000001</span><span class="s2">,</span>
        <span class="s1">C=</span><span class="s3">1.0 </span><span class="s1">/ alpha / n_samples</span><span class="s2">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">10</span><span class="s2">,</span>
        <span class="s1">multi_class=</span><span class="s4">&quot;ovr&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">clf2 = clone(clf1)</span>
    <span class="s1">clf3 = LogisticRegression(</span>
        <span class="s1">fit_intercept=</span><span class="s2">False,</span>
        <span class="s1">tol=</span><span class="s3">0.0000001</span><span class="s2">,</span>
        <span class="s1">C=</span><span class="s3">1.0 </span><span class="s1">/ alpha / n_samples</span><span class="s2">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">10</span><span class="s2">,</span>
        <span class="s1">multi_class=</span><span class="s4">&quot;ovr&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf2.fit(sp.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf3.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">pobj1 = get_pobj(clf1.coef_</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">log_loss)</span>
    <span class="s1">pobj2 = get_pobj(clf2.coef_</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">log_loss)</span>
    <span class="s1">pobj3 = get_pobj(clf3.coef_</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">log_loss)</span>

    <span class="s1">assert_array_almost_equal(pobj1</span><span class="s2">, </span><span class="s1">pobj2</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">4</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(pobj2</span><span class="s2">, </span><span class="s1">pobj3</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">4</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(pobj3</span><span class="s2">, </span><span class="s1">pobj1</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">4</span><span class="s1">)</span>


<span class="s1">@pytest.mark.filterwarnings(</span><span class="s4">&quot;ignore:The max_iter was reached&quot;</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_sag_pobj_matches_ridge_regression():</span>
    <span class="s5">&quot;&quot;&quot;tests if the sag pobj matches ridge reg&quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s3">100</span>
    <span class="s1">n_features = </span><span class="s3">10</span>
    <span class="s1">alpha = </span><span class="s3">1.0</span>
    <span class="s1">n_iter = </span><span class="s3">100</span>
    <span class="s1">fit_intercept = </span><span class="s2">False</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s2">, </span><span class="s1">n_features))</span>
    <span class="s1">true_w = rng.normal(size=n_features)</span>
    <span class="s1">y = X.dot(true_w)</span>

    <span class="s1">clf1 = Ridge(</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s3">0.00000000001</span><span class="s2">,</span>
        <span class="s1">solver=</span><span class="s4">&quot;sag&quot;</span><span class="s2">,</span>
        <span class="s1">alpha=alpha</span><span class="s2">,</span>
        <span class="s1">max_iter=n_iter</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">42</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">clf2 = clone(clf1)</span>
    <span class="s1">clf3 = Ridge(</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s3">0.00001</span><span class="s2">,</span>
        <span class="s1">solver=</span><span class="s4">&quot;lsqr&quot;</span><span class="s2">,</span>
        <span class="s1">alpha=alpha</span><span class="s2">,</span>
        <span class="s1">max_iter=n_iter</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">42</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf2.fit(sp.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf3.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">pobj1 = get_pobj(clf1.coef_</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">squared_loss)</span>
    <span class="s1">pobj2 = get_pobj(clf2.coef_</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">squared_loss)</span>
    <span class="s1">pobj3 = get_pobj(clf3.coef_</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">squared_loss)</span>

    <span class="s1">assert_array_almost_equal(pobj1</span><span class="s2">, </span><span class="s1">pobj2</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">4</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(pobj1</span><span class="s2">, </span><span class="s1">pobj3</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">4</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(pobj3</span><span class="s2">, </span><span class="s1">pobj2</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">4</span><span class="s1">)</span>


<span class="s1">@pytest.mark.filterwarnings(</span><span class="s4">&quot;ignore:The max_iter was reached&quot;</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_sag_regressor_computed_correctly():</span>
    <span class="s5">&quot;&quot;&quot;tests if the sag regressor is computed correctly&quot;&quot;&quot;</span>
    <span class="s1">alpha = </span><span class="s3">0.1</span>
    <span class="s1">n_features = </span><span class="s3">10</span>
    <span class="s1">n_samples = </span><span class="s3">40</span>
    <span class="s1">max_iter = </span><span class="s3">100</span>
    <span class="s1">tol = </span><span class="s3">0.000001</span>
    <span class="s1">fit_intercept = </span><span class="s2">True</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s2">, </span><span class="s1">n_features))</span>
    <span class="s1">w = rng.normal(size=n_features)</span>
    <span class="s1">y = np.dot(X</span><span class="s2">, </span><span class="s1">w) + </span><span class="s3">2.0</span>
    <span class="s1">step_size = get_step_size(X</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">fit_intercept</span><span class="s2">, </span><span class="s1">classification=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s1">clf1 = Ridge(</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">tol=tol</span><span class="s2">,</span>
        <span class="s1">solver=</span><span class="s4">&quot;sag&quot;</span><span class="s2">,</span>
        <span class="s1">alpha=alpha * n_samples</span><span class="s2">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
        <span class="s1">random_state=rng</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">clf2 = clone(clf1)</span>

    <span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf2.fit(sp.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">spweights1</span><span class="s2">, </span><span class="s1">spintercept1 = sag_sparse(</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">y</span><span class="s2">,</span>
        <span class="s1">step_size</span><span class="s2">,</span>
        <span class="s1">alpha</span><span class="s2">,</span>
        <span class="s1">n_iter=max_iter</span><span class="s2">,</span>
        <span class="s1">dloss=squared_dloss</span><span class="s2">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">random_state=rng</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">spweights2</span><span class="s2">, </span><span class="s1">spintercept2 = sag_sparse(</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">y</span><span class="s2">,</span>
        <span class="s1">step_size</span><span class="s2">,</span>
        <span class="s1">alpha</span><span class="s2">,</span>
        <span class="s1">n_iter=max_iter</span><span class="s2">,</span>
        <span class="s1">dloss=squared_dloss</span><span class="s2">,</span>
        <span class="s1">sparse=</span><span class="s2">True,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">random_state=rng</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(clf1.coef_.ravel()</span><span class="s2">, </span><span class="s1">spweights1.ravel()</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf1.intercept_</span><span class="s2">, </span><span class="s1">spintercept1</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s0"># TODO: uncomment when sparse Ridge with intercept will be fixed (#4710)</span>
    <span class="s0"># assert_array_almost_equal(clf2.coef_.ravel(),</span>
    <span class="s0">#                          spweights2.ravel(),</span>
    <span class="s0">#                          decimal=3)</span>
    <span class="s0"># assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)'''</span>


<span class="s2">def </span><span class="s1">test_get_auto_step_size():</span>
    <span class="s1">X = np.array([[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s3">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s1">alpha = </span><span class="s3">1.2</span>
    <span class="s1">fit_intercept = </span><span class="s2">False</span>
    <span class="s0"># sum the squares of the second sample because that's the largest</span>
    <span class="s1">max_squared_sum = </span><span class="s3">4 </span><span class="s1">+ </span><span class="s3">9 </span><span class="s1">+ </span><span class="s3">16</span>
    <span class="s1">max_squared_sum_ = row_norms(X</span><span class="s2">, </span><span class="s1">squared=</span><span class="s2">True</span><span class="s1">).max()</span>
    <span class="s1">n_samples = X.shape[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">assert_almost_equal(max_squared_sum</span><span class="s2">, </span><span class="s1">max_squared_sum_</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">4</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">saga </span><span class="s2">in </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">]:</span>
        <span class="s2">for </span><span class="s1">fit_intercept </span><span class="s2">in </span><span class="s1">(</span><span class="s2">True, False</span><span class="s1">):</span>
            <span class="s2">if </span><span class="s1">saga:</span>
                <span class="s1">L_sqr = max_squared_sum + alpha + int(fit_intercept)</span>
                <span class="s1">L_log = (max_squared_sum + </span><span class="s3">4.0 </span><span class="s1">* alpha + int(fit_intercept)) / </span><span class="s3">4.0</span>
                <span class="s1">mun_sqr = min(</span><span class="s3">2 </span><span class="s1">* n_samples * alpha</span><span class="s2">, </span><span class="s1">L_sqr)</span>
                <span class="s1">mun_log = min(</span><span class="s3">2 </span><span class="s1">* n_samples * alpha</span><span class="s2">, </span><span class="s1">L_log)</span>
                <span class="s1">step_size_sqr = </span><span class="s3">1 </span><span class="s1">/ (</span><span class="s3">2 </span><span class="s1">* L_sqr + mun_sqr)</span>
                <span class="s1">step_size_log = </span><span class="s3">1 </span><span class="s1">/ (</span><span class="s3">2 </span><span class="s1">* L_log + mun_log)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">step_size_sqr = </span><span class="s3">1.0 </span><span class="s1">/ (max_squared_sum + alpha + int(fit_intercept))</span>
                <span class="s1">step_size_log = </span><span class="s3">4.0 </span><span class="s1">/ (</span>
                    <span class="s1">max_squared_sum + </span><span class="s3">4.0 </span><span class="s1">* alpha + int(fit_intercept)</span>
                <span class="s1">)</span>

            <span class="s1">step_size_sqr_ = get_auto_step_size(</span>
                <span class="s1">max_squared_sum_</span><span class="s2">,</span>
                <span class="s1">alpha</span><span class="s2">,</span>
                <span class="s4">&quot;squared&quot;</span><span class="s2">,</span>
                <span class="s1">fit_intercept</span><span class="s2">,</span>
                <span class="s1">n_samples=n_samples</span><span class="s2">,</span>
                <span class="s1">is_saga=saga</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">step_size_log_ = get_auto_step_size(</span>
                <span class="s1">max_squared_sum_</span><span class="s2">,</span>
                <span class="s1">alpha</span><span class="s2">,</span>
                <span class="s4">&quot;log&quot;</span><span class="s2">,</span>
                <span class="s1">fit_intercept</span><span class="s2">,</span>
                <span class="s1">n_samples=n_samples</span><span class="s2">,</span>
                <span class="s1">is_saga=saga</span><span class="s2">,</span>
            <span class="s1">)</span>

            <span class="s1">assert_almost_equal(step_size_sqr</span><span class="s2">, </span><span class="s1">step_size_sqr_</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">4</span><span class="s1">)</span>
            <span class="s1">assert_almost_equal(step_size_log</span><span class="s2">, </span><span class="s1">step_size_log_</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">4</span><span class="s1">)</span>

    <span class="s1">msg = </span><span class="s4">&quot;Unknown loss function for SAG solver, got wrong instead of&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">get_auto_step_size(max_squared_sum_</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s4">&quot;wrong&quot;</span><span class="s2">, </span><span class="s1">fit_intercept)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;seed&quot;</span><span class="s2">, </span><span class="s1">range(</span><span class="s3">3</span><span class="s1">))  </span><span class="s0"># locally tested with 1000 seeds</span>
<span class="s2">def </span><span class="s1">test_sag_regressor(seed):</span>
    <span class="s5">&quot;&quot;&quot;tests if the sag regressor performs well&quot;&quot;&quot;</span>
    <span class="s1">xmin</span><span class="s2">, </span><span class="s1">xmax = -</span><span class="s3">5</span><span class="s2">, </span><span class="s3">5</span>
    <span class="s1">n_samples = </span><span class="s3">300</span>
    <span class="s1">tol = </span><span class="s3">0.001</span>
    <span class="s1">max_iter = </span><span class="s3">100</span>
    <span class="s1">alpha = </span><span class="s3">0.1</span>
    <span class="s1">rng = np.random.RandomState(seed)</span>
    <span class="s1">X = np.linspace(xmin</span><span class="s2">, </span><span class="s1">xmax</span><span class="s2">, </span><span class="s1">n_samples).reshape(n_samples</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>

    <span class="s0"># simple linear function without noise</span>
    <span class="s1">y = </span><span class="s3">0.5 </span><span class="s1">* X.ravel()</span>

    <span class="s1">clf1 = Ridge(</span>
        <span class="s1">tol=tol</span><span class="s2">,</span>
        <span class="s1">solver=</span><span class="s4">&quot;sag&quot;</span><span class="s2">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
        <span class="s1">alpha=alpha * n_samples</span><span class="s2">,</span>
        <span class="s1">random_state=rng</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">clf2 = clone(clf1)</span>
    <span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf2.fit(sp.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">score1 = clf1.score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">score2 = clf2.score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">score1 &gt; </span><span class="s3">0.98</span>
    <span class="s2">assert </span><span class="s1">score2 &gt; </span><span class="s3">0.98</span>

    <span class="s0"># simple linear function with noise</span>
    <span class="s1">y = </span><span class="s3">0.5 </span><span class="s1">* X.ravel() + rng.randn(n_samples</span><span class="s2">, </span><span class="s3">1</span><span class="s1">).ravel()</span>

    <span class="s1">clf1 = Ridge(tol=tol</span><span class="s2">, </span><span class="s1">solver=</span><span class="s4">&quot;sag&quot;</span><span class="s2">, </span><span class="s1">max_iter=max_iter</span><span class="s2">, </span><span class="s1">alpha=alpha * n_samples)</span>
    <span class="s1">clf2 = clone(clf1)</span>
    <span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf2.fit(sp.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">score1 = clf1.score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">score2 = clf2.score(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">score1 &gt; </span><span class="s3">0.45</span>
    <span class="s2">assert </span><span class="s1">score2 &gt; </span><span class="s3">0.45</span>


<span class="s1">@pytest.mark.filterwarnings(</span><span class="s4">&quot;ignore:The max_iter was reached&quot;</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_sag_classifier_computed_correctly():</span>
    <span class="s5">&quot;&quot;&quot;tests if the binary classifier is computed correctly&quot;&quot;&quot;</span>
    <span class="s1">alpha = </span><span class="s3">0.1</span>
    <span class="s1">n_samples = </span><span class="s3">50</span>
    <span class="s1">n_iter = </span><span class="s3">50</span>
    <span class="s1">tol = </span><span class="s3">0.00001</span>
    <span class="s1">fit_intercept = </span><span class="s2">True</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(n_samples=n_samples</span><span class="s2">, </span><span class="s1">centers=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">cluster_std=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">step_size = get_step_size(X</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">fit_intercept</span><span class="s2">, </span><span class="s1">classification=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">classes = np.unique(y)</span>
    <span class="s1">y_tmp = np.ones(n_samples)</span>
    <span class="s1">y_tmp[y != classes[</span><span class="s3">1</span><span class="s1">]] = -</span><span class="s3">1</span>
    <span class="s1">y = y_tmp</span>

    <span class="s1">clf1 = LogisticRegression(</span>
        <span class="s1">solver=</span><span class="s4">&quot;sag&quot;</span><span class="s2">,</span>
        <span class="s1">C=</span><span class="s3">1.0 </span><span class="s1">/ alpha / n_samples</span><span class="s2">,</span>
        <span class="s1">max_iter=n_iter</span><span class="s2">,</span>
        <span class="s1">tol=tol</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">77</span><span class="s2">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">multi_class=</span><span class="s4">&quot;ovr&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">clf2 = clone(clf1)</span>

    <span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf2.fit(sp.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">spweights</span><span class="s2">, </span><span class="s1">spintercept = sag_sparse(</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">y</span><span class="s2">,</span>
        <span class="s1">step_size</span><span class="s2">,</span>
        <span class="s1">alpha</span><span class="s2">,</span>
        <span class="s1">n_iter=n_iter</span><span class="s2">,</span>
        <span class="s1">dloss=log_dloss</span><span class="s2">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">spweights2</span><span class="s2">, </span><span class="s1">spintercept2 = sag_sparse(</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">y</span><span class="s2">,</span>
        <span class="s1">step_size</span><span class="s2">,</span>
        <span class="s1">alpha</span><span class="s2">,</span>
        <span class="s1">n_iter=n_iter</span><span class="s2">,</span>
        <span class="s1">dloss=log_dloss</span><span class="s2">,</span>
        <span class="s1">sparse=</span><span class="s2">True,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(clf1.coef_.ravel()</span><span class="s2">, </span><span class="s1">spweights.ravel()</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf1.intercept_</span><span class="s2">, </span><span class="s1">spintercept</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(clf2.coef_.ravel()</span><span class="s2">, </span><span class="s1">spweights2.ravel()</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf2.intercept_</span><span class="s2">, </span><span class="s1">spintercept2</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">1</span><span class="s1">)</span>


<span class="s1">@pytest.mark.filterwarnings(</span><span class="s4">&quot;ignore:The max_iter was reached&quot;</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_sag_multiclass_computed_correctly():</span>
    <span class="s5">&quot;&quot;&quot;tests if the multiclass classifier is computed correctly&quot;&quot;&quot;</span>
    <span class="s1">alpha = </span><span class="s3">0.1</span>
    <span class="s1">n_samples = </span><span class="s3">20</span>
    <span class="s1">tol = </span><span class="s3">0.00001</span>
    <span class="s1">max_iter = </span><span class="s3">40</span>
    <span class="s1">fit_intercept = </span><span class="s2">True</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(n_samples=n_samples</span><span class="s2">, </span><span class="s1">centers=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">cluster_std=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">step_size = get_step_size(X</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">fit_intercept</span><span class="s2">, </span><span class="s1">classification=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">classes = np.unique(y)</span>

    <span class="s1">clf1 = LogisticRegression(</span>
        <span class="s1">solver=</span><span class="s4">&quot;sag&quot;</span><span class="s2">,</span>
        <span class="s1">C=</span><span class="s3">1.0 </span><span class="s1">/ alpha / n_samples</span><span class="s2">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
        <span class="s1">tol=tol</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">77</span><span class="s2">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">multi_class=</span><span class="s4">&quot;ovr&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">clf2 = clone(clf1)</span>

    <span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf2.fit(sp.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">coef1 = []</span>
    <span class="s1">intercept1 = []</span>
    <span class="s1">coef2 = []</span>
    <span class="s1">intercept2 = []</span>
    <span class="s2">for </span><span class="s1">cl </span><span class="s2">in </span><span class="s1">classes:</span>
        <span class="s1">y_encoded = np.ones(n_samples)</span>
        <span class="s1">y_encoded[y != cl] = -</span><span class="s3">1</span>

        <span class="s1">spweights1</span><span class="s2">, </span><span class="s1">spintercept1 = sag_sparse(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">y_encoded</span><span class="s2">,</span>
            <span class="s1">step_size</span><span class="s2">,</span>
            <span class="s1">alpha</span><span class="s2">,</span>
            <span class="s1">dloss=log_dloss</span><span class="s2">,</span>
            <span class="s1">n_iter=max_iter</span><span class="s2">,</span>
            <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">spweights2</span><span class="s2">, </span><span class="s1">spintercept2 = sag_sparse(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">y_encoded</span><span class="s2">,</span>
            <span class="s1">step_size</span><span class="s2">,</span>
            <span class="s1">alpha</span><span class="s2">,</span>
            <span class="s1">dloss=log_dloss</span><span class="s2">,</span>
            <span class="s1">n_iter=max_iter</span><span class="s2">,</span>
            <span class="s1">sparse=</span><span class="s2">True,</span>
            <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">coef1.append(spweights1)</span>
        <span class="s1">intercept1.append(spintercept1)</span>

        <span class="s1">coef2.append(spweights2)</span>
        <span class="s1">intercept2.append(spintercept2)</span>

    <span class="s1">coef1 = np.vstack(coef1)</span>
    <span class="s1">intercept1 = np.array(intercept1)</span>
    <span class="s1">coef2 = np.vstack(coef2)</span>
    <span class="s1">intercept2 = np.array(intercept2)</span>

    <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">cl </span><span class="s2">in </span><span class="s1">enumerate(classes):</span>
        <span class="s1">assert_array_almost_equal(clf1.coef_[i].ravel()</span><span class="s2">, </span><span class="s1">coef1[i].ravel()</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>
        <span class="s1">assert_almost_equal(clf1.intercept_[i]</span><span class="s2">, </span><span class="s1">intercept1[i]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">1</span><span class="s1">)</span>

        <span class="s1">assert_array_almost_equal(clf2.coef_[i].ravel()</span><span class="s2">, </span><span class="s1">coef2[i].ravel()</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>
        <span class="s1">assert_almost_equal(clf2.intercept_[i]</span><span class="s2">, </span><span class="s1">intercept2[i]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">1</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_classifier_results():</span>
    <span class="s5">&quot;&quot;&quot;tests if classifier results match target&quot;&quot;&quot;</span>
    <span class="s1">alpha = </span><span class="s3">0.1</span>
    <span class="s1">n_features = </span><span class="s3">20</span>
    <span class="s1">n_samples = </span><span class="s3">10</span>
    <span class="s1">tol = </span><span class="s3">0.01</span>
    <span class="s1">max_iter = </span><span class="s3">200</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s2">, </span><span class="s1">n_features))</span>
    <span class="s1">w = rng.normal(size=n_features)</span>
    <span class="s1">y = np.dot(X</span><span class="s2">, </span><span class="s1">w)</span>
    <span class="s1">y = np.sign(y)</span>
    <span class="s1">clf1 = LogisticRegression(</span>
        <span class="s1">solver=</span><span class="s4">&quot;sag&quot;</span><span class="s2">,</span>
        <span class="s1">C=</span><span class="s3">1.0 </span><span class="s1">/ alpha / n_samples</span><span class="s2">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
        <span class="s1">tol=tol</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">77</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">clf2 = clone(clf1)</span>

    <span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf2.fit(sp.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">pred1 = clf1.predict(X)</span>
    <span class="s1">pred2 = clf2.predict(X)</span>
    <span class="s1">assert_almost_equal(pred1</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">12</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(pred2</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">12</span><span class="s1">)</span>


<span class="s1">@pytest.mark.filterwarnings(</span><span class="s4">&quot;ignore:The max_iter was reached&quot;</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_binary_classifier_class_weight():</span>
    <span class="s5">&quot;&quot;&quot;tests binary classifier with classweights for each class&quot;&quot;&quot;</span>
    <span class="s1">alpha = </span><span class="s3">0.1</span>
    <span class="s1">n_samples = </span><span class="s3">50</span>
    <span class="s1">n_iter = </span><span class="s3">20</span>
    <span class="s1">tol = </span><span class="s3">0.00001</span>
    <span class="s1">fit_intercept = </span><span class="s2">True</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(n_samples=n_samples</span><span class="s2">, </span><span class="s1">centers=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">cluster_std=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">step_size = get_step_size(X</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">fit_intercept</span><span class="s2">, </span><span class="s1">classification=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">classes = np.unique(y)</span>
    <span class="s1">y_tmp = np.ones(n_samples)</span>
    <span class="s1">y_tmp[y != classes[</span><span class="s3">1</span><span class="s1">]] = -</span><span class="s3">1</span>
    <span class="s1">y = y_tmp</span>

    <span class="s1">class_weight = {</span><span class="s3">1</span><span class="s1">: </span><span class="s3">0.45</span><span class="s2">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">: </span><span class="s3">0.55</span><span class="s1">}</span>
    <span class="s1">clf1 = LogisticRegression(</span>
        <span class="s1">solver=</span><span class="s4">&quot;sag&quot;</span><span class="s2">,</span>
        <span class="s1">C=</span><span class="s3">1.0 </span><span class="s1">/ alpha / n_samples</span><span class="s2">,</span>
        <span class="s1">max_iter=n_iter</span><span class="s2">,</span>
        <span class="s1">tol=tol</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">77</span><span class="s2">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">multi_class=</span><span class="s4">&quot;ovr&quot;</span><span class="s2">,</span>
        <span class="s1">class_weight=class_weight</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">clf2 = clone(clf1)</span>

    <span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf2.fit(sp.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">le = LabelEncoder()</span>
    <span class="s1">class_weight_ = compute_class_weight(class_weight</span><span class="s2">, </span><span class="s1">classes=np.unique(y)</span><span class="s2">, </span><span class="s1">y=y)</span>
    <span class="s1">sample_weight = class_weight_[le.fit_transform(y)]</span>
    <span class="s1">spweights</span><span class="s2">, </span><span class="s1">spintercept = sag_sparse(</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">y</span><span class="s2">,</span>
        <span class="s1">step_size</span><span class="s2">,</span>
        <span class="s1">alpha</span><span class="s2">,</span>
        <span class="s1">n_iter=n_iter</span><span class="s2">,</span>
        <span class="s1">dloss=log_dloss</span><span class="s2">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s2">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">spweights2</span><span class="s2">, </span><span class="s1">spintercept2 = sag_sparse(</span>
        <span class="s1">X</span><span class="s2">,</span>
        <span class="s1">y</span><span class="s2">,</span>
        <span class="s1">step_size</span><span class="s2">,</span>
        <span class="s1">alpha</span><span class="s2">,</span>
        <span class="s1">n_iter=n_iter</span><span class="s2">,</span>
        <span class="s1">dloss=log_dloss</span><span class="s2">,</span>
        <span class="s1">sparse=</span><span class="s2">True,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s2">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(clf1.coef_.ravel()</span><span class="s2">, </span><span class="s1">spweights.ravel()</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf1.intercept_</span><span class="s2">, </span><span class="s1">spintercept</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(clf2.coef_.ravel()</span><span class="s2">, </span><span class="s1">spweights2.ravel()</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf2.intercept_</span><span class="s2">, </span><span class="s1">spintercept2</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">1</span><span class="s1">)</span>


<span class="s1">@pytest.mark.filterwarnings(</span><span class="s4">&quot;ignore:The max_iter was reached&quot;</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_multiclass_classifier_class_weight():</span>
    <span class="s5">&quot;&quot;&quot;tests multiclass with classweights for each class&quot;&quot;&quot;</span>
    <span class="s1">alpha = </span><span class="s3">0.1</span>
    <span class="s1">n_samples = </span><span class="s3">20</span>
    <span class="s1">tol = </span><span class="s3">0.00001</span>
    <span class="s1">max_iter = </span><span class="s3">50</span>
    <span class="s1">class_weight = {</span><span class="s3">0</span><span class="s1">: </span><span class="s3">0.45</span><span class="s2">, </span><span class="s3">1</span><span class="s1">: </span><span class="s3">0.55</span><span class="s2">, </span><span class="s3">2</span><span class="s1">: </span><span class="s3">0.75</span><span class="s1">}</span>
    <span class="s1">fit_intercept = </span><span class="s2">True</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_blobs(n_samples=n_samples</span><span class="s2">, </span><span class="s1">centers=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">cluster_std=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">step_size = get_step_size(X</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">fit_intercept</span><span class="s2">, </span><span class="s1">classification=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">classes = np.unique(y)</span>

    <span class="s1">clf1 = LogisticRegression(</span>
        <span class="s1">solver=</span><span class="s4">&quot;sag&quot;</span><span class="s2">,</span>
        <span class="s1">C=</span><span class="s3">1.0 </span><span class="s1">/ alpha / n_samples</span><span class="s2">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
        <span class="s1">tol=tol</span><span class="s2">,</span>
        <span class="s1">random_state=</span><span class="s3">77</span><span class="s2">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s2">,</span>
        <span class="s1">multi_class=</span><span class="s4">&quot;ovr&quot;</span><span class="s2">,</span>
        <span class="s1">class_weight=class_weight</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">clf2 = clone(clf1)</span>
    <span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">clf2.fit(sp.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">le = LabelEncoder()</span>
    <span class="s1">class_weight_ = compute_class_weight(class_weight</span><span class="s2">, </span><span class="s1">classes=np.unique(y)</span><span class="s2">, </span><span class="s1">y=y)</span>
    <span class="s1">sample_weight = class_weight_[le.fit_transform(y)]</span>

    <span class="s1">coef1 = []</span>
    <span class="s1">intercept1 = []</span>
    <span class="s1">coef2 = []</span>
    <span class="s1">intercept2 = []</span>
    <span class="s2">for </span><span class="s1">cl </span><span class="s2">in </span><span class="s1">classes:</span>
        <span class="s1">y_encoded = np.ones(n_samples)</span>
        <span class="s1">y_encoded[y != cl] = -</span><span class="s3">1</span>

        <span class="s1">spweights1</span><span class="s2">, </span><span class="s1">spintercept1 = sag_sparse(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">y_encoded</span><span class="s2">,</span>
            <span class="s1">step_size</span><span class="s2">,</span>
            <span class="s1">alpha</span><span class="s2">,</span>
            <span class="s1">n_iter=max_iter</span><span class="s2">,</span>
            <span class="s1">dloss=log_dloss</span><span class="s2">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">spweights2</span><span class="s2">, </span><span class="s1">spintercept2 = sag_sparse(</span>
            <span class="s1">X</span><span class="s2">,</span>
            <span class="s1">y_encoded</span><span class="s2">,</span>
            <span class="s1">step_size</span><span class="s2">,</span>
            <span class="s1">alpha</span><span class="s2">,</span>
            <span class="s1">n_iter=max_iter</span><span class="s2">,</span>
            <span class="s1">dloss=log_dloss</span><span class="s2">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s2">,</span>
            <span class="s1">sparse=</span><span class="s2">True,</span>
        <span class="s1">)</span>
        <span class="s1">coef1.append(spweights1)</span>
        <span class="s1">intercept1.append(spintercept1)</span>
        <span class="s1">coef2.append(spweights2)</span>
        <span class="s1">intercept2.append(spintercept2)</span>

    <span class="s1">coef1 = np.vstack(coef1)</span>
    <span class="s1">intercept1 = np.array(intercept1)</span>
    <span class="s1">coef2 = np.vstack(coef2)</span>
    <span class="s1">intercept2 = np.array(intercept2)</span>

    <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">cl </span><span class="s2">in </span><span class="s1">enumerate(classes):</span>
        <span class="s1">assert_array_almost_equal(clf1.coef_[i].ravel()</span><span class="s2">, </span><span class="s1">coef1[i].ravel()</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>
        <span class="s1">assert_almost_equal(clf1.intercept_[i]</span><span class="s2">, </span><span class="s1">intercept1[i]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">1</span><span class="s1">)</span>

        <span class="s1">assert_array_almost_equal(clf2.coef_[i].ravel()</span><span class="s2">, </span><span class="s1">coef2[i].ravel()</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>
        <span class="s1">assert_almost_equal(clf2.intercept_[i]</span><span class="s2">, </span><span class="s1">intercept2[i]</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s3">1</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_classifier_single_class():</span>
    <span class="s5">&quot;&quot;&quot;tests if ValueError is thrown with only one class&quot;&quot;&quot;</span>
    <span class="s1">X = [[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">3</span><span class="s2">, </span><span class="s3">4</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">msg = </span><span class="s4">&quot;This solver needs samples of at least 2 classes in the data&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">LogisticRegression(solver=</span><span class="s4">&quot;sag&quot;</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_step_size_alpha_error():</span>
    <span class="s1">X = [[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s3">1</span><span class="s2">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">fit_intercept = </span><span class="s2">False</span>
    <span class="s1">alpha = </span><span class="s3">1.0</span>
    <span class="s1">msg = re.escape(</span>
        <span class="s4">&quot;Current sag implementation does not handle the case&quot;</span>
        <span class="s4">&quot; step_size * alpha_scaled == 1&quot;</span>
    <span class="s1">)</span>

    <span class="s1">clf1 = LogisticRegression(solver=</span><span class="s4">&quot;sag&quot;</span><span class="s2">, </span><span class="s1">C=</span><span class="s3">1.0 </span><span class="s1">/ alpha</span><span class="s2">, </span><span class="s1">fit_intercept=fit_intercept)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ZeroDivisionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf1.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">clf2 = Ridge(fit_intercept=fit_intercept</span><span class="s2">, </span><span class="s1">solver=</span><span class="s4">&quot;sag&quot;</span><span class="s2">, </span><span class="s1">alpha=alpha)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ZeroDivisionError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf2.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_multinomial_loss():</span>
    <span class="s0"># test if the multinomial loss and gradient computations are consistent</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = iris.data</span><span class="s2">, </span><span class="s1">iris.target.astype(np.float64)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">n_classes = len(np.unique(y))</span>

    <span class="s1">rng = check_random_state(</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">weights = rng.randn(n_features</span><span class="s2">, </span><span class="s1">n_classes)</span>
    <span class="s1">intercept = rng.randn(n_classes)</span>
    <span class="s1">sample_weights = rng.randn(n_samples)</span>
    <span class="s1">np.abs(sample_weights</span><span class="s2">, </span><span class="s1">sample_weights)</span>

    <span class="s0"># compute loss and gradient like in multinomial SAG</span>
    <span class="s1">dataset</span><span class="s2">, </span><span class="s1">_ = make_dataset(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weights</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">loss_1</span><span class="s2">, </span><span class="s1">grad_1 = _multinomial_grad_loss_all_samples(</span>
        <span class="s1">dataset</span><span class="s2">, </span><span class="s1">weights</span><span class="s2">, </span><span class="s1">intercept</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_classes</span>
    <span class="s1">)</span>
    <span class="s0"># compute loss and gradient like in multinomial LogisticRegression</span>
    <span class="s1">loss = LinearModelLoss(</span>
        <span class="s1">base_loss=HalfMultinomialLoss(n_classes=n_classes)</span><span class="s2">,</span>
        <span class="s1">fit_intercept=</span><span class="s2">True,</span>
    <span class="s1">)</span>
    <span class="s1">weights_intercept = np.vstack((weights</span><span class="s2">, </span><span class="s1">intercept)).T</span>
    <span class="s1">loss_2</span><span class="s2">, </span><span class="s1">grad_2 = loss.loss_gradient(</span>
        <span class="s1">weights_intercept</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">l2_reg_strength=</span><span class="s3">0.0</span><span class="s2">, </span><span class="s1">sample_weight=sample_weights</span>
    <span class="s1">)</span>
    <span class="s1">grad_2 = grad_2[:</span><span class="s2">, </span><span class="s1">:-</span><span class="s3">1</span><span class="s1">].T</span>

    <span class="s0"># comparison</span>
    <span class="s1">assert_array_almost_equal(grad_1</span><span class="s2">, </span><span class="s1">grad_2)</span>
    <span class="s1">assert_almost_equal(loss_1</span><span class="s2">, </span><span class="s1">loss_2)</span>


<span class="s2">def </span><span class="s1">test_multinomial_loss_ground_truth():</span>
    <span class="s0"># n_samples, n_features, n_classes = 4, 2, 3</span>
    <span class="s1">n_classes = </span><span class="s3">3</span>
    <span class="s1">X = np.array([[</span><span class="s3">1.1</span><span class="s2">, </span><span class="s3">2.2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">2.2</span><span class="s2">, </span><span class="s1">-</span><span class="s3">4.4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">3.3</span><span class="s2">, </span><span class="s1">-</span><span class="s3">2.2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">1.1</span><span class="s2">, </span><span class="s3">1.1</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s1">lbin = LabelBinarizer()</span>
    <span class="s1">Y_bin = lbin.fit_transform(y)</span>

    <span class="s1">weights = np.array([[</span><span class="s3">0.1</span><span class="s2">, </span><span class="s3">0.2</span><span class="s2">, </span><span class="s3">0.3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">1.1</span><span class="s2">, </span><span class="s3">1.2</span><span class="s2">, </span><span class="s1">-</span><span class="s3">1.3</span><span class="s1">]])</span>
    <span class="s1">intercept = np.array([</span><span class="s3">1.0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s1">-</span><span class="s3">0.2</span><span class="s1">])</span>
    <span class="s1">sample_weights = np.array([</span><span class="s3">0.8</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0.8</span><span class="s1">])</span>

    <span class="s1">prediction = np.dot(X</span><span class="s2">, </span><span class="s1">weights) + intercept</span>
    <span class="s1">logsumexp_prediction = logsumexp(prediction</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">p = prediction - logsumexp_prediction[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">loss_1 = -(sample_weights[:</span><span class="s2">, </span><span class="s1">np.newaxis] * p * Y_bin).sum()</span>
    <span class="s1">diff = sample_weights[:</span><span class="s2">, </span><span class="s1">np.newaxis] * (np.exp(p) - Y_bin)</span>
    <span class="s1">grad_1 = np.dot(X.T</span><span class="s2">, </span><span class="s1">diff)</span>

    <span class="s1">loss = LinearModelLoss(</span>
        <span class="s1">base_loss=HalfMultinomialLoss(n_classes=n_classes)</span><span class="s2">,</span>
        <span class="s1">fit_intercept=</span><span class="s2">True,</span>
    <span class="s1">)</span>
    <span class="s1">weights_intercept = np.vstack((weights</span><span class="s2">, </span><span class="s1">intercept)).T</span>
    <span class="s1">loss_2</span><span class="s2">, </span><span class="s1">grad_2 = loss.loss_gradient(</span>
        <span class="s1">weights_intercept</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">l2_reg_strength=</span><span class="s3">0.0</span><span class="s2">, </span><span class="s1">sample_weight=sample_weights</span>
    <span class="s1">)</span>
    <span class="s1">grad_2 = grad_2[:</span><span class="s2">, </span><span class="s1">:-</span><span class="s3">1</span><span class="s1">].T</span>

    <span class="s1">assert_almost_equal(loss_1</span><span class="s2">, </span><span class="s1">loss_2)</span>
    <span class="s1">assert_array_almost_equal(grad_1</span><span class="s2">, </span><span class="s1">grad_2)</span>

    <span class="s0"># ground truth</span>
    <span class="s1">loss_gt = </span><span class="s3">11.680360354325961</span>
    <span class="s1">grad_gt = np.array(</span>
        <span class="s1">[[-</span><span class="s3">0.557487</span><span class="s2">, </span><span class="s1">-</span><span class="s3">1.619151</span><span class="s2">, </span><span class="s1">+</span><span class="s3">2.176638</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s3">0.903942</span><span class="s2">, </span><span class="s1">+</span><span class="s3">5.258745</span><span class="s2">, </span><span class="s1">-</span><span class="s3">4.354803</span><span class="s1">]]</span>
    <span class="s1">)</span>
    <span class="s1">assert_almost_equal(loss_1</span><span class="s2">, </span><span class="s1">loss_gt)</span>
    <span class="s1">assert_array_almost_equal(grad_1</span><span class="s2">, </span><span class="s1">grad_gt)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;solver&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">&quot;sag&quot;</span><span class="s2">, </span><span class="s4">&quot;saga&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_sag_classifier_raises_error(solver):</span>
    <span class="s0"># Following #13316, the error handling behavior changed in cython sag. This</span>
    <span class="s0"># is simply a non-regression test to make sure numerical errors are</span>
    <span class="s0"># properly raised.</span>

    <span class="s0"># Train a classifier on a simple problem</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(random_state=rng)</span>
    <span class="s1">clf = LogisticRegression(solver=solver</span><span class="s2">, </span><span class="s1">random_state=rng</span><span class="s2">, </span><span class="s1">warm_start=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s0"># Trigger a numerical error by:</span>
    <span class="s0"># - corrupting the fitted coefficients of the classifier</span>
    <span class="s0"># - fit it again starting from its current state thanks to warm_start</span>
    <span class="s1">clf.coef_[:] = np.nan</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s4">&quot;Floating-point under-/overflow&quot;</span><span class="s1">):</span>
        <span class="s1">clf.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
</pre>
</body>
</html>