<html>
<head>
<title>kernel_regression.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
kernel_regression.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Multivariate Conditional and Unconditional Kernel Density Estimation 
with Mixed Data Types 
 
References 
---------- 
[1] Racine, J., Li, Q. Nonparametric econometrics: theory and practice. 
    Princeton University Press. (2007) 
[2] Racine, Jeff. &quot;Nonparametric Econometrics: A Primer,&quot; Foundation 
    and Trends in Econometrics: Vol 3: No 1, pp1-88. (2008) 
    http://dx.doi.org/10.1561/0800000009 
[3] Racine, J., Li, Q. &quot;Nonparametric Estimation of Distributions 
    with Categorical and Continuous Data.&quot; Working Paper. (2000) 
[4] Racine, J. Li, Q. &quot;Kernel Estimation of Multivariate Conditional 
    Distributions Annals of Economics and Finance 5, 211-235 (2004) 
[5] Liu, R., Yang, L. &quot;Kernel estimation of multivariate 
    cumulative distribution function.&quot; 
    Journal of Nonparametric Statistics (2008) 
[6] Li, R., Ju, G. &quot;Nonparametric Estimation of Multivariate CDF 
    with Categorical and Continuous Data.&quot; Working Paper 
[7] Li, Q., Racine, J. &quot;Cross-validated local linear nonparametric 
    regression&quot; Statistica Sinica 14(2004), pp. 485-512 
[8] Racine, J.: &quot;Consistent Significance Testing for Nonparametric 
        Regression&quot; Journal of Business &amp; Economics Statistics 
[9] Racine, J., Hart, J., Li, Q., &quot;Testing the Significance of 
        Categorical Predictor Variables in Nonparametric Regression 
        Models&quot;, 2006, Econometric Reviews 25, 523-544 
 
&quot;&quot;&quot;</span>

<span class="s2"># TODO: make default behavior efficient=True above a certain n_obs</span>
<span class="s3">import </span><span class="s1">copy</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">optimize</span>
<span class="s3">from </span><span class="s1">scipy.stats.mstats </span><span class="s3">import </span><span class="s1">mquantiles</span>

<span class="s3">from </span><span class="s1">._kernel_base </span><span class="s3">import </span><span class="s1">GenericKDE</span><span class="s3">, </span><span class="s1">EstimatorSettings</span><span class="s3">, </span><span class="s1">gpke</span><span class="s3">, </span><span class="s1">\</span>
    <span class="s1">LeaveOneOut</span><span class="s3">, </span><span class="s1">_get_type_pos</span><span class="s3">, </span><span class="s1">_adjust_shape</span><span class="s3">, </span><span class="s1">_compute_min_std_IQR</span><span class="s3">, </span><span class="s1">kernel_func</span>


<span class="s1">__all__ = [</span><span class="s4">'KernelReg'</span><span class="s3">, </span><span class="s4">'KernelCensoredReg'</span><span class="s1">]</span>


<span class="s3">class </span><span class="s1">KernelReg(GenericKDE):</span>
    <span class="s0">&quot;&quot;&quot; 
    Nonparametric kernel regression class. 
 
    Calculates the conditional mean ``E[y|X]`` where ``y = g(X) + e``. 
    Note that the &quot;local constant&quot; type of regression provided here is also 
    known as Nadaraya-Watson kernel regression; &quot;local linear&quot; is an extension 
    of that which suffers less from bias issues at the edge of the support. Note 
    that specifying a custom kernel works only with &quot;local linear&quot; kernel 
    regression. For example, a custom ``tricube`` kernel yields LOESS regression. 
 
    Parameters 
    ---------- 
    endog : array_like 
        This is the dependent variable. 
    exog : array_like 
        The training data for the independent variable(s) 
        Each element in the list is a separate variable 
    var_type : str 
        The type of the variables, one character per variable: 
 
            - c: continuous 
            - u: unordered (discrete) 
            - o: ordered (discrete) 
 
    reg_type : {'lc', 'll'}, optional 
        Type of regression estimator. 'lc' means local constant and 
        'll' local Linear estimator.  Default is 'll' 
    bw : str or array_like, optional 
        Either a user-specified bandwidth or the method for bandwidth 
        selection. If a string, valid values are 'cv_ls' (least-squares 
        cross-validation) and 'aic' (AIC Hurvich bandwidth estimation). 
        Default is 'cv_ls'. User specified bandwidth must have as many 
        entries as the number of variables. 
    ckertype : str, optional 
        The kernel used for the continuous variables. 
    okertype : str, optional 
        The kernel used for the ordered discrete variables. 
    ukertype : str, optional 
        The kernel used for the unordered discrete variables. 
    defaults : EstimatorSettings instance, optional 
        The default values for the efficient bandwidth estimation. 
 
    Attributes 
    ---------- 
    bw : array_like 
        The bandwidth parameters. 
    &quot;&quot;&quot;</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">var_type</span><span class="s3">, </span><span class="s1">reg_type=</span><span class="s4">'ll'</span><span class="s3">, </span><span class="s1">bw=</span><span class="s4">'cv_ls'</span><span class="s3">,</span>
                 <span class="s1">ckertype=</span><span class="s4">'gaussian'</span><span class="s3">, </span><span class="s1">okertype=</span><span class="s4">'wangryzin'</span><span class="s3">,</span>
                 <span class="s1">ukertype=</span><span class="s4">'aitchisonaitken'</span><span class="s3">, </span><span class="s1">defaults=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">self.var_type = var_type</span>
        <span class="s1">self.data_type = var_type</span>
        <span class="s1">self.reg_type = reg_type</span>
        <span class="s1">self.ckertype = ckertype</span>
        <span class="s1">self.okertype = okertype</span>
        <span class="s1">self.ukertype = ukertype</span>
        <span class="s3">if not </span><span class="s1">(self.ckertype </span><span class="s3">in </span><span class="s1">kernel_func </span><span class="s3">and </span><span class="s1">self.ukertype </span><span class="s3">in </span><span class="s1">kernel_func</span>
                <span class="s3">and </span><span class="s1">self.okertype </span><span class="s3">in </span><span class="s1">kernel_func):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'user specified kernel must be a supported '</span>
                             <span class="s4">'kernel from statsmodels.nonparametric.kernels.'</span><span class="s1">)</span>

        <span class="s1">self.k_vars = len(self.var_type)</span>
        <span class="s1">self.endog = _adjust_shape(endog</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">self.exog = _adjust_shape(exog</span><span class="s3">, </span><span class="s1">self.k_vars)</span>
        <span class="s1">self.data = np.column_stack((self.endog</span><span class="s3">, </span><span class="s1">self.exog))</span>
        <span class="s1">self.nobs = np.shape(self.exog)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">self.est = dict(lc=self._est_loc_constant</span><span class="s3">, </span><span class="s1">ll=self._est_loc_linear)</span>
        <span class="s1">defaults = EstimatorSettings() </span><span class="s3">if </span><span class="s1">defaults </span><span class="s3">is None else </span><span class="s1">defaults</span>
        <span class="s1">self._set_defaults(defaults)</span>
        <span class="s3">if not </span><span class="s1">isinstance(bw</span><span class="s3">, </span><span class="s1">str):</span>
            <span class="s1">bw = np.asarray(bw)</span>
            <span class="s3">if </span><span class="s1">len(bw) != self.k_vars:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'bw must have the same dimension as the '</span>
                                 <span class="s4">'number of variables.'</span><span class="s1">)</span>
        <span class="s3">if not </span><span class="s1">self.efficient:</span>
            <span class="s1">self.bw = self._compute_reg_bw(bw)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.bw = self._compute_efficient(bw)</span>

    <span class="s3">def </span><span class="s1">_compute_reg_bw(self</span><span class="s3">, </span><span class="s1">bw):</span>
        <span class="s3">if not </span><span class="s1">isinstance(bw</span><span class="s3">, </span><span class="s1">str):</span>
            <span class="s1">self._bw_method = </span><span class="s4">&quot;user-specified&quot;</span>
            <span class="s3">return </span><span class="s1">np.asarray(bw)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># The user specified a bandwidth selection method e.g. 'cv_ls'</span>
            <span class="s1">self._bw_method = bw</span>
            <span class="s2"># Workaround to avoid instance methods in __dict__</span>
            <span class="s3">if </span><span class="s1">bw == </span><span class="s4">'cv_ls'</span><span class="s1">:</span>
                <span class="s1">res = self.cv_loo</span>
            <span class="s3">else</span><span class="s1">:  </span><span class="s2"># bw == 'aic'</span>
                <span class="s1">res = self.aic_hurvich</span>
            <span class="s1">X = np.std(self.exog</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
            <span class="s1">h0 = </span><span class="s5">1.06 </span><span class="s1">* X * \</span>
                 <span class="s1">self.nobs ** (- </span><span class="s5">1. </span><span class="s1">/ (</span><span class="s5">4 </span><span class="s1">+ np.size(self.exog</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)))</span>

            <span class="s1">func = self.est[self.reg_type]</span>
            <span class="s1">bw_estimated = optimize.fmin(res</span><span class="s3">, </span><span class="s1">x0=h0</span><span class="s3">, </span><span class="s1">args=(func</span><span class="s3">, </span><span class="s1">)</span><span class="s3">,</span>
                                         <span class="s1">maxiter=</span><span class="s5">1e3</span><span class="s3">, </span><span class="s1">maxfun=</span><span class="s5">1e3</span><span class="s3">, </span><span class="s1">disp=</span><span class="s5">0</span><span class="s1">)</span>
            <span class="s3">return </span><span class="s1">bw_estimated</span>

    <span class="s3">def </span><span class="s1">_est_loc_linear(self</span><span class="s3">, </span><span class="s1">bw</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">data_predict):</span>
        <span class="s0">&quot;&quot;&quot; 
        Local linear estimator of g(x) in the regression ``y = g(x) + e``. 
 
        Parameters 
        ---------- 
        bw : array_like 
            Vector of bandwidth value(s). 
        endog : 1D array_like 
            The dependent variable. 
        exog : 1D or 2D array_like 
            The independent variable(s). 
        data_predict : 1D array_like of length K, where K is the number of variables. 
            The point at which the density is estimated. 
 
        Returns 
        ------- 
        D_x : array_like 
            The value of the conditional mean at `data_predict`. 
 
        Notes 
        ----- 
        See p. 81 in [1] and p.38 in [2] for the formulas. 
        Unlike other methods, this one requires that `data_predict` be 1D. 
        &quot;&quot;&quot;</span>
        <span class="s1">nobs</span><span class="s3">, </span><span class="s1">k_vars = exog.shape</span>
        <span class="s1">ker = gpke(bw</span><span class="s3">, </span><span class="s1">data=exog</span><span class="s3">, </span><span class="s1">data_predict=data_predict</span><span class="s3">,</span>
                   <span class="s1">var_type=self.var_type</span><span class="s3">,</span>
                   <span class="s1">ckertype=self.ckertype</span><span class="s3">,</span>
                   <span class="s1">ukertype=self.ukertype</span><span class="s3">,</span>
                   <span class="s1">okertype=self.okertype</span><span class="s3">,</span>
                   <span class="s1">tosum=</span><span class="s3">False</span><span class="s1">) / float(nobs)</span>
        <span class="s2"># Create the matrix on p.492 in [7], after the multiplication w/ K_h,ij</span>
        <span class="s2"># See also p. 38 in [2]</span>
        <span class="s2">#ix_cont = np.arange(self.k_vars)  # Use all vars instead of continuous only</span>
        <span class="s2"># Note: because ix_cont was defined here such that it selected all</span>
        <span class="s2"># columns, I removed the indexing with it from exog/data_predict.</span>

        <span class="s2"># Convert ker to a 2-D array to make matrix operations below work</span>
        <span class="s1">ker = ker[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>

        <span class="s1">M12 = exog - data_predict</span>
        <span class="s1">M22 = np.dot(M12.T</span><span class="s3">, </span><span class="s1">M12 * ker)</span>
        <span class="s1">M12 = (M12 * ker).sum(axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">M = np.empty((k_vars + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">k_vars + </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">M[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = ker.sum()</span>
        <span class="s1">M[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">:] = M12</span>
        <span class="s1">M[</span><span class="s5">1</span><span class="s1">:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = M12</span>
        <span class="s1">M[</span><span class="s5">1</span><span class="s1">:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">:] = M22</span>

        <span class="s1">ker_endog = ker * endog</span>
        <span class="s1">V = np.empty((k_vars + </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">V[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = ker_endog.sum()</span>
        <span class="s1">V[</span><span class="s5">1</span><span class="s1">:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = ((exog - data_predict) * ker_endog).sum(axis=</span><span class="s5">0</span><span class="s1">)</span>

        <span class="s1">mean_mfx = np.dot(np.linalg.pinv(M)</span><span class="s3">, </span><span class="s1">V)</span>
        <span class="s1">mean = mean_mfx[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">mfx = mean_mfx[</span><span class="s5">1</span><span class="s1">:</span><span class="s3">, </span><span class="s1">:]</span>
        <span class="s3">return </span><span class="s1">mean</span><span class="s3">, </span><span class="s1">mfx</span>

    <span class="s3">def </span><span class="s1">_est_loc_constant(self</span><span class="s3">, </span><span class="s1">bw</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">data_predict):</span>
        <span class="s0">&quot;&quot;&quot; 
        Local constant estimator of g(x) in the regression 
        y = g(x) + e 
 
        Parameters 
        ---------- 
        bw : array_like 
            Array of bandwidth value(s). 
        endog : 1D array_like 
            The dependent variable. 
        exog : 1D or 2D array_like 
            The independent variable(s). 
        data_predict : 1D or 2D array_like 
            The point(s) at which the density is estimated. 
 
        Returns 
        ------- 
        G : ndarray 
            The value of the conditional mean at `data_predict`. 
        B_x : ndarray 
            The marginal effects. 
        &quot;&quot;&quot;</span>
        <span class="s1">ker_x = gpke(bw</span><span class="s3">, </span><span class="s1">data=exog</span><span class="s3">, </span><span class="s1">data_predict=data_predict</span><span class="s3">,</span>
                     <span class="s1">var_type=self.var_type</span><span class="s3">,</span>
                     <span class="s1">ckertype=self.ckertype</span><span class="s3">,</span>
                     <span class="s1">ukertype=self.ukertype</span><span class="s3">,</span>
                     <span class="s1">okertype=self.okertype</span><span class="s3">,</span>
                     <span class="s1">tosum=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">ker_x = np.reshape(ker_x</span><span class="s3">, </span><span class="s1">np.shape(endog))</span>
        <span class="s1">G_numer = (ker_x * endog).sum(axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">G_denom = ker_x.sum(axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">G = G_numer / G_denom</span>
        <span class="s1">nobs = exog.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">f_x = G_denom / float(nobs)</span>
        <span class="s1">ker_xc = gpke(bw</span><span class="s3">, </span><span class="s1">data=exog</span><span class="s3">, </span><span class="s1">data_predict=data_predict</span><span class="s3">,</span>
                      <span class="s1">var_type=self.var_type</span><span class="s3">,</span>
                      <span class="s1">ckertype=</span><span class="s4">'d_gaussian'</span><span class="s3">,</span>
                      <span class="s2">#okertype='wangryzin_reg',</span>
                      <span class="s1">tosum=</span><span class="s3">False</span><span class="s1">)</span>

        <span class="s1">ker_xc = ker_xc[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">d_mx = -(endog * ker_xc).sum(axis=</span><span class="s5">0</span><span class="s1">) / float(nobs) </span><span class="s2">#* np.prod(bw[:, ix_cont]))</span>
        <span class="s1">d_fx = -ker_xc.sum(axis=</span><span class="s5">0</span><span class="s1">) / float(nobs) </span><span class="s2">#* np.prod(bw[:, ix_cont]))</span>
        <span class="s1">B_x = d_mx / f_x - G * d_fx / f_x</span>
        <span class="s1">B_x = (G_numer * d_fx - G_denom * d_mx) / (G_denom**</span><span class="s5">2</span><span class="s1">)</span>
        <span class="s2">#B_x = (f_x * d_mx - m_x * d_fx) / (f_x ** 2)</span>
        <span class="s3">return </span><span class="s1">G</span><span class="s3">, </span><span class="s1">B_x</span>

    <span class="s3">def </span><span class="s1">aic_hurvich(self</span><span class="s3">, </span><span class="s1">bw</span><span class="s3">, </span><span class="s1">func=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Computes the AIC Hurvich criteria for the estimation of the bandwidth. 
 
        Parameters 
        ---------- 
        bw : str or array_like 
            See the ``bw`` parameter of `KernelReg` for details. 
 
        Returns 
        ------- 
        aic : ndarray 
            The AIC Hurvich criteria, one element for each variable. 
        func : None 
            Unused here, needed in signature because it's used in `cv_loo`. 
 
        References 
        ---------- 
        See ch.2 in [1] and p.35 in [2]. 
        &quot;&quot;&quot;</span>
        <span class="s1">H = np.empty((self.nobs</span><span class="s3">, </span><span class="s1">self.nobs))</span>
        <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(self.nobs):</span>
            <span class="s1">H[:</span><span class="s3">, </span><span class="s1">j] = gpke(bw</span><span class="s3">, </span><span class="s1">data=self.exog</span><span class="s3">, </span><span class="s1">data_predict=self.exog[j</span><span class="s3">,</span><span class="s1">:]</span><span class="s3">,</span>
                           <span class="s1">ckertype=self.ckertype</span><span class="s3">, </span><span class="s1">ukertype=self.ukertype</span><span class="s3">,</span>
                           <span class="s1">okertype=self.okertype</span><span class="s3">, </span><span class="s1">var_type=self.var_type</span><span class="s3">,</span>
                           <span class="s1">tosum=</span><span class="s3">False</span><span class="s1">)</span>

        <span class="s1">denom = H.sum(axis=</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">H = H / denom</span>
        <span class="s1">gx = KernelReg(endog=self.endog</span><span class="s3">, </span><span class="s1">exog=self.exog</span><span class="s3">, </span><span class="s1">var_type=self.var_type</span><span class="s3">,</span>
                       <span class="s1">reg_type=self.reg_type</span><span class="s3">, </span><span class="s1">bw=bw</span><span class="s3">,</span>
                       <span class="s1">defaults=EstimatorSettings(efficient=</span><span class="s3">False</span><span class="s1">)).fit()[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">gx = np.reshape(gx</span><span class="s3">, </span><span class="s1">(self.nobs</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">sigma = ((self.endog - gx)**</span><span class="s5">2</span><span class="s1">).sum(axis=</span><span class="s5">0</span><span class="s1">) / float(self.nobs)</span>

        <span class="s1">frac = (</span><span class="s5">1 </span><span class="s1">+ np.trace(H) / float(self.nobs)) / \</span>
               <span class="s1">(</span><span class="s5">1 </span><span class="s1">- (np.trace(H) + </span><span class="s5">2</span><span class="s1">) / float(self.nobs))</span>
        <span class="s2">#siga = np.dot(self.endog.T, (I - H).T)</span>
        <span class="s2">#sigb = np.dot((I - H), self.endog)</span>
        <span class="s2">#sigma = np.dot(siga, sigb) / float(self.nobs)</span>
        <span class="s1">aic = np.log(sigma) + frac</span>
        <span class="s3">return </span><span class="s1">aic</span>

    <span class="s3">def </span><span class="s1">cv_loo(self</span><span class="s3">, </span><span class="s1">bw</span><span class="s3">, </span><span class="s1">func):</span>
        <span class="s0">r&quot;&quot;&quot; 
        The cross-validation function with leave-one-out estimator. 
 
        Parameters 
        ---------- 
        bw : array_like 
            Vector of bandwidth values. 
        func : callable function 
            Returns the estimator of g(x).  Can be either ``_est_loc_constant`` 
            (local constant) or ``_est_loc_linear`` (local_linear). 
 
        Returns 
        ------- 
        L : float 
            The value of the CV function. 
 
        Notes 
        ----- 
        Calculates the cross-validation least-squares function. This function 
        is minimized by compute_bw to calculate the optimal value of `bw`. 
 
        For details see p.35 in [2] 
 
        .. math:: CV(h)=n^{-1}\sum_{i=1}^{n}(Y_{i}-g_{-i}(X_{i}))^{2} 
 
        where :math:`g_{-i}(X_{i})` is the leave-one-out estimator of g(X) 
        and :math:`h` is the vector of bandwidths 
        &quot;&quot;&quot;</span>
        <span class="s1">LOO_X = LeaveOneOut(self.exog)</span>
        <span class="s1">LOO_Y = LeaveOneOut(self.endog).__iter__()</span>
        <span class="s1">L = </span><span class="s5">0</span>
        <span class="s3">for </span><span class="s1">ii</span><span class="s3">, </span><span class="s1">X_not_i </span><span class="s3">in </span><span class="s1">enumerate(LOO_X):</span>
            <span class="s1">Y = next(LOO_Y)</span>
            <span class="s1">G = func(bw</span><span class="s3">, </span><span class="s1">endog=Y</span><span class="s3">, </span><span class="s1">exog=-X_not_i</span><span class="s3">,</span>
                     <span class="s1">data_predict=-self.exog[ii</span><span class="s3">, </span><span class="s1">:])[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">L += (self.endog[ii] - G) ** </span><span class="s5">2</span>

        <span class="s2"># Note: There might be a way to vectorize this. See p.72 in [1]</span>
        <span class="s3">return </span><span class="s1">L / self.nobs</span>

    <span class="s3">def </span><span class="s1">r_squared(self):</span>
        <span class="s0">r&quot;&quot;&quot; 
        Returns the R-Squared for the nonparametric regression. 
 
        Notes 
        ----- 
        For more details see p.45 in [2] 
        The R-Squared is calculated by: 
 
        .. math:: R^{2}=\frac{\left[\sum_{i=1}^{n} 
            (Y_{i}-\bar{y})(\hat{Y_{i}}-\bar{y}\right]^{2}}{\sum_{i=1}^{n} 
            (Y_{i}-\bar{y})^{2}\sum_{i=1}^{n}(\hat{Y_{i}}-\bar{y})^{2}}, 
 
        where :math:`\hat{Y_{i}}` is the mean calculated in `fit` at the exog 
        points. 
        &quot;&quot;&quot;</span>
        <span class="s1">Y = np.squeeze(self.endog)</span>
        <span class="s1">Yhat = self.fit()[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">Y_bar = np.mean(Yhat)</span>
        <span class="s1">R2_numer = (((Y - Y_bar) * (Yhat - Y_bar)).sum())**</span><span class="s5">2</span>
        <span class="s1">R2_denom = ((Y - Y_bar)**</span><span class="s5">2</span><span class="s1">).sum(axis=</span><span class="s5">0</span><span class="s1">) * \</span>
                   <span class="s1">((Yhat - Y_bar)**</span><span class="s5">2</span><span class="s1">).sum(axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">R2_numer / R2_denom</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">data_predict=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the mean and marginal effects at the `data_predict` points. 
 
        Parameters 
        ---------- 
        data_predict : array_like, optional 
            Points at which to return the mean and marginal effects.  If not 
            given, ``data_predict == exog``. 
 
        Returns 
        ------- 
        mean : ndarray 
            The regression result for the mean (i.e. the actual curve). 
        mfx : ndarray 
            The marginal effects, i.e. the partial derivatives of the mean. 
        &quot;&quot;&quot;</span>
        <span class="s1">func = self.est[self.reg_type]</span>
        <span class="s3">if </span><span class="s1">data_predict </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">data_predict = self.exog</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">data_predict = _adjust_shape(data_predict</span><span class="s3">, </span><span class="s1">self.k_vars)</span>

        <span class="s1">N_data_predict = np.shape(data_predict)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">mean = np.empty((N_data_predict</span><span class="s3">,</span><span class="s1">))</span>
        <span class="s1">mfx = np.empty((N_data_predict</span><span class="s3">, </span><span class="s1">self.k_vars))</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(N_data_predict):</span>
            <span class="s1">mean_mfx = func(self.bw</span><span class="s3">, </span><span class="s1">self.endog</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">,</span>
                            <span class="s1">data_predict=data_predict[i</span><span class="s3">, </span><span class="s1">:])</span>
            <span class="s1">mean[i] = np.squeeze(mean_mfx[</span><span class="s5">0</span><span class="s1">])</span>
            <span class="s1">mfx_c = np.squeeze(mean_mfx[</span><span class="s5">1</span><span class="s1">])</span>
            <span class="s1">mfx[i</span><span class="s3">, </span><span class="s1">:] = mfx_c</span>

        <span class="s3">return </span><span class="s1">mean</span><span class="s3">, </span><span class="s1">mfx</span>

    <span class="s3">def </span><span class="s1">sig_test(self</span><span class="s3">, </span><span class="s1">var_pos</span><span class="s3">, </span><span class="s1">nboot=</span><span class="s5">50</span><span class="s3">, </span><span class="s1">nested_res=</span><span class="s5">25</span><span class="s3">, </span><span class="s1">pivot=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Significance test for the variables in the regression. 
 
        Parameters 
        ---------- 
        var_pos : sequence 
            The position of the variable in exog to be tested. 
 
        Returns 
        ------- 
        sig : str 
            The level of significance: 
 
                - `*` : at 90% confidence level 
                - `**` : at 95% confidence level 
                - `***` : at 99* confidence level 
                - &quot;Not Significant&quot; : if not significant 
        &quot;&quot;&quot;</span>
        <span class="s1">var_pos = np.asarray(var_pos)</span>
        <span class="s1">ix_cont</span><span class="s3">, </span><span class="s1">ix_ord</span><span class="s3">, </span><span class="s1">ix_unord = _get_type_pos(self.var_type)</span>
        <span class="s3">if </span><span class="s1">np.any(ix_cont[var_pos]):</span>
            <span class="s3">if </span><span class="s1">np.any(ix_ord[var_pos]) </span><span class="s3">or </span><span class="s1">np.any(ix_unord[var_pos]):</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Discrete variable in hypothesis. Must be continuous&quot;</span><span class="s1">)</span>

            <span class="s1">Sig = TestRegCoefC(self</span><span class="s3">, </span><span class="s1">var_pos</span><span class="s3">, </span><span class="s1">nboot</span><span class="s3">, </span><span class="s1">nested_res</span><span class="s3">, </span><span class="s1">pivot)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">Sig = TestRegCoefD(self</span><span class="s3">, </span><span class="s1">var_pos</span><span class="s3">, </span><span class="s1">nboot)</span>

        <span class="s3">return </span><span class="s1">Sig.sig</span>

    <span class="s3">def </span><span class="s1">__repr__(self):</span>
        <span class="s0">&quot;&quot;&quot;Provide something sane to print.&quot;&quot;&quot;</span>
        <span class="s1">rpr = </span><span class="s4">&quot;KernelReg instance</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Number of variables: k_vars = &quot; </span><span class="s1">+ str(self.k_vars) + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Number of samples:   N = &quot; </span><span class="s1">+ str(self.nobs) + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Variable types:      &quot; </span><span class="s1">+ self.var_type + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;BW selection method: &quot; </span><span class="s1">+ self._bw_method + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Estimator type: &quot; </span><span class="s1">+ self.reg_type + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s3">return </span><span class="s1">rpr</span>

    <span class="s3">def </span><span class="s1">_get_class_vars_type(self):</span>
        <span class="s0">&quot;&quot;&quot;Helper method to be able to pass needed vars to _compute_subset.&quot;&quot;&quot;</span>
        <span class="s1">class_type = </span><span class="s4">'KernelReg'</span>
        <span class="s1">class_vars = (self.var_type</span><span class="s3">, </span><span class="s1">self.k_vars</span><span class="s3">, </span><span class="s1">self.reg_type)</span>
        <span class="s3">return </span><span class="s1">class_type</span><span class="s3">, </span><span class="s1">class_vars</span>

    <span class="s3">def </span><span class="s1">_compute_dispersion(self</span><span class="s3">, </span><span class="s1">data):</span>
        <span class="s0">&quot;&quot;&quot; 
        Computes the measure of dispersion. 
 
        The minimum of the standard deviation and interquartile range / 1.349 
 
        References 
        ---------- 
        See the user guide for the np package in R. 
        In the notes on bwscaling option in npreg, npudens, npcdens there is 
        a discussion on the measure of dispersion 
        &quot;&quot;&quot;</span>
        <span class="s1">data = data[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">:]</span>
        <span class="s3">return </span><span class="s1">_compute_min_std_IQR(data)</span>


<span class="s3">class </span><span class="s1">KernelCensoredReg(KernelReg):</span>
    <span class="s0">&quot;&quot;&quot; 
    Nonparametric censored regression. 
 
    Calculates the conditional mean ``E[y|X]`` where ``y = g(X) + e``, 
    where y is left-censored.  Left censored variable Y is defined as 
    ``Y = min {Y', L}`` where ``L`` is the value at which ``Y`` is censored 
    and ``Y'`` is the true value of the variable. 
 
    Parameters 
    ---------- 
    endog : list with one element which is array_like 
        This is the dependent variable. 
    exog : list 
        The training data for the independent variable(s) 
        Each element in the list is a separate variable 
    dep_type : str 
        The type of the dependent variable(s) 
        c: Continuous 
        u: Unordered (Discrete) 
        o: Ordered (Discrete) 
    reg_type : str 
        Type of regression estimator 
        lc: Local Constant Estimator 
        ll: Local Linear Estimator 
    bw : array_like 
        Either a user-specified bandwidth or 
        the method for bandwidth selection. 
        cv_ls: cross-validation least squares 
        aic: AIC Hurvich Estimator 
    ckertype : str, optional 
        The kernel used for the continuous variables. 
    okertype : str, optional 
        The kernel used for the ordered discrete variables. 
    ukertype : str, optional 
        The kernel used for the unordered discrete variables. 
    censor_val : float 
        Value at which the dependent variable is censored 
    defaults : EstimatorSettings instance, optional 
        The default values for the efficient bandwidth estimation 
 
    Attributes 
    ---------- 
    bw : array_like 
        The bandwidth parameters 
    &quot;&quot;&quot;</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">var_type</span><span class="s3">, </span><span class="s1">reg_type</span><span class="s3">, </span><span class="s1">bw=</span><span class="s4">'cv_ls'</span><span class="s3">,</span>
                 <span class="s1">ckertype=</span><span class="s4">'gaussian'</span><span class="s3">,</span>
                 <span class="s1">ukertype=</span><span class="s4">'aitchison_aitken_reg'</span><span class="s3">,</span>
                 <span class="s1">okertype=</span><span class="s4">'wangryzin_reg'</span><span class="s3">,</span>
                 <span class="s1">censor_val=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">defaults=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">self.var_type = var_type</span>
        <span class="s1">self.data_type = var_type</span>
        <span class="s1">self.reg_type = reg_type</span>
        <span class="s1">self.ckertype = ckertype</span>
        <span class="s1">self.okertype = okertype</span>
        <span class="s1">self.ukertype = ukertype</span>
        <span class="s3">if not </span><span class="s1">(self.ckertype </span><span class="s3">in </span><span class="s1">kernel_func </span><span class="s3">and </span><span class="s1">self.ukertype </span><span class="s3">in </span><span class="s1">kernel_func</span>
                <span class="s3">and </span><span class="s1">self.okertype </span><span class="s3">in </span><span class="s1">kernel_func):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'user specified kernel must be a supported '</span>
                             <span class="s4">'kernel from statsmodels.nonparametric.kernels.'</span><span class="s1">)</span>

        <span class="s1">self.k_vars = len(self.var_type)</span>
        <span class="s1">self.endog = _adjust_shape(endog</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">self.exog = _adjust_shape(exog</span><span class="s3">, </span><span class="s1">self.k_vars)</span>
        <span class="s1">self.data = np.column_stack((self.endog</span><span class="s3">, </span><span class="s1">self.exog))</span>
        <span class="s1">self.nobs = np.shape(self.exog)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">self.est = dict(lc=self._est_loc_constant</span><span class="s3">, </span><span class="s1">ll=self._est_loc_linear)</span>
        <span class="s1">defaults = EstimatorSettings() </span><span class="s3">if </span><span class="s1">defaults </span><span class="s3">is None else </span><span class="s1">defaults</span>
        <span class="s1">self._set_defaults(defaults)</span>
        <span class="s1">self.censor_val = censor_val</span>
        <span class="s3">if </span><span class="s1">self.censor_val </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">self.censored(censor_val)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.W_in = np.ones((self.nobs</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>

        <span class="s3">if not </span><span class="s1">self.efficient:</span>
            <span class="s1">self.bw = self._compute_reg_bw(bw)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.bw = self._compute_efficient(bw)</span>

    <span class="s3">def </span><span class="s1">censored(self</span><span class="s3">, </span><span class="s1">censor_val):</span>
        <span class="s2"># see pp. 341-344 in [1]</span>
        <span class="s1">self.d = (self.endog != censor_val) * </span><span class="s5">1.</span>
        <span class="s1">ix = np.argsort(np.squeeze(self.endog))</span>
        <span class="s1">self.sortix = ix</span>
        <span class="s1">self.sortix_rev = np.zeros(ix.shape</span><span class="s3">, </span><span class="s1">int)</span>
        <span class="s1">self.sortix_rev[ix] = np.arange(len(ix))</span>
        <span class="s1">self.endog = np.squeeze(self.endog[ix])</span>
        <span class="s1">self.endog = _adjust_shape(self.endog</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">self.exog = np.squeeze(self.exog[ix])</span>
        <span class="s1">self.d = np.squeeze(self.d[ix])</span>
        <span class="s1">self.W_in = np.empty((self.nobs</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">self.nobs + </span><span class="s5">1</span><span class="s1">):</span>
            <span class="s1">P=</span><span class="s5">1</span>
            <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">i):</span>
                <span class="s1">P *= ((self.nobs - j)/(float(self.nobs)-j+</span><span class="s5">1</span><span class="s1">))**self.d[j-</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s1">self.W_in[i-</span><span class="s5">1</span><span class="s3">,</span><span class="s5">0</span><span class="s1">] = P * self.d[i-</span><span class="s5">1</span><span class="s1">] / (float(self.nobs) - i + </span><span class="s5">1 </span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">__repr__(self):</span>
        <span class="s0">&quot;&quot;&quot;Provide something sane to print.&quot;&quot;&quot;</span>
        <span class="s1">rpr = </span><span class="s4">&quot;KernelCensoredReg instance</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Number of variables: k_vars = &quot; </span><span class="s1">+ str(self.k_vars) + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Number of samples:   nobs = &quot; </span><span class="s1">+ str(self.nobs) + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Variable types:      &quot; </span><span class="s1">+ self.var_type + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;BW selection method: &quot; </span><span class="s1">+ self._bw_method + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s1">rpr += </span><span class="s4">&quot;Estimator type: &quot; </span><span class="s1">+ self.reg_type + </span><span class="s4">&quot;</span><span class="s3">\n</span><span class="s4">&quot;</span>
        <span class="s3">return </span><span class="s1">rpr</span>

    <span class="s3">def </span><span class="s1">_est_loc_linear(self</span><span class="s3">, </span><span class="s1">bw</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">data_predict</span><span class="s3">, </span><span class="s1">W):</span>
        <span class="s0">&quot;&quot;&quot; 
        Local linear estimator of g(x) in the regression ``y = g(x) + e``. 
 
        Parameters 
        ---------- 
        bw : array_like 
            Vector of bandwidth value(s) 
        endog : 1D array_like 
            The dependent variable 
        exog : 1D or 2D array_like 
            The independent variable(s) 
        data_predict : 1D array_like of length K, where K is 
            the number of variables. The point at which 
            the density is estimated 
 
        Returns 
        ------- 
        D_x : array_like 
            The value of the conditional mean at data_predict 
 
        Notes 
        ----- 
        See p. 81 in [1] and p.38 in [2] for the formulas 
        Unlike other methods, this one requires that data_predict be 1D 
        &quot;&quot;&quot;</span>
        <span class="s1">nobs</span><span class="s3">, </span><span class="s1">k_vars = exog.shape</span>
        <span class="s1">ker = gpke(bw</span><span class="s3">, </span><span class="s1">data=exog</span><span class="s3">, </span><span class="s1">data_predict=data_predict</span><span class="s3">,</span>
                   <span class="s1">var_type=self.var_type</span><span class="s3">,</span>
                   <span class="s1">ckertype=self.ckertype</span><span class="s3">,</span>
                   <span class="s1">ukertype=self.ukertype</span><span class="s3">,</span>
                   <span class="s1">okertype=self.okertype</span><span class="s3">, </span><span class="s1">tosum=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s2"># Create the matrix on p.492 in [7], after the multiplication w/ K_h,ij</span>
        <span class="s2"># See also p. 38 in [2]</span>

        <span class="s2"># Convert ker to a 2-D array to make matrix operations below work</span>
        <span class="s1">ker = W * ker[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>

        <span class="s1">M12 = exog - data_predict</span>
        <span class="s1">M22 = np.dot(M12.T</span><span class="s3">, </span><span class="s1">M12 * ker)</span>
        <span class="s1">M12 = (M12 * ker).sum(axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">M = np.empty((k_vars + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">k_vars + </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">M[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = ker.sum()</span>
        <span class="s1">M[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">:] = M12</span>
        <span class="s1">M[</span><span class="s5">1</span><span class="s1">:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = M12</span>
        <span class="s1">M[</span><span class="s5">1</span><span class="s1">:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">:] = M22</span>

        <span class="s1">ker_endog = ker * endog</span>
        <span class="s1">V = np.empty((k_vars + </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">V[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = ker_endog.sum()</span>
        <span class="s1">V[</span><span class="s5">1</span><span class="s1">:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = ((exog - data_predict) * ker_endog).sum(axis=</span><span class="s5">0</span><span class="s1">)</span>

        <span class="s1">mean_mfx = np.dot(np.linalg.pinv(M)</span><span class="s3">, </span><span class="s1">V)</span>
        <span class="s1">mean = mean_mfx[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">mfx = mean_mfx[</span><span class="s5">1</span><span class="s1">:</span><span class="s3">, </span><span class="s1">:]</span>
        <span class="s3">return </span><span class="s1">mean</span><span class="s3">, </span><span class="s1">mfx</span>


    <span class="s3">def </span><span class="s1">cv_loo(self</span><span class="s3">, </span><span class="s1">bw</span><span class="s3">, </span><span class="s1">func):</span>
        <span class="s0">r&quot;&quot;&quot; 
        The cross-validation function with leave-one-out 
        estimator 
 
        Parameters 
        ---------- 
        bw : array_like 
            Vector of bandwidth values 
        func : callable function 
            Returns the estimator of g(x). 
            Can be either ``_est_loc_constant`` (local constant) or 
            ``_est_loc_linear`` (local_linear). 
 
        Returns 
        ------- 
        L : float 
            The value of the CV function 
 
        Notes 
        ----- 
        Calculates the cross-validation least-squares 
        function. This function is minimized by compute_bw 
        to calculate the optimal value of bw 
 
        For details see p.35 in [2] 
 
        .. math:: CV(h)=n^{-1}\sum_{i=1}^{n}(Y_{i}-g_{-i}(X_{i}))^{2} 
 
        where :math:`g_{-i}(X_{i})` is the leave-one-out estimator of g(X) 
        and :math:`h` is the vector of bandwidths 
        &quot;&quot;&quot;</span>
        <span class="s1">LOO_X = LeaveOneOut(self.exog)</span>
        <span class="s1">LOO_Y = LeaveOneOut(self.endog).__iter__()</span>
        <span class="s1">LOO_W = LeaveOneOut(self.W_in).__iter__()</span>
        <span class="s1">L = </span><span class="s5">0</span>
        <span class="s3">for </span><span class="s1">ii</span><span class="s3">, </span><span class="s1">X_not_i </span><span class="s3">in </span><span class="s1">enumerate(LOO_X):</span>
            <span class="s1">Y = next(LOO_Y)</span>
            <span class="s1">w = next(LOO_W)</span>
            <span class="s1">G = func(bw</span><span class="s3">, </span><span class="s1">endog=Y</span><span class="s3">, </span><span class="s1">exog=-X_not_i</span><span class="s3">,</span>
                     <span class="s1">data_predict=-self.exog[ii</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">W=w)[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">L += (self.endog[ii] - G) ** </span><span class="s5">2</span>

        <span class="s2"># Note: There might be a way to vectorize this. See p.72 in [1]</span>
        <span class="s3">return </span><span class="s1">L / self.nobs</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">data_predict=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the marginal effects at the data_predict points. 
        &quot;&quot;&quot;</span>
        <span class="s1">func = self.est[self.reg_type]</span>
        <span class="s3">if </span><span class="s1">data_predict </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">data_predict = self.exog</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">data_predict = _adjust_shape(data_predict</span><span class="s3">, </span><span class="s1">self.k_vars)</span>

        <span class="s1">N_data_predict = np.shape(data_predict)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">mean = np.empty((N_data_predict</span><span class="s3">,</span><span class="s1">))</span>
        <span class="s1">mfx = np.empty((N_data_predict</span><span class="s3">, </span><span class="s1">self.k_vars))</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(N_data_predict):</span>
            <span class="s1">mean_mfx = func(self.bw</span><span class="s3">, </span><span class="s1">self.endog</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">,</span>
                            <span class="s1">data_predict=data_predict[i</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">,</span>
                            <span class="s1">W=self.W_in)</span>
            <span class="s1">mean[i] = np.squeeze(mean_mfx[</span><span class="s5">0</span><span class="s1">])</span>
            <span class="s1">mfx_c = np.squeeze(mean_mfx[</span><span class="s5">1</span><span class="s1">])</span>
            <span class="s1">mfx[i</span><span class="s3">, </span><span class="s1">:] = mfx_c</span>

        <span class="s3">return </span><span class="s1">mean</span><span class="s3">, </span><span class="s1">mfx</span>


<span class="s3">class </span><span class="s1">TestRegCoefC:</span>
    <span class="s0">&quot;&quot;&quot; 
    Significance test for continuous variables in a nonparametric regression. 
 
    The null hypothesis is ``dE(Y|X)/dX_not_i = 0``, the alternative hypothesis 
    is ``dE(Y|X)/dX_not_i != 0``. 
 
    Parameters 
    ---------- 
    model : KernelReg instance 
        This is the nonparametric regression model whose elements 
        are tested for significance. 
    test_vars : tuple, list of integers, array_like 
        index of position of the continuous variables to be tested 
        for significance. E.g. (1,3,5) jointly tests variables at 
        position 1,3 and 5 for significance. 
    nboot : int 
        Number of bootstrap samples used to determine the distribution 
        of the test statistic in a finite sample. Default is 400 
    nested_res : int 
        Number of nested resamples used to calculate lambda. 
        Must enable the pivot option 
    pivot : bool 
        Pivot the test statistic by dividing by its standard error 
        Significantly increases computational time. But pivot statistics 
        have more desirable properties 
        (See references) 
 
    Attributes 
    ---------- 
    sig : str 
        The significance level of the variable(s) tested 
        &quot;Not Significant&quot;: Not significant at the 90% confidence level 
                            Fails to reject the null 
        &quot;*&quot;: Significant at the 90% confidence level 
        &quot;**&quot;: Significant at the 95% confidence level 
        &quot;***&quot;: Significant at the 99% confidence level 
 
    Notes 
    ----- 
    This class allows testing of joint hypothesis as long as all variables 
    are continuous. 
 
    References 
    ---------- 
    Racine, J.: &quot;Consistent Significance Testing for Nonparametric Regression&quot; 
    Journal of Business &amp; Economics Statistics. 
 
    Chapter 12 in [1]. 
    &quot;&quot;&quot;</span>
    <span class="s2"># Significance of continuous vars in nonparametric regression</span>
    <span class="s2"># Racine: Consistent Significance Testing for Nonparametric Regression</span>
    <span class="s2"># Journal of Business &amp; Economics Statistics</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">model</span><span class="s3">, </span><span class="s1">test_vars</span><span class="s3">, </span><span class="s1">nboot=</span><span class="s5">400</span><span class="s3">, </span><span class="s1">nested_res=</span><span class="s5">400</span><span class="s3">,</span>
                 <span class="s1">pivot=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s1">self.nboot = nboot</span>
        <span class="s1">self.nres = nested_res</span>
        <span class="s1">self.test_vars = test_vars</span>
        <span class="s1">self.model = model</span>
        <span class="s1">self.bw = model.bw</span>
        <span class="s1">self.var_type = model.var_type</span>
        <span class="s1">self.k_vars = len(self.var_type)</span>
        <span class="s1">self.endog = model.endog</span>
        <span class="s1">self.exog = model.exog</span>
        <span class="s1">self.gx = model.est[model.reg_type]</span>
        <span class="s1">self.test_vars = test_vars</span>
        <span class="s1">self.pivot = pivot</span>
        <span class="s1">self.run()</span>

    <span class="s3">def </span><span class="s1">run(self):</span>
        <span class="s1">self.test_stat = self._compute_test_stat(self.endog</span><span class="s3">, </span><span class="s1">self.exog)</span>
        <span class="s1">self.sig = self._compute_sig()</span>

    <span class="s3">def </span><span class="s1">_compute_test_stat(self</span><span class="s3">, </span><span class="s1">Y</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        Computes the test statistic.  See p.371 in [8]. 
        &quot;&quot;&quot;</span>
        <span class="s1">lam = self._compute_lambda(Y</span><span class="s3">, </span><span class="s1">X)</span>
        <span class="s1">t = lam</span>
        <span class="s3">if </span><span class="s1">self.pivot:</span>
            <span class="s1">se_lam = self._compute_se_lambda(Y</span><span class="s3">, </span><span class="s1">X)</span>
            <span class="s1">t = lam / float(se_lam)</span>

        <span class="s3">return </span><span class="s1">t</span>

    <span class="s3">def </span><span class="s1">_compute_lambda(self</span><span class="s3">, </span><span class="s1">Y</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Computes only lambda -- the main part of the test statistic&quot;&quot;&quot;</span>
        <span class="s1">n = np.shape(X)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">Y = _adjust_shape(Y</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">X = _adjust_shape(X</span><span class="s3">, </span><span class="s1">self.k_vars)</span>
        <span class="s1">b = KernelReg(Y</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">self.var_type</span><span class="s3">, </span><span class="s1">self.model.reg_type</span><span class="s3">, </span><span class="s1">self.bw</span><span class="s3">,</span>
                        <span class="s1">defaults = EstimatorSettings(efficient=</span><span class="s3">False</span><span class="s1">)).fit()[</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">b = b[:</span><span class="s3">, </span><span class="s1">self.test_vars]</span>
        <span class="s1">b = np.reshape(b</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s1">len(self.test_vars)))</span>
        <span class="s2">#fct = np.std(b)  # Pivot the statistic by dividing by SE</span>
        <span class="s1">fct = </span><span class="s5">1.  </span><span class="s2"># Do not Pivot -- Bootstrapping works better if Pivot</span>
        <span class="s1">lam = ((b / fct) ** </span><span class="s5">2</span><span class="s1">).sum() / float(n)</span>
        <span class="s3">return </span><span class="s1">lam</span>

    <span class="s3">def </span><span class="s1">_compute_se_lambda(self</span><span class="s3">, </span><span class="s1">Y</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        Calculates the SE of lambda by nested resampling 
        Used to pivot the statistic. 
        Bootstrapping works better with estimating pivotal statistics 
        but slows down computation significantly. 
        &quot;&quot;&quot;</span>
        <span class="s1">n = np.shape(Y)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">lam = np.empty(shape=(self.nres</span><span class="s3">,</span><span class="s1">))</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.nres):</span>
            <span class="s1">ind = np.random.randint(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">size=(n</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
            <span class="s1">Y1 = Y[ind</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">X1 = X[ind</span><span class="s3">, </span><span class="s1">:]</span>
            <span class="s1">lam[i] = self._compute_lambda(Y1</span><span class="s3">, </span><span class="s1">X1)</span>

        <span class="s1">se_lambda = np.std(lam)</span>
        <span class="s3">return </span><span class="s1">se_lambda</span>

    <span class="s3">def </span><span class="s1">_compute_sig(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Computes the significance value for the variable(s) tested. 
 
        The empirical distribution of the test statistic is obtained through 
        bootstrapping the sample.  The null hypothesis is rejected if the test 
        statistic is larger than the 90, 95, 99 percentiles. 
        &quot;&quot;&quot;</span>
        <span class="s1">t_dist = np.empty(shape=(self.nboot</span><span class="s3">, </span><span class="s1">))</span>
        <span class="s1">Y = self.endog</span>
        <span class="s1">X = copy.deepcopy(self.exog)</span>
        <span class="s1">n = np.shape(Y)[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s1">X[:</span><span class="s3">, </span><span class="s1">self.test_vars] = np.mean(X[:</span><span class="s3">, </span><span class="s1">self.test_vars]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s2"># Calculate the restricted mean. See p. 372 in [8]</span>
        <span class="s1">M = KernelReg(Y</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">self.var_type</span><span class="s3">, </span><span class="s1">self.model.reg_type</span><span class="s3">, </span><span class="s1">self.bw</span><span class="s3">,</span>
                      <span class="s1">defaults=EstimatorSettings(efficient=</span><span class="s3">False</span><span class="s1">)).fit()[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">M = np.reshape(M</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">e = Y - M</span>
        <span class="s1">e = e - np.mean(e)  </span><span class="s2"># recenter residuals</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.nboot):</span>
            <span class="s1">ind = np.random.randint(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">n</span><span class="s3">, </span><span class="s1">size=(n</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
            <span class="s1">e_boot = e[ind</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">Y_boot = M + e_boot</span>
            <span class="s1">t_dist[i] = self._compute_test_stat(Y_boot</span><span class="s3">, </span><span class="s1">self.exog)</span>

        <span class="s1">self.t_dist = t_dist</span>
        <span class="s1">sig = </span><span class="s4">&quot;Not Significant&quot;</span>
        <span class="s3">if </span><span class="s1">self.test_stat &gt; mquantiles(t_dist</span><span class="s3">, </span><span class="s5">0.9</span><span class="s1">):</span>
            <span class="s1">sig = </span><span class="s4">&quot;*&quot;</span>
        <span class="s3">if </span><span class="s1">self.test_stat &gt; mquantiles(t_dist</span><span class="s3">, </span><span class="s5">0.95</span><span class="s1">):</span>
            <span class="s1">sig = </span><span class="s4">&quot;**&quot;</span>
        <span class="s3">if </span><span class="s1">self.test_stat &gt; mquantiles(t_dist</span><span class="s3">, </span><span class="s5">0.99</span><span class="s1">):</span>
            <span class="s1">sig = </span><span class="s4">&quot;***&quot;</span>

        <span class="s3">return </span><span class="s1">sig</span>


<span class="s3">class </span><span class="s1">TestRegCoefD(TestRegCoefC):</span>
    <span class="s0">&quot;&quot;&quot; 
    Significance test for the categorical variables in a nonparametric 
    regression. 
 
    Parameters 
    ---------- 
    model : Instance of KernelReg class 
        This is the nonparametric regression model whose elements 
        are tested for significance. 
    test_vars : tuple, list of one element 
        index of position of the discrete variable to be tested 
        for significance. E.g. (3) tests variable at 
        position 3 for significance. 
    nboot : int 
        Number of bootstrap samples used to determine the distribution 
        of the test statistic in a finite sample. Default is 400 
 
    Attributes 
    ---------- 
    sig : str 
        The significance level of the variable(s) tested 
        &quot;Not Significant&quot;: Not significant at the 90% confidence level 
                            Fails to reject the null 
        &quot;*&quot;: Significant at the 90% confidence level 
        &quot;**&quot;: Significant at the 95% confidence level 
        &quot;***&quot;: Significant at the 99% confidence level 
 
    Notes 
    ----- 
    This class currently does not allow joint hypothesis. 
    Only one variable can be tested at a time 
 
    References 
    ---------- 
    See [9] and chapter 12 in [1]. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">_compute_test_stat(self</span><span class="s3">, </span><span class="s1">Y</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Computes the test statistic&quot;&quot;&quot;</span>

        <span class="s1">dom_x = np.sort(np.unique(self.exog[:</span><span class="s3">, </span><span class="s1">self.test_vars]))</span>

        <span class="s1">n = np.shape(X)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">model = KernelReg(Y</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">self.var_type</span><span class="s3">, </span><span class="s1">self.model.reg_type</span><span class="s3">, </span><span class="s1">self.bw</span><span class="s3">,</span>
                          <span class="s1">defaults = EstimatorSettings(efficient=</span><span class="s3">False</span><span class="s1">))</span>
        <span class="s1">X1 = copy.deepcopy(X)</span>
        <span class="s1">X1[:</span><span class="s3">, </span><span class="s1">self.test_vars] = </span><span class="s5">0</span>

        <span class="s1">m0 = model.fit(data_predict=X1)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">m0 = np.reshape(m0</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">zvec = np.zeros((n</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))  </span><span class="s2"># noqa:E741</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">dom_x[</span><span class="s5">1</span><span class="s1">:] :</span>
            <span class="s1">X1[:</span><span class="s3">, </span><span class="s1">self.test_vars] = i</span>
            <span class="s1">m1 = model.fit(data_predict=X1)[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">m1 = np.reshape(m1</span><span class="s3">, </span><span class="s1">(n</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
            <span class="s1">zvec += (m1 - m0) ** </span><span class="s5">2  </span><span class="s2"># noqa:E741</span>

        <span class="s1">avg = zvec.sum(axis=</span><span class="s5">0</span><span class="s1">) / float(n)</span>
        <span class="s3">return </span><span class="s1">avg</span>

    <span class="s3">def </span><span class="s1">_compute_sig(self):</span>
        <span class="s0">&quot;&quot;&quot;Calculates the significance level of the variable tested&quot;&quot;&quot;</span>

        <span class="s1">m = self._est_cond_mean()</span>
        <span class="s1">Y = self.endog</span>
        <span class="s1">X = self.exog</span>
        <span class="s1">n = np.shape(X)[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">u = Y - m</span>
        <span class="s1">u = u - np.mean(u)  </span><span class="s2"># center</span>
        <span class="s1">fct1 = (</span><span class="s5">1 </span><span class="s1">- </span><span class="s5">5</span><span class="s1">**</span><span class="s5">0.5</span><span class="s1">) / </span><span class="s5">2.</span>
        <span class="s1">fct2 = (</span><span class="s5">1 </span><span class="s1">+ </span><span class="s5">5</span><span class="s1">**</span><span class="s5">0.5</span><span class="s1">) / </span><span class="s5">2.</span>
        <span class="s1">u1 = fct1 * u</span>
        <span class="s1">u2 = fct2 * u</span>
        <span class="s1">r = fct2 / (</span><span class="s5">5 </span><span class="s1">** </span><span class="s5">0.5</span><span class="s1">)</span>
        <span class="s1">I_dist = np.empty((self.nboot</span><span class="s3">,</span><span class="s5">1</span><span class="s1">))</span>
        <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(self.nboot):</span>
            <span class="s1">u_boot = copy.deepcopy(u2)</span>

            <span class="s1">prob = np.random.uniform(</span><span class="s5">0</span><span class="s3">,</span><span class="s5">1</span><span class="s3">, </span><span class="s1">size = (n</span><span class="s3">,</span><span class="s5">1</span><span class="s1">))</span>
            <span class="s1">ind = prob &lt; r</span>
            <span class="s1">u_boot[ind] = u1[ind]</span>
            <span class="s1">Y_boot = m + u_boot</span>
            <span class="s1">I_dist[j] = self._compute_test_stat(Y_boot</span><span class="s3">, </span><span class="s1">X)</span>

        <span class="s1">sig = </span><span class="s4">&quot;Not Significant&quot;</span>
        <span class="s3">if </span><span class="s1">self.test_stat &gt; mquantiles(I_dist</span><span class="s3">, </span><span class="s5">0.9</span><span class="s1">):</span>
            <span class="s1">sig = </span><span class="s4">&quot;*&quot;</span>
        <span class="s3">if </span><span class="s1">self.test_stat &gt; mquantiles(I_dist</span><span class="s3">, </span><span class="s5">0.95</span><span class="s1">):</span>
            <span class="s1">sig = </span><span class="s4">&quot;**&quot;</span>
        <span class="s3">if </span><span class="s1">self.test_stat &gt; mquantiles(I_dist</span><span class="s3">, </span><span class="s5">0.99</span><span class="s1">):</span>
            <span class="s1">sig = </span><span class="s4">&quot;***&quot;</span>

        <span class="s3">return </span><span class="s1">sig</span>

    <span class="s3">def </span><span class="s1">_est_cond_mean(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Calculates the expected conditional mean 
        m(X, Z=l) for all possible l 
        &quot;&quot;&quot;</span>
        <span class="s1">self.dom_x = np.sort(np.unique(self.exog[:</span><span class="s3">, </span><span class="s1">self.test_vars]))</span>
        <span class="s1">X = copy.deepcopy(self.exog)</span>
        <span class="s1">m=</span><span class="s5">0</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">self.dom_x:</span>
            <span class="s1">X[:</span><span class="s3">, </span><span class="s1">self.test_vars]  = i</span>
            <span class="s1">m += self.model.fit(data_predict = X)[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s1">m = m / float(len(self.dom_x))</span>
        <span class="s1">m = np.reshape(m</span><span class="s3">, </span><span class="s1">(np.shape(self.exog)[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s3">return </span><span class="s1">m</span>
</pre>
</body>
</html>