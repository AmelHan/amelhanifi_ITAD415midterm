<html>
<head>
<title>vq.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
vq.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
K-means clustering and vector quantization (:mod:`scipy.cluster.vq`) 
==================================================================== 
 
Provides routines for k-means clustering, generating code books 
from k-means models and quantizing vectors by comparing them with 
centroids in a code book. 
 
.. autosummary:: 
   :toctree: generated/ 
 
   whiten -- Normalize a group of observations so each feature has unit variance 
   vq -- Calculate code book membership of a set of observation vectors 
   kmeans -- Perform k-means on a set of observation vectors forming k clusters 
   kmeans2 -- A different implementation of k-means with more methods 
           -- for initializing centroids 
 
Background information 
---------------------- 
The k-means algorithm takes as input the number of clusters to 
generate, k, and a set of observation vectors to cluster. It 
returns a set of centroids, one for each of the k clusters. An 
observation vector is classified with the cluster number or 
centroid index of the centroid closest to it. 
 
A vector v belongs to cluster i if it is closer to centroid i than 
any other centroid. If v belongs to i, we say centroid i is the 
dominating centroid of v. The k-means algorithm tries to 
minimize distortion, which is defined as the sum of the squared distances 
between each observation vector and its dominating centroid. 
The minimization is achieved by iteratively reclassifying 
the observations into clusters and recalculating the centroids until 
a configuration is reached in which the centroids are stable. One can 
also define a maximum number of iterations. 
 
Since vector quantization is a natural application for k-means, 
information theory terminology is often used. The centroid index 
or cluster index is also referred to as a &quot;code&quot; and the table 
mapping codes to centroids and, vice versa, is often referred to as a 
&quot;code book&quot;. The result of k-means, a set of centroids, can be 
used to quantize vectors. Quantization aims to find an encoding of 
vectors that reduces the expected distortion. 
 
All routines expect obs to be an M by N array, where the rows are 
the observation vectors. The codebook is a k by N array, where the 
ith row is the centroid of code word i. The observation vectors 
and centroids have the same feature dimension. 
 
As an example, suppose we wish to compress a 24-bit color image 
(each pixel is represented by one byte for red, one for blue, and 
one for green) before sending it over the web. By using a smaller 
8-bit encoding, we can reduce the amount of data by two 
thirds. Ideally, the colors for each of the 256 possible 8-bit 
encoding values should be chosen to minimize distortion of the 
color. Running k-means with k=256 generates a code book of 256 
codes, which fills up all possible 8-bit sequences. Instead of 
sending a 3-byte value for each pixel, the 8-bit centroid index 
(or code word) of the dominating centroid is transmitted. The code 
book is also sent over the wire so each 8-bit code can be 
translated back to a 24-bit pixel value representation. If the 
image of interest was of an ocean, we would expect many 24-bit 
blues to be represented by 8-bit codes. If it was an image of a 
human face, more flesh-tone colors would be represented in the 
code book. 
 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">collections </span><span class="s2">import </span><span class="s1">deque</span>
<span class="s2">from </span><span class="s1">scipy._lib._util </span><span class="s2">import </span><span class="s1">_asarray_validated</span><span class="s2">, </span><span class="s1">check_random_state</span><span class="s2">,</span><span class="s1">\</span>
    <span class="s1">rng_integers</span>
<span class="s2">from </span><span class="s1">scipy.spatial.distance </span><span class="s2">import </span><span class="s1">cdist</span>

<span class="s2">from </span><span class="s1">. </span><span class="s2">import </span><span class="s1">_vq</span>

<span class="s1">__docformat__ = </span><span class="s3">'restructuredtext'</span>

<span class="s1">__all__ = [</span><span class="s3">'whiten'</span><span class="s2">, </span><span class="s3">'vq'</span><span class="s2">, </span><span class="s3">'kmeans'</span><span class="s2">, </span><span class="s3">'kmeans2'</span><span class="s1">]</span>


<span class="s2">class </span><span class="s1">ClusterError(Exception):</span>
    <span class="s2">pass</span>


<span class="s2">def </span><span class="s1">whiten(obs</span><span class="s2">, </span><span class="s1">check_finite=</span><span class="s2">True</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Normalize a group of observations on a per feature basis. 
 
    Before running k-means, it is beneficial to rescale each feature 
    dimension of the observation set by its standard deviation (i.e. &quot;whiten&quot; 
    it - as in &quot;white noise&quot; where each frequency has equal power). 
    Each feature is divided by its standard deviation across all observations 
    to give it unit variance. 
 
    Parameters 
    ---------- 
    obs : ndarray 
        Each row of the array is an observation.  The 
        columns are the features seen during each observation. 
 
        &gt;&gt;&gt; #         f0    f1    f2 
        &gt;&gt;&gt; obs = [[  1.,   1.,   1.],  #o0 
        ...        [  2.,   2.,   2.],  #o1 
        ...        [  3.,   3.,   3.],  #o2 
        ...        [  4.,   4.,   4.]]  #o3 
 
    check_finite : bool, optional 
        Whether to check that the input matrices contain only finite numbers. 
        Disabling may give a performance gain, but may result in problems 
        (crashes, non-termination) if the inputs do contain infinities or NaNs. 
        Default: True 
 
    Returns 
    ------- 
    result : ndarray 
        Contains the values in `obs` scaled by the standard deviation 
        of each column. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.cluster.vq import whiten 
    &gt;&gt;&gt; features  = np.array([[1.9, 2.3, 1.7], 
    ...                       [1.5, 2.5, 2.2], 
    ...                       [0.8, 0.6, 1.7,]]) 
    &gt;&gt;&gt; whiten(features) 
    array([[ 4.17944278,  2.69811351,  7.21248917], 
           [ 3.29956009,  2.93273208,  9.33380951], 
           [ 1.75976538,  0.7038557 ,  7.21248917]]) 
 
    &quot;&quot;&quot;</span>
    <span class="s1">obs = _asarray_validated(obs</span><span class="s2">, </span><span class="s1">check_finite=check_finite)</span>
    <span class="s1">std_dev = obs.std(axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">zero_std_mask = std_dev == </span><span class="s4">0</span>
    <span class="s2">if </span><span class="s1">zero_std_mask.any():</span>
        <span class="s1">std_dev[zero_std_mask] = </span><span class="s4">1.0</span>
        <span class="s1">warnings.warn(</span><span class="s3">&quot;Some columns have standard deviation zero. &quot;</span>
                      <span class="s3">&quot;The values of these columns will not change.&quot;</span><span class="s2">,</span>
                      <span class="s1">RuntimeWarning)</span>
    <span class="s2">return </span><span class="s1">obs / std_dev</span>


<span class="s2">def </span><span class="s1">vq(obs</span><span class="s2">, </span><span class="s1">code_book</span><span class="s2">, </span><span class="s1">check_finite=</span><span class="s2">True</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Assign codes from a code book to observations. 
 
    Assigns a code from a code book to each observation. Each 
    observation vector in the 'M' by 'N' `obs` array is compared with the 
    centroids in the code book and assigned the code of the closest 
    centroid. 
 
    The features in `obs` should have unit variance, which can be 
    achieved by passing them through the whiten function. The code 
    book can be created with the k-means algorithm or a different 
    encoding algorithm. 
 
    Parameters 
    ---------- 
    obs : ndarray 
        Each row of the 'M' x 'N' array is an observation. The columns are 
        the &quot;features&quot; seen during each observation. The features must be 
        whitened first using the whiten function or something equivalent. 
    code_book : ndarray 
        The code book is usually generated using the k-means algorithm. 
        Each row of the array holds a different code, and the columns are 
        the features of the code. 
 
         &gt;&gt;&gt; #              f0    f1    f2   f3 
         &gt;&gt;&gt; code_book = [ 
         ...             [  1.,   2.,   3.,   4.],  #c0 
         ...             [  1.,   2.,   3.,   4.],  #c1 
         ...             [  1.,   2.,   3.,   4.]]  #c2 
 
    check_finite : bool, optional 
        Whether to check that the input matrices contain only finite numbers. 
        Disabling may give a performance gain, but may result in problems 
        (crashes, non-termination) if the inputs do contain infinities or NaNs. 
        Default: True 
 
    Returns 
    ------- 
    code : ndarray 
        A length M array holding the code book index for each observation. 
    dist : ndarray 
        The distortion (distance) between the observation and its nearest 
        code. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.cluster.vq import vq 
    &gt;&gt;&gt; code_book = np.array([[1.,1.,1.], 
    ...                       [2.,2.,2.]]) 
    &gt;&gt;&gt; features  = np.array([[  1.9,2.3,1.7], 
    ...                       [  1.5,2.5,2.2], 
    ...                       [  0.8,0.6,1.7]]) 
    &gt;&gt;&gt; vq(features,code_book) 
    (array([1, 1, 0],'i'), array([ 0.43588989,  0.73484692,  0.83066239])) 
 
    &quot;&quot;&quot;</span>
    <span class="s1">obs = _asarray_validated(obs</span><span class="s2">, </span><span class="s1">check_finite=check_finite)</span>
    <span class="s1">code_book = _asarray_validated(code_book</span><span class="s2">, </span><span class="s1">check_finite=check_finite)</span>
    <span class="s1">ct = np.common_type(obs</span><span class="s2">, </span><span class="s1">code_book)</span>

    <span class="s1">c_obs = obs.astype(ct</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">c_code_book = code_book.astype(ct</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">np.issubdtype(ct</span><span class="s2">, </span><span class="s1">np.float64) </span><span class="s2">or </span><span class="s1">np.issubdtype(ct</span><span class="s2">, </span><span class="s1">np.float32):</span>
        <span class="s2">return </span><span class="s1">_vq.vq(c_obs</span><span class="s2">, </span><span class="s1">c_code_book)</span>
    <span class="s2">return </span><span class="s1">py_vq(obs</span><span class="s2">, </span><span class="s1">code_book</span><span class="s2">, </span><span class="s1">check_finite=</span><span class="s2">False</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">py_vq(obs</span><span class="s2">, </span><span class="s1">code_book</span><span class="s2">, </span><span class="s1">check_finite=</span><span class="s2">True</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; Python version of vq algorithm. 
 
    The algorithm computes the Euclidean distance between each 
    observation and every frame in the code_book. 
 
    Parameters 
    ---------- 
    obs : ndarray 
        Expects a rank 2 array. Each row is one observation. 
    code_book : ndarray 
        Code book to use. Same format than obs. Should have same number of 
        features (e.g., columns) than obs. 
    check_finite : bool, optional 
        Whether to check that the input matrices contain only finite numbers. 
        Disabling may give a performance gain, but may result in problems 
        (crashes, non-termination) if the inputs do contain infinities or NaNs. 
        Default: True 
 
    Returns 
    ------- 
    code : ndarray 
        code[i] gives the label of the ith obversation; its code is 
        code_book[code[i]]. 
    mind_dist : ndarray 
        min_dist[i] gives the distance between the ith observation and its 
        corresponding code. 
 
    Notes 
    ----- 
    This function is slower than the C version but works for 
    all input types. If the inputs have the wrong types for the 
    C versions of the function, this one is called as a last resort. 
 
    It is about 20 times slower than the C version. 
 
    &quot;&quot;&quot;</span>
    <span class="s1">obs = _asarray_validated(obs</span><span class="s2">, </span><span class="s1">check_finite=check_finite)</span>
    <span class="s1">code_book = _asarray_validated(code_book</span><span class="s2">, </span><span class="s1">check_finite=check_finite)</span>

    <span class="s2">if </span><span class="s1">obs.ndim != code_book.ndim:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Observation and code_book should have the same rank&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">obs.ndim == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">obs = obs[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">code_book = code_book[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>

    <span class="s1">dist = cdist(obs</span><span class="s2">, </span><span class="s1">code_book)</span>
    <span class="s1">code = dist.argmin(axis=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">min_dist = dist[np.arange(len(code))</span><span class="s2">, </span><span class="s1">code]</span>
    <span class="s2">return </span><span class="s1">code</span><span class="s2">, </span><span class="s1">min_dist</span>


<span class="s2">def </span><span class="s1">_kmeans(obs</span><span class="s2">, </span><span class="s1">guess</span><span class="s2">, </span><span class="s1">thresh=</span><span class="s4">1e-5</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; &quot;raw&quot; version of k-means. 
 
    Returns 
    ------- 
    code_book 
        The lowest distortion codebook found. 
    avg_dist 
        The average distance a observation is from a code in the book. 
        Lower means the code_book matches the data better. 
 
    See Also 
    -------- 
    kmeans : wrapper around k-means 
 
    Examples 
    -------- 
    Note: not whitened in this example. 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.cluster.vq import _kmeans 
    &gt;&gt;&gt; features  = np.array([[ 1.9,2.3], 
    ...                       [ 1.5,2.5], 
    ...                       [ 0.8,0.6], 
    ...                       [ 0.4,1.8], 
    ...                       [ 1.0,1.0]]) 
    &gt;&gt;&gt; book = np.array((features[0],features[2])) 
    &gt;&gt;&gt; _kmeans(features,book) 
    (array([[ 1.7       ,  2.4       ], 
           [ 0.73333333,  1.13333333]]), 0.40563916697728591) 
 
    &quot;&quot;&quot;</span>

    <span class="s1">code_book = np.asarray(guess)</span>
    <span class="s1">diff = np.inf</span>
    <span class="s1">prev_avg_dists = deque([diff]</span><span class="s2">, </span><span class="s1">maxlen=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s2">while </span><span class="s1">diff &gt; thresh:</span>
        <span class="s5"># compute membership and distances between obs and code_book</span>
        <span class="s1">obs_code</span><span class="s2">, </span><span class="s1">distort = vq(obs</span><span class="s2">, </span><span class="s1">code_book</span><span class="s2">, </span><span class="s1">check_finite=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">prev_avg_dists.append(distort.mean(axis=-</span><span class="s4">1</span><span class="s1">))</span>
        <span class="s5"># recalc code_book as centroids of associated obs</span>
        <span class="s1">code_book</span><span class="s2">, </span><span class="s1">has_members = _vq.update_cluster_means(obs</span><span class="s2">, </span><span class="s1">obs_code</span><span class="s2">,</span>
                                                          <span class="s1">code_book.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">code_book = code_book[has_members]</span>
        <span class="s1">diff = np.absolute(prev_avg_dists[</span><span class="s4">0</span><span class="s1">] - prev_avg_dists[</span><span class="s4">1</span><span class="s1">])</span>

    <span class="s2">return </span><span class="s1">code_book</span><span class="s2">, </span><span class="s1">prev_avg_dists[</span><span class="s4">1</span><span class="s1">]</span>


<span class="s2">def </span><span class="s1">kmeans(obs</span><span class="s2">, </span><span class="s1">k_or_guess</span><span class="s2">, </span><span class="s1">iter=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">thresh=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">check_finite=</span><span class="s2">True,</span>
           <span class="s1">*</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Performs k-means on a set of observation vectors forming k clusters. 
 
    The k-means algorithm adjusts the classification of the observations 
    into clusters and updates the cluster centroids until the position of 
    the centroids is stable over successive iterations. In this 
    implementation of the algorithm, the stability of the centroids is 
    determined by comparing the absolute value of the change in the average 
    Euclidean distance between the observations and their corresponding 
    centroids against a threshold. This yields 
    a code book mapping centroids to codes and vice versa. 
 
    Parameters 
    ---------- 
    obs : ndarray 
       Each row of the M by N array is an observation vector. The 
       columns are the features seen during each observation. 
       The features must be whitened first with the `whiten` function. 
 
    k_or_guess : int or ndarray 
       The number of centroids to generate. A code is assigned to 
       each centroid, which is also the row index of the centroid 
       in the code_book matrix generated. 
 
       The initial k centroids are chosen by randomly selecting 
       observations from the observation matrix. Alternatively, 
       passing a k by N array specifies the initial k centroids. 
 
    iter : int, optional 
       The number of times to run k-means, returning the codebook 
       with the lowest distortion. This argument is ignored if 
       initial centroids are specified with an array for the 
       ``k_or_guess`` parameter. This parameter does not represent the 
       number of iterations of the k-means algorithm. 
 
    thresh : float, optional 
       Terminates the k-means algorithm if the change in 
       distortion since the last k-means iteration is less than 
       or equal to threshold. 
 
    check_finite : bool, optional 
        Whether to check that the input matrices contain only finite numbers. 
        Disabling may give a performance gain, but may result in problems 
        (crashes, non-termination) if the inputs do contain infinities or NaNs. 
        Default: True 
 
    seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional 
        Seed for initializing the pseudo-random number generator. 
        If `seed` is None (or `numpy.random`), the `numpy.random.RandomState` 
        singleton is used. 
        If `seed` is an int, a new ``RandomState`` instance is used, 
        seeded with `seed`. 
        If `seed` is already a ``Generator`` or ``RandomState`` instance then 
        that instance is used. 
        The default is None. 
 
    Returns 
    ------- 
    codebook : ndarray 
       A k by N array of k centroids. The ith centroid 
       codebook[i] is represented with the code i. The centroids 
       and codes generated represent the lowest distortion seen, 
       not necessarily the globally minimal distortion. 
       Note that the number of centroids is not necessarily the same as the 
       ``k_or_guess`` parameter, because centroids assigned to no observations 
       are removed during iterations. 
 
    distortion : float 
       The mean (non-squared) Euclidean distance between the observations 
       passed and the centroids generated. Note the difference to the standard 
       definition of distortion in the context of the k-means algorithm, which 
       is the sum of the squared distances. 
 
    See Also 
    -------- 
    kmeans2 : a different implementation of k-means clustering 
       with more methods for generating initial centroids but without 
       using a distortion change threshold as a stopping criterion. 
 
    whiten : must be called prior to passing an observation matrix 
       to kmeans. 
 
    Notes 
    ----- 
    For more functionalities or optimal performance, you can use 
    `sklearn.cluster.KMeans &lt;https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html&gt;`_. 
    `This &lt;https://hdbscan.readthedocs.io/en/latest/performance_and_scalability.html#comparison-of-high-performance-implementations&gt;`_ 
    is a benchmark result of several implementations. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.cluster.vq import vq, kmeans, whiten 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; features  = np.array([[ 1.9,2.3], 
    ...                       [ 1.5,2.5], 
    ...                       [ 0.8,0.6], 
    ...                       [ 0.4,1.8], 
    ...                       [ 0.1,0.1], 
    ...                       [ 0.2,1.8], 
    ...                       [ 2.0,0.5], 
    ...                       [ 0.3,1.5], 
    ...                       [ 1.0,1.0]]) 
    &gt;&gt;&gt; whitened = whiten(features) 
    &gt;&gt;&gt; book = np.array((whitened[0],whitened[2])) 
    &gt;&gt;&gt; kmeans(whitened,book) 
    (array([[ 2.3110306 ,  2.86287398],    # random 
           [ 0.93218041,  1.24398691]]), 0.85684700941625547) 
 
    &gt;&gt;&gt; codes = 3 
    &gt;&gt;&gt; kmeans(whitened,codes) 
    (array([[ 2.3110306 ,  2.86287398],    # random 
           [ 1.32544402,  0.65607529], 
           [ 0.40782893,  2.02786907]]), 0.5196582527686241) 
 
    &gt;&gt;&gt; # Create 50 datapoints in two clusters a and b 
    &gt;&gt;&gt; pts = 50 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; a = rng.multivariate_normal([0, 0], [[4, 1], [1, 4]], size=pts) 
    &gt;&gt;&gt; b = rng.multivariate_normal([30, 10], 
    ...                             [[10, 2], [2, 1]], 
    ...                             size=pts) 
    &gt;&gt;&gt; features = np.concatenate((a, b)) 
    &gt;&gt;&gt; # Whiten data 
    &gt;&gt;&gt; whitened = whiten(features) 
    &gt;&gt;&gt; # Find 2 clusters in the data 
    &gt;&gt;&gt; codebook, distortion = kmeans(whitened, 2) 
    &gt;&gt;&gt; # Plot whitened data and cluster centers in red 
    &gt;&gt;&gt; plt.scatter(whitened[:, 0], whitened[:, 1]) 
    &gt;&gt;&gt; plt.scatter(codebook[:, 0], codebook[:, 1], c='r') 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s1">obs = _asarray_validated(obs</span><span class="s2">, </span><span class="s1">check_finite=check_finite)</span>
    <span class="s2">if </span><span class="s1">iter &lt; </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;iter must be at least 1, got %s&quot; </span><span class="s1">% iter)</span>

    <span class="s5"># Determine whether a count (scalar) or an initial guess (array) was passed.</span>
    <span class="s2">if not </span><span class="s1">np.isscalar(k_or_guess):</span>
        <span class="s1">guess = _asarray_validated(k_or_guess</span><span class="s2">, </span><span class="s1">check_finite=check_finite)</span>
        <span class="s2">if </span><span class="s1">guess.size &lt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Asked for 0 clusters. Initial book was %s&quot; </span><span class="s1">%</span>
                             <span class="s1">guess)</span>
        <span class="s2">return </span><span class="s1">_kmeans(obs</span><span class="s2">, </span><span class="s1">guess</span><span class="s2">, </span><span class="s1">thresh=thresh)</span>

    <span class="s5"># k_or_guess is a scalar, now verify that it's an integer</span>
    <span class="s1">k = int(k_or_guess)</span>
    <span class="s2">if </span><span class="s1">k != k_or_guess:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;If k_or_guess is a scalar, it must be an integer.&quot;</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">k &lt; </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Asked for %d clusters.&quot; </span><span class="s1">% k)</span>

    <span class="s1">rng = check_random_state(seed)</span>

    <span class="s5"># initialize best distance value to a large value</span>
    <span class="s1">best_dist = np.inf</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(iter):</span>
        <span class="s5"># the initial code book is randomly selected from observations</span>
        <span class="s1">guess = _kpoints(obs</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">rng)</span>
        <span class="s1">book</span><span class="s2">, </span><span class="s1">dist = _kmeans(obs</span><span class="s2">, </span><span class="s1">guess</span><span class="s2">, </span><span class="s1">thresh=thresh)</span>
        <span class="s2">if </span><span class="s1">dist &lt; best_dist:</span>
            <span class="s1">best_book = book</span>
            <span class="s1">best_dist = dist</span>
    <span class="s2">return </span><span class="s1">best_book</span><span class="s2">, </span><span class="s1">best_dist</span>


<span class="s2">def </span><span class="s1">_kpoints(data</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">rng):</span>
    <span class="s0">&quot;&quot;&quot;Pick k points at random in data (one row = one observation). 
 
    Parameters 
    ---------- 
    data : ndarray 
        Expect a rank 1 or 2 array. Rank 1 are assumed to describe one 
        dimensional data, rank 2 multidimensional data, in which case one 
        row is one observation. 
    k : int 
        Number of samples to generate. 
    rng : `numpy.random.Generator` or `numpy.random.RandomState` 
        Random number generator. 
 
    Returns 
    ------- 
    x : ndarray 
        A 'k' by 'N' containing the initial centroids 
 
    &quot;&quot;&quot;</span>
    <span class="s1">idx = rng.choice(data.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">size=k</span><span class="s2">, </span><span class="s1">replace=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">data[idx]</span>


<span class="s2">def </span><span class="s1">_krandinit(data</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">rng):</span>
    <span class="s0">&quot;&quot;&quot;Returns k samples of a random variable whose parameters depend on data. 
 
    More precisely, it returns k observations sampled from a Gaussian random 
    variable whose mean and covariances are the ones estimated from the data. 
 
    Parameters 
    ---------- 
    data : ndarray 
        Expect a rank 1 or 2 array. Rank 1 is assumed to describe 1-D 
        data, rank 2 multidimensional data, in which case one 
        row is one observation. 
    k : int 
        Number of samples to generate. 
    rng : `numpy.random.Generator` or `numpy.random.RandomState` 
        Random number generator. 
 
    Returns 
    ------- 
    x : ndarray 
        A 'k' by 'N' containing the initial centroids 
 
    &quot;&quot;&quot;</span>
    <span class="s1">mu = data.mean(axis=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">data.ndim == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">cov = np.cov(data)</span>
        <span class="s1">x = rng.standard_normal(size=k)</span>
        <span class="s1">x *= np.sqrt(cov)</span>
    <span class="s2">elif </span><span class="s1">data.shape[</span><span class="s4">1</span><span class="s1">] &gt; data.shape[</span><span class="s4">0</span><span class="s1">]:</span>
        <span class="s5"># initialize when the covariance matrix is rank deficient</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">vh = np.linalg.svd(data - mu</span><span class="s2">, </span><span class="s1">full_matrices=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">x = rng.standard_normal(size=(k</span><span class="s2">, </span><span class="s1">s.size))</span>
        <span class="s1">sVh = s[:</span><span class="s2">, None</span><span class="s1">] * vh / np.sqrt(data.shape[</span><span class="s4">0</span><span class="s1">] - </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">x = x.dot(sVh)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">cov = np.atleast_2d(np.cov(data</span><span class="s2">, </span><span class="s1">rowvar=</span><span class="s2">False</span><span class="s1">))</span>

        <span class="s5"># k rows, d cols (one row = one obs)</span>
        <span class="s5"># Generate k sample of a random variable ~ Gaussian(mu, cov)</span>
        <span class="s1">x = rng.standard_normal(size=(k</span><span class="s2">, </span><span class="s1">mu.size))</span>
        <span class="s1">x = x.dot(np.linalg.cholesky(cov).T)</span>

    <span class="s1">x += mu</span>
    <span class="s2">return </span><span class="s1">x</span>


<span class="s2">def </span><span class="s1">_kpp(data</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">rng):</span>
    <span class="s0">&quot;&quot;&quot; Picks k points in the data based on the kmeans++ method. 
 
    Parameters 
    ---------- 
    data : ndarray 
        Expect a rank 1 or 2 array. Rank 1 is assumed to describe 1-D 
        data, rank 2 multidimensional data, in which case one 
        row is one observation. 
    k : int 
        Number of samples to generate. 
    rng : `numpy.random.Generator` or `numpy.random.RandomState` 
        Random number generator. 
 
    Returns 
    ------- 
    init : ndarray 
        A 'k' by 'N' containing the initial centroids. 
 
    References 
    ---------- 
    .. [1] D. Arthur and S. Vassilvitskii, &quot;k-means++: the advantages of 
       careful seeding&quot;, Proceedings of the Eighteenth Annual ACM-SIAM Symposium 
       on Discrete Algorithms, 2007. 
    &quot;&quot;&quot;</span>

    <span class="s1">dims = data.shape[</span><span class="s4">1</span><span class="s1">] </span><span class="s2">if </span><span class="s1">len(data.shape) &gt; </span><span class="s4">1 </span><span class="s2">else </span><span class="s4">1</span>
    <span class="s1">init = np.ndarray((k</span><span class="s2">, </span><span class="s1">dims))</span>

    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(k):</span>
        <span class="s2">if </span><span class="s1">i == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">init[i</span><span class="s2">, </span><span class="s1">:] = data[rng_integers(rng</span><span class="s2">, </span><span class="s1">data.shape[</span><span class="s4">0</span><span class="s1">])]</span>

        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">D2 = cdist(init[:i</span><span class="s2">,</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">metric=</span><span class="s3">'sqeuclidean'</span><span class="s1">).min(axis=</span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">probs = D2/D2.sum()</span>
            <span class="s1">cumprobs = probs.cumsum()</span>
            <span class="s1">r = rng.uniform()</span>
            <span class="s1">init[i</span><span class="s2">, </span><span class="s1">:] = data[np.searchsorted(cumprobs</span><span class="s2">, </span><span class="s1">r)]</span>

    <span class="s2">return </span><span class="s1">init</span>


<span class="s1">_valid_init_meth = {</span><span class="s3">'random'</span><span class="s1">: _krandinit</span><span class="s2">, </span><span class="s3">'points'</span><span class="s1">: _kpoints</span><span class="s2">, </span><span class="s3">'++'</span><span class="s1">: _kpp}</span>


<span class="s2">def </span><span class="s1">_missing_warn():</span>
    <span class="s0">&quot;&quot;&quot;Print a warning when called.&quot;&quot;&quot;</span>
    <span class="s1">warnings.warn(</span><span class="s3">&quot;One of the clusters is empty. &quot;</span>
                  <span class="s3">&quot;Re-run kmeans with a different initialization.&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">_missing_raise():</span>
    <span class="s0">&quot;&quot;&quot;Raise a ClusterError when called.&quot;&quot;&quot;</span>
    <span class="s2">raise </span><span class="s1">ClusterError(</span><span class="s3">&quot;One of the clusters is empty. &quot;</span>
                       <span class="s3">&quot;Re-run kmeans with a different initialization.&quot;</span><span class="s1">)</span>


<span class="s1">_valid_miss_meth = {</span><span class="s3">'warn'</span><span class="s1">: _missing_warn</span><span class="s2">, </span><span class="s3">'raise'</span><span class="s1">: _missing_raise}</span>


<span class="s2">def </span><span class="s1">kmeans2(data</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">iter=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">thresh=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">minit=</span><span class="s3">'random'</span><span class="s2">,</span>
            <span class="s1">missing=</span><span class="s3">'warn'</span><span class="s2">, </span><span class="s1">check_finite=</span><span class="s2">True, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Classify a set of observations into k clusters using the k-means algorithm. 
 
    The algorithm attempts to minimize the Euclidean distance between 
    observations and centroids. Several initialization methods are 
    included. 
 
    Parameters 
    ---------- 
    data : ndarray 
        A 'M' by 'N' array of 'M' observations in 'N' dimensions or a length 
        'M' array of 'M' 1-D observations. 
    k : int or ndarray 
        The number of clusters to form as well as the number of 
        centroids to generate. If `minit` initialization string is 
        'matrix', or if a ndarray is given instead, it is 
        interpreted as initial cluster to use instead. 
    iter : int, optional 
        Number of iterations of the k-means algorithm to run. Note 
        that this differs in meaning from the iters parameter to 
        the kmeans function. 
    thresh : float, optional 
        (not used yet) 
    minit : str, optional 
        Method for initialization. Available methods are 'random', 
        'points', '++' and 'matrix': 
 
        'random': generate k centroids from a Gaussian with mean and 
        variance estimated from the data. 
 
        'points': choose k observations (rows) at random from data for 
        the initial centroids. 
 
        '++': choose k observations accordingly to the kmeans++ method 
        (careful seeding) 
 
        'matrix': interpret the k parameter as a k by M (or length k 
        array for 1-D data) array of initial centroids. 
    missing : str, optional 
        Method to deal with empty clusters. Available methods are 
        'warn' and 'raise': 
 
        'warn': give a warning and continue. 
 
        'raise': raise an ClusterError and terminate the algorithm. 
    check_finite : bool, optional 
        Whether to check that the input matrices contain only finite numbers. 
        Disabling may give a performance gain, but may result in problems 
        (crashes, non-termination) if the inputs do contain infinities or NaNs. 
        Default: True 
    seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional 
        Seed for initializing the pseudo-random number generator. 
        If `seed` is None (or `numpy.random`), the `numpy.random.RandomState` 
        singleton is used. 
        If `seed` is an int, a new ``RandomState`` instance is used, 
        seeded with `seed`. 
        If `seed` is already a ``Generator`` or ``RandomState`` instance then 
        that instance is used. 
        The default is None. 
 
    Returns 
    ------- 
    centroid : ndarray 
        A 'k' by 'N' array of centroids found at the last iteration of 
        k-means. 
    label : ndarray 
        label[i] is the code or index of the centroid the 
        ith observation is closest to. 
 
    See Also 
    -------- 
    kmeans 
 
    References 
    ---------- 
    .. [1] D. Arthur and S. Vassilvitskii, &quot;k-means++: the advantages of 
       careful seeding&quot;, Proceedings of the Eighteenth Annual ACM-SIAM Symposium 
       on Discrete Algorithms, 2007. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from scipy.cluster.vq import kmeans2 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; import numpy as np 
 
    Create z, an array with shape (100, 2) containing a mixture of samples 
    from three multivariate normal distributions. 
 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; a = rng.multivariate_normal([0, 6], [[2, 1], [1, 1.5]], size=45) 
    &gt;&gt;&gt; b = rng.multivariate_normal([2, 0], [[1, -1], [-1, 3]], size=30) 
    &gt;&gt;&gt; c = rng.multivariate_normal([6, 4], [[5, 0], [0, 1.2]], size=25) 
    &gt;&gt;&gt; z = np.concatenate((a, b, c)) 
    &gt;&gt;&gt; rng.shuffle(z) 
 
    Compute three clusters. 
 
    &gt;&gt;&gt; centroid, label = kmeans2(z, 3, minit='points') 
    &gt;&gt;&gt; centroid 
    array([[ 2.22274463, -0.61666946],  # may vary 
           [ 0.54069047,  5.86541444], 
           [ 6.73846769,  4.01991898]]) 
 
    How many points are in each cluster? 
 
    &gt;&gt;&gt; counts = np.bincount(label) 
    &gt;&gt;&gt; counts 
    array([29, 51, 20])  # may vary 
 
    Plot the clusters. 
 
    &gt;&gt;&gt; w0 = z[label == 0] 
    &gt;&gt;&gt; w1 = z[label == 1] 
    &gt;&gt;&gt; w2 = z[label == 2] 
    &gt;&gt;&gt; plt.plot(w0[:, 0], w0[:, 1], 'o', alpha=0.5, label='cluster 0') 
    &gt;&gt;&gt; plt.plot(w1[:, 0], w1[:, 1], 'd', alpha=0.5, label='cluster 1') 
    &gt;&gt;&gt; plt.plot(w2[:, 0], w2[:, 1], 's', alpha=0.5, label='cluster 2') 
    &gt;&gt;&gt; plt.plot(centroid[:, 0], centroid[:, 1], 'k*', label='centroids') 
    &gt;&gt;&gt; plt.axis('equal') 
    &gt;&gt;&gt; plt.legend(shadow=True) 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">int(iter) &lt; </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Invalid iter (%s), &quot;</span>
                         <span class="s3">&quot;must be a positive integer.&quot; </span><span class="s1">% iter)</span>
    <span class="s2">try</span><span class="s1">:</span>
        <span class="s1">miss_meth = _valid_miss_meth[missing]</span>
    <span class="s2">except </span><span class="s1">KeyError </span><span class="s2">as </span><span class="s1">e:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;Unknown missing method </span><span class="s2">{</span><span class="s1">missing</span><span class="s2">!r}</span><span class="s3">&quot;</span><span class="s1">) </span><span class="s2">from </span><span class="s1">e</span>

    <span class="s1">data = _asarray_validated(data</span><span class="s2">, </span><span class="s1">check_finite=check_finite)</span>
    <span class="s2">if </span><span class="s1">data.ndim == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">d = </span><span class="s4">1</span>
    <span class="s2">elif </span><span class="s1">data.ndim == </span><span class="s4">2</span><span class="s1">:</span>
        <span class="s1">d = data.shape[</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Input of rank &gt; 2 is not supported.&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">data.size &lt; </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Empty input is not supported.&quot;</span><span class="s1">)</span>

    <span class="s5"># If k is not a single value, it should be compatible with data's shape</span>
    <span class="s2">if </span><span class="s1">minit == </span><span class="s3">'matrix' </span><span class="s2">or not </span><span class="s1">np.isscalar(k):</span>
        <span class="s1">code_book = np.array(k</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">data.ndim != code_book.ndim:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;k array doesn't match data rank&quot;</span><span class="s1">)</span>
        <span class="s1">nc = len(code_book)</span>
        <span class="s2">if </span><span class="s1">data.ndim &gt; </span><span class="s4">1 </span><span class="s2">and </span><span class="s1">code_book.shape[</span><span class="s4">1</span><span class="s1">] != d:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;k array doesn't match data dimension&quot;</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">nc = int(k)</span>

        <span class="s2">if </span><span class="s1">nc &lt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Cannot ask kmeans2 for %d clusters&quot;</span>
                             <span class="s3">&quot; (k was %s)&quot; </span><span class="s1">% (nc</span><span class="s2">, </span><span class="s1">k))</span>
        <span class="s2">elif </span><span class="s1">nc != k:</span>
            <span class="s1">warnings.warn(</span><span class="s3">&quot;k was not an integer, was converted.&quot;</span><span class="s1">)</span>

        <span class="s2">try</span><span class="s1">:</span>
            <span class="s1">init_meth = _valid_init_meth[minit]</span>
        <span class="s2">except </span><span class="s1">KeyError </span><span class="s2">as </span><span class="s1">e:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;Unknown init method </span><span class="s2">{</span><span class="s1">minit</span><span class="s2">!r}</span><span class="s3">&quot;</span><span class="s1">) </span><span class="s2">from </span><span class="s1">e</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">rng = check_random_state(seed)</span>
            <span class="s1">code_book = init_meth(data</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">rng)</span>

    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(iter):</span>
        <span class="s5"># Compute the nearest neighbor for each obs using the current code book</span>
        <span class="s1">label = vq(data</span><span class="s2">, </span><span class="s1">code_book</span><span class="s2">, </span><span class="s1">check_finite=check_finite)[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s5"># Update the code book by computing centroids</span>
        <span class="s1">new_code_book</span><span class="s2">, </span><span class="s1">has_members = _vq.update_cluster_means(data</span><span class="s2">, </span><span class="s1">label</span><span class="s2">, </span><span class="s1">nc)</span>
        <span class="s2">if not </span><span class="s1">has_members.all():</span>
            <span class="s1">miss_meth()</span>
            <span class="s5"># Set the empty clusters to their previous positions</span>
            <span class="s1">new_code_book[~has_members] = code_book[~has_members]</span>
        <span class="s1">code_book = new_code_book</span>

    <span class="s2">return </span><span class="s1">code_book</span><span class="s2">, </span><span class="s1">label</span>
</pre>
</body>
</html>