<html>
<head>
<title>test_gpr.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_gpr.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Testing for Gaussian process regression &quot;&quot;&quot;</span>

<span class="s2"># Author: Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de&gt;</span>
<span class="s2"># Modified by: Pete Green &lt;p.l.green@liverpool.ac.uk&gt;</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">re</span>
<span class="s3">import </span><span class="s1">sys</span>
<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">pytest</span>
<span class="s3">from </span><span class="s1">scipy.optimize </span><span class="s3">import </span><span class="s1">approx_fprime</span>

<span class="s3">from </span><span class="s1">sklearn.exceptions </span><span class="s3">import </span><span class="s1">ConvergenceWarning</span>
<span class="s3">from </span><span class="s1">sklearn.gaussian_process </span><span class="s3">import </span><span class="s1">GaussianProcessRegressor</span>
<span class="s3">from </span><span class="s1">sklearn.gaussian_process.kernels </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">RBF</span><span class="s3">,</span>
    <span class="s1">DotProduct</span><span class="s3">,</span>
    <span class="s1">ExpSineSquared</span><span class="s3">,</span>
    <span class="s1">WhiteKernel</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">sklearn.gaussian_process.kernels </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">ConstantKernel </span><span class="s3">as </span><span class="s1">C</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">sklearn.gaussian_process.tests._mini_sequence_kernel </span><span class="s3">import </span><span class="s1">MiniSeqKernel</span>
<span class="s3">from </span><span class="s1">sklearn.utils._testing </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s3">,</span>
    <span class="s1">assert_almost_equal</span><span class="s3">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s3">,</span>
    <span class="s1">assert_array_less</span><span class="s3">,</span>
<span class="s1">)</span>


<span class="s3">def </span><span class="s1">f(x):</span>
    <span class="s3">return </span><span class="s1">x * np.sin(x)</span>


<span class="s1">X = np.atleast_2d([</span><span class="s4">1.0</span><span class="s3">, </span><span class="s4">3.0</span><span class="s3">, </span><span class="s4">5.0</span><span class="s3">, </span><span class="s4">6.0</span><span class="s3">, </span><span class="s4">7.0</span><span class="s3">, </span><span class="s4">8.0</span><span class="s1">]).T</span>
<span class="s1">X2 = np.atleast_2d([</span><span class="s4">2.0</span><span class="s3">, </span><span class="s4">4.0</span><span class="s3">, </span><span class="s4">5.5</span><span class="s3">, </span><span class="s4">6.5</span><span class="s3">, </span><span class="s4">7.5</span><span class="s1">]).T</span>
<span class="s1">y = f(X).ravel()</span>

<span class="s1">fixed_kernel = RBF(length_scale=</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">length_scale_bounds=</span><span class="s5">&quot;fixed&quot;</span><span class="s1">)</span>
<span class="s1">kernels = [</span>
    <span class="s1">RBF(length_scale=</span><span class="s4">1.0</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">fixed_kernel</span><span class="s3">,</span>
    <span class="s1">RBF(length_scale=</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">length_scale_bounds=(</span><span class="s4">1e-3</span><span class="s3">, </span><span class="s4">1e3</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">C(</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1e-2</span><span class="s3">, </span><span class="s4">1e2</span><span class="s1">)) * RBF(length_scale=</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">length_scale_bounds=(</span><span class="s4">1e-3</span><span class="s3">, </span><span class="s4">1e3</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">C(</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1e-2</span><span class="s3">, </span><span class="s4">1e2</span><span class="s1">)) * RBF(length_scale=</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">length_scale_bounds=(</span><span class="s4">1e-3</span><span class="s3">, </span><span class="s4">1e3</span><span class="s1">))</span>
    <span class="s1">+ C(</span><span class="s4">1e-5</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1e-5</span><span class="s3">, </span><span class="s4">1e2</span><span class="s1">))</span><span class="s3">,</span>
    <span class="s1">C(</span><span class="s4">0.1</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1e-2</span><span class="s3">, </span><span class="s4">1e2</span><span class="s1">)) * RBF(length_scale=</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">length_scale_bounds=(</span><span class="s4">1e-3</span><span class="s3">, </span><span class="s4">1e3</span><span class="s1">))</span>
    <span class="s1">+ C(</span><span class="s4">1e-5</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1e-5</span><span class="s3">, </span><span class="s4">1e2</span><span class="s1">))</span><span class="s3">,</span>
<span class="s1">]</span>
<span class="s1">non_fixed_kernels = [kernel </span><span class="s3">for </span><span class="s1">kernel </span><span class="s3">in </span><span class="s1">kernels </span><span class="s3">if </span><span class="s1">kernel != fixed_kernel]</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_gpr_interpolation(kernel):</span>
    <span class="s3">if </span><span class="s1">sys.maxsize &lt;= </span><span class="s4">2</span><span class="s1">**</span><span class="s4">32</span><span class="s1">:</span>
        <span class="s1">pytest.xfail(</span><span class="s5">&quot;This test may fail on 32 bit Python&quot;</span><span class="s1">)</span>

    <span class="s2"># Test the interpolating property for different kernels.</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">y_pred</span><span class="s3">, </span><span class="s1">y_cov = gpr.predict(X</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">assert_almost_equal(y_pred</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(np.diag(y_cov)</span><span class="s3">, </span><span class="s4">0.0</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_gpr_interpolation_structured():</span>
    <span class="s2"># Test the interpolating property for different kernels.</span>
    <span class="s1">kernel = MiniSeqKernel(baseline_similarity_bounds=</span><span class="s5">&quot;fixed&quot;</span><span class="s1">)</span>
    <span class="s1">X = [</span><span class="s5">&quot;A&quot;</span><span class="s3">, </span><span class="s5">&quot;B&quot;</span><span class="s3">, </span><span class="s5">&quot;C&quot;</span><span class="s1">]</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">3</span><span class="s1">])</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">y_pred</span><span class="s3">, </span><span class="s1">y_cov = gpr.predict(X</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">assert_almost_equal(</span>
        <span class="s1">kernel(X</span><span class="s3">, </span><span class="s1">eval_gradient=</span><span class="s3">True</span><span class="s1">)[</span><span class="s4">1</span><span class="s1">].ravel()</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1 </span><span class="s1">- np.eye(len(X))).ravel()</span>
    <span class="s1">)</span>
    <span class="s1">assert_almost_equal(y_pred</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(np.diag(y_cov)</span><span class="s3">, </span><span class="s4">0.0</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">non_fixed_kernels)</span>
<span class="s3">def </span><span class="s1">test_lml_improving(kernel):</span>
    <span class="s3">if </span><span class="s1">sys.maxsize &lt;= </span><span class="s4">2</span><span class="s1">**</span><span class="s4">32</span><span class="s1">:</span>
        <span class="s1">pytest.xfail(</span><span class="s5">&quot;This test may fail on 32 bit Python&quot;</span><span class="s1">)</span>

    <span class="s2"># Test that hyperparameter-tuning improves log-marginal likelihood.</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">gpr.log_marginal_likelihood(gpr.kernel_.theta) &gt; gpr.log_marginal_likelihood(</span>
        <span class="s1">kernel.theta</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_lml_precomputed(kernel):</span>
    <span class="s2"># Test that lml of optimized kernel is stored correctly.</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">gpr.log_marginal_likelihood(gpr.kernel_.theta) == pytest.approx(</span>
        <span class="s1">gpr.log_marginal_likelihood()</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_lml_without_cloning_kernel(kernel):</span>
    <span class="s2"># Test that lml of optimized kernel is stored correctly.</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">input_theta = np.ones(gpr.kernel_.theta.shape</span><span class="s3">, </span><span class="s1">dtype=np.float64)</span>

    <span class="s1">gpr.log_marginal_likelihood(input_theta</span><span class="s3">, </span><span class="s1">clone_kernel=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(gpr.kernel_.theta</span><span class="s3">, </span><span class="s1">input_theta</span><span class="s3">, </span><span class="s4">7</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">non_fixed_kernels)</span>
<span class="s3">def </span><span class="s1">test_converged_to_local_maximum(kernel):</span>
    <span class="s2"># Test that we are in local maximum after hyperparameter-optimization.</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">lml</span><span class="s3">, </span><span class="s1">lml_gradient = gpr.log_marginal_likelihood(gpr.kernel_.theta</span><span class="s3">, True</span><span class="s1">)</span>

    <span class="s3">assert </span><span class="s1">np.all(</span>
        <span class="s1">(np.abs(lml_gradient) &lt; </span><span class="s4">1e-4</span><span class="s1">)</span>
        <span class="s1">| (gpr.kernel_.theta == gpr.kernel_.bounds[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">| (gpr.kernel_.theta == gpr.kernel_.bounds[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">non_fixed_kernels)</span>
<span class="s3">def </span><span class="s1">test_solution_inside_bounds(kernel):</span>
    <span class="s2"># Test that hyperparameter-optimization remains in bounds#</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">bounds = gpr.kernel_.bounds</span>
    <span class="s1">max_ = np.finfo(gpr.kernel_.theta.dtype).max</span>
    <span class="s1">tiny = </span><span class="s4">1e-10</span>
    <span class="s1">bounds[~np.isfinite(bounds[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">])</span><span class="s3">, </span><span class="s4">1</span><span class="s1">] = max_</span>

    <span class="s1">assert_array_less(bounds[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">gpr.kernel_.theta + tiny)</span>
    <span class="s1">assert_array_less(gpr.kernel_.theta</span><span class="s3">, </span><span class="s1">bounds[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">] + tiny)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_lml_gradient(kernel):</span>
    <span class="s2"># Compare analytic and numeric gradient of log marginal likelihood.</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">lml</span><span class="s3">, </span><span class="s1">lml_gradient = gpr.log_marginal_likelihood(kernel.theta</span><span class="s3">, True</span><span class="s1">)</span>
    <span class="s1">lml_gradient_approx = approx_fprime(</span>
        <span class="s1">kernel.theta</span><span class="s3">, lambda </span><span class="s1">theta: gpr.log_marginal_likelihood(theta</span><span class="s3">, False</span><span class="s1">)</span><span class="s3">, </span><span class="s4">1e-10</span>
    <span class="s1">)</span>

    <span class="s1">assert_almost_equal(lml_gradient</span><span class="s3">, </span><span class="s1">lml_gradient_approx</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_prior(kernel):</span>
    <span class="s2"># Test that GP prior has mean 0 and identical variances.</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel)</span>

    <span class="s1">y_mean</span><span class="s3">, </span><span class="s1">y_cov = gpr.predict(X</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">assert_almost_equal(y_mean</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">5</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">len(gpr.kernel.theta) &gt; </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s2"># XXX: quite hacky, works only for current kernels</span>
        <span class="s1">assert_almost_equal(np.diag(y_cov)</span><span class="s3">, </span><span class="s1">np.exp(kernel.theta[</span><span class="s4">0</span><span class="s1">])</span><span class="s3">, </span><span class="s4">5</span><span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">assert_almost_equal(np.diag(y_cov)</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">5</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_sample_statistics(kernel):</span>
    <span class="s2"># Test that statistics of samples drawn from GP are correct.</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">y_mean</span><span class="s3">, </span><span class="s1">y_cov = gpr.predict(X2</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">samples = gpr.sample_y(X2</span><span class="s3">, </span><span class="s4">300000</span><span class="s1">)</span>

    <span class="s2"># More digits accuracy would require many more samples</span>
    <span class="s1">assert_almost_equal(y_mean</span><span class="s3">, </span><span class="s1">np.mean(samples</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(</span>
        <span class="s1">np.diag(y_cov) / np.diag(y_cov).max()</span><span class="s3">,</span>
        <span class="s1">np.var(samples</span><span class="s3">, </span><span class="s4">1</span><span class="s1">) / np.diag(y_cov).max()</span><span class="s3">,</span>
        <span class="s4">1</span><span class="s3">,</span>
    <span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_no_optimizer():</span>
    <span class="s2"># Test that kernel parameters are unmodified when optimizer is None.</span>
    <span class="s1">kernel = RBF(</span><span class="s4">1.0</span><span class="s1">)</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel</span><span class="s3">, </span><span class="s1">optimizer=</span><span class="s3">None</span><span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">np.exp(gpr.kernel_.theta) == </span><span class="s4">1.0</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;target&quot;</span><span class="s3">, </span><span class="s1">[y</span><span class="s3">, </span><span class="s1">np.ones(X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.float64)])</span>
<span class="s3">def </span><span class="s1">test_predict_cov_vs_std(kernel</span><span class="s3">, </span><span class="s1">target):</span>
    <span class="s3">if </span><span class="s1">sys.maxsize &lt;= </span><span class="s4">2</span><span class="s1">**</span><span class="s4">32</span><span class="s1">:</span>
        <span class="s1">pytest.xfail(</span><span class="s5">&quot;This test may fail on 32 bit Python&quot;</span><span class="s1">)</span>

    <span class="s2"># Test that predicted std.-dev. is consistent with cov's diagonal.</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">y_mean</span><span class="s3">, </span><span class="s1">y_cov = gpr.predict(X2</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">y_mean</span><span class="s3">, </span><span class="s1">y_std = gpr.predict(X2</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(np.sqrt(np.diag(y_cov))</span><span class="s3">, </span><span class="s1">y_std)</span>


<span class="s3">def </span><span class="s1">test_anisotropic_kernel():</span>
    <span class="s2"># Test that GPR can identify meaningful anisotropic length-scales.</span>
    <span class="s2"># We learn a function which varies in one dimension ten-times slower</span>
    <span class="s2"># than in the other. The corresponding length-scales should differ by at</span>
    <span class="s2"># least a factor 5</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = rng.uniform(-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s1">(</span><span class="s4">50</span><span class="s3">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">y = X[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">] + </span><span class="s4">0.1 </span><span class="s1">* X[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span>

    <span class="s1">kernel = RBF([</span><span class="s4">1.0</span><span class="s3">, </span><span class="s4">1.0</span><span class="s1">])</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">np.exp(gpr.kernel_.theta[</span><span class="s4">1</span><span class="s1">]) &gt; np.exp(gpr.kernel_.theta[</span><span class="s4">0</span><span class="s1">]) * </span><span class="s4">5</span>


<span class="s3">def </span><span class="s1">test_random_starts():</span>
    <span class="s2"># Test that an increasing number of random-starts of GP fitting only</span>
    <span class="s2"># increases the log marginal likelihood of the chosen theta.</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = </span><span class="s4">25</span><span class="s3">, </span><span class="s4">2</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s3">, </span><span class="s1">n_features) * </span><span class="s4">2 </span><span class="s1">- </span><span class="s4">1</span>
    <span class="s1">y = (</span>
        <span class="s1">np.sin(X).sum(axis=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">+ np.sin(</span><span class="s4">3 </span><span class="s1">* X).sum(axis=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">+ rng.normal(scale=</span><span class="s4">0.1</span><span class="s3">, </span><span class="s1">size=n_samples)</span>
    <span class="s1">)</span>

    <span class="s1">kernel = C(</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1e-2</span><span class="s3">, </span><span class="s4">1e2</span><span class="s1">)) * RBF(</span>
        <span class="s1">length_scale=[</span><span class="s4">1.0</span><span class="s1">] * n_features</span><span class="s3">, </span><span class="s1">length_scale_bounds=[(</span><span class="s4">1e-4</span><span class="s3">, </span><span class="s4">1e2</span><span class="s1">)] * n_features</span>
    <span class="s1">) + WhiteKernel(noise_level=</span><span class="s4">1e-5</span><span class="s3">, </span><span class="s1">noise_level_bounds=(</span><span class="s4">1e-5</span><span class="s3">, </span><span class="s4">1e1</span><span class="s1">))</span>
    <span class="s1">last_lml = -np.inf</span>
    <span class="s3">for </span><span class="s1">n_restarts_optimizer </span><span class="s3">in </span><span class="s1">range(</span><span class="s4">5</span><span class="s1">):</span>
        <span class="s1">gp = GaussianProcessRegressor(</span>
            <span class="s1">kernel=kernel</span><span class="s3">,</span>
            <span class="s1">n_restarts_optimizer=n_restarts_optimizer</span><span class="s3">,</span>
            <span class="s1">random_state=</span><span class="s4">0</span><span class="s3">,</span>
        <span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s1">lml = gp.log_marginal_likelihood(gp.kernel_.theta)</span>
        <span class="s3">assert </span><span class="s1">lml &gt; last_lml - np.finfo(np.float32).eps</span>
        <span class="s1">last_lml = lml</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_y_normalization(kernel):</span>
    <span class="s0">&quot;&quot;&quot; 
    Test normalization of the target values in GP 
 
    Fitting non-normalizing GP on normalized y and fitting normalizing GP 
    on unnormalized y should yield identical results. Note that, here, 
    'normalized y' refers to y that has been made zero mean and unit 
    variance. 
 
    &quot;&quot;&quot;</span>

    <span class="s1">y_mean = np.mean(y)</span>
    <span class="s1">y_std = np.std(y)</span>
    <span class="s1">y_norm = (y - y_mean) / y_std</span>

    <span class="s2"># Fit non-normalizing GP on normalized y</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel)</span>
    <span class="s1">gpr.fit(X</span><span class="s3">, </span><span class="s1">y_norm)</span>

    <span class="s2"># Fit normalizing GP on unnormalized y</span>
    <span class="s1">gpr_norm = GaussianProcessRegressor(kernel=kernel</span><span class="s3">, </span><span class="s1">normalize_y=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">gpr_norm.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s2"># Compare predicted mean, std-devs and covariances</span>
    <span class="s1">y_pred</span><span class="s3">, </span><span class="s1">y_pred_std = gpr.predict(X2</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">y_pred = y_pred * y_std + y_mean</span>
    <span class="s1">y_pred_std = y_pred_std * y_std</span>
    <span class="s1">y_pred_norm</span><span class="s3">, </span><span class="s1">y_pred_std_norm = gpr_norm.predict(X2</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">assert_almost_equal(y_pred</span><span class="s3">, </span><span class="s1">y_pred_norm)</span>
    <span class="s1">assert_almost_equal(y_pred_std</span><span class="s3">, </span><span class="s1">y_pred_std_norm)</span>

    <span class="s1">_</span><span class="s3">, </span><span class="s1">y_cov = gpr.predict(X2</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">y_cov = y_cov * y_std**</span><span class="s4">2</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">y_cov_norm = gpr_norm.predict(X2</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">assert_almost_equal(y_cov</span><span class="s3">, </span><span class="s1">y_cov_norm)</span>


<span class="s3">def </span><span class="s1">test_large_variance_y():</span>
    <span class="s0">&quot;&quot;&quot; 
    Here we test that, when noramlize_y=True, our GP can produce a 
    sensible fit to training data whose variance is significantly 
    larger than unity. This test was made in response to issue #15612. 
 
    GP predictions are verified against predictions that were made 
    using GPy which, here, is treated as the 'gold standard'. Note that we 
    only investigate the RBF kernel here, as that is what was used in the 
    GPy implementation. 
 
    The following code can be used to recreate the GPy data: 
 
    -------------------------------------------------------------------------- 
    import GPy 
 
    kernel_gpy = GPy.kern.RBF(input_dim=1, lengthscale=1.) 
    gpy = GPy.models.GPRegression(X, np.vstack(y_large), kernel_gpy) 
    gpy.optimize() 
    y_pred_gpy, y_var_gpy = gpy.predict(X2) 
    y_pred_std_gpy = np.sqrt(y_var_gpy) 
    -------------------------------------------------------------------------- 
    &quot;&quot;&quot;</span>

    <span class="s2"># Here we utilise a larger variance version of the training data</span>
    <span class="s1">y_large = </span><span class="s4">10 </span><span class="s1">* y</span>

    <span class="s2"># Standard GP with normalize_y=True</span>
    <span class="s1">RBF_params = {</span><span class="s5">&quot;length_scale&quot;</span><span class="s1">: </span><span class="s4">1.0</span><span class="s1">}</span>
    <span class="s1">kernel = RBF(**RBF_params)</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel</span><span class="s3">, </span><span class="s1">normalize_y=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">gpr.fit(X</span><span class="s3">, </span><span class="s1">y_large)</span>
    <span class="s1">y_pred</span><span class="s3">, </span><span class="s1">y_pred_std = gpr.predict(X2</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s2"># 'Gold standard' mean predictions from GPy</span>
    <span class="s1">y_pred_gpy = np.array(</span>
        <span class="s1">[</span><span class="s4">15.16918303</span><span class="s3">, </span><span class="s1">-</span><span class="s4">27.98707845</span><span class="s3">, </span><span class="s1">-</span><span class="s4">39.31636019</span><span class="s3">, </span><span class="s4">14.52605515</span><span class="s3">, </span><span class="s4">69.18503589</span><span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s2"># 'Gold standard' std predictions from GPy</span>
    <span class="s1">y_pred_std_gpy = np.array(</span>
        <span class="s1">[</span><span class="s4">7.78860962</span><span class="s3">, </span><span class="s4">3.83179178</span><span class="s3">, </span><span class="s4">0.63149951</span><span class="s3">, </span><span class="s4">0.52745188</span><span class="s3">, </span><span class="s4">0.86170042</span><span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s2"># Based on numerical experiments, it's reasonable to expect our</span>
    <span class="s2"># GP's mean predictions to get within 7% of predictions of those</span>
    <span class="s2"># made by GPy.</span>
    <span class="s1">assert_allclose(y_pred</span><span class="s3">, </span><span class="s1">y_pred_gpy</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">0.07</span><span class="s3">, </span><span class="s1">atol=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s2"># Based on numerical experiments, it's reasonable to expect our</span>
    <span class="s2"># GP's std predictions to get within 15% of predictions of those</span>
    <span class="s2"># made by GPy.</span>
    <span class="s1">assert_allclose(y_pred_std</span><span class="s3">, </span><span class="s1">y_pred_std_gpy</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">0.15</span><span class="s3">, </span><span class="s1">atol=</span><span class="s4">0</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_y_multioutput():</span>
    <span class="s2"># Test that GPR can deal with multi-dimensional target values</span>
    <span class="s1">y_2d = np.vstack((y</span><span class="s3">, </span><span class="s1">y * </span><span class="s4">2</span><span class="s1">)).T</span>

    <span class="s2"># Test for fixed kernel that first dimension of 2d GP equals the output</span>
    <span class="s2"># of 1d GP and that second dimension is twice as large</span>
    <span class="s1">kernel = RBF(length_scale=</span><span class="s4">1.0</span><span class="s1">)</span>

    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel</span><span class="s3">, </span><span class="s1">optimizer=</span><span class="s3">None, </span><span class="s1">normalize_y=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">gpr.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">gpr_2d = GaussianProcessRegressor(kernel=kernel</span><span class="s3">, </span><span class="s1">optimizer=</span><span class="s3">None, </span><span class="s1">normalize_y=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">gpr_2d.fit(X</span><span class="s3">, </span><span class="s1">y_2d)</span>

    <span class="s1">y_pred_1d</span><span class="s3">, </span><span class="s1">y_std_1d = gpr.predict(X2</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">y_pred_2d</span><span class="s3">, </span><span class="s1">y_std_2d = gpr_2d.predict(X2</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">y_cov_1d = gpr.predict(X2</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">y_cov_2d = gpr_2d.predict(X2</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">assert_almost_equal(y_pred_1d</span><span class="s3">, </span><span class="s1">y_pred_2d[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">assert_almost_equal(y_pred_1d</span><span class="s3">, </span><span class="s1">y_pred_2d[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">] / </span><span class="s4">2</span><span class="s1">)</span>

    <span class="s2"># Standard deviation and covariance do not depend on output</span>
    <span class="s3">for </span><span class="s1">target </span><span class="s3">in </span><span class="s1">range(y_2d.shape[</span><span class="s4">1</span><span class="s1">]):</span>
        <span class="s1">assert_almost_equal(y_std_1d</span><span class="s3">, </span><span class="s1">y_std_2d[...</span><span class="s3">, </span><span class="s1">target])</span>
        <span class="s1">assert_almost_equal(y_cov_1d</span><span class="s3">, </span><span class="s1">y_cov_2d[...</span><span class="s3">, </span><span class="s1">target])</span>

    <span class="s1">y_sample_1d = gpr.sample_y(X2</span><span class="s3">, </span><span class="s1">n_samples=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">y_sample_2d = gpr_2d.sample_y(X2</span><span class="s3">, </span><span class="s1">n_samples=</span><span class="s4">10</span><span class="s1">)</span>

    <span class="s3">assert </span><span class="s1">y_sample_1d.shape == (</span><span class="s4">5</span><span class="s3">, </span><span class="s4">10</span><span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">y_sample_2d.shape == (</span><span class="s4">5</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">10</span><span class="s1">)</span>
    <span class="s2"># Only the first target will be equal</span>
    <span class="s1">assert_almost_equal(y_sample_1d</span><span class="s3">, </span><span class="s1">y_sample_2d[:</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">:])</span>

    <span class="s2"># Test hyperparameter optimization</span>
    <span class="s3">for </span><span class="s1">kernel </span><span class="s3">in </span><span class="s1">kernels:</span>
        <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel</span><span class="s3">, </span><span class="s1">normalize_y=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">gpr.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s1">gpr_2d = GaussianProcessRegressor(kernel=kernel</span><span class="s3">, </span><span class="s1">normalize_y=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">gpr_2d.fit(X</span><span class="s3">, </span><span class="s1">np.vstack((y</span><span class="s3">, </span><span class="s1">y)).T)</span>

        <span class="s1">assert_almost_equal(gpr.kernel_.theta</span><span class="s3">, </span><span class="s1">gpr_2d.kernel_.theta</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">non_fixed_kernels)</span>
<span class="s3">def </span><span class="s1">test_custom_optimizer(kernel):</span>
    <span class="s2"># Test that GPR can use externally defined optimizers.</span>
    <span class="s2"># Define a dummy optimizer that simply tests 50 random hyperparameters</span>
    <span class="s3">def </span><span class="s1">optimizer(obj_func</span><span class="s3">, </span><span class="s1">initial_theta</span><span class="s3">, </span><span class="s1">bounds):</span>
        <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">theta_opt</span><span class="s3">, </span><span class="s1">func_min = initial_theta</span><span class="s3">, </span><span class="s1">obj_func(</span>
            <span class="s1">initial_theta</span><span class="s3">, </span><span class="s1">eval_gradient=</span><span class="s3">False</span>
        <span class="s1">)</span>
        <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(</span><span class="s4">50</span><span class="s1">):</span>
            <span class="s1">theta = np.atleast_1d(</span>
                <span class="s1">rng.uniform(np.maximum(-</span><span class="s4">2</span><span class="s3">, </span><span class="s1">bounds[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])</span><span class="s3">, </span><span class="s1">np.minimum(</span><span class="s4">1</span><span class="s3">, </span><span class="s1">bounds[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]))</span>
            <span class="s1">)</span>
            <span class="s1">f = obj_func(theta</span><span class="s3">, </span><span class="s1">eval_gradient=</span><span class="s3">False</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">f &lt; func_min:</span>
                <span class="s1">theta_opt</span><span class="s3">, </span><span class="s1">func_min = theta</span><span class="s3">, </span><span class="s1">f</span>
        <span class="s3">return </span><span class="s1">theta_opt</span><span class="s3">, </span><span class="s1">func_min</span>

    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel</span><span class="s3">, </span><span class="s1">optimizer=optimizer)</span>
    <span class="s1">gpr.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s2"># Checks that optimizer improved marginal likelihood</span>
    <span class="s3">assert </span><span class="s1">gpr.log_marginal_likelihood(gpr.kernel_.theta) &gt; gpr.log_marginal_likelihood(</span>
        <span class="s1">gpr.kernel.theta</span>
    <span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_gpr_correct_error_message():</span>
    <span class="s1">X = np.arange(</span><span class="s4">12</span><span class="s1">).reshape(</span><span class="s4">6</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">y = np.ones(</span><span class="s4">6</span><span class="s1">)</span>
    <span class="s1">kernel = DotProduct()</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s4">0.0</span><span class="s1">)</span>
    <span class="s1">message = (</span>
        <span class="s5">&quot;The kernel, %s, is not returning a &quot;</span>
        <span class="s5">&quot;positive definite matrix. Try gradually increasing &quot;</span>
        <span class="s5">&quot;the 'alpha' parameter of your &quot;</span>
        <span class="s5">&quot;GaussianProcessRegressor estimator.&quot; </span><span class="s1">% kernel</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(np.linalg.LinAlgError</span><span class="s3">, </span><span class="s1">match=re.escape(message)):</span>
        <span class="s1">gpr.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_duplicate_input(kernel):</span>
    <span class="s2"># Test GPR can handle two different output-values for the same input.</span>
    <span class="s1">gpr_equal_inputs = GaussianProcessRegressor(kernel=kernel</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s4">1e-2</span><span class="s1">)</span>
    <span class="s1">gpr_similar_inputs = GaussianProcessRegressor(kernel=kernel</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s4">1e-2</span><span class="s1">)</span>

    <span class="s1">X_ = np.vstack((X</span><span class="s3">, </span><span class="s1">X[</span><span class="s4">0</span><span class="s1">]))</span>
    <span class="s1">y_ = np.hstack((y</span><span class="s3">, </span><span class="s1">y[</span><span class="s4">0</span><span class="s1">] + </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">gpr_equal_inputs.fit(X_</span><span class="s3">, </span><span class="s1">y_)</span>

    <span class="s1">X_ = np.vstack((X</span><span class="s3">, </span><span class="s1">X[</span><span class="s4">0</span><span class="s1">] + </span><span class="s4">1e-15</span><span class="s1">))</span>
    <span class="s1">y_ = np.hstack((y</span><span class="s3">, </span><span class="s1">y[</span><span class="s4">0</span><span class="s1">] + </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">gpr_similar_inputs.fit(X_</span><span class="s3">, </span><span class="s1">y_)</span>

    <span class="s1">X_test = np.linspace(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">10</span><span class="s3">, </span><span class="s4">100</span><span class="s1">)[:</span><span class="s3">, None</span><span class="s1">]</span>
    <span class="s1">y_pred_equal</span><span class="s3">, </span><span class="s1">y_std_equal = gpr_equal_inputs.predict(X_test</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">y_pred_similar</span><span class="s3">, </span><span class="s1">y_std_similar = gpr_similar_inputs.predict(X_test</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">assert_almost_equal(y_pred_equal</span><span class="s3">, </span><span class="s1">y_pred_similar)</span>
    <span class="s1">assert_almost_equal(y_std_equal</span><span class="s3">, </span><span class="s1">y_std_similar)</span>


<span class="s3">def </span><span class="s1">test_no_fit_default_predict():</span>
    <span class="s2"># Test that GPR predictions without fit does not break by default.</span>
    <span class="s1">default_kernel = C(</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">constant_value_bounds=</span><span class="s5">&quot;fixed&quot;</span><span class="s1">) * RBF(</span>
        <span class="s4">1.0</span><span class="s3">, </span><span class="s1">length_scale_bounds=</span><span class="s5">&quot;fixed&quot;</span>
    <span class="s1">)</span>
    <span class="s1">gpr1 = GaussianProcessRegressor()</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">y_std1 = gpr1.predict(X</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">y_cov1 = gpr1.predict(X</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">gpr2 = GaussianProcessRegressor(kernel=default_kernel)</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">y_std2 = gpr2.predict(X</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">y_cov2 = gpr2.predict(X</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(y_std1</span><span class="s3">, </span><span class="s1">y_std2)</span>
    <span class="s1">assert_array_almost_equal(y_cov1</span><span class="s3">, </span><span class="s1">y_cov2)</span>


<span class="s3">def </span><span class="s1">test_warning_bounds():</span>
    <span class="s1">kernel = RBF(length_scale_bounds=[</span><span class="s4">1e-5</span><span class="s3">, </span><span class="s4">1e-3</span><span class="s1">])</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel)</span>
    <span class="s1">warning_message = (</span>
        <span class="s5">&quot;The optimal value found for dimension 0 of parameter &quot;</span>
        <span class="s5">&quot;length_scale is close to the specified upper bound &quot;</span>
        <span class="s5">&quot;0.001. Increasing the bound and calling fit again may &quot;</span>
        <span class="s5">&quot;find a better value.&quot;</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s3">, </span><span class="s1">match=warning_message):</span>
        <span class="s1">gpr.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">kernel_sum = WhiteKernel(noise_level_bounds=[</span><span class="s4">1e-5</span><span class="s3">, </span><span class="s4">1e-3</span><span class="s1">]) + RBF(</span>
        <span class="s1">length_scale_bounds=[</span><span class="s4">1e3</span><span class="s3">, </span><span class="s4">1e5</span><span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">gpr_sum = GaussianProcessRegressor(kernel=kernel_sum)</span>
    <span class="s3">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s3">True</span><span class="s1">) </span><span class="s3">as </span><span class="s1">record:</span>
        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;always&quot;</span><span class="s1">)</span>
        <span class="s1">gpr_sum.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s3">assert </span><span class="s1">len(record) == </span><span class="s4">2</span>

        <span class="s3">assert </span><span class="s1">issubclass(record[</span><span class="s4">0</span><span class="s1">].category</span><span class="s3">, </span><span class="s1">ConvergenceWarning)</span>
        <span class="s3">assert </span><span class="s1">(</span>
            <span class="s1">record[</span><span class="s4">0</span><span class="s1">].message.args[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">== </span><span class="s5">&quot;The optimal value found for &quot;</span>
            <span class="s5">&quot;dimension 0 of parameter &quot;</span>
            <span class="s5">&quot;k1__noise_level is close to the &quot;</span>
            <span class="s5">&quot;specified upper bound 0.001. &quot;</span>
            <span class="s5">&quot;Increasing the bound and calling &quot;</span>
            <span class="s5">&quot;fit again may find a better value.&quot;</span>
        <span class="s1">)</span>

        <span class="s3">assert </span><span class="s1">issubclass(record[</span><span class="s4">1</span><span class="s1">].category</span><span class="s3">, </span><span class="s1">ConvergenceWarning)</span>
        <span class="s3">assert </span><span class="s1">(</span>
            <span class="s1">record[</span><span class="s4">1</span><span class="s1">].message.args[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">== </span><span class="s5">&quot;The optimal value found for &quot;</span>
            <span class="s5">&quot;dimension 0 of parameter &quot;</span>
            <span class="s5">&quot;k2__length_scale is close to the &quot;</span>
            <span class="s5">&quot;specified lower bound 1000.0. &quot;</span>
            <span class="s5">&quot;Decreasing the bound and calling &quot;</span>
            <span class="s5">&quot;fit again may find a better value.&quot;</span>
        <span class="s1">)</span>

    <span class="s1">X_tile = np.tile(X</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">kernel_dims = RBF(length_scale=[</span><span class="s4">1.0</span><span class="s3">, </span><span class="s4">2.0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">length_scale_bounds=[</span><span class="s4">1e1</span><span class="s3">, </span><span class="s4">1e2</span><span class="s1">])</span>
    <span class="s1">gpr_dims = GaussianProcessRegressor(kernel=kernel_dims)</span>

    <span class="s3">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s3">True</span><span class="s1">) </span><span class="s3">as </span><span class="s1">record:</span>
        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;always&quot;</span><span class="s1">)</span>
        <span class="s1">gpr_dims.fit(X_tile</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s3">assert </span><span class="s1">len(record) == </span><span class="s4">2</span>

        <span class="s3">assert </span><span class="s1">issubclass(record[</span><span class="s4">0</span><span class="s1">].category</span><span class="s3">, </span><span class="s1">ConvergenceWarning)</span>
        <span class="s3">assert </span><span class="s1">(</span>
            <span class="s1">record[</span><span class="s4">0</span><span class="s1">].message.args[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">== </span><span class="s5">&quot;The optimal value found for &quot;</span>
            <span class="s5">&quot;dimension 0 of parameter &quot;</span>
            <span class="s5">&quot;length_scale is close to the &quot;</span>
            <span class="s5">&quot;specified lower bound 10.0. &quot;</span>
            <span class="s5">&quot;Decreasing the bound and calling &quot;</span>
            <span class="s5">&quot;fit again may find a better value.&quot;</span>
        <span class="s1">)</span>

        <span class="s3">assert </span><span class="s1">issubclass(record[</span><span class="s4">1</span><span class="s1">].category</span><span class="s3">, </span><span class="s1">ConvergenceWarning)</span>
        <span class="s3">assert </span><span class="s1">(</span>
            <span class="s1">record[</span><span class="s4">1</span><span class="s1">].message.args[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">== </span><span class="s5">&quot;The optimal value found for &quot;</span>
            <span class="s5">&quot;dimension 1 of parameter &quot;</span>
            <span class="s5">&quot;length_scale is close to the &quot;</span>
            <span class="s5">&quot;specified lower bound 10.0. &quot;</span>
            <span class="s5">&quot;Decreasing the bound and calling &quot;</span>
            <span class="s5">&quot;fit again may find a better value.&quot;</span>
        <span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_bound_check_fixed_hyperparameter():</span>
    <span class="s2"># Regression test for issue #17943</span>
    <span class="s2"># Check that having a hyperparameter with fixed bounds doesn't cause an</span>
    <span class="s2"># error</span>
    <span class="s1">k1 = </span><span class="s4">50.0</span><span class="s1">**</span><span class="s4">2 </span><span class="s1">* RBF(length_scale=</span><span class="s4">50.0</span><span class="s1">)  </span><span class="s2"># long term smooth rising trend</span>
    <span class="s1">k2 = ExpSineSquared(</span>
        <span class="s1">length_scale=</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">periodicity=</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">periodicity_bounds=</span><span class="s5">&quot;fixed&quot;</span>
    <span class="s1">)  </span><span class="s2"># seasonal component</span>
    <span class="s1">kernel = k1 + k2</span>
    <span class="s1">GaussianProcessRegressor(kernel=kernel).fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;kernel&quot;</span><span class="s3">, </span><span class="s1">kernels)</span>
<span class="s3">def </span><span class="s1">test_constant_target(kernel):</span>
    <span class="s0">&quot;&quot;&quot;Check that the std. dev. is affected to 1 when normalizing a constant 
    feature. 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/18318 
    NaN where affected to the target when scaling due to null std. dev. with 
    constant target. 
    &quot;&quot;&quot;</span>
    <span class="s1">y_constant = np.ones(X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.float64)</span>

    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel</span><span class="s3">, </span><span class="s1">normalize_y=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">gpr.fit(X</span><span class="s3">, </span><span class="s1">y_constant)</span>
    <span class="s3">assert </span><span class="s1">gpr._y_train_std == pytest.approx(</span><span class="s4">1.0</span><span class="s1">)</span>

    <span class="s1">y_pred</span><span class="s3">, </span><span class="s1">y_cov = gpr.predict(X</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">assert_allclose(y_pred</span><span class="s3">, </span><span class="s1">y_constant)</span>
    <span class="s2"># set atol because we compare to zero</span>
    <span class="s1">assert_allclose(np.diag(y_cov)</span><span class="s3">, </span><span class="s4">0.0</span><span class="s3">, </span><span class="s1">atol=</span><span class="s4">1e-9</span><span class="s1">)</span>

    <span class="s2"># Test multi-target data</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_targets = X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s4">2</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">y = np.concatenate(</span>
        <span class="s1">[</span>
            <span class="s1">rng.normal(size=(n_samples</span><span class="s3">, </span><span class="s4">1</span><span class="s1">))</span><span class="s3">,  </span><span class="s2"># non-constant target</span>
            <span class="s1">np.full(shape=(n_samples</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">fill_value=</span><span class="s4">2</span><span class="s1">)</span><span class="s3">,  </span><span class="s2"># constant target</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s1">axis=</span><span class="s4">1</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s1">gpr.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">Y_pred</span><span class="s3">, </span><span class="s1">Y_cov = gpr.predict(X</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">assert_allclose(Y_pred[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">assert_allclose(np.diag(Y_cov[...</span><span class="s3">, </span><span class="s4">1</span><span class="s1">])</span><span class="s3">, </span><span class="s4">0.0</span><span class="s3">, </span><span class="s1">atol=</span><span class="s4">1e-9</span><span class="s1">)</span>

    <span class="s3">assert </span><span class="s1">Y_pred.shape == (n_samples</span><span class="s3">, </span><span class="s1">n_targets)</span>
    <span class="s3">assert </span><span class="s1">Y_cov.shape == (n_samples</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_targets)</span>


<span class="s3">def </span><span class="s1">test_gpr_consistency_std_cov_non_invertible_kernel():</span>
    <span class="s0">&quot;&quot;&quot;Check the consistency between the returned std. dev. and the covariance. 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/19936 
    Inconsistencies were observed when the kernel cannot be inverted (or 
    numerically stable). 
    &quot;&quot;&quot;</span>
    <span class="s1">kernel = C(</span><span class="s4">8.98576054e05</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1e-12</span><span class="s3">, </span><span class="s4">1e12</span><span class="s1">)) * RBF(</span>
        <span class="s1">[</span><span class="s4">5.91326520e02</span><span class="s3">, </span><span class="s4">1.32584051e03</span><span class="s1">]</span><span class="s3">, </span><span class="s1">(</span><span class="s4">1e-12</span><span class="s3">, </span><span class="s4">1e12</span><span class="s1">)</span>
    <span class="s1">) + WhiteKernel(noise_level=</span><span class="s4">1e-5</span><span class="s1">)</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=kernel</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">optimizer=</span><span class="s3">None</span><span class="s1">)</span>
    <span class="s1">X_train = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">0.0</span><span class="s3">, </span><span class="s4">0.0</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">1.54919334</span><span class="s3">, </span><span class="s1">-</span><span class="s4">0.77459667</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[-</span><span class="s4">1.54919334</span><span class="s3">, </span><span class="s4">0.0</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">0.0</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1.54919334</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">0.77459667</span><span class="s3">, </span><span class="s4">0.77459667</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[-</span><span class="s4">0.77459667</span><span class="s3">, </span><span class="s4">1.54919334</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">y_train = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[-</span><span class="s4">2.14882017e-10</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[-</span><span class="s4">4.66975823e00</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">4.01823986e00</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[-</span><span class="s4">1.30303674e00</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[-</span><span class="s4">1.35760156e00</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">3.31215668e00</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">gpr.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s1">X_test = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[-</span><span class="s4">1.93649167</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1.93649167</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">1.93649167</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1.93649167</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[-</span><span class="s4">1.93649167</span><span class="s3">, </span><span class="s4">1.93649167</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">1.93649167</span><span class="s3">, </span><span class="s4">1.93649167</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">pred1</span><span class="s3">, </span><span class="s1">std = gpr.predict(X_test</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">pred2</span><span class="s3">, </span><span class="s1">cov = gpr.predict(X_test</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">assert_allclose(std</span><span class="s3">, </span><span class="s1">np.sqrt(np.diagonal(cov))</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;params, TypeError, err_msg&quot;</span><span class="s3">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: np.zeros(</span><span class="s4">100</span><span class="s1">)}</span><span class="s3">,</span>
            <span class="s1">ValueError</span><span class="s3">,</span>
            <span class="s5">&quot;alpha must be a scalar or an array with same number of entries as y&quot;</span><span class="s3">,</span>
        <span class="s1">)</span><span class="s3">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span>
                <span class="s5">&quot;kernel&quot;</span><span class="s1">: WhiteKernel(noise_level_bounds=(-np.inf</span><span class="s3">, </span><span class="s1">np.inf))</span><span class="s3">,</span>
                <span class="s5">&quot;n_restarts_optimizer&quot;</span><span class="s1">: </span><span class="s4">2</span><span class="s3">,</span>
            <span class="s1">}</span><span class="s3">,</span>
            <span class="s1">ValueError</span><span class="s3">,</span>
            <span class="s5">&quot;requires that all bounds are finite&quot;</span><span class="s3">,</span>
        <span class="s1">)</span><span class="s3">,</span>
    <span class="s1">]</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">test_gpr_fit_error(params</span><span class="s3">, </span><span class="s1">TypeError</span><span class="s3">, </span><span class="s1">err_msg):</span>
    <span class="s0">&quot;&quot;&quot;Check that expected error are raised during fit.&quot;&quot;&quot;</span>
    <span class="s1">gpr = GaussianProcessRegressor(**params)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(TypeError</span><span class="s3">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">gpr.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">test_gpr_lml_error():</span>
    <span class="s0">&quot;&quot;&quot;Check that we raise the proper error in the LML method.&quot;&quot;&quot;</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=RBF()).fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">err_msg = </span><span class="s5">&quot;Gradient can only be evaluated for theta!=None&quot;</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError</span><span class="s3">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">gpr.log_marginal_likelihood(eval_gradient=</span><span class="s3">True</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_gpr_predict_error():</span>
    <span class="s0">&quot;&quot;&quot;Check that we raise the proper error during predict.&quot;&quot;&quot;</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=RBF()).fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">err_msg = </span><span class="s5">&quot;At most one of return_std or return_cov can be requested.&quot;</span>
    <span class="s3">with </span><span class="s1">pytest.raises(RuntimeError</span><span class="s3">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">gpr.predict(X</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;normalize_y&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s3">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;n_targets&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s3">None, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">10</span><span class="s1">])</span>
<span class="s3">def </span><span class="s1">test_predict_shapes(normalize_y</span><span class="s3">, </span><span class="s1">n_targets):</span>
    <span class="s0">&quot;&quot;&quot;Check the shapes of y_mean, y_std, and y_cov in single-output 
    (n_targets=None) and multi-output settings, including the edge case when 
    n_targets=1, where the sklearn convention is to squeeze the predictions. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/17394 
    https://github.com/scikit-learn/scikit-learn/issues/18065 
    https://github.com/scikit-learn/scikit-learn/issues/22174 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">1234</span><span class="s1">)</span>

    <span class="s1">n_features</span><span class="s3">, </span><span class="s1">n_samples_train</span><span class="s3">, </span><span class="s1">n_samples_test = </span><span class="s4">6</span><span class="s3">, </span><span class="s4">9</span><span class="s3">, </span><span class="s4">7</span>

    <span class="s1">y_train_shape = (n_samples_train</span><span class="s3">,</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">n_targets </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">y_train_shape = y_train_shape + (n_targets</span><span class="s3">,</span><span class="s1">)</span>

    <span class="s2"># By convention single-output data is squeezed upon prediction</span>
    <span class="s1">y_test_shape = (n_samples_test</span><span class="s3">,</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">n_targets </span><span class="s3">is not None and </span><span class="s1">n_targets &gt; </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">y_test_shape = y_test_shape + (n_targets</span><span class="s3">,</span><span class="s1">)</span>

    <span class="s1">X_train = rng.randn(n_samples_train</span><span class="s3">, </span><span class="s1">n_features)</span>
    <span class="s1">X_test = rng.randn(n_samples_test</span><span class="s3">, </span><span class="s1">n_features)</span>
    <span class="s1">y_train = rng.randn(*y_train_shape)</span>

    <span class="s1">model = GaussianProcessRegressor(normalize_y=normalize_y)</span>
    <span class="s1">model.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s1">y_pred</span><span class="s3">, </span><span class="s1">y_std = model.predict(X_test</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">y_cov = model.predict(X_test</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s3">assert </span><span class="s1">y_pred.shape == y_test_shape</span>
    <span class="s3">assert </span><span class="s1">y_std.shape == y_test_shape</span>
    <span class="s3">assert </span><span class="s1">y_cov.shape == (n_samples_test</span><span class="s3">,</span><span class="s1">) + y_test_shape</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;normalize_y&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s3">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;n_targets&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s3">None, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">10</span><span class="s1">])</span>
<span class="s3">def </span><span class="s1">test_sample_y_shapes(normalize_y</span><span class="s3">, </span><span class="s1">n_targets):</span>
    <span class="s0">&quot;&quot;&quot;Check the shapes of y_samples in single-output (n_targets=0) and 
    multi-output settings, including the edge case when n_targets=1, where the 
    sklearn convention is to squeeze the predictions. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/22175 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">1234</span><span class="s1">)</span>

    <span class="s1">n_features</span><span class="s3">, </span><span class="s1">n_samples_train = </span><span class="s4">6</span><span class="s3">, </span><span class="s4">9</span>
    <span class="s2"># Number of spatial locations to predict at</span>
    <span class="s1">n_samples_X_test = </span><span class="s4">7</span>
    <span class="s2"># Number of sample predictions per test point</span>
    <span class="s1">n_samples_y_test = </span><span class="s4">5</span>

    <span class="s1">y_train_shape = (n_samples_train</span><span class="s3">,</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">n_targets </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">y_train_shape = y_train_shape + (n_targets</span><span class="s3">,</span><span class="s1">)</span>

    <span class="s2"># By convention single-output data is squeezed upon prediction</span>
    <span class="s3">if </span><span class="s1">n_targets </span><span class="s3">is not None and </span><span class="s1">n_targets &gt; </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">y_test_shape = (n_samples_X_test</span><span class="s3">, </span><span class="s1">n_targets</span><span class="s3">, </span><span class="s1">n_samples_y_test)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">y_test_shape = (n_samples_X_test</span><span class="s3">, </span><span class="s1">n_samples_y_test)</span>

    <span class="s1">X_train = rng.randn(n_samples_train</span><span class="s3">, </span><span class="s1">n_features)</span>
    <span class="s1">X_test = rng.randn(n_samples_X_test</span><span class="s3">, </span><span class="s1">n_features)</span>
    <span class="s1">y_train = rng.randn(*y_train_shape)</span>

    <span class="s1">model = GaussianProcessRegressor(normalize_y=normalize_y)</span>

    <span class="s2"># FIXME: before fitting, the estimator does not have information regarding</span>
    <span class="s2"># the number of targets and default to 1. This is inconsistent with the shape</span>
    <span class="s2"># provided after `fit`. This assert should be made once the following issue</span>
    <span class="s2"># is fixed:</span>
    <span class="s2"># https://github.com/scikit-learn/scikit-learn/issues/22430</span>
    <span class="s2"># y_samples = model.sample_y(X_test, n_samples=n_samples_y_test)</span>
    <span class="s2"># assert y_samples.shape == y_test_shape</span>

    <span class="s1">model.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s1">y_samples = model.sample_y(X_test</span><span class="s3">, </span><span class="s1">n_samples=n_samples_y_test)</span>
    <span class="s3">assert </span><span class="s1">y_samples.shape == y_test_shape</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;n_targets&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s3">None, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">3</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;n_samples&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">5</span><span class="s1">])</span>
<span class="s3">def </span><span class="s1">test_sample_y_shape_with_prior(n_targets</span><span class="s3">, </span><span class="s1">n_samples):</span>
    <span class="s0">&quot;&quot;&quot;Check the output shape of `sample_y` is consistent before and after `fit`.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">1024</span><span class="s1">)</span>

    <span class="s1">X = rng.randn(</span><span class="s4">10</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">y = rng.randn(</span><span class="s4">10</span><span class="s3">, </span><span class="s1">n_targets </span><span class="s3">if </span><span class="s1">n_targets </span><span class="s3">is not None else </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s1">model = GaussianProcessRegressor(n_targets=n_targets)</span>
    <span class="s1">shape_before_fit = model.sample_y(X</span><span class="s3">, </span><span class="s1">n_samples=n_samples).shape</span>
    <span class="s1">model.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">shape_after_fit = model.sample_y(X</span><span class="s3">, </span><span class="s1">n_samples=n_samples).shape</span>
    <span class="s3">assert </span><span class="s1">shape_before_fit == shape_after_fit</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;n_targets&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s3">None, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">3</span><span class="s1">])</span>
<span class="s3">def </span><span class="s1">test_predict_shape_with_prior(n_targets):</span>
    <span class="s0">&quot;&quot;&quot;Check the output shape of `predict` with prior distribution.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">1024</span><span class="s1">)</span>

    <span class="s1">n_sample = </span><span class="s4">10</span>
    <span class="s1">X = rng.randn(n_sample</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">y = rng.randn(n_sample</span><span class="s3">, </span><span class="s1">n_targets </span><span class="s3">if </span><span class="s1">n_targets </span><span class="s3">is not None else </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s1">model = GaussianProcessRegressor(n_targets=n_targets)</span>
    <span class="s1">mean_prior</span><span class="s3">, </span><span class="s1">cov_prior = model.predict(X</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">std_prior = model.predict(X</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">model.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">mean_post</span><span class="s3">, </span><span class="s1">cov_post = model.predict(X</span><span class="s3">, </span><span class="s1">return_cov=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">std_post = model.predict(X</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s3">assert </span><span class="s1">mean_prior.shape == mean_post.shape</span>
    <span class="s3">assert </span><span class="s1">cov_prior.shape == cov_post.shape</span>
    <span class="s3">assert </span><span class="s1">std_prior.shape == std_post.shape</span>


<span class="s3">def </span><span class="s1">test_n_targets_error():</span>
    <span class="s0">&quot;&quot;&quot;Check that an error is raised when the number of targets seen at fit is 
    inconsistent with n_targets. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(</span><span class="s4">10</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">y = rng.randn(</span><span class="s4">10</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span>

    <span class="s1">model = GaussianProcessRegressor(n_targets=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError</span><span class="s3">, </span><span class="s1">match=</span><span class="s5">&quot;The number of targets seen in `y`&quot;</span><span class="s1">):</span>
        <span class="s1">model.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">class </span><span class="s1">CustomKernel(C):</span>
    <span class="s0">&quot;&quot;&quot; 
    A custom kernel that has a diag method that returns the first column of the 
    input matrix X. This is a helper for the test to check that the input 
    matrix X is not mutated. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">diag(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s3">return </span><span class="s1">X[:</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span>


<span class="s3">def </span><span class="s1">test_gpr_predict_input_not_modified():</span>
    <span class="s0">&quot;&quot;&quot; 
    Check that the input X is not modified by the predict method of the 
    GaussianProcessRegressor when setting return_std=True. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/24340 
    &quot;&quot;&quot;</span>
    <span class="s1">gpr = GaussianProcessRegressor(kernel=CustomKernel()).fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">X2_copy = np.copy(X2)</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">_ = gpr.predict(X2</span><span class="s3">, </span><span class="s1">return_std=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">assert_allclose(X2</span><span class="s3">, </span><span class="s1">X2_copy)</span>
</pre>
</body>
</html>