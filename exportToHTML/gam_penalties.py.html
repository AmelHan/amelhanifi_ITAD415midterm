<html>
<head>
<title>gam_penalties.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
gam_penalties.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot; 
Penalty classes for Generalized Additive Models 
 
Author: Luca Puggini 
Author: Josef Perktold 
 
&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy.linalg </span><span class="s3">import </span><span class="s1">block_diag</span>
<span class="s3">from </span><span class="s1">statsmodels.base._penalties </span><span class="s3">import </span><span class="s1">Penalty</span>


<span class="s3">class </span><span class="s1">UnivariateGamPenalty(Penalty):</span>
    <span class="s2">&quot;&quot;&quot; 
    Penalty for smooth term in Generalized Additive Models 
 
    Parameters 
    ---------- 
    univariate_smoother : instance 
        instance of univariate smoother or spline class 
    alpha : float 
        default penalty weight, alpha can be provided to each method 
    weights: 
        TODO: not used and verified, might be removed 
 
    Attributes 
    ---------- 
    Parameters are stored, additionally 
    nob s: The number of samples used during the estimation 
    n_columns : number of columns in smoother basis 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">univariate_smoother</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">weights=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s1">self.weights = weights</span>
        <span class="s1">self.alpha = alpha</span>
        <span class="s1">self.univariate_smoother = univariate_smoother</span>
        <span class="s1">self.nobs = self.univariate_smoother.nobs</span>
        <span class="s1">self.n_columns = self.univariate_smoother.dim_basis</span>

    <span class="s3">def </span><span class="s1">func(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;evaluate penalization at params 
 
        Parameters 
        ---------- 
        params : ndarray 
            coefficients for the spline basis in the regression model 
        alpha : float 
            default penalty weight 
 
        Returns 
        ------- 
        func : float 
            value of the penalty evaluated at params 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">alpha </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">alpha = self.alpha</span>

        <span class="s1">f = params.dot(self.univariate_smoother.cov_der2.dot(params))</span>
        <span class="s3">return </span><span class="s1">alpha * f / self.nobs</span>

    <span class="s3">def </span><span class="s1">deriv(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;evaluate derivative of penalty with respect to params 
 
        Parameters 
        ---------- 
        params : ndarray 
            coefficients for the spline basis in the regression model 
        alpha : float 
            default penalty weight 
 
        Returns 
        ------- 
        deriv : ndarray 
            derivative, gradient of the penalty with respect to params 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">alpha </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">alpha = self.alpha</span>

        <span class="s1">d = </span><span class="s4">2 </span><span class="s1">* alpha * np.dot(self.univariate_smoother.cov_der2</span><span class="s3">, </span><span class="s1">params)</span>
        <span class="s1">d /= self.nobs</span>
        <span class="s3">return </span><span class="s1">d</span>

    <span class="s3">def </span><span class="s1">deriv2(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;evaluate second derivative of penalty with respect to params 
 
        Parameters 
        ---------- 
        params : ndarray 
            coefficients for the spline basis in the regression model 
        alpha : float 
            default penalty weight 
 
        Returns 
        ------- 
        deriv2 : ndarray, 2-Dim 
            second derivative, hessian of the penalty with respect to params 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">alpha </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">alpha = self.alpha</span>

        <span class="s1">d2 = </span><span class="s4">2 </span><span class="s1">* alpha * self.univariate_smoother.cov_der2</span>
        <span class="s1">d2 /= self.nobs</span>
        <span class="s3">return </span><span class="s1">d2</span>

    <span class="s3">def </span><span class="s1">penalty_matrix(self</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;penalty matrix for the smooth term of a GAM 
 
        Parameters 
        ---------- 
        alpha : list of floats or None 
            penalty weights 
 
        Returns 
        ------- 
        penalty matrix 
            square penalty matrix for quadratic penalization. The number 
            of rows and columns are equal to the number of columns in the 
            smooth terms, i.e. the number of parameters for this smooth 
            term in the regression model 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">alpha </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">alpha = self.alpha</span>

        <span class="s3">return </span><span class="s1">alpha * self.univariate_smoother.cov_der2</span>


<span class="s3">class </span><span class="s1">MultivariateGamPenalty(Penalty):</span>
    <span class="s2">&quot;&quot;&quot; 
    Penalty for Generalized Additive Models 
 
    Parameters 
    ---------- 
    multivariate_smoother : instance 
        instance of additive smoother or spline class 
    alpha : list of float 
        default penalty weight, list with length equal to the number of smooth 
        terms. ``alpha`` can also be provided to each method. 
    weights : array_like 
        currently not used 
        is a list of doubles of the same length as alpha or a list 
        of ndarrays where each component has the length equal to the number 
        of columns in that component 
    start_idx : int 
        number of parameters that come before the smooth terms. If the model 
        has a linear component, then the parameters for the smooth components 
        start at ``start_index``. 
 
    Attributes 
    ---------- 
    Parameters are stored, additionally 
    nob s: The number of samples used during the estimation 
 
    dim_basis : number of columns of additive smoother. Number of columns 
        in all smoothers. 
    k_variables : number of smooth terms 
    k_params : total number of parameters in the regression model 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">multivariate_smoother</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">weights=</span><span class="s3">None,</span>
                 <span class="s1">start_idx=</span><span class="s4">0</span><span class="s1">):</span>

        <span class="s3">if </span><span class="s1">len(multivariate_smoother.smoothers) != len(alpha):</span>
            <span class="s1">msg = (</span><span class="s5">'all the input values should be of the same length.'</span>
                   <span class="s5">' len(smoothers)=%d, len(alphas)=%d'</span><span class="s1">) % (</span>
                   <span class="s1">len(multivariate_smoother.smoothers)</span><span class="s3">, </span><span class="s1">len(alpha))</span>
            <span class="s3">raise </span><span class="s1">ValueError(msg)</span>

        <span class="s1">self.multivariate_smoother = multivariate_smoother</span>
        <span class="s1">self.dim_basis = self.multivariate_smoother.dim_basis</span>
        <span class="s1">self.k_variables = self.multivariate_smoother.k_variables</span>
        <span class="s1">self.nobs = self.multivariate_smoother.nobs</span>
        <span class="s1">self.alpha = alpha</span>
        <span class="s1">self.start_idx = start_idx</span>
        <span class="s1">self.k_params = start_idx + self.dim_basis</span>

        <span class="s0"># TODO: Review this,</span>
        <span class="s3">if </span><span class="s1">weights </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s0"># weights should have total length as params</span>
            <span class="s0"># but it can also be scalar in individual component</span>
            <span class="s1">self.weights = [</span><span class="s4">1. </span><span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(self.k_variables)]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">import </span><span class="s1">warnings</span>
            <span class="s1">warnings.warn(</span><span class="s5">'weights is currently ignored'</span><span class="s1">)</span>
            <span class="s1">self.weights = weights</span>

        <span class="s1">self.mask = [np.zeros(self.k_params</span><span class="s3">, </span><span class="s1">dtype=bool)</span>
                     <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range(self.k_variables)]</span>
        <span class="s1">param_count = start_idx</span>
        <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">smoother </span><span class="s3">in </span><span class="s1">enumerate(self.multivariate_smoother.smoothers):</span>
            <span class="s0"># the mask[i] contains a vector of length k_columns. The index</span>
            <span class="s0"># corresponding to the i-th input variable are set to True.</span>
            <span class="s1">self.mask[i][param_count: param_count + smoother.dim_basis] = </span><span class="s3">True</span>
            <span class="s1">param_count += smoother.dim_basis</span>

        <span class="s1">self.gp = []</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.k_variables):</span>
            <span class="s1">gp = UnivariateGamPenalty(self.multivariate_smoother.smoothers[i]</span><span class="s3">,</span>
                                      <span class="s1">weights=self.weights[i]</span><span class="s3">,</span>
                                      <span class="s1">alpha=self.alpha[i])</span>
            <span class="s1">self.gp.append(gp)</span>

    <span class="s3">def </span><span class="s1">func(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;evaluate penalization at params 
 
        Parameters 
        ---------- 
        params : ndarray 
            coefficients in the regression model 
        alpha : float or list of floats 
            penalty weights 
 
        Returns 
        ------- 
        func : float 
            value of the penalty evaluated at params 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">alpha </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">alpha = [</span><span class="s3">None</span><span class="s1">] * self.k_variables</span>

        <span class="s1">cost = </span><span class="s4">0</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.k_variables):</span>
            <span class="s1">params_i = params[self.mask[i]]</span>
            <span class="s1">cost += self.gp[i].func(params_i</span><span class="s3">, </span><span class="s1">alpha=alpha[i])</span>

        <span class="s3">return </span><span class="s1">cost</span>

    <span class="s3">def </span><span class="s1">deriv(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;evaluate derivative of penalty with respect to params 
 
        Parameters 
        ---------- 
        params : ndarray 
            coefficients in the regression model 
        alpha : list of floats or None 
            penalty weights 
 
        Returns 
        ------- 
        deriv : ndarray 
            derivative, gradient of the penalty with respect to params 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">alpha </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">alpha = [</span><span class="s3">None</span><span class="s1">] * self.k_variables</span>

        <span class="s1">grad = [np.zeros(self.start_idx)]</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.k_variables):</span>
            <span class="s1">params_i = params[self.mask[i]]</span>
            <span class="s1">grad.append(self.gp[i].deriv(params_i</span><span class="s3">, </span><span class="s1">alpha=alpha[i]))</span>

        <span class="s3">return </span><span class="s1">np.concatenate(grad)</span>

    <span class="s3">def </span><span class="s1">deriv2(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;evaluate second derivative of penalty with respect to params 
 
        Parameters 
        ---------- 
        params : ndarray 
            coefficients in the regression model 
        alpha : list of floats or None 
            penalty weights 
 
        Returns 
        ------- 
        deriv2 : ndarray, 2-Dim 
            second derivative, hessian of the penalty with respect to params 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">alpha </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">alpha = [</span><span class="s3">None</span><span class="s1">] * self.k_variables</span>

        <span class="s1">deriv2 = [np.zeros((self.start_idx</span><span class="s3">, </span><span class="s1">self.start_idx))]</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.k_variables):</span>
            <span class="s1">params_i = params[self.mask[i]]</span>
            <span class="s1">deriv2.append(self.gp[i].deriv2(params_i</span><span class="s3">, </span><span class="s1">alpha=alpha[i]))</span>

        <span class="s3">return </span><span class="s1">block_diag(*deriv2)</span>

    <span class="s3">def </span><span class="s1">penalty_matrix(self</span><span class="s3">, </span><span class="s1">alpha=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;penalty matrix for generalized additive model 
 
        Parameters 
        ---------- 
        alpha : list of floats or None 
            penalty weights 
 
        Returns 
        ------- 
        penalty matrix 
            block diagonal, square penalty matrix for quadratic penalization. 
            The number of rows and columns are equal to the number of 
            parameters in the regression model ``k_params``. 
 
        Notes 
        ----- 
        statsmodels does not support backwards compatibility when keywords are 
        used as positional arguments. The order of keywords might change. 
        We might need to add a ``params`` keyword if the need arises. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">alpha </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">alpha = self.alpha</span>

        <span class="s1">s_all = [np.zeros((self.start_idx</span><span class="s3">, </span><span class="s1">self.start_idx))]</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(self.k_variables):</span>
            <span class="s1">s_all.append(self.gp[i].penalty_matrix(alpha=alpha[i]))</span>

        <span class="s3">return </span><span class="s1">block_diag(*s_all)</span>
</pre>
</body>
</html>