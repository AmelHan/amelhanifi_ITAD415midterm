<html>
<head>
<title>test_pls.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_pls.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">warnings</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">assert_allclose</span><span class="s0">, </span><span class="s1">assert_array_almost_equal</span><span class="s0">, </span><span class="s1">assert_array_equal</span>

<span class="s0">from </span><span class="s1">sklearn.cross_decomposition </span><span class="s0">import </span><span class="s1">CCA</span><span class="s0">, </span><span class="s1">PLSSVD</span><span class="s0">, </span><span class="s1">PLSCanonical</span><span class="s0">, </span><span class="s1">PLSRegression</span>
<span class="s0">from </span><span class="s1">sklearn.cross_decomposition._pls </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">_center_scale_xy</span><span class="s0">,</span>
    <span class="s1">_get_first_singular_vectors_power_method</span><span class="s0">,</span>
    <span class="s1">_get_first_singular_vectors_svd</span><span class="s0">,</span>
    <span class="s1">_svd_flip_1d</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">load_linnerud</span><span class="s0">, </span><span class="s1">make_regression</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble </span><span class="s0">import </span><span class="s1">VotingRegressor</span>
<span class="s0">from </span><span class="s1">sklearn.exceptions </span><span class="s0">import </span><span class="s1">ConvergenceWarning</span>
<span class="s0">from </span><span class="s1">sklearn.linear_model </span><span class="s0">import </span><span class="s1">LinearRegression</span>
<span class="s0">from </span><span class="s1">sklearn.utils </span><span class="s0">import </span><span class="s1">check_random_state</span>
<span class="s0">from </span><span class="s1">sklearn.utils.extmath </span><span class="s0">import </span><span class="s1">svd_flip</span>


<span class="s0">def </span><span class="s1">assert_matrix_orthogonal(M):</span>
    <span class="s1">K = np.dot(M.T</span><span class="s0">, </span><span class="s1">M)</span>
    <span class="s1">assert_array_almost_equal(K</span><span class="s0">, </span><span class="s1">np.diag(np.diag(K)))</span>


<span class="s0">def </span><span class="s1">test_pls_canonical_basics():</span>
    <span class="s2"># Basic checks for PLSCanonical</span>
    <span class="s1">d = load_linnerud()</span>
    <span class="s1">X = d.data</span>
    <span class="s1">Y = d.target</span>

    <span class="s1">pls = PLSCanonical(n_components=X.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">pls.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">assert_matrix_orthogonal(pls.x_weights_)</span>
    <span class="s1">assert_matrix_orthogonal(pls.y_weights_)</span>
    <span class="s1">assert_matrix_orthogonal(pls._x_scores)</span>
    <span class="s1">assert_matrix_orthogonal(pls._y_scores)</span>

    <span class="s2"># Check X = TP' and Y = UQ'</span>
    <span class="s1">T = pls._x_scores</span>
    <span class="s1">P = pls.x_loadings_</span>
    <span class="s1">U = pls._y_scores</span>
    <span class="s1">Q = pls.y_loadings_</span>
    <span class="s2"># Need to scale first</span>
    <span class="s1">Xc</span><span class="s0">, </span><span class="s1">Yc</span><span class="s0">, </span><span class="s1">x_mean</span><span class="s0">, </span><span class="s1">y_mean</span><span class="s0">, </span><span class="s1">x_std</span><span class="s0">, </span><span class="s1">y_std = _center_scale_xy(</span>
        <span class="s1">X.copy()</span><span class="s0">, </span><span class="s1">Y.copy()</span><span class="s0">, </span><span class="s1">scale=</span><span class="s0">True</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(Xc</span><span class="s0">, </span><span class="s1">np.dot(T</span><span class="s0">, </span><span class="s1">P.T))</span>
    <span class="s1">assert_array_almost_equal(Yc</span><span class="s0">, </span><span class="s1">np.dot(U</span><span class="s0">, </span><span class="s1">Q.T))</span>

    <span class="s2"># Check that rotations on training data lead to scores</span>
    <span class="s1">Xt = pls.transform(X)</span>
    <span class="s1">assert_array_almost_equal(Xt</span><span class="s0">, </span><span class="s1">pls._x_scores)</span>
    <span class="s1">Xt</span><span class="s0">, </span><span class="s1">Yt = pls.transform(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">assert_array_almost_equal(Xt</span><span class="s0">, </span><span class="s1">pls._x_scores)</span>
    <span class="s1">assert_array_almost_equal(Yt</span><span class="s0">, </span><span class="s1">pls._y_scores)</span>

    <span class="s2"># Check that inverse_transform works</span>
    <span class="s1">X_back = pls.inverse_transform(Xt)</span>
    <span class="s1">assert_array_almost_equal(X_back</span><span class="s0">, </span><span class="s1">X)</span>
    <span class="s1">_</span><span class="s0">, </span><span class="s1">Y_back = pls.inverse_transform(Xt</span><span class="s0">, </span><span class="s1">Yt)</span>
    <span class="s1">assert_array_almost_equal(Y_back</span><span class="s0">, </span><span class="s1">Y)</span>


<span class="s0">def </span><span class="s1">test_sanity_check_pls_regression():</span>
    <span class="s2"># Sanity check for PLSRegression</span>
    <span class="s2"># The results were checked against the R-packages plspm, misOmics and pls</span>

    <span class="s1">d = load_linnerud()</span>
    <span class="s1">X = d.data</span>
    <span class="s1">Y = d.target</span>

    <span class="s1">pls = PLSRegression(n_components=X.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">X_trans</span><span class="s0">, </span><span class="s1">_ = pls.fit_transform(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s2"># FIXME: one would expect y_trans == pls.y_scores_ but this is not</span>
    <span class="s2"># the case.</span>
    <span class="s2"># xref: https://github.com/scikit-learn/scikit-learn/issues/22420</span>
    <span class="s1">assert_allclose(X_trans</span><span class="s0">, </span><span class="s1">pls.x_scores_)</span>

    <span class="s1">expected_x_weights = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[-</span><span class="s3">0.61330704</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.00443647</span><span class="s0">, </span><span class="s3">0.78983213</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.74697144</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.32172099</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.58183269</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.25668686</span><span class="s0">, </span><span class="s3">0.94682413</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.19399983</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">expected_x_loadings = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[-</span><span class="s3">0.61470416</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.24574278</span><span class="s0">, </span><span class="s3">0.78983213</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.65625755</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.14396183</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.58183269</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.51733059</span><span class="s0">, </span><span class="s3">1.00609417</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.19399983</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">expected_y_weights = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[+</span><span class="s3">0.32456184</span><span class="s0">, </span><span class="s3">0.29892183</span><span class="s0">, </span><span class="s3">0.20316322</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[+</span><span class="s3">0.42439636</span><span class="s0">, </span><span class="s3">0.61970543</span><span class="s0">, </span><span class="s3">0.19320542</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.13143144</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.26348971</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.17092916</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">expected_y_loadings = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[+</span><span class="s3">0.32456184</span><span class="s0">, </span><span class="s3">0.29892183</span><span class="s0">, </span><span class="s3">0.20316322</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[+</span><span class="s3">0.42439636</span><span class="s0">, </span><span class="s3">0.61970543</span><span class="s0">, </span><span class="s3">0.19320542</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.13143144</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.26348971</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.17092916</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(np.abs(pls.x_loadings_)</span><span class="s0">, </span><span class="s1">np.abs(expected_x_loadings))</span>
    <span class="s1">assert_array_almost_equal(np.abs(pls.x_weights_)</span><span class="s0">, </span><span class="s1">np.abs(expected_x_weights))</span>
    <span class="s1">assert_array_almost_equal(np.abs(pls.y_loadings_)</span><span class="s0">, </span><span class="s1">np.abs(expected_y_loadings))</span>
    <span class="s1">assert_array_almost_equal(np.abs(pls.y_weights_)</span><span class="s0">, </span><span class="s1">np.abs(expected_y_weights))</span>

    <span class="s2"># The R / Python difference in the signs should be consistent across</span>
    <span class="s2"># loadings, weights, etc.</span>
    <span class="s1">x_loadings_sign_flip = np.sign(pls.x_loadings_ / expected_x_loadings)</span>
    <span class="s1">x_weights_sign_flip = np.sign(pls.x_weights_ / expected_x_weights)</span>
    <span class="s1">y_weights_sign_flip = np.sign(pls.y_weights_ / expected_y_weights)</span>
    <span class="s1">y_loadings_sign_flip = np.sign(pls.y_loadings_ / expected_y_loadings)</span>
    <span class="s1">assert_array_almost_equal(x_loadings_sign_flip</span><span class="s0">, </span><span class="s1">x_weights_sign_flip)</span>
    <span class="s1">assert_array_almost_equal(y_loadings_sign_flip</span><span class="s0">, </span><span class="s1">y_weights_sign_flip)</span>


<span class="s0">def </span><span class="s1">test_sanity_check_pls_regression_constant_column_Y():</span>
    <span class="s2"># Check behavior when the first column of Y is constant</span>
    <span class="s2"># The results are checked against a modified version of plsreg2</span>
    <span class="s2"># from the R-package plsdepot</span>
    <span class="s1">d = load_linnerud()</span>
    <span class="s1">X = d.data</span>
    <span class="s1">Y = d.target</span>
    <span class="s1">Y[:</span><span class="s0">, </span><span class="s3">0</span><span class="s1">] = </span><span class="s3">1</span>
    <span class="s1">pls = PLSRegression(n_components=X.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">pls.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">expected_x_weights = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[-</span><span class="s3">0.6273573</span><span class="s0">, </span><span class="s3">0.007081799</span><span class="s0">, </span><span class="s3">0.7786994</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.7493417</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.277612681</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.6011807</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.2119194</span><span class="s0">, </span><span class="s3">0.960666981</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1794690</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">expected_x_loadings = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[-</span><span class="s3">0.6273512</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.22464538</span><span class="s0">, </span><span class="s3">0.7786994</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.6643156</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.09871193</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.6011807</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.5125877</span><span class="s0">, </span><span class="s3">1.01407380</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1794690</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">expected_y_loadings = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s3">0.0000000</span><span class="s0">, </span><span class="s3">0.0000000</span><span class="s0">, </span><span class="s3">0.0000000</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.4357300</span><span class="s0">, </span><span class="s3">0.5828479</span><span class="s0">, </span><span class="s3">0.2174802</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.1353739</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.2486423</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1810386</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(np.abs(expected_x_weights)</span><span class="s0">, </span><span class="s1">np.abs(pls.x_weights_))</span>
    <span class="s1">assert_array_almost_equal(np.abs(expected_x_loadings)</span><span class="s0">, </span><span class="s1">np.abs(pls.x_loadings_))</span>
    <span class="s2"># For the PLSRegression with default parameters, y_loadings == y_weights</span>
    <span class="s1">assert_array_almost_equal(np.abs(pls.y_loadings_)</span><span class="s0">, </span><span class="s1">np.abs(expected_y_loadings))</span>
    <span class="s1">assert_array_almost_equal(np.abs(pls.y_weights_)</span><span class="s0">, </span><span class="s1">np.abs(expected_y_loadings))</span>

    <span class="s1">x_loadings_sign_flip = np.sign(expected_x_loadings / pls.x_loadings_)</span>
    <span class="s1">x_weights_sign_flip = np.sign(expected_x_weights / pls.x_weights_)</span>
    <span class="s2"># we ignore the first full-zeros row for y</span>
    <span class="s1">y_loadings_sign_flip = np.sign(expected_y_loadings[</span><span class="s3">1</span><span class="s1">:] / pls.y_loadings_[</span><span class="s3">1</span><span class="s1">:])</span>

    <span class="s1">assert_array_equal(x_loadings_sign_flip</span><span class="s0">, </span><span class="s1">x_weights_sign_flip)</span>
    <span class="s1">assert_array_equal(x_loadings_sign_flip[</span><span class="s3">1</span><span class="s1">:]</span><span class="s0">, </span><span class="s1">y_loadings_sign_flip)</span>


<span class="s0">def </span><span class="s1">test_sanity_check_pls_canonical():</span>
    <span class="s2"># Sanity check for PLSCanonical</span>
    <span class="s2"># The results were checked against the R-package plspm</span>

    <span class="s1">d = load_linnerud()</span>
    <span class="s1">X = d.data</span>
    <span class="s1">Y = d.target</span>

    <span class="s1">pls = PLSCanonical(n_components=X.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">pls.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">expected_x_weights = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[-</span><span class="s3">0.61330704</span><span class="s0">, </span><span class="s3">0.25616119</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.74715187</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.74697144</span><span class="s0">, </span><span class="s3">0.11930791</span><span class="s0">, </span><span class="s3">0.65406368</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.25668686</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.95924297</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.11817271</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">expected_x_rotations = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[-</span><span class="s3">0.61330704</span><span class="s0">, </span><span class="s3">0.41591889</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.62297525</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.74697144</span><span class="s0">, </span><span class="s3">0.31388326</span><span class="s0">, </span><span class="s3">0.77368233</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.25668686</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.89237972</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.24121788</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">expected_y_weights = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[+</span><span class="s3">0.58989127</span><span class="s0">, </span><span class="s3">0.7890047</span><span class="s0">, </span><span class="s3">0.1717553</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[+</span><span class="s3">0.77134053</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.61351791</span><span class="s0">, </span><span class="s3">0.16920272</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.23887670</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.03267062</span><span class="s0">, </span><span class="s3">0.97050016</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">expected_y_rotations = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[+</span><span class="s3">0.58989127</span><span class="s0">, </span><span class="s3">0.7168115</span><span class="s0">, </span><span class="s3">0.30665872</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[+</span><span class="s3">0.77134053</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.70791757</span><span class="s0">, </span><span class="s3">0.19786539</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.23887670</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.00343595</span><span class="s0">, </span><span class="s3">0.94162826</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(np.abs(pls.x_rotations_)</span><span class="s0">, </span><span class="s1">np.abs(expected_x_rotations))</span>
    <span class="s1">assert_array_almost_equal(np.abs(pls.x_weights_)</span><span class="s0">, </span><span class="s1">np.abs(expected_x_weights))</span>
    <span class="s1">assert_array_almost_equal(np.abs(pls.y_rotations_)</span><span class="s0">, </span><span class="s1">np.abs(expected_y_rotations))</span>
    <span class="s1">assert_array_almost_equal(np.abs(pls.y_weights_)</span><span class="s0">, </span><span class="s1">np.abs(expected_y_weights))</span>

    <span class="s1">x_rotations_sign_flip = np.sign(pls.x_rotations_ / expected_x_rotations)</span>
    <span class="s1">x_weights_sign_flip = np.sign(pls.x_weights_ / expected_x_weights)</span>
    <span class="s1">y_rotations_sign_flip = np.sign(pls.y_rotations_ / expected_y_rotations)</span>
    <span class="s1">y_weights_sign_flip = np.sign(pls.y_weights_ / expected_y_weights)</span>
    <span class="s1">assert_array_almost_equal(x_rotations_sign_flip</span><span class="s0">, </span><span class="s1">x_weights_sign_flip)</span>
    <span class="s1">assert_array_almost_equal(y_rotations_sign_flip</span><span class="s0">, </span><span class="s1">y_weights_sign_flip)</span>

    <span class="s1">assert_matrix_orthogonal(pls.x_weights_)</span>
    <span class="s1">assert_matrix_orthogonal(pls.y_weights_)</span>

    <span class="s1">assert_matrix_orthogonal(pls._x_scores)</span>
    <span class="s1">assert_matrix_orthogonal(pls._y_scores)</span>


<span class="s0">def </span><span class="s1">test_sanity_check_pls_canonical_random():</span>
    <span class="s2"># Sanity check for PLSCanonical on random data</span>
    <span class="s2"># The results were checked against the R-package plspm</span>
    <span class="s1">n = </span><span class="s3">500</span>
    <span class="s1">p_noise = </span><span class="s3">10</span>
    <span class="s1">q_noise = </span><span class="s3">5</span>
    <span class="s2"># 2 latents vars:</span>
    <span class="s1">rng = check_random_state(</span><span class="s3">11</span><span class="s1">)</span>
    <span class="s1">l1 = rng.normal(size=n)</span>
    <span class="s1">l2 = rng.normal(size=n)</span>
    <span class="s1">latents = np.array([l1</span><span class="s0">, </span><span class="s1">l1</span><span class="s0">, </span><span class="s1">l2</span><span class="s0">, </span><span class="s1">l2]).T</span>
    <span class="s1">X = latents + rng.normal(size=</span><span class="s3">4 </span><span class="s1">* n).reshape((n</span><span class="s0">, </span><span class="s3">4</span><span class="s1">))</span>
    <span class="s1">Y = latents + rng.normal(size=</span><span class="s3">4 </span><span class="s1">* n).reshape((n</span><span class="s0">, </span><span class="s3">4</span><span class="s1">))</span>
    <span class="s1">X = np.concatenate((X</span><span class="s0">, </span><span class="s1">rng.normal(size=p_noise * n).reshape(n</span><span class="s0">, </span><span class="s1">p_noise))</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">Y = np.concatenate((Y</span><span class="s0">, </span><span class="s1">rng.normal(size=q_noise * n).reshape(n</span><span class="s0">, </span><span class="s1">q_noise))</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">pls = PLSCanonical(n_components=</span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">pls.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">expected_x_weights = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s3">0.65803719</span><span class="s0">, </span><span class="s3">0.19197924</span><span class="s0">, </span><span class="s3">0.21769083</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.7009113</span><span class="s0">, </span><span class="s3">0.13303969</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.15376699</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.13528197</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.68636408</span><span class="s0">, </span><span class="s3">0.13856546</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.16854574</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.66788088</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.12485304</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.03232333</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.04189855</span><span class="s0">, </span><span class="s3">0.40690153</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.1148816</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.09643158</span><span class="s0">, </span><span class="s3">0.1613305</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.04792138</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.02384992</span><span class="s0">, </span><span class="s3">0.17175319</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.06781</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.01666137</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.18556747</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.00266945</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.00160224</span><span class="s0">, </span><span class="s3">0.11893098</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.00849528</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.07706095</span><span class="s0">, </span><span class="s3">0.1570547</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.00949471</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.02964127</span><span class="s0">, </span><span class="s3">0.34657036</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.03572177</span><span class="s0">, </span><span class="s3">0.0945091</span><span class="s0">, </span><span class="s3">0.3414855</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.05584937</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.02028961</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.57682568</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.05744254</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.01482333</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.17431274</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">expected_x_loadings = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s3">0.65649254</span><span class="s0">, </span><span class="s3">0.1847647</span><span class="s0">, </span><span class="s3">0.15270699</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.67554234</span><span class="s0">, </span><span class="s3">0.15237508</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.09182247</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.19219925</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.67750975</span><span class="s0">, </span><span class="s3">0.08673128</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.2133631</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.67034809</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.08835483</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.03178912</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.06668336</span><span class="s0">, </span><span class="s3">0.43395268</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.15684588</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.13350241</span><span class="s0">, </span><span class="s3">0.20578984</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.03337736</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.03807306</span><span class="s0">, </span><span class="s3">0.09871553</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.06199844</span><span class="s0">, </span><span class="s3">0.01559854</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1881785</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.00406146</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.00587025</span><span class="s0">, </span><span class="s3">0.16413253</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.00374239</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.05848466</span><span class="s0">, </span><span class="s3">0.19140336</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.00139214</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.01033161</span><span class="s0">, </span><span class="s3">0.32239136</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.05292828</span><span class="s0">, </span><span class="s3">0.0953533</span><span class="s0">, </span><span class="s3">0.31916881</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.04031924</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.01961045</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.65174036</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.06172484</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.06597366</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1244497</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">expected_y_weights = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s3">0.66101097</span><span class="s0">, </span><span class="s3">0.18672553</span><span class="s0">, </span><span class="s3">0.22826092</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.69347861</span><span class="s0">, </span><span class="s3">0.18463471</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.23995597</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.14462724</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.66504085</span><span class="s0">, </span><span class="s3">0.17082434</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.22247955</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.6932605</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.09832993</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.07035859</span><span class="s0">, </span><span class="s3">0.00714283</span><span class="s0">, </span><span class="s3">0.67810124</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.07765351</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.0105204</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.44108074</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.00917056</span><span class="s0">, </span><span class="s3">0.04322147</span><span class="s0">, </span><span class="s3">0.10062478</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.01909512</span><span class="s0">, </span><span class="s3">0.06182718</span><span class="s0">, </span><span class="s3">0.28830475</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.01756709</span><span class="s0">, </span><span class="s3">0.04797666</span><span class="s0">, </span><span class="s3">0.32225745</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">expected_y_loadings = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s3">0.68568625</span><span class="s0">, </span><span class="s3">0.1674376</span><span class="s0">, </span><span class="s3">0.0969508</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.68782064</span><span class="s0">, </span><span class="s3">0.20375837</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1164448</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.11712173</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.68046903</span><span class="s0">, </span><span class="s3">0.12001505</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.17860457</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.6798319</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.05089681</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.06265739</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.0277703</span><span class="s0">, </span><span class="s3">0.74729584</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.0914178</span><span class="s0">, </span><span class="s3">0.00403751</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.5135078</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.02196918</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.01377169</span><span class="s0">, </span><span class="s3">0.09564505</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.03288952</span><span class="s0">, </span><span class="s3">0.09039729</span><span class="s0">, </span><span class="s3">0.31858973</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.04287624</span><span class="s0">, </span><span class="s3">0.05254676</span><span class="s0">, </span><span class="s3">0.27836841</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(np.abs(pls.x_loadings_)</span><span class="s0">, </span><span class="s1">np.abs(expected_x_loadings))</span>
    <span class="s1">assert_array_almost_equal(np.abs(pls.x_weights_)</span><span class="s0">, </span><span class="s1">np.abs(expected_x_weights))</span>
    <span class="s1">assert_array_almost_equal(np.abs(pls.y_loadings_)</span><span class="s0">, </span><span class="s1">np.abs(expected_y_loadings))</span>
    <span class="s1">assert_array_almost_equal(np.abs(pls.y_weights_)</span><span class="s0">, </span><span class="s1">np.abs(expected_y_weights))</span>

    <span class="s1">x_loadings_sign_flip = np.sign(pls.x_loadings_ / expected_x_loadings)</span>
    <span class="s1">x_weights_sign_flip = np.sign(pls.x_weights_ / expected_x_weights)</span>
    <span class="s1">y_weights_sign_flip = np.sign(pls.y_weights_ / expected_y_weights)</span>
    <span class="s1">y_loadings_sign_flip = np.sign(pls.y_loadings_ / expected_y_loadings)</span>
    <span class="s1">assert_array_almost_equal(x_loadings_sign_flip</span><span class="s0">, </span><span class="s1">x_weights_sign_flip)</span>
    <span class="s1">assert_array_almost_equal(y_loadings_sign_flip</span><span class="s0">, </span><span class="s1">y_weights_sign_flip)</span>

    <span class="s1">assert_matrix_orthogonal(pls.x_weights_)</span>
    <span class="s1">assert_matrix_orthogonal(pls.y_weights_)</span>

    <span class="s1">assert_matrix_orthogonal(pls._x_scores)</span>
    <span class="s1">assert_matrix_orthogonal(pls._y_scores)</span>


<span class="s0">def </span><span class="s1">test_convergence_fail():</span>
    <span class="s2"># Make sure ConvergenceWarning is raised if max_iter is too small</span>
    <span class="s1">d = load_linnerud()</span>
    <span class="s1">X = d.data</span>
    <span class="s1">Y = d.target</span>
    <span class="s1">pls_nipals = PLSCanonical(n_components=X.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(ConvergenceWarning):</span>
        <span class="s1">pls_nipals.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(PLSSVD</span><span class="s0">, </span><span class="s1">PLSRegression</span><span class="s0">, </span><span class="s1">PLSCanonical))</span>
<span class="s0">def </span><span class="s1">test_attibutes_shapes(Est):</span>
    <span class="s2"># Make sure attributes are of the correct shape depending on n_components</span>
    <span class="s1">d = load_linnerud()</span>
    <span class="s1">X = d.data</span>
    <span class="s1">Y = d.target</span>
    <span class="s1">n_components = </span><span class="s3">2</span>
    <span class="s1">pls = Est(n_components=n_components)</span>
    <span class="s1">pls.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s0">assert </span><span class="s1">all(</span>
        <span class="s1">attr.shape[</span><span class="s3">1</span><span class="s1">] == n_components </span><span class="s0">for </span><span class="s1">attr </span><span class="s0">in </span><span class="s1">(pls.x_weights_</span><span class="s0">, </span><span class="s1">pls.y_weights_)</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(PLSRegression</span><span class="s0">, </span><span class="s1">PLSCanonical</span><span class="s0">, </span><span class="s1">CCA))</span>
<span class="s0">def </span><span class="s1">test_univariate_equivalence(Est):</span>
    <span class="s2"># Ensure 2D Y with 1 column is equivalent to 1D Y</span>
    <span class="s1">d = load_linnerud()</span>
    <span class="s1">X = d.data</span>
    <span class="s1">Y = d.target</span>

    <span class="s1">est = Est(n_components=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">one_d_coeff = est.fit(X</span><span class="s0">, </span><span class="s1">Y[:</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]).coef_</span>
    <span class="s1">two_d_coeff = est.fit(X</span><span class="s0">, </span><span class="s1">Y[:</span><span class="s0">, </span><span class="s1">:</span><span class="s3">1</span><span class="s1">]).coef_</span>

    <span class="s0">assert </span><span class="s1">one_d_coeff.shape == two_d_coeff.shape</span>
    <span class="s1">assert_array_almost_equal(one_d_coeff</span><span class="s0">, </span><span class="s1">two_d_coeff)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(PLSRegression</span><span class="s0">, </span><span class="s1">PLSCanonical</span><span class="s0">, </span><span class="s1">CCA</span><span class="s0">, </span><span class="s1">PLSSVD))</span>
<span class="s0">def </span><span class="s1">test_copy(Est):</span>
    <span class="s2"># check that the &quot;copy&quot; keyword works</span>
    <span class="s1">d = load_linnerud()</span>
    <span class="s1">X = d.data</span>
    <span class="s1">Y = d.target</span>
    <span class="s1">X_orig = X.copy()</span>

    <span class="s2"># copy=True won't modify inplace</span>
    <span class="s1">pls = Est(copy=</span><span class="s0">True</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">assert_array_equal(X</span><span class="s0">, </span><span class="s1">X_orig)</span>

    <span class="s2"># copy=False will modify inplace</span>
    <span class="s0">with </span><span class="s1">pytest.raises(AssertionError):</span>
        <span class="s1">Est(copy=</span><span class="s0">False</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
        <span class="s1">assert_array_almost_equal(X</span><span class="s0">, </span><span class="s1">X_orig)</span>

    <span class="s0">if </span><span class="s1">Est </span><span class="s0">is </span><span class="s1">PLSSVD:</span>
        <span class="s0">return  </span><span class="s2"># PLSSVD does not support copy param in predict or transform</span>

    <span class="s1">X_orig = X.copy()</span>
    <span class="s0">with </span><span class="s1">pytest.raises(AssertionError):</span>
        <span class="s1">pls.transform(X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">assert_array_almost_equal(X</span><span class="s0">, </span><span class="s1">X_orig)</span>

    <span class="s1">X_orig = X.copy()</span>
    <span class="s0">with </span><span class="s1">pytest.raises(AssertionError):</span>
        <span class="s1">pls.predict(X</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">assert_array_almost_equal(X</span><span class="s0">, </span><span class="s1">X_orig)</span>

    <span class="s2"># Make sure copy=True gives same transform and predictions as predict=False</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">pls.transform(X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">True</span><span class="s1">)</span><span class="s0">, </span><span class="s1">pls.transform(X.copy()</span><span class="s0">, </span><span class="s1">Y.copy()</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">pls.predict(X</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">True</span><span class="s1">)</span><span class="s0">, </span><span class="s1">pls.predict(X.copy()</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">_generate_test_scale_and_stability_datasets():</span>
    <span class="s5">&quot;&quot;&quot;Generate dataset for test_scale_and_stability&quot;&quot;&quot;</span>
    <span class="s2"># dataset for non-regression 7818</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s3">1000</span>
    <span class="s1">n_targets = </span><span class="s3">5</span>
    <span class="s1">n_features = </span><span class="s3">10</span>
    <span class="s1">Q = rng.randn(n_targets</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">Y = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_targets)</span>
    <span class="s1">X = np.dot(Y</span><span class="s0">, </span><span class="s1">Q) + </span><span class="s3">2 </span><span class="s1">* rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features) + </span><span class="s3">1</span>
    <span class="s1">X *= </span><span class="s3">1000</span>
    <span class="s0">yield </span><span class="s1">X</span><span class="s0">, </span><span class="s1">Y</span>

    <span class="s2"># Data set where one of the features is constraint</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = load_linnerud(return_X_y=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s2"># causes X[:, -1].std() to be zero</span>
    <span class="s1">X[:</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">] = </span><span class="s3">1.0</span>
    <span class="s0">yield </span><span class="s1">X</span><span class="s0">, </span><span class="s1">Y</span>

    <span class="s1">X = np.array([[</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">2.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">3.0</span><span class="s0">, </span><span class="s3">5.0</span><span class="s0">, </span><span class="s3">4.0</span><span class="s1">]])</span>
    <span class="s1">Y = np.array([[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.9</span><span class="s0">, </span><span class="s3">1.1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">6.2</span><span class="s0">, </span><span class="s3">5.9</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">11.9</span><span class="s0">, </span><span class="s3">12.3</span><span class="s1">]])</span>
    <span class="s0">yield </span><span class="s1">X</span><span class="s0">, </span><span class="s1">Y</span>

    <span class="s2"># Seeds that provide a non-regression test for #18746, where CCA fails</span>
    <span class="s1">seeds = [</span><span class="s3">530</span><span class="s0">, </span><span class="s3">741</span><span class="s1">]</span>
    <span class="s0">for </span><span class="s1">seed </span><span class="s0">in </span><span class="s1">seeds:</span>
        <span class="s1">rng = np.random.RandomState(seed)</span>
        <span class="s1">X = rng.randn(</span><span class="s3">4</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span>
        <span class="s1">Y = rng.randn(</span><span class="s3">4</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
        <span class="s0">yield </span><span class="s1">X</span><span class="s0">, </span><span class="s1">Y</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(CCA</span><span class="s0">, </span><span class="s1">PLSCanonical</span><span class="s0">, </span><span class="s1">PLSRegression</span><span class="s0">, </span><span class="s1">PLSSVD))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;X, Y&quot;</span><span class="s0">, </span><span class="s1">_generate_test_scale_and_stability_datasets())</span>
<span class="s0">def </span><span class="s1">test_scale_and_stability(Est</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">Y):</span>
    <span class="s5">&quot;&quot;&quot;scale=True is equivalent to scale=False on centered/scaled data 
    This allows to check numerical stability over platforms as well&quot;&quot;&quot;</span>

    <span class="s1">X_s</span><span class="s0">, </span><span class="s1">Y_s</span><span class="s0">, </span><span class="s1">*_ = _center_scale_xy(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">X_score</span><span class="s0">, </span><span class="s1">Y_score = Est(scale=</span><span class="s0">True</span><span class="s1">).fit_transform(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">X_s_score</span><span class="s0">, </span><span class="s1">Y_s_score = Est(scale=</span><span class="s0">False</span><span class="s1">).fit_transform(X_s</span><span class="s0">, </span><span class="s1">Y_s)</span>

    <span class="s1">assert_allclose(X_s_score</span><span class="s0">, </span><span class="s1">X_score</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-4</span><span class="s1">)</span>
    <span class="s1">assert_allclose(Y_s_score</span><span class="s0">, </span><span class="s1">Y_score</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-4</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s1">(PLSSVD</span><span class="s0">, </span><span class="s1">PLSRegression</span><span class="s0">, </span><span class="s1">PLSCanonical</span><span class="s0">, </span><span class="s1">CCA))</span>
<span class="s0">def </span><span class="s1">test_n_components_upper_bounds(Estimator):</span>
    <span class="s5">&quot;&quot;&quot;Check the validation of `n_components` upper bounds for `PLS` regressors.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(</span><span class="s3">10</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span>
    <span class="s1">Y = rng.randn(</span><span class="s3">10</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">est = Estimator(n_components=</span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">err_msg = </span><span class="s4">&quot;`n_components` upper bound is .*. Got 10 instead. Reduce `n_components`.&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">est.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;n_samples, n_features&quot;</span><span class="s0">, </span><span class="s1">[(</span><span class="s3">100</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s3">100</span><span class="s0">, </span><span class="s3">200</span><span class="s1">)])</span>
<span class="s0">def </span><span class="s1">test_singular_value_helpers(n_samples</span><span class="s0">, </span><span class="s1">n_features</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s2"># Make sure SVD and power method give approximately the same results</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = make_regression(</span>
        <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features</span><span class="s0">, </span><span class="s1">n_targets=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>
    <span class="s1">u1</span><span class="s0">, </span><span class="s1">v1</span><span class="s0">, </span><span class="s1">_ = _get_first_singular_vectors_power_method(X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">norm_y_weights=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">u2</span><span class="s0">, </span><span class="s1">v2 = _get_first_singular_vectors_svd(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">_svd_flip_1d(u1</span><span class="s0">, </span><span class="s1">v1)</span>
    <span class="s1">_svd_flip_1d(u2</span><span class="s0">, </span><span class="s1">v2)</span>

    <span class="s1">rtol = </span><span class="s3">1e-3</span>
    <span class="s2"># Setting atol because some coordinates are very close to zero</span>
    <span class="s1">assert_allclose(u1</span><span class="s0">, </span><span class="s1">u2</span><span class="s0">, </span><span class="s1">atol=u2.max() * rtol)</span>
    <span class="s1">assert_allclose(v1</span><span class="s0">, </span><span class="s1">v2</span><span class="s0">, </span><span class="s1">atol=v2.max() * rtol)</span>


<span class="s0">def </span><span class="s1">test_one_component_equivalence(global_random_seed):</span>
    <span class="s2"># PLSSVD, PLSRegression and PLSCanonical should all be equivalent when</span>
    <span class="s2"># n_components is 1</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = make_regression(</span><span class="s3">100</span><span class="s0">, </span><span class="s3">10</span><span class="s0">, </span><span class="s1">n_targets=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s1">svd = PLSSVD(n_components=</span><span class="s3">1</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">Y).transform(X)</span>
    <span class="s1">reg = PLSRegression(n_components=</span><span class="s3">1</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">Y).transform(X)</span>
    <span class="s1">canonical = PLSCanonical(n_components=</span><span class="s3">1</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">Y).transform(X)</span>

    <span class="s1">rtol = </span><span class="s3">1e-3</span>
    <span class="s2"># Setting atol because some entries are very close to zero</span>
    <span class="s1">assert_allclose(svd</span><span class="s0">, </span><span class="s1">reg</span><span class="s0">, </span><span class="s1">atol=reg.max() * rtol)</span>
    <span class="s1">assert_allclose(svd</span><span class="s0">, </span><span class="s1">canonical</span><span class="s0">, </span><span class="s1">atol=canonical.max() * rtol)</span>


<span class="s0">def </span><span class="s1">test_svd_flip_1d():</span>
    <span class="s2"># Make sure svd_flip_1d is equivalent to svd_flip</span>
    <span class="s1">u = np.array([</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">4</span><span class="s0">, </span><span class="s3">2</span><span class="s1">])</span>
    <span class="s1">v = np.array([</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s1">])</span>

    <span class="s1">u_expected</span><span class="s0">, </span><span class="s1">v_expected = svd_flip(u.reshape(-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">v.reshape(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">))</span>
    <span class="s1">_svd_flip_1d(u</span><span class="s0">, </span><span class="s1">v)  </span><span class="s2"># inplace</span>

    <span class="s1">assert_allclose(u</span><span class="s0">, </span><span class="s1">u_expected.ravel())</span>
    <span class="s1">assert_allclose(u</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">4</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2</span><span class="s1">])</span>

    <span class="s1">assert_allclose(v</span><span class="s0">, </span><span class="s1">v_expected.ravel())</span>
    <span class="s1">assert_allclose(v</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3</span><span class="s1">])</span>


<span class="s0">def </span><span class="s1">test_loadings_converges(global_random_seed):</span>
    <span class="s5">&quot;&quot;&quot;Test that CCA converges. Non-regression test for #19549.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">n_samples=</span><span class="s3">200</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s3">20</span><span class="s0">, </span><span class="s1">n_targets=</span><span class="s3">20</span><span class="s0">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>

    <span class="s1">cca = CCA(n_components=</span><span class="s3">10</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">500</span><span class="s1">)</span>

    <span class="s0">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s4">&quot;error&quot;</span><span class="s0">, </span><span class="s1">ConvergenceWarning)</span>

        <span class="s1">cca.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s2"># Loadings converges to reasonable values</span>
    <span class="s0">assert </span><span class="s1">np.all(np.abs(cca.x_loadings_) &lt; </span><span class="s3">1</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_pls_constant_y():</span>
    <span class="s5">&quot;&quot;&quot;Checks warning when y is constant. Non-regression test for #19831&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">x = rng.rand(</span><span class="s3">100</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">y = np.zeros(</span><span class="s3">100</span><span class="s1">)</span>

    <span class="s1">pls = PLSRegression()</span>

    <span class="s1">msg = </span><span class="s4">&quot;Y residual is constant at iteration&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">pls.fit(x</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">assert_allclose(pls.x_rotations_</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;PLSEstimator&quot;</span><span class="s0">, </span><span class="s1">[PLSRegression</span><span class="s0">, </span><span class="s1">PLSCanonical</span><span class="s0">, </span><span class="s1">CCA])</span>
<span class="s0">def </span><span class="s1">test_pls_coef_shape(PLSEstimator):</span>
    <span class="s5">&quot;&quot;&quot;Check the shape of `coef_` attribute. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/12410 
    &quot;&quot;&quot;</span>
    <span class="s1">d = load_linnerud()</span>
    <span class="s1">X = d.data</span>
    <span class="s1">Y = d.target</span>

    <span class="s1">pls = PLSEstimator(copy=</span><span class="s0">True</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">n_targets</span><span class="s0">, </span><span class="s1">n_features = Y.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">X.shape[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s0">assert </span><span class="s1">pls.coef_.shape == (n_targets</span><span class="s0">, </span><span class="s1">n_features)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;scale&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;PLSEstimator&quot;</span><span class="s0">, </span><span class="s1">[PLSRegression</span><span class="s0">, </span><span class="s1">PLSCanonical</span><span class="s0">, </span><span class="s1">CCA])</span>
<span class="s0">def </span><span class="s1">test_pls_prediction(PLSEstimator</span><span class="s0">, </span><span class="s1">scale):</span>
    <span class="s5">&quot;&quot;&quot;Check the behaviour of the prediction function.&quot;&quot;&quot;</span>
    <span class="s1">d = load_linnerud()</span>
    <span class="s1">X = d.data</span>
    <span class="s1">Y = d.target</span>

    <span class="s1">pls = PLSEstimator(copy=</span><span class="s0">True, </span><span class="s1">scale=scale).fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">Y_pred = pls.predict(X</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">True</span><span class="s1">)</span>

    <span class="s1">y_mean = Y.mean(axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X_trans = X - X.mean(axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">scale:</span>
        <span class="s1">X_trans /= X.std(axis=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">ddof=</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">assert_allclose(pls.intercept_</span><span class="s0">, </span><span class="s1">y_mean)</span>
    <span class="s1">assert_allclose(Y_pred</span><span class="s0">, </span><span class="s1">X_trans @ pls.coef_.T + pls.intercept_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Klass&quot;</span><span class="s0">, </span><span class="s1">[CCA</span><span class="s0">, </span><span class="s1">PLSSVD</span><span class="s0">, </span><span class="s1">PLSRegression</span><span class="s0">, </span><span class="s1">PLSCanonical])</span>
<span class="s0">def </span><span class="s1">test_pls_feature_names_out(Klass):</span>
    <span class="s5">&quot;&quot;&quot;Check `get_feature_names_out` cross_decomposition module.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = load_linnerud(return_X_y=</span><span class="s0">True</span><span class="s1">)</span>

    <span class="s1">est = Klass().fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">names_out = est.get_feature_names_out()</span>

    <span class="s1">class_name_lower = Klass.__name__.lower()</span>
    <span class="s1">expected_names_out = np.array(</span>
        <span class="s1">[</span><span class="s4">f&quot;</span><span class="s0">{</span><span class="s1">class_name_lower</span><span class="s0">}{</span><span class="s1">i</span><span class="s0">}</span><span class="s4">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(est.x_weights_.shape[</span><span class="s3">1</span><span class="s1">])]</span><span class="s0">,</span>
        <span class="s1">dtype=object</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(names_out</span><span class="s0">, </span><span class="s1">expected_names_out)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Klass&quot;</span><span class="s0">, </span><span class="s1">[CCA</span><span class="s0">, </span><span class="s1">PLSSVD</span><span class="s0">, </span><span class="s1">PLSRegression</span><span class="s0">, </span><span class="s1">PLSCanonical])</span>
<span class="s0">def </span><span class="s1">test_pls_set_output(Klass):</span>
    <span class="s5">&quot;&quot;&quot;Check `set_output` in cross_decomposition module.&quot;&quot;&quot;</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s4">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = load_linnerud(return_X_y=</span><span class="s0">True, </span><span class="s1">as_frame=</span><span class="s0">True</span><span class="s1">)</span>

    <span class="s1">est = Klass().set_output(transform=</span><span class="s4">&quot;pandas&quot;</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">X_trans</span><span class="s0">, </span><span class="s1">y_trans = est.transform(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s0">assert </span><span class="s1">isinstance(y_trans</span><span class="s0">, </span><span class="s1">np.ndarray)</span>
    <span class="s0">assert </span><span class="s1">isinstance(X_trans</span><span class="s0">, </span><span class="s1">pd.DataFrame)</span>
    <span class="s1">assert_array_equal(X_trans.columns</span><span class="s0">, </span><span class="s1">est.get_feature_names_out())</span>


<span class="s0">def </span><span class="s1">test_pls_regression_fit_1d_y():</span>
    <span class="s5">&quot;&quot;&quot;Check that when fitting with 1d `y`, prediction should also be 1d. 
 
    Non-regression test for Issue #26549. 
    &quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s0">, </span><span class="s3">4</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">3</span><span class="s0">, </span><span class="s3">9</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">4</span><span class="s0">, </span><span class="s3">16</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">5</span><span class="s0">, </span><span class="s3">25</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">6</span><span class="s0">, </span><span class="s3">36</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s3">2</span><span class="s0">, </span><span class="s3">6</span><span class="s0">, </span><span class="s3">12</span><span class="s0">, </span><span class="s3">20</span><span class="s0">, </span><span class="s3">30</span><span class="s0">, </span><span class="s3">42</span><span class="s1">])</span>
    <span class="s1">expected = y.copy()</span>

    <span class="s1">plsr = PLSRegression().fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">y_pred = plsr.predict(X)</span>
    <span class="s0">assert </span><span class="s1">y_pred.shape == expected.shape</span>

    <span class="s2"># Check that it works in VotingRegressor</span>
    <span class="s1">lr = LinearRegression().fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">vr = VotingRegressor([(</span><span class="s4">&quot;lr&quot;</span><span class="s0">, </span><span class="s1">lr)</span><span class="s0">, </span><span class="s1">(</span><span class="s4">&quot;plsr&quot;</span><span class="s0">, </span><span class="s1">plsr)])</span>
    <span class="s1">y_pred = vr.fit(X</span><span class="s0">, </span><span class="s1">y).predict(X)</span>
    <span class="s0">assert </span><span class="s1">y_pred.shape == expected.shape</span>
    <span class="s1">assert_allclose(y_pred</span><span class="s0">, </span><span class="s1">expected)</span>
</pre>
</body>
</html>