<html>
<head>
<title>_ridge.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_ridge.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Ridge regression 
&quot;&quot;&quot;</span>

<span class="s2"># Author: Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span>
<span class="s2">#         Reuben Fletcher-Costin &lt;reuben.fletchercostin@gmail.com&gt;</span>
<span class="s2">#         Fabian Pedregosa &lt;fabian@fseoane.net&gt;</span>
<span class="s2">#         Michael Eickenberg &lt;michael.eickenberg@nsup.org&gt;</span>
<span class="s2"># License: BSD 3 clause</span>


<span class="s3">import </span><span class="s1">numbers</span>
<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">abc </span><span class="s3">import </span><span class="s1">ABCMeta</span><span class="s3">, </span><span class="s1">abstractmethod</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">linalg</span><span class="s3">, </span><span class="s1">optimize</span><span class="s3">, </span><span class="s1">sparse</span>
<span class="s3">from </span><span class="s1">scipy.sparse </span><span class="s3">import </span><span class="s1">linalg </span><span class="s3">as </span><span class="s1">sp_linalg</span>

<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">MultiOutputMixin</span><span class="s3">, </span><span class="s1">RegressorMixin</span><span class="s3">, </span><span class="s1">_fit_context</span><span class="s3">, </span><span class="s1">is_classifier</span>
<span class="s3">from </span><span class="s1">..exceptions </span><span class="s3">import </span><span class="s1">ConvergenceWarning</span>
<span class="s3">from </span><span class="s1">..metrics </span><span class="s3">import </span><span class="s1">check_scoring</span><span class="s3">, </span><span class="s1">get_scorer_names</span>
<span class="s3">from </span><span class="s1">..model_selection </span><span class="s3">import </span><span class="s1">GridSearchCV</span>
<span class="s3">from </span><span class="s1">..preprocessing </span><span class="s3">import </span><span class="s1">LabelBinarizer</span>
<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">check_array</span><span class="s3">,</span>
    <span class="s1">check_consistent_length</span><span class="s3">,</span>
    <span class="s1">check_scalar</span><span class="s3">,</span>
    <span class="s1">column_or_1d</span><span class="s3">,</span>
    <span class="s1">compute_sample_weight</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">StrOptions</span>
<span class="s3">from </span><span class="s1">..utils.extmath </span><span class="s3">import </span><span class="s1">row_norms</span><span class="s3">, </span><span class="s1">safe_sparse_dot</span>
<span class="s3">from </span><span class="s1">..utils.fixes </span><span class="s3">import </span><span class="s1">_sparse_linalg_cg</span>
<span class="s3">from </span><span class="s1">..utils.sparsefuncs </span><span class="s3">import </span><span class="s1">mean_variance_axis</span>
<span class="s3">from </span><span class="s1">..utils.validation </span><span class="s3">import </span><span class="s1">_check_sample_weight</span><span class="s3">, </span><span class="s1">check_is_fitted</span>
<span class="s3">from </span><span class="s1">._base </span><span class="s3">import </span><span class="s1">LinearClassifierMixin</span><span class="s3">, </span><span class="s1">LinearModel</span><span class="s3">, </span><span class="s1">_preprocess_data</span><span class="s3">, </span><span class="s1">_rescale_data</span>
<span class="s3">from </span><span class="s1">._sag </span><span class="s3">import </span><span class="s1">sag_solver</span>


<span class="s3">def </span><span class="s1">_get_rescaled_operator(X</span><span class="s3">, </span><span class="s1">X_offset</span><span class="s3">, </span><span class="s1">sample_weight_sqrt):</span>
    <span class="s0">&quot;&quot;&quot;Create LinearOperator for matrix products with implicit centering. 
 
    Matrix product `LinearOperator @ coef` returns `(X - X_offset) @ coef`. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">matvec(b):</span>
        <span class="s3">return </span><span class="s1">X.dot(b) - sample_weight_sqrt * b.dot(X_offset)</span>

    <span class="s3">def </span><span class="s1">rmatvec(b):</span>
        <span class="s3">return </span><span class="s1">X.T.dot(b) - X_offset * b.dot(sample_weight_sqrt)</span>

    <span class="s1">X1 = sparse.linalg.LinearOperator(shape=X.shape</span><span class="s3">, </span><span class="s1">matvec=matvec</span><span class="s3">, </span><span class="s1">rmatvec=rmatvec)</span>
    <span class="s3">return </span><span class="s1">X1</span>


<span class="s3">def </span><span class="s1">_solve_sparse_cg(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">y</span><span class="s3">,</span>
    <span class="s1">alpha</span><span class="s3">,</span>
    <span class="s1">max_iter=</span><span class="s3">None,</span>
    <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
    <span class="s1">verbose=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">X_offset=</span><span class="s3">None,</span>
    <span class="s1">X_scale=</span><span class="s3">None,</span>
    <span class="s1">sample_weight_sqrt=</span><span class="s3">None,</span>
<span class="s1">):</span>
    <span class="s3">if </span><span class="s1">sample_weight_sqrt </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">sample_weight_sqrt = np.ones(X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>

    <span class="s3">if </span><span class="s1">X_offset </span><span class="s3">is None or </span><span class="s1">X_scale </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">X1 = sp_linalg.aslinearoperator(X)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">X_offset_scale = X_offset / X_scale</span>
        <span class="s1">X1 = _get_rescaled_operator(X</span><span class="s3">, </span><span class="s1">X_offset_scale</span><span class="s3">, </span><span class="s1">sample_weight_sqrt)</span>

    <span class="s1">coefs = np.empty((y.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">n_features)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s3">if </span><span class="s1">n_features &gt; n_samples:</span>

        <span class="s3">def </span><span class="s1">create_mv(curr_alpha):</span>
            <span class="s3">def </span><span class="s1">_mv(x):</span>
                <span class="s3">return </span><span class="s1">X1.matvec(X1.rmatvec(x)) + curr_alpha * x</span>

            <span class="s3">return </span><span class="s1">_mv</span>

    <span class="s3">else</span><span class="s1">:</span>

        <span class="s3">def </span><span class="s1">create_mv(curr_alpha):</span>
            <span class="s3">def </span><span class="s1">_mv(x):</span>
                <span class="s3">return </span><span class="s1">X1.rmatvec(X1.matvec(x)) + curr_alpha * x</span>

            <span class="s3">return </span><span class="s1">_mv</span>

    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(y.shape[</span><span class="s4">1</span><span class="s1">]):</span>
        <span class="s1">y_column = y[:</span><span class="s3">, </span><span class="s1">i]</span>

        <span class="s1">mv = create_mv(alpha[i])</span>
        <span class="s3">if </span><span class="s1">n_features &gt; n_samples:</span>
            <span class="s2"># kernel ridge</span>
            <span class="s2"># w = X.T * inv(X X^t + alpha*Id) y</span>
            <span class="s1">C = sp_linalg.LinearOperator(</span>
                <span class="s1">(n_samples</span><span class="s3">, </span><span class="s1">n_samples)</span><span class="s3">, </span><span class="s1">matvec=mv</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span>
            <span class="s1">)</span>
            <span class="s1">coef</span><span class="s3">, </span><span class="s1">info = _sparse_linalg_cg(C</span><span class="s3">, </span><span class="s1">y_column</span><span class="s3">, </span><span class="s1">rtol=tol)</span>
            <span class="s1">coefs[i] = X1.rmatvec(coef)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># linear ridge</span>
            <span class="s2"># w = inv(X^t X + alpha*Id) * X.T y</span>
            <span class="s1">y_column = X1.rmatvec(y_column)</span>
            <span class="s1">C = sp_linalg.LinearOperator(</span>
                <span class="s1">(n_features</span><span class="s3">, </span><span class="s1">n_features)</span><span class="s3">, </span><span class="s1">matvec=mv</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span>
            <span class="s1">)</span>
            <span class="s1">coefs[i]</span><span class="s3">, </span><span class="s1">info = _sparse_linalg_cg(C</span><span class="s3">, </span><span class="s1">y_column</span><span class="s3">, </span><span class="s1">maxiter=max_iter</span><span class="s3">, </span><span class="s1">rtol=tol)</span>

        <span class="s3">if </span><span class="s1">info &lt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Failed with error code %d&quot; </span><span class="s1">% info)</span>

        <span class="s3">if </span><span class="s1">max_iter </span><span class="s3">is None and </span><span class="s1">info &gt; </span><span class="s4">0 </span><span class="s3">and </span><span class="s1">verbose:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s5">&quot;sparse_cg did not converge after %d iterations.&quot; </span><span class="s1">% info</span><span class="s3">,</span>
                <span class="s1">ConvergenceWarning</span><span class="s3">,</span>
            <span class="s1">)</span>

    <span class="s3">return </span><span class="s1">coefs</span>


<span class="s3">def </span><span class="s1">_solve_lsqr(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">y</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">alpha</span><span class="s3">,</span>
    <span class="s1">fit_intercept=</span><span class="s3">True,</span>
    <span class="s1">max_iter=</span><span class="s3">None,</span>
    <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
    <span class="s1">X_offset=</span><span class="s3">None,</span>
    <span class="s1">X_scale=</span><span class="s3">None,</span>
    <span class="s1">sample_weight_sqrt=</span><span class="s3">None,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Solve Ridge regression via LSQR. 
 
    We expect that y is always mean centered. 
    If X is dense, we expect it to be mean centered such that we can solve 
        ||y - Xw||_2^2 + alpha * ||w||_2^2 
 
    If X is sparse, we expect X_offset to be given such that we can solve 
        ||y - (X - X_offset)w||_2^2 + alpha * ||w||_2^2 
 
    With sample weights S=diag(sample_weight), this becomes 
        ||sqrt(S) (y - (X - X_offset) w)||_2^2 + alpha * ||w||_2^2 
    and we expect y and X to already be rescaled, i.e. sqrt(S) @ y, sqrt(S) @ X. In 
    this case, X_offset is the sample_weight weighted mean of X before scaling by 
    sqrt(S). The objective then reads 
       ||y - (X - sqrt(S) X_offset) w)||_2^2 + alpha * ||w||_2^2 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">sample_weight_sqrt </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">sample_weight_sqrt = np.ones(X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s3">if </span><span class="s1">sparse.issparse(X) </span><span class="s3">and </span><span class="s1">fit_intercept:</span>
        <span class="s1">X_offset_scale = X_offset / X_scale</span>
        <span class="s1">X1 = _get_rescaled_operator(X</span><span class="s3">, </span><span class="s1">X_offset_scale</span><span class="s3">, </span><span class="s1">sample_weight_sqrt)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s2"># No need to touch anything</span>
        <span class="s1">X1 = X</span>

    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">coefs = np.empty((y.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">n_features)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
    <span class="s1">n_iter = np.empty(y.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.int32)</span>

    <span class="s2"># According to the lsqr documentation, alpha = damp^2.</span>
    <span class="s1">sqrt_alpha = np.sqrt(alpha)</span>

    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(y.shape[</span><span class="s4">1</span><span class="s1">]):</span>
        <span class="s1">y_column = y[:</span><span class="s3">, </span><span class="s1">i]</span>
        <span class="s1">info = sp_linalg.lsqr(</span>
            <span class="s1">X1</span><span class="s3">, </span><span class="s1">y_column</span><span class="s3">, </span><span class="s1">damp=sqrt_alpha[i]</span><span class="s3">, </span><span class="s1">atol=tol</span><span class="s3">, </span><span class="s1">btol=tol</span><span class="s3">, </span><span class="s1">iter_lim=max_iter</span>
        <span class="s1">)</span>
        <span class="s1">coefs[i] = info[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">n_iter[i] = info[</span><span class="s4">2</span><span class="s1">]</span>

    <span class="s3">return </span><span class="s1">coefs</span><span class="s3">, </span><span class="s1">n_iter</span>


<span class="s3">def </span><span class="s1">_solve_cholesky(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">alpha):</span>
    <span class="s2"># w = inv(X^t X + alpha*Id) * X.T y</span>
    <span class="s1">n_features = X.shape[</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">n_targets = y.shape[</span><span class="s4">1</span><span class="s1">]</span>

    <span class="s1">A = safe_sparse_dot(X.T</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">Xy = safe_sparse_dot(X.T</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s1">one_alpha = np.array_equal(alpha</span><span class="s3">, </span><span class="s1">len(alpha) * [alpha[</span><span class="s4">0</span><span class="s1">]])</span>

    <span class="s3">if </span><span class="s1">one_alpha:</span>
        <span class="s1">A.flat[:: n_features + </span><span class="s4">1</span><span class="s1">] += alpha[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s3">return </span><span class="s1">linalg.solve(A</span><span class="s3">, </span><span class="s1">Xy</span><span class="s3">, </span><span class="s1">assume_a=</span><span class="s5">&quot;pos&quot;</span><span class="s3">, </span><span class="s1">overwrite_a=</span><span class="s3">True</span><span class="s1">).T</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">coefs = np.empty([n_targets</span><span class="s3">, </span><span class="s1">n_features]</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s3">for </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">target</span><span class="s3">, </span><span class="s1">current_alpha </span><span class="s3">in </span><span class="s1">zip(coefs</span><span class="s3">, </span><span class="s1">Xy.T</span><span class="s3">, </span><span class="s1">alpha):</span>
            <span class="s1">A.flat[:: n_features + </span><span class="s4">1</span><span class="s1">] += current_alpha</span>
            <span class="s1">coef[:] = linalg.solve(A</span><span class="s3">, </span><span class="s1">target</span><span class="s3">, </span><span class="s1">assume_a=</span><span class="s5">&quot;pos&quot;</span><span class="s3">, </span><span class="s1">overwrite_a=</span><span class="s3">False</span><span class="s1">).ravel()</span>
            <span class="s1">A.flat[:: n_features + </span><span class="s4">1</span><span class="s1">] -= current_alpha</span>
        <span class="s3">return </span><span class="s1">coefs</span>


<span class="s3">def </span><span class="s1">_solve_cholesky_kernel(K</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">):</span>
    <span class="s2"># dual_coef = inv(X X^t + alpha*Id) y</span>
    <span class="s1">n_samples = K.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">n_targets = y.shape[</span><span class="s4">1</span><span class="s1">]</span>

    <span class="s3">if </span><span class="s1">copy:</span>
        <span class="s1">K = K.copy()</span>

    <span class="s1">alpha = np.atleast_1d(alpha)</span>
    <span class="s1">one_alpha = (alpha == alpha[</span><span class="s4">0</span><span class="s1">]).all()</span>
    <span class="s1">has_sw = isinstance(sample_weight</span><span class="s3">, </span><span class="s1">np.ndarray) </span><span class="s3">or </span><span class="s1">sample_weight </span><span class="s3">not in </span><span class="s1">[</span><span class="s4">1.0</span><span class="s3">, None</span><span class="s1">]</span>

    <span class="s3">if </span><span class="s1">has_sw:</span>
        <span class="s2"># Unlike other solvers, we need to support sample_weight directly</span>
        <span class="s2"># because K might be a pre-computed kernel.</span>
        <span class="s1">sw = np.sqrt(np.atleast_1d(sample_weight))</span>
        <span class="s1">y = y * sw[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">K *= np.outer(sw</span><span class="s3">, </span><span class="s1">sw)</span>

    <span class="s3">if </span><span class="s1">one_alpha:</span>
        <span class="s2"># Only one penalty, we can solve multi-target problems in one time.</span>
        <span class="s1">K.flat[:: n_samples + </span><span class="s4">1</span><span class="s1">] += alpha[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s3">try</span><span class="s1">:</span>
            <span class="s2"># Note: we must use overwrite_a=False in order to be able to</span>
            <span class="s2">#       use the fall-back solution below in case a LinAlgError</span>
            <span class="s2">#       is raised</span>
            <span class="s1">dual_coef = linalg.solve(K</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">assume_a=</span><span class="s5">&quot;pos&quot;</span><span class="s3">, </span><span class="s1">overwrite_a=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s3">except </span><span class="s1">np.linalg.LinAlgError:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s5">&quot;Singular matrix in solving dual problem. Using &quot;</span>
                <span class="s5">&quot;least-squares solution instead.&quot;</span>
            <span class="s1">)</span>
            <span class="s1">dual_coef = linalg.lstsq(K</span><span class="s3">, </span><span class="s1">y)[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s2"># K is expensive to compute and store in memory so change it back in</span>
        <span class="s2"># case it was user-given.</span>
        <span class="s1">K.flat[:: n_samples + </span><span class="s4">1</span><span class="s1">] -= alpha[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s3">if </span><span class="s1">has_sw:</span>
            <span class="s1">dual_coef *= sw[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>

        <span class="s3">return </span><span class="s1">dual_coef</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s2"># One penalty per target. We need to solve each target separately.</span>
        <span class="s1">dual_coefs = np.empty([n_targets</span><span class="s3">, </span><span class="s1">n_samples]</span><span class="s3">, </span><span class="s1">K.dtype)</span>

        <span class="s3">for </span><span class="s1">dual_coef</span><span class="s3">, </span><span class="s1">target</span><span class="s3">, </span><span class="s1">current_alpha </span><span class="s3">in </span><span class="s1">zip(dual_coefs</span><span class="s3">, </span><span class="s1">y.T</span><span class="s3">, </span><span class="s1">alpha):</span>
            <span class="s1">K.flat[:: n_samples + </span><span class="s4">1</span><span class="s1">] += current_alpha</span>

            <span class="s1">dual_coef[:] = linalg.solve(</span>
                <span class="s1">K</span><span class="s3">, </span><span class="s1">target</span><span class="s3">, </span><span class="s1">assume_a=</span><span class="s5">&quot;pos&quot;</span><span class="s3">, </span><span class="s1">overwrite_a=</span><span class="s3">False</span>
            <span class="s1">).ravel()</span>

            <span class="s1">K.flat[:: n_samples + </span><span class="s4">1</span><span class="s1">] -= current_alpha</span>

        <span class="s3">if </span><span class="s1">has_sw:</span>
            <span class="s1">dual_coefs *= sw[np.newaxis</span><span class="s3">, </span><span class="s1">:]</span>

        <span class="s3">return </span><span class="s1">dual_coefs.T</span>


<span class="s3">def </span><span class="s1">_solve_svd(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">alpha):</span>
    <span class="s1">U</span><span class="s3">, </span><span class="s1">s</span><span class="s3">, </span><span class="s1">Vt = linalg.svd(X</span><span class="s3">, </span><span class="s1">full_matrices=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">idx = s &gt; </span><span class="s4">1e-15  </span><span class="s2"># same default value as scipy.linalg.pinv</span>
    <span class="s1">s_nnz = s[idx][:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">UTy = np.dot(U.T</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">d = np.zeros((s.size</span><span class="s3">, </span><span class="s1">alpha.size)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
    <span class="s1">d[idx] = s_nnz / (s_nnz**</span><span class="s4">2 </span><span class="s1">+ alpha)</span>
    <span class="s1">d_UT_y = d * UTy</span>
    <span class="s3">return </span><span class="s1">np.dot(Vt.T</span><span class="s3">, </span><span class="s1">d_UT_y).T</span>


<span class="s3">def </span><span class="s1">_solve_lbfgs(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">y</span><span class="s3">,</span>
    <span class="s1">alpha</span><span class="s3">,</span>
    <span class="s1">positive=</span><span class="s3">True,</span>
    <span class="s1">max_iter=</span><span class="s3">None,</span>
    <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
    <span class="s1">X_offset=</span><span class="s3">None,</span>
    <span class="s1">X_scale=</span><span class="s3">None,</span>
    <span class="s1">sample_weight_sqrt=</span><span class="s3">None,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Solve ridge regression with LBFGS. 
 
    The main purpose is fitting with forcing coefficients to be positive. 
    For unconstrained ridge regression, there are faster dedicated solver methods. 
    Note that with positive bounds on the coefficients, LBFGS seems faster 
    than scipy.optimize.lsq_linear. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>

    <span class="s1">options = {}</span>
    <span class="s3">if </span><span class="s1">max_iter </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">options[</span><span class="s5">&quot;maxiter&quot;</span><span class="s1">] = max_iter</span>
    <span class="s1">config = {</span>
        <span class="s5">&quot;method&quot;</span><span class="s1">: </span><span class="s5">&quot;L-BFGS-B&quot;</span><span class="s3">,</span>
        <span class="s5">&quot;tol&quot;</span><span class="s1">: tol</span><span class="s3">,</span>
        <span class="s5">&quot;jac&quot;</span><span class="s1">: </span><span class="s3">True,</span>
        <span class="s5">&quot;options&quot;</span><span class="s1">: options</span><span class="s3">,</span>
    <span class="s1">}</span>
    <span class="s3">if </span><span class="s1">positive:</span>
        <span class="s1">config[</span><span class="s5">&quot;bounds&quot;</span><span class="s1">] = [(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">np.inf)] * n_features</span>

    <span class="s3">if </span><span class="s1">X_offset </span><span class="s3">is not None and </span><span class="s1">X_scale </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">X_offset_scale = X_offset / X_scale</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">X_offset_scale = </span><span class="s3">None</span>

    <span class="s3">if </span><span class="s1">sample_weight_sqrt </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">sample_weight_sqrt = np.ones(X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s1">coefs = np.empty((y.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">n_features)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(y.shape[</span><span class="s4">1</span><span class="s1">]):</span>
        <span class="s1">x0 = np.zeros((n_features</span><span class="s3">,</span><span class="s1">))</span>
        <span class="s1">y_column = y[:</span><span class="s3">, </span><span class="s1">i]</span>

        <span class="s3">def </span><span class="s1">func(w):</span>
            <span class="s1">residual = X.dot(w) - y_column</span>
            <span class="s3">if </span><span class="s1">X_offset_scale </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">residual -= sample_weight_sqrt * w.dot(X_offset_scale)</span>
            <span class="s1">f = </span><span class="s4">0.5 </span><span class="s1">* residual.dot(residual) + </span><span class="s4">0.5 </span><span class="s1">* alpha[i] * w.dot(w)</span>
            <span class="s1">grad = X.T @ residual + alpha[i] * w</span>
            <span class="s3">if </span><span class="s1">X_offset_scale </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">grad -= X_offset_scale * residual.dot(sample_weight_sqrt)</span>

            <span class="s3">return </span><span class="s1">f</span><span class="s3">, </span><span class="s1">grad</span>

        <span class="s1">result = optimize.minimize(func</span><span class="s3">, </span><span class="s1">x0</span><span class="s3">, </span><span class="s1">**config)</span>
        <span class="s3">if not </span><span class="s1">result[</span><span class="s5">&quot;success&quot;</span><span class="s1">]:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s5">&quot;The lbfgs solver did not converge. Try increasing max_iter &quot;</span>
                    <span class="s5">f&quot;or tol. Currently: max_iter=</span><span class="s3">{</span><span class="s1">max_iter</span><span class="s3">} </span><span class="s5">and tol=</span><span class="s3">{</span><span class="s1">tol</span><span class="s3">}</span><span class="s5">&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">ConvergenceWarning</span><span class="s3">,</span>
            <span class="s1">)</span>
        <span class="s1">coefs[i] = result[</span><span class="s5">&quot;x&quot;</span><span class="s1">]</span>

    <span class="s3">return </span><span class="s1">coefs</span>


<span class="s3">def </span><span class="s1">_get_valid_accept_sparse(is_X_sparse</span><span class="s3">, </span><span class="s1">solver):</span>
    <span class="s3">if </span><span class="s1">is_X_sparse </span><span class="s3">and </span><span class="s1">solver </span><span class="s3">in </span><span class="s1">[</span><span class="s5">&quot;auto&quot;</span><span class="s3">, </span><span class="s5">&quot;sag&quot;</span><span class="s3">, </span><span class="s5">&quot;saga&quot;</span><span class="s1">]:</span>
        <span class="s3">return </span><span class="s5">&quot;csr&quot;</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">[</span><span class="s5">&quot;csr&quot;</span><span class="s3">, </span><span class="s5">&quot;csc&quot;</span><span class="s3">, </span><span class="s5">&quot;coo&quot;</span><span class="s1">]</span>


<span class="s3">def </span><span class="s1">ridge_regression(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">y</span><span class="s3">,</span>
    <span class="s1">alpha</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">sample_weight=</span><span class="s3">None,</span>
    <span class="s1">solver=</span><span class="s5">&quot;auto&quot;</span><span class="s3">,</span>
    <span class="s1">max_iter=</span><span class="s3">None,</span>
    <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
    <span class="s1">verbose=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">positive=</span><span class="s3">False,</span>
    <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">return_n_iter=</span><span class="s3">False,</span>
    <span class="s1">return_intercept=</span><span class="s3">False,</span>
    <span class="s1">check_input=</span><span class="s3">True,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Solve the ridge equation by the method of normal equations. 
 
    Read more in the :ref:`User Guide &lt;ridge_regression&gt;`. 
 
    Parameters 
    ---------- 
    X : {ndarray, sparse matrix, LinearOperator} of shape \ 
        (n_samples, n_features) 
        Training data. 
 
    y : ndarray of shape (n_samples,) or (n_samples, n_targets) 
        Target values. 
 
    alpha : float or array-like of shape (n_targets,) 
        Constant that multiplies the L2 term, controlling regularization 
        strength. `alpha` must be a non-negative float i.e. in `[0, inf)`. 
 
        When `alpha = 0`, the objective is equivalent to ordinary least 
        squares, solved by the :class:`LinearRegression` object. For numerical 
        reasons, using `alpha = 0` with the `Ridge` object is not advised. 
        Instead, you should use the :class:`LinearRegression` object. 
 
        If an array is passed, penalties are assumed to be specific to the 
        targets. Hence they must correspond in number. 
 
    sample_weight : float or array-like of shape (n_samples,), default=None 
        Individual weights for each sample. If given a float, every sample 
        will have the same weight. If sample_weight is not None and 
        solver='auto', the solver will be set to 'cholesky'. 
 
        .. versionadded:: 0.17 
 
    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', \ 
            'sag', 'saga', 'lbfgs'}, default='auto' 
        Solver to use in the computational routines: 
 
        - 'auto' chooses the solver automatically based on the type of data. 
 
        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge 
          coefficients. It is the most stable solver, in particular more stable 
          for singular matrices than 'cholesky' at the cost of being slower. 
 
        - 'cholesky' uses the standard scipy.linalg.solve function to 
          obtain a closed-form solution via a Cholesky decomposition of 
          dot(X.T, X) 
 
        - 'sparse_cg' uses the conjugate gradient solver as found in 
          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is 
          more appropriate than 'cholesky' for large-scale data 
          (possibility to set `tol` and `max_iter`). 
 
        - 'lsqr' uses the dedicated regularized least-squares routine 
          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative 
          procedure. 
 
        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses 
          its improved, unbiased version named SAGA. Both methods also use an 
          iterative procedure, and are often faster than other solvers when 
          both n_samples and n_features are large. Note that 'sag' and 
          'saga' fast convergence is only guaranteed on features with 
          approximately the same scale. You can preprocess the data with a 
          scaler from sklearn.preprocessing. 
 
        - 'lbfgs' uses L-BFGS-B algorithm implemented in 
          `scipy.optimize.minimize`. It can be used only when `positive` 
          is True. 
 
        All solvers except 'svd' support both dense and sparse data. However, only 
        'lsqr', 'sag', 'sparse_cg', and 'lbfgs' support sparse input when 
        `fit_intercept` is True. 
 
        .. versionadded:: 0.17 
           Stochastic Average Gradient descent solver. 
        .. versionadded:: 0.19 
           SAGA solver. 
 
    max_iter : int, default=None 
        Maximum number of iterations for conjugate gradient solver. 
        For the 'sparse_cg' and 'lsqr' solvers, the default value is determined 
        by scipy.sparse.linalg. For 'sag' and saga solver, the default value is 
        1000. For 'lbfgs' solver, the default value is 15000. 
 
    tol : float, default=1e-4 
        Precision of the solution. Note that `tol` has no effect for solvers 'svd' and 
        'cholesky'. 
 
        .. versionchanged:: 1.2 
           Default value changed from 1e-3 to 1e-4 for consistency with other linear 
           models. 
 
    verbose : int, default=0 
        Verbosity level. Setting verbose &gt; 0 will display additional 
        information depending on the solver used. 
 
    positive : bool, default=False 
        When set to ``True``, forces the coefficients to be positive. 
        Only 'lbfgs' solver is supported in this case. 
 
    random_state : int, RandomState instance, default=None 
        Used when ``solver`` == 'sag' or 'saga' to shuffle the data. 
        See :term:`Glossary &lt;random_state&gt;` for details. 
 
    return_n_iter : bool, default=False 
        If True, the method also returns `n_iter`, the actual number of 
        iteration performed by the solver. 
 
        .. versionadded:: 0.17 
 
    return_intercept : bool, default=False 
        If True and if X is sparse, the method also returns the intercept, 
        and the solver is automatically changed to 'sag'. This is only a 
        temporary fix for fitting the intercept with sparse data. For dense 
        data, use sklearn.linear_model._preprocess_data before your regression. 
 
        .. versionadded:: 0.17 
 
    check_input : bool, default=True 
        If False, the input arrays X and y will not be checked. 
 
        .. versionadded:: 0.21 
 
    Returns 
    ------- 
    coef : ndarray of shape (n_features,) or (n_targets, n_features) 
        Weight vector(s). 
 
    n_iter : int, optional 
        The actual number of iteration performed by the solver. 
        Only returned if `return_n_iter` is True. 
 
    intercept : float or ndarray of shape (n_targets,) 
        The intercept of the model. Only returned if `return_intercept` 
        is True and if X is a scipy sparse array. 
 
    Notes 
    ----- 
    This function won't compute the intercept. 
 
    Regularization improves the conditioning of the problem and 
    reduces the variance of the estimates. Larger values specify stronger 
    regularization. Alpha corresponds to ``1 / (2C)`` in other linear 
    models such as :class:`~sklearn.linear_model.LogisticRegression` or 
    :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are 
    assumed to be specific to the targets. Hence they must correspond in 
    number. 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">_ridge_regression(</span>
        <span class="s1">X</span><span class="s3">,</span>
        <span class="s1">y</span><span class="s3">,</span>
        <span class="s1">alpha</span><span class="s3">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
        <span class="s1">solver=solver</span><span class="s3">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
        <span class="s1">tol=tol</span><span class="s3">,</span>
        <span class="s1">verbose=verbose</span><span class="s3">,</span>
        <span class="s1">positive=positive</span><span class="s3">,</span>
        <span class="s1">random_state=random_state</span><span class="s3">,</span>
        <span class="s1">return_n_iter=return_n_iter</span><span class="s3">,</span>
        <span class="s1">return_intercept=return_intercept</span><span class="s3">,</span>
        <span class="s1">X_scale=</span><span class="s3">None,</span>
        <span class="s1">X_offset=</span><span class="s3">None,</span>
        <span class="s1">check_input=check_input</span><span class="s3">,</span>
    <span class="s1">)</span>


<span class="s3">def </span><span class="s1">_ridge_regression(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">y</span><span class="s3">,</span>
    <span class="s1">alpha</span><span class="s3">,</span>
    <span class="s1">sample_weight=</span><span class="s3">None,</span>
    <span class="s1">solver=</span><span class="s5">&quot;auto&quot;</span><span class="s3">,</span>
    <span class="s1">max_iter=</span><span class="s3">None,</span>
    <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
    <span class="s1">verbose=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">positive=</span><span class="s3">False,</span>
    <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">return_n_iter=</span><span class="s3">False,</span>
    <span class="s1">return_intercept=</span><span class="s3">False,</span>
    <span class="s1">X_scale=</span><span class="s3">None,</span>
    <span class="s1">X_offset=</span><span class="s3">None,</span>
    <span class="s1">check_input=</span><span class="s3">True,</span>
    <span class="s1">fit_intercept=</span><span class="s3">False,</span>
<span class="s1">):</span>
    <span class="s1">has_sw = sample_weight </span><span class="s3">is not None</span>

    <span class="s3">if </span><span class="s1">solver == </span><span class="s5">&quot;auto&quot;</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">positive:</span>
            <span class="s1">solver = </span><span class="s5">&quot;lbfgs&quot;</span>
        <span class="s3">elif </span><span class="s1">return_intercept:</span>
            <span class="s2"># sag supports fitting intercept directly</span>
            <span class="s1">solver = </span><span class="s5">&quot;sag&quot;</span>
        <span class="s3">elif not </span><span class="s1">sparse.issparse(X):</span>
            <span class="s1">solver = </span><span class="s5">&quot;cholesky&quot;</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">solver = </span><span class="s5">&quot;sparse_cg&quot;</span>

    <span class="s3">if </span><span class="s1">solver </span><span class="s3">not in </span><span class="s1">(</span><span class="s5">&quot;sparse_cg&quot;</span><span class="s3">, </span><span class="s5">&quot;cholesky&quot;</span><span class="s3">, </span><span class="s5">&quot;svd&quot;</span><span class="s3">, </span><span class="s5">&quot;lsqr&quot;</span><span class="s3">, </span><span class="s5">&quot;sag&quot;</span><span class="s3">, </span><span class="s5">&quot;saga&quot;</span><span class="s3">, </span><span class="s5">&quot;lbfgs&quot;</span><span class="s1">):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s5">&quot;Known solvers are 'sparse_cg', 'cholesky', 'svd'&quot;</span>
            <span class="s5">&quot; 'lsqr', 'sag', 'saga' or 'lbfgs'. Got %s.&quot; </span><span class="s1">% solver</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">positive </span><span class="s3">and </span><span class="s1">solver != </span><span class="s5">&quot;lbfgs&quot;</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s5">&quot;When positive=True, only 'lbfgs' solver can be used. &quot;</span>
            <span class="s5">f&quot;Please change solver </span><span class="s3">{</span><span class="s1">solver</span><span class="s3">} </span><span class="s5">to 'lbfgs' &quot;</span>
            <span class="s5">&quot;or set positive=False.&quot;</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">solver == </span><span class="s5">&quot;lbfgs&quot; </span><span class="s3">and not </span><span class="s1">positive:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s5">&quot;'lbfgs' solver can be used only when positive=True. &quot;</span>
            <span class="s5">&quot;Please use another solver.&quot;</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">return_intercept </span><span class="s3">and </span><span class="s1">solver != </span><span class="s5">&quot;sag&quot;</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s5">&quot;In Ridge, only 'sag' solver can directly fit the &quot;</span>
            <span class="s5">&quot;intercept. Please change solver to 'sag' or set &quot;</span>
            <span class="s5">&quot;return_intercept=False.&quot;</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">check_input:</span>
        <span class="s1">_dtype = [np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span>
        <span class="s1">_accept_sparse = _get_valid_accept_sparse(sparse.issparse(X)</span><span class="s3">, </span><span class="s1">solver)</span>
        <span class="s1">X = check_array(X</span><span class="s3">, </span><span class="s1">accept_sparse=_accept_sparse</span><span class="s3">, </span><span class="s1">dtype=_dtype</span><span class="s3">, </span><span class="s1">order=</span><span class="s5">&quot;C&quot;</span><span class="s1">)</span>
        <span class="s1">y = check_array(y</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span><span class="s3">, </span><span class="s1">ensure_2d=</span><span class="s3">False, </span><span class="s1">order=</span><span class="s3">None</span><span class="s1">)</span>
    <span class="s1">check_consistent_length(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>

    <span class="s3">if </span><span class="s1">y.ndim &gt; </span><span class="s4">2</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Target y has the wrong shape %s&quot; </span><span class="s1">% str(y.shape))</span>

    <span class="s1">ravel = </span><span class="s3">False</span>
    <span class="s3">if </span><span class="s1">y.ndim == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">y = y.reshape(-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">ravel = </span><span class="s3">True</span>

    <span class="s1">n_samples_</span><span class="s3">, </span><span class="s1">n_targets = y.shape</span>

    <span class="s3">if </span><span class="s1">n_samples != n_samples_:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s5">&quot;Number of samples in X and y does not correspond: %d != %d&quot;</span>
            <span class="s1">% (n_samples</span><span class="s3">, </span><span class="s1">n_samples_)</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">has_sw:</span>
        <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s3">if </span><span class="s1">solver </span><span class="s3">not in </span><span class="s1">[</span><span class="s5">&quot;sag&quot;</span><span class="s3">, </span><span class="s5">&quot;saga&quot;</span><span class="s1">]:</span>
            <span class="s2"># SAG supports sample_weight directly. For other solvers,</span>
            <span class="s2"># we implement sample_weight via a simple rescaling.</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight_sqrt = _rescale_data(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight)</span>

    <span class="s2"># Some callers of this method might pass alpha as single</span>
    <span class="s2"># element array which already has been validated.</span>
    <span class="s3">if </span><span class="s1">alpha </span><span class="s3">is not None and not </span><span class="s1">isinstance(alpha</span><span class="s3">, </span><span class="s1">np.ndarray):</span>
        <span class="s1">alpha = check_scalar(</span>
            <span class="s1">alpha</span><span class="s3">,</span>
            <span class="s5">&quot;alpha&quot;</span><span class="s3">,</span>
            <span class="s1">target_type=numbers.Real</span><span class="s3">,</span>
            <span class="s1">min_val=</span><span class="s4">0.0</span><span class="s3">,</span>
            <span class="s1">include_boundaries=</span><span class="s5">&quot;left&quot;</span><span class="s3">,</span>
        <span class="s1">)</span>

    <span class="s2"># There should be either 1 or n_targets penalties</span>
    <span class="s1">alpha = np.asarray(alpha</span><span class="s3">, </span><span class="s1">dtype=X.dtype).ravel()</span>
    <span class="s3">if </span><span class="s1">alpha.size </span><span class="s3">not in </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s1">n_targets]:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s5">&quot;Number of targets and number of penalties do not correspond: %d != %d&quot;</span>
            <span class="s1">% (alpha.size</span><span class="s3">, </span><span class="s1">n_targets)</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">alpha.size == </span><span class="s4">1 </span><span class="s3">and </span><span class="s1">n_targets &gt; </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">alpha = np.repeat(alpha</span><span class="s3">, </span><span class="s1">n_targets)</span>

    <span class="s1">n_iter = </span><span class="s3">None</span>
    <span class="s3">if </span><span class="s1">solver == </span><span class="s5">&quot;sparse_cg&quot;</span><span class="s1">:</span>
        <span class="s1">coef = _solve_sparse_cg(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">alpha</span><span class="s3">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
            <span class="s1">tol=tol</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
            <span class="s1">X_offset=X_offset</span><span class="s3">,</span>
            <span class="s1">X_scale=X_scale</span><span class="s3">,</span>
            <span class="s1">sample_weight_sqrt=sample_weight_sqrt </span><span class="s3">if </span><span class="s1">has_sw </span><span class="s3">else None,</span>
        <span class="s1">)</span>

    <span class="s3">elif </span><span class="s1">solver == </span><span class="s5">&quot;lsqr&quot;</span><span class="s1">:</span>
        <span class="s1">coef</span><span class="s3">, </span><span class="s1">n_iter = _solve_lsqr(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">alpha=alpha</span><span class="s3">,</span>
            <span class="s1">fit_intercept=fit_intercept</span><span class="s3">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
            <span class="s1">tol=tol</span><span class="s3">,</span>
            <span class="s1">X_offset=X_offset</span><span class="s3">,</span>
            <span class="s1">X_scale=X_scale</span><span class="s3">,</span>
            <span class="s1">sample_weight_sqrt=sample_weight_sqrt </span><span class="s3">if </span><span class="s1">has_sw </span><span class="s3">else None,</span>
        <span class="s1">)</span>

    <span class="s3">elif </span><span class="s1">solver == </span><span class="s5">&quot;cholesky&quot;</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">n_features &gt; n_samples:</span>
            <span class="s1">K = safe_sparse_dot(X</span><span class="s3">, </span><span class="s1">X.T</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">)</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s1">dual_coef = _solve_cholesky_kernel(K</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">alpha)</span>

                <span class="s1">coef = safe_sparse_dot(X.T</span><span class="s3">, </span><span class="s1">dual_coef</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">).T</span>
            <span class="s3">except </span><span class="s1">linalg.LinAlgError:</span>
                <span class="s2"># use SVD solver if matrix is singular</span>
                <span class="s1">solver = </span><span class="s5">&quot;svd&quot;</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s1">coef = _solve_cholesky(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">alpha)</span>
            <span class="s3">except </span><span class="s1">linalg.LinAlgError:</span>
                <span class="s2"># use SVD solver if matrix is singular</span>
                <span class="s1">solver = </span><span class="s5">&quot;svd&quot;</span>

    <span class="s3">elif </span><span class="s1">solver </span><span class="s3">in </span><span class="s1">[</span><span class="s5">&quot;sag&quot;</span><span class="s3">, </span><span class="s5">&quot;saga&quot;</span><span class="s1">]:</span>
        <span class="s2"># precompute max_squared_sum for all targets</span>
        <span class="s1">max_squared_sum = row_norms(X</span><span class="s3">, </span><span class="s1">squared=</span><span class="s3">True</span><span class="s1">).max()</span>

        <span class="s1">coef = np.empty((y.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">n_features)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s1">n_iter = np.empty(y.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.int32)</span>
        <span class="s1">intercept = np.zeros((y.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">(alpha_i</span><span class="s3">, </span><span class="s1">target) </span><span class="s3">in </span><span class="s1">enumerate(zip(alpha</span><span class="s3">, </span><span class="s1">y.T)):</span>
            <span class="s1">init = {</span>
                <span class="s5">&quot;coef&quot;</span><span class="s1">: np.zeros((n_features + int(return_intercept)</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
            <span class="s1">}</span>
            <span class="s1">coef_</span><span class="s3">, </span><span class="s1">n_iter_</span><span class="s3">, </span><span class="s1">_ = sag_solver(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">target.ravel()</span><span class="s3">,</span>
                <span class="s1">sample_weight</span><span class="s3">,</span>
                <span class="s5">&quot;squared&quot;</span><span class="s3">,</span>
                <span class="s1">alpha_i</span><span class="s3">,</span>
                <span class="s4">0</span><span class="s3">,</span>
                <span class="s1">max_iter</span><span class="s3">,</span>
                <span class="s1">tol</span><span class="s3">,</span>
                <span class="s1">verbose</span><span class="s3">,</span>
                <span class="s1">random_state</span><span class="s3">,</span>
                <span class="s3">False,</span>
                <span class="s1">max_squared_sum</span><span class="s3">,</span>
                <span class="s1">init</span><span class="s3">,</span>
                <span class="s1">is_saga=solver == </span><span class="s5">&quot;saga&quot;</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">if </span><span class="s1">return_intercept:</span>
                <span class="s1">coef[i] = coef_[:-</span><span class="s4">1</span><span class="s1">]</span>
                <span class="s1">intercept[i] = coef_[-</span><span class="s4">1</span><span class="s1">]</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">coef[i] = coef_</span>
            <span class="s1">n_iter[i] = n_iter_</span>

        <span class="s3">if </span><span class="s1">intercept.shape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">intercept = intercept[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">coef = np.asarray(coef)</span>

    <span class="s3">elif </span><span class="s1">solver == </span><span class="s5">&quot;lbfgs&quot;</span><span class="s1">:</span>
        <span class="s1">coef = _solve_lbfgs(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">alpha</span><span class="s3">,</span>
            <span class="s1">positive=positive</span><span class="s3">,</span>
            <span class="s1">tol=tol</span><span class="s3">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
            <span class="s1">X_offset=X_offset</span><span class="s3">,</span>
            <span class="s1">X_scale=X_scale</span><span class="s3">,</span>
            <span class="s1">sample_weight_sqrt=sample_weight_sqrt </span><span class="s3">if </span><span class="s1">has_sw </span><span class="s3">else None,</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">solver == </span><span class="s5">&quot;svd&quot;</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">sparse.issparse(X):</span>
            <span class="s3">raise </span><span class="s1">TypeError(</span><span class="s5">&quot;SVD solver does not support sparse inputs currently&quot;</span><span class="s1">)</span>
        <span class="s1">coef = _solve_svd(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">alpha)</span>

    <span class="s3">if </span><span class="s1">ravel:</span>
        <span class="s2"># When y was passed as a 1d-array, we flatten the coefficients.</span>
        <span class="s1">coef = coef.ravel()</span>

    <span class="s3">if </span><span class="s1">return_n_iter </span><span class="s3">and </span><span class="s1">return_intercept:</span>
        <span class="s3">return </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">n_iter</span><span class="s3">, </span><span class="s1">intercept</span>
    <span class="s3">elif </span><span class="s1">return_intercept:</span>
        <span class="s3">return </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">intercept</span>
    <span class="s3">elif </span><span class="s1">return_n_iter:</span>
        <span class="s3">return </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">n_iter</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">coef</span>


<span class="s3">class </span><span class="s1">_BaseRidge(LinearModel</span><span class="s3">, </span><span class="s1">metaclass=ABCMeta):</span>
    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s5">&quot;alpha&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.ndarray]</span><span class="s3">,</span>
        <span class="s5">&quot;fit_intercept&quot;</span><span class="s1">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;copy_X&quot;</span><span class="s1">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s5">&quot;solver&quot;</span><span class="s1">: [</span>
            <span class="s1">StrOptions(</span>
                <span class="s1">{</span><span class="s5">&quot;auto&quot;</span><span class="s3">, </span><span class="s5">&quot;svd&quot;</span><span class="s3">, </span><span class="s5">&quot;cholesky&quot;</span><span class="s3">, </span><span class="s5">&quot;lsqr&quot;</span><span class="s3">, </span><span class="s5">&quot;sparse_cg&quot;</span><span class="s3">, </span><span class="s5">&quot;sag&quot;</span><span class="s3">, </span><span class="s5">&quot;saga&quot;</span><span class="s3">, </span><span class="s5">&quot;lbfgs&quot;</span><span class="s1">}</span>
            <span class="s1">)</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;positive&quot;</span><span class="s1">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s5">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s1">@abstractmethod</span>
    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">alpha=</span><span class="s4">1.0</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">fit_intercept=</span><span class="s3">True,</span>
        <span class="s1">copy_X=</span><span class="s3">True,</span>
        <span class="s1">max_iter=</span><span class="s3">None,</span>
        <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
        <span class="s1">solver=</span><span class="s5">&quot;auto&quot;</span><span class="s3">,</span>
        <span class="s1">positive=</span><span class="s3">False,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">):</span>
        <span class="s1">self.alpha = alpha</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.copy_X = copy_X</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.solver = solver</span>
        <span class="s1">self.positive = positive</span>
        <span class="s1">self.random_state = random_state</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s3">if </span><span class="s1">self.solver == </span><span class="s5">&quot;lbfgs&quot; </span><span class="s3">and not </span><span class="s1">self.positive:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s5">&quot;'lbfgs' solver can be used only when positive=True. &quot;</span>
                <span class="s5">&quot;Please use another solver.&quot;</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.positive:</span>
            <span class="s3">if </span><span class="s1">self.solver </span><span class="s3">not in </span><span class="s1">[</span><span class="s5">&quot;auto&quot;</span><span class="s3">, </span><span class="s5">&quot;lbfgs&quot;</span><span class="s1">]:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s5">f&quot;solver='</span><span class="s3">{</span><span class="s1">self.solver</span><span class="s3">}</span><span class="s5">' does not support positive fitting. Please&quot;</span>
                    <span class="s5">&quot; set the solver to 'auto' or 'lbfgs', or set `positive=False`&quot;</span>
                <span class="s1">)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">solver = self.solver</span>
        <span class="s3">elif </span><span class="s1">sparse.issparse(X) </span><span class="s3">and </span><span class="s1">self.fit_intercept:</span>
            <span class="s3">if </span><span class="s1">self.solver </span><span class="s3">not in </span><span class="s1">[</span><span class="s5">&quot;auto&quot;</span><span class="s3">, </span><span class="s5">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s5">&quot;lsqr&quot;</span><span class="s3">, </span><span class="s5">&quot;sag&quot;</span><span class="s3">, </span><span class="s5">&quot;sparse_cg&quot;</span><span class="s1">]:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s5">&quot;solver='{}' does not support fitting the intercept &quot;</span>
                    <span class="s5">&quot;on sparse data. Please set the solver to 'auto' or &quot;</span>
                    <span class="s5">&quot;'lsqr', 'sparse_cg', 'sag', 'lbfgs' &quot;</span>
                    <span class="s5">&quot;or set `fit_intercept=False`&quot;</span><span class="s1">.format(self.solver)</span>
                <span class="s1">)</span>
            <span class="s3">if </span><span class="s1">self.solver </span><span class="s3">in </span><span class="s1">[</span><span class="s5">&quot;lsqr&quot;</span><span class="s3">, </span><span class="s5">&quot;lbfgs&quot;</span><span class="s1">]:</span>
                <span class="s1">solver = self.solver</span>
            <span class="s3">elif </span><span class="s1">self.solver == </span><span class="s5">&quot;sag&quot; </span><span class="s3">and </span><span class="s1">self.max_iter </span><span class="s3">is None and </span><span class="s1">self.tol &gt; </span><span class="s4">1e-4</span><span class="s1">:</span>
                <span class="s1">warnings.warn(</span>
                    <span class="s5">'&quot;sag&quot; solver requires many iterations to fit '</span>
                    <span class="s5">&quot;an intercept with sparse inputs. Either set the &quot;</span>
                    <span class="s5">'solver to &quot;auto&quot; or &quot;sparse_cg&quot;, or set a low '</span>
                    <span class="s5">'&quot;tol&quot; and a high &quot;max_iter&quot; (especially if inputs are '</span>
                    <span class="s5">&quot;not standardized).&quot;</span>
                <span class="s1">)</span>
                <span class="s1">solver = </span><span class="s5">&quot;sag&quot;</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">solver = </span><span class="s5">&quot;sparse_cg&quot;</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">solver = self.solver</span>

        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s2"># when X is sparse we only remove offset from y</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">X_offset</span><span class="s3">, </span><span class="s1">y_offset</span><span class="s3">, </span><span class="s1">X_scale = _preprocess_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">self.fit_intercept</span><span class="s3">,</span>
            <span class="s1">copy=self.copy_X</span><span class="s3">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">solver == </span><span class="s5">&quot;sag&quot; </span><span class="s3">and </span><span class="s1">sparse.issparse(X) </span><span class="s3">and </span><span class="s1">self.fit_intercept:</span>
            <span class="s1">self.coef_</span><span class="s3">, </span><span class="s1">self.n_iter_</span><span class="s3">, </span><span class="s1">self.intercept_ = _ridge_regression(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">y</span><span class="s3">,</span>
                <span class="s1">alpha=self.alpha</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
                <span class="s1">max_iter=self.max_iter</span><span class="s3">,</span>
                <span class="s1">tol=self.tol</span><span class="s3">,</span>
                <span class="s1">solver=</span><span class="s5">&quot;sag&quot;</span><span class="s3">,</span>
                <span class="s1">positive=self.positive</span><span class="s3">,</span>
                <span class="s1">random_state=self.random_state</span><span class="s3">,</span>
                <span class="s1">return_n_iter=</span><span class="s3">True,</span>
                <span class="s1">return_intercept=</span><span class="s3">True,</span>
                <span class="s1">check_input=</span><span class="s3">False,</span>
            <span class="s1">)</span>
            <span class="s2"># add the offset which was subtracted by _preprocess_data</span>
            <span class="s1">self.intercept_ += y_offset</span>

        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">sparse.issparse(X) </span><span class="s3">and </span><span class="s1">self.fit_intercept:</span>
                <span class="s2"># required to fit intercept with sparse_cg and lbfgs solver</span>
                <span class="s1">params = {</span><span class="s5">&quot;X_offset&quot;</span><span class="s1">: X_offset</span><span class="s3">, </span><span class="s5">&quot;X_scale&quot;</span><span class="s1">: X_scale}</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s2"># for dense matrices or when intercept is set to 0</span>
                <span class="s1">params = {}</span>

            <span class="s1">self.coef_</span><span class="s3">, </span><span class="s1">self.n_iter_ = _ridge_regression(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">y</span><span class="s3">,</span>
                <span class="s1">alpha=self.alpha</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
                <span class="s1">max_iter=self.max_iter</span><span class="s3">,</span>
                <span class="s1">tol=self.tol</span><span class="s3">,</span>
                <span class="s1">solver=solver</span><span class="s3">,</span>
                <span class="s1">positive=self.positive</span><span class="s3">,</span>
                <span class="s1">random_state=self.random_state</span><span class="s3">,</span>
                <span class="s1">return_n_iter=</span><span class="s3">True,</span>
                <span class="s1">return_intercept=</span><span class="s3">False,</span>
                <span class="s1">check_input=</span><span class="s3">False,</span>
                <span class="s1">fit_intercept=self.fit_intercept</span><span class="s3">,</span>
                <span class="s1">**params</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">self._set_intercept(X_offset</span><span class="s3">, </span><span class="s1">y_offset</span><span class="s3">, </span><span class="s1">X_scale)</span>

        <span class="s3">return </span><span class="s1">self</span>


<span class="s3">class </span><span class="s1">Ridge(MultiOutputMixin</span><span class="s3">, </span><span class="s1">RegressorMixin</span><span class="s3">, </span><span class="s1">_BaseRidge):</span>
    <span class="s0">&quot;&quot;&quot;Linear least squares with l2 regularization. 
 
    Minimizes the objective function:: 
 
    ||y - Xw||^2_2 + alpha * ||w||^2_2 
 
    This model solves a regression model where the loss function is 
    the linear least squares function and regularization is given by 
    the l2-norm. Also known as Ridge Regression or Tikhonov regularization. 
    This estimator has built-in support for multi-variate regression 
    (i.e., when y is a 2d-array of shape (n_samples, n_targets)). 
 
    Read more in the :ref:`User Guide &lt;ridge_regression&gt;`. 
 
    Parameters 
    ---------- 
    alpha : {float, ndarray of shape (n_targets,)}, default=1.0 
        Constant that multiplies the L2 term, controlling regularization 
        strength. `alpha` must be a non-negative float i.e. in `[0, inf)`. 
 
        When `alpha = 0`, the objective is equivalent to ordinary least 
        squares, solved by the :class:`LinearRegression` object. For numerical 
        reasons, using `alpha = 0` with the `Ridge` object is not advised. 
        Instead, you should use the :class:`LinearRegression` object. 
 
        If an array is passed, penalties are assumed to be specific to the 
        targets. Hence they must correspond in number. 
 
    fit_intercept : bool, default=True 
        Whether to fit the intercept for this model. If set 
        to false, no intercept will be used in calculations 
        (i.e. ``X`` and ``y`` are expected to be centered). 
 
    copy_X : bool, default=True 
        If True, X will be copied; else, it may be overwritten. 
 
    max_iter : int, default=None 
        Maximum number of iterations for conjugate gradient solver. 
        For 'sparse_cg' and 'lsqr' solvers, the default value is determined 
        by scipy.sparse.linalg. For 'sag' solver, the default value is 1000. 
        For 'lbfgs' solver, the default value is 15000. 
 
    tol : float, default=1e-4 
        The precision of the solution (`coef_`) is determined by `tol` which 
        specifies a different convergence criterion for each solver: 
 
        - 'svd': `tol` has no impact. 
 
        - 'cholesky': `tol` has no impact. 
 
        - 'sparse_cg': norm of residuals smaller than `tol`. 
 
        - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr, 
          which control the norm of the residual vector in terms of the norms of 
          matrix and coefficients. 
 
        - 'sag' and 'saga': relative change of coef smaller than `tol`. 
 
        - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals| 
          smaller than `tol`. 
 
        .. versionchanged:: 1.2 
           Default value changed from 1e-3 to 1e-4 for consistency with other linear 
           models. 
 
    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', \ 
            'sag', 'saga', 'lbfgs'}, default='auto' 
        Solver to use in the computational routines: 
 
        - 'auto' chooses the solver automatically based on the type of data. 
 
        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge 
          coefficients. It is the most stable solver, in particular more stable 
          for singular matrices than 'cholesky' at the cost of being slower. 
 
        - 'cholesky' uses the standard scipy.linalg.solve function to 
          obtain a closed-form solution. 
 
        - 'sparse_cg' uses the conjugate gradient solver as found in 
          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is 
          more appropriate than 'cholesky' for large-scale data 
          (possibility to set `tol` and `max_iter`). 
 
        - 'lsqr' uses the dedicated regularized least-squares routine 
          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative 
          procedure. 
 
        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses 
          its improved, unbiased version named SAGA. Both methods also use an 
          iterative procedure, and are often faster than other solvers when 
          both n_samples and n_features are large. Note that 'sag' and 
          'saga' fast convergence is only guaranteed on features with 
          approximately the same scale. You can preprocess the data with a 
          scaler from sklearn.preprocessing. 
 
        - 'lbfgs' uses L-BFGS-B algorithm implemented in 
          `scipy.optimize.minimize`. It can be used only when `positive` 
          is True. 
 
        All solvers except 'svd' support both dense and sparse data. However, only 
        'lsqr', 'sag', 'sparse_cg', and 'lbfgs' support sparse input when 
        `fit_intercept` is True. 
 
        .. versionadded:: 0.17 
           Stochastic Average Gradient descent solver. 
        .. versionadded:: 0.19 
           SAGA solver. 
 
    positive : bool, default=False 
        When set to ``True``, forces the coefficients to be positive. 
        Only 'lbfgs' solver is supported in this case. 
 
    random_state : int, RandomState instance, default=None 
        Used when ``solver`` == 'sag' or 'saga' to shuffle the data. 
        See :term:`Glossary &lt;random_state&gt;` for details. 
 
        .. versionadded:: 0.17 
           `random_state` to support Stochastic Average Gradient. 
 
    Attributes 
    ---------- 
    coef_ : ndarray of shape (n_features,) or (n_targets, n_features) 
        Weight vector(s). 
 
    intercept_ : float or ndarray of shape (n_targets,) 
        Independent term in decision function. Set to 0.0 if 
        ``fit_intercept = False``. 
 
    n_iter_ : None or ndarray of shape (n_targets,) 
        Actual number of iterations for each target. Available only for 
        sag and lsqr solvers. Other solvers will return None. 
 
        .. versionadded:: 0.17 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    RidgeClassifier : Ridge classifier. 
    RidgeCV : Ridge regression with built-in cross validation. 
    :class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression 
        combines ridge regression with the kernel trick. 
 
    Notes 
    ----- 
    Regularization improves the conditioning of the problem and 
    reduces the variance of the estimates. Larger values specify stronger 
    regularization. Alpha corresponds to ``1 / (2C)`` in other linear 
    models such as :class:`~sklearn.linear_model.LogisticRegression` or 
    :class:`~sklearn.svm.LinearSVC`. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.linear_model import Ridge 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; n_samples, n_features = 10, 5 
    &gt;&gt;&gt; rng = np.random.RandomState(0) 
    &gt;&gt;&gt; y = rng.randn(n_samples) 
    &gt;&gt;&gt; X = rng.randn(n_samples, n_features) 
    &gt;&gt;&gt; clf = Ridge(alpha=1.0) 
    &gt;&gt;&gt; clf.fit(X, y) 
    Ridge() 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">alpha=</span><span class="s4">1.0</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">fit_intercept=</span><span class="s3">True,</span>
        <span class="s1">copy_X=</span><span class="s3">True,</span>
        <span class="s1">max_iter=</span><span class="s3">None,</span>
        <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
        <span class="s1">solver=</span><span class="s5">&quot;auto&quot;</span><span class="s3">,</span>
        <span class="s1">positive=</span><span class="s3">False,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">alpha=alpha</span><span class="s3">,</span>
            <span class="s1">fit_intercept=fit_intercept</span><span class="s3">,</span>
            <span class="s1">copy_X=copy_X</span><span class="s3">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
            <span class="s1">tol=tol</span><span class="s3">,</span>
            <span class="s1">solver=solver</span><span class="s3">,</span>
            <span class="s1">positive=positive</span><span class="s3">,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
        <span class="s1">)</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit Ridge regression model. 
 
        Parameters 
        ---------- 
        X : {ndarray, sparse matrix} of shape (n_samples, n_features) 
            Training data. 
 
        y : ndarray of shape (n_samples,) or (n_samples, n_targets) 
            Target values. 
 
        sample_weight : float or ndarray of shape (n_samples,), default=None 
            Individual weights for each sample. If given a float, every sample 
            will have the same weight. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">_accept_sparse = _get_valid_accept_sparse(sparse.issparse(X)</span><span class="s3">, </span><span class="s1">self.solver)</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">accept_sparse=_accept_sparse</span><span class="s3">,</span>
            <span class="s1">dtype=[np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span><span class="s3">,</span>
            <span class="s1">multi_output=</span><span class="s3">True,</span>
            <span class="s1">y_numeric=</span><span class="s3">True,</span>
        <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">super().fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>


<span class="s3">class </span><span class="s1">_RidgeClassifierMixin(LinearClassifierMixin):</span>
    <span class="s3">def </span><span class="s1">_prepare_data(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">solver):</span>
        <span class="s0">&quot;&quot;&quot;Validate `X` and `y` and binarize `y`. 
 
        Parameters 
        ---------- 
        X : {ndarray, sparse matrix} of shape (n_samples, n_features) 
            Training data. 
 
        y : ndarray of shape (n_samples,) 
            Target values. 
 
        sample_weight : float or ndarray of shape (n_samples,), default=None 
            Individual weights for each sample. If given a float, every sample 
            will have the same weight. 
 
        solver : str 
            The solver used in `Ridge` to know which sparse format to support. 
 
        Returns 
        ------- 
        X : {ndarray, sparse matrix} of shape (n_samples, n_features) 
            Validated training data. 
 
        y : ndarray of shape (n_samples,) 
            Validated target values. 
 
        sample_weight : ndarray of shape (n_samples,) 
            Validated sample weights. 
 
        Y : ndarray of shape (n_samples, n_classes) 
            The binarized version of `y`. 
        &quot;&quot;&quot;</span>
        <span class="s1">accept_sparse = _get_valid_accept_sparse(sparse.issparse(X)</span><span class="s3">, </span><span class="s1">solver)</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">accept_sparse=accept_sparse</span><span class="s3">,</span>
            <span class="s1">multi_output=</span><span class="s3">True,</span>
            <span class="s1">y_numeric=</span><span class="s3">False,</span>
        <span class="s1">)</span>

        <span class="s1">self._label_binarizer = LabelBinarizer(pos_label=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">neg_label=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">Y = self._label_binarizer.fit_transform(y)</span>
        <span class="s3">if not </span><span class="s1">self._label_binarizer.y_type_.startswith(</span><span class="s5">&quot;multilabel&quot;</span><span class="s1">):</span>
            <span class="s1">y = column_or_1d(y</span><span class="s3">, </span><span class="s1">warn=</span><span class="s3">True</span><span class="s1">)</span>

        <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s3">if </span><span class="s1">self.class_weight:</span>
            <span class="s1">sample_weight = sample_weight * compute_sample_weight(self.class_weight</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s3">return </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">Y</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict class labels for samples in `X`. 
 
        Parameters 
        ---------- 
        X : {array-like, spare matrix} of shape (n_samples, n_features) 
            The data matrix for which we want to predict the targets. 
 
        Returns 
        ------- 
        y_pred : ndarray of shape (n_samples,) or (n_samples, n_outputs) 
            Vector or matrix containing the predictions. In binary and 
            multiclass problems, this is a vector containing `n_samples`. In 
            a multilabel problem, it returns a matrix of shape 
            `(n_samples, n_outputs)`. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self</span><span class="s3">, </span><span class="s1">attributes=[</span><span class="s5">&quot;_label_binarizer&quot;</span><span class="s1">])</span>
        <span class="s3">if </span><span class="s1">self._label_binarizer.y_type_.startswith(</span><span class="s5">&quot;multilabel&quot;</span><span class="s1">):</span>
            <span class="s2"># Threshold such that the negative label is -1 and positive label</span>
            <span class="s2"># is 1 to use the inverse transform of the label binarizer fitted</span>
            <span class="s2"># during fit.</span>
            <span class="s1">scores = </span><span class="s4">2 </span><span class="s1">* (self.decision_function(X) &gt; </span><span class="s4">0</span><span class="s1">) - </span><span class="s4">1</span>
            <span class="s3">return </span><span class="s1">self._label_binarizer.inverse_transform(scores)</span>
        <span class="s3">return </span><span class="s1">super().predict(X)</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">classes_(self):</span>
        <span class="s0">&quot;&quot;&quot;Classes labels.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self._label_binarizer.classes_</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span><span class="s5">&quot;multilabel&quot;</span><span class="s1">: </span><span class="s3">True</span><span class="s1">}</span>


<span class="s3">class </span><span class="s1">RidgeClassifier(_RidgeClassifierMixin</span><span class="s3">, </span><span class="s1">_BaseRidge):</span>
    <span class="s0">&quot;&quot;&quot;Classifier using Ridge regression. 
 
    This classifier first converts the target values into ``{-1, 1}`` and 
    then treats the problem as a regression task (multi-output regression in 
    the multiclass case). 
 
    Read more in the :ref:`User Guide &lt;ridge_regression&gt;`. 
 
    Parameters 
    ---------- 
    alpha : float, default=1.0 
        Regularization strength; must be a positive float. Regularization 
        improves the conditioning of the problem and reduces the variance of 
        the estimates. Larger values specify stronger regularization. 
        Alpha corresponds to ``1 / (2C)`` in other linear models such as 
        :class:`~sklearn.linear_model.LogisticRegression` or 
        :class:`~sklearn.svm.LinearSVC`. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. If set to false, no 
        intercept will be used in calculations (e.g. data is expected to be 
        already centered). 
 
    copy_X : bool, default=True 
        If True, X will be copied; else, it may be overwritten. 
 
    max_iter : int, default=None 
        Maximum number of iterations for conjugate gradient solver. 
        The default value is determined by scipy.sparse.linalg. 
 
    tol : float, default=1e-4 
        The precision of the solution (`coef_`) is determined by `tol` which 
        specifies a different convergence criterion for each solver: 
 
        - 'svd': `tol` has no impact. 
 
        - 'cholesky': `tol` has no impact. 
 
        - 'sparse_cg': norm of residuals smaller than `tol`. 
 
        - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr, 
          which control the norm of the residual vector in terms of the norms of 
          matrix and coefficients. 
 
        - 'sag' and 'saga': relative change of coef smaller than `tol`. 
 
        - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals| 
          smaller than `tol`. 
 
        .. versionchanged:: 1.2 
           Default value changed from 1e-3 to 1e-4 for consistency with other linear 
           models. 
 
    class_weight : dict or 'balanced', default=None 
        Weights associated with classes in the form ``{class_label: weight}``. 
        If not given, all classes are supposed to have weight one. 
 
        The &quot;balanced&quot; mode uses the values of y to automatically adjust 
        weights inversely proportional to class frequencies in the input data 
        as ``n_samples / (n_classes * np.bincount(y))``. 
 
    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', \ 
            'sag', 'saga', 'lbfgs'}, default='auto' 
        Solver to use in the computational routines: 
 
        - 'auto' chooses the solver automatically based on the type of data. 
 
        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge 
          coefficients. It is the most stable solver, in particular more stable 
          for singular matrices than 'cholesky' at the cost of being slower. 
 
        - 'cholesky' uses the standard scipy.linalg.solve function to 
          obtain a closed-form solution. 
 
        - 'sparse_cg' uses the conjugate gradient solver as found in 
          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is 
          more appropriate than 'cholesky' for large-scale data 
          (possibility to set `tol` and `max_iter`). 
 
        - 'lsqr' uses the dedicated regularized least-squares routine 
          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative 
          procedure. 
 
        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses 
          its unbiased and more flexible version named SAGA. Both methods 
          use an iterative procedure, and are often faster than other solvers 
          when both n_samples and n_features are large. Note that 'sag' and 
          'saga' fast convergence is only guaranteed on features with 
          approximately the same scale. You can preprocess the data with a 
          scaler from sklearn.preprocessing. 
 
          .. versionadded:: 0.17 
             Stochastic Average Gradient descent solver. 
          .. versionadded:: 0.19 
             SAGA solver. 
 
        - 'lbfgs' uses L-BFGS-B algorithm implemented in 
          `scipy.optimize.minimize`. It can be used only when `positive` 
          is True. 
 
    positive : bool, default=False 
        When set to ``True``, forces the coefficients to be positive. 
        Only 'lbfgs' solver is supported in this case. 
 
    random_state : int, RandomState instance, default=None 
        Used when ``solver`` == 'sag' or 'saga' to shuffle the data. 
        See :term:`Glossary &lt;random_state&gt;` for details. 
 
    Attributes 
    ---------- 
    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features) 
        Coefficient of the features in the decision function. 
 
        ``coef_`` is of shape (1, n_features) when the given problem is binary. 
 
    intercept_ : float or ndarray of shape (n_targets,) 
        Independent term in decision function. Set to 0.0 if 
        ``fit_intercept = False``. 
 
    n_iter_ : None or ndarray of shape (n_targets,) 
        Actual number of iterations for each target. Available only for 
        sag and lsqr solvers. Other solvers will return None. 
 
    classes_ : ndarray of shape (n_classes,) 
        The classes labels. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    Ridge : Ridge regression. 
    RidgeClassifierCV :  Ridge classifier with built-in cross validation. 
 
    Notes 
    ----- 
    For multi-class classification, n_class classifiers are trained in 
    a one-versus-all approach. Concretely, this is implemented by taking 
    advantage of the multi-variate response support in Ridge. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import load_breast_cancer 
    &gt;&gt;&gt; from sklearn.linear_model import RidgeClassifier 
    &gt;&gt;&gt; X, y = load_breast_cancer(return_X_y=True) 
    &gt;&gt;&gt; clf = RidgeClassifier().fit(X, y) 
    &gt;&gt;&gt; clf.score(X, y) 
    0.9595... 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**_BaseRidge._parameter_constraints</span><span class="s3">,</span>
        <span class="s5">&quot;class_weight&quot;</span><span class="s1">: [dict</span><span class="s3">, </span><span class="s1">StrOptions({</span><span class="s5">&quot;balanced&quot;</span><span class="s1">})</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">alpha=</span><span class="s4">1.0</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">fit_intercept=</span><span class="s3">True,</span>
        <span class="s1">copy_X=</span><span class="s3">True,</span>
        <span class="s1">max_iter=</span><span class="s3">None,</span>
        <span class="s1">tol=</span><span class="s4">1e-4</span><span class="s3">,</span>
        <span class="s1">class_weight=</span><span class="s3">None,</span>
        <span class="s1">solver=</span><span class="s5">&quot;auto&quot;</span><span class="s3">,</span>
        <span class="s1">positive=</span><span class="s3">False,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">alpha=alpha</span><span class="s3">,</span>
            <span class="s1">fit_intercept=fit_intercept</span><span class="s3">,</span>
            <span class="s1">copy_X=copy_X</span><span class="s3">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
            <span class="s1">tol=tol</span><span class="s3">,</span>
            <span class="s1">solver=solver</span><span class="s3">,</span>
            <span class="s1">positive=positive</span><span class="s3">,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">self.class_weight = class_weight</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit Ridge classifier model. 
 
        Parameters 
        ---------- 
        X : {ndarray, sparse matrix} of shape (n_samples, n_features) 
            Training data. 
 
        y : ndarray of shape (n_samples,) 
            Target values. 
 
        sample_weight : float or ndarray of shape (n_samples,), default=None 
            Individual weights for each sample. If given a float, every sample 
            will have the same weight. 
 
            .. versionadded:: 0.17 
               *sample_weight* support to RidgeClassifier. 
 
        Returns 
        ------- 
        self : object 
            Instance of the estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">Y = self._prepare_data(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">self.solver)</span>

        <span class="s1">super().fit(X</span><span class="s3">, </span><span class="s1">Y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>
        <span class="s3">return </span><span class="s1">self</span>


<span class="s3">def </span><span class="s1">_check_gcv_mode(X</span><span class="s3">, </span><span class="s1">gcv_mode):</span>
    <span class="s3">if </span><span class="s1">gcv_mode </span><span class="s3">in </span><span class="s1">[</span><span class="s5">&quot;eigen&quot;</span><span class="s3">, </span><span class="s5">&quot;svd&quot;</span><span class="s1">]:</span>
        <span class="s3">return </span><span class="s1">gcv_mode</span>
    <span class="s2"># if X has more rows than columns, use decomposition of X^T.X,</span>
    <span class="s2"># otherwise X.X^T</span>
    <span class="s3">if </span><span class="s1">X.shape[</span><span class="s4">0</span><span class="s1">] &gt; X.shape[</span><span class="s4">1</span><span class="s1">]:</span>
        <span class="s3">return </span><span class="s5">&quot;svd&quot;</span>
    <span class="s3">return </span><span class="s5">&quot;eigen&quot;</span>


<span class="s3">def </span><span class="s1">_find_smallest_angle(query</span><span class="s3">, </span><span class="s1">vectors):</span>
    <span class="s0">&quot;&quot;&quot;Find the column of vectors that is most aligned with the query. 
 
    Both query and the columns of vectors must have their l2 norm equal to 1. 
 
    Parameters 
    ---------- 
    query : ndarray of shape (n_samples,) 
        Normalized query vector. 
 
    vectors : ndarray of shape (n_samples, n_features) 
        Vectors to which we compare query, as columns. Must be normalized. 
    &quot;&quot;&quot;</span>
    <span class="s1">abs_cosine = np.abs(query.dot(vectors))</span>
    <span class="s1">index = np.argmax(abs_cosine)</span>
    <span class="s3">return </span><span class="s1">index</span>


<span class="s3">class </span><span class="s1">_X_CenterStackOp(sparse.linalg.LinearOperator):</span>
    <span class="s0">&quot;&quot;&quot;Behaves as centered and scaled X with an added intercept column. 
 
    This operator behaves as 
    np.hstack([X - sqrt_sw[:, None] * X_mean, sqrt_sw[:, None]]) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">sqrt_sw):</span>
        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>
        <span class="s1">super().__init__(X.dtype</span><span class="s3">, </span><span class="s1">(n_samples</span><span class="s3">, </span><span class="s1">n_features + </span><span class="s4">1</span><span class="s1">))</span>
        <span class="s1">self.X = X</span>
        <span class="s1">self.X_mean = X_mean</span>
        <span class="s1">self.sqrt_sw = sqrt_sw</span>

    <span class="s3">def </span><span class="s1">_matvec(self</span><span class="s3">, </span><span class="s1">v):</span>
        <span class="s1">v = v.ravel()</span>
        <span class="s3">return </span><span class="s1">(</span>
            <span class="s1">safe_sparse_dot(self.X</span><span class="s3">, </span><span class="s1">v[:-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">)</span>
            <span class="s1">- self.sqrt_sw * self.X_mean.dot(v[:-</span><span class="s4">1</span><span class="s1">])</span>
            <span class="s1">+ v[-</span><span class="s4">1</span><span class="s1">] * self.sqrt_sw</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_matmat(self</span><span class="s3">, </span><span class="s1">v):</span>
        <span class="s3">return </span><span class="s1">(</span>
            <span class="s1">safe_sparse_dot(self.X</span><span class="s3">, </span><span class="s1">v[:-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">)</span>
            <span class="s1">- self.sqrt_sw[:</span><span class="s3">, None</span><span class="s1">] * self.X_mean.dot(v[:-</span><span class="s4">1</span><span class="s1">])</span>
            <span class="s1">+ v[-</span><span class="s4">1</span><span class="s1">] * self.sqrt_sw[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_transpose(self):</span>
        <span class="s3">return </span><span class="s1">_XT_CenterStackOp(self.X</span><span class="s3">, </span><span class="s1">self.X_mean</span><span class="s3">, </span><span class="s1">self.sqrt_sw)</span>


<span class="s3">class </span><span class="s1">_XT_CenterStackOp(sparse.linalg.LinearOperator):</span>
    <span class="s0">&quot;&quot;&quot;Behaves as transposed centered and scaled X with an intercept column. 
 
    This operator behaves as 
    np.hstack([X - sqrt_sw[:, None] * X_mean, sqrt_sw[:, None]]).T 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">sqrt_sw):</span>
        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>
        <span class="s1">super().__init__(X.dtype</span><span class="s3">, </span><span class="s1">(n_features + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">n_samples))</span>
        <span class="s1">self.X = X</span>
        <span class="s1">self.X_mean = X_mean</span>
        <span class="s1">self.sqrt_sw = sqrt_sw</span>

    <span class="s3">def </span><span class="s1">_matvec(self</span><span class="s3">, </span><span class="s1">v):</span>
        <span class="s1">v = v.ravel()</span>
        <span class="s1">n_features = self.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">res = np.empty(n_features</span><span class="s3">, </span><span class="s1">dtype=self.X.dtype)</span>
        <span class="s1">res[:-</span><span class="s4">1</span><span class="s1">] = safe_sparse_dot(self.X.T</span><span class="s3">, </span><span class="s1">v</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">) - (</span>
            <span class="s1">self.X_mean * self.sqrt_sw.dot(v)</span>
        <span class="s1">)</span>
        <span class="s1">res[-</span><span class="s4">1</span><span class="s1">] = np.dot(v</span><span class="s3">, </span><span class="s1">self.sqrt_sw)</span>
        <span class="s3">return </span><span class="s1">res</span>

    <span class="s3">def </span><span class="s1">_matmat(self</span><span class="s3">, </span><span class="s1">v):</span>
        <span class="s1">n_features = self.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">res = np.empty((n_features</span><span class="s3">, </span><span class="s1">v.shape[</span><span class="s4">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">dtype=self.X.dtype)</span>
        <span class="s1">res[:-</span><span class="s4">1</span><span class="s1">] = safe_sparse_dot(self.X.T</span><span class="s3">, </span><span class="s1">v</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">) - self.X_mean[</span>
            <span class="s1">:</span><span class="s3">, None</span>
        <span class="s1">] * self.sqrt_sw.dot(v)</span>
        <span class="s1">res[-</span><span class="s4">1</span><span class="s1">] = np.dot(self.sqrt_sw</span><span class="s3">, </span><span class="s1">v)</span>
        <span class="s3">return </span><span class="s1">res</span>


<span class="s3">class </span><span class="s1">_IdentityRegressor:</span>
    <span class="s0">&quot;&quot;&quot;Fake regressor which will directly output the prediction.&quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">decision_function(self</span><span class="s3">, </span><span class="s1">y_predict):</span>
        <span class="s3">return </span><span class="s1">y_predict</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">y_predict):</span>
        <span class="s3">return </span><span class="s1">y_predict</span>


<span class="s3">class </span><span class="s1">_IdentityClassifier(LinearClassifierMixin):</span>
    <span class="s0">&quot;&quot;&quot;Fake classifier which will directly output the prediction. 
 
    We inherit from LinearClassifierMixin to get the proper shape for the 
    output `y`. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">classes):</span>
        <span class="s1">self.classes_ = classes</span>

    <span class="s3">def </span><span class="s1">decision_function(self</span><span class="s3">, </span><span class="s1">y_predict):</span>
        <span class="s3">return </span><span class="s1">y_predict</span>


<span class="s3">class </span><span class="s1">_RidgeGCV(LinearModel):</span>
    <span class="s0">&quot;&quot;&quot;Ridge regression with built-in Leave-one-out Cross-Validation. 
 
    This class is not intended to be used directly. Use RidgeCV instead. 
 
    Notes 
    ----- 
 
    We want to solve (K + alpha*Id)c = y, 
    where K = X X^T is the kernel matrix. 
 
    Let G = (K + alpha*Id). 
 
    Dual solution: c = G^-1y 
    Primal solution: w = X^T c 
 
    Compute eigendecomposition K = Q V Q^T. 
    Then G^-1 = Q (V + alpha*Id)^-1 Q^T, 
    where (V + alpha*Id) is diagonal. 
    It is thus inexpensive to inverse for many alphas. 
 
    Let loov be the vector of prediction values for each example 
    when the model was fitted with all examples but this example. 
 
    loov = (KG^-1Y - diag(KG^-1)Y) / diag(I-KG^-1) 
 
    Let looe be the vector of prediction errors for each example 
    when the model was fitted with all examples but this example. 
 
    looe = y - loov = c / diag(G^-1) 
 
    The best score (negative mean squared error or user-provided scoring) is 
    stored in the `best_score_` attribute, and the selected hyperparameter in 
    `alpha_`. 
 
    References 
    ---------- 
    http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf 
    https://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">alphas=(</span><span class="s4">0.1</span><span class="s3">, </span><span class="s4">1.0</span><span class="s3">, </span><span class="s4">10.0</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">fit_intercept=</span><span class="s3">True,</span>
        <span class="s1">scoring=</span><span class="s3">None,</span>
        <span class="s1">copy_X=</span><span class="s3">True,</span>
        <span class="s1">gcv_mode=</span><span class="s3">None,</span>
        <span class="s1">store_cv_values=</span><span class="s3">False,</span>
        <span class="s1">is_clf=</span><span class="s3">False,</span>
        <span class="s1">alpha_per_target=</span><span class="s3">False,</span>
    <span class="s1">):</span>
        <span class="s1">self.alphas = alphas</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.scoring = scoring</span>
        <span class="s1">self.copy_X = copy_X</span>
        <span class="s1">self.gcv_mode = gcv_mode</span>
        <span class="s1">self.store_cv_values = store_cv_values</span>
        <span class="s1">self.is_clf = is_clf</span>
        <span class="s1">self.alpha_per_target = alpha_per_target</span>

    <span class="s1">@staticmethod</span>
    <span class="s3">def </span><span class="s1">_decomp_diag(v_prime</span><span class="s3">, </span><span class="s1">Q):</span>
        <span class="s2"># compute diagonal of the matrix: dot(Q, dot(diag(v_prime), Q^T))</span>
        <span class="s3">return </span><span class="s1">(v_prime * Q**</span><span class="s4">2</span><span class="s1">).sum(axis=-</span><span class="s4">1</span><span class="s1">)</span>

    <span class="s1">@staticmethod</span>
    <span class="s3">def </span><span class="s1">_diag_dot(D</span><span class="s3">, </span><span class="s1">B):</span>
        <span class="s2"># compute dot(diag(D), B)</span>
        <span class="s3">if </span><span class="s1">len(B.shape) &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2"># handle case where B is &gt; 1-d</span>
            <span class="s1">D = D[(slice(</span><span class="s3">None</span><span class="s1">)</span><span class="s3">,</span><span class="s1">) + (np.newaxis</span><span class="s3">,</span><span class="s1">) * (len(B.shape) - </span><span class="s4">1</span><span class="s1">)]</span>
        <span class="s3">return </span><span class="s1">D * B</span>

    <span class="s3">def </span><span class="s1">_compute_gram(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">sqrt_sw):</span>
        <span class="s0">&quot;&quot;&quot;Computes the Gram matrix XX^T with possible centering. 
 
        Parameters 
        ---------- 
        X : {ndarray, sparse matrix} of shape (n_samples, n_features) 
            The preprocessed design matrix. 
 
        sqrt_sw : ndarray of shape (n_samples,) 
            square roots of sample weights 
 
        Returns 
        ------- 
        gram : ndarray of shape (n_samples, n_samples) 
            The Gram matrix. 
        X_mean : ndarray of shape (n_feature,) 
            The weighted mean of ``X`` for each feature. 
 
        Notes 
        ----- 
        When X is dense the centering has been done in preprocessing 
        so the mean is 0 and we just compute XX^T. 
 
        When X is sparse it has not been centered in preprocessing, but it has 
        been scaled by sqrt(sample weights). 
 
        When self.fit_intercept is False no centering is done. 
 
        The centered X is never actually computed because centering would break 
        the sparsity of X. 
        &quot;&quot;&quot;</span>
        <span class="s1">center = self.fit_intercept </span><span class="s3">and </span><span class="s1">sparse.issparse(X)</span>
        <span class="s3">if not </span><span class="s1">center:</span>
            <span class="s2"># in this case centering has been done in preprocessing</span>
            <span class="s2"># or we are not fitting an intercept.</span>
            <span class="s1">X_mean = np.zeros(X.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
            <span class="s3">return </span><span class="s1">safe_sparse_dot(X</span><span class="s3">, </span><span class="s1">X.T</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">, </span><span class="s1">X_mean</span>
        <span class="s2"># X is sparse</span>
        <span class="s1">n_samples = X.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">sample_weight_matrix = sparse.dia_matrix(</span>
            <span class="s1">(sqrt_sw</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">shape=(n_samples</span><span class="s3">, </span><span class="s1">n_samples)</span>
        <span class="s1">)</span>
        <span class="s1">X_weighted = sample_weight_matrix.dot(X)</span>
        <span class="s1">X_mean</span><span class="s3">, </span><span class="s1">_ = mean_variance_axis(X_weighted</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">X_mean *= n_samples / sqrt_sw.dot(sqrt_sw)</span>
        <span class="s1">X_mX = sqrt_sw[:</span><span class="s3">, None</span><span class="s1">] * safe_sparse_dot(X_mean</span><span class="s3">, </span><span class="s1">X.T</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">X_mX_m = np.outer(sqrt_sw</span><span class="s3">, </span><span class="s1">sqrt_sw) * np.dot(X_mean</span><span class="s3">, </span><span class="s1">X_mean)</span>
        <span class="s3">return </span><span class="s1">(</span>
            <span class="s1">safe_sparse_dot(X</span><span class="s3">, </span><span class="s1">X.T</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">) + X_mX_m - X_mX - X_mX.T</span><span class="s3">,</span>
            <span class="s1">X_mean</span><span class="s3">,</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_compute_covariance(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">sqrt_sw):</span>
        <span class="s0">&quot;&quot;&quot;Computes covariance matrix X^TX with possible centering. 
 
        Parameters 
        ---------- 
        X : sparse matrix of shape (n_samples, n_features) 
            The preprocessed design matrix. 
 
        sqrt_sw : ndarray of shape (n_samples,) 
            square roots of sample weights 
 
        Returns 
        ------- 
        covariance : ndarray of shape (n_features, n_features) 
            The covariance matrix. 
        X_mean : ndarray of shape (n_feature,) 
            The weighted mean of ``X`` for each feature. 
 
        Notes 
        ----- 
        Since X is sparse it has not been centered in preprocessing, but it has 
        been scaled by sqrt(sample weights). 
 
        When self.fit_intercept is False no centering is done. 
 
        The centered X is never actually computed because centering would break 
        the sparsity of X. 
        &quot;&quot;&quot;</span>
        <span class="s3">if not </span><span class="s1">self.fit_intercept:</span>
            <span class="s2"># in this case centering has been done in preprocessing</span>
            <span class="s2"># or we are not fitting an intercept.</span>
            <span class="s1">X_mean = np.zeros(X.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
            <span class="s3">return </span><span class="s1">safe_sparse_dot(X.T</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">, </span><span class="s1">X_mean</span>
        <span class="s2"># this function only gets called for sparse X</span>
        <span class="s1">n_samples = X.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">sample_weight_matrix = sparse.dia_matrix(</span>
            <span class="s1">(sqrt_sw</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">shape=(n_samples</span><span class="s3">, </span><span class="s1">n_samples)</span>
        <span class="s1">)</span>
        <span class="s1">X_weighted = sample_weight_matrix.dot(X)</span>
        <span class="s1">X_mean</span><span class="s3">, </span><span class="s1">_ = mean_variance_axis(X_weighted</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">X_mean = X_mean * n_samples / sqrt_sw.dot(sqrt_sw)</span>
        <span class="s1">weight_sum = sqrt_sw.dot(sqrt_sw)</span>
        <span class="s3">return </span><span class="s1">(</span>
            <span class="s1">safe_sparse_dot(X.T</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">)</span>
            <span class="s1">- weight_sum * np.outer(X_mean</span><span class="s3">, </span><span class="s1">X_mean)</span><span class="s3">,</span>
            <span class="s1">X_mean</span><span class="s3">,</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_sparse_multidot_diag(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">A</span><span class="s3">, </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">sqrt_sw):</span>
        <span class="s0">&quot;&quot;&quot;Compute the diagonal of (X - X_mean).dot(A).dot((X - X_mean).T) 
        without explicitly centering X nor computing X.dot(A) 
        when X is sparse. 
 
        Parameters 
        ---------- 
        X : sparse matrix of shape (n_samples, n_features) 
 
        A : ndarray of shape (n_features, n_features) 
 
        X_mean : ndarray of shape (n_features,) 
 
        sqrt_sw : ndarray of shape (n_features,) 
            square roots of sample weights 
 
        Returns 
        ------- 
        diag : np.ndarray, shape (n_samples,) 
            The computed diagonal. 
        &quot;&quot;&quot;</span>
        <span class="s1">intercept_col = scale = sqrt_sw</span>
        <span class="s1">batch_size = X.shape[</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">diag = np.empty(X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s3">for </span><span class="s1">start </span><span class="s3">in </span><span class="s1">range(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">batch_size):</span>
            <span class="s1">batch = slice(start</span><span class="s3">, </span><span class="s1">min(X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">start + batch_size)</span><span class="s3">, </span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">X_batch = np.empty(</span>
                <span class="s1">(X[batch].shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s4">1</span><span class="s1">] + self.fit_intercept)</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span>
            <span class="s1">)</span>
            <span class="s3">if </span><span class="s1">self.fit_intercept:</span>
                <span class="s1">X_batch[:</span><span class="s3">, </span><span class="s1">:-</span><span class="s4">1</span><span class="s1">] = X[batch].A - X_mean * scale[batch][:</span><span class="s3">, None</span><span class="s1">]</span>
                <span class="s1">X_batch[:</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">] = intercept_col[batch]</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">X_batch = X[batch].A</span>
            <span class="s1">diag[batch] = (X_batch.dot(A) * X_batch).sum(axis=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">diag</span>

    <span class="s3">def </span><span class="s1">_eigen_decompose_gram(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sqrt_sw):</span>
        <span class="s0">&quot;&quot;&quot;Eigendecomposition of X.X^T, used when n_samples &lt;= n_features.&quot;&quot;&quot;</span>
        <span class="s2"># if X is dense it has already been centered in preprocessing</span>
        <span class="s1">K</span><span class="s3">, </span><span class="s1">X_mean = self._compute_gram(X</span><span class="s3">, </span><span class="s1">sqrt_sw)</span>
        <span class="s3">if </span><span class="s1">self.fit_intercept:</span>
            <span class="s2"># to emulate centering X with sample weights,</span>
            <span class="s2"># ie removing the weighted average, we add a column</span>
            <span class="s2"># containing the square roots of the sample weights.</span>
            <span class="s2"># by centering, it is orthogonal to the other columns</span>
            <span class="s1">K += np.outer(sqrt_sw</span><span class="s3">, </span><span class="s1">sqrt_sw)</span>
        <span class="s1">eigvals</span><span class="s3">, </span><span class="s1">Q = linalg.eigh(K)</span>
        <span class="s1">QT_y = np.dot(Q.T</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s3">return </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">eigvals</span><span class="s3">, </span><span class="s1">Q</span><span class="s3">, </span><span class="s1">QT_y</span>

    <span class="s3">def </span><span class="s1">_solve_eigen_gram(self</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sqrt_sw</span><span class="s3">, </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">eigvals</span><span class="s3">, </span><span class="s1">Q</span><span class="s3">, </span><span class="s1">QT_y):</span>
        <span class="s0">&quot;&quot;&quot;Compute dual coefficients and diagonal of G^-1. 
 
        Used when we have a decomposition of X.X^T (n_samples &lt;= n_features). 
        &quot;&quot;&quot;</span>
        <span class="s1">w = </span><span class="s4">1.0 </span><span class="s1">/ (eigvals + alpha)</span>
        <span class="s3">if </span><span class="s1">self.fit_intercept:</span>
            <span class="s2"># the vector containing the square roots of the sample weights (1</span>
            <span class="s2"># when no sample weights) is the eigenvector of XX^T which</span>
            <span class="s2"># corresponds to the intercept; we cancel the regularization on</span>
            <span class="s2"># this dimension. the corresponding eigenvalue is</span>
            <span class="s2"># sum(sample_weight).</span>
            <span class="s1">normalized_sw = sqrt_sw / np.linalg.norm(sqrt_sw)</span>
            <span class="s1">intercept_dim = _find_smallest_angle(normalized_sw</span><span class="s3">, </span><span class="s1">Q)</span>
            <span class="s1">w[intercept_dim] = </span><span class="s4">0  </span><span class="s2"># cancel regularization for the intercept</span>

        <span class="s1">c = np.dot(Q</span><span class="s3">, </span><span class="s1">self._diag_dot(w</span><span class="s3">, </span><span class="s1">QT_y))</span>
        <span class="s1">G_inverse_diag = self._decomp_diag(w</span><span class="s3">, </span><span class="s1">Q)</span>
        <span class="s2"># handle case where y is 2-d</span>
        <span class="s3">if </span><span class="s1">len(y.shape) != </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">G_inverse_diag = G_inverse_diag[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
        <span class="s3">return </span><span class="s1">G_inverse_diag</span><span class="s3">, </span><span class="s1">c</span>

    <span class="s3">def </span><span class="s1">_eigen_decompose_covariance(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sqrt_sw):</span>
        <span class="s0">&quot;&quot;&quot;Eigendecomposition of X^T.X, used when n_samples &gt; n_features 
        and X is sparse. 
        &quot;&quot;&quot;</span>
        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>
        <span class="s1">cov = np.empty((n_features + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">n_features + </span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s1">cov[:-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">:-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">X_mean = self._compute_covariance(X</span><span class="s3">, </span><span class="s1">sqrt_sw)</span>
        <span class="s3">if not </span><span class="s1">self.fit_intercept:</span>
            <span class="s1">cov = cov[:-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">:-</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s2"># to emulate centering X with sample weights,</span>
        <span class="s2"># ie removing the weighted average, we add a column</span>
        <span class="s2"># containing the square roots of the sample weights.</span>
        <span class="s2"># by centering, it is orthogonal to the other columns</span>
        <span class="s2"># when all samples have the same weight we add a column of 1</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">cov[-</span><span class="s4">1</span><span class="s1">] = </span><span class="s4">0</span>
            <span class="s1">cov[:</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">] = </span><span class="s4">0</span>
            <span class="s1">cov[-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">] = sqrt_sw.dot(sqrt_sw)</span>
        <span class="s1">nullspace_dim = max(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">n_features - n_samples)</span>
        <span class="s1">eigvals</span><span class="s3">, </span><span class="s1">V = linalg.eigh(cov)</span>
        <span class="s2"># remove eigenvalues and vectors in the null space of X^T.X</span>
        <span class="s1">eigvals = eigvals[nullspace_dim:]</span>
        <span class="s1">V = V[:</span><span class="s3">, </span><span class="s1">nullspace_dim:]</span>
        <span class="s3">return </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">eigvals</span><span class="s3">, </span><span class="s1">V</span><span class="s3">, </span><span class="s1">X</span>

    <span class="s3">def </span><span class="s1">_solve_eigen_covariance_no_intercept(</span>
        <span class="s1">self</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sqrt_sw</span><span class="s3">, </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">eigvals</span><span class="s3">, </span><span class="s1">V</span><span class="s3">, </span><span class="s1">X</span>
    <span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Compute dual coefficients and diagonal of G^-1. 
 
        Used when we have a decomposition of X^T.X 
        (n_samples &gt; n_features and X is sparse), and not fitting an intercept. 
        &quot;&quot;&quot;</span>
        <span class="s1">w = </span><span class="s4">1 </span><span class="s1">/ (eigvals + alpha)</span>
        <span class="s1">A = (V * w).dot(V.T)</span>
        <span class="s1">AXy = A.dot(safe_sparse_dot(X.T</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">))</span>
        <span class="s1">y_hat = safe_sparse_dot(X</span><span class="s3">, </span><span class="s1">AXy</span><span class="s3">, </span><span class="s1">dense_output=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">hat_diag = self._sparse_multidot_diag(X</span><span class="s3">, </span><span class="s1">A</span><span class="s3">, </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">sqrt_sw)</span>
        <span class="s3">if </span><span class="s1">len(y.shape) != </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2"># handle case where y is 2-d</span>
            <span class="s1">hat_diag = hat_diag[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
        <span class="s3">return </span><span class="s1">(</span><span class="s4">1 </span><span class="s1">- hat_diag) / alpha</span><span class="s3">, </span><span class="s1">(y - y_hat) / alpha</span>

    <span class="s3">def </span><span class="s1">_solve_eigen_covariance_intercept(</span>
        <span class="s1">self</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sqrt_sw</span><span class="s3">, </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">eigvals</span><span class="s3">, </span><span class="s1">V</span><span class="s3">, </span><span class="s1">X</span>
    <span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Compute dual coefficients and diagonal of G^-1. 
 
        Used when we have a decomposition of X^T.X 
        (n_samples &gt; n_features and X is sparse), 
        and we are fitting an intercept. 
        &quot;&quot;&quot;</span>
        <span class="s2"># the vector [0, 0, ..., 0, 1]</span>
        <span class="s2"># is the eigenvector of X^TX which</span>
        <span class="s2"># corresponds to the intercept; we cancel the regularization on</span>
        <span class="s2"># this dimension. the corresponding eigenvalue is</span>
        <span class="s2"># sum(sample_weight), e.g. n when uniform sample weights.</span>
        <span class="s1">intercept_sv = np.zeros(V.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">intercept_sv[-</span><span class="s4">1</span><span class="s1">] = </span><span class="s4">1</span>
        <span class="s1">intercept_dim = _find_smallest_angle(intercept_sv</span><span class="s3">, </span><span class="s1">V)</span>
        <span class="s1">w = </span><span class="s4">1 </span><span class="s1">/ (eigvals + alpha)</span>
        <span class="s1">w[intercept_dim] = </span><span class="s4">1 </span><span class="s1">/ eigvals[intercept_dim]</span>
        <span class="s1">A = (V * w).dot(V.T)</span>
        <span class="s2"># add a column to X containing the square roots of sample weights</span>
        <span class="s1">X_op = _X_CenterStackOp(X</span><span class="s3">, </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">sqrt_sw)</span>
        <span class="s1">AXy = A.dot(X_op.T.dot(y))</span>
        <span class="s1">y_hat = X_op.dot(AXy)</span>
        <span class="s1">hat_diag = self._sparse_multidot_diag(X</span><span class="s3">, </span><span class="s1">A</span><span class="s3">, </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">sqrt_sw)</span>
        <span class="s2"># return (1 - hat_diag), (y - y_hat)</span>
        <span class="s3">if </span><span class="s1">len(y.shape) != </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2"># handle case where y is 2-d</span>
            <span class="s1">hat_diag = hat_diag[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
        <span class="s3">return </span><span class="s1">(</span><span class="s4">1 </span><span class="s1">- hat_diag) / alpha</span><span class="s3">, </span><span class="s1">(y - y_hat) / alpha</span>

    <span class="s3">def </span><span class="s1">_solve_eigen_covariance(self</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sqrt_sw</span><span class="s3">, </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">eigvals</span><span class="s3">, </span><span class="s1">V</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Compute dual coefficients and diagonal of G^-1. 
 
        Used when we have a decomposition of X^T.X 
        (n_samples &gt; n_features and X is sparse). 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self.fit_intercept:</span>
            <span class="s3">return </span><span class="s1">self._solve_eigen_covariance_intercept(</span>
                <span class="s1">alpha</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sqrt_sw</span><span class="s3">, </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">eigvals</span><span class="s3">, </span><span class="s1">V</span><span class="s3">, </span><span class="s1">X</span>
            <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">self._solve_eigen_covariance_no_intercept(</span>
            <span class="s1">alpha</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sqrt_sw</span><span class="s3">, </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">eigvals</span><span class="s3">, </span><span class="s1">V</span><span class="s3">, </span><span class="s1">X</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_svd_decompose_design_matrix(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sqrt_sw):</span>
        <span class="s2"># X already centered</span>
        <span class="s1">X_mean = np.zeros(X.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s3">if </span><span class="s1">self.fit_intercept:</span>
            <span class="s2"># to emulate fit_intercept=True situation, add a column</span>
            <span class="s2"># containing the square roots of the sample weights</span>
            <span class="s2"># by centering, the other columns are orthogonal to that one</span>
            <span class="s1">intercept_column = sqrt_sw[:</span><span class="s3">, None</span><span class="s1">]</span>
            <span class="s1">X = np.hstack((X</span><span class="s3">, </span><span class="s1">intercept_column))</span>
        <span class="s1">U</span><span class="s3">, </span><span class="s1">singvals</span><span class="s3">, </span><span class="s1">_ = linalg.svd(X</span><span class="s3">, </span><span class="s1">full_matrices=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">singvals_sq = singvals**</span><span class="s4">2</span>
        <span class="s1">UT_y = np.dot(U.T</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s3">return </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">singvals_sq</span><span class="s3">, </span><span class="s1">U</span><span class="s3">, </span><span class="s1">UT_y</span>

    <span class="s3">def </span><span class="s1">_solve_svd_design_matrix(self</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sqrt_sw</span><span class="s3">, </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">singvals_sq</span><span class="s3">, </span><span class="s1">U</span><span class="s3">, </span><span class="s1">UT_y):</span>
        <span class="s0">&quot;&quot;&quot;Compute dual coefficients and diagonal of G^-1. 
 
        Used when we have an SVD decomposition of X 
        (n_samples &gt; n_features and X is dense). 
        &quot;&quot;&quot;</span>
        <span class="s1">w = ((singvals_sq + alpha) ** -</span><span class="s4">1</span><span class="s1">) - (alpha**-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">self.fit_intercept:</span>
            <span class="s2"># detect intercept column</span>
            <span class="s1">normalized_sw = sqrt_sw / np.linalg.norm(sqrt_sw)</span>
            <span class="s1">intercept_dim = _find_smallest_angle(normalized_sw</span><span class="s3">, </span><span class="s1">U)</span>
            <span class="s2"># cancel the regularization for the intercept</span>
            <span class="s1">w[intercept_dim] = -(alpha**-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">c = np.dot(U</span><span class="s3">, </span><span class="s1">self._diag_dot(w</span><span class="s3">, </span><span class="s1">UT_y)) + (alpha**-</span><span class="s4">1</span><span class="s1">) * y</span>
        <span class="s1">G_inverse_diag = self._decomp_diag(w</span><span class="s3">, </span><span class="s1">U) + (alpha**-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">len(y.shape) != </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2"># handle case where y is 2-d</span>
            <span class="s1">G_inverse_diag = G_inverse_diag[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
        <span class="s3">return </span><span class="s1">G_inverse_diag</span><span class="s3">, </span><span class="s1">c</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit Ridge regression model with gcv. 
 
        Parameters 
        ---------- 
        X : {ndarray, sparse matrix} of shape (n_samples, n_features) 
            Training data. Will be cast to float64 if necessary. 
 
        y : ndarray of shape (n_samples,) or (n_samples, n_targets) 
            Target values. Will be cast to float64 if necessary. 
 
        sample_weight : float or ndarray of shape (n_samples,), default=None 
            Individual weights for each sample. If given a float, every sample 
            will have the same weight. 
 
        Returns 
        ------- 
        self : object 
        &quot;&quot;&quot;</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">accept_sparse=[</span><span class="s5">&quot;csr&quot;</span><span class="s3">, </span><span class="s5">&quot;csc&quot;</span><span class="s3">, </span><span class="s5">&quot;coo&quot;</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">dtype=[np.float64]</span><span class="s3">,</span>
            <span class="s1">multi_output=</span><span class="s3">True,</span>
            <span class="s1">y_numeric=</span><span class="s3">True,</span>
        <span class="s1">)</span>

        <span class="s2"># alpha_per_target cannot be used in classifier mode. All subclasses</span>
        <span class="s2"># of _RidgeGCV that are classifiers keep alpha_per_target at its</span>
        <span class="s2"># default value: False, so the condition below should never happen.</span>
        <span class="s3">assert not </span><span class="s1">(self.is_clf </span><span class="s3">and </span><span class="s1">self.alpha_per_target)</span>

        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s1">self.alphas = np.asarray(self.alphas)</span>

        <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">X_offset</span><span class="s3">, </span><span class="s1">y_offset</span><span class="s3">, </span><span class="s1">X_scale = _preprocess_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">self.fit_intercept</span><span class="s3">,</span>
            <span class="s1">copy=self.copy_X</span><span class="s3">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s1">gcv_mode = _check_gcv_mode(X</span><span class="s3">, </span><span class="s1">self.gcv_mode)</span>

        <span class="s3">if </span><span class="s1">gcv_mode == </span><span class="s5">&quot;eigen&quot;</span><span class="s1">:</span>
            <span class="s1">decompose = self._eigen_decompose_gram</span>
            <span class="s1">solve = self._solve_eigen_gram</span>
        <span class="s3">elif </span><span class="s1">gcv_mode == </span><span class="s5">&quot;svd&quot;</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">sparse.issparse(X):</span>
                <span class="s1">decompose = self._eigen_decompose_covariance</span>
                <span class="s1">solve = self._solve_eigen_covariance</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">decompose = self._svd_decompose_design_matrix</span>
                <span class="s1">solve = self._solve_svd_design_matrix</span>

        <span class="s1">n_samples = X.shape[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sqrt_sw = _rescale_data(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">sqrt_sw = np.ones(n_samples</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s1">X_mean</span><span class="s3">, </span><span class="s1">*decomposition = decompose(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sqrt_sw)</span>

        <span class="s1">scorer = check_scoring(self</span><span class="s3">, </span><span class="s1">scoring=self.scoring</span><span class="s3">, </span><span class="s1">allow_none=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">error = scorer </span><span class="s3">is None</span>

        <span class="s1">n_y = </span><span class="s4">1 </span><span class="s3">if </span><span class="s1">len(y.shape) == </span><span class="s4">1 </span><span class="s3">else </span><span class="s1">y.shape[</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">n_alphas = </span><span class="s4">1 </span><span class="s3">if </span><span class="s1">np.ndim(self.alphas) == </span><span class="s4">0 </span><span class="s3">else </span><span class="s1">len(self.alphas)</span>

        <span class="s3">if </span><span class="s1">self.store_cv_values:</span>
            <span class="s1">self.cv_values_ = np.empty((n_samples * n_y</span><span class="s3">, </span><span class="s1">n_alphas)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

        <span class="s1">best_coef</span><span class="s3">, </span><span class="s1">best_score</span><span class="s3">, </span><span class="s1">best_alpha = </span><span class="s3">None, None, None</span>

        <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">alpha </span><span class="s3">in </span><span class="s1">enumerate(np.atleast_1d(self.alphas)):</span>
            <span class="s1">G_inverse_diag</span><span class="s3">, </span><span class="s1">c = solve(float(alpha)</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sqrt_sw</span><span class="s3">, </span><span class="s1">X_mean</span><span class="s3">, </span><span class="s1">*decomposition)</span>
            <span class="s3">if </span><span class="s1">error:</span>
                <span class="s1">squared_errors = (c / G_inverse_diag) ** </span><span class="s4">2</span>
                <span class="s3">if </span><span class="s1">self.alpha_per_target:</span>
                    <span class="s1">alpha_score = -squared_errors.mean(axis=</span><span class="s4">0</span><span class="s1">)</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">alpha_score = -squared_errors.mean()</span>
                <span class="s3">if </span><span class="s1">self.store_cv_values:</span>
                    <span class="s1">self.cv_values_[:</span><span class="s3">, </span><span class="s1">i] = squared_errors.ravel()</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">predictions = y - (c / G_inverse_diag)</span>
                <span class="s3">if </span><span class="s1">self.store_cv_values:</span>
                    <span class="s1">self.cv_values_[:</span><span class="s3">, </span><span class="s1">i] = predictions.ravel()</span>

                <span class="s3">if </span><span class="s1">self.is_clf:</span>
                    <span class="s1">identity_estimator = _IdentityClassifier(classes=np.arange(n_y))</span>
                    <span class="s1">alpha_score = scorer(</span>
                        <span class="s1">identity_estimator</span><span class="s3">, </span><span class="s1">predictions</span><span class="s3">, </span><span class="s1">y.argmax(axis=</span><span class="s4">1</span><span class="s1">)</span>
                    <span class="s1">)</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">identity_estimator = _IdentityRegressor()</span>
                    <span class="s3">if </span><span class="s1">self.alpha_per_target:</span>
                        <span class="s1">alpha_score = np.array(</span>
                            <span class="s1">[</span>
                                <span class="s1">scorer(identity_estimator</span><span class="s3">, </span><span class="s1">predictions[:</span><span class="s3">, </span><span class="s1">j]</span><span class="s3">, </span><span class="s1">y[:</span><span class="s3">, </span><span class="s1">j])</span>
                                <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range(n_y)</span>
                            <span class="s1">]</span>
                        <span class="s1">)</span>
                    <span class="s3">else</span><span class="s1">:</span>
                        <span class="s1">alpha_score = scorer(</span>
                            <span class="s1">identity_estimator</span><span class="s3">, </span><span class="s1">predictions.ravel()</span><span class="s3">, </span><span class="s1">y.ravel()</span>
                        <span class="s1">)</span>

            <span class="s2"># Keep track of the best model</span>
            <span class="s3">if </span><span class="s1">best_score </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s2"># initialize</span>
                <span class="s3">if </span><span class="s1">self.alpha_per_target </span><span class="s3">and </span><span class="s1">n_y &gt; </span><span class="s4">1</span><span class="s1">:</span>
                    <span class="s1">best_coef = c</span>
                    <span class="s1">best_score = np.atleast_1d(alpha_score)</span>
                    <span class="s1">best_alpha = np.full(n_y</span><span class="s3">, </span><span class="s1">alpha)</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">best_coef = c</span>
                    <span class="s1">best_score = alpha_score</span>
                    <span class="s1">best_alpha = alpha</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s2"># update</span>
                <span class="s3">if </span><span class="s1">self.alpha_per_target </span><span class="s3">and </span><span class="s1">n_y &gt; </span><span class="s4">1</span><span class="s1">:</span>
                    <span class="s1">to_update = alpha_score &gt; best_score</span>
                    <span class="s1">best_coef[:</span><span class="s3">, </span><span class="s1">to_update] = c[:</span><span class="s3">, </span><span class="s1">to_update]</span>
                    <span class="s1">best_score[to_update] = alpha_score[to_update]</span>
                    <span class="s1">best_alpha[to_update] = alpha</span>
                <span class="s3">elif </span><span class="s1">alpha_score &gt; best_score:</span>
                    <span class="s1">best_coef</span><span class="s3">, </span><span class="s1">best_score</span><span class="s3">, </span><span class="s1">best_alpha = c</span><span class="s3">, </span><span class="s1">alpha_score</span><span class="s3">, </span><span class="s1">alpha</span>

        <span class="s1">self.alpha_ = best_alpha</span>
        <span class="s1">self.best_score_ = best_score</span>
        <span class="s1">self.dual_coef_ = best_coef</span>
        <span class="s1">self.coef_ = safe_sparse_dot(self.dual_coef_.T</span><span class="s3">, </span><span class="s1">X)</span>

        <span class="s3">if </span><span class="s1">sparse.issparse(X):</span>
            <span class="s1">X_offset = X_mean * X_scale</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">X_offset += X_mean * X_scale</span>
        <span class="s1">self._set_intercept(X_offset</span><span class="s3">, </span><span class="s1">y_offset</span><span class="s3">, </span><span class="s1">X_scale)</span>

        <span class="s3">if </span><span class="s1">self.store_cv_values:</span>
            <span class="s3">if </span><span class="s1">len(y.shape) == </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">cv_values_shape = n_samples</span><span class="s3">, </span><span class="s1">n_alphas</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">cv_values_shape = n_samples</span><span class="s3">, </span><span class="s1">n_y</span><span class="s3">, </span><span class="s1">n_alphas</span>
            <span class="s1">self.cv_values_ = self.cv_values_.reshape(cv_values_shape)</span>

        <span class="s3">return </span><span class="s1">self</span>


<span class="s3">class </span><span class="s1">_BaseRidgeCV(LinearModel):</span>
    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s5">&quot;alphas&quot;</span><span class="s1">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s3">, </span><span class="s1">Interval(Real</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s5">&quot;neither&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s5">&quot;fit_intercept&quot;</span><span class="s1">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;scoring&quot;</span><span class="s1">: [StrOptions(set(get_scorer_names()))</span><span class="s3">, </span><span class="s1">callable</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;cv&quot;</span><span class="s1">: [</span><span class="s5">&quot;cv_object&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;gcv_mode&quot;</span><span class="s1">: [StrOptions({</span><span class="s5">&quot;auto&quot;</span><span class="s3">, </span><span class="s5">&quot;svd&quot;</span><span class="s3">, </span><span class="s5">&quot;eigen&quot;</span><span class="s1">})</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;store_cv_values&quot;</span><span class="s1">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;alpha_per_target&quot;</span><span class="s1">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">alphas=(</span><span class="s4">0.1</span><span class="s3">, </span><span class="s4">1.0</span><span class="s3">, </span><span class="s4">10.0</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">fit_intercept=</span><span class="s3">True,</span>
        <span class="s1">scoring=</span><span class="s3">None,</span>
        <span class="s1">cv=</span><span class="s3">None,</span>
        <span class="s1">gcv_mode=</span><span class="s3">None,</span>
        <span class="s1">store_cv_values=</span><span class="s3">False,</span>
        <span class="s1">alpha_per_target=</span><span class="s3">False,</span>
    <span class="s1">):</span>
        <span class="s1">self.alphas = alphas</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.scoring = scoring</span>
        <span class="s1">self.cv = cv</span>
        <span class="s1">self.gcv_mode = gcv_mode</span>
        <span class="s1">self.store_cv_values = store_cv_values</span>
        <span class="s1">self.alpha_per_target = alpha_per_target</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit Ridge regression model with cv. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples, n_features) 
            Training data. If using GCV, will be cast to float64 
            if necessary. 
 
        y : ndarray of shape (n_samples,) or (n_samples, n_targets) 
            Target values. Will be cast to X's dtype if necessary. 
 
        sample_weight : float or ndarray of shape (n_samples,), default=None 
            Individual weights for each sample. If given a float, every sample 
            will have the same weight. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
 
        Notes 
        ----- 
        When sample_weight is provided, the selected hyperparameter may depend 
        on whether we use leave-one-out cross-validation (cv=None or cv='auto') 
        or another form of cross-validation, because only leave-one-out 
        cross-validation takes the sample weights into account when computing 
        the validation score. 
        &quot;&quot;&quot;</span>
        <span class="s1">cv = self.cv</span>

        <span class="s1">check_scalar_alpha = partial(</span>
            <span class="s1">check_scalar</span><span class="s3">,</span>
            <span class="s1">target_type=numbers.Real</span><span class="s3">,</span>
            <span class="s1">min_val=</span><span class="s4">0.0</span><span class="s3">,</span>
            <span class="s1">include_boundaries=</span><span class="s5">&quot;neither&quot;</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">isinstance(self.alphas</span><span class="s3">, </span><span class="s1">(np.ndarray</span><span class="s3">, </span><span class="s1">list</span><span class="s3">, </span><span class="s1">tuple)):</span>
            <span class="s1">n_alphas = </span><span class="s4">1 </span><span class="s3">if </span><span class="s1">np.ndim(self.alphas) == </span><span class="s4">0 </span><span class="s3">else </span><span class="s1">len(self.alphas)</span>
            <span class="s3">if </span><span class="s1">n_alphas != </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s3">for </span><span class="s1">index</span><span class="s3">, </span><span class="s1">alpha </span><span class="s3">in </span><span class="s1">enumerate(self.alphas):</span>
                    <span class="s1">alpha = check_scalar_alpha(alpha</span><span class="s3">, </span><span class="s5">f&quot;alphas[</span><span class="s3">{</span><span class="s1">index</span><span class="s3">}</span><span class="s5">]&quot;</span><span class="s1">)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">self.alphas[</span><span class="s4">0</span><span class="s1">] = check_scalar_alpha(self.alphas[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">&quot;alphas&quot;</span><span class="s1">)</span>
        <span class="s1">alphas = np.asarray(self.alphas)</span>

        <span class="s3">if </span><span class="s1">cv </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">estimator = _RidgeGCV(</span>
                <span class="s1">alphas</span><span class="s3">,</span>
                <span class="s1">fit_intercept=self.fit_intercept</span><span class="s3">,</span>
                <span class="s1">scoring=self.scoring</span><span class="s3">,</span>
                <span class="s1">gcv_mode=self.gcv_mode</span><span class="s3">,</span>
                <span class="s1">store_cv_values=self.store_cv_values</span><span class="s3">,</span>
                <span class="s1">is_clf=is_classifier(self)</span><span class="s3">,</span>
                <span class="s1">alpha_per_target=self.alpha_per_target</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">estimator.fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>
            <span class="s1">self.alpha_ = estimator.alpha_</span>
            <span class="s1">self.best_score_ = estimator.best_score_</span>
            <span class="s3">if </span><span class="s1">self.store_cv_values:</span>
                <span class="s1">self.cv_values_ = estimator.cv_values_</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self.store_cv_values:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;cv!=None and store_cv_values=True are incompatible&quot;</span><span class="s1">)</span>
            <span class="s3">if </span><span class="s1">self.alpha_per_target:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;cv!=None and alpha_per_target=True are incompatible&quot;</span><span class="s1">)</span>

            <span class="s1">parameters = {</span><span class="s5">&quot;alpha&quot;</span><span class="s1">: alphas}</span>
            <span class="s1">solver = </span><span class="s5">&quot;sparse_cg&quot; </span><span class="s3">if </span><span class="s1">sparse.issparse(X) </span><span class="s3">else </span><span class="s5">&quot;auto&quot;</span>
            <span class="s1">model = RidgeClassifier </span><span class="s3">if </span><span class="s1">is_classifier(self) </span><span class="s3">else </span><span class="s1">Ridge</span>
            <span class="s1">gs = GridSearchCV(</span>
                <span class="s1">model(</span>
                    <span class="s1">fit_intercept=self.fit_intercept</span><span class="s3">,</span>
                    <span class="s1">solver=solver</span><span class="s3">,</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">parameters</span><span class="s3">,</span>
                <span class="s1">cv=cv</span><span class="s3">,</span>
                <span class="s1">scoring=self.scoring</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">gs.fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>
            <span class="s1">estimator = gs.best_estimator_</span>
            <span class="s1">self.alpha_ = gs.best_estimator_.alpha</span>
            <span class="s1">self.best_score_ = gs.best_score_</span>

        <span class="s1">self.coef_ = estimator.coef_</span>
        <span class="s1">self.intercept_ = estimator.intercept_</span>
        <span class="s1">self.n_features_in_ = estimator.n_features_in_</span>
        <span class="s3">if </span><span class="s1">hasattr(estimator</span><span class="s3">, </span><span class="s5">&quot;feature_names_in_&quot;</span><span class="s1">):</span>
            <span class="s1">self.feature_names_in_ = estimator.feature_names_in_</span>

        <span class="s3">return </span><span class="s1">self</span>


<span class="s3">class </span><span class="s1">RidgeCV(MultiOutputMixin</span><span class="s3">, </span><span class="s1">RegressorMixin</span><span class="s3">, </span><span class="s1">_BaseRidgeCV):</span>
    <span class="s0">&quot;&quot;&quot;Ridge regression with built-in cross-validation. 
 
    See glossary entry for :term:`cross-validation estimator`. 
 
    By default, it performs efficient Leave-One-Out Cross-Validation. 
 
    Read more in the :ref:`User Guide &lt;ridge_regression&gt;`. 
 
    Parameters 
    ---------- 
    alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0) 
        Array of alpha values to try. 
        Regularization strength; must be a positive float. Regularization 
        improves the conditioning of the problem and reduces the variance of 
        the estimates. Larger values specify stronger regularization. 
        Alpha corresponds to ``1 / (2C)`` in other linear models such as 
        :class:`~sklearn.linear_model.LogisticRegression` or 
        :class:`~sklearn.svm.LinearSVC`. 
        If using Leave-One-Out cross-validation, alphas must be positive. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. If set 
        to false, no intercept will be used in calculations 
        (i.e. data is expected to be centered). 
 
    scoring : str, callable, default=None 
        A string (see model evaluation documentation) or 
        a scorer callable object / function with signature 
        ``scorer(estimator, X, y)``. 
        If None, the negative mean squared error if cv is 'auto' or None 
        (i.e. when using leave-one-out cross-validation), and r2 score 
        otherwise. 
 
    cv : int, cross-validation generator or an iterable, default=None 
        Determines the cross-validation splitting strategy. 
        Possible inputs for cv are: 
 
        - None, to use the efficient Leave-One-Out cross-validation 
        - integer, to specify the number of folds. 
        - :term:`CV splitter`, 
        - An iterable yielding (train, test) splits as arrays of indices. 
 
        For integer/None inputs, if ``y`` is binary or multiclass, 
        :class:`~sklearn.model_selection.StratifiedKFold` is used, else, 
        :class:`~sklearn.model_selection.KFold` is used. 
 
        Refer :ref:`User Guide &lt;cross_validation&gt;` for the various 
        cross-validation strategies that can be used here. 
 
    gcv_mode : {'auto', 'svd', 'eigen'}, default='auto' 
        Flag indicating which strategy to use when performing 
        Leave-One-Out Cross-Validation. Options are:: 
 
            'auto' : use 'svd' if n_samples &gt; n_features, otherwise use 'eigen' 
            'svd' : force use of singular value decomposition of X when X is 
                dense, eigenvalue decomposition of X^T.X when X is sparse. 
            'eigen' : force computation via eigendecomposition of X.X^T 
 
        The 'auto' mode is the default and is intended to pick the cheaper 
        option of the two depending on the shape of the training data. 
 
    store_cv_values : bool, default=False 
        Flag indicating if the cross-validation values corresponding to 
        each alpha should be stored in the ``cv_values_`` attribute (see 
        below). This flag is only compatible with ``cv=None`` (i.e. using 
        Leave-One-Out Cross-Validation). 
 
    alpha_per_target : bool, default=False 
        Flag indicating whether to optimize the alpha value (picked from the 
        `alphas` parameter list) for each target separately (for multi-output 
        settings: multiple prediction targets). When set to `True`, after 
        fitting, the `alpha_` attribute will contain a value for each target. 
        When set to `False`, a single alpha is used for all targets. 
 
        .. versionadded:: 0.24 
 
    Attributes 
    ---------- 
    cv_values_ : ndarray of shape (n_samples, n_alphas) or \ 
            shape (n_samples, n_targets, n_alphas), optional 
        Cross-validation values for each alpha (only available if 
        ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been 
        called, this attribute will contain the mean squared errors if 
        `scoring is None` otherwise it will contain standardized per point 
        prediction values. 
 
    coef_ : ndarray of shape (n_features) or (n_targets, n_features) 
        Weight vector(s). 
 
    intercept_ : float or ndarray of shape (n_targets,) 
        Independent term in decision function. Set to 0.0 if 
        ``fit_intercept = False``. 
 
    alpha_ : float or ndarray of shape (n_targets,) 
        Estimated regularization parameter, or, if ``alpha_per_target=True``, 
        the estimated regularization parameter for each target. 
 
    best_score_ : float or ndarray of shape (n_targets,) 
        Score of base estimator with best alpha, or, if 
        ``alpha_per_target=True``, a score for each target. 
 
        .. versionadded:: 0.23 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    Ridge : Ridge regression. 
    RidgeClassifier : Classifier based on ridge regression on {-1, 1} labels. 
    RidgeClassifierCV : Ridge classifier with built-in cross validation. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import load_diabetes 
    &gt;&gt;&gt; from sklearn.linear_model import RidgeCV 
    &gt;&gt;&gt; X, y = load_diabetes(return_X_y=True) 
    &gt;&gt;&gt; clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y) 
    &gt;&gt;&gt; clf.score(X, y) 
    0.5166... 
    &quot;&quot;&quot;</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit Ridge regression model with cv. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples, n_features) 
            Training data. If using GCV, will be cast to float64 
            if necessary. 
 
        y : ndarray of shape (n_samples,) or (n_samples, n_targets) 
            Target values. Will be cast to X's dtype if necessary. 
 
        sample_weight : float or ndarray of shape (n_samples,), default=None 
            Individual weights for each sample. If given a float, every sample 
            will have the same weight. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
 
        Notes 
        ----- 
        When sample_weight is provided, the selected hyperparameter may depend 
        on whether we use leave-one-out cross-validation (cv=None or cv='auto') 
        or another form of cross-validation, because only leave-one-out 
        cross-validation takes the sample weights into account when computing 
        the validation score. 
        &quot;&quot;&quot;</span>
        <span class="s1">super().fit(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>
        <span class="s3">return </span><span class="s1">self</span>


<span class="s3">class </span><span class="s1">RidgeClassifierCV(_RidgeClassifierMixin</span><span class="s3">, </span><span class="s1">_BaseRidgeCV):</span>
    <span class="s0">&quot;&quot;&quot;Ridge classifier with built-in cross-validation. 
 
    See glossary entry for :term:`cross-validation estimator`. 
 
    By default, it performs Leave-One-Out Cross-Validation. Currently, 
    only the n_features &gt; n_samples case is handled efficiently. 
 
    Read more in the :ref:`User Guide &lt;ridge_regression&gt;`. 
 
    Parameters 
    ---------- 
    alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0) 
        Array of alpha values to try. 
        Regularization strength; must be a positive float. Regularization 
        improves the conditioning of the problem and reduces the variance of 
        the estimates. Larger values specify stronger regularization. 
        Alpha corresponds to ``1 / (2C)`` in other linear models such as 
        :class:`~sklearn.linear_model.LogisticRegression` or 
        :class:`~sklearn.svm.LinearSVC`. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. If set 
        to false, no intercept will be used in calculations 
        (i.e. data is expected to be centered). 
 
    scoring : str, callable, default=None 
        A string (see model evaluation documentation) or 
        a scorer callable object / function with signature 
        ``scorer(estimator, X, y)``. 
 
    cv : int, cross-validation generator or an iterable, default=None 
        Determines the cross-validation splitting strategy. 
        Possible inputs for cv are: 
 
        - None, to use the efficient Leave-One-Out cross-validation 
        - integer, to specify the number of folds. 
        - :term:`CV splitter`, 
        - An iterable yielding (train, test) splits as arrays of indices. 
 
        Refer :ref:`User Guide &lt;cross_validation&gt;` for the various 
        cross-validation strategies that can be used here. 
 
    class_weight : dict or 'balanced', default=None 
        Weights associated with classes in the form ``{class_label: weight}``. 
        If not given, all classes are supposed to have weight one. 
 
        The &quot;balanced&quot; mode uses the values of y to automatically adjust 
        weights inversely proportional to class frequencies in the input data 
        as ``n_samples / (n_classes * np.bincount(y))``. 
 
    store_cv_values : bool, default=False 
        Flag indicating if the cross-validation values corresponding to 
        each alpha should be stored in the ``cv_values_`` attribute (see 
        below). This flag is only compatible with ``cv=None`` (i.e. using 
        Leave-One-Out Cross-Validation). 
 
    Attributes 
    ---------- 
    cv_values_ : ndarray of shape (n_samples, n_targets, n_alphas), optional 
        Cross-validation values for each alpha (only if ``store_cv_values=True`` and 
        ``cv=None``). After ``fit()`` has been called, this attribute will 
        contain the mean squared errors if `scoring is None` otherwise it 
        will contain standardized per point prediction values. 
 
    coef_ : ndarray of shape (1, n_features) or (n_targets, n_features) 
        Coefficient of the features in the decision function. 
 
        ``coef_`` is of shape (1, n_features) when the given problem is binary. 
 
    intercept_ : float or ndarray of shape (n_targets,) 
        Independent term in decision function. Set to 0.0 if 
        ``fit_intercept = False``. 
 
    alpha_ : float 
        Estimated regularization parameter. 
 
    best_score_ : float 
        Score of base estimator with best alpha. 
 
        .. versionadded:: 0.23 
 
    classes_ : ndarray of shape (n_classes,) 
        The classes labels. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    Ridge : Ridge regression. 
    RidgeClassifier : Ridge classifier. 
    RidgeCV : Ridge regression with built-in cross validation. 
 
    Notes 
    ----- 
    For multi-class classification, n_class classifiers are trained in 
    a one-versus-all approach. Concretely, this is implemented by taking 
    advantage of the multi-variate response support in Ridge. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import load_breast_cancer 
    &gt;&gt;&gt; from sklearn.linear_model import RidgeClassifierCV 
    &gt;&gt;&gt; X, y = load_breast_cancer(return_X_y=True) 
    &gt;&gt;&gt; clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y) 
    &gt;&gt;&gt; clf.score(X, y) 
    0.9630... 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**_BaseRidgeCV._parameter_constraints</span><span class="s3">,</span>
        <span class="s5">&quot;class_weight&quot;</span><span class="s1">: [dict</span><span class="s3">, </span><span class="s1">StrOptions({</span><span class="s5">&quot;balanced&quot;</span><span class="s1">})</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>
    <span class="s3">for </span><span class="s1">param </span><span class="s3">in </span><span class="s1">(</span><span class="s5">&quot;gcv_mode&quot;</span><span class="s3">, </span><span class="s5">&quot;alpha_per_target&quot;</span><span class="s1">):</span>
        <span class="s1">_parameter_constraints.pop(param)</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">alphas=(</span><span class="s4">0.1</span><span class="s3">, </span><span class="s4">1.0</span><span class="s3">, </span><span class="s4">10.0</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">fit_intercept=</span><span class="s3">True,</span>
        <span class="s1">scoring=</span><span class="s3">None,</span>
        <span class="s1">cv=</span><span class="s3">None,</span>
        <span class="s1">class_weight=</span><span class="s3">None,</span>
        <span class="s1">store_cv_values=</span><span class="s3">False,</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">alphas=alphas</span><span class="s3">,</span>
            <span class="s1">fit_intercept=fit_intercept</span><span class="s3">,</span>
            <span class="s1">scoring=scoring</span><span class="s3">,</span>
            <span class="s1">cv=cv</span><span class="s3">,</span>
            <span class="s1">store_cv_values=store_cv_values</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">self.class_weight = class_weight</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit Ridge classifier with cv. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples, n_features) 
            Training vectors, where `n_samples` is the number of samples 
            and `n_features` is the number of features. When using GCV, 
            will be cast to float64 if necessary. 
 
        y : ndarray of shape (n_samples,) 
            Target values. Will be cast to X's dtype if necessary. 
 
        sample_weight : float or ndarray of shape (n_samples,), default=None 
            Individual weights for each sample. If given a float, every sample 
            will have the same weight. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s2"># `RidgeClassifier` does not accept &quot;sag&quot; or &quot;saga&quot; solver and thus support</span>
        <span class="s2"># csr, csc, and coo sparse matrices. By using solver=&quot;eigen&quot; we force to accept</span>
        <span class="s2"># all sparse format.</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">Y = self._prepare_data(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">solver=</span><span class="s5">&quot;eigen&quot;</span><span class="s1">)</span>

        <span class="s2"># If cv is None, gcv mode will be used and we used the binarized Y</span>
        <span class="s2"># since y will not be binarized in _RidgeGCV estimator.</span>
        <span class="s2"># If cv is not None, a GridSearchCV with some RidgeClassifier</span>
        <span class="s2"># estimators are used where y will be binarized. Thus, we pass y</span>
        <span class="s2"># instead of the binarized Y.</span>
        <span class="s1">target = Y </span><span class="s3">if </span><span class="s1">self.cv </span><span class="s3">is None else </span><span class="s1">y</span>
        <span class="s1">super().fit(X</span><span class="s3">, </span><span class="s1">target</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span>
            <span class="s5">&quot;multilabel&quot;</span><span class="s1">: </span><span class="s3">True,</span>
            <span class="s5">&quot;_xfail_checks&quot;</span><span class="s1">: {</span>
                <span class="s5">&quot;check_sample_weights_invariance&quot;</span><span class="s1">: (</span>
                    <span class="s5">&quot;zero sample_weight is not equivalent to removing samples&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">}</span><span class="s3">,</span>
        <span class="s1">}</span>
</pre>
</body>
</html>