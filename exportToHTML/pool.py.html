<html>
<head>
<title>pool.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
pool.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Custom implementation of multiprocessing.Pool with custom pickler. 
 
This module provides efficient ways of working with data stored in 
shared memory with numpy.memmap arrays without inducing any memory 
copy between the parent and child processes. 
 
This module should not be imported if multiprocessing is not 
available as it implements subclasses of multiprocessing Pool 
that uses a custom alternative to SimpleQueue. 
 
&quot;&quot;&quot;</span>
<span class="s2"># Author: Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span>
<span class="s2"># Copyright: 2012, Olivier Grisel</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">copyreg</span>
<span class="s3">import </span><span class="s1">sys</span>
<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">time </span><span class="s3">import </span><span class="s1">sleep</span>

<span class="s3">try</span><span class="s1">:</span>
    <span class="s1">WindowsError</span>
<span class="s3">except </span><span class="s1">NameError:</span>
    <span class="s1">WindowsError = type(</span><span class="s3">None</span><span class="s1">)</span>

<span class="s3">from </span><span class="s1">pickle </span><span class="s3">import </span><span class="s1">Pickler</span>

<span class="s3">from </span><span class="s1">pickle </span><span class="s3">import </span><span class="s1">HIGHEST_PROTOCOL</span>
<span class="s3">from </span><span class="s1">io </span><span class="s3">import </span><span class="s1">BytesIO</span>

<span class="s3">from </span><span class="s1">._memmapping_reducer </span><span class="s3">import </span><span class="s1">get_memmapping_reducers</span>
<span class="s3">from </span><span class="s1">._memmapping_reducer </span><span class="s3">import </span><span class="s1">TemporaryResourcesManager</span>
<span class="s3">from </span><span class="s1">._multiprocessing_helpers </span><span class="s3">import </span><span class="s1">mp</span><span class="s3">, </span><span class="s1">assert_spawning</span>

<span class="s2"># We need the class definition to derive from it, not the multiprocessing.Pool</span>
<span class="s2"># factory function</span>
<span class="s3">from </span><span class="s1">multiprocessing.pool </span><span class="s3">import </span><span class="s1">Pool</span>

<span class="s3">try</span><span class="s1">:</span>
    <span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">except </span><span class="s1">ImportError:</span>
    <span class="s1">np = </span><span class="s3">None</span>


<span class="s2">###############################################################################</span>
<span class="s2"># Enable custom pickling in Pool queues</span>

<span class="s3">class </span><span class="s1">CustomizablePickler(Pickler):</span>
    <span class="s0">&quot;&quot;&quot;Pickler that accepts custom reducers. 
 
    TODO python2_drop : can this be simplified ? 
 
    HIGHEST_PROTOCOL is selected by default as this pickler is used 
    to pickle ephemeral datastructures for interprocess communication 
    hence no backward compatibility is required. 
 
    `reducers` is expected to be a dictionary with key/values 
    being `(type, callable)` pairs where `callable` is a function that 
    give an instance of `type` will return a tuple `(constructor, 
    tuple_of_objects)` to rebuild an instance out of the pickled 
    `tuple_of_objects` as would return a `__reduce__` method. See the 
    standard library documentation on pickling for more details. 
 
    &quot;&quot;&quot;</span>

    <span class="s2"># We override the pure Python pickler as its the only way to be able to</span>
    <span class="s2"># customize the dispatch table without side effects in Python 2.7</span>
    <span class="s2"># to 3.2. For Python 3.3+ leverage the new dispatch_table</span>
    <span class="s2"># feature from https://bugs.python.org/issue14166 that makes it possible</span>
    <span class="s2"># to use the C implementation of the Pickler which is faster.</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">writer</span><span class="s3">, </span><span class="s1">reducers=</span><span class="s3">None, </span><span class="s1">protocol=HIGHEST_PROTOCOL):</span>
        <span class="s1">Pickler.__init__(self</span><span class="s3">, </span><span class="s1">writer</span><span class="s3">, </span><span class="s1">protocol=protocol)</span>
        <span class="s3">if </span><span class="s1">reducers </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">reducers = {}</span>
        <span class="s3">if </span><span class="s1">hasattr(Pickler</span><span class="s3">, </span><span class="s4">'dispatch'</span><span class="s1">):</span>
            <span class="s2"># Make the dispatch registry an instance level attribute instead of</span>
            <span class="s2"># a reference to the class dictionary under Python 2</span>
            <span class="s1">self.dispatch = Pickler.dispatch.copy()</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># Under Python 3 initialize the dispatch table with a copy of the</span>
            <span class="s2"># default registry</span>
            <span class="s1">self.dispatch_table = copyreg.dispatch_table.copy()</span>
        <span class="s3">for </span><span class="s1">type</span><span class="s3">, </span><span class="s1">reduce_func </span><span class="s3">in </span><span class="s1">reducers.items():</span>
            <span class="s1">self.register(type</span><span class="s3">, </span><span class="s1">reduce_func)</span>

    <span class="s3">def </span><span class="s1">register(self</span><span class="s3">, </span><span class="s1">type</span><span class="s3">, </span><span class="s1">reduce_func):</span>
        <span class="s0">&quot;&quot;&quot;Attach a reducer function to a given type in the dispatch table.&quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">hasattr(Pickler</span><span class="s3">, </span><span class="s4">'dispatch'</span><span class="s1">):</span>
            <span class="s2"># Python 2 pickler dispatching is not explicitly customizable.</span>
            <span class="s2"># Let us use a closure to workaround this limitation.</span>
            <span class="s3">def </span><span class="s1">dispatcher(self</span><span class="s3">, </span><span class="s1">obj):</span>
                <span class="s1">reduced = reduce_func(obj)</span>
                <span class="s1">self.save_reduce(obj=obj</span><span class="s3">, </span><span class="s1">*reduced)</span>
            <span class="s1">self.dispatch[type] = dispatcher</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.dispatch_table[type] = reduce_func</span>


<span class="s3">class </span><span class="s1">CustomizablePicklingQueue(object):</span>
    <span class="s0">&quot;&quot;&quot;Locked Pipe implementation that uses a customizable pickler. 
 
    This class is an alternative to the multiprocessing implementation 
    of SimpleQueue in order to make it possible to pass custom 
    pickling reducers, for instance to avoid memory copy when passing 
    memory mapped datastructures. 
 
    `reducers` is expected to be a dict with key / values being 
    `(type, callable)` pairs where `callable` is a function that, given an 
    instance of `type`, will return a tuple `(constructor, tuple_of_objects)` 
    to rebuild an instance out of the pickled `tuple_of_objects` as would 
    return a `__reduce__` method. 
 
    See the standard library documentation on pickling for more details. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">context</span><span class="s3">, </span><span class="s1">reducers=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">self._reducers = reducers</span>
        <span class="s1">self._reader</span><span class="s3">, </span><span class="s1">self._writer = context.Pipe(duplex=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">self._rlock = context.Lock()</span>
        <span class="s3">if </span><span class="s1">sys.platform == </span><span class="s4">'win32'</span><span class="s1">:</span>
            <span class="s1">self._wlock = </span><span class="s3">None</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._wlock = context.Lock()</span>
        <span class="s1">self._make_methods()</span>

    <span class="s3">def </span><span class="s1">__getstate__(self):</span>
        <span class="s1">assert_spawning(self)</span>
        <span class="s3">return </span><span class="s1">(self._reader</span><span class="s3">, </span><span class="s1">self._writer</span><span class="s3">, </span><span class="s1">self._rlock</span><span class="s3">, </span><span class="s1">self._wlock</span><span class="s3">,</span>
                <span class="s1">self._reducers)</span>

    <span class="s3">def </span><span class="s1">__setstate__(self</span><span class="s3">, </span><span class="s1">state):</span>
        <span class="s1">(self._reader</span><span class="s3">, </span><span class="s1">self._writer</span><span class="s3">, </span><span class="s1">self._rlock</span><span class="s3">, </span><span class="s1">self._wlock</span><span class="s3">,</span>
         <span class="s1">self._reducers) = state</span>
        <span class="s1">self._make_methods()</span>

    <span class="s3">def </span><span class="s1">empty(self):</span>
        <span class="s3">return not </span><span class="s1">self._reader.poll()</span>

    <span class="s3">def </span><span class="s1">_make_methods(self):</span>
        <span class="s1">self._recv = recv = self._reader.recv</span>
        <span class="s1">racquire</span><span class="s3">, </span><span class="s1">rrelease = self._rlock.acquire</span><span class="s3">, </span><span class="s1">self._rlock.release</span>

        <span class="s3">def </span><span class="s1">get():</span>
            <span class="s1">racquire()</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s3">return </span><span class="s1">recv()</span>
            <span class="s3">finally</span><span class="s1">:</span>
                <span class="s1">rrelease()</span>

        <span class="s1">self.get = get</span>

        <span class="s3">if </span><span class="s1">self._reducers:</span>
            <span class="s3">def </span><span class="s1">send(obj):</span>
                <span class="s1">buffer = BytesIO()</span>
                <span class="s1">CustomizablePickler(buffer</span><span class="s3">, </span><span class="s1">self._reducers).dump(obj)</span>
                <span class="s1">self._writer.send_bytes(buffer.getvalue())</span>
            <span class="s1">self._send = send</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._send = send = self._writer.send</span>
        <span class="s3">if </span><span class="s1">self._wlock </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s2"># writes to a message oriented win32 pipe are atomic</span>
            <span class="s1">self.put = send</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">wlock_acquire</span><span class="s3">, </span><span class="s1">wlock_release = (</span>
                <span class="s1">self._wlock.acquire</span><span class="s3">, </span><span class="s1">self._wlock.release)</span>

            <span class="s3">def </span><span class="s1">put(obj):</span>
                <span class="s1">wlock_acquire()</span>
                <span class="s3">try</span><span class="s1">:</span>
                    <span class="s3">return </span><span class="s1">send(obj)</span>
                <span class="s3">finally</span><span class="s1">:</span>
                    <span class="s1">wlock_release()</span>

            <span class="s1">self.put = put</span>


<span class="s3">class </span><span class="s1">PicklingPool(Pool):</span>
    <span class="s0">&quot;&quot;&quot;Pool implementation with customizable pickling reducers. 
 
    This is useful to control how data is shipped between processes 
    and makes it possible to use shared memory without useless 
    copies induces by the default pickling methods of the original 
    objects passed as arguments to dispatch. 
 
    `forward_reducers` and `backward_reducers` are expected to be 
    dictionaries with key/values being `(type, callable)` pairs where 
    `callable` is a function that, given an instance of `type`, will return a 
    tuple `(constructor, tuple_of_objects)` to rebuild an instance out of the 
    pickled `tuple_of_objects` as would return a `__reduce__` method. 
    See the standard library documentation about pickling for more details. 
 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">processes=</span><span class="s3">None, </span><span class="s1">forward_reducers=</span><span class="s3">None,</span>
                 <span class="s1">backward_reducers=</span><span class="s3">None, </span><span class="s1">**kwargs):</span>
        <span class="s3">if </span><span class="s1">forward_reducers </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">forward_reducers = dict()</span>
        <span class="s3">if </span><span class="s1">backward_reducers </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">backward_reducers = dict()</span>
        <span class="s1">self._forward_reducers = forward_reducers</span>
        <span class="s1">self._backward_reducers = backward_reducers</span>
        <span class="s1">poolargs = dict(processes=processes)</span>
        <span class="s1">poolargs.update(kwargs)</span>
        <span class="s1">super(PicklingPool</span><span class="s3">, </span><span class="s1">self).__init__(**poolargs)</span>

    <span class="s3">def </span><span class="s1">_setup_queues(self):</span>
        <span class="s1">context = getattr(self</span><span class="s3">, </span><span class="s4">'_ctx'</span><span class="s3">, </span><span class="s1">mp)</span>
        <span class="s1">self._inqueue = CustomizablePicklingQueue(context</span><span class="s3">,</span>
                                                  <span class="s1">self._forward_reducers)</span>
        <span class="s1">self._outqueue = CustomizablePicklingQueue(context</span><span class="s3">,</span>
                                                   <span class="s1">self._backward_reducers)</span>
        <span class="s1">self._quick_put = self._inqueue._send</span>
        <span class="s1">self._quick_get = self._outqueue._recv</span>


<span class="s3">class </span><span class="s1">MemmappingPool(PicklingPool):</span>
    <span class="s0">&quot;&quot;&quot;Process pool that shares large arrays to avoid memory copy. 
 
    This drop-in replacement for `multiprocessing.pool.Pool` makes 
    it possible to work efficiently with shared memory in a numpy 
    context. 
 
    Existing instances of numpy.memmap are preserved: the child 
    suprocesses will have access to the same shared memory in the 
    original mode except for the 'w+' mode that is automatically 
    transformed as 'r+' to avoid zeroing the original data upon 
    instantiation. 
 
    Furthermore large arrays from the parent process are automatically 
    dumped to a temporary folder on the filesystem such as child 
    processes to access their content via memmapping (file system 
    backed shared memory). 
 
    Note: it is important to call the terminate method to collect 
    the temporary folder used by the pool. 
 
    Parameters 
    ---------- 
    processes: int, optional 
        Number of worker processes running concurrently in the pool. 
    initializer: callable, optional 
        Callable executed on worker process creation. 
    initargs: tuple, optional 
        Arguments passed to the initializer callable. 
    temp_folder: (str, callable) optional 
        If str: 
          Folder to be used by the pool for memmapping large arrays 
          for sharing memory with worker processes. If None, this will try in 
          order: 
          - a folder pointed by the JOBLIB_TEMP_FOLDER environment variable, 
          - /dev/shm if the folder exists and is writable: this is a RAMdisk 
            filesystem available by default on modern Linux distributions, 
          - the default system temporary folder that can be overridden 
            with TMP, TMPDIR or TEMP environment variables, typically /tmp 
            under Unix operating systems. 
        if callable: 
            An callable in charge of dynamically resolving a temporary folder 
            for memmapping large arrays. 
    max_nbytes int or None, optional, 1e6 by default 
        Threshold on the size of arrays passed to the workers that 
        triggers automated memory mapping in temp_folder. 
        Use None to disable memmapping of large arrays. 
    mmap_mode: {'r+', 'r', 'w+', 'c'} 
        Memmapping mode for numpy arrays passed to workers. 
        See 'max_nbytes' parameter documentation for more details. 
    forward_reducers: dictionary, optional 
        Reducers used to pickle objects passed from master to worker 
        processes: see below. 
    backward_reducers: dictionary, optional 
        Reducers used to pickle return values from workers back to the 
        master process. 
    verbose: int, optional 
        Make it possible to monitor how the communication of numpy arrays 
        with the subprocess is handled (pickling or memmapping) 
    prewarm: bool or str, optional, &quot;auto&quot; by default. 
        If True, force a read on newly memmapped array to make sure that OS 
        pre-cache it in memory. This can be useful to avoid concurrent disk 
        access when the same data array is passed to different worker 
        processes. If &quot;auto&quot; (by default), prewarm is set to True, unless the 
        Linux shared memory partition /dev/shm is available and used as temp 
        folder. 
 
    `forward_reducers` and `backward_reducers` are expected to be 
    dictionaries with key/values being `(type, callable)` pairs where 
    `callable` is a function that give an instance of `type` will return 
    a tuple `(constructor, tuple_of_objects)` to rebuild an instance out 
    of the pickled `tuple_of_objects` as would return a `__reduce__` 
    method. See the standard library documentation on pickling for more 
    details. 
 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">processes=</span><span class="s3">None, </span><span class="s1">temp_folder=</span><span class="s3">None, </span><span class="s1">max_nbytes=</span><span class="s5">1e6</span><span class="s3">,</span>
                 <span class="s1">mmap_mode=</span><span class="s4">'r'</span><span class="s3">, </span><span class="s1">forward_reducers=</span><span class="s3">None, </span><span class="s1">backward_reducers=</span><span class="s3">None,</span>
                 <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">context_id=</span><span class="s3">None, </span><span class="s1">prewarm=</span><span class="s3">False, </span><span class="s1">**kwargs):</span>

        <span class="s3">if </span><span class="s1">context_id </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span><span class="s4">'context_id is deprecated and ignored in joblib'</span>
                          <span class="s4">' 0.9.4 and will be removed in 0.11'</span><span class="s3">,</span>
                          <span class="s1">DeprecationWarning)</span>

        <span class="s1">manager = TemporaryResourcesManager(temp_folder)</span>
        <span class="s1">self._temp_folder_manager = manager</span>

        <span class="s2"># The usage of a temp_folder_resolver over a simple temp_folder is</span>
        <span class="s2"># superfluous for multiprocessing pools, as they don't get reused, see</span>
        <span class="s2"># get_memmapping_executor for more details. We still use it for code</span>
        <span class="s2"># simplicity.</span>
        <span class="s1">forward_reducers</span><span class="s3">, </span><span class="s1">backward_reducers = \</span>
            <span class="s1">get_memmapping_reducers(</span>
                <span class="s1">temp_folder_resolver=manager.resolve_temp_folder_name</span><span class="s3">,</span>
                <span class="s1">max_nbytes=max_nbytes</span><span class="s3">, </span><span class="s1">mmap_mode=mmap_mode</span><span class="s3">,</span>
                <span class="s1">forward_reducers=forward_reducers</span><span class="s3">,</span>
                <span class="s1">backward_reducers=backward_reducers</span><span class="s3">, </span><span class="s1">verbose=verbose</span><span class="s3">,</span>
                <span class="s1">unlink_on_gc_collect=</span><span class="s3">False, </span><span class="s1">prewarm=prewarm)</span>

        <span class="s1">poolargs = dict(</span>
            <span class="s1">processes=processes</span><span class="s3">,</span>
            <span class="s1">forward_reducers=forward_reducers</span><span class="s3">,</span>
            <span class="s1">backward_reducers=backward_reducers)</span>
        <span class="s1">poolargs.update(kwargs)</span>
        <span class="s1">super(MemmappingPool</span><span class="s3">, </span><span class="s1">self).__init__(**poolargs)</span>

    <span class="s3">def </span><span class="s1">terminate(self):</span>
        <span class="s1">n_retries = </span><span class="s5">10</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_retries):</span>
            <span class="s3">try</span><span class="s1">:</span>
                <span class="s1">super(MemmappingPool</span><span class="s3">, </span><span class="s1">self).terminate()</span>
                <span class="s3">break</span>
            <span class="s3">except </span><span class="s1">OSError </span><span class="s3">as </span><span class="s1">e:</span>
                <span class="s3">if </span><span class="s1">isinstance(e</span><span class="s3">, </span><span class="s1">WindowsError):</span>
                    <span class="s2"># Workaround  occasional &quot;[Error 5] Access is denied&quot; issue</span>
                    <span class="s2"># when trying to terminate a process under windows.</span>
                    <span class="s1">sleep(</span><span class="s5">0.1</span><span class="s1">)</span>
                    <span class="s3">if </span><span class="s1">i + </span><span class="s5">1 </span><span class="s1">== n_retries:</span>
                        <span class="s1">warnings.warn(</span><span class="s4">&quot;Failed to terminate worker processes in&quot;</span>
                                      <span class="s4">&quot; multiprocessing pool: %r&quot; </span><span class="s1">% e)</span>

        <span class="s2"># Clean up the temporary resources as the workers should now be off.</span>
        <span class="s1">self._temp_folder_manager._clean_temporary_resources()</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">_temp_folder(self):</span>
        <span class="s2"># Legacy property in tests. could be removed if we refactored the</span>
        <span class="s2"># memmapping tests. SHOULD ONLY BE USED IN TESTS!</span>
        <span class="s2"># We cache this property because it is called late in the tests - at</span>
        <span class="s2"># this point, all context have been unregistered, and</span>
        <span class="s2"># resolve_temp_folder_name raises an error.</span>
        <span class="s3">if </span><span class="s1">getattr(self</span><span class="s3">, </span><span class="s4">'_cached_temp_folder'</span><span class="s3">, None</span><span class="s1">) </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">self._cached_temp_folder</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self._cached_temp_folder = self._temp_folder_manager.resolve_temp_folder_name()  </span><span class="s2"># noqa</span>
            <span class="s3">return </span><span class="s1">self._cached_temp_folder</span>
</pre>
</body>
</html>