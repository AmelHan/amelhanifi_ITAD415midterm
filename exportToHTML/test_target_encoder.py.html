<html>
<head>
<title>test_target_encoder.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_target_encoder.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">assert_allclose</span><span class="s0">, </span><span class="s1">assert_array_equal</span>

<span class="s0">from </span><span class="s1">sklearn.ensemble </span><span class="s0">import </span><span class="s1">RandomForestRegressor</span>
<span class="s0">from </span><span class="s1">sklearn.linear_model </span><span class="s0">import </span><span class="s1">Ridge</span>
<span class="s0">from </span><span class="s1">sklearn.model_selection </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">KFold</span><span class="s0">,</span>
    <span class="s1">ShuffleSplit</span><span class="s0">,</span>
    <span class="s1">StratifiedKFold</span><span class="s0">,</span>
    <span class="s1">cross_val_score</span><span class="s0">,</span>
    <span class="s1">train_test_split</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.pipeline </span><span class="s0">import </span><span class="s1">make_pipeline</span>
<span class="s0">from </span><span class="s1">sklearn.preprocessing </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">KBinsDiscretizer</span><span class="s0">,</span>
    <span class="s1">LabelEncoder</span><span class="s0">,</span>
    <span class="s1">TargetEncoder</span><span class="s0">,</span>
<span class="s1">)</span>


<span class="s0">def </span><span class="s1">_encode_target(X_ordinal</span><span class="s0">, </span><span class="s1">y_int</span><span class="s0">, </span><span class="s1">n_categories</span><span class="s0">, </span><span class="s1">smooth):</span>
    <span class="s2">&quot;&quot;&quot;Simple Python implementation of target encoding.&quot;&quot;&quot;</span>
    <span class="s1">cur_encodings = np.zeros(n_categories</span><span class="s0">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s1">y_mean = np.mean(y_int)</span>

    <span class="s0">if </span><span class="s1">smooth == </span><span class="s3">&quot;auto&quot;</span><span class="s1">:</span>
        <span class="s1">y_variance = np.var(y_int)</span>
        <span class="s0">for </span><span class="s1">c </span><span class="s0">in </span><span class="s1">range(n_categories):</span>
            <span class="s1">y_subset = y_int[X_ordinal == c]</span>
            <span class="s1">n_i = y_subset.shape[</span><span class="s4">0</span><span class="s1">]</span>

            <span class="s0">if </span><span class="s1">n_i == </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s1">cur_encodings[c] = y_mean</span>
                <span class="s0">continue</span>

            <span class="s1">y_subset_variance = np.var(y_subset)</span>
            <span class="s1">m = y_subset_variance / y_variance</span>
            <span class="s1">lambda_ = n_i / (n_i + m)</span>

            <span class="s1">cur_encodings[c] = lambda_ * np.mean(y_subset) + (</span><span class="s4">1 </span><span class="s1">- lambda_) * y_mean</span>
        <span class="s0">return </span><span class="s1">cur_encodings</span>
    <span class="s0">else</span><span class="s1">:  </span><span class="s5"># float</span>
        <span class="s0">for </span><span class="s1">c </span><span class="s0">in </span><span class="s1">range(n_categories):</span>
            <span class="s1">y_subset = y_int[X_ordinal == c]</span>
            <span class="s1">current_sum = np.sum(y_subset) + y_mean * smooth</span>
            <span class="s1">current_cnt = y_subset.shape[</span><span class="s4">0</span><span class="s1">] + smooth</span>
            <span class="s1">cur_encodings[c] = current_sum / current_cnt</span>
        <span class="s0">return </span><span class="s1">cur_encodings</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;categories, unknown_value&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">([np.array([</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=np.int64)]</span><span class="s0">, </span><span class="s4">4</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">([np.array([</span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">3.0</span><span class="s0">, </span><span class="s1">np.nan]</span><span class="s0">, </span><span class="s1">dtype=np.float64)]</span><span class="s0">, </span><span class="s4">6.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">([np.array([</span><span class="s3">&quot;cat&quot;</span><span class="s0">, </span><span class="s3">&quot;dog&quot;</span><span class="s0">, </span><span class="s3">&quot;snake&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=object)]</span><span class="s0">, </span><span class="s3">&quot;bear&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">&quot;auto&quot;</span><span class="s0">, </span><span class="s4">3</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;smooth&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">5.0</span><span class="s0">, </span><span class="s3">&quot;auto&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;target_type&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">&quot;binary&quot;</span><span class="s0">, </span><span class="s3">&quot;continuous&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_encoding(categories</span><span class="s0">, </span><span class="s1">unknown_value</span><span class="s0">, </span><span class="s1">global_random_seed</span><span class="s0">, </span><span class="s1">smooth</span><span class="s0">, </span><span class="s1">target_type):</span>
    <span class="s2">&quot;&quot;&quot;Check encoding for binary and continuous targets. 
 
    Compare the values returned by `TargetEncoder.fit_transform` against the 
    expected encodings for cv splits from a naive reference Python 
    implementation in _encode_target. 
    &quot;&quot;&quot;</span>

    <span class="s1">n_categories = </span><span class="s4">3</span>
    <span class="s1">X_train_int_array = np.array([[</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">20 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">30 </span><span class="s1">+ [</span><span class="s4">2</span><span class="s1">] * </span><span class="s4">40</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=np.int64).T</span>
    <span class="s1">X_test_int_array = np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">dtype=np.int64).T</span>
    <span class="s1">n_samples = X_train_int_array.shape[</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s0">if </span><span class="s1">categories == </span><span class="s3">&quot;auto&quot;</span><span class="s1">:</span>
        <span class="s1">X_train = X_train_int_array</span>
        <span class="s1">X_test = X_test_int_array</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">X_train = categories[</span><span class="s4">0</span><span class="s1">][X_train_int_array]</span>
        <span class="s1">X_test = categories[</span><span class="s4">0</span><span class="s1">][X_test_int_array]</span>

    <span class="s1">X_test = np.concatenate((X_test</span><span class="s0">, </span><span class="s1">[[unknown_value]]))</span>

    <span class="s1">data_rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">n_splits = </span><span class="s4">3</span>
    <span class="s0">if </span><span class="s1">target_type == </span><span class="s3">&quot;binary&quot;</span><span class="s1">:</span>
        <span class="s1">y_int = data_rng.randint(low=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">high=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
        <span class="s1">target_names = np.array([</span><span class="s3">&quot;cat&quot;</span><span class="s0">, </span><span class="s3">&quot;dog&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=object)</span>
        <span class="s1">y_train = target_names[y_int]</span>

    <span class="s0">else</span><span class="s1">:  </span><span class="s5"># target_type == continuous</span>
        <span class="s1">y_int = data_rng.uniform(low=-</span><span class="s4">10</span><span class="s0">, </span><span class="s1">high=</span><span class="s4">20</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
        <span class="s1">y_train = y_int</span>

    <span class="s1">shuffled_idx = data_rng.permutation(n_samples)</span>
    <span class="s1">X_train_int_array = X_train_int_array[shuffled_idx]</span>
    <span class="s1">X_train = X_train[shuffled_idx]</span>
    <span class="s1">y_train = y_train[shuffled_idx]</span>
    <span class="s1">y_int = y_int[shuffled_idx]</span>

    <span class="s5"># Define our CV splitting strategy</span>
    <span class="s0">if </span><span class="s1">target_type == </span><span class="s3">&quot;binary&quot;</span><span class="s1">:</span>
        <span class="s1">cv = StratifiedKFold(</span>
            <span class="s1">n_splits=n_splits</span><span class="s0">, </span><span class="s1">random_state=global_random_seed</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">True</span>
        <span class="s1">)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">cv = KFold(n_splits=n_splits</span><span class="s0">, </span><span class="s1">random_state=global_random_seed</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">True</span><span class="s1">)</span>

    <span class="s5"># Compute the expected values using our reference Python implementation of</span>
    <span class="s5"># target encoding:</span>
    <span class="s1">expected_X_fit_transform = np.empty_like(X_train_int_array</span><span class="s0">, </span><span class="s1">dtype=np.float64)</span>

    <span class="s0">for </span><span class="s1">train_idx</span><span class="s0">, </span><span class="s1">test_idx </span><span class="s0">in </span><span class="s1">cv.split(X_train_int_array</span><span class="s0">, </span><span class="s1">y_train):</span>
        <span class="s1">X_</span><span class="s0">, </span><span class="s1">y_ = X_train_int_array[train_idx</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y_int[train_idx]</span>
        <span class="s1">cur_encodings = _encode_target(X_</span><span class="s0">, </span><span class="s1">y_</span><span class="s0">, </span><span class="s1">n_categories</span><span class="s0">, </span><span class="s1">smooth)</span>
        <span class="s1">expected_X_fit_transform[test_idx</span><span class="s0">, </span><span class="s4">0</span><span class="s1">] = cur_encodings[</span>
            <span class="s1">X_train_int_array[test_idx</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">]</span>

    <span class="s5"># Check that we can obtain the same encodings by calling `fit_transform` on</span>
    <span class="s5"># the estimator with the same CV parameters:</span>
    <span class="s1">target_encoder = TargetEncoder(</span>
        <span class="s1">smooth=smooth</span><span class="s0">,</span>
        <span class="s1">categories=categories</span><span class="s0">,</span>
        <span class="s1">cv=n_splits</span><span class="s0">,</span>
        <span class="s1">random_state=global_random_seed</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">X_fit_transform = target_encoder.fit_transform(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s0">assert </span><span class="s1">target_encoder.target_type_ == target_type</span>
    <span class="s1">assert_allclose(X_fit_transform</span><span class="s0">, </span><span class="s1">expected_X_fit_transform)</span>
    <span class="s0">assert </span><span class="s1">len(target_encoder.encodings_) == </span><span class="s4">1</span>

    <span class="s5"># compute encodings for all data to validate `transform`</span>
    <span class="s1">y_mean = np.mean(y_int)</span>
    <span class="s1">expected_encodings = _encode_target(</span>
        <span class="s1">X_train_int_array[:</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y_int</span><span class="s0">, </span><span class="s1">n_categories</span><span class="s0">, </span><span class="s1">smooth</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(target_encoder.encodings_[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">expected_encodings)</span>
    <span class="s0">assert </span><span class="s1">target_encoder.target_mean_ == pytest.approx(y_mean)</span>

    <span class="s5"># Transform on test data, the last value is unknown so it is encoded as the target</span>
    <span class="s5"># mean</span>
    <span class="s1">expected_X_test_transform = np.concatenate(</span>
        <span class="s1">(expected_encodings</span><span class="s0">, </span><span class="s1">np.array([y_mean]))</span>
    <span class="s1">).reshape(-</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s1">X_test_transform = target_encoder.transform(X_test)</span>
    <span class="s1">assert_allclose(X_test_transform</span><span class="s0">, </span><span class="s1">expected_X_test_transform)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;X, categories&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">np.array([[</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">10 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">10 </span><span class="s1">+ [</span><span class="s4">3</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">dtype=np.int64).T</span><span class="s0">,  </span><span class="s5"># 3 is unknown</span>
            <span class="s1">[[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]]</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">np.array(</span>
                <span class="s1">[[</span><span class="s3">&quot;cat&quot;</span><span class="s1">] * </span><span class="s4">10 </span><span class="s1">+ [</span><span class="s3">&quot;dog&quot;</span><span class="s1">] * </span><span class="s4">10 </span><span class="s1">+ [</span><span class="s3">&quot;snake&quot;</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">dtype=object</span>
            <span class="s1">).T</span><span class="s0">,  </span><span class="s5"># snake is unknown</span>
            <span class="s1">[[</span><span class="s3">&quot;dog&quot;</span><span class="s0">, </span><span class="s3">&quot;cat&quot;</span><span class="s0">, </span><span class="s3">&quot;cow&quot;</span><span class="s1">]]</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;smooth&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">4.0</span><span class="s0">, </span><span class="s3">&quot;auto&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_custom_categories(X</span><span class="s0">, </span><span class="s1">categories</span><span class="s0">, </span><span class="s1">smooth):</span>
    <span class="s2">&quot;&quot;&quot;Custom categories with unknown categories that are not in training data.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">y = rng.uniform(low=-</span><span class="s4">10</span><span class="s0">, </span><span class="s1">high=</span><span class="s4">20</span><span class="s0">, </span><span class="s1">size=X.shape[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">enc = TargetEncoder(categories=categories</span><span class="s0">, </span><span class="s1">smooth=smooth</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s5"># The last element is unknown and encoded as the mean</span>
    <span class="s1">y_mean = y.mean()</span>
    <span class="s1">X_trans = enc.transform(X[-</span><span class="s4">1</span><span class="s1">:])</span>
    <span class="s0">assert </span><span class="s1">X_trans[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">] == pytest.approx(y_mean)</span>

    <span class="s0">assert </span><span class="s1">len(enc.encodings_) == </span><span class="s4">1</span>
    <span class="s5"># custom category that is not in training data</span>
    <span class="s0">assert </span><span class="s1">enc.encodings_[</span><span class="s4">0</span><span class="s1">][-</span><span class="s4">1</span><span class="s1">] == pytest.approx(y_mean)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;y, msg&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">([</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s3">&quot;Found input variables with inconsistent&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">np.array([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s4">3</span><span class="s1">]]).T</span><span class="s0">,</span>
            <span class="s3">&quot;Target type was inferred to be 'multiclass-multioutput'&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">([</span><span class="s3">&quot;cat&quot;</span><span class="s0">, </span><span class="s3">&quot;dog&quot;</span><span class="s0">, </span><span class="s3">&quot;bear&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s3">&quot;Target type was inferred to be 'multiclass'&quot;</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_errors(y</span><span class="s0">, </span><span class="s1">msg):</span>
    <span class="s2">&quot;&quot;&quot;Check invalidate input.&quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]).T</span>

    <span class="s1">enc = TargetEncoder()</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">enc.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_use_regression_target():</span>
    <span class="s2">&quot;&quot;&quot;Custom target_type to avoid inferring the target type.&quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]).T</span>

    <span class="s5"># XXX: When multiclass is supported, then the following `y`</span>
    <span class="s5"># is considered a multiclass problem and `TargetEncoder` will not error.</span>
    <span class="s5"># type_of_target would be 'multiclass'</span>
    <span class="s1">y = np.array([</span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">2.0</span><span class="s0">, </span><span class="s4">3.0</span><span class="s0">, </span><span class="s4">2.0</span><span class="s0">, </span><span class="s4">3.0</span><span class="s0">, </span><span class="s4">4.0</span><span class="s1">])</span>
    <span class="s1">enc = TargetEncoder()</span>
    <span class="s1">msg = </span><span class="s3">&quot;Target type was inferred to be 'multiclass'&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">enc.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">enc = TargetEncoder(target_type=</span><span class="s3">&quot;continuous&quot;</span><span class="s1">)</span>
    <span class="s1">enc.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">enc.target_type_ == </span><span class="s3">&quot;continuous&quot;</span>


<span class="s0">def </span><span class="s1">test_feature_names_out_set_output():</span>
    <span class="s2">&quot;&quot;&quot;Check TargetEncoder works with set_output.&quot;&quot;&quot;</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s3">&quot;pandas&quot;</span><span class="s1">)</span>

    <span class="s1">X_df = pd.DataFrame({</span><span class="s3">&quot;A&quot;</span><span class="s1">: [</span><span class="s3">&quot;a&quot;</span><span class="s0">, </span><span class="s3">&quot;b&quot;</span><span class="s1">] * </span><span class="s4">10</span><span class="s0">, </span><span class="s3">&quot;B&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">] * </span><span class="s4">10</span><span class="s1">})</span>
    <span class="s1">y = [</span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">] * </span><span class="s4">10</span>

    <span class="s1">enc_default = TargetEncoder(cv=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">smooth=</span><span class="s4">3.0</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">enc_default.set_output(transform=</span><span class="s3">&quot;default&quot;</span><span class="s1">)</span>
    <span class="s1">enc_pandas = TargetEncoder(cv=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">smooth=</span><span class="s4">3.0</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">enc_pandas.set_output(transform=</span><span class="s3">&quot;pandas&quot;</span><span class="s1">)</span>

    <span class="s1">X_default = enc_default.fit_transform(X_df</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">X_pandas = enc_pandas.fit_transform(X_df</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">assert_allclose(X_pandas.to_numpy()</span><span class="s0">, </span><span class="s1">X_default)</span>
    <span class="s1">assert_array_equal(enc_pandas.get_feature_names_out()</span><span class="s0">, </span><span class="s1">[</span><span class="s3">&quot;A&quot;</span><span class="s0">, </span><span class="s3">&quot;B&quot;</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(enc_pandas.get_feature_names_out()</span><span class="s0">, </span><span class="s1">X_pandas.columns)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;to_pandas&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;smooth&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1.0</span><span class="s0">, </span><span class="s3">&quot;auto&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;target_type&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">&quot;binary-ints&quot;</span><span class="s0">, </span><span class="s3">&quot;binary-str&quot;</span><span class="s0">, </span><span class="s3">&quot;continuous&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_multiple_features_quick(to_pandas</span><span class="s0">, </span><span class="s1">smooth</span><span class="s0">, </span><span class="s1">target_type):</span>
    <span class="s2">&quot;&quot;&quot;Check target encoder with multiple features.&quot;&quot;&quot;</span>
    <span class="s1">X_ordinal = np.array(</span>
        <span class="s1">[[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">2</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">dtype=np.int64</span>
    <span class="s1">)</span>
    <span class="s0">if </span><span class="s1">target_type == </span><span class="s3">&quot;binary-str&quot;</span><span class="s1">:</span>
        <span class="s1">y_train = np.array([</span><span class="s3">&quot;a&quot;</span><span class="s0">, </span><span class="s3">&quot;b&quot;</span><span class="s0">, </span><span class="s3">&quot;a&quot;</span><span class="s0">, </span><span class="s3">&quot;a&quot;</span><span class="s0">, </span><span class="s3">&quot;b&quot;</span><span class="s0">, </span><span class="s3">&quot;b&quot;</span><span class="s0">, </span><span class="s3">&quot;a&quot;</span><span class="s0">, </span><span class="s3">&quot;b&quot;</span><span class="s1">])</span>
        <span class="s1">y_integer = LabelEncoder().fit_transform(y_train)</span>
        <span class="s1">cv = StratifiedKFold(</span><span class="s4">2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s0">elif </span><span class="s1">target_type == </span><span class="s3">&quot;binary-ints&quot;</span><span class="s1">:</span>
        <span class="s1">y_train = np.array([</span><span class="s4">3</span><span class="s0">, </span><span class="s4">4</span><span class="s0">, </span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s0">, </span><span class="s4">3</span><span class="s0">, </span><span class="s4">4</span><span class="s0">, </span><span class="s4">4</span><span class="s0">, </span><span class="s4">4</span><span class="s1">])</span>
        <span class="s1">y_integer = LabelEncoder().fit_transform(y_train)</span>
        <span class="s1">cv = StratifiedKFold(</span><span class="s4">2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y_train = np.array([</span><span class="s4">3.0</span><span class="s0">, </span><span class="s4">5.1</span><span class="s0">, </span><span class="s4">2.4</span><span class="s0">, </span><span class="s4">3.5</span><span class="s0">, </span><span class="s4">4.1</span><span class="s0">, </span><span class="s4">5.5</span><span class="s0">, </span><span class="s4">10.3</span><span class="s0">, </span><span class="s4">7.3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=np.float32)</span>
        <span class="s1">y_integer = y_train</span>
        <span class="s1">cv = KFold(</span><span class="s4">2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">y_mean = np.mean(y_integer)</span>
    <span class="s1">categories = [[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span>

    <span class="s1">X_test = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s4">3</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">,  </span><span class="s5"># 3 is unknown</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s0">, </span><span class="s4">10</span><span class="s1">]</span><span class="s0">,  </span><span class="s5"># 10 is unknown</span>
        <span class="s1">]</span><span class="s0">,</span>
        <span class="s1">dtype=np.int64</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">if </span><span class="s1">to_pandas:</span>
        <span class="s1">pd = pytest.importorskip(</span><span class="s3">&quot;pandas&quot;</span><span class="s1">)</span>
        <span class="s5"># convert second feature to an object</span>
        <span class="s1">X_train = pd.DataFrame(</span>
            <span class="s1">{</span>
                <span class="s3">&quot;feat0&quot;</span><span class="s1">: X_ordinal[:</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">,</span>
                <span class="s3">&quot;feat1&quot;</span><span class="s1">: np.array([</span><span class="s3">&quot;cat&quot;</span><span class="s0">, </span><span class="s3">&quot;dog&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=object)[X_ordinal[:</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s0">,</span>
            <span class="s1">}</span>
        <span class="s1">)</span>
        <span class="s5"># &quot;snake&quot; is unknown</span>
        <span class="s1">X_test = pd.DataFrame({</span><span class="s3">&quot;feat0&quot;</span><span class="s1">: X_test[:</span><span class="s0">, </span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s3">&quot;feat1&quot;</span><span class="s1">: [</span><span class="s3">&quot;dog&quot;</span><span class="s0">, </span><span class="s3">&quot;cat&quot;</span><span class="s0">, </span><span class="s3">&quot;snake&quot;</span><span class="s1">]})</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">X_train = X_ordinal</span>

    <span class="s5"># manually compute encoding for fit_transform</span>
    <span class="s1">expected_X_fit_transform = np.empty_like(X_ordinal</span><span class="s0">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s0">for </span><span class="s1">f_idx</span><span class="s0">, </span><span class="s1">cats </span><span class="s0">in </span><span class="s1">enumerate(categories):</span>
        <span class="s0">for </span><span class="s1">train_idx</span><span class="s0">, </span><span class="s1">test_idx </span><span class="s0">in </span><span class="s1">cv.split(X_ordinal</span><span class="s0">, </span><span class="s1">y_integer):</span>
            <span class="s1">X_</span><span class="s0">, </span><span class="s1">y_ = X_ordinal[train_idx</span><span class="s0">, </span><span class="s1">f_idx]</span><span class="s0">, </span><span class="s1">y_integer[train_idx]</span>
            <span class="s1">current_encoding = _encode_target(X_</span><span class="s0">, </span><span class="s1">y_</span><span class="s0">, </span><span class="s1">len(cats)</span><span class="s0">, </span><span class="s1">smooth)</span>
            <span class="s1">expected_X_fit_transform[test_idx</span><span class="s0">, </span><span class="s1">f_idx] = current_encoding[</span>
                <span class="s1">X_ordinal[test_idx</span><span class="s0">, </span><span class="s1">f_idx]</span>
            <span class="s1">]</span>

    <span class="s5"># manually compute encoding for transform</span>
    <span class="s1">expected_encodings = []</span>
    <span class="s0">for </span><span class="s1">f_idx</span><span class="s0">, </span><span class="s1">cats </span><span class="s0">in </span><span class="s1">enumerate(categories):</span>
        <span class="s1">current_encoding = _encode_target(</span>
            <span class="s1">X_ordinal[:</span><span class="s0">, </span><span class="s1">f_idx]</span><span class="s0">, </span><span class="s1">y_integer</span><span class="s0">, </span><span class="s1">len(cats)</span><span class="s0">, </span><span class="s1">smooth</span>
        <span class="s1">)</span>
        <span class="s1">expected_encodings.append(current_encoding)</span>

    <span class="s1">expected_X_test_transform = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[expected_encodings[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">expected_encodings[</span><span class="s4">1</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]]</span><span class="s0">,</span>
            <span class="s1">[y_mean</span><span class="s0">, </span><span class="s1">expected_encodings[</span><span class="s4">1</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]]</span><span class="s0">,</span>
            <span class="s1">[expected_encodings[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y_mean]</span><span class="s0">,</span>
        <span class="s1">]</span><span class="s0">,</span>
        <span class="s1">dtype=np.float64</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">enc = TargetEncoder(smooth=smooth</span><span class="s0">, </span><span class="s1">cv=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_fit_transform = enc.fit_transform(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">assert_allclose(X_fit_transform</span><span class="s0">, </span><span class="s1">expected_X_fit_transform)</span>

    <span class="s0">assert </span><span class="s1">len(enc.encodings_) == </span><span class="s4">2</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">2</span><span class="s1">):</span>
        <span class="s1">assert_allclose(enc.encodings_[i]</span><span class="s0">, </span><span class="s1">expected_encodings[i])</span>

    <span class="s1">X_test_transform = enc.transform(X_test)</span>
    <span class="s1">assert_allclose(X_test_transform</span><span class="s0">, </span><span class="s1">expected_X_test_transform)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;y, y_mean&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(np.array([</span><span class="s4">3.4</span><span class="s1">] * </span><span class="s4">20</span><span class="s1">)</span><span class="s0">, </span><span class="s4">3.4</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(np.array([</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">20</span><span class="s1">)</span><span class="s0">, </span><span class="s4">0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(np.array([</span><span class="s3">&quot;a&quot;</span><span class="s1">] * </span><span class="s4">20</span><span class="s0">, </span><span class="s1">dtype=object)</span><span class="s0">, </span><span class="s4">0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
    <span class="s1">ids=[</span><span class="s3">&quot;continuous&quot;</span><span class="s0">, </span><span class="s3">&quot;binary&quot;</span><span class="s0">, </span><span class="s3">&quot;binary-string&quot;</span><span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;smooth&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">&quot;auto&quot;</span><span class="s0">, </span><span class="s4">4.0</span><span class="s0">, </span><span class="s4">0.0</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_constant_target_and_feature(y</span><span class="s0">, </span><span class="s1">y_mean</span><span class="s0">, </span><span class="s1">smooth):</span>
    <span class="s2">&quot;&quot;&quot;Check edge case where feature and target is constant.&quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">20</span><span class="s1">]).T</span>
    <span class="s1">n_samples = X.shape[</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s1">enc = TargetEncoder(cv=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">smooth=smooth</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_trans = enc.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_allclose(X_trans</span><span class="s0">, </span><span class="s1">np.repeat([[y_mean]]</span><span class="s0">, </span><span class="s1">n_samples</span><span class="s0">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">))</span>
    <span class="s0">assert </span><span class="s1">enc.encodings_[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">] == pytest.approx(y_mean)</span>
    <span class="s0">assert </span><span class="s1">enc.target_mean_ == pytest.approx(y_mean)</span>

    <span class="s1">X_test = np.array([[</span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]])</span>
    <span class="s1">X_test_trans = enc.transform(X_test)</span>
    <span class="s1">assert_allclose(X_test_trans</span><span class="s0">, </span><span class="s1">np.repeat([[y_mean]]</span><span class="s0">, </span><span class="s4">2</span><span class="s0">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">))</span>


<span class="s0">def </span><span class="s1">test_fit_transform_not_associated_with_y_if_ordinal_categorical_is_not(</span>
    <span class="s1">global_random_seed</span><span class="s0">,</span>
<span class="s1">):</span>
    <span class="s1">cardinality = </span><span class="s4">30  </span><span class="s5"># not too large, otherwise we need a very large n_samples</span>
    <span class="s1">n_samples = </span><span class="s4">3000</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">y_train = rng.normal(size=n_samples)</span>
    <span class="s1">X_train = rng.randint(</span><span class="s4">0</span><span class="s0">, </span><span class="s1">cardinality</span><span class="s0">, </span><span class="s1">size=n_samples).reshape(-</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s5"># Sort by y_train to attempt to cause a leak</span>
    <span class="s1">y_sorted_indices = y_train.argsort()</span>
    <span class="s1">y_train = y_train[y_sorted_indices]</span>
    <span class="s1">X_train = X_train[y_sorted_indices]</span>

    <span class="s1">target_encoder = TargetEncoder(shuffle=</span><span class="s0">True, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s1">X_encoded_train_shuffled = target_encoder.fit_transform(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s1">target_encoder = TargetEncoder(shuffle=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">X_encoded_train_no_shuffled = target_encoder.fit_transform(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s5"># Check that no information about y_train has leaked into X_train:</span>
    <span class="s1">regressor = RandomForestRegressor(</span>
        <span class="s1">n_estimators=</span><span class="s4">10</span><span class="s0">, </span><span class="s1">min_samples_leaf=</span><span class="s4">20</span><span class="s0">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>

    <span class="s5"># It's impossible to learn a good predictive model on the training set when</span>
    <span class="s5"># using the original representation X_train or the target encoded</span>
    <span class="s5"># representation with shuffled inner CV. For the latter, no information</span>
    <span class="s5"># about y_train has inadvertently leaked into the prior used to generate</span>
    <span class="s5"># `X_encoded_train_shuffled`:</span>
    <span class="s1">cv = ShuffleSplit(n_splits=</span><span class="s4">50</span><span class="s0">, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s0">assert </span><span class="s1">cross_val_score(regressor</span><span class="s0">, </span><span class="s1">X_train</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">cv=cv).mean() &lt; </span><span class="s4">0.1</span>
    <span class="s0">assert </span><span class="s1">(</span>
        <span class="s1">cross_val_score(regressor</span><span class="s0">, </span><span class="s1">X_encoded_train_shuffled</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">cv=cv).mean()</span>
        <span class="s1">&lt; </span><span class="s4">0.1</span>
    <span class="s1">)</span>

    <span class="s5"># Without the inner CV shuffling, a lot of information about y_train goes into the</span>
    <span class="s5"># the per-fold y_train.mean() priors: shrinkage is no longer effective in this</span>
    <span class="s5"># case and would no longer be able to prevent downstream over-fitting.</span>
    <span class="s0">assert </span><span class="s1">(</span>
        <span class="s1">cross_val_score(regressor</span><span class="s0">, </span><span class="s1">X_encoded_train_no_shuffled</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">cv=cv).mean()</span>
        <span class="s1">&gt; </span><span class="s4">0.5</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_smooth_zero():</span>
    <span class="s2">&quot;&quot;&quot;Check edge case with zero smoothing and cv does not contain category.&quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[</span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">0</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">]]).T</span>
    <span class="s1">y = np.array([</span><span class="s4">2.1</span><span class="s0">, </span><span class="s4">4.3</span><span class="s0">, </span><span class="s4">1.2</span><span class="s0">, </span><span class="s4">3.1</span><span class="s0">, </span><span class="s4">1.0</span><span class="s0">, </span><span class="s4">9.0</span><span class="s0">, </span><span class="s4">10.3</span><span class="s0">, </span><span class="s4">14.2</span><span class="s0">, </span><span class="s4">13.3</span><span class="s0">, </span><span class="s4">15.0</span><span class="s1">])</span>

    <span class="s1">enc = TargetEncoder(smooth=</span><span class="s4">0.0</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False, </span><span class="s1">cv=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">X_trans = enc.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s5"># With cv = 2, category 0 does not exist in the second half, thus</span>
    <span class="s5"># it will be encoded as the mean of the second half</span>
    <span class="s1">assert_allclose(X_trans[</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.mean(y[</span><span class="s4">5</span><span class="s1">:]))</span>

    <span class="s5"># category 1 does not exist in the first half, thus it will be encoded as</span>
    <span class="s5"># the mean of the first half</span>
    <span class="s1">assert_allclose(X_trans[-</span><span class="s4">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.mean(y[:</span><span class="s4">5</span><span class="s1">]))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;smooth&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0.0</span><span class="s0">, </span><span class="s4">1e3</span><span class="s0">, </span><span class="s3">&quot;auto&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_invariance_of_encoding_under_label_permutation(smooth</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s5"># Check that the encoding does not depend on the integer of the value of</span>
    <span class="s5"># the integer labels. This is quite a trivial property but it is helpful</span>
    <span class="s5"># to understand the following test.</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>

    <span class="s5"># Random y and informative categorical X to make the test non-trivial when</span>
    <span class="s5"># using smoothing.</span>
    <span class="s1">y = rng.normal(size=</span><span class="s4">1000</span><span class="s1">)</span>
    <span class="s1">n_categories = </span><span class="s4">30</span>
    <span class="s1">X = KBinsDiscretizer(n_bins=n_categories</span><span class="s0">, </span><span class="s1">encode=</span><span class="s3">&quot;ordinal&quot;</span><span class="s1">).fit_transform(</span>
        <span class="s1">y.reshape(-</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">)</span>

    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>

    <span class="s5"># Shuffle the labels to make sure that the encoding is invariant to the</span>
    <span class="s5"># permutation of the labels</span>
    <span class="s1">permutated_labels = rng.permutation(n_categories)</span>
    <span class="s1">X_train_permuted = permutated_labels[X_train.astype(np.int32)]</span>
    <span class="s1">X_test_permuted = permutated_labels[X_test.astype(np.int32)]</span>

    <span class="s1">target_encoder = TargetEncoder(smooth=smooth</span><span class="s0">, </span><span class="s1">random_state=global_random_seed)</span>
    <span class="s1">X_train_encoded = target_encoder.fit_transform(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">X_test_encoded = target_encoder.transform(X_test)</span>

    <span class="s1">X_train_permuted_encoded = target_encoder.fit_transform(X_train_permuted</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s1">X_test_permuted_encoded = target_encoder.transform(X_test_permuted)</span>

    <span class="s1">assert_allclose(X_train_encoded</span><span class="s0">, </span><span class="s1">X_train_permuted_encoded)</span>
    <span class="s1">assert_allclose(X_test_encoded</span><span class="s0">, </span><span class="s1">X_test_permuted_encoded)</span>


<span class="s5"># TODO(1.5) remove warning filter when kbd's subsample default is changed</span>
<span class="s1">@pytest.mark.filterwarnings(</span><span class="s3">&quot;ignore:In version 1.5 onwards, subsample=200_000&quot;</span><span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;smooth&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">0.0</span><span class="s0">, </span><span class="s3">&quot;auto&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_target_encoding_for_linear_regression(smooth</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s5"># Check some expected statistical properties when fitting a linear</span>
    <span class="s5"># regression model on target encoded features depending on their relation</span>
    <span class="s5"># with that target.</span>

    <span class="s5"># In this test, we use the Ridge class with the &quot;lsqr&quot; solver and a little</span>
    <span class="s5"># bit of regularization to implement a linear regression model that</span>
    <span class="s5"># converges quickly for large `n_samples` and robustly in case of</span>
    <span class="s5"># correlated features. Since we will fit this model on a mean centered</span>
    <span class="s5"># target, we do not need to fit an intercept and this will help simplify</span>
    <span class="s5"># the analysis with respect to the expected coefficients.</span>
    <span class="s1">linear_regression = Ridge(alpha=</span><span class="s4">1e-6</span><span class="s0">, </span><span class="s1">solver=</span><span class="s3">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s5"># Construct a random target variable. We need a large number of samples for</span>
    <span class="s5"># this test to be stable across all values of the random seed.</span>
    <span class="s1">n_samples = </span><span class="s4">50_000</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">y = rng.randn(n_samples)</span>

    <span class="s5"># Generate a single informative ordinal feature with medium cardinality.</span>
    <span class="s5"># Inject some irreducible noise to make it harder for a multivariate model</span>
    <span class="s5"># to identify the informative feature from other pure noise features.</span>
    <span class="s1">noise = </span><span class="s4">0.8 </span><span class="s1">* rng.randn(n_samples)</span>
    <span class="s1">n_categories = </span><span class="s4">100</span>
    <span class="s1">X_informative = KBinsDiscretizer(</span>
        <span class="s1">n_bins=n_categories</span><span class="s0">,</span>
        <span class="s1">encode=</span><span class="s3">&quot;ordinal&quot;</span><span class="s0">,</span>
        <span class="s1">strategy=</span><span class="s3">&quot;uniform&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=rng</span><span class="s0">,</span>
    <span class="s1">).fit_transform((y + noise).reshape(-</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">))</span>

    <span class="s5"># Let's permute the labels to hide the fact that this feature is</span>
    <span class="s5"># informative to naive linear regression model trained on the raw ordinal</span>
    <span class="s5"># values. As highlighted in the previous test, the target encoding should be</span>
    <span class="s5"># invariant to such a permutation.</span>
    <span class="s1">permutated_labels = rng.permutation(n_categories)</span>
    <span class="s1">X_informative = permutated_labels[X_informative.astype(np.int32)]</span>

    <span class="s5"># Generate a shuffled copy of the informative feature to destroy the</span>
    <span class="s5"># relationship with the target.</span>
    <span class="s1">X_shuffled = rng.permutation(X_informative)</span>

    <span class="s5"># Also include a very high cardinality categorical feature that is by</span>
    <span class="s5"># itself independent of the target variable: target encoding such a feature</span>
    <span class="s5"># without internal cross-validation should cause catastrophic overfitting</span>
    <span class="s5"># for the downstream regressor, even with shrinkage. This kind of features</span>
    <span class="s5"># typically represents near unique identifiers of samples. In general they</span>
    <span class="s5"># should be removed from a machine learning datasets but here we want to</span>
    <span class="s5"># study the ability of the default behavior of TargetEncoder to mitigate</span>
    <span class="s5"># them automatically.</span>
    <span class="s1">X_near_unique_categories = rng.choice(</span>
        <span class="s1">int(</span><span class="s4">0.9 </span><span class="s1">* n_samples)</span><span class="s0">, </span><span class="s1">size=n_samples</span><span class="s0">, </span><span class="s1">replace=</span><span class="s0">True</span>
    <span class="s1">).reshape(-</span><span class="s4">1</span><span class="s0">, </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s5"># Assemble the dataset and do a train-test split:</span>
    <span class="s1">X = np.concatenate(</span>
        <span class="s1">[X_informative</span><span class="s0">, </span><span class="s1">X_shuffled</span><span class="s0">, </span><span class="s1">X_near_unique_categories]</span><span class="s0">,</span>
        <span class="s1">axis=</span><span class="s4">1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X_train</span><span class="s0">, </span><span class="s1">X_test</span><span class="s0">, </span><span class="s1">y_train</span><span class="s0">, </span><span class="s1">y_test = train_test_split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s5"># Let's first check that a linear regression model trained on the raw</span>
    <span class="s5"># features underfits because of the meaning-less ordinal encoding of the</span>
    <span class="s5"># labels.</span>
    <span class="s1">raw_model = linear_regression.fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>
    <span class="s0">assert </span><span class="s1">raw_model.score(X_train</span><span class="s0">, </span><span class="s1">y_train) &lt; </span><span class="s4">0.1</span>
    <span class="s0">assert </span><span class="s1">raw_model.score(X_test</span><span class="s0">, </span><span class="s1">y_test) &lt; </span><span class="s4">0.1</span>

    <span class="s5"># Now do the same with target encoding using the internal CV mechanism</span>
    <span class="s5"># implemented when using fit_transform.</span>
    <span class="s1">model_with_cv = make_pipeline(</span>
        <span class="s1">TargetEncoder(smooth=smooth</span><span class="s0">, </span><span class="s1">random_state=rng)</span><span class="s0">, </span><span class="s1">linear_regression</span>
    <span class="s1">).fit(X_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s5"># This model should be able to fit the data well and also generalise to the</span>
    <span class="s5"># test data (assuming that the binning is fine-grained enough). The R2</span>
    <span class="s5"># scores are not perfect because of the noise injected during the</span>
    <span class="s5"># generation of the unique informative feature.</span>
    <span class="s1">coef = model_with_cv[-</span><span class="s4">1</span><span class="s1">].coef_</span>
    <span class="s0">assert </span><span class="s1">model_with_cv.score(X_train</span><span class="s0">, </span><span class="s1">y_train) &gt; </span><span class="s4">0.5</span><span class="s0">, </span><span class="s1">coef</span>
    <span class="s0">assert </span><span class="s1">model_with_cv.score(X_test</span><span class="s0">, </span><span class="s1">y_test) &gt; </span><span class="s4">0.5</span><span class="s0">, </span><span class="s1">coef</span>

    <span class="s5"># The target encoder recovers the linear relationship with slope 1 between</span>
    <span class="s5"># the target encoded unique informative predictor and the target. Since the</span>
    <span class="s5"># target encoding of the 2 other features is not informative thanks to the</span>
    <span class="s5"># use of internal cross-validation, the multivariate linear regressor</span>
    <span class="s5"># assigns a coef of 1 to the first feature and 0 to the other 2.</span>
    <span class="s0">assert </span><span class="s1">coef[</span><span class="s4">0</span><span class="s1">] == pytest.approx(</span><span class="s4">1</span><span class="s0">, </span><span class="s1">abs=</span><span class="s4">1e-2</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">(np.abs(coef[</span><span class="s4">1</span><span class="s1">:]) &lt; </span><span class="s4">0.2</span><span class="s1">).all()</span>

    <span class="s5"># Let's now disable the internal cross-validation by calling fit and then</span>
    <span class="s5"># transform separately on the training set:</span>
    <span class="s1">target_encoder = TargetEncoder(smooth=smooth</span><span class="s0">, </span><span class="s1">random_state=rng).fit(</span>
        <span class="s1">X_train</span><span class="s0">, </span><span class="s1">y_train</span>
    <span class="s1">)</span>
    <span class="s1">X_enc_no_cv_train = target_encoder.transform(X_train)</span>
    <span class="s1">X_enc_no_cv_test = target_encoder.transform(X_test)</span>
    <span class="s1">model_no_cv = linear_regression.fit(X_enc_no_cv_train</span><span class="s0">, </span><span class="s1">y_train)</span>

    <span class="s5"># The linear regression model should always overfit because it assigns</span>
    <span class="s5"># too much weight to the extremely high cardinality feature relatively to</span>
    <span class="s5"># the informative feature. Note that this is the case even when using</span>
    <span class="s5"># the empirical Bayes smoothing which is not enough to prevent such</span>
    <span class="s5"># overfitting alone.</span>
    <span class="s1">coef = model_no_cv.coef_</span>
    <span class="s0">assert </span><span class="s1">model_no_cv.score(X_enc_no_cv_train</span><span class="s0">, </span><span class="s1">y_train) &gt; </span><span class="s4">0.7</span><span class="s0">, </span><span class="s1">coef</span>
    <span class="s0">assert </span><span class="s1">model_no_cv.score(X_enc_no_cv_test</span><span class="s0">, </span><span class="s1">y_test) &lt; </span><span class="s4">0.5</span><span class="s0">, </span><span class="s1">coef</span>

    <span class="s5"># The model overfits because it assigns too much weight to the high</span>
    <span class="s5"># cardinality yet non-informative feature instead of the lower</span>
    <span class="s5"># cardinality yet informative feature:</span>
    <span class="s0">assert </span><span class="s1">abs(coef[</span><span class="s4">0</span><span class="s1">]) &lt; abs(coef[</span><span class="s4">2</span><span class="s1">])</span>
</pre>
</body>
</html>