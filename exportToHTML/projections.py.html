<html>
<head>
<title>projections.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #808080;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
projections.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Basic linear factorizations needed by the solver.&quot;&quot;&quot;</span>

<span class="s2">from </span><span class="s1">scipy.sparse </span><span class="s2">import </span><span class="s1">(bmat</span><span class="s2">, </span><span class="s1">csc_matrix</span><span class="s2">, </span><span class="s1">eye</span><span class="s2">, </span><span class="s1">issparse)</span>
<span class="s2">from </span><span class="s1">scipy.sparse.linalg </span><span class="s2">import </span><span class="s1">LinearOperator</span>
<span class="s2">import </span><span class="s1">scipy.linalg</span>
<span class="s2">import </span><span class="s1">scipy.sparse.linalg</span>
<span class="s2">try</span><span class="s1">:</span>
    <span class="s2">from </span><span class="s1">sksparse.cholmod </span><span class="s2">import </span><span class="s1">cholesky_AAt</span>
    <span class="s1">sksparse_available = </span><span class="s2">True</span>
<span class="s2">except </span><span class="s1">ImportError:</span>
    <span class="s2">import </span><span class="s1">warnings</span>
    <span class="s1">sksparse_available = </span><span class="s2">False</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">warnings </span><span class="s2">import </span><span class="s1">warn</span>

<span class="s1">__all__ = [</span>
    <span class="s3">'orthogonality'</span><span class="s2">,</span>
    <span class="s3">'projections'</span><span class="s2">,</span>
<span class="s1">]</span>


<span class="s2">def </span><span class="s1">orthogonality(A</span><span class="s2">, </span><span class="s1">g):</span>
    <span class="s0">&quot;&quot;&quot;Measure orthogonality between a vector and the null space of a matrix. 
 
    Compute a measure of orthogonality between the null space 
    of the (possibly sparse) matrix ``A`` and a given vector ``g``. 
 
    The formula is a simplified (and cheaper) version of formula (3.13) 
    from [1]_. 
    ``orth =  norm(A g, ord=2)/(norm(A, ord='fro')*norm(g, ord=2))``. 
 
    References 
    ---------- 
    .. [1] Gould, Nicholas IM, Mary E. Hribar, and Jorge Nocedal. 
           &quot;On the solution of equality constrained quadratic 
            programming problems arising in optimization.&quot; 
            SIAM Journal on Scientific Computing 23.4 (2001): 1376-1395. 
    &quot;&quot;&quot;</span>
    <span class="s4"># Compute vector norms</span>
    <span class="s1">norm_g = np.linalg.norm(g)</span>
    <span class="s4"># Compute Froebnius norm of the matrix A</span>
    <span class="s2">if </span><span class="s1">issparse(A):</span>
        <span class="s1">norm_A = scipy.sparse.linalg.norm(A</span><span class="s2">, </span><span class="s1">ord=</span><span class="s3">'fro'</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">norm_A = np.linalg.norm(A</span><span class="s2">, </span><span class="s1">ord=</span><span class="s3">'fro'</span><span class="s1">)</span>

    <span class="s4"># Check if norms are zero</span>
    <span class="s2">if </span><span class="s1">norm_g == </span><span class="s5">0 </span><span class="s2">or </span><span class="s1">norm_A == </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s5">0</span>

    <span class="s1">norm_A_g = np.linalg.norm(A.dot(g))</span>
    <span class="s4"># Orthogonality measure</span>
    <span class="s1">orth = norm_A_g / (norm_A*norm_g)</span>
    <span class="s2">return </span><span class="s1">orth</span>


<span class="s2">def </span><span class="s1">normal_equation_projections(A</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">orth_tol</span><span class="s2">, </span><span class="s1">max_refin</span><span class="s2">, </span><span class="s1">tol):</span>
    <span class="s0">&quot;&quot;&quot;Return linear operators for matrix A using ``NormalEquation`` approach. 
    &quot;&quot;&quot;</span>
    <span class="s4"># Cholesky factorization</span>
    <span class="s1">factor = cholesky_AAt(A)</span>

    <span class="s4"># z = x - A.T inv(A A.T) A x</span>
    <span class="s2">def </span><span class="s1">null_space(x):</span>
        <span class="s1">v = factor(A.dot(x))</span>
        <span class="s1">z = x - A.T.dot(v)</span>

        <span class="s4"># Iterative refinement to improve roundoff</span>
        <span class="s4"># errors described in [2]_, algorithm 5.1.</span>
        <span class="s1">k = </span><span class="s5">0</span>
        <span class="s2">while </span><span class="s1">orthogonality(A</span><span class="s2">, </span><span class="s1">z) &gt; orth_tol:</span>
            <span class="s2">if </span><span class="s1">k &gt;= max_refin:</span>
                <span class="s2">break</span>
            <span class="s4"># z_next = z - A.T inv(A A.T) A z</span>
            <span class="s1">v = factor(A.dot(z))</span>
            <span class="s1">z = z - A.T.dot(v)</span>
            <span class="s1">k += </span><span class="s5">1</span>

        <span class="s2">return </span><span class="s1">z</span>

    <span class="s4"># z = inv(A A.T) A x</span>
    <span class="s2">def </span><span class="s1">least_squares(x):</span>
        <span class="s2">return </span><span class="s1">factor(A.dot(x))</span>

    <span class="s4"># z = A.T inv(A A.T) x</span>
    <span class="s2">def </span><span class="s1">row_space(x):</span>
        <span class="s2">return </span><span class="s1">A.T.dot(factor(x))</span>

    <span class="s2">return </span><span class="s1">null_space</span><span class="s2">, </span><span class="s1">least_squares</span><span class="s2">, </span><span class="s1">row_space</span>


<span class="s2">def </span><span class="s1">augmented_system_projections(A</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">orth_tol</span><span class="s2">, </span><span class="s1">max_refin</span><span class="s2">, </span><span class="s1">tol):</span>
    <span class="s0">&quot;&quot;&quot;Return linear operators for matrix A - ``AugmentedSystem``.&quot;&quot;&quot;</span>
    <span class="s4"># Form augmented system</span>
    <span class="s1">K = csc_matrix(bmat([[eye(n)</span><span class="s2">, </span><span class="s1">A.T]</span><span class="s2">, </span><span class="s1">[A</span><span class="s2">, None</span><span class="s1">]]))</span>
    <span class="s4"># LU factorization</span>
    <span class="s4"># TODO: Use a symmetric indefinite factorization</span>
    <span class="s4">#       to solve the system twice as fast (because</span>
    <span class="s4">#       of the symmetry).</span>
    <span class="s2">try</span><span class="s1">:</span>
        <span class="s1">solve = scipy.sparse.linalg.factorized(K)</span>
    <span class="s2">except </span><span class="s1">RuntimeError:</span>
        <span class="s1">warn(</span><span class="s3">&quot;Singular Jacobian matrix. Using dense SVD decomposition to &quot;</span>
             <span class="s3">&quot;perform the factorizations.&quot;</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">svd_factorization_projections(A.toarray()</span><span class="s2">,</span>
                                             <span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">orth_tol</span><span class="s2">,</span>
                                             <span class="s1">max_refin</span><span class="s2">, </span><span class="s1">tol)</span>

    <span class="s4"># z = x - A.T inv(A A.T) A x</span>
    <span class="s4"># is computed solving the extended system:</span>
    <span class="s4"># [I A.T] * [ z ] = [x]</span>
    <span class="s4"># [A  O ]   [aux]   [0]</span>
    <span class="s2">def </span><span class="s1">null_space(x):</span>
        <span class="s4"># v = [x]</span>
        <span class="s4">#     [0]</span>
        <span class="s1">v = np.hstack([x</span><span class="s2">, </span><span class="s1">np.zeros(m)])</span>
        <span class="s4"># lu_sol = [ z ]</span>
        <span class="s4">#          [aux]</span>
        <span class="s1">lu_sol = solve(v)</span>
        <span class="s1">z = lu_sol[:n]</span>

        <span class="s4"># Iterative refinement to improve roundoff</span>
        <span class="s4"># errors described in [2]_, algorithm 5.2.</span>
        <span class="s1">k = </span><span class="s5">0</span>
        <span class="s2">while </span><span class="s1">orthogonality(A</span><span class="s2">, </span><span class="s1">z) &gt; orth_tol:</span>
            <span class="s2">if </span><span class="s1">k &gt;= max_refin:</span>
                <span class="s2">break</span>
            <span class="s4"># new_v = [x] - [I A.T] * [ z ]</span>
            <span class="s4">#         [0]   [A  O ]   [aux]</span>
            <span class="s1">new_v = v - K.dot(lu_sol)</span>
            <span class="s4"># [I A.T] * [delta  z ] = new_v</span>
            <span class="s4"># [A  O ]   [delta aux]</span>
            <span class="s1">lu_update = solve(new_v)</span>
            <span class="s4">#  [ z ] += [delta  z ]</span>
            <span class="s4">#  [aux]    [delta aux]</span>
            <span class="s1">lu_sol += lu_update</span>
            <span class="s1">z = lu_sol[:n]</span>
            <span class="s1">k += </span><span class="s5">1</span>

        <span class="s4"># return z = x - A.T inv(A A.T) A x</span>
        <span class="s2">return </span><span class="s1">z</span>

    <span class="s4"># z = inv(A A.T) A x</span>
    <span class="s4"># is computed solving the extended system:</span>
    <span class="s4"># [I A.T] * [aux] = [x]</span>
    <span class="s4"># [A  O ]   [ z ]   [0]</span>
    <span class="s2">def </span><span class="s1">least_squares(x):</span>
        <span class="s4"># v = [x]</span>
        <span class="s4">#     [0]</span>
        <span class="s1">v = np.hstack([x</span><span class="s2">, </span><span class="s1">np.zeros(m)])</span>
        <span class="s4"># lu_sol = [aux]</span>
        <span class="s4">#          [ z ]</span>
        <span class="s1">lu_sol = solve(v)</span>
        <span class="s4"># return z = inv(A A.T) A x</span>
        <span class="s2">return </span><span class="s1">lu_sol[n:m+n]</span>

    <span class="s4"># z = A.T inv(A A.T) x</span>
    <span class="s4"># is computed solving the extended system:</span>
    <span class="s4"># [I A.T] * [ z ] = [0]</span>
    <span class="s4"># [A  O ]   [aux]   [x]</span>
    <span class="s2">def </span><span class="s1">row_space(x):</span>
        <span class="s4"># v = [0]</span>
        <span class="s4">#     [x]</span>
        <span class="s1">v = np.hstack([np.zeros(n)</span><span class="s2">, </span><span class="s1">x])</span>
        <span class="s4"># lu_sol = [ z ]</span>
        <span class="s4">#          [aux]</span>
        <span class="s1">lu_sol = solve(v)</span>
        <span class="s4"># return z = A.T inv(A A.T) x</span>
        <span class="s2">return </span><span class="s1">lu_sol[:n]</span>

    <span class="s2">return </span><span class="s1">null_space</span><span class="s2">, </span><span class="s1">least_squares</span><span class="s2">, </span><span class="s1">row_space</span>


<span class="s2">def </span><span class="s1">qr_factorization_projections(A</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">orth_tol</span><span class="s2">, </span><span class="s1">max_refin</span><span class="s2">, </span><span class="s1">tol):</span>
    <span class="s0">&quot;&quot;&quot;Return linear operators for matrix A using ``QRFactorization`` approach. 
    &quot;&quot;&quot;</span>
    <span class="s4"># QRFactorization</span>
    <span class="s1">Q</span><span class="s2">, </span><span class="s1">R</span><span class="s2">, </span><span class="s1">P = scipy.linalg.qr(A.T</span><span class="s2">, </span><span class="s1">pivoting=</span><span class="s2">True, </span><span class="s1">mode=</span><span class="s3">'economic'</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">np.linalg.norm(R[-</span><span class="s5">1</span><span class="s2">, </span><span class="s1">:]</span><span class="s2">, </span><span class="s1">np.inf) &lt; tol:</span>
        <span class="s1">warn(</span><span class="s3">'Singular Jacobian matrix. Using SVD decomposition to ' </span><span class="s1">+</span>
             <span class="s3">'perform the factorizations.'</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">svd_factorization_projections(A</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">,</span>
                                             <span class="s1">orth_tol</span><span class="s2">,</span>
                                             <span class="s1">max_refin</span><span class="s2">,</span>
                                             <span class="s1">tol)</span>

    <span class="s4"># z = x - A.T inv(A A.T) A x</span>
    <span class="s2">def </span><span class="s1">null_space(x):</span>
        <span class="s4"># v = P inv(R) Q.T x</span>
        <span class="s1">aux1 = Q.T.dot(x)</span>
        <span class="s1">aux2 = scipy.linalg.solve_triangular(R</span><span class="s2">, </span><span class="s1">aux1</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">v = np.zeros(m)</span>
        <span class="s1">v[P] = aux2</span>
        <span class="s1">z = x - A.T.dot(v)</span>

        <span class="s4"># Iterative refinement to improve roundoff</span>
        <span class="s4"># errors described in [2]_, algorithm 5.1.</span>
        <span class="s1">k = </span><span class="s5">0</span>
        <span class="s2">while </span><span class="s1">orthogonality(A</span><span class="s2">, </span><span class="s1">z) &gt; orth_tol:</span>
            <span class="s2">if </span><span class="s1">k &gt;= max_refin:</span>
                <span class="s2">break</span>
            <span class="s4"># v = P inv(R) Q.T x</span>
            <span class="s1">aux1 = Q.T.dot(z)</span>
            <span class="s1">aux2 = scipy.linalg.solve_triangular(R</span><span class="s2">, </span><span class="s1">aux1</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">False</span><span class="s1">)</span>
            <span class="s1">v[P] = aux2</span>
            <span class="s4"># z_next = z - A.T v</span>
            <span class="s1">z = z - A.T.dot(v)</span>
            <span class="s1">k += </span><span class="s5">1</span>

        <span class="s2">return </span><span class="s1">z</span>

    <span class="s4"># z = inv(A A.T) A x</span>
    <span class="s2">def </span><span class="s1">least_squares(x):</span>
        <span class="s4"># z = P inv(R) Q.T x</span>
        <span class="s1">aux1 = Q.T.dot(x)</span>
        <span class="s1">aux2 = scipy.linalg.solve_triangular(R</span><span class="s2">, </span><span class="s1">aux1</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">z = np.zeros(m)</span>
        <span class="s1">z[P] = aux2</span>
        <span class="s2">return </span><span class="s1">z</span>

    <span class="s4"># z = A.T inv(A A.T) x</span>
    <span class="s2">def </span><span class="s1">row_space(x):</span>
        <span class="s4"># z = Q inv(R.T) P.T x</span>
        <span class="s1">aux1 = x[P]</span>
        <span class="s1">aux2 = scipy.linalg.solve_triangular(R</span><span class="s2">, </span><span class="s1">aux1</span><span class="s2">,</span>
                                             <span class="s1">lower=</span><span class="s2">False,</span>
                                             <span class="s1">trans=</span><span class="s3">'T'</span><span class="s1">)</span>
        <span class="s1">z = Q.dot(aux2)</span>
        <span class="s2">return </span><span class="s1">z</span>

    <span class="s2">return </span><span class="s1">null_space</span><span class="s2">, </span><span class="s1">least_squares</span><span class="s2">, </span><span class="s1">row_space</span>


<span class="s2">def </span><span class="s1">svd_factorization_projections(A</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">orth_tol</span><span class="s2">, </span><span class="s1">max_refin</span><span class="s2">, </span><span class="s1">tol):</span>
    <span class="s0">&quot;&quot;&quot;Return linear operators for matrix A using ``SVDFactorization`` approach. 
    &quot;&quot;&quot;</span>
    <span class="s4"># SVD Factorization</span>
    <span class="s1">U</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">Vt = scipy.linalg.svd(A</span><span class="s2">, </span><span class="s1">full_matrices=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s4"># Remove dimensions related with very small singular values</span>
    <span class="s1">U = U[:</span><span class="s2">, </span><span class="s1">s &gt; tol]</span>
    <span class="s1">Vt = Vt[s &gt; tol</span><span class="s2">, </span><span class="s1">:]</span>
    <span class="s1">s = s[s &gt; tol]</span>

    <span class="s4"># z = x - A.T inv(A A.T) A x</span>
    <span class="s2">def </span><span class="s1">null_space(x):</span>
        <span class="s4"># v = U 1/s V.T x = inv(A A.T) A x</span>
        <span class="s1">aux1 = Vt.dot(x)</span>
        <span class="s1">aux2 = </span><span class="s5">1</span><span class="s1">/s*aux1</span>
        <span class="s1">v = U.dot(aux2)</span>
        <span class="s1">z = x - A.T.dot(v)</span>

        <span class="s4"># Iterative refinement to improve roundoff</span>
        <span class="s4"># errors described in [2]_, algorithm 5.1.</span>
        <span class="s1">k = </span><span class="s5">0</span>
        <span class="s2">while </span><span class="s1">orthogonality(A</span><span class="s2">, </span><span class="s1">z) &gt; orth_tol:</span>
            <span class="s2">if </span><span class="s1">k &gt;= max_refin:</span>
                <span class="s2">break</span>
            <span class="s4"># v = U 1/s V.T x = inv(A A.T) A x</span>
            <span class="s1">aux1 = Vt.dot(z)</span>
            <span class="s1">aux2 = </span><span class="s5">1</span><span class="s1">/s*aux1</span>
            <span class="s1">v = U.dot(aux2)</span>
            <span class="s4"># z_next = z - A.T v</span>
            <span class="s1">z = z - A.T.dot(v)</span>
            <span class="s1">k += </span><span class="s5">1</span>

        <span class="s2">return </span><span class="s1">z</span>

    <span class="s4"># z = inv(A A.T) A x</span>
    <span class="s2">def </span><span class="s1">least_squares(x):</span>
        <span class="s4"># z = U 1/s V.T x = inv(A A.T) A x</span>
        <span class="s1">aux1 = Vt.dot(x)</span>
        <span class="s1">aux2 = </span><span class="s5">1</span><span class="s1">/s*aux1</span>
        <span class="s1">z = U.dot(aux2)</span>
        <span class="s2">return </span><span class="s1">z</span>

    <span class="s4"># z = A.T inv(A A.T) x</span>
    <span class="s2">def </span><span class="s1">row_space(x):</span>
        <span class="s4"># z = V 1/s U.T x</span>
        <span class="s1">aux1 = U.T.dot(x)</span>
        <span class="s1">aux2 = </span><span class="s5">1</span><span class="s1">/s*aux1</span>
        <span class="s1">z = Vt.T.dot(aux2)</span>
        <span class="s2">return </span><span class="s1">z</span>

    <span class="s2">return </span><span class="s1">null_space</span><span class="s2">, </span><span class="s1">least_squares</span><span class="s2">, </span><span class="s1">row_space</span>


<span class="s2">def </span><span class="s1">projections(A</span><span class="s2">, </span><span class="s1">method=</span><span class="s2">None, </span><span class="s1">orth_tol=</span><span class="s5">1e-12</span><span class="s2">, </span><span class="s1">max_refin=</span><span class="s5">3</span><span class="s2">, </span><span class="s1">tol=</span><span class="s5">1e-15</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Return three linear operators related with a given matrix A. 
 
    Parameters 
    ---------- 
    A : sparse matrix (or ndarray), shape (m, n) 
        Matrix ``A`` used in the projection. 
    method : string, optional 
        Method used for compute the given linear 
        operators. Should be one of: 
 
            - 'NormalEquation': The operators 
               will be computed using the 
               so-called normal equation approach 
               explained in [1]_. In order to do 
               so the Cholesky factorization of 
               ``(A A.T)`` is computed. Exclusive 
               for sparse matrices. 
            - 'AugmentedSystem': The operators 
               will be computed using the 
               so-called augmented system approach 
               explained in [1]_. Exclusive 
               for sparse matrices. 
            - 'QRFactorization': Compute projections 
               using QR factorization. Exclusive for 
               dense matrices. 
            - 'SVDFactorization': Compute projections 
               using SVD factorization. Exclusive for 
               dense matrices. 
 
    orth_tol : float, optional 
        Tolerance for iterative refinements. 
    max_refin : int, optional 
        Maximum number of iterative refinements. 
    tol : float, optional 
        Tolerance for singular values. 
 
    Returns 
    ------- 
    Z : LinearOperator, shape (n, n) 
        Null-space operator. For a given vector ``x``, 
        the null space operator is equivalent to apply 
        a projection matrix ``P = I - A.T inv(A A.T) A`` 
        to the vector. It can be shown that this is 
        equivalent to project ``x`` into the null space 
        of A. 
    LS : LinearOperator, shape (m, n) 
        Least-squares operator. For a given vector ``x``, 
        the least-squares operator is equivalent to apply a 
        pseudoinverse matrix ``pinv(A.T) = inv(A A.T) A`` 
        to the vector. It can be shown that this vector 
        ``pinv(A.T) x`` is the least_square solution to 
        ``A.T y = x``. 
    Y : LinearOperator, shape (n, m) 
        Row-space operator. For a given vector ``x``, 
        the row-space operator is equivalent to apply a 
        projection matrix ``Q = A.T inv(A A.T)`` 
        to the vector.  It can be shown that this 
        vector ``y = Q x``  the minimum norm solution 
        of ``A y = x``. 
 
    Notes 
    ----- 
    Uses iterative refinements described in [1] 
    during the computation of ``Z`` in order to 
    cope with the possibility of large roundoff errors. 
 
    References 
    ---------- 
    .. [1] Gould, Nicholas IM, Mary E. Hribar, and Jorge Nocedal. 
        &quot;On the solution of equality constrained quadratic 
        programming problems arising in optimization.&quot; 
        SIAM Journal on Scientific Computing 23.4 (2001): 1376-1395. 
    &quot;&quot;&quot;</span>
    <span class="s1">m</span><span class="s2">, </span><span class="s1">n = np.shape(A)</span>

    <span class="s4"># The factorization of an empty matrix</span>
    <span class="s4"># only works for the sparse representation.</span>
    <span class="s2">if </span><span class="s1">m*n == </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s1">A = csc_matrix(A)</span>

    <span class="s4"># Check Argument</span>
    <span class="s2">if </span><span class="s1">issparse(A):</span>
        <span class="s2">if </span><span class="s1">method </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">method = </span><span class="s3">&quot;AugmentedSystem&quot;</span>
        <span class="s2">if </span><span class="s1">method </span><span class="s2">not in </span><span class="s1">(</span><span class="s3">&quot;NormalEquation&quot;</span><span class="s2">, </span><span class="s3">&quot;AugmentedSystem&quot;</span><span class="s1">):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Method not allowed for sparse matrix.&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">method == </span><span class="s3">&quot;NormalEquation&quot; </span><span class="s2">and not </span><span class="s1">sksparse_available:</span>
            <span class="s1">warnings.warn((</span><span class="s3">&quot;Only accepts 'NormalEquation' option when&quot;</span>
                           <span class="s3">&quot; scikit-sparse is available. Using &quot;</span>
                           <span class="s3">&quot;'AugmentedSystem' option instead.&quot;</span><span class="s1">)</span><span class="s2">,</span>
                          <span class="s1">ImportWarning)</span>
            <span class="s1">method = </span><span class="s3">'AugmentedSystem'</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">if </span><span class="s1">method </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">method = </span><span class="s3">&quot;QRFactorization&quot;</span>
        <span class="s2">if </span><span class="s1">method </span><span class="s2">not in </span><span class="s1">(</span><span class="s3">&quot;QRFactorization&quot;</span><span class="s2">, </span><span class="s3">&quot;SVDFactorization&quot;</span><span class="s1">):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Method not allowed for dense array.&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">method == </span><span class="s3">'NormalEquation'</span><span class="s1">:</span>
        <span class="s1">null_space</span><span class="s2">, </span><span class="s1">least_squares</span><span class="s2">, </span><span class="s1">row_space \</span>
            <span class="s1">= normal_equation_projections(A</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">orth_tol</span><span class="s2">, </span><span class="s1">max_refin</span><span class="s2">, </span><span class="s1">tol)</span>
    <span class="s2">elif </span><span class="s1">method == </span><span class="s3">'AugmentedSystem'</span><span class="s1">:</span>
        <span class="s1">null_space</span><span class="s2">, </span><span class="s1">least_squares</span><span class="s2">, </span><span class="s1">row_space \</span>
            <span class="s1">= augmented_system_projections(A</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">orth_tol</span><span class="s2">, </span><span class="s1">max_refin</span><span class="s2">, </span><span class="s1">tol)</span>
    <span class="s2">elif </span><span class="s1">method == </span><span class="s3">&quot;QRFactorization&quot;</span><span class="s1">:</span>
        <span class="s1">null_space</span><span class="s2">, </span><span class="s1">least_squares</span><span class="s2">, </span><span class="s1">row_space \</span>
            <span class="s1">= qr_factorization_projections(A</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">orth_tol</span><span class="s2">, </span><span class="s1">max_refin</span><span class="s2">, </span><span class="s1">tol)</span>
    <span class="s2">elif </span><span class="s1">method == </span><span class="s3">&quot;SVDFactorization&quot;</span><span class="s1">:</span>
        <span class="s1">null_space</span><span class="s2">, </span><span class="s1">least_squares</span><span class="s2">, </span><span class="s1">row_space \</span>
            <span class="s1">= svd_factorization_projections(A</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">orth_tol</span><span class="s2">, </span><span class="s1">max_refin</span><span class="s2">, </span><span class="s1">tol)</span>

    <span class="s1">Z = LinearOperator((n</span><span class="s2">, </span><span class="s1">n)</span><span class="s2">, </span><span class="s1">null_space)</span>
    <span class="s1">LS = LinearOperator((m</span><span class="s2">, </span><span class="s1">n)</span><span class="s2">, </span><span class="s1">least_squares)</span>
    <span class="s1">Y = LinearOperator((n</span><span class="s2">, </span><span class="s1">m)</span><span class="s2">, </span><span class="s1">row_space)</span>

    <span class="s2">return </span><span class="s1">Z</span><span class="s2">, </span><span class="s1">LS</span><span class="s2">, </span><span class="s1">Y</span>
</pre>
</body>
</html>