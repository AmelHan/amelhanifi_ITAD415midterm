<html>
<head>
<title>test_feature_select.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_feature_select.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Todo: cross-check the F-value with stats model 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">itertools</span>
<span class="s2">import </span><span class="s1">warnings</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">numpy.testing </span><span class="s2">import </span><span class="s1">assert_allclose</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">sparse</span><span class="s2">, </span><span class="s1">stats</span>

<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">load_iris</span><span class="s2">, </span><span class="s1">make_classification</span><span class="s2">, </span><span class="s1">make_regression</span>
<span class="s2">from </span><span class="s1">sklearn.feature_selection </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">GenericUnivariateSelect</span><span class="s2">,</span>
    <span class="s1">SelectFdr</span><span class="s2">,</span>
    <span class="s1">SelectFpr</span><span class="s2">,</span>
    <span class="s1">SelectFwe</span><span class="s2">,</span>
    <span class="s1">SelectKBest</span><span class="s2">,</span>
    <span class="s1">SelectPercentile</span><span class="s2">,</span>
    <span class="s1">chi2</span><span class="s2">,</span>
    <span class="s1">f_classif</span><span class="s2">,</span>
    <span class="s1">f_oneway</span><span class="s2">,</span>
    <span class="s1">f_regression</span><span class="s2">,</span>
    <span class="s1">mutual_info_classif</span><span class="s2">,</span>
    <span class="s1">mutual_info_regression</span><span class="s2">,</span>
    <span class="s1">r_regression</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">safe_mask</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">_convert_container</span><span class="s2">,</span>
    <span class="s1">assert_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_equal</span><span class="s2">,</span>
    <span class="s1">ignore_warnings</span><span class="s2">,</span>
<span class="s1">)</span>

<span class="s3">##############################################################################</span>
<span class="s3"># Test the score functions</span>


<span class="s2">def </span><span class="s1">test_f_oneway_vs_scipy_stats():</span>
    <span class="s3"># Test that our f_oneway gives the same result as scipy.stats</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X1 = rng.randn(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">X2 = </span><span class="s4">1 </span><span class="s1">+ rng.randn(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">f</span><span class="s2">, </span><span class="s1">pv = stats.f_oneway(X1</span><span class="s2">, </span><span class="s1">X2)</span>
    <span class="s1">f2</span><span class="s2">, </span><span class="s1">pv2 = f_oneway(X1</span><span class="s2">, </span><span class="s1">X2)</span>
    <span class="s2">assert </span><span class="s1">np.allclose(f</span><span class="s2">, </span><span class="s1">f2)</span>
    <span class="s2">assert </span><span class="s1">np.allclose(pv</span><span class="s2">, </span><span class="s1">pv2)</span>


<span class="s2">def </span><span class="s1">test_f_oneway_ints():</span>
    <span class="s3"># Smoke test f_oneway on integers: that it does raise casting errors</span>
    <span class="s3"># with recent numpys</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randint(</span><span class="s4">10</span><span class="s2">, </span><span class="s1">size=(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">y = np.arange(</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">fint</span><span class="s2">, </span><span class="s1">pint = f_oneway(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s3"># test that is gives the same result as with float</span>
    <span class="s1">f</span><span class="s2">, </span><span class="s1">p = f_oneway(X.astype(float)</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(f</span><span class="s2">, </span><span class="s1">fint</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">4</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(p</span><span class="s2">, </span><span class="s1">pint</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">4</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_f_classif():</span>
    <span class="s3"># Test whether the F test yields meaningful results</span>
    <span class="s3"># on a simple simulated classification problem</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">200</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">20</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">n_redundant=</span><span class="s4">2</span><span class="s2">,</span>
        <span class="s1">n_repeated=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">n_classes=</span><span class="s4">8</span><span class="s2">,</span>
        <span class="s1">n_clusters_per_class=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">flip_y=</span><span class="s4">0.0</span><span class="s2">,</span>
        <span class="s1">class_sep=</span><span class="s4">10</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">F</span><span class="s2">, </span><span class="s1">pv = f_classif(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">F_sparse</span><span class="s2">, </span><span class="s1">pv_sparse = f_classif(sparse.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">(F &gt; </span><span class="s4">0</span><span class="s1">).all()</span>
    <span class="s2">assert </span><span class="s1">(pv &gt; </span><span class="s4">0</span><span class="s1">).all()</span>
    <span class="s2">assert </span><span class="s1">(pv &lt; </span><span class="s4">1</span><span class="s1">).all()</span>
    <span class="s2">assert </span><span class="s1">(pv[:</span><span class="s4">5</span><span class="s1">] &lt; </span><span class="s4">0.05</span><span class="s1">).all()</span>
    <span class="s2">assert </span><span class="s1">(pv[</span><span class="s4">5</span><span class="s1">:] &gt; </span><span class="s4">1.0e-4</span><span class="s1">).all()</span>
    <span class="s1">assert_array_almost_equal(F_sparse</span><span class="s2">, </span><span class="s1">F)</span>
    <span class="s1">assert_array_almost_equal(pv_sparse</span><span class="s2">, </span><span class="s1">pv)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;center&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_r_regression(center):</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">n_samples=</span><span class="s4">2000</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">n_informative=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">shuffle=</span><span class="s2">False, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>

    <span class="s1">corr_coeffs = r_regression(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">center=center)</span>
    <span class="s2">assert </span><span class="s1">(-</span><span class="s4">1 </span><span class="s1">&lt; corr_coeffs).all()</span>
    <span class="s2">assert </span><span class="s1">(corr_coeffs &lt; </span><span class="s4">1</span><span class="s1">).all()</span>

    <span class="s1">sparse_X = _convert_container(X</span><span class="s2">, </span><span class="s5">&quot;sparse&quot;</span><span class="s1">)</span>

    <span class="s1">sparse_corr_coeffs = r_regression(sparse_X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">center=center)</span>
    <span class="s1">assert_allclose(sparse_corr_coeffs</span><span class="s2">, </span><span class="s1">corr_coeffs)</span>

    <span class="s3"># Testing against numpy for reference</span>
    <span class="s1">Z = np.hstack((X</span><span class="s2">, </span><span class="s1">y[:</span><span class="s2">, </span><span class="s1">np.newaxis]))</span>
    <span class="s1">correlation_matrix = np.corrcoef(Z</span><span class="s2">, </span><span class="s1">rowvar=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">np_corr_coeffs = correlation_matrix[:-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">assert_array_almost_equal(np_corr_coeffs</span><span class="s2">, </span><span class="s1">corr_coeffs</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">3</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_f_regression():</span>
    <span class="s3"># Test whether the F test yields meaningful results</span>
    <span class="s3"># on a simple simulated regression problem</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">n_samples=</span><span class="s4">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">n_informative=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">shuffle=</span><span class="s2">False, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>

    <span class="s1">F</span><span class="s2">, </span><span class="s1">pv = f_regression(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">(F &gt; </span><span class="s4">0</span><span class="s1">).all()</span>
    <span class="s2">assert </span><span class="s1">(pv &gt; </span><span class="s4">0</span><span class="s1">).all()</span>
    <span class="s2">assert </span><span class="s1">(pv &lt; </span><span class="s4">1</span><span class="s1">).all()</span>
    <span class="s2">assert </span><span class="s1">(pv[:</span><span class="s4">5</span><span class="s1">] &lt; </span><span class="s4">0.05</span><span class="s1">).all()</span>
    <span class="s2">assert </span><span class="s1">(pv[</span><span class="s4">5</span><span class="s1">:] &gt; </span><span class="s4">1.0e-4</span><span class="s1">).all()</span>

    <span class="s3"># with centering, compare with sparse</span>
    <span class="s1">F</span><span class="s2">, </span><span class="s1">pv = f_regression(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">center=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">F_sparse</span><span class="s2">, </span><span class="s1">pv_sparse = f_regression(sparse.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">center=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">assert_allclose(F_sparse</span><span class="s2">, </span><span class="s1">F)</span>
    <span class="s1">assert_allclose(pv_sparse</span><span class="s2">, </span><span class="s1">pv)</span>

    <span class="s3"># again without centering, compare with sparse</span>
    <span class="s1">F</span><span class="s2">, </span><span class="s1">pv = f_regression(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">center=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">F_sparse</span><span class="s2">, </span><span class="s1">pv_sparse = f_regression(sparse.csr_matrix(X)</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">center=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">assert_allclose(F_sparse</span><span class="s2">, </span><span class="s1">F)</span>
    <span class="s1">assert_allclose(pv_sparse</span><span class="s2">, </span><span class="s1">pv)</span>


<span class="s2">def </span><span class="s1">test_f_regression_input_dtype():</span>
    <span class="s3"># Test whether f_regression returns the same value</span>
    <span class="s3"># for any numeric data_type</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = rng.rand(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">y = np.arange(</span><span class="s4">10</span><span class="s1">).astype(int)</span>

    <span class="s1">F1</span><span class="s2">, </span><span class="s1">pv1 = f_regression(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">F2</span><span class="s2">, </span><span class="s1">pv2 = f_regression(X</span><span class="s2">, </span><span class="s1">y.astype(float))</span>
    <span class="s1">assert_allclose(F1</span><span class="s2">, </span><span class="s1">F2</span><span class="s2">, </span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">assert_allclose(pv1</span><span class="s2">, </span><span class="s1">pv2</span><span class="s2">, </span><span class="s4">5</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_f_regression_center():</span>
    <span class="s3"># Test whether f_regression preserves dof according to 'center' argument</span>
    <span class="s3"># We use two centered variates so we have a simple relationship between</span>
    <span class="s3"># F-score with variates centering and F-score without variates centering.</span>
    <span class="s3"># Create toy example</span>
    <span class="s1">X = np.arange(-</span><span class="s4">5</span><span class="s2">, </span><span class="s4">6</span><span class="s1">).reshape(-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)  </span><span class="s3"># X has zero mean</span>
    <span class="s1">n_samples = X.size</span>
    <span class="s1">Y = np.ones(n_samples)</span>
    <span class="s1">Y[::</span><span class="s4">2</span><span class="s1">] *= -</span><span class="s4">1.0</span>
    <span class="s1">Y[</span><span class="s4">0</span><span class="s1">] = </span><span class="s4">0.0  </span><span class="s3"># have Y mean being null</span>

    <span class="s1">F1</span><span class="s2">, </span><span class="s1">_ = f_regression(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">center=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">F2</span><span class="s2">, </span><span class="s1">_ = f_regression(X</span><span class="s2">, </span><span class="s1">Y</span><span class="s2">, </span><span class="s1">center=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">assert_allclose(F1 * (n_samples - </span><span class="s4">1.0</span><span class="s1">) / (n_samples - </span><span class="s4">2.0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">F2)</span>
    <span class="s1">assert_almost_equal(F2[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s4">0.232558139</span><span class="s1">)  </span><span class="s3"># value from statsmodels OLS</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;X, y, expected_corr_coef, force_finite&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s3"># A feature in X is constant - forcing finite</span>
            <span class="s1">np.array([[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">0.32075</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s2">True,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s3"># The target y is constant - forcing finite</span>
            <span class="s1">np.array([[</span><span class="s4">5</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">8</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">0.0</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s2">True,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s3"># A feature in X is constant - not forcing finite</span>
            <span class="s1">np.array([[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([np.nan</span><span class="s2">, </span><span class="s4">0.32075</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s2">False,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s3"># The target y is constant - not forcing finite</span>
            <span class="s1">np.array([[</span><span class="s4">5</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">8</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([np.nan</span><span class="s2">, </span><span class="s1">np.nan])</span><span class="s2">,</span>
            <span class="s2">False,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_r_regression_force_finite(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">expected_corr_coef</span><span class="s2">, </span><span class="s1">force_finite):</span>
    <span class="s0">&quot;&quot;&quot;Check the behaviour of `force_finite` for some corner cases with `r_regression`. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/15672 
    &quot;&quot;&quot;</span>
    <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;error&quot;</span><span class="s2">, </span><span class="s1">RuntimeWarning)</span>
        <span class="s1">corr_coef = r_regression(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">force_finite=force_finite)</span>
    <span class="s1">np.testing.assert_array_almost_equal(corr_coef</span><span class="s2">, </span><span class="s1">expected_corr_coef)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;X, y, expected_f_statistic, expected_p_values, force_finite&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s3"># A feature in X is constant - forcing finite</span>
            <span class="s1">np.array([[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">0.2293578</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">0.67924985</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s2">True,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s3"># The target y is constant - forcing finite</span>
            <span class="s1">np.array([[</span><span class="s4">5</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">8</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">0.0</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">1.0</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s2">True,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s3"># Feature in X correlated with y - forcing finite</span>
            <span class="s1">np.array([[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([np.finfo(np.float64).max</span><span class="s2">, </span><span class="s4">0.845433</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">0.454913</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s2">True,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s3"># Feature in X anti-correlated with y - forcing finite</span>
            <span class="s1">np.array([[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([np.finfo(np.float64).max</span><span class="s2">, </span><span class="s4">0.845433</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">0.454913</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s2">True,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s3"># A feature in X is constant - not forcing finite</span>
            <span class="s1">np.array([[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([np.nan</span><span class="s2">, </span><span class="s4">0.2293578</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([np.nan</span><span class="s2">, </span><span class="s4">0.67924985</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s2">False,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s3"># The target y is constant - not forcing finite</span>
            <span class="s1">np.array([[</span><span class="s4">5</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">8</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([np.nan</span><span class="s2">, </span><span class="s1">np.nan])</span><span class="s2">,</span>
            <span class="s1">np.array([np.nan</span><span class="s2">, </span><span class="s1">np.nan])</span><span class="s2">,</span>
            <span class="s2">False,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s3"># Feature in X correlated with y - not forcing finite</span>
            <span class="s1">np.array([[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([np.inf</span><span class="s2">, </span><span class="s4">0.845433</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">0.454913</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s2">False,</span>
        <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">(</span>
            <span class="s3"># Feature in X anti-correlated with y - not forcing finite</span>
            <span class="s1">np.array([[</span><span class="s4">3</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">4</span><span class="s1">]])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([np.inf</span><span class="s2">, </span><span class="s4">0.845433</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s1">np.array([</span><span class="s4">0.0</span><span class="s2">, </span><span class="s4">0.454913</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s2">False,</span>
        <span class="s1">)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_f_regression_corner_case(</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">expected_f_statistic</span><span class="s2">, </span><span class="s1">expected_p_values</span><span class="s2">, </span><span class="s1">force_finite</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Check the behaviour of `force_finite` for some corner cases with `f_regression`. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/15672 
    &quot;&quot;&quot;</span>
    <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;error&quot;</span><span class="s2">, </span><span class="s1">RuntimeWarning)</span>
        <span class="s1">f_statistic</span><span class="s2">, </span><span class="s1">p_values = f_regression(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">force_finite=force_finite)</span>
    <span class="s1">np.testing.assert_array_almost_equal(f_statistic</span><span class="s2">, </span><span class="s1">expected_f_statistic)</span>
    <span class="s1">np.testing.assert_array_almost_equal(p_values</span><span class="s2">, </span><span class="s1">expected_p_values)</span>


<span class="s2">def </span><span class="s1">test_f_classif_multi_class():</span>
    <span class="s3"># Test whether the F test yields meaningful results</span>
    <span class="s3"># on a simple simulated classification problem</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">200</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">20</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">n_redundant=</span><span class="s4">2</span><span class="s2">,</span>
        <span class="s1">n_repeated=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">n_classes=</span><span class="s4">8</span><span class="s2">,</span>
        <span class="s1">n_clusters_per_class=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">flip_y=</span><span class="s4">0.0</span><span class="s2">,</span>
        <span class="s1">class_sep=</span><span class="s4">10</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">F</span><span class="s2">, </span><span class="s1">pv = f_classif(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">(F &gt; </span><span class="s4">0</span><span class="s1">).all()</span>
    <span class="s2">assert </span><span class="s1">(pv &gt; </span><span class="s4">0</span><span class="s1">).all()</span>
    <span class="s2">assert </span><span class="s1">(pv &lt; </span><span class="s4">1</span><span class="s1">).all()</span>
    <span class="s2">assert </span><span class="s1">(pv[:</span><span class="s4">5</span><span class="s1">] &lt; </span><span class="s4">0.05</span><span class="s1">).all()</span>
    <span class="s2">assert </span><span class="s1">(pv[</span><span class="s4">5</span><span class="s1">:] &gt; </span><span class="s4">1.0e-4</span><span class="s1">).all()</span>


<span class="s2">def </span><span class="s1">test_select_percentile_classif():</span>
    <span class="s3"># Test whether the relative univariate feature selection</span>
    <span class="s3"># gets the correct items in a simple classification problem</span>
    <span class="s3"># with the percentile heuristic</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">200</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">20</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">n_redundant=</span><span class="s4">2</span><span class="s2">,</span>
        <span class="s1">n_repeated=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">n_classes=</span><span class="s4">8</span><span class="s2">,</span>
        <span class="s1">n_clusters_per_class=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">flip_y=</span><span class="s4">0.0</span><span class="s2">,</span>
        <span class="s1">class_sep=</span><span class="s4">10</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">univariate_filter = SelectPercentile(f_classif</span><span class="s2">, </span><span class="s1">percentile=</span><span class="s4">25</span><span class="s1">)</span>
    <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
    <span class="s1">X_r2 = (</span>
        <span class="s1">GenericUnivariateSelect(f_classif</span><span class="s2">, </span><span class="s1">mode=</span><span class="s5">&quot;percentile&quot;</span><span class="s2">, </span><span class="s1">param=</span><span class="s4">25</span><span class="s1">)</span>
        <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">.transform(X)</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">X_r2)</span>
    <span class="s1">support = univariate_filter.get_support()</span>
    <span class="s1">gtruth = np.zeros(</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">gtruth[:</span><span class="s4">5</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">assert_array_equal(support</span><span class="s2">, </span><span class="s1">gtruth)</span>


<span class="s2">def </span><span class="s1">test_select_percentile_classif_sparse():</span>
    <span class="s3"># Test whether the relative univariate feature selection</span>
    <span class="s3"># gets the correct items in a simple classification problem</span>
    <span class="s3"># with the percentile heuristic</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">200</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">20</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">n_redundant=</span><span class="s4">2</span><span class="s2">,</span>
        <span class="s1">n_repeated=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">n_classes=</span><span class="s4">8</span><span class="s2">,</span>
        <span class="s1">n_clusters_per_class=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">flip_y=</span><span class="s4">0.0</span><span class="s2">,</span>
        <span class="s1">class_sep=</span><span class="s4">10</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">X = sparse.csr_matrix(X)</span>
    <span class="s1">univariate_filter = SelectPercentile(f_classif</span><span class="s2">, </span><span class="s1">percentile=</span><span class="s4">25</span><span class="s1">)</span>
    <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
    <span class="s1">X_r2 = (</span>
        <span class="s1">GenericUnivariateSelect(f_classif</span><span class="s2">, </span><span class="s1">mode=</span><span class="s5">&quot;percentile&quot;</span><span class="s2">, </span><span class="s1">param=</span><span class="s4">25</span><span class="s1">)</span>
        <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">.transform(X)</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(X_r.toarray()</span><span class="s2">, </span><span class="s1">X_r2.toarray())</span>
    <span class="s1">support = univariate_filter.get_support()</span>
    <span class="s1">gtruth = np.zeros(</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">gtruth[:</span><span class="s4">5</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">assert_array_equal(support</span><span class="s2">, </span><span class="s1">gtruth)</span>

    <span class="s1">X_r2inv = univariate_filter.inverse_transform(X_r2)</span>
    <span class="s2">assert </span><span class="s1">sparse.issparse(X_r2inv)</span>
    <span class="s1">support_mask = safe_mask(X_r2inv</span><span class="s2">, </span><span class="s1">support)</span>
    <span class="s2">assert </span><span class="s1">X_r2inv.shape == X.shape</span>
    <span class="s1">assert_array_equal(X_r2inv[:</span><span class="s2">, </span><span class="s1">support_mask].toarray()</span><span class="s2">, </span><span class="s1">X_r.toarray())</span>
    <span class="s3"># Check other columns are empty</span>
    <span class="s2">assert </span><span class="s1">X_r2inv.getnnz() == X_r.getnnz()</span>


<span class="s3">##############################################################################</span>
<span class="s3"># Test univariate selection in classification settings</span>


<span class="s2">def </span><span class="s1">test_select_kbest_classif():</span>
    <span class="s3"># Test whether the relative univariate feature selection</span>
    <span class="s3"># gets the correct items in a simple classification problem</span>
    <span class="s3"># with the k best heuristic</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">200</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">20</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">n_redundant=</span><span class="s4">2</span><span class="s2">,</span>
        <span class="s1">n_repeated=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">n_classes=</span><span class="s4">8</span><span class="s2">,</span>
        <span class="s1">n_clusters_per_class=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">flip_y=</span><span class="s4">0.0</span><span class="s2">,</span>
        <span class="s1">class_sep=</span><span class="s4">10</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">univariate_filter = SelectKBest(f_classif</span><span class="s2">, </span><span class="s1">k=</span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
    <span class="s1">X_r2 = (</span>
        <span class="s1">GenericUnivariateSelect(f_classif</span><span class="s2">, </span><span class="s1">mode=</span><span class="s5">&quot;k_best&quot;</span><span class="s2">, </span><span class="s1">param=</span><span class="s4">5</span><span class="s1">)</span>
        <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">.transform(X)</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">X_r2)</span>
    <span class="s1">support = univariate_filter.get_support()</span>
    <span class="s1">gtruth = np.zeros(</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">gtruth[:</span><span class="s4">5</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">assert_array_equal(support</span><span class="s2">, </span><span class="s1">gtruth)</span>


<span class="s2">def </span><span class="s1">test_select_kbest_all():</span>
    <span class="s3"># Test whether k=&quot;all&quot; correctly returns all features.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">shuffle=</span><span class="s2">False, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>

    <span class="s1">univariate_filter = SelectKBest(f_classif</span><span class="s2">, </span><span class="s1">k=</span><span class="s5">&quot;all&quot;</span><span class="s1">)</span>
    <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
    <span class="s1">assert_array_equal(X</span><span class="s2">, </span><span class="s1">X_r)</span>
    <span class="s3"># Non-regression test for:</span>
    <span class="s3"># https://github.com/scikit-learn/scikit-learn/issues/24949</span>
    <span class="s1">X_r2 = (</span>
        <span class="s1">GenericUnivariateSelect(f_classif</span><span class="s2">, </span><span class="s1">mode=</span><span class="s5">&quot;k_best&quot;</span><span class="s2">, </span><span class="s1">param=</span><span class="s5">&quot;all&quot;</span><span class="s1">)</span>
        <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">.transform(X)</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">X_r2)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;dtype_in&quot;</span><span class="s2">, </span><span class="s1">[np.float32</span><span class="s2">, </span><span class="s1">np.float64])</span>
<span class="s2">def </span><span class="s1">test_select_kbest_zero(dtype_in):</span>
    <span class="s3"># Test whether k=0 correctly returns no features.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">shuffle=</span><span class="s2">False, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">X = X.astype(dtype_in)</span>

    <span class="s1">univariate_filter = SelectKBest(f_classif</span><span class="s2">, </span><span class="s1">k=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">support = univariate_filter.get_support()</span>
    <span class="s1">gtruth = np.zeros(</span><span class="s4">10</span><span class="s2">, </span><span class="s1">dtype=bool)</span>
    <span class="s1">assert_array_equal(support</span><span class="s2">, </span><span class="s1">gtruth)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;No features were selected&quot;</span><span class="s1">):</span>
        <span class="s1">X_selected = univariate_filter.transform(X)</span>
    <span class="s2">assert </span><span class="s1">X_selected.shape == (</span><span class="s4">20</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">X_selected.dtype == dtype_in</span>


<span class="s2">def </span><span class="s1">test_select_heuristics_classif():</span>
    <span class="s3"># Test whether the relative univariate feature selection</span>
    <span class="s3"># gets the correct items in a simple classification problem</span>
    <span class="s3"># with the fdr, fwe and fpr heuristics</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">200</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">20</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">n_redundant=</span><span class="s4">2</span><span class="s2">,</span>
        <span class="s1">n_repeated=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">n_classes=</span><span class="s4">8</span><span class="s2">,</span>
        <span class="s1">n_clusters_per_class=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">flip_y=</span><span class="s4">0.0</span><span class="s2">,</span>
        <span class="s1">class_sep=</span><span class="s4">10</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">univariate_filter = SelectFwe(f_classif</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s4">0.01</span><span class="s1">)</span>
    <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
    <span class="s1">gtruth = np.zeros(</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">gtruth[:</span><span class="s4">5</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s2">for </span><span class="s1">mode </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;fdr&quot;</span><span class="s2">, </span><span class="s5">&quot;fpr&quot;</span><span class="s2">, </span><span class="s5">&quot;fwe&quot;</span><span class="s1">]:</span>
        <span class="s1">X_r2 = (</span>
            <span class="s1">GenericUnivariateSelect(f_classif</span><span class="s2">, </span><span class="s1">mode=mode</span><span class="s2">, </span><span class="s1">param=</span><span class="s4">0.01</span><span class="s1">)</span>
            <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
            <span class="s1">.transform(X)</span>
        <span class="s1">)</span>
        <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">X_r2)</span>
        <span class="s1">support = univariate_filter.get_support()</span>
        <span class="s1">assert_allclose(support</span><span class="s2">, </span><span class="s1">gtruth)</span>


<span class="s3">##############################################################################</span>
<span class="s3"># Test univariate selection in regression settings</span>


<span class="s2">def </span><span class="s1">assert_best_scores_kept(score_filter):</span>
    <span class="s1">scores = score_filter.scores_</span>
    <span class="s1">support = score_filter.get_support()</span>
    <span class="s1">assert_allclose(np.sort(scores[support])</span><span class="s2">, </span><span class="s1">np.sort(scores)[-support.sum() :])</span>


<span class="s2">def </span><span class="s1">test_select_percentile_regression():</span>
    <span class="s3"># Test whether the relative univariate feature selection</span>
    <span class="s3"># gets the correct items in a simple regression problem</span>
    <span class="s3"># with the percentile heuristic</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">n_samples=</span><span class="s4">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">n_informative=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">shuffle=</span><span class="s2">False, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>

    <span class="s1">univariate_filter = SelectPercentile(f_regression</span><span class="s2">, </span><span class="s1">percentile=</span><span class="s4">25</span><span class="s1">)</span>
    <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
    <span class="s1">assert_best_scores_kept(univariate_filter)</span>
    <span class="s1">X_r2 = (</span>
        <span class="s1">GenericUnivariateSelect(f_regression</span><span class="s2">, </span><span class="s1">mode=</span><span class="s5">&quot;percentile&quot;</span><span class="s2">, </span><span class="s1">param=</span><span class="s4">25</span><span class="s1">)</span>
        <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">.transform(X)</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">X_r2)</span>
    <span class="s1">support = univariate_filter.get_support()</span>
    <span class="s1">gtruth = np.zeros(</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">gtruth[:</span><span class="s4">5</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">assert_array_equal(support</span><span class="s2">, </span><span class="s1">gtruth)</span>
    <span class="s1">X_2 = X.copy()</span>
    <span class="s1">X_2[:</span><span class="s2">, </span><span class="s1">np.logical_not(support)] = </span><span class="s4">0</span>
    <span class="s1">assert_array_equal(X_2</span><span class="s2">, </span><span class="s1">univariate_filter.inverse_transform(X_r))</span>
    <span class="s3"># Check inverse_transform respects dtype</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">X_2.astype(bool)</span><span class="s2">, </span><span class="s1">univariate_filter.inverse_transform(X_r.astype(bool))</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_select_percentile_regression_full():</span>
    <span class="s3"># Test whether the relative univariate feature selection</span>
    <span class="s3"># selects all features when '100%' is asked.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">n_samples=</span><span class="s4">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">n_informative=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">shuffle=</span><span class="s2">False, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>

    <span class="s1">univariate_filter = SelectPercentile(f_regression</span><span class="s2">, </span><span class="s1">percentile=</span><span class="s4">100</span><span class="s1">)</span>
    <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
    <span class="s1">assert_best_scores_kept(univariate_filter)</span>
    <span class="s1">X_r2 = (</span>
        <span class="s1">GenericUnivariateSelect(f_regression</span><span class="s2">, </span><span class="s1">mode=</span><span class="s5">&quot;percentile&quot;</span><span class="s2">, </span><span class="s1">param=</span><span class="s4">100</span><span class="s1">)</span>
        <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">.transform(X)</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">X_r2)</span>
    <span class="s1">support = univariate_filter.get_support()</span>
    <span class="s1">gtruth = np.ones(</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">assert_array_equal(support</span><span class="s2">, </span><span class="s1">gtruth)</span>


<span class="s2">def </span><span class="s1">test_select_kbest_regression():</span>
    <span class="s3"># Test whether the relative univariate feature selection</span>
    <span class="s3"># gets the correct items in a simple regression problem</span>
    <span class="s3"># with the k best heuristic</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">n_samples=</span><span class="s4">200</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">20</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s4">5</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">noise=</span><span class="s4">10</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">univariate_filter = SelectKBest(f_regression</span><span class="s2">, </span><span class="s1">k=</span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
    <span class="s1">assert_best_scores_kept(univariate_filter)</span>
    <span class="s1">X_r2 = (</span>
        <span class="s1">GenericUnivariateSelect(f_regression</span><span class="s2">, </span><span class="s1">mode=</span><span class="s5">&quot;k_best&quot;</span><span class="s2">, </span><span class="s1">param=</span><span class="s4">5</span><span class="s1">)</span>
        <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">.transform(X)</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">X_r2)</span>
    <span class="s1">support = univariate_filter.get_support()</span>
    <span class="s1">gtruth = np.zeros(</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">gtruth[:</span><span class="s4">5</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">assert_array_equal(support</span><span class="s2">, </span><span class="s1">gtruth)</span>


<span class="s2">def </span><span class="s1">test_select_heuristics_regression():</span>
    <span class="s3"># Test whether the relative univariate feature selection</span>
    <span class="s3"># gets the correct items in a simple regression problem</span>
    <span class="s3"># with the fpr, fdr or fwe heuristics</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">n_samples=</span><span class="s4">200</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">20</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s4">5</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">noise=</span><span class="s4">10</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">univariate_filter = SelectFpr(f_regression</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s4">0.01</span><span class="s1">)</span>
    <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
    <span class="s1">gtruth = np.zeros(</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">gtruth[:</span><span class="s4">5</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s2">for </span><span class="s1">mode </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;fdr&quot;</span><span class="s2">, </span><span class="s5">&quot;fpr&quot;</span><span class="s2">, </span><span class="s5">&quot;fwe&quot;</span><span class="s1">]:</span>
        <span class="s1">X_r2 = (</span>
            <span class="s1">GenericUnivariateSelect(f_regression</span><span class="s2">, </span><span class="s1">mode=mode</span><span class="s2">, </span><span class="s1">param=</span><span class="s4">0.01</span><span class="s1">)</span>
            <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
            <span class="s1">.transform(X)</span>
        <span class="s1">)</span>
        <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">X_r2)</span>
        <span class="s1">support = univariate_filter.get_support()</span>
        <span class="s1">assert_array_equal(support[:</span><span class="s4">5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">np.ones((</span><span class="s4">5</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype=bool))</span>
        <span class="s2">assert </span><span class="s1">np.sum(support[</span><span class="s4">5</span><span class="s1">:] == </span><span class="s4">1</span><span class="s1">) &lt; </span><span class="s4">3</span>


<span class="s2">def </span><span class="s1">test_boundary_case_ch2():</span>
    <span class="s3"># Test boundary case, and always aim to select 1 feature.</span>
    <span class="s1">X = np.array([[</span><span class="s4">10</span><span class="s2">, </span><span class="s4">20</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">20</span><span class="s2">, </span><span class="s4">20</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">20</span><span class="s2">, </span><span class="s4">30</span><span class="s1">]])</span>
    <span class="s1">y = np.array([[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]])</span>
    <span class="s1">scores</span><span class="s2">, </span><span class="s1">pvalues = chi2(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(scores</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s4">4.0</span><span class="s2">, </span><span class="s4">0.71428571</span><span class="s1">]))</span>
    <span class="s1">assert_array_almost_equal(pvalues</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s4">0.04550026</span><span class="s2">, </span><span class="s4">0.39802472</span><span class="s1">]))</span>

    <span class="s1">filter_fdr = SelectFdr(chi2</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s4">0.1</span><span class="s1">)</span>
    <span class="s1">filter_fdr.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">support_fdr = filter_fdr.get_support()</span>
    <span class="s1">assert_array_equal(support_fdr</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s2">True, False</span><span class="s1">]))</span>

    <span class="s1">filter_kbest = SelectKBest(chi2</span><span class="s2">, </span><span class="s1">k=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">filter_kbest.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">support_kbest = filter_kbest.get_support()</span>
    <span class="s1">assert_array_equal(support_kbest</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s2">True, False</span><span class="s1">]))</span>

    <span class="s1">filter_percentile = SelectPercentile(chi2</span><span class="s2">, </span><span class="s1">percentile=</span><span class="s4">50</span><span class="s1">)</span>
    <span class="s1">filter_percentile.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">support_percentile = filter_percentile.get_support()</span>
    <span class="s1">assert_array_equal(support_percentile</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s2">True, False</span><span class="s1">]))</span>

    <span class="s1">filter_fpr = SelectFpr(chi2</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s4">0.1</span><span class="s1">)</span>
    <span class="s1">filter_fpr.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">support_fpr = filter_fpr.get_support()</span>
    <span class="s1">assert_array_equal(support_fpr</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s2">True, False</span><span class="s1">]))</span>

    <span class="s1">filter_fwe = SelectFwe(chi2</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s4">0.1</span><span class="s1">)</span>
    <span class="s1">filter_fwe.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">support_fwe = filter_fwe.get_support()</span>
    <span class="s1">assert_array_equal(support_fwe</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s2">True, False</span><span class="s1">]))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;alpha&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.001</span><span class="s2">, </span><span class="s4">0.01</span><span class="s2">, </span><span class="s4">0.1</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;n_informative&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">10</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_select_fdr_regression(alpha</span><span class="s2">, </span><span class="s1">n_informative):</span>
    <span class="s3"># Test that fdr heuristic actually has low FDR.</span>
    <span class="s2">def </span><span class="s1">single_fdr(alpha</span><span class="s2">, </span><span class="s1">n_informative</span><span class="s2">, </span><span class="s1">random_state):</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(</span>
            <span class="s1">n_samples=</span><span class="s4">150</span><span class="s2">,</span>
            <span class="s1">n_features=</span><span class="s4">20</span><span class="s2">,</span>
            <span class="s1">n_informative=n_informative</span><span class="s2">,</span>
            <span class="s1">shuffle=</span><span class="s2">False,</span>
            <span class="s1">random_state=random_state</span><span class="s2">,</span>
            <span class="s1">noise=</span><span class="s4">10</span><span class="s2">,</span>
        <span class="s1">)</span>

        <span class="s2">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s2">True</span><span class="s1">):</span>
            <span class="s3"># Warnings can be raised when no features are selected</span>
            <span class="s3"># (low alpha or very noisy data)</span>
            <span class="s1">univariate_filter = SelectFdr(f_regression</span><span class="s2">, </span><span class="s1">alpha=alpha)</span>
            <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
            <span class="s1">X_r2 = (</span>
                <span class="s1">GenericUnivariateSelect(f_regression</span><span class="s2">, </span><span class="s1">mode=</span><span class="s5">&quot;fdr&quot;</span><span class="s2">, </span><span class="s1">param=alpha)</span>
                <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
                <span class="s1">.transform(X)</span>
            <span class="s1">)</span>

        <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">X_r2)</span>
        <span class="s1">support = univariate_filter.get_support()</span>
        <span class="s1">num_false_positives = np.sum(support[n_informative:] == </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">num_true_positives = np.sum(support[:n_informative] == </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">num_false_positives == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s4">0.0</span>
        <span class="s1">false_discovery_rate = num_false_positives / (</span>
            <span class="s1">num_true_positives + num_false_positives</span>
        <span class="s1">)</span>
        <span class="s2">return </span><span class="s1">false_discovery_rate</span>

    <span class="s3"># As per Benjamini-Hochberg, the expected false discovery rate</span>
    <span class="s3"># should be lower than alpha:</span>
    <span class="s3"># FDR = E(FP / (TP + FP)) &lt;= alpha</span>
    <span class="s1">false_discovery_rate = np.mean(</span>
        <span class="s1">[single_fdr(alpha</span><span class="s2">, </span><span class="s1">n_informative</span><span class="s2">, </span><span class="s1">random_state) </span><span class="s2">for </span><span class="s1">random_state </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">100</span><span class="s1">)]</span>
    <span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">alpha &gt;= false_discovery_rate</span>

    <span class="s3"># Make sure that the empirical false discovery rate increases</span>
    <span class="s3"># with alpha:</span>
    <span class="s2">if </span><span class="s1">false_discovery_rate != </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s2">assert </span><span class="s1">false_discovery_rate &gt; alpha / </span><span class="s4">10</span>


<span class="s2">def </span><span class="s1">test_select_fwe_regression():</span>
    <span class="s3"># Test whether the relative univariate feature selection</span>
    <span class="s3"># gets the correct items in a simple regression problem</span>
    <span class="s3"># with the fwe heuristic</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">n_samples=</span><span class="s4">200</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">n_informative=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">shuffle=</span><span class="s2">False, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>

    <span class="s1">univariate_filter = SelectFwe(f_regression</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s4">0.01</span><span class="s1">)</span>
    <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
    <span class="s1">X_r2 = (</span>
        <span class="s1">GenericUnivariateSelect(f_regression</span><span class="s2">, </span><span class="s1">mode=</span><span class="s5">&quot;fwe&quot;</span><span class="s2">, </span><span class="s1">param=</span><span class="s4">0.01</span><span class="s1">)</span>
        <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">.transform(X)</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">X_r2)</span>
    <span class="s1">support = univariate_filter.get_support()</span>
    <span class="s1">gtruth = np.zeros(</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">gtruth[:</span><span class="s4">5</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">assert_array_equal(support[:</span><span class="s4">5</span><span class="s1">]</span><span class="s2">, </span><span class="s1">np.ones((</span><span class="s4">5</span><span class="s2">,</span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype=bool))</span>
    <span class="s2">assert </span><span class="s1">np.sum(support[</span><span class="s4">5</span><span class="s1">:] == </span><span class="s4">1</span><span class="s1">) &lt; </span><span class="s4">2</span>


<span class="s2">def </span><span class="s1">test_selectkbest_tiebreaking():</span>
    <span class="s3"># Test whether SelectKBest actually selects k features in case of ties.</span>
    <span class="s3"># Prior to 0.11, SelectKBest would return more features than requested.</span>
    <span class="s1">Xs = [[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">dummy_score = </span><span class="s2">lambda </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y: (X[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s2">for </span><span class="s1">X </span><span class="s2">in </span><span class="s1">Xs:</span>
        <span class="s1">sel = SelectKBest(dummy_score</span><span class="s2">, </span><span class="s1">k=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">X1 = ignore_warnings(sel.fit_transform)([X]</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">X1.shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">1</span>
        <span class="s1">assert_best_scores_kept(sel)</span>

        <span class="s1">sel = SelectKBest(dummy_score</span><span class="s2">, </span><span class="s1">k=</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">X2 = ignore_warnings(sel.fit_transform)([X]</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">X2.shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">2</span>
        <span class="s1">assert_best_scores_kept(sel)</span>


<span class="s2">def </span><span class="s1">test_selectpercentile_tiebreaking():</span>
    <span class="s3"># Test if SelectPercentile selects the right n_features in case of ties.</span>
    <span class="s1">Xs = [[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">dummy_score = </span><span class="s2">lambda </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y: (X[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s2">for </span><span class="s1">X </span><span class="s2">in </span><span class="s1">Xs:</span>
        <span class="s1">sel = SelectPercentile(dummy_score</span><span class="s2">, </span><span class="s1">percentile=</span><span class="s4">34</span><span class="s1">)</span>
        <span class="s1">X1 = ignore_warnings(sel.fit_transform)([X]</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">X1.shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">1</span>
        <span class="s1">assert_best_scores_kept(sel)</span>

        <span class="s1">sel = SelectPercentile(dummy_score</span><span class="s2">, </span><span class="s1">percentile=</span><span class="s4">67</span><span class="s1">)</span>
        <span class="s1">X2 = ignore_warnings(sel.fit_transform)([X]</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">X2.shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">2</span>
        <span class="s1">assert_best_scores_kept(sel)</span>


<span class="s2">def </span><span class="s1">test_tied_pvalues():</span>
    <span class="s3"># Test whether k-best and percentiles work with tied pvalues from chi2.</span>
    <span class="s3"># chi2 will return the same p-values for the following features, but it</span>
    <span class="s3"># will return different scores.</span>
    <span class="s1">X0 = np.array([[</span><span class="s4">10000</span><span class="s2">, </span><span class="s4">9999</span><span class="s2">, </span><span class="s4">9998</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">y = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>

    <span class="s2">for </span><span class="s1">perm </span><span class="s2">in </span><span class="s1">itertools.permutations((</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)):</span>
        <span class="s1">X = X0[:</span><span class="s2">, </span><span class="s1">perm]</span>
        <span class="s1">Xt = SelectKBest(chi2</span><span class="s2">, </span><span class="s1">k=</span><span class="s4">2</span><span class="s1">).fit_transform(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">Xt.shape == (</span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s2">assert </span><span class="s4">9998 </span><span class="s2">not in </span><span class="s1">Xt</span>

        <span class="s1">Xt = SelectPercentile(chi2</span><span class="s2">, </span><span class="s1">percentile=</span><span class="s4">67</span><span class="s1">).fit_transform(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s2">assert </span><span class="s1">Xt.shape == (</span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s2">assert </span><span class="s4">9998 </span><span class="s2">not in </span><span class="s1">Xt</span>


<span class="s2">def </span><span class="s1">test_scorefunc_multilabel():</span>
    <span class="s3"># Test whether k-best and percentiles works with multilabels with chi2.</span>

    <span class="s1">X = np.array([[</span><span class="s4">10000</span><span class="s2">, </span><span class="s4">9999</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">100</span><span class="s2">, </span><span class="s4">9999</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1000</span><span class="s2">, </span><span class="s4">99</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]])</span>
    <span class="s1">y = [[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]]</span>

    <span class="s1">Xt = SelectKBest(chi2</span><span class="s2">, </span><span class="s1">k=</span><span class="s4">2</span><span class="s1">).fit_transform(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">Xt.shape == (</span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s4">0 </span><span class="s2">not in </span><span class="s1">Xt</span>

    <span class="s1">Xt = SelectPercentile(chi2</span><span class="s2">, </span><span class="s1">percentile=</span><span class="s4">67</span><span class="s1">).fit_transform(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">Xt.shape == (</span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s4">0 </span><span class="s2">not in </span><span class="s1">Xt</span>


<span class="s2">def </span><span class="s1">test_tied_scores():</span>
    <span class="s3"># Test for stable sorting in k-best with tied scores.</span>
    <span class="s1">X_train = np.array([[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]])</span>
    <span class="s1">y_train = [</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>

    <span class="s2">for </span><span class="s1">n_features </span><span class="s2">in </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s1">]:</span>
        <span class="s1">sel = SelectKBest(chi2</span><span class="s2">, </span><span class="s1">k=n_features).fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>
        <span class="s1">X_test = sel.transform([[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]])</span>
        <span class="s1">assert_array_equal(X_test[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">np.arange(</span><span class="s4">3</span><span class="s1">)[-n_features:])</span>


<span class="s2">def </span><span class="s1">test_nans():</span>
    <span class="s3"># Assert that SelectKBest and SelectPercentile can handle NaNs.</span>
    <span class="s3"># First feature has zero variance to confuse f_classif (ANOVA) and</span>
    <span class="s3"># make it return a NaN.</span>
    <span class="s1">X = [[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>

    <span class="s2">for </span><span class="s1">select </span><span class="s2">in </span><span class="s1">(</span>
        <span class="s1">SelectKBest(f_classif</span><span class="s2">, </span><span class="s1">k=</span><span class="s4">2</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s1">SelectPercentile(f_classif</span><span class="s2">, </span><span class="s1">percentile=</span><span class="s4">67</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">ignore_warnings(select.fit)(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">assert_array_equal(select.get_support(indices=</span><span class="s2">True</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.array([</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]))</span>


<span class="s2">def </span><span class="s1">test_invalid_k():</span>
    <span class="s1">X = [[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">SelectKBest(k=</span><span class="s4">4</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">GenericUnivariateSelect(mode=</span><span class="s5">&quot;k_best&quot;</span><span class="s2">, </span><span class="s1">param=</span><span class="s4">4</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_f_classif_constant_feature():</span>
    <span class="s3"># Test that f_classif warns if a feature is constant throughout.</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">X[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">2.0</span>
    <span class="s2">with </span><span class="s1">pytest.warns(UserWarning):</span>
        <span class="s1">f_classif(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s2">def </span><span class="s1">test_no_feature_selected():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s3"># Generate random uncorrelated data: a strict univariate test should</span>
    <span class="s3"># rejects all the features</span>
    <span class="s1">X = rng.rand(</span><span class="s4">40</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">y = rng.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">40</span><span class="s1">)</span>
    <span class="s1">strict_selectors = [</span>
        <span class="s1">SelectFwe(alpha=</span><span class="s4">0.01</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">,</span>
        <span class="s1">SelectFdr(alpha=</span><span class="s4">0.01</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">,</span>
        <span class="s1">SelectFpr(alpha=</span><span class="s4">0.01</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">,</span>
        <span class="s1">SelectPercentile(percentile=</span><span class="s4">0</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">,</span>
        <span class="s1">SelectKBest(k=</span><span class="s4">0</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span><span class="s2">,</span>
    <span class="s1">]</span>
    <span class="s2">for </span><span class="s1">selector </span><span class="s2">in </span><span class="s1">strict_selectors:</span>
        <span class="s1">assert_array_equal(selector.get_support()</span><span class="s2">, </span><span class="s1">np.zeros(</span><span class="s4">10</span><span class="s1">))</span>
        <span class="s2">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;No features were selected&quot;</span><span class="s1">):</span>
            <span class="s1">X_selected = selector.transform(X)</span>
        <span class="s2">assert </span><span class="s1">X_selected.shape == (</span><span class="s4">40</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_mutual_info_classif():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_classification(</span>
        <span class="s1">n_samples=</span><span class="s4">100</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">5</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">n_redundant=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">n_repeated=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">n_classes=</span><span class="s4">2</span><span class="s2">,</span>
        <span class="s1">n_clusters_per_class=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">flip_y=</span><span class="s4">0.0</span><span class="s2">,</span>
        <span class="s1">class_sep=</span><span class="s4">10</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s3"># Test in KBest mode.</span>
    <span class="s1">univariate_filter = SelectKBest(mutual_info_classif</span><span class="s2">, </span><span class="s1">k=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
    <span class="s1">X_r2 = (</span>
        <span class="s1">GenericUnivariateSelect(mutual_info_classif</span><span class="s2">, </span><span class="s1">mode=</span><span class="s5">&quot;k_best&quot;</span><span class="s2">, </span><span class="s1">param=</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">.transform(X)</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">X_r2)</span>
    <span class="s1">support = univariate_filter.get_support()</span>
    <span class="s1">gtruth = np.zeros(</span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">gtruth[:</span><span class="s4">2</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">assert_array_equal(support</span><span class="s2">, </span><span class="s1">gtruth)</span>

    <span class="s3"># Test in Percentile mode.</span>
    <span class="s1">univariate_filter = SelectPercentile(mutual_info_classif</span><span class="s2">, </span><span class="s1">percentile=</span><span class="s4">40</span><span class="s1">)</span>
    <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
    <span class="s1">X_r2 = (</span>
        <span class="s1">GenericUnivariateSelect(mutual_info_classif</span><span class="s2">, </span><span class="s1">mode=</span><span class="s5">&quot;percentile&quot;</span><span class="s2">, </span><span class="s1">param=</span><span class="s4">40</span><span class="s1">)</span>
        <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">.transform(X)</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">X_r2)</span>
    <span class="s1">support = univariate_filter.get_support()</span>
    <span class="s1">gtruth = np.zeros(</span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">gtruth[:</span><span class="s4">2</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">assert_array_equal(support</span><span class="s2">, </span><span class="s1">gtruth)</span>


<span class="s2">def </span><span class="s1">test_mutual_info_regression():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_regression(</span>
        <span class="s1">n_samples=</span><span class="s4">100</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">10</span><span class="s2">,</span>
        <span class="s1">n_informative=</span><span class="s4">2</span><span class="s2">,</span>
        <span class="s1">shuffle=</span><span class="s2">False,</span>
        <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">noise=</span><span class="s4">10</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s3"># Test in KBest mode.</span>
    <span class="s1">univariate_filter = SelectKBest(mutual_info_regression</span><span class="s2">, </span><span class="s1">k=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
    <span class="s1">assert_best_scores_kept(univariate_filter)</span>
    <span class="s1">X_r2 = (</span>
        <span class="s1">GenericUnivariateSelect(mutual_info_regression</span><span class="s2">, </span><span class="s1">mode=</span><span class="s5">&quot;k_best&quot;</span><span class="s2">, </span><span class="s1">param=</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">.transform(X)</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">X_r2)</span>
    <span class="s1">support = univariate_filter.get_support()</span>
    <span class="s1">gtruth = np.zeros(</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">gtruth[:</span><span class="s4">2</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">assert_array_equal(support</span><span class="s2">, </span><span class="s1">gtruth)</span>

    <span class="s3"># Test in Percentile mode.</span>
    <span class="s1">univariate_filter = SelectPercentile(mutual_info_regression</span><span class="s2">, </span><span class="s1">percentile=</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">X_r = univariate_filter.fit(X</span><span class="s2">, </span><span class="s1">y).transform(X)</span>
    <span class="s1">X_r2 = (</span>
        <span class="s1">GenericUnivariateSelect(mutual_info_regression</span><span class="s2">, </span><span class="s1">mode=</span><span class="s5">&quot;percentile&quot;</span><span class="s2">, </span><span class="s1">param=</span><span class="s4">20</span><span class="s1">)</span>
        <span class="s1">.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">.transform(X)</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">X_r2)</span>
    <span class="s1">support = univariate_filter.get_support()</span>
    <span class="s1">gtruth = np.zeros(</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">gtruth[:</span><span class="s4">2</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">assert_array_equal(support</span><span class="s2">, </span><span class="s1">gtruth)</span>


<span class="s2">def </span><span class="s1">test_dataframe_output_dtypes():</span>
    <span class="s0">&quot;&quot;&quot;Check that the output datafarme dtypes are the same as the input. 
 
    Non-regression test for gh-24860. 
    &quot;&quot;&quot;</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s5">&quot;pandas&quot;</span><span class="s1">)</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = load_iris(return_X_y=</span><span class="s2">True, </span><span class="s1">as_frame=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">X = X.astype(</span>
        <span class="s1">{</span>
            <span class="s5">&quot;petal length (cm)&quot;</span><span class="s1">: np.float32</span><span class="s2">,</span>
            <span class="s5">&quot;petal width (cm)&quot;</span><span class="s1">: np.float64</span><span class="s2">,</span>
        <span class="s1">}</span>
    <span class="s1">)</span>
    <span class="s1">X[</span><span class="s5">&quot;petal_width_binned&quot;</span><span class="s1">] = pd.cut(X[</span><span class="s5">&quot;petal width (cm)&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">bins=</span><span class="s4">10</span><span class="s1">)</span>

    <span class="s1">column_order = X.columns</span>

    <span class="s2">def </span><span class="s1">selector(X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s1">ranking = {</span>
            <span class="s5">&quot;sepal length (cm)&quot;</span><span class="s1">: </span><span class="s4">1</span><span class="s2">,</span>
            <span class="s5">&quot;sepal width (cm)&quot;</span><span class="s1">: </span><span class="s4">2</span><span class="s2">,</span>
            <span class="s5">&quot;petal length (cm)&quot;</span><span class="s1">: </span><span class="s4">3</span><span class="s2">,</span>
            <span class="s5">&quot;petal width (cm)&quot;</span><span class="s1">: </span><span class="s4">4</span><span class="s2">,</span>
            <span class="s5">&quot;petal_width_binned&quot;</span><span class="s1">: </span><span class="s4">5</span><span class="s2">,</span>
        <span class="s1">}</span>
        <span class="s2">return </span><span class="s1">np.asarray([ranking[name] </span><span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">column_order])</span>

    <span class="s1">univariate_filter = SelectKBest(selector</span><span class="s2">, </span><span class="s1">k=</span><span class="s4">3</span><span class="s1">).set_output(transform=</span><span class="s5">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">output = univariate_filter.fit_transform(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">assert_array_equal(</span>
        <span class="s1">output.columns</span><span class="s2">, </span><span class="s1">[</span><span class="s5">&quot;petal length (cm)&quot;</span><span class="s2">, </span><span class="s5">&quot;petal width (cm)&quot;</span><span class="s2">, </span><span class="s5">&quot;petal_width_binned&quot;</span><span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s2">for </span><span class="s1">name</span><span class="s2">, </span><span class="s1">dtype </span><span class="s2">in </span><span class="s1">output.dtypes.items():</span>
        <span class="s2">assert </span><span class="s1">dtype == X.dtypes[name]</span>
</pre>
</body>
</html>