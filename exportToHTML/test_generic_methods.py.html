<html>
<head>
<title>test_generic_methods.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_generic_methods.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot;Tests that use cross-checks for generic methods 
 
Should be easy to check consistency across models 
Does not cover tsa 
 
Initial cases copied from test_shrink_pickle 
 
Created on Wed Oct 30 14:01:27 2013 
 
Author: Josef Perktold 
&quot;&quot;&quot;</span>
<span class="s3">from </span><span class="s1">statsmodels.compat.pytest </span><span class="s3">import </span><span class="s1">pytest_warns</span>
<span class="s3">from </span><span class="s1">statsmodels.compat.pandas </span><span class="s3">import </span><span class="s1">assert_index_equal</span><span class="s3">, </span><span class="s1">assert_series_equal</span>
<span class="s3">from </span><span class="s1">statsmodels.compat.platform </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">PLATFORM_LINUX32</span><span class="s3">,</span>
    <span class="s1">PLATFORM_OSX</span><span class="s3">,</span>
    <span class="s1">PLATFORM_WIN32</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">statsmodels.compat.scipy </span><span class="s3">import </span><span class="s1">SCIPY_GT_14</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">numpy.testing </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">assert_</span><span class="s3">,</span>
    <span class="s1">assert_allclose</span><span class="s3">,</span>
    <span class="s1">assert_array_equal</span><span class="s3">,</span>
    <span class="s1">assert_equal</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">import </span><span class="s1">pandas </span><span class="s3">as </span><span class="s1">pd</span>
<span class="s3">import </span><span class="s1">pytest</span>

<span class="s3">import </span><span class="s1">statsmodels.api </span><span class="s3">as </span><span class="s1">sm</span>
<span class="s3">from </span><span class="s1">statsmodels.formula.api </span><span class="s3">import </span><span class="s1">glm</span><span class="s3">, </span><span class="s1">ols</span>
<span class="s3">import </span><span class="s1">statsmodels.tools._testing </span><span class="s3">as </span><span class="s1">smt</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s3">import </span><span class="s1">HessianInversionWarning</span>


<span class="s3">class </span><span class="s1">CheckGenericMixin:</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">nobs = </span><span class="s4">500</span>
        <span class="s1">np.random.seed(</span><span class="s4">987689</span><span class="s1">)</span>
        <span class="s1">x = np.random.randn(nobs</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span>
        <span class="s1">x = sm.add_constant(x)</span>
        <span class="s1">cls.exog = x</span>
        <span class="s1">cls.xf = </span><span class="s4">0.25 </span><span class="s1">* np.ones((</span><span class="s4">2</span><span class="s3">, </span><span class="s4">4</span><span class="s1">))</span>
        <span class="s1">cls.predict_kwds = {}</span>
        <span class="s1">cls.transform_index = </span><span class="s3">None</span>

    <span class="s3">def </span><span class="s1">test_ttest_tvalues(self):</span>
        <span class="s0"># test that t_test has same results a params, bse, tvalues, ...</span>
        <span class="s1">smt.check_ttest_tvalues(self.results)</span>

        <span class="s1">res = self.results</span>
        <span class="s1">mat = np.eye(len(res.params))</span>

        <span class="s1">tt = res.t_test(mat[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">string_confint = </span><span class="s3">lambda </span><span class="s1">alpha: </span><span class="s5">&quot;[%4.3F      %4.3F]&quot; </span><span class="s1">% (</span>
                                       <span class="s1">alpha / </span><span class="s4">2</span><span class="s3">, </span><span class="s4">1</span><span class="s1">- alpha / </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">summ = tt.summary()   </span><span class="s0"># smoke test for #1323</span>
        <span class="s1">assert_allclose(tt.pvalue</span><span class="s3">, </span><span class="s1">res.pvalues[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">5e-10</span><span class="s1">)</span>
        <span class="s1">assert_(string_confint(</span><span class="s4">0.05</span><span class="s1">) </span><span class="s3">in </span><span class="s1">str(summ))</span>

        <span class="s0"># issue #3116 alpha not used in column headers</span>
        <span class="s1">summ = tt.summary(alpha=</span><span class="s4">0.1</span><span class="s1">)</span>
        <span class="s1">ss = </span><span class="s5">&quot;[0.05       0.95]&quot;   </span><span class="s0"># different formatting</span>
        <span class="s1">assert_(ss </span><span class="s3">in </span><span class="s1">str(summ))</span>

        <span class="s1">summf = tt.summary_frame(alpha=</span><span class="s4">0.1</span><span class="s1">)</span>
        <span class="s1">pvstring_use_t = </span><span class="s5">'P&gt;|z|' </span><span class="s3">if </span><span class="s1">res.use_t </span><span class="s3">is False else </span><span class="s5">'P&gt;|t|'</span>
        <span class="s1">tstring_use_t = </span><span class="s5">'z' </span><span class="s3">if </span><span class="s1">res.use_t </span><span class="s3">is False else </span><span class="s5">'t'</span>
        <span class="s1">cols = [</span><span class="s5">'coef'</span><span class="s3">, </span><span class="s5">'std err'</span><span class="s3">, </span><span class="s1">tstring_use_t</span><span class="s3">, </span><span class="s1">pvstring_use_t</span><span class="s3">,</span>
                <span class="s5">'Conf. Int. Low'</span><span class="s3">, </span><span class="s5">'Conf. Int. Upp.'</span><span class="s1">]</span>
        <span class="s1">assert_array_equal(summf.columns.values</span><span class="s3">, </span><span class="s1">cols)</span>

    <span class="s3">def </span><span class="s1">test_ftest_pvalues(self):</span>
        <span class="s1">smt.check_ftest_pvalues(self.results)</span>

    <span class="s3">def </span><span class="s1">test_fitted(self):</span>
        <span class="s1">smt.check_fitted(self.results)</span>

    <span class="s3">def </span><span class="s1">test_predict_types(self):</span>
        <span class="s1">smt.check_predict_types(self.results)</span>

    <span class="s3">def </span><span class="s1">test_zero_constrained(self):</span>
        <span class="s0"># not completely generic yet</span>
        <span class="s3">if </span><span class="s1">(isinstance(self.results.model</span><span class="s3">, </span><span class="s1">(sm.GEE))):</span>
            <span class="s0"># GEE does not subclass LikelihoodModel</span>
            <span class="s1">pytest.skip(</span><span class="s5">'GEE does not subclass LikelihoodModel'</span><span class="s1">)</span>

        <span class="s1">use_start_params = </span><span class="s3">not </span><span class="s1">isinstance(self.results.model</span><span class="s3">,</span>
                                          <span class="s1">(sm.RLM</span><span class="s3">, </span><span class="s1">sm.OLS</span><span class="s3">, </span><span class="s1">sm.WLS))</span>
        <span class="s1">self.use_start_params = use_start_params  </span><span class="s0"># attach for _get_constrained</span>

        <span class="s1">keep_index = list(range(self.results.model.exog.shape[</span><span class="s4">1</span><span class="s1">]))</span>
        <span class="s0"># index for params might include extra params</span>
        <span class="s1">keep_index_p = list(range(self.results.params.shape[</span><span class="s4">0</span><span class="s1">]))</span>
        <span class="s1">drop_index = [</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">drop_index:</span>
            <span class="s3">del </span><span class="s1">keep_index[i]</span>
            <span class="s3">del </span><span class="s1">keep_index_p[i]</span>

        <span class="s3">if </span><span class="s1">use_start_params:</span>
            <span class="s1">res1 = self.results.model._fit_zeros(keep_index</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s4">500</span><span class="s3">,</span>
                                        <span class="s1">start_params=self.results.params)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">res1 = self.results.model._fit_zeros(keep_index</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s4">500</span><span class="s1">)</span>

        <span class="s1">res2 = self._get_constrained(keep_index</span><span class="s3">, </span><span class="s1">keep_index_p)</span>

        <span class="s1">assert_allclose(res1.params[keep_index_p]</span><span class="s3">, </span><span class="s1">res2.params</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s3">,</span>
                        <span class="s1">atol=</span><span class="s4">1e-10</span><span class="s1">)</span>
        <span class="s1">assert_equal(res1.params[drop_index]</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">assert_allclose(res1.bse[keep_index_p]</span><span class="s3">, </span><span class="s1">res2.bse</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s3">,</span>
                        <span class="s1">atol=</span><span class="s4">1e-10</span><span class="s1">)</span>
        <span class="s1">assert_equal(res1.bse[drop_index]</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s0"># OSX has many slight failures on this test</span>
        <span class="s1">tol = </span><span class="s4">1e-8 </span><span class="s3">if </span><span class="s1">PLATFORM_OSX </span><span class="s3">else </span><span class="s4">1e-10</span>
        <span class="s1">tvals1 = res1.tvalues[keep_index_p]</span>
        <span class="s1">assert_allclose(tvals1</span><span class="s3">, </span><span class="s1">res2.tvalues</span><span class="s3">, </span><span class="s1">rtol=tol</span><span class="s3">, </span><span class="s1">atol=tol)</span>

        <span class="s0"># See gh5993</span>
        <span class="s3">if </span><span class="s1">PLATFORM_LINUX32 </span><span class="s3">or </span><span class="s1">SCIPY_GT_14:</span>
            <span class="s1">pvals1 = res1.pvalues[keep_index_p]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">pvals1 = res1.pvalues[keep_index_p]</span>
        <span class="s1">assert_allclose(pvals1</span><span class="s3">, </span><span class="s1">res2.pvalues</span><span class="s3">, </span><span class="s1">rtol=tol</span><span class="s3">, </span><span class="s1">atol=tol)</span>

        <span class="s3">if </span><span class="s1">hasattr(res1</span><span class="s3">, </span><span class="s5">'resid'</span><span class="s1">):</span>
            <span class="s0"># discrete models, Logit do not have `resid` yet</span>
            <span class="s0"># atol discussion at gh-5158</span>
            <span class="s1">rtol = </span><span class="s4">1e-10</span>
            <span class="s1">atol = </span><span class="s4">1e-12</span>
            <span class="s3">if </span><span class="s1">PLATFORM_OSX </span><span class="s3">or </span><span class="s1">PLATFORM_WIN32:</span>
                <span class="s0"># GH 5628</span>
                <span class="s1">rtol = </span><span class="s4">1e-8</span>
                <span class="s1">atol = </span><span class="s4">1e-10</span>
            <span class="s1">assert_allclose(res1.resid</span><span class="s3">, </span><span class="s1">res2.resid</span><span class="s3">, </span><span class="s1">rtol=rtol</span><span class="s3">, </span><span class="s1">atol=atol)</span>

        <span class="s1">ex = self.results.model.exog.mean(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">predicted1 = res1.predict(ex</span><span class="s3">, </span><span class="s1">**self.predict_kwds)</span>
        <span class="s1">predicted2 = res2.predict(ex[keep_index]</span><span class="s3">, </span><span class="s1">**self.predict_kwds)</span>
        <span class="s1">assert_allclose(predicted1</span><span class="s3">, </span><span class="s1">predicted2</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s1">)</span>

        <span class="s1">ex = self.results.model.exog[:</span><span class="s4">5</span><span class="s1">]</span>
        <span class="s1">predicted1 = res1.predict(ex</span><span class="s3">, </span><span class="s1">**self.predict_kwds)</span>
        <span class="s1">predicted2 = res2.predict(ex[:</span><span class="s3">, </span><span class="s1">keep_index]</span><span class="s3">, </span><span class="s1">**self.predict_kwds)</span>
        <span class="s1">assert_allclose(predicted1</span><span class="s3">, </span><span class="s1">predicted2</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_get_constrained(self</span><span class="s3">, </span><span class="s1">keep_index</span><span class="s3">, </span><span class="s1">keep_index_p):</span>
        <span class="s0"># override in some test classes, no fit_kwds yet, e.g. cov_type</span>
        <span class="s1">mod2 = self.results.model</span>
        <span class="s1">mod_cls = mod2.__class__</span>
        <span class="s1">init_kwds = mod2._get_init_kwds()</span>
        <span class="s1">mod = mod_cls(mod2.endog</span><span class="s3">, </span><span class="s1">mod2.exog[:</span><span class="s3">, </span><span class="s1">keep_index]</span><span class="s3">, </span><span class="s1">**init_kwds)</span>
        <span class="s3">if </span><span class="s1">self.use_start_params:</span>
            <span class="s1">res = mod.fit(start_params=self.results.params[keep_index_p]</span><span class="s3">,</span>
                          <span class="s1">maxiter=</span><span class="s4">500</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">res = mod.fit(maxiter=</span><span class="s4">500</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">res</span>

    <span class="s3">def </span><span class="s1">test_zero_collinear(self):</span>
        <span class="s0"># not completely generic yet</span>
        <span class="s3">if </span><span class="s1">isinstance(self.results.model</span><span class="s3">, </span><span class="s1">(sm.GEE)):</span>
            <span class="s1">pytest.skip(</span><span class="s5">'Not completely generic yet'</span><span class="s1">)</span>

        <span class="s1">use_start_params = </span><span class="s3">not </span><span class="s1">isinstance(self.results.model</span><span class="s3">,</span>
                                          <span class="s1">(sm.RLM</span><span class="s3">, </span><span class="s1">sm.OLS</span><span class="s3">, </span><span class="s1">sm.WLS</span><span class="s3">, </span><span class="s1">sm.GLM))</span>
        <span class="s1">self.use_start_params = use_start_params  </span><span class="s0"># attach for _get_constrained</span>
        <span class="s1">keep_index = list(range(self.results.model.exog.shape[</span><span class="s4">1</span><span class="s1">]))</span>
        <span class="s0"># index for params might include extra params</span>
        <span class="s1">keep_index_p = list(range(self.results.params.shape[</span><span class="s4">0</span><span class="s1">]))</span>
        <span class="s1">drop_index = []</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">drop_index:</span>
            <span class="s3">del </span><span class="s1">keep_index[i]</span>
            <span class="s3">del </span><span class="s1">keep_index_p[i]</span>

        <span class="s1">keep_index_p = list(range(self.results.params.shape[</span><span class="s4">0</span><span class="s1">]))</span>

        <span class="s0"># create collinear model</span>
        <span class="s1">mod2 = self.results.model</span>
        <span class="s1">mod_cls = mod2.__class__</span>
        <span class="s1">init_kwds = mod2._get_init_kwds()</span>
        <span class="s1">ex = np.column_stack((mod2.exog</span><span class="s3">, </span><span class="s1">mod2.exog))</span>
        <span class="s1">mod = mod_cls(mod2.endog</span><span class="s3">, </span><span class="s1">ex</span><span class="s3">, </span><span class="s1">**init_kwds)</span>

        <span class="s1">keep_index = list(range(self.results.model.exog.shape[</span><span class="s4">1</span><span class="s1">]))</span>
        <span class="s1">keep_index_p = list(range(self.results.model.exog.shape[</span><span class="s4">1</span><span class="s1">]))</span>
        <span class="s1">k_vars = ex.shape[</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">k_extra = </span><span class="s4">0</span>
        <span class="s3">if </span><span class="s1">hasattr(mod</span><span class="s3">, </span><span class="s5">'k_extra'</span><span class="s1">) </span><span class="s3">and </span><span class="s1">mod.k_extra &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">keep_index_p += list(range(k_vars</span><span class="s3">, </span><span class="s1">k_vars + mod.k_extra))</span>
            <span class="s1">k_extra = mod.k_extra</span>

        <span class="s0"># TODO: Can we choose a test case without this issue?</span>
        <span class="s0">#  If not, should we be getting this warning for all</span>
        <span class="s0">#  model subclasses?</span>
        <span class="s1">warn_cls = HessianInversionWarning </span><span class="s3">if </span><span class="s1">isinstance(mod</span><span class="s3">, </span><span class="s1">sm.GLM) </span><span class="s3">else None</span>

        <span class="s1">cov_types = [</span><span class="s5">'nonrobust'</span><span class="s3">, </span><span class="s5">'HC0'</span><span class="s1">]</span>

        <span class="s3">for </span><span class="s1">cov_type </span><span class="s3">in </span><span class="s1">cov_types:</span>
            <span class="s0"># Note: for RLM we only check default when cov_type is 'nonrobust'</span>
            <span class="s0"># cov_type is otherwise ignored</span>
            <span class="s3">if </span><span class="s1">cov_type != </span><span class="s5">'nonrobust' </span><span class="s3">and </span><span class="s1">(isinstance(self.results.model</span><span class="s3">,</span>
                                                       <span class="s1">sm.RLM)):</span>
                <span class="s3">return</span>

            <span class="s3">if </span><span class="s1">use_start_params:</span>
                <span class="s1">start_params = np.zeros(k_vars + k_extra)</span>
                <span class="s1">method = self.results.mle_settings[</span><span class="s5">'optimizer'</span><span class="s1">]</span>
                <span class="s0"># string in `method` is not mutable, so no need for copy</span>
                <span class="s1">sp = self.results.mle_settings[</span><span class="s5">'start_params'</span><span class="s1">].copy()</span>
                <span class="s3">if </span><span class="s1">self.transform_index </span><span class="s3">is not None</span><span class="s1">:</span>
                    <span class="s0"># work around internal transform_params, currently in NB</span>
                    <span class="s1">sp[self.transform_index] = np.exp(sp[self.transform_index])</span>

                <span class="s1">start_params[keep_index_p] = sp</span>
                <span class="s3">with </span><span class="s1">pytest_warns(warn_cls):</span>
                    <span class="s1">res1 = mod._fit_collinear(cov_type=cov_type</span><span class="s3">,</span>
                                              <span class="s1">start_params=start_params</span><span class="s3">,</span>
                                              <span class="s1">method=method</span><span class="s3">, </span><span class="s1">disp=</span><span class="s4">0</span><span class="s1">)</span>
                <span class="s3">if </span><span class="s1">cov_type != </span><span class="s5">'nonrobust'</span><span class="s1">:</span>
                    <span class="s0"># reestimate original model to get robust cov</span>
                    <span class="s3">with </span><span class="s1">pytest_warns(warn_cls):</span>
                        <span class="s1">res2 = self.results.model.fit(cov_type=cov_type</span><span class="s3">,</span>
                                                      <span class="s1">start_params=sp</span><span class="s3">,</span>
                                                      <span class="s1">method=method</span><span class="s3">, </span><span class="s1">disp=</span><span class="s4">0</span><span class="s1">)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s3">with </span><span class="s1">pytest_warns(warn_cls):</span>
                    <span class="s0"># more special casing RLM</span>
                    <span class="s3">if </span><span class="s1">(isinstance(self.results.model</span><span class="s3">, </span><span class="s1">(sm.RLM))):</span>
                        <span class="s1">res1 = mod._fit_collinear()</span>
                    <span class="s3">else</span><span class="s1">:</span>
                        <span class="s1">res1 = mod._fit_collinear(cov_type=cov_type)</span>
                <span class="s3">if </span><span class="s1">cov_type != </span><span class="s5">'nonrobust'</span><span class="s1">:</span>
                    <span class="s0"># reestimate original model to get robust cov</span>
                    <span class="s1">res2 = self.results.model.fit(cov_type=cov_type)</span>

            <span class="s3">if </span><span class="s1">cov_type == </span><span class="s5">'nonrobust'</span><span class="s1">:</span>
                <span class="s1">res2 = self.results</span>

            <span class="s0"># check fit optimizer arguments, if mle_settings is available</span>
            <span class="s3">if </span><span class="s1">hasattr(res2</span><span class="s3">, </span><span class="s5">'mle_settings'</span><span class="s1">):</span>
                <span class="s1">assert_equal(res1.results_constrained.mle_settings[</span><span class="s5">'optimizer'</span><span class="s1">]</span><span class="s3">,</span>
                             <span class="s1">res2.mle_settings[</span><span class="s5">'optimizer'</span><span class="s1">])</span>
                <span class="s3">if </span><span class="s5">'start_params' </span><span class="s3">in </span><span class="s1">res2.mle_settings:</span>
                    <span class="s1">spc = res1.results_constrained.mle_settings[</span><span class="s5">'start_params'</span><span class="s1">]</span>
                    <span class="s1">assert_allclose(spc</span><span class="s3">,</span>
                                    <span class="s1">res2.mle_settings[</span><span class="s5">'start_params'</span><span class="s1">]</span><span class="s3">,</span>
                                    <span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s3">, </span><span class="s1">atol=</span><span class="s4">1e-20</span><span class="s1">)</span>
                    <span class="s1">assert_equal(res1.mle_settings[</span><span class="s5">'optimizer'</span><span class="s1">]</span><span class="s3">,</span>
                                 <span class="s1">res2.mle_settings[</span><span class="s5">'optimizer'</span><span class="s1">])</span>
                    <span class="s1">assert_allclose(res1.mle_settings[</span><span class="s5">'start_params'</span><span class="s1">]</span><span class="s3">,</span>
                                    <span class="s1">res2.mle_settings[</span><span class="s5">'start_params'</span><span class="s1">]</span><span class="s3">,</span>
                                    <span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s3">, </span><span class="s1">atol=</span><span class="s4">1e-20</span><span class="s1">)</span>

            <span class="s0"># Poisson has reduced precision in params, difficult optimization?</span>
            <span class="s1">assert_allclose(res1.params[keep_index_p]</span><span class="s3">, </span><span class="s1">res2.params</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>
            <span class="s1">assert_allclose(res1.params[drop_index]</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s1">)</span>
            <span class="s1">assert_allclose(res1.bse[keep_index_p]</span><span class="s3">, </span><span class="s1">res2.bse</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-8</span><span class="s1">)</span>
            <span class="s1">assert_allclose(res1.bse[drop_index]</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s1">)</span>
            <span class="s1">tvals1 = res1.tvalues[keep_index_p]</span>
            <span class="s1">assert_allclose(tvals1</span><span class="s3">, </span><span class="s1">res2.tvalues</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">5e-8</span><span class="s1">)</span>

            <span class="s0"># See gh5993</span>
            <span class="s3">if </span><span class="s1">PLATFORM_LINUX32 </span><span class="s3">or </span><span class="s1">SCIPY_GT_14:</span>
                <span class="s1">pvals1 = res1.pvalues[keep_index_p]</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">pvals1 = res1.pvalues[keep_index_p]</span>
            <span class="s1">assert_allclose(pvals1</span><span class="s3">, </span><span class="s1">res2.pvalues</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s3">, </span><span class="s1">atol=</span><span class="s4">1e-30</span><span class="s1">)</span>

            <span class="s3">if </span><span class="s1">hasattr(res1</span><span class="s3">, </span><span class="s5">'resid'</span><span class="s1">):</span>
                <span class="s0"># discrete models, Logit do not have `resid` yet</span>
                <span class="s1">assert_allclose(res1.resid</span><span class="s3">, </span><span class="s1">res2.resid</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s3">, </span><span class="s1">atol=</span><span class="s4">1e-10</span><span class="s1">)</span>

            <span class="s1">ex = res1.model.exog.mean(</span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">predicted1 = res1.predict(ex</span><span class="s3">, </span><span class="s1">**self.predict_kwds)</span>
            <span class="s1">predicted2 = res2.predict(ex[keep_index]</span><span class="s3">, </span><span class="s1">**self.predict_kwds)</span>
            <span class="s1">assert_allclose(predicted1</span><span class="s3">, </span><span class="s1">predicted2</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-8</span><span class="s3">, </span><span class="s1">atol=</span><span class="s4">1e-11</span><span class="s1">)</span>

            <span class="s1">ex = res1.model.exog[:</span><span class="s4">5</span><span class="s1">]</span>
            <span class="s1">kwds = getattr(self</span><span class="s3">, </span><span class="s5">'predict_kwds_5'</span><span class="s3">, </span><span class="s1">{})</span>

            <span class="s1">predicted1 = res1.predict(ex</span><span class="s3">, </span><span class="s1">**kwds)</span>
            <span class="s1">predicted2 = res2.predict(ex[:</span><span class="s3">, </span><span class="s1">keep_index]</span><span class="s3">, </span><span class="s1">**kwds)</span>
            <span class="s1">assert_allclose(predicted1</span><span class="s3">, </span><span class="s1">predicted2</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-8</span><span class="s3">, </span><span class="s1">atol=</span><span class="s4">1e-11</span><span class="s1">)</span>


<span class="s0">#########  subclasses for individual models, unchanged from test_shrink_pickle</span>
<span class="s0"># TODO: check if setup_class is faster than setup</span>

<span class="s3">class </span><span class="s1">TestGenericOLS(CheckGenericMixin):</span>

    <span class="s3">def </span><span class="s1">setup_method(self):</span>
        <span class="s0">#fit for each test, because results will be changed by test</span>
        <span class="s1">x = self.exog</span>
        <span class="s1">np.random.seed(</span><span class="s4">987689</span><span class="s1">)</span>
        <span class="s1">y = x.sum(</span><span class="s4">1</span><span class="s1">) + np.random.randn(x.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">self.results = sm.OLS(y</span><span class="s3">, </span><span class="s1">self.exog).fit()</span>


<span class="s3">class </span><span class="s1">TestGenericOLSOneExog(CheckGenericMixin):</span>
    <span class="s0"># check with single regressor (no constant)</span>

    <span class="s3">def </span><span class="s1">setup_method(self):</span>
        <span class="s0">#fit for each test, because results will be changed by test</span>
        <span class="s1">x = self.exog[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">np.random.seed(</span><span class="s4">987689</span><span class="s1">)</span>
        <span class="s1">y = x + np.random.randn(x.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">self.results = sm.OLS(y</span><span class="s3">, </span><span class="s1">x).fit()</span>

    <span class="s3">def </span><span class="s1">test_zero_constrained(self):</span>
        <span class="s0"># override, we cannot remove the only regressor</span>
        <span class="s1">pytest.skip(</span><span class="s5">'Override since cannot remove the only regressor'</span><span class="s1">)</span>
        <span class="s3">pass</span>


<span class="s3">class </span><span class="s1">TestGenericWLS(CheckGenericMixin):</span>

    <span class="s3">def </span><span class="s1">setup_method(self):</span>
        <span class="s0">#fit for each test, because results will be changed by test</span>
        <span class="s1">x = self.exog</span>
        <span class="s1">np.random.seed(</span><span class="s4">987689</span><span class="s1">)</span>
        <span class="s1">y = x.sum(</span><span class="s4">1</span><span class="s1">) + np.random.randn(x.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">self.results = sm.WLS(y</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">, </span><span class="s1">weights=np.ones(len(y))).fit()</span>


<span class="s3">class </span><span class="s1">TestGenericPoisson(CheckGenericMixin):</span>

    <span class="s3">def </span><span class="s1">setup_method(self):</span>
        <span class="s0">#fit for each test, because results will be changed by test</span>
        <span class="s1">x = self.exog</span>
        <span class="s1">np.random.seed(</span><span class="s4">987689</span><span class="s1">)</span>
        <span class="s1">y_count = np.random.poisson(np.exp(x.sum(</span><span class="s4">1</span><span class="s1">) - x.mean()))</span>
        <span class="s1">model = sm.Poisson(y_count</span><span class="s3">, </span><span class="s1">x)</span>
        <span class="s0"># use start_params to converge faster</span>
        <span class="s1">start_params = np.array([</span><span class="s4">0.75334818</span><span class="s3">, </span><span class="s4">0.99425553</span><span class="s3">, </span><span class="s4">1.00494724</span><span class="s3">, </span><span class="s4">1.00247112</span><span class="s1">])</span>
        <span class="s1">self.results = model.fit(start_params=start_params</span><span class="s3">, </span><span class="s1">method=</span><span class="s5">'bfgs'</span><span class="s3">,</span>
                                 <span class="s1">disp=</span><span class="s4">0</span><span class="s1">)</span>


<span class="s3">class </span><span class="s1">TestGenericPoissonOffset(CheckGenericMixin):</span>

    <span class="s3">def </span><span class="s1">setup_method(self):</span>
        <span class="s0">#fit for each test, because results will be changed by test</span>
        <span class="s1">x = self.exog</span>
        <span class="s1">nobs = x.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">np.random.seed(</span><span class="s4">987689</span><span class="s1">)</span>
        <span class="s1">y_count = np.random.poisson(np.exp(x.sum(</span><span class="s4">1</span><span class="s1">) - x.mean()))</span>
        <span class="s1">model = sm.Poisson(y_count</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">offset=</span><span class="s4">0.01 </span><span class="s1">* np.ones(nobs)</span><span class="s3">,</span>
                           <span class="s1">exposure=np.ones(nobs))  </span><span class="s0"># bug with default</span>
        <span class="s0"># use start_params to converge faster</span>
        <span class="s1">start_params = np.array([</span><span class="s4">0.75334818</span><span class="s3">, </span><span class="s4">0.99425553</span><span class="s3">, </span><span class="s4">1.00494724</span><span class="s3">, </span><span class="s4">1.00247112</span><span class="s1">])</span>
        <span class="s1">self.results = model.fit(start_params=start_params</span><span class="s3">, </span><span class="s1">method=</span><span class="s5">'bfgs'</span><span class="s3">,</span>
                                 <span class="s1">disp=</span><span class="s4">0</span><span class="s1">)</span>

        <span class="s1">self.predict_kwds_5 = dict(exposure=</span><span class="s4">0.01 </span><span class="s1">* np.ones(</span><span class="s4">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">offset=np.ones(</span><span class="s4">5</span><span class="s1">))</span>
        <span class="s1">self.predict_kwds = dict(exposure=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">offset=</span><span class="s4">0</span><span class="s1">)</span>


<span class="s3">class </span><span class="s1">TestGenericNegativeBinomial(CheckGenericMixin):</span>

    <span class="s3">def </span><span class="s1">setup_method(self):</span>
        <span class="s0">#fit for each test, because results will be changed by test</span>
        <span class="s1">np.random.seed(</span><span class="s4">987689</span><span class="s1">)</span>
        <span class="s1">data = sm.datasets.randhie.load()</span>
        <span class="s1">data.exog = np.asarray(data.exog)</span>
        <span class="s1">data.endog = np.asarray(data.endog)</span>
        <span class="s1">exog = sm.add_constant(data.exog</span><span class="s3">, </span><span class="s1">prepend=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">mod = sm.NegativeBinomial(data.endog</span><span class="s3">, </span><span class="s1">exog)</span>
        <span class="s1">start_params = np.array([-</span><span class="s4">0.05783623</span><span class="s3">, </span><span class="s1">-</span><span class="s4">0.26655806</span><span class="s3">,  </span><span class="s4">0.04109148</span><span class="s3">, </span><span class="s1">-</span><span class="s4">0.03815837</span><span class="s3">,</span>
                                 <span class="s4">0.2685168 </span><span class="s3">,   </span><span class="s4">0.03811594</span><span class="s3">, </span><span class="s1">-</span><span class="s4">0.04426238</span><span class="s3">,  </span><span class="s4">0.01614795</span><span class="s3">,</span>
                                 <span class="s4">0.17490962</span><span class="s3">,  </span><span class="s4">0.66461151</span><span class="s3">,   </span><span class="s4">1.2925957 </span><span class="s1">])</span>
        <span class="s1">self.results = mod.fit(start_params=start_params</span><span class="s3">, </span><span class="s1">disp=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s4">500</span><span class="s1">)</span>
        <span class="s1">self.transform_index = -</span><span class="s4">1</span>


<span class="s3">class </span><span class="s1">TestGenericLogit(CheckGenericMixin):</span>

    <span class="s3">def </span><span class="s1">setup_method(self):</span>
        <span class="s0">#fit for each test, because results will be changed by test</span>
        <span class="s1">x = self.exog</span>
        <span class="s1">nobs = x.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">np.random.seed(</span><span class="s4">987689</span><span class="s1">)</span>
        <span class="s1">y_bin = (np.random.rand(nobs) &lt; </span><span class="s4">1.0 </span><span class="s1">/ (</span><span class="s4">1 </span><span class="s1">+ np.exp(x.sum(</span><span class="s4">1</span><span class="s1">) - x.mean()))).astype(int)</span>
        <span class="s1">model = sm.Logit(y_bin</span><span class="s3">, </span><span class="s1">x)  </span><span class="s0">#, exposure=np.ones(nobs), offset=np.zeros(nobs)) #bug with default</span>
        <span class="s0"># use start_params to converge faster</span>
        <span class="s1">start_params = np.array([-</span><span class="s4">0.73403806</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1.00901514</span><span class="s3">, </span><span class="s1">-</span><span class="s4">0.97754543</span><span class="s3">, </span><span class="s1">-</span><span class="s4">0.95648212</span><span class="s1">])</span>
        <span class="s3">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s3">,</span>
                          <span class="s1">match=</span><span class="s5">&quot;Keyword arguments have been passed&quot;</span><span class="s1">):</span>
            <span class="s1">self.results = model.fit(start_params=start_params</span><span class="s3">,</span>
                                     <span class="s1">method=</span><span class="s5">'bfgs'</span><span class="s3">, </span><span class="s1">disp=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">tol=</span><span class="s4">1e-5</span><span class="s1">)</span>


<span class="s3">class </span><span class="s1">TestGenericRLM(CheckGenericMixin):</span>

    <span class="s3">def </span><span class="s1">setup_method(self):</span>
        <span class="s0">#fit for each test, because results will be changed by test</span>
        <span class="s1">x = self.exog</span>
        <span class="s1">np.random.seed(</span><span class="s4">987689</span><span class="s1">)</span>
        <span class="s1">y = x.sum(</span><span class="s4">1</span><span class="s1">) + np.random.randn(x.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">self.results = sm.RLM(y</span><span class="s3">, </span><span class="s1">self.exog).fit()</span>


<span class="s3">class </span><span class="s1">TestGenericGLM(CheckGenericMixin):</span>

    <span class="s3">def </span><span class="s1">setup_method(self):</span>
        <span class="s0">#fit for each test, because results will be changed by test</span>
        <span class="s1">x = self.exog</span>
        <span class="s1">np.random.seed(</span><span class="s4">987689</span><span class="s1">)</span>
        <span class="s1">y = x.sum(</span><span class="s4">1</span><span class="s1">) + np.random.randn(x.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">self.results = sm.GLM(y</span><span class="s3">, </span><span class="s1">self.exog).fit()</span>


<span class="s3">class </span><span class="s1">TestGenericGLMPoissonOffset(CheckGenericMixin):</span>

    <span class="s3">def </span><span class="s1">setup_method(self):</span>
        <span class="s0">#fit for each test, because results will be changed by test</span>
        <span class="s1">x = self.exog</span>
        <span class="s1">nobs = x.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">np.random.seed(</span><span class="s4">987689</span><span class="s1">)</span>
        <span class="s1">y_count = np.random.poisson(np.exp(x.sum(</span><span class="s4">1</span><span class="s1">) - x.mean()))</span>
        <span class="s1">model = sm.GLM(y_count</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">family=sm.families.Poisson()</span><span class="s3">,</span>
                       <span class="s1">offset=</span><span class="s4">0.01 </span><span class="s1">* np.ones(nobs)</span><span class="s3">,</span>
                       <span class="s1">exposure=np.ones(nobs))</span>
        <span class="s0"># use start_params to converge faster</span>
        <span class="s1">start_params = np.array([</span><span class="s4">0.75334818</span><span class="s3">, </span><span class="s4">0.99425553</span><span class="s3">, </span><span class="s4">1.00494724</span><span class="s3">, </span><span class="s4">1.00247112</span><span class="s1">])</span>
        <span class="s1">self.results = model.fit(start_params=start_params</span><span class="s3">, </span><span class="s1">method=</span><span class="s5">'bfgs'</span><span class="s3">,</span>
                                 <span class="s1">disp=</span><span class="s4">0</span><span class="s1">)</span>

        <span class="s1">self.predict_kwds_5 = dict(exposure=</span><span class="s4">0.01 </span><span class="s1">* np.ones(</span><span class="s4">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">offset=np.ones(</span><span class="s4">5</span><span class="s1">))</span>
        <span class="s1">self.predict_kwds = dict(exposure=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">offset=</span><span class="s4">0</span><span class="s1">)</span>


<span class="s3">class </span><span class="s1">TestGenericGEEPoisson(CheckGenericMixin):</span>

    <span class="s3">def </span><span class="s1">setup_method(self):</span>
        <span class="s0">#fit for each test, because results will be changed by test</span>
        <span class="s1">x = self.exog</span>
        <span class="s1">np.random.seed(</span><span class="s4">987689</span><span class="s1">)</span>
        <span class="s1">y_count = np.random.poisson(np.exp(x.sum(</span><span class="s4">1</span><span class="s1">) - x.mean()))</span>
        <span class="s1">groups = np.random.randint(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">4</span><span class="s3">, </span><span class="s1">size=x.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s0"># use start_params to speed up test, difficult convergence not tested</span>
        <span class="s1">start_params = np.array([</span><span class="s4">0.</span><span class="s3">, </span><span class="s4">1.</span><span class="s3">, </span><span class="s4">1.</span><span class="s3">, </span><span class="s4">1.</span><span class="s1">])</span>

        <span class="s1">vi = sm.cov_struct.Independence()</span>
        <span class="s1">family = sm.families.Poisson()</span>
        <span class="s1">self.results = sm.GEE(y_count</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">, </span><span class="s1">groups</span><span class="s3">, </span><span class="s1">family=family</span><span class="s3">,</span>
                                <span class="s1">cov_struct=vi).fit(start_params=start_params)</span>


<span class="s3">class </span><span class="s1">TestGenericGEEPoissonNaive(CheckGenericMixin):</span>

    <span class="s3">def </span><span class="s1">setup_method(self):</span>
        <span class="s0">#fit for each test, because results will be changed by test</span>
        <span class="s1">x = self.exog</span>
        <span class="s1">np.random.seed(</span><span class="s4">987689</span><span class="s1">)</span>
        <span class="s0">#y_count = np.random.poisson(np.exp(x.sum(1) - x.mean()))</span>
        <span class="s1">y_count = np.random.poisson(np.exp(x.sum(</span><span class="s4">1</span><span class="s1">) - x.sum(</span><span class="s4">1</span><span class="s1">).mean(</span><span class="s4">0</span><span class="s1">)))</span>
        <span class="s1">groups = np.random.randint(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">4</span><span class="s3">, </span><span class="s1">size=x.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s0"># use start_params to speed up test, difficult convergence not tested</span>
        <span class="s1">start_params = np.array([</span><span class="s4">0.</span><span class="s3">, </span><span class="s4">1.</span><span class="s3">, </span><span class="s4">1.</span><span class="s3">, </span><span class="s4">1.</span><span class="s1">])</span>

        <span class="s1">vi = sm.cov_struct.Independence()</span>
        <span class="s1">family = sm.families.Poisson()</span>
        <span class="s1">self.results = sm.GEE(y_count</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">, </span><span class="s1">groups</span><span class="s3">, </span><span class="s1">family=family</span><span class="s3">,</span>
                                <span class="s1">cov_struct=vi).fit(start_params=start_params</span><span class="s3">,</span>
                                                   <span class="s1">cov_type=</span><span class="s5">'naive'</span><span class="s1">)</span>


<span class="s3">class </span><span class="s1">TestGenericGEEPoissonBC(CheckGenericMixin):</span>

    <span class="s3">def </span><span class="s1">setup_method(self):</span>
        <span class="s0">#fit for each test, because results will be changed by test</span>
        <span class="s1">x = self.exog</span>
        <span class="s1">np.random.seed(</span><span class="s4">987689</span><span class="s1">)</span>
        <span class="s0">#y_count = np.random.poisson(np.exp(x.sum(1) - x.mean()))</span>
        <span class="s1">y_count = np.random.poisson(np.exp(x.sum(</span><span class="s4">1</span><span class="s1">) - x.sum(</span><span class="s4">1</span><span class="s1">).mean(</span><span class="s4">0</span><span class="s1">)))</span>
        <span class="s1">groups = np.random.randint(</span><span class="s4">0</span><span class="s3">, </span><span class="s4">4</span><span class="s3">, </span><span class="s1">size=x.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s0"># use start_params to speed up test, difficult convergence not tested</span>
        <span class="s1">start_params = np.array([</span><span class="s4">0.</span><span class="s3">, </span><span class="s4">1.</span><span class="s3">, </span><span class="s4">1.</span><span class="s3">, </span><span class="s4">1.</span><span class="s1">])</span>
        <span class="s0"># params_est = np.array([-0.0063238 ,  0.99463752,  1.02790201,  0.98080081])</span>

        <span class="s1">vi = sm.cov_struct.Independence()</span>
        <span class="s1">family = sm.families.Poisson()</span>
        <span class="s1">mod = sm.GEE(y_count</span><span class="s3">, </span><span class="s1">self.exog</span><span class="s3">, </span><span class="s1">groups</span><span class="s3">, </span><span class="s1">family=family</span><span class="s3">, </span><span class="s1">cov_struct=vi)</span>
        <span class="s1">self.results = mod.fit(start_params=start_params</span><span class="s3">,</span>
                               <span class="s1">cov_type=</span><span class="s5">'bias_reduced'</span><span class="s1">)</span>


<span class="s0"># Other test classes</span>

<span class="s3">class </span><span class="s1">CheckAnovaMixin:</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s3">import </span><span class="s1">statsmodels.stats.tests.test_anova </span><span class="s3">as </span><span class="s1">ttmod</span>

        <span class="s1">test = ttmod.TestAnova3()</span>
        <span class="s1">test.setup_class()</span>

        <span class="s1">cls.data = test.data.drop([</span><span class="s4">0</span><span class="s3">,</span><span class="s4">1</span><span class="s3">,</span><span class="s4">2</span><span class="s1">])</span>
        <span class="s1">cls.initialize()</span>

    <span class="s3">def </span><span class="s1">test_combined(self):</span>
        <span class="s1">res = self.res</span>
        <span class="s1">wa = res.wald_test_terms(skip_single=</span><span class="s3">False, </span><span class="s1">combine_terms=[</span><span class="s5">'Duration'</span><span class="s3">, </span><span class="s5">'Weight'</span><span class="s1">]</span><span class="s3">, </span><span class="s1">scalar=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">eye = np.eye(len(res.params))</span>
        <span class="s1">c_const = eye[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">c_w = eye[[</span><span class="s4">2</span><span class="s3">,</span><span class="s4">3</span><span class="s1">]]</span>
        <span class="s1">c_d = eye[</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">c_dw = eye[[</span><span class="s4">4</span><span class="s3">,</span><span class="s4">5</span><span class="s1">]]</span>
        <span class="s1">c_weight = eye[</span><span class="s4">2</span><span class="s1">:</span><span class="s4">6</span><span class="s1">]</span>
        <span class="s1">c_duration = eye[[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">4</span><span class="s3">, </span><span class="s4">5</span><span class="s1">]]</span>

        <span class="s1">compare_waldres(res</span><span class="s3">, </span><span class="s1">wa</span><span class="s3">, </span><span class="s1">[c_const</span><span class="s3">, </span><span class="s1">c_d</span><span class="s3">, </span><span class="s1">c_w</span><span class="s3">, </span><span class="s1">c_dw</span><span class="s3">, </span><span class="s1">c_duration</span><span class="s3">, </span><span class="s1">c_weight])</span>

    <span class="s3">def </span><span class="s1">test_categories(self):</span>
        <span class="s0"># test only multicolumn terms</span>
        <span class="s1">res = self.res</span>
        <span class="s1">wa = res.wald_test_terms(skip_single=</span><span class="s3">True, </span><span class="s1">scalar=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">eye = np.eye(len(res.params))</span>
        <span class="s1">c_w = eye[[</span><span class="s4">2</span><span class="s3">,</span><span class="s4">3</span><span class="s1">]]</span>
        <span class="s1">c_dw = eye[[</span><span class="s4">4</span><span class="s3">,</span><span class="s4">5</span><span class="s1">]]</span>

        <span class="s1">compare_waldres(res</span><span class="s3">, </span><span class="s1">wa</span><span class="s3">, </span><span class="s1">[c_w</span><span class="s3">, </span><span class="s1">c_dw])</span>


<span class="s3">def </span><span class="s1">compare_waldres(res</span><span class="s3">, </span><span class="s1">wa</span><span class="s3">, </span><span class="s1">constrasts):</span>
    <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">c </span><span class="s3">in </span><span class="s1">enumerate(constrasts):</span>
        <span class="s1">wt = res.wald_test(c</span><span class="s3">, </span><span class="s1">scalar=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">assert_allclose(wa.table.values[i</span><span class="s3">, </span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">wt.statistic)</span>
        <span class="s1">assert_allclose(wa.table.values[i</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">wt.pvalue)</span>
        <span class="s1">df = c.shape[</span><span class="s4">0</span><span class="s1">] </span><span class="s3">if </span><span class="s1">c.ndim == </span><span class="s4">2 </span><span class="s3">else </span><span class="s4">1</span>
        <span class="s1">assert_equal(wa.table.values[i</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">df)</span>
        <span class="s0"># attributes</span>
        <span class="s1">assert_allclose(wa.statistic[i]</span><span class="s3">, </span><span class="s1">wt.statistic)</span>
        <span class="s1">assert_allclose(wa.pvalues[i]</span><span class="s3">, </span><span class="s1">wt.pvalue)</span>
        <span class="s1">assert_equal(wa.df_constraints[i]</span><span class="s3">, </span><span class="s1">df)</span>
        <span class="s3">if </span><span class="s1">res.use_t:</span>
            <span class="s1">assert_equal(wa.df_denom[i]</span><span class="s3">, </span><span class="s1">res.df_resid)</span>

    <span class="s1">col_names = wa.col_names</span>
    <span class="s3">if </span><span class="s1">res.use_t:</span>
        <span class="s1">assert_equal(wa.distribution</span><span class="s3">, </span><span class="s5">'F'</span><span class="s1">)</span>
        <span class="s1">assert_equal(col_names[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">'F'</span><span class="s1">)</span>
        <span class="s1">assert_equal(col_names[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s5">'P&gt;F'</span><span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">assert_equal(wa.distribution</span><span class="s3">, </span><span class="s5">'chi2'</span><span class="s1">)</span>
        <span class="s1">assert_equal(col_names[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">'chi2'</span><span class="s1">)</span>
        <span class="s1">assert_equal(col_names[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s5">'P&gt;chi2'</span><span class="s1">)</span>

    <span class="s0"># SMOKETEST</span>
    <span class="s1">wa.summary_frame()</span>


<span class="s3">class </span><span class="s1">TestWaldAnovaOLS(CheckAnovaMixin):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">initialize(cls):</span>
        <span class="s1">mod = ols(</span><span class="s5">&quot;np.log(Days+1) ~ C(Duration, Sum)*C(Weight, Sum)&quot;</span><span class="s3">, </span><span class="s1">cls.data)</span>
        <span class="s1">cls.res = mod.fit(use_t=</span><span class="s3">False</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">test_noformula(self):</span>
        <span class="s0"># this verifies single and composite constraints against explicit</span>
        <span class="s0">#     wald test</span>
        <span class="s1">endog = self.res.model.endog</span>
        <span class="s1">exog = self.res.model.data.orig_exog</span>
        <span class="s1">exog = pd.DataFrame(exog)</span>

        <span class="s1">res = sm.OLS(endog</span><span class="s3">, </span><span class="s1">exog).fit()</span>
        <span class="s1">wa = res.wald_test_terms(skip_single=</span><span class="s3">False,</span>
                                 <span class="s1">combine_terms=[</span><span class="s5">'Duration'</span><span class="s3">, </span><span class="s5">'Weight'</span><span class="s1">]</span><span class="s3">,</span>
                                 <span class="s1">scalar=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">eye = np.eye(len(res.params))</span>

        <span class="s1">c_single = [row </span><span class="s3">for </span><span class="s1">row </span><span class="s3">in </span><span class="s1">eye]</span>
        <span class="s1">c_weight = eye[</span><span class="s4">2</span><span class="s1">:</span><span class="s4">6</span><span class="s1">]</span>
        <span class="s1">c_duration = eye[[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">4</span><span class="s3">, </span><span class="s4">5</span><span class="s1">]]</span>

        <span class="s1">compare_waldres(res</span><span class="s3">, </span><span class="s1">wa</span><span class="s3">, </span><span class="s1">c_single + [c_duration</span><span class="s3">, </span><span class="s1">c_weight])</span>

        <span class="s0"># assert correct df_constraints, see #5475 for bug in single constraint</span>
        <span class="s1">df_constraints = [</span><span class="s4">1</span><span class="s1">] * len(c_single) + [</span><span class="s4">3</span><span class="s3">, </span><span class="s4">4</span><span class="s1">]</span>
        <span class="s1">assert_equal(wa.df_constraints</span><span class="s3">, </span><span class="s1">df_constraints)</span>


<span class="s3">class </span><span class="s1">TestWaldAnovaOLSF(CheckAnovaMixin):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">initialize(cls):</span>
        <span class="s1">mod = ols(</span><span class="s5">&quot;np.log(Days+1) ~ C(Duration, Sum)*C(Weight, Sum)&quot;</span><span class="s3">, </span><span class="s1">cls.data)</span>
        <span class="s1">cls.res = mod.fit()  </span><span class="s0"># default use_t=True</span>

    <span class="s3">def </span><span class="s1">test_predict_missing(self):</span>
        <span class="s1">ex = self.data[:</span><span class="s4">5</span><span class="s1">].copy()</span>
        <span class="s1">ex.iloc[</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">] = np.nan</span>
        <span class="s1">predicted1 = self.res.predict(ex)</span>
        <span class="s1">predicted2 = self.res.predict(ex[</span><span class="s4">1</span><span class="s1">:])</span>

        <span class="s1">assert_index_equal(predicted1.index</span><span class="s3">, </span><span class="s1">ex.index)</span>
        <span class="s1">assert_series_equal(predicted1.iloc[</span><span class="s4">1</span><span class="s1">:]</span><span class="s3">, </span><span class="s1">predicted2)</span>
        <span class="s1">assert_equal(predicted1.values[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">np.nan)</span>


<span class="s3">class </span><span class="s1">TestWaldAnovaGLM(CheckAnovaMixin):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">initialize(cls):</span>
        <span class="s1">mod = glm(</span><span class="s5">&quot;np.log(Days+1) ~ C(Duration, Sum)*C(Weight, Sum)&quot;</span><span class="s3">, </span><span class="s1">cls.data)</span>
        <span class="s1">cls.res = mod.fit(use_t=</span><span class="s3">False</span><span class="s1">)</span>


<span class="s3">class </span><span class="s1">TestWaldAnovaPoisson(CheckAnovaMixin):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">initialize(cls):</span>
        <span class="s3">from </span><span class="s1">statsmodels.discrete.discrete_model </span><span class="s3">import </span><span class="s1">Poisson</span>

        <span class="s1">mod = Poisson.from_formula(</span><span class="s5">&quot;Days ~ C(Duration, Sum)*C(Weight, Sum)&quot;</span><span class="s3">, </span><span class="s1">cls.data)</span>
        <span class="s1">cls.res = mod.fit(cov_type=</span><span class="s5">'HC0'</span><span class="s1">)</span>


<span class="s3">class </span><span class="s1">TestWaldAnovaNegBin(CheckAnovaMixin):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">initialize(cls):</span>
        <span class="s3">from </span><span class="s1">statsmodels.discrete.discrete_model </span><span class="s3">import </span><span class="s1">NegativeBinomial</span>

        <span class="s1">formula = </span><span class="s5">&quot;Days ~ C(Duration, Sum)*C(Weight, Sum)&quot;</span>
        <span class="s1">mod = NegativeBinomial.from_formula(formula</span><span class="s3">, </span><span class="s1">cls.data</span><span class="s3">,</span>
                                            <span class="s1">loglike_method=</span><span class="s5">'nb2'</span><span class="s1">)</span>
        <span class="s1">cls.res = mod.fit()</span>


<span class="s3">class </span><span class="s1">TestWaldAnovaNegBin1(CheckAnovaMixin):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">initialize(cls):</span>
        <span class="s3">from </span><span class="s1">statsmodels.discrete.discrete_model </span><span class="s3">import </span><span class="s1">NegativeBinomial</span>

        <span class="s1">formula = </span><span class="s5">&quot;Days ~ C(Duration, Sum)*C(Weight, Sum)&quot;</span>
        <span class="s1">mod = NegativeBinomial.from_formula(formula</span><span class="s3">, </span><span class="s1">cls.data</span><span class="s3">,</span>
                                            <span class="s1">loglike_method=</span><span class="s5">'nb1'</span><span class="s1">)</span>
        <span class="s1">cls.res = mod.fit(cov_type=</span><span class="s5">'HC0'</span><span class="s1">)</span>


<span class="s3">class </span><span class="s1">CheckPairwise:</span>

    <span class="s3">def </span><span class="s1">test_default(self):</span>
        <span class="s1">res = self.res</span>

        <span class="s1">tt = res.t_test(self.constraints)</span>

        <span class="s1">pw = res.t_test_pairwise(self.term_name)</span>
        <span class="s1">pw_frame = pw.result_frame</span>
        <span class="s1">assert_allclose(pw_frame.iloc[:</span><span class="s3">, </span><span class="s1">:</span><span class="s4">6</span><span class="s1">].values</span><span class="s3">,</span>
                        <span class="s1">tt.summary_frame().values)</span>


<span class="s3">class </span><span class="s1">TestTTestPairwiseOLS(CheckPairwise):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s3">from </span><span class="s1">statsmodels.formula.api </span><span class="s3">import </span><span class="s1">ols</span>
        <span class="s3">import </span><span class="s1">statsmodels.stats.tests.test_anova </span><span class="s3">as </span><span class="s1">ttmod</span>

        <span class="s1">test = ttmod.TestAnova3()</span>
        <span class="s1">test.setup_class()</span>
        <span class="s1">cls.data = test.data.drop([</span><span class="s4">0</span><span class="s3">,</span><span class="s4">1</span><span class="s3">,</span><span class="s4">2</span><span class="s1">])</span>

        <span class="s1">mod = ols(</span><span class="s5">&quot;np.log(Days+1) ~ C(Duration) + C(Weight)&quot;</span><span class="s3">, </span><span class="s1">cls.data)</span>
        <span class="s1">cls.res = mod.fit()</span>
        <span class="s1">cls.term_name = </span><span class="s5">&quot;C(Weight)&quot;</span>
        <span class="s1">cls.constraints = [</span><span class="s5">'C(Weight)[T.2]'</span><span class="s3">,</span>
                           <span class="s5">'C(Weight)[T.3]'</span><span class="s3">,</span>
                           <span class="s5">'C(Weight)[T.3] - C(Weight)[T.2]'</span><span class="s1">]</span>

    <span class="s3">def </span><span class="s1">test_alpha(self):</span>
        <span class="s1">pw1 = self.res.t_test_pairwise(self.term_name</span><span class="s3">, </span><span class="s1">method=</span><span class="s5">'hommel'</span><span class="s3">,</span>
                                       <span class="s1">factor_labels=</span><span class="s5">'A B C'</span><span class="s1">.split())</span>
        <span class="s1">pw2 = self.res.t_test_pairwise(self.term_name</span><span class="s3">, </span><span class="s1">method=</span><span class="s5">'hommel'</span><span class="s3">,</span>
                                       <span class="s1">alpha=</span><span class="s4">0.01</span><span class="s1">)</span>
        <span class="s1">assert_allclose(pw1.result_frame.iloc[:</span><span class="s3">, </span><span class="s1">:</span><span class="s4">7</span><span class="s1">].values</span><span class="s3">,</span>
                        <span class="s1">pw2.result_frame.iloc[:</span><span class="s3">, </span><span class="s1">:</span><span class="s4">7</span><span class="s1">].values</span><span class="s3">, </span><span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s1">)</span>
        <span class="s1">assert_equal(pw1.result_frame.iloc[:</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">].values</span><span class="s3">,</span>
                     <span class="s1">[</span><span class="s3">True</span><span class="s1">]*</span><span class="s4">3</span><span class="s1">)</span>
        <span class="s1">assert_equal(pw2.result_frame.iloc[:</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">].values</span><span class="s3">,</span>
                     <span class="s1">[</span><span class="s3">False, True, False</span><span class="s1">])</span>

        <span class="s1">assert_equal(pw1.result_frame.index.values</span><span class="s3">,</span>
                     <span class="s1">np.array([</span><span class="s5">'B-A'</span><span class="s3">, </span><span class="s5">'C-A'</span><span class="s3">, </span><span class="s5">'C-B'</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=object))</span>


<span class="s3">class </span><span class="s1">TestTTestPairwiseOLS2(CheckPairwise):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s3">from </span><span class="s1">statsmodels.formula.api </span><span class="s3">import </span><span class="s1">ols</span>
        <span class="s3">import </span><span class="s1">statsmodels.stats.tests.test_anova </span><span class="s3">as </span><span class="s1">ttmod</span>

        <span class="s1">test = ttmod.TestAnova3()</span>
        <span class="s1">test.setup_class()</span>
        <span class="s1">cls.data = test.data.drop([</span><span class="s4">0</span><span class="s3">,</span><span class="s4">1</span><span class="s3">,</span><span class="s4">2</span><span class="s1">])</span>

        <span class="s1">mod = ols(</span><span class="s5">&quot;np.log(Days+1) ~ C(Weight) + C(Duration)&quot;</span><span class="s3">, </span><span class="s1">cls.data)</span>
        <span class="s1">cls.res = mod.fit()</span>
        <span class="s1">cls.term_name = </span><span class="s5">&quot;C(Weight)&quot;</span>
        <span class="s1">cls.constraints = [</span><span class="s5">'C(Weight)[T.2]'</span><span class="s3">,</span>
                           <span class="s5">'C(Weight)[T.3]'</span><span class="s3">,</span>
                           <span class="s5">'C(Weight)[T.3] - C(Weight)[T.2]'</span><span class="s1">]</span>


<span class="s3">class </span><span class="s1">TestTTestPairwiseOLS3(CheckPairwise):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s3">from </span><span class="s1">statsmodels.formula.api </span><span class="s3">import </span><span class="s1">ols</span>
        <span class="s3">import </span><span class="s1">statsmodels.stats.tests.test_anova </span><span class="s3">as </span><span class="s1">ttmod</span>

        <span class="s1">test = ttmod.TestAnova3()</span>
        <span class="s1">test.setup_class()</span>
        <span class="s1">cls.data = test.data.drop([</span><span class="s4">0</span><span class="s3">,</span><span class="s4">1</span><span class="s3">,</span><span class="s4">2</span><span class="s1">])</span>

        <span class="s1">mod = ols(</span><span class="s5">&quot;np.log(Days+1) ~ C(Weight) + C(Duration) - 1&quot;</span><span class="s3">, </span><span class="s1">cls.data)</span>
        <span class="s1">cls.res = mod.fit()</span>
        <span class="s1">cls.term_name = </span><span class="s5">&quot;C(Weight)&quot;</span>
        <span class="s1">cls.constraints = [</span><span class="s5">'C(Weight)[2] - C(Weight)[1]'</span><span class="s3">,</span>
                           <span class="s5">'C(Weight)[3] - C(Weight)[1]'</span><span class="s3">,</span>
                           <span class="s5">'C(Weight)[3] - C(Weight)[2]'</span><span class="s1">]</span>


<span class="s3">class </span><span class="s1">TestTTestPairwiseOLS4(CheckPairwise):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s3">from </span><span class="s1">statsmodels.formula.api </span><span class="s3">import </span><span class="s1">ols</span>
        <span class="s3">import </span><span class="s1">statsmodels.stats.tests.test_anova </span><span class="s3">as </span><span class="s1">ttmod</span>

        <span class="s1">test = ttmod.TestAnova3()</span>
        <span class="s1">test.setup_class()</span>
        <span class="s1">cls.data = test.data.drop([</span><span class="s4">0</span><span class="s3">,</span><span class="s4">1</span><span class="s3">,</span><span class="s4">2</span><span class="s1">])</span>

        <span class="s1">mod = ols(</span><span class="s5">&quot;np.log(Days+1) ~ C(Weight, Treatment(2)) + C(Duration)&quot;</span><span class="s3">, </span><span class="s1">cls.data)</span>
        <span class="s1">cls.res = mod.fit()</span>
        <span class="s1">cls.term_name = </span><span class="s5">&quot;C(Weight, Treatment(2))&quot;</span>
        <span class="s1">cls.constraints = [</span><span class="s5">'-C(Weight, Treatment(2))[T.1]'</span><span class="s3">,</span>
                           <span class="s5">'C(Weight, Treatment(2))[T.3] - C(Weight, Treatment(2))[T.1]'</span><span class="s3">,</span>
                           <span class="s5">'C(Weight, Treatment(2))[T.3]'</span><span class="s3">,</span><span class="s1">]</span>


<span class="s3">class </span><span class="s1">TestTTestPairwisePoisson(CheckPairwise):</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">setup_class(cls):</span>
        <span class="s3">from </span><span class="s1">statsmodels.discrete.discrete_model </span><span class="s3">import </span><span class="s1">Poisson</span>
        <span class="s3">import </span><span class="s1">statsmodels.stats.tests.test_anova </span><span class="s3">as </span><span class="s1">ttmod</span>

        <span class="s1">test = ttmod.TestAnova3()</span>
        <span class="s1">test.setup_class()</span>
        <span class="s1">cls.data = test.data.drop([</span><span class="s4">0</span><span class="s3">,</span><span class="s4">1</span><span class="s3">,</span><span class="s4">2</span><span class="s1">])</span>

        <span class="s1">mod = Poisson.from_formula(</span><span class="s5">&quot;Days ~ C(Duration) + C(Weight)&quot;</span><span class="s3">, </span><span class="s1">cls.data)</span>
        <span class="s1">cls.res = mod.fit(cov_type=</span><span class="s5">'HC0'</span><span class="s1">)</span>
        <span class="s1">cls.term_name = </span><span class="s5">&quot;C(Weight)&quot;</span>
        <span class="s1">cls.constraints = [</span><span class="s5">'C(Weight)[T.2]'</span><span class="s3">,</span>
                           <span class="s5">'C(Weight)[T.3]'</span><span class="s3">,</span>
                           <span class="s5">'C(Weight)[T.3] - C(Weight)[T.2]'</span><span class="s1">]</span>
</pre>
</body>
</html>