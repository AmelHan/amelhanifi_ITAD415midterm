<html>
<head>
<title>_penalized.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_penalized.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot; 
Created on Sun May 10 08:23:48 2015 
 
Author: Josef Perktold 
License: BSD-3 
&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">._penalties </span><span class="s3">import </span><span class="s1">NonePenalty</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.numdiff </span><span class="s3">import </span><span class="s1">approx_fprime_cs</span><span class="s3">, </span><span class="s1">approx_fprime</span>


<span class="s3">class </span><span class="s1">PenalizedMixin:</span>
    <span class="s2">&quot;&quot;&quot;Mixin class for Maximum Penalized Likelihood 
 
    Parameters 
    ---------- 
    args and kwds for the model super class 
    penal : None or instance of Penalized function class 
        If penal is None, then NonePenalty is used. 
    pen_weight : float or None 
        factor for weighting the penalization term. 
        If None, then pen_weight is set to nobs. 
 
 
    TODO: missing **kwds or explicit keywords 
 
    TODO: do we adjust the inherited docstrings? 
    We would need templating to add the penalization parameters 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwds):</span>

        <span class="s0"># pop extra kwds before calling super</span>
        <span class="s1">self.penal = kwds.pop(</span><span class="s4">'penal'</span><span class="s3">, None</span><span class="s1">)</span>
        <span class="s1">self.pen_weight =  kwds.pop(</span><span class="s4">'pen_weight'</span><span class="s3">, None</span><span class="s1">)</span>

        <span class="s1">super(PenalizedMixin</span><span class="s3">, </span><span class="s1">self).__init__(*args</span><span class="s3">, </span><span class="s1">**kwds)</span>

        <span class="s0"># TODO: define pen_weight as average pen_weight? i.e. per observation</span>
        <span class="s0"># I would have prefered len(self.endog) * kwds.get('pen_weight', 1)</span>
        <span class="s0"># or use pen_weight_factor in signature</span>
        <span class="s3">if </span><span class="s1">self.pen_weight </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self.pen_weight = len(self.endog)</span>

        <span class="s3">if </span><span class="s1">self.penal </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s0"># unpenalized by default</span>
            <span class="s1">self.penal = NonePenalty()</span>
            <span class="s1">self.pen_weight = </span><span class="s5">0</span>

        <span class="s1">self._init_keys.extend([</span><span class="s4">'penal'</span><span class="s3">, </span><span class="s4">'pen_weight'</span><span class="s1">])</span>
        <span class="s1">self._null_drop_keys = getattr(self</span><span class="s3">, </span><span class="s4">'_null_drop_keys'</span><span class="s3">, </span><span class="s1">[])</span>
        <span class="s1">self._null_drop_keys.extend([</span><span class="s4">'penal'</span><span class="s3">, </span><span class="s4">'pen_weight'</span><span class="s1">])</span>

    <span class="s3">def </span><span class="s1">_handle_scale(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">scale=</span><span class="s3">None, </span><span class="s1">**kwds):</span>

        <span class="s3">if </span><span class="s1">scale </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s0"># special handling for GLM</span>
            <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'scaletype'</span><span class="s1">):</span>
                <span class="s1">mu = self.predict(params)</span>
                <span class="s1">scale = self.estimate_scale(mu)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">scale = </span><span class="s5">1</span>

        <span class="s3">return </span><span class="s1">scale</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">pen_weight=</span><span class="s3">None, </span><span class="s1">**kwds):</span>
        <span class="s2">&quot;&quot;&quot; 
        Log-likelihood of model at params 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">pen_weight </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">pen_weight = self.pen_weight</span>

        <span class="s1">llf = super(PenalizedMixin</span><span class="s3">, </span><span class="s1">self).loglike(params</span><span class="s3">, </span><span class="s1">**kwds)</span>
        <span class="s3">if </span><span class="s1">pen_weight != </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">scale = self._handle_scale(params</span><span class="s3">, </span><span class="s1">**kwds)</span>
            <span class="s1">llf -= </span><span class="s5">1</span><span class="s1">/scale * pen_weight * self.penal.func(params)</span>

        <span class="s3">return </span><span class="s1">llf</span>

    <span class="s3">def </span><span class="s1">loglikeobs(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">pen_weight=</span><span class="s3">None, </span><span class="s1">**kwds):</span>
        <span class="s2">&quot;&quot;&quot; 
        Log-likelihood of model observations at params 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">pen_weight </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">pen_weight = self.pen_weight</span>

        <span class="s1">llf = super(PenalizedMixin</span><span class="s3">, </span><span class="s1">self).loglikeobs(params</span><span class="s3">, </span><span class="s1">**kwds)</span>
        <span class="s1">nobs_llf = float(llf.shape[</span><span class="s5">0</span><span class="s1">])</span>

        <span class="s3">if </span><span class="s1">pen_weight != </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">scale = self._handle_scale(params</span><span class="s3">, </span><span class="s1">**kwds)</span>
            <span class="s1">llf -= </span><span class="s5">1</span><span class="s1">/scale * pen_weight / nobs_llf * self.penal.func(params)</span>

        <span class="s3">return </span><span class="s1">llf</span>

    <span class="s3">def </span><span class="s1">score_numdiff(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">pen_weight=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s4">'fd'</span><span class="s3">, </span><span class="s1">**kwds):</span>
        <span class="s2">&quot;&quot;&quot;score based on finite difference derivative 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">pen_weight </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">pen_weight = self.pen_weight</span>

        <span class="s1">loglike = </span><span class="s3">lambda </span><span class="s1">p: self.loglike(p</span><span class="s3">, </span><span class="s1">pen_weight=pen_weight</span><span class="s3">, </span><span class="s1">**kwds)</span>

        <span class="s3">if </span><span class="s1">method == </span><span class="s4">'cs'</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">approx_fprime_cs(params</span><span class="s3">, </span><span class="s1">loglike)</span>
        <span class="s3">elif </span><span class="s1">method == </span><span class="s4">'fd'</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">approx_fprime(params</span><span class="s3">, </span><span class="s1">loglike</span><span class="s3">, </span><span class="s1">centered=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'method not recognized, should be &quot;fd&quot; or &quot;cs&quot;'</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">pen_weight=</span><span class="s3">None, </span><span class="s1">**kwds):</span>
        <span class="s2">&quot;&quot;&quot; 
        Gradient of model at params 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">pen_weight </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">pen_weight = self.pen_weight</span>

        <span class="s1">sc = super(PenalizedMixin</span><span class="s3">, </span><span class="s1">self).score(params</span><span class="s3">, </span><span class="s1">**kwds)</span>
        <span class="s3">if </span><span class="s1">pen_weight != </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">scale = self._handle_scale(params</span><span class="s3">, </span><span class="s1">**kwds)</span>
            <span class="s1">sc -= </span><span class="s5">1</span><span class="s1">/scale * pen_weight * self.penal.deriv(params)</span>

        <span class="s3">return </span><span class="s1">sc</span>

    <span class="s3">def </span><span class="s1">score_obs(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">pen_weight=</span><span class="s3">None, </span><span class="s1">**kwds):</span>
        <span class="s2">&quot;&quot;&quot; 
        Gradient of model observations at params 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">pen_weight </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">pen_weight = self.pen_weight</span>

        <span class="s1">sc = super(PenalizedMixin</span><span class="s3">, </span><span class="s1">self).score_obs(params</span><span class="s3">, </span><span class="s1">**kwds)</span>
        <span class="s1">nobs_sc = float(sc.shape[</span><span class="s5">0</span><span class="s1">])</span>
        <span class="s3">if </span><span class="s1">pen_weight != </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">scale = self._handle_scale(params</span><span class="s3">, </span><span class="s1">**kwds)</span>
            <span class="s1">sc -= </span><span class="s5">1</span><span class="s1">/scale * pen_weight / nobs_sc  * self.penal.deriv(params)</span>

        <span class="s3">return </span><span class="s1">sc</span>

    <span class="s3">def </span><span class="s1">hessian_numdiff(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">pen_weight=</span><span class="s3">None, </span><span class="s1">**kwds):</span>
        <span class="s2">&quot;&quot;&quot;hessian based on finite difference derivative 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">pen_weight </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">pen_weight = self.pen_weight</span>
        <span class="s1">loglike = </span><span class="s3">lambda </span><span class="s1">p: self.loglike(p</span><span class="s3">, </span><span class="s1">pen_weight=pen_weight</span><span class="s3">, </span><span class="s1">**kwds)</span>

        <span class="s3">from </span><span class="s1">statsmodels.tools.numdiff </span><span class="s3">import </span><span class="s1">approx_hess</span>
        <span class="s3">return </span><span class="s1">approx_hess(params</span><span class="s3">, </span><span class="s1">loglike)</span>

    <span class="s3">def </span><span class="s1">hessian(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">pen_weight=</span><span class="s3">None, </span><span class="s1">**kwds):</span>
        <span class="s2">&quot;&quot;&quot; 
        Hessian of model at params 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">pen_weight </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">pen_weight = self.pen_weight</span>

        <span class="s1">hess = super(PenalizedMixin</span><span class="s3">, </span><span class="s1">self).hessian(params</span><span class="s3">, </span><span class="s1">**kwds)</span>
        <span class="s3">if </span><span class="s1">pen_weight != </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">scale = self._handle_scale(params</span><span class="s3">, </span><span class="s1">**kwds)</span>
            <span class="s1">h = self.penal.deriv2(params)</span>
            <span class="s3">if </span><span class="s1">h.ndim == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">hess -= </span><span class="s5">1</span><span class="s1">/scale * np.diag(pen_weight * h)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">hess -= </span><span class="s5">1</span><span class="s1">/scale * pen_weight * h</span>

        <span class="s3">return </span><span class="s1">hess</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">method=</span><span class="s3">None, </span><span class="s1">trim=</span><span class="s3">None, </span><span class="s1">**kwds):</span>
        <span class="s2">&quot;&quot;&quot;minimize negative penalized log-likelihood 
 
        Parameters 
        ---------- 
        method : None or str 
            Method specifies the scipy optimizer as in nonlinear MLE models. 
        trim : {bool, float} 
            Default is False or None, which uses no trimming. 
            If trim is True or a float, then small parameters are set to zero. 
            If True, then a default threshold is used. If trim is a float, then 
            it will be used as threshold. 
            The default threshold is currently 1e-4, but it will change in 
            future and become penalty function dependent. 
        kwds : extra keyword arguments 
            This keyword arguments are treated in the same way as in the 
            fit method of the underlying model class. 
            Specifically, additional optimizer keywords and cov_type related 
            keywords can be added. 
        &quot;&quot;&quot;</span>
        <span class="s0"># If method is None, then we choose a default method ourselves</span>

        <span class="s0"># TODO: temporary hack, need extra fit kwds</span>
        <span class="s0"># we need to rule out fit methods in a model that will not work with</span>
        <span class="s0"># penalization</span>
        <span class="s3">from </span><span class="s1">statsmodels.gam.generalized_additive_model </span><span class="s3">import </span><span class="s1">GLMGam</span>
        <span class="s3">from </span><span class="s1">statsmodels.genmod.generalized_linear_model </span><span class="s3">import </span><span class="s1">GLM</span>
        <span class="s0"># Only for fit methods supporting max_start_irls</span>
        <span class="s3">if </span><span class="s1">isinstance(self</span><span class="s3">, </span><span class="s1">(GLM</span><span class="s3">, </span><span class="s1">GLMGam)):</span>
            <span class="s1">kwds.update({</span><span class="s4">'max_start_irls'</span><span class="s1">: </span><span class="s5">0</span><span class="s1">})</span>

        <span class="s0"># currently we use `bfgs` by default</span>
        <span class="s3">if </span><span class="s1">method </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">method = </span><span class="s4">'bfgs'</span>

        <span class="s3">if </span><span class="s1">trim </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">trim = </span><span class="s3">False</span>

        <span class="s1">res = super(PenalizedMixin</span><span class="s3">, </span><span class="s1">self).fit(method=method</span><span class="s3">, </span><span class="s1">**kwds)</span>

        <span class="s3">if </span><span class="s1">trim </span><span class="s3">is False</span><span class="s1">:</span>
            <span class="s0"># note boolean check for &quot;is False&quot;, not &quot;False_like&quot;</span>
            <span class="s3">return </span><span class="s1">res</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">trim </span><span class="s3">is True</span><span class="s1">:</span>
                <span class="s1">trim = </span><span class="s5">1e-4  </span><span class="s0"># trim threshold</span>
            <span class="s0"># TODO: make it penal function dependent</span>
            <span class="s0"># temporary standin, only checked for Poisson and GLM,</span>
            <span class="s0"># and is computationally inefficient</span>
            <span class="s1">drop_index = np.nonzero(np.abs(res.params) &lt; trim)[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">keep_index = np.nonzero(np.abs(res.params) &gt; trim)[</span><span class="s5">0</span><span class="s1">]</span>

            <span class="s3">if </span><span class="s1">drop_index.any():</span>
                <span class="s0"># TODO: do we need to add results attributes?</span>
                <span class="s1">res_aux = self._fit_zeros(keep_index</span><span class="s3">, </span><span class="s1">**kwds)</span>
                <span class="s3">return </span><span class="s1">res_aux</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s3">return </span><span class="s1">res</span>
</pre>
</body>
</html>