<html>
<head>
<title>tmodel.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
tmodel.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Linear Model with Student-t distributed errors 
 
Because the t distribution has fatter tails than the normal distribution, it 
can be used to model observations with heavier tails and observations that have 
some outliers. For the latter case, the t-distribution provides more robust 
estimators for mean or mean parameters (what about var?). 
 
 
 
References 
---------- 
Kenneth L. Lange, Roderick J. A. Little, Jeremy M. G. Taylor (1989) 
Robust Statistical Modeling Using the t Distribution 
Journal of the American Statistical Association 
Vol. 84, No. 408 (Dec., 1989), pp. 881-896 
Published by: American Statistical Association 
Stable URL: http://www.jstor.org/stable/2290063 
 
not read yet 
 
 
Created on 2010-09-24 
Author: josef-pktd 
License: BSD 
 
TODO 
---- 
* add starting values based on OLS 
* bugs: store_params does not seem to be defined, I think this was a module 
        global for debugging - commented out 
* parameter restriction: check whether version with some fixed parameters works 
 
 
&quot;&quot;&quot;</span>
<span class="s2">#mostly copied from the examples directory written for trying out generic mle.</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">special</span><span class="s3">, </span><span class="s1">stats</span>

<span class="s3">from </span><span class="s1">statsmodels.base.model </span><span class="s3">import </span><span class="s1">GenericLikelihoodModel</span>
<span class="s3">from </span><span class="s1">statsmodels.tsa.arma_mle </span><span class="s3">import </span><span class="s1">Arma</span>


<span class="s2">#redefine some shortcuts</span>
<span class="s1">np_log = np.log</span>
<span class="s1">np_pi = np.pi</span>
<span class="s1">sps_gamln = special.gammaln</span>


<span class="s3">class </span><span class="s1">TLinearModel(GenericLikelihoodModel):</span>
    <span class="s0">'''Maximum Likelihood Estimation of Linear Model with t-distributed errors 
 
    This is an example for generic MLE. 
 
    Except for defining the negative log-likelihood method, all 
    methods and results are generic. Gradients and Hessian 
    and all resulting statistics are based on numerical 
    differentiation. 
 
    '''</span>

    <span class="s3">def </span><span class="s1">initialize(self):</span>
        <span class="s1">print(</span><span class="s4">&quot;running Tmodel initialize&quot;</span><span class="s1">)</span>
        <span class="s2"># TODO: here or in __init__</span>
        <span class="s1">self.k_vars = self.exog.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">'fix_df'</span><span class="s1">):</span>
            <span class="s1">self.fix_df = </span><span class="s3">False</span>

        <span class="s3">if </span><span class="s1">self.fix_df </span><span class="s3">is False</span><span class="s1">:</span>
            <span class="s2"># df will be estimated, no parameter restrictions</span>
            <span class="s1">self.fixed_params = </span><span class="s3">None</span>
            <span class="s1">self.fixed_paramsmask = </span><span class="s3">None</span>
            <span class="s1">self.k_params = self.exog.shape[</span><span class="s5">1</span><span class="s1">] + </span><span class="s5">2</span>
            <span class="s1">extra_params_names = [</span><span class="s4">'df'</span><span class="s3">, </span><span class="s4">'scale'</span><span class="s1">]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># df fixed</span>
            <span class="s1">self.k_params = self.exog.shape[</span><span class="s5">1</span><span class="s1">] + </span><span class="s5">1</span>
            <span class="s1">fixdf = np.nan * np.zeros(self.exog.shape[</span><span class="s5">1</span><span class="s1">] + </span><span class="s5">2</span><span class="s1">)</span>
            <span class="s1">fixdf[-</span><span class="s5">2</span><span class="s1">] = self.fix_df</span>
            <span class="s1">self.fixed_params = fixdf</span>
            <span class="s1">self.fixed_paramsmask = np.isnan(fixdf)</span>
            <span class="s1">extra_params_names = [</span><span class="s4">'scale'</span><span class="s1">]</span>

        <span class="s1">super(TLinearModel</span><span class="s3">, </span><span class="s1">self).initialize()</span>

        <span class="s2"># Note: this needs to be after super initialize</span>
        <span class="s2"># super initialize sets default df_resid,</span>
        <span class="s2">#_set_extra_params_names adjusts it</span>
        <span class="s1">self._set_extra_params_names(extra_params_names)</span>
        <span class="s1">self._set_start_params()</span>


    <span class="s3">def </span><span class="s1">_set_start_params(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">use_kurtosis=</span><span class="s3">False</span><span class="s1">):</span>
        <span class="s3">if </span><span class="s1">start_params </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">self.start_params = start_params</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">from </span><span class="s1">statsmodels.regression.linear_model </span><span class="s3">import </span><span class="s1">OLS</span>
            <span class="s1">res_ols = OLS(self.endog</span><span class="s3">, </span><span class="s1">self.exog).fit()</span>
            <span class="s1">start_params = </span><span class="s5">0.1</span><span class="s1">*np.ones(self.k_params)</span>
            <span class="s1">start_params[:self.k_vars] = res_ols.params</span>

            <span class="s3">if </span><span class="s1">self.fix_df </span><span class="s3">is False</span><span class="s1">:</span>

                <span class="s3">if </span><span class="s1">use_kurtosis:</span>
                    <span class="s1">kurt = stats.kurtosis(res_ols.resid)</span>
                    <span class="s1">df = </span><span class="s5">6.</span><span class="s1">/kurt + </span><span class="s5">4</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">df = </span><span class="s5">5</span>

                <span class="s1">start_params[-</span><span class="s5">2</span><span class="s1">] = df</span>
                <span class="s2">#TODO adjust scale for df</span>
                <span class="s1">start_params[-</span><span class="s5">1</span><span class="s1">] = np.sqrt(res_ols.scale)</span>

            <span class="s1">self.start_params = start_params</span>




    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">return </span><span class="s1">-self.nloglikeobs(params).sum(</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">nloglikeobs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Loglikelihood of linear model with t distributed errors. 
 
        Parameters 
        ---------- 
        params : ndarray 
            The parameters of the model. The last 2 parameters are degrees of 
            freedom and scale. 
 
        Returns 
        ------- 
        loglike : ndarray 
            The log likelihood of the model evaluated at `params` for each 
            observation defined by self.endog and self.exog. 
 
        Notes 
        ----- 
        .. math:: \\ln L=\\sum_{i=1}^{n}\\left[-\\lambda_{i}+y_{i}x_{i}^{\\prime}\\beta-\\ln y_{i}!\\right] 
 
        The t distribution is the standard t distribution and not a standardized 
        t distribution, which means that the scale parameter is not equal to the 
        standard deviation. 
 
        self.fixed_params and self.expandparams can be used to fix some 
        parameters. (I doubt this has been tested in this model.) 
        &quot;&quot;&quot;</span>
        <span class="s2">#print len(params),</span>
        <span class="s2">#store_params.append(params)</span>
        <span class="s3">if </span><span class="s1">self.fixed_params </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s2">#print 'using fixed'</span>
            <span class="s1">params = self.expandparams(params)</span>

        <span class="s1">beta = params[:-</span><span class="s5">2</span><span class="s1">]</span>
        <span class="s1">df = params[-</span><span class="s5">2</span><span class="s1">]</span>
        <span class="s1">scale = np.abs(params[-</span><span class="s5">1</span><span class="s1">])  </span><span class="s2">#TODO check behavior around zero</span>
        <span class="s1">loc = np.dot(self.exog</span><span class="s3">, </span><span class="s1">beta)</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">x = (endog - loc)/scale</span>
        <span class="s2">#next part is stats.t._logpdf</span>
        <span class="s1">lPx = sps_gamln((df+</span><span class="s5">1</span><span class="s1">)/</span><span class="s5">2</span><span class="s1">) - sps_gamln(df/</span><span class="s5">2.</span><span class="s1">)</span>
        <span class="s1">lPx -= </span><span class="s5">0.5</span><span class="s1">*np_log(df*np_pi) + (df+</span><span class="s5">1</span><span class="s1">)/</span><span class="s5">2.</span><span class="s1">*np_log(</span><span class="s5">1</span><span class="s1">+(x**</span><span class="s5">2</span><span class="s1">)/df)</span>
        <span class="s1">lPx -= np_log(scale)  </span><span class="s2"># correction for scale</span>
        <span class="s3">return </span><span class="s1">-lPx</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>
        <span class="s3">return </span><span class="s1">np.dot(exog</span><span class="s3">, </span><span class="s1">params[:self.exog.shape[</span><span class="s5">1</span><span class="s1">]])</span>


<span class="s3">class </span><span class="s1">TArma(Arma):</span>
    <span class="s0">'''Univariate Arma Model with t-distributed errors 
 
    This inherit all methods except loglike from tsa.arma_mle.Arma 
 
    This uses the standard t-distribution, the implied variance of 
    the error is not equal to scale, but :: 
 
        error_variance = df/(df-2)*scale**2 
 
    Notes 
    ----- 
    This might be replaced by a standardized t-distribution with scale**2 
    equal to variance 
 
    '''</span>

    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s3">return </span><span class="s1">-self.nloglikeobs(params).sum(</span><span class="s5">0</span><span class="s1">)</span>


    <span class="s2">#add for Jacobian calculation  bsejac in GenericMLE, copied from loglike</span>
    <span class="s3">def </span><span class="s1">nloglikeobs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Loglikelihood for arma model for each observation, t-distribute 
 
        Notes 
        ----- 
        The ancillary parameter is assumed to be the last element of 
        the params vector 
        &quot;&quot;&quot;</span>

        <span class="s1">errorsest = self.geterrors(params[:-</span><span class="s5">2</span><span class="s1">])</span>
        <span class="s2">#sigma2 = np.maximum(params[-1]**2, 1e-6)  #do I need this</span>
        <span class="s2">#axis = 0</span>
        <span class="s2">#nobs = len(errorsest)</span>

        <span class="s1">df = params[-</span><span class="s5">2</span><span class="s1">]</span>
        <span class="s1">scale = np.abs(params[-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s1">llike  = - stats.t._logpdf(errorsest/scale</span><span class="s3">, </span><span class="s1">df) + np_log(scale)</span>
        <span class="s3">return </span><span class="s1">llike</span>

    <span class="s2">#TODO rename fit_mle -&gt; fit, fit -&gt; fit_ls</span>
    <span class="s3">def </span><span class="s1">fit_mle(self</span><span class="s3">, </span><span class="s1">order</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s4">'nm'</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s5">5000</span><span class="s3">,</span>
            <span class="s1">tol=</span><span class="s5">1e-08</span><span class="s3">, </span><span class="s1">**kwds):</span>
        <span class="s1">nar</span><span class="s3">, </span><span class="s1">nma = order</span>
        <span class="s3">if </span><span class="s1">start_params </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">len(start_params) != nar + nma + </span><span class="s5">2</span><span class="s1">:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'start_param need sum(order) + 2 elements'</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">start_params = np.concatenate((</span><span class="s5">0.05</span><span class="s1">*np.ones(nar + nma)</span><span class="s3">, </span><span class="s1">[</span><span class="s5">5</span><span class="s3">, </span><span class="s5">1</span><span class="s1">]))</span>


        <span class="s1">res = super(TArma</span><span class="s3">, </span><span class="s1">self).fit_mle(order=order</span><span class="s3">,</span>
                                         <span class="s1">start_params=start_params</span><span class="s3">,</span>
                                         <span class="s1">method=method</span><span class="s3">, </span><span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                                         <span class="s1">tol=tol</span><span class="s3">, </span><span class="s1">**kwds)</span>

        <span class="s3">return </span><span class="s1">res</span>
</pre>
</body>
</html>