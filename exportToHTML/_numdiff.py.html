<html>
<head>
<title>_numdiff.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_numdiff.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Routines for numerical differentiation.&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">functools</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">numpy.linalg </span><span class="s2">import </span><span class="s1">norm</span>

<span class="s2">from </span><span class="s1">scipy.sparse.linalg </span><span class="s2">import </span><span class="s1">LinearOperator</span>
<span class="s2">from </span><span class="s1">..sparse </span><span class="s2">import </span><span class="s1">issparse</span><span class="s2">, </span><span class="s1">csc_matrix</span><span class="s2">, </span><span class="s1">csr_matrix</span><span class="s2">, </span><span class="s1">coo_matrix</span><span class="s2">, </span><span class="s1">find</span>
<span class="s2">from </span><span class="s1">._group_columns </span><span class="s2">import </span><span class="s1">group_dense</span><span class="s2">, </span><span class="s1">group_sparse</span>


<span class="s2">def </span><span class="s1">_adjust_scheme_to_bounds(x0</span><span class="s2">, </span><span class="s1">h</span><span class="s2">, </span><span class="s1">num_steps</span><span class="s2">, </span><span class="s1">scheme</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">ub):</span>
    <span class="s0">&quot;&quot;&quot;Adjust final difference scheme to the presence of bounds. 
 
    Parameters 
    ---------- 
    x0 : ndarray, shape (n,) 
        Point at which we wish to estimate derivative. 
    h : ndarray, shape (n,) 
        Desired absolute finite difference steps. 
    num_steps : int 
        Number of `h` steps in one direction required to implement finite 
        difference scheme. For example, 2 means that we need to evaluate 
        f(x0 + 2 * h) or f(x0 - 2 * h) 
    scheme : {'1-sided', '2-sided'} 
        Whether steps in one or both directions are required. In other 
        words '1-sided' applies to forward and backward schemes, '2-sided' 
        applies to center schemes. 
    lb : ndarray, shape (n,) 
        Lower bounds on independent variables. 
    ub : ndarray, shape (n,) 
        Upper bounds on independent variables. 
 
    Returns 
    ------- 
    h_adjusted : ndarray, shape (n,) 
        Adjusted absolute step sizes. Step size decreases only if a sign flip 
        or switching to one-sided scheme doesn't allow to take a full step. 
    use_one_sided : ndarray of bool, shape (n,) 
        Whether to switch to one-sided scheme. Informative only for 
        ``scheme='2-sided'``. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">scheme == </span><span class="s3">'1-sided'</span><span class="s1">:</span>
        <span class="s1">use_one_sided = np.ones_like(h</span><span class="s2">, </span><span class="s1">dtype=bool)</span>
    <span class="s2">elif </span><span class="s1">scheme == </span><span class="s3">'2-sided'</span><span class="s1">:</span>
        <span class="s1">h = np.abs(h)</span>
        <span class="s1">use_one_sided = np.zeros_like(h</span><span class="s2">, </span><span class="s1">dtype=bool)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`scheme` must be '1-sided' or '2-sided'.&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">np.all((lb == -np.inf) &amp; (ub == np.inf)):</span>
        <span class="s2">return </span><span class="s1">h</span><span class="s2">, </span><span class="s1">use_one_sided</span>

    <span class="s1">h_total = h * num_steps</span>
    <span class="s1">h_adjusted = h.copy()</span>

    <span class="s1">lower_dist = x0 - lb</span>
    <span class="s1">upper_dist = ub - x0</span>

    <span class="s2">if </span><span class="s1">scheme == </span><span class="s3">'1-sided'</span><span class="s1">:</span>
        <span class="s1">x = x0 + h_total</span>
        <span class="s1">violated = (x &lt; lb) | (x &gt; ub)</span>
        <span class="s1">fitting = np.abs(h_total) &lt;= np.maximum(lower_dist</span><span class="s2">, </span><span class="s1">upper_dist)</span>
        <span class="s1">h_adjusted[violated &amp; fitting] *= -</span><span class="s4">1</span>

        <span class="s1">forward = (upper_dist &gt;= lower_dist) &amp; ~fitting</span>
        <span class="s1">h_adjusted[forward] = upper_dist[forward] / num_steps</span>
        <span class="s1">backward = (upper_dist &lt; lower_dist) &amp; ~fitting</span>
        <span class="s1">h_adjusted[backward] = -lower_dist[backward] / num_steps</span>
    <span class="s2">elif </span><span class="s1">scheme == </span><span class="s3">'2-sided'</span><span class="s1">:</span>
        <span class="s1">central = (lower_dist &gt;= h_total) &amp; (upper_dist &gt;= h_total)</span>

        <span class="s1">forward = (upper_dist &gt;= lower_dist) &amp; ~central</span>
        <span class="s1">h_adjusted[forward] = np.minimum(</span>
            <span class="s1">h[forward]</span><span class="s2">, </span><span class="s4">0.5 </span><span class="s1">* upper_dist[forward] / num_steps)</span>
        <span class="s1">use_one_sided[forward] = </span><span class="s2">True</span>

        <span class="s1">backward = (upper_dist &lt; lower_dist) &amp; ~central</span>
        <span class="s1">h_adjusted[backward] = -np.minimum(</span>
            <span class="s1">h[backward]</span><span class="s2">, </span><span class="s4">0.5 </span><span class="s1">* lower_dist[backward] / num_steps)</span>
        <span class="s1">use_one_sided[backward] = </span><span class="s2">True</span>

        <span class="s1">min_dist = np.minimum(upper_dist</span><span class="s2">, </span><span class="s1">lower_dist) / num_steps</span>
        <span class="s1">adjusted_central = (~central &amp; (np.abs(h_adjusted) &lt;= min_dist))</span>
        <span class="s1">h_adjusted[adjusted_central] = min_dist[adjusted_central]</span>
        <span class="s1">use_one_sided[adjusted_central] = </span><span class="s2">False</span>

    <span class="s2">return </span><span class="s1">h_adjusted</span><span class="s2">, </span><span class="s1">use_one_sided</span>


<span class="s1">@functools.lru_cache</span>
<span class="s2">def </span><span class="s1">_eps_for_method(x0_dtype</span><span class="s2">, </span><span class="s1">f0_dtype</span><span class="s2">, </span><span class="s1">method):</span>
    <span class="s0">&quot;&quot;&quot; 
    Calculates relative EPS step to use for a given data type 
    and numdiff step method. 
 
    Progressively smaller steps are used for larger floating point types. 
 
    Parameters 
    ---------- 
    f0_dtype: np.dtype 
        dtype of function evaluation 
 
    x0_dtype: np.dtype 
        dtype of parameter vector 
 
    method: {'2-point', '3-point', 'cs'} 
 
    Returns 
    ------- 
    EPS: float 
        relative step size. May be np.float16, np.float32, np.float64 
 
    Notes 
    ----- 
    The default relative step will be np.float64. However, if x0 or f0 are 
    smaller floating point types (np.float16, np.float32), then the smallest 
    floating point type is chosen. 
    &quot;&quot;&quot;</span>
    <span class="s5"># the default EPS value</span>
    <span class="s1">EPS = np.finfo(np.float64).eps</span>

    <span class="s1">x0_is_fp = </span><span class="s2">False</span>
    <span class="s2">if </span><span class="s1">np.issubdtype(x0_dtype</span><span class="s2">, </span><span class="s1">np.inexact):</span>
        <span class="s5"># if you're a floating point type then over-ride the default EPS</span>
        <span class="s1">EPS = np.finfo(x0_dtype).eps</span>
        <span class="s1">x0_itemsize = np.dtype(x0_dtype).itemsize</span>
        <span class="s1">x0_is_fp = </span><span class="s2">True</span>

    <span class="s2">if </span><span class="s1">np.issubdtype(f0_dtype</span><span class="s2">, </span><span class="s1">np.inexact):</span>
        <span class="s1">f0_itemsize = np.dtype(f0_dtype).itemsize</span>
        <span class="s5"># choose the smallest itemsize between x0 and f0</span>
        <span class="s2">if </span><span class="s1">x0_is_fp </span><span class="s2">and </span><span class="s1">f0_itemsize &lt; x0_itemsize:</span>
            <span class="s1">EPS = np.finfo(f0_dtype).eps</span>

    <span class="s2">if </span><span class="s1">method </span><span class="s2">in </span><span class="s1">[</span><span class="s3">&quot;2-point&quot;</span><span class="s2">, </span><span class="s3">&quot;cs&quot;</span><span class="s1">]:</span>
        <span class="s2">return </span><span class="s1">EPS**</span><span class="s4">0.5</span>
    <span class="s2">elif </span><span class="s1">method </span><span class="s2">in </span><span class="s1">[</span><span class="s3">&quot;3-point&quot;</span><span class="s1">]:</span>
        <span class="s2">return </span><span class="s1">EPS**(</span><span class="s4">1</span><span class="s1">/</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">RuntimeError(</span><span class="s3">&quot;Unknown step method, should be one of &quot;</span>
                           <span class="s3">&quot;{'2-point', '3-point', 'cs'}&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">_compute_absolute_step(rel_step</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">f0</span><span class="s2">, </span><span class="s1">method):</span>
    <span class="s0">&quot;&quot;&quot; 
    Computes an absolute step from a relative step for finite difference 
    calculation. 
 
    Parameters 
    ---------- 
    rel_step: None or array-like 
        Relative step for the finite difference calculation 
    x0 : np.ndarray 
        Parameter vector 
    f0 : np.ndarray or scalar 
    method : {'2-point', '3-point', 'cs'} 
 
    Returns 
    ------- 
    h : float 
        The absolute step size 
 
    Notes 
    ----- 
    `h` will always be np.float64. However, if `x0` or `f0` are 
    smaller floating point dtypes (e.g. np.float32), then the absolute 
    step size will be calculated from the smallest floating point size. 
    &quot;&quot;&quot;</span>
    <span class="s5"># this is used instead of np.sign(x0) because we need</span>
    <span class="s5"># sign_x0 to be 1 when x0 == 0.</span>
    <span class="s1">sign_x0 = (x0 &gt;= </span><span class="s4">0</span><span class="s1">).astype(float) * </span><span class="s4">2 </span><span class="s1">- </span><span class="s4">1</span>

    <span class="s1">rstep = _eps_for_method(x0.dtype</span><span class="s2">, </span><span class="s1">f0.dtype</span><span class="s2">, </span><span class="s1">method)</span>

    <span class="s2">if </span><span class="s1">rel_step </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">abs_step = rstep * sign_x0 * np.maximum(</span><span class="s4">1.0</span><span class="s2">, </span><span class="s1">np.abs(x0))</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s5"># User has requested specific relative steps.</span>
        <span class="s5"># Don't multiply by max(1, abs(x0) because if x0 &lt; 1 then their</span>
        <span class="s5"># requested step is not used.</span>
        <span class="s1">abs_step = rel_step * sign_x0 * np.abs(x0)</span>

        <span class="s5"># however we don't want an abs_step of 0, which can happen if</span>
        <span class="s5"># rel_step is 0, or x0 is 0. Instead, substitute a realistic step</span>
        <span class="s1">dx = ((x0 + abs_step) - x0)</span>
        <span class="s1">abs_step = np.where(dx == </span><span class="s4">0</span><span class="s2">,</span>
                            <span class="s1">rstep * sign_x0 * np.maximum(</span><span class="s4">1.0</span><span class="s2">, </span><span class="s1">np.abs(x0))</span><span class="s2">,</span>
                            <span class="s1">abs_step)</span>

    <span class="s2">return </span><span class="s1">abs_step</span>


<span class="s2">def </span><span class="s1">_prepare_bounds(bounds</span><span class="s2">, </span><span class="s1">x0):</span>
    <span class="s0">&quot;&quot;&quot; 
    Prepares new-style bounds from a two-tuple specifying the lower and upper 
    limits for values in x0. If a value is not bound then the lower/upper bound 
    will be expected to be -np.inf/np.inf. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; _prepare_bounds([(0, 1, 2), (1, 2, np.inf)], [0.5, 1.5, 2.5]) 
    (array([0., 1., 2.]), array([ 1.,  2., inf])) 
    &quot;&quot;&quot;</span>
    <span class="s1">lb</span><span class="s2">, </span><span class="s1">ub = (np.asarray(b</span><span class="s2">, </span><span class="s1">dtype=float) </span><span class="s2">for </span><span class="s1">b </span><span class="s2">in </span><span class="s1">bounds)</span>
    <span class="s2">if </span><span class="s1">lb.ndim == </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s1">lb = np.resize(lb</span><span class="s2">, </span><span class="s1">x0.shape)</span>

    <span class="s2">if </span><span class="s1">ub.ndim == </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s1">ub = np.resize(ub</span><span class="s2">, </span><span class="s1">x0.shape)</span>

    <span class="s2">return </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">ub</span>


<span class="s2">def </span><span class="s1">group_columns(A</span><span class="s2">, </span><span class="s1">order=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Group columns of a 2-D matrix for sparse finite differencing [1]_. 
 
    Two columns are in the same group if in each row at least one of them 
    has zero. A greedy sequential algorithm is used to construct groups. 
 
    Parameters 
    ---------- 
    A : array_like or sparse matrix, shape (m, n) 
        Matrix of which to group columns. 
    order : int, iterable of int with shape (n,) or None 
        Permutation array which defines the order of columns enumeration. 
        If int or None, a random permutation is used with `order` used as 
        a random seed. Default is 0, that is use a random permutation but 
        guarantee repeatability. 
 
    Returns 
    ------- 
    groups : ndarray of int, shape (n,) 
        Contains values from 0 to n_groups-1, where n_groups is the number 
        of found groups. Each value ``groups[i]`` is an index of a group to 
        which ith column assigned. The procedure was helpful only if 
        n_groups is significantly less than n. 
 
    References 
    ---------- 
    .. [1] A. Curtis, M. J. D. Powell, and J. Reid, &quot;On the estimation of 
           sparse Jacobian matrices&quot;, Journal of the Institute of Mathematics 
           and its Applications, 13 (1974), pp. 117-120. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">issparse(A):</span>
        <span class="s1">A = csc_matrix(A)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">A = np.atleast_2d(A)</span>
        <span class="s1">A = (A != </span><span class="s4">0</span><span class="s1">).astype(np.int32)</span>

    <span class="s2">if </span><span class="s1">A.ndim != </span><span class="s4">2</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`A` must be 2-dimensional.&quot;</span><span class="s1">)</span>

    <span class="s1">m</span><span class="s2">, </span><span class="s1">n = A.shape</span>

    <span class="s2">if </span><span class="s1">order </span><span class="s2">is None or </span><span class="s1">np.isscalar(order):</span>
        <span class="s1">rng = np.random.RandomState(order)</span>
        <span class="s1">order = rng.permutation(n)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">order = np.asarray(order)</span>
        <span class="s2">if </span><span class="s1">order.shape != (n</span><span class="s2">,</span><span class="s1">):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`order` has incorrect shape.&quot;</span><span class="s1">)</span>

    <span class="s1">A = A[:</span><span class="s2">, </span><span class="s1">order]</span>

    <span class="s2">if </span><span class="s1">issparse(A):</span>
        <span class="s1">groups = group_sparse(m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">A.indices</span><span class="s2">, </span><span class="s1">A.indptr)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">groups = group_dense(m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">A)</span>

    <span class="s1">groups[order] = groups.copy()</span>

    <span class="s2">return </span><span class="s1">groups</span>


<span class="s2">def </span><span class="s1">approx_derivative(fun</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">method=</span><span class="s3">'3-point'</span><span class="s2">, </span><span class="s1">rel_step=</span><span class="s2">None, </span><span class="s1">abs_step=</span><span class="s2">None,</span>
                      <span class="s1">f0=</span><span class="s2">None, </span><span class="s1">bounds=(-np.inf</span><span class="s2">, </span><span class="s1">np.inf)</span><span class="s2">, </span><span class="s1">sparsity=</span><span class="s2">None,</span>
                      <span class="s1">as_linear_operator=</span><span class="s2">False, </span><span class="s1">args=()</span><span class="s2">, </span><span class="s1">kwargs={}):</span>
    <span class="s0">&quot;&quot;&quot;Compute finite difference approximation of the derivatives of a 
    vector-valued function. 
 
    If a function maps from R^n to R^m, its derivatives form m-by-n matrix 
    called the Jacobian, where an element (i, j) is a partial derivative of 
    f[i] with respect to x[j]. 
 
    Parameters 
    ---------- 
    fun : callable 
        Function of which to estimate the derivatives. The argument x 
        passed to this function is ndarray of shape (n,) (never a scalar 
        even if n=1). It must return 1-D array_like of shape (m,) or a scalar. 
    x0 : array_like of shape (n,) or float 
        Point at which to estimate the derivatives. Float will be converted 
        to a 1-D array. 
    method : {'3-point', '2-point', 'cs'}, optional 
        Finite difference method to use: 
            - '2-point' - use the first order accuracy forward or backward 
                          difference. 
            - '3-point' - use central difference in interior points and the 
                          second order accuracy forward or backward difference 
                          near the boundary. 
            - 'cs' - use a complex-step finite difference scheme. This assumes 
                     that the user function is real-valued and can be 
                     analytically continued to the complex plane. Otherwise, 
                     produces bogus results. 
    rel_step : None or array_like, optional 
        Relative step size to use. If None (default) the absolute step size is 
        computed as ``h = rel_step * sign(x0) * max(1, abs(x0))``, with 
        `rel_step` being selected automatically, see Notes. Otherwise 
        ``h = rel_step * sign(x0) * abs(x0)``. For ``method='3-point'`` the 
        sign of `h` is ignored. The calculated step size is possibly adjusted 
        to fit into the bounds. 
    abs_step : array_like, optional 
        Absolute step size to use, possibly adjusted to fit into the bounds. 
        For ``method='3-point'`` the sign of `abs_step` is ignored. By default 
        relative steps are used, only if ``abs_step is not None`` are absolute 
        steps used. 
    f0 : None or array_like, optional 
        If not None it is assumed to be equal to ``fun(x0)``, in this case 
        the ``fun(x0)`` is not called. Default is None. 
    bounds : tuple of array_like, optional 
        Lower and upper bounds on independent variables. Defaults to no bounds. 
        Each bound must match the size of `x0` or be a scalar, in the latter 
        case the bound will be the same for all variables. Use it to limit the 
        range of function evaluation. Bounds checking is not implemented 
        when `as_linear_operator` is True. 
    sparsity : {None, array_like, sparse matrix, 2-tuple}, optional 
        Defines a sparsity structure of the Jacobian matrix. If the Jacobian 
        matrix is known to have only few non-zero elements in each row, then 
        it's possible to estimate its several columns by a single function 
        evaluation [3]_. To perform such economic computations two ingredients 
        are required: 
 
        * structure : array_like or sparse matrix of shape (m, n). A zero 
          element means that a corresponding element of the Jacobian 
          identically equals to zero. 
        * groups : array_like of shape (n,). A column grouping for a given 
          sparsity structure, use `group_columns` to obtain it. 
 
        A single array or a sparse matrix is interpreted as a sparsity 
        structure, and groups are computed inside the function. A tuple is 
        interpreted as (structure, groups). If None (default), a standard 
        dense differencing will be used. 
 
        Note, that sparse differencing makes sense only for large Jacobian 
        matrices where each row contains few non-zero elements. 
    as_linear_operator : bool, optional 
        When True the function returns an `scipy.sparse.linalg.LinearOperator`. 
        Otherwise it returns a dense array or a sparse matrix depending on 
        `sparsity`. The linear operator provides an efficient way of computing 
        ``J.dot(p)`` for any vector ``p`` of shape (n,), but does not allow 
        direct access to individual elements of the matrix. By default 
        `as_linear_operator` is False. 
    args, kwargs : tuple and dict, optional 
        Additional arguments passed to `fun`. Both empty by default. 
        The calling signature is ``fun(x, *args, **kwargs)``. 
 
    Returns 
    ------- 
    J : {ndarray, sparse matrix, LinearOperator} 
        Finite difference approximation of the Jacobian matrix. 
        If `as_linear_operator` is True returns a LinearOperator 
        with shape (m, n). Otherwise it returns a dense array or sparse 
        matrix depending on how `sparsity` is defined. If `sparsity` 
        is None then a ndarray with shape (m, n) is returned. If 
        `sparsity` is not None returns a csr_matrix with shape (m, n). 
        For sparse matrices and linear operators it is always returned as 
        a 2-D structure, for ndarrays, if m=1 it is returned 
        as a 1-D gradient array with shape (n,). 
 
    See Also 
    -------- 
    check_derivative : Check correctness of a function computing derivatives. 
 
    Notes 
    ----- 
    If `rel_step` is not provided, it assigned as ``EPS**(1/s)``, where EPS is 
    determined from the smallest floating point dtype of `x0` or `fun(x0)`, 
    ``np.finfo(x0.dtype).eps``, s=2 for '2-point' method and 
    s=3 for '3-point' method. Such relative step approximately minimizes a sum 
    of truncation and round-off errors, see [1]_. Relative steps are used by 
    default. However, absolute steps are used when ``abs_step is not None``. 
    If any of the absolute or relative steps produces an indistinguishable 
    difference from the original `x0`, ``(x0 + dx) - x0 == 0``, then a 
    automatic step size is substituted for that particular entry. 
 
    A finite difference scheme for '3-point' method is selected automatically. 
    The well-known central difference scheme is used for points sufficiently 
    far from the boundary, and 3-point forward or backward scheme is used for 
    points near the boundary. Both schemes have the second-order accuracy in 
    terms of Taylor expansion. Refer to [2]_ for the formulas of 3-point 
    forward and backward difference schemes. 
 
    For dense differencing when m=1 Jacobian is returned with a shape (n,), 
    on the other hand when n=1 Jacobian is returned with a shape (m, 1). 
    Our motivation is the following: a) It handles a case of gradient 
    computation (m=1) in a conventional way. b) It clearly separates these two 
    different cases. b) In all cases np.atleast_2d can be called to get 2-D 
    Jacobian with correct dimensions. 
 
    References 
    ---------- 
    .. [1] W. H. Press et. al. &quot;Numerical Recipes. The Art of Scientific 
           Computing. 3rd edition&quot;, sec. 5.7. 
 
    .. [2] A. Curtis, M. J. D. Powell, and J. Reid, &quot;On the estimation of 
           sparse Jacobian matrices&quot;, Journal of the Institute of Mathematics 
           and its Applications, 13 (1974), pp. 117-120. 
 
    .. [3] B. Fornberg, &quot;Generation of Finite Difference Formulas on 
           Arbitrarily Spaced Grids&quot;, Mathematics of Computation 51, 1988. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.optimize._numdiff import approx_derivative 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; def f(x, c1, c2): 
    ...     return np.array([x[0] * np.sin(c1 * x[1]), 
    ...                      x[0] * np.cos(c2 * x[1])]) 
    ... 
    &gt;&gt;&gt; x0 = np.array([1.0, 0.5 * np.pi]) 
    &gt;&gt;&gt; approx_derivative(f, x0, args=(1, 2)) 
    array([[ 1.,  0.], 
           [-1.,  0.]]) 
 
    Bounds can be used to limit the region of function evaluation. 
    In the example below we compute left and right derivative at point 1.0. 
 
    &gt;&gt;&gt; def g(x): 
    ...     return x**2 if x &gt;= 1 else x 
    ... 
    &gt;&gt;&gt; x0 = 1.0 
    &gt;&gt;&gt; approx_derivative(g, x0, bounds=(-np.inf, 1.0)) 
    array([ 1.]) 
    &gt;&gt;&gt; approx_derivative(g, x0, bounds=(1.0, np.inf)) 
    array([ 2.]) 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">method </span><span class="s2">not in </span><span class="s1">[</span><span class="s3">'2-point'</span><span class="s2">, </span><span class="s3">'3-point'</span><span class="s2">, </span><span class="s3">'cs'</span><span class="s1">]:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Unknown method '%s'. &quot; </span><span class="s1">% method)</span>

    <span class="s1">x0 = np.atleast_1d(x0)</span>
    <span class="s2">if </span><span class="s1">x0.ndim &gt; </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`x0` must have at most 1 dimension.&quot;</span><span class="s1">)</span>

    <span class="s1">lb</span><span class="s2">, </span><span class="s1">ub = _prepare_bounds(bounds</span><span class="s2">, </span><span class="s1">x0)</span>

    <span class="s2">if </span><span class="s1">lb.shape != x0.shape </span><span class="s2">or </span><span class="s1">ub.shape != x0.shape:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Inconsistent shapes between bounds and `x0`.&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">as_linear_operator </span><span class="s2">and not </span><span class="s1">(np.all(np.isinf(lb))</span>
                                   <span class="s2">and </span><span class="s1">np.all(np.isinf(ub))):</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Bounds not supported when &quot;</span>
                         <span class="s3">&quot;`as_linear_operator` is True.&quot;</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">fun_wrapped(x):</span>
        <span class="s1">f = np.atleast_1d(fun(x</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">**kwargs))</span>
        <span class="s2">if </span><span class="s1">f.ndim &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">RuntimeError(</span><span class="s3">&quot;`fun` return value has &quot;</span>
                               <span class="s3">&quot;more than 1 dimension.&quot;</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">f</span>

    <span class="s2">if </span><span class="s1">f0 </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">f0 = fun_wrapped(x0)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">f0 = np.atleast_1d(f0)</span>
        <span class="s2">if </span><span class="s1">f0.ndim &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`f0` passed has more than 1 dimension.&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">np.any((x0 &lt; lb) | (x0 &gt; ub)):</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`x0` violates bound constraints.&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">as_linear_operator:</span>
        <span class="s2">if </span><span class="s1">rel_step </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">rel_step = _eps_for_method(x0.dtype</span><span class="s2">, </span><span class="s1">f0.dtype</span><span class="s2">, </span><span class="s1">method)</span>

        <span class="s2">return </span><span class="s1">_linear_operator_difference(fun_wrapped</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">,</span>
                                           <span class="s1">f0</span><span class="s2">, </span><span class="s1">rel_step</span><span class="s2">, </span><span class="s1">method)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s5"># by default we use rel_step</span>
        <span class="s2">if </span><span class="s1">abs_step </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">h = _compute_absolute_step(rel_step</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">f0</span><span class="s2">, </span><span class="s1">method)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s5"># user specifies an absolute step</span>
            <span class="s1">sign_x0 = (x0 &gt;= </span><span class="s4">0</span><span class="s1">).astype(float) * </span><span class="s4">2 </span><span class="s1">- </span><span class="s4">1</span>
            <span class="s1">h = abs_step</span>

            <span class="s5"># cannot have a zero step. This might happen if x0 is very large</span>
            <span class="s5"># or small. In which case fall back to relative step.</span>
            <span class="s1">dx = ((x0 + h) - x0)</span>
            <span class="s1">h = np.where(dx == </span><span class="s4">0</span><span class="s2">,</span>
                         <span class="s1">_eps_for_method(x0.dtype</span><span class="s2">, </span><span class="s1">f0.dtype</span><span class="s2">, </span><span class="s1">method) *</span>
                         <span class="s1">sign_x0 * np.maximum(</span><span class="s4">1.0</span><span class="s2">, </span><span class="s1">np.abs(x0))</span><span class="s2">,</span>
                         <span class="s1">h)</span>

        <span class="s2">if </span><span class="s1">method == </span><span class="s3">'2-point'</span><span class="s1">:</span>
            <span class="s1">h</span><span class="s2">, </span><span class="s1">use_one_sided = _adjust_scheme_to_bounds(</span>
                <span class="s1">x0</span><span class="s2">, </span><span class="s1">h</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s3">'1-sided'</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">ub)</span>
        <span class="s2">elif </span><span class="s1">method == </span><span class="s3">'3-point'</span><span class="s1">:</span>
            <span class="s1">h</span><span class="s2">, </span><span class="s1">use_one_sided = _adjust_scheme_to_bounds(</span>
                <span class="s1">x0</span><span class="s2">, </span><span class="s1">h</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s3">'2-sided'</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">ub)</span>
        <span class="s2">elif </span><span class="s1">method == </span><span class="s3">'cs'</span><span class="s1">:</span>
            <span class="s1">use_one_sided = </span><span class="s2">False</span>

        <span class="s2">if </span><span class="s1">sparsity </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">_dense_difference(fun_wrapped</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">f0</span><span class="s2">, </span><span class="s1">h</span><span class="s2">,</span>
                                     <span class="s1">use_one_sided</span><span class="s2">, </span><span class="s1">method)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">if not </span><span class="s1">issparse(sparsity) </span><span class="s2">and </span><span class="s1">len(sparsity) == </span><span class="s4">2</span><span class="s1">:</span>
                <span class="s1">structure</span><span class="s2">, </span><span class="s1">groups = sparsity</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">structure = sparsity</span>
                <span class="s1">groups = group_columns(sparsity)</span>

            <span class="s2">if </span><span class="s1">issparse(structure):</span>
                <span class="s1">structure = csc_matrix(structure)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">structure = np.atleast_2d(structure)</span>

            <span class="s1">groups = np.atleast_1d(groups)</span>
            <span class="s2">return </span><span class="s1">_sparse_difference(fun_wrapped</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">f0</span><span class="s2">, </span><span class="s1">h</span><span class="s2">,</span>
                                      <span class="s1">use_one_sided</span><span class="s2">, </span><span class="s1">structure</span><span class="s2">,</span>
                                      <span class="s1">groups</span><span class="s2">, </span><span class="s1">method)</span>


<span class="s2">def </span><span class="s1">_linear_operator_difference(fun</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">f0</span><span class="s2">, </span><span class="s1">h</span><span class="s2">, </span><span class="s1">method):</span>
    <span class="s1">m = f0.size</span>
    <span class="s1">n = x0.size</span>

    <span class="s2">if </span><span class="s1">method == </span><span class="s3">'2-point'</span><span class="s1">:</span>
        <span class="s2">def </span><span class="s1">matvec(p):</span>
            <span class="s2">if </span><span class="s1">np.array_equal(p</span><span class="s2">, </span><span class="s1">np.zeros_like(p)):</span>
                <span class="s2">return </span><span class="s1">np.zeros(m)</span>
            <span class="s1">dx = h / norm(p)</span>
            <span class="s1">x = x0 + dx*p</span>
            <span class="s1">df = fun(x) - f0</span>
            <span class="s2">return </span><span class="s1">df / dx</span>

    <span class="s2">elif </span><span class="s1">method == </span><span class="s3">'3-point'</span><span class="s1">:</span>
        <span class="s2">def </span><span class="s1">matvec(p):</span>
            <span class="s2">if </span><span class="s1">np.array_equal(p</span><span class="s2">, </span><span class="s1">np.zeros_like(p)):</span>
                <span class="s2">return </span><span class="s1">np.zeros(m)</span>
            <span class="s1">dx = </span><span class="s4">2</span><span class="s1">*h / norm(p)</span>
            <span class="s1">x1 = x0 - (dx/</span><span class="s4">2</span><span class="s1">)*p</span>
            <span class="s1">x2 = x0 + (dx/</span><span class="s4">2</span><span class="s1">)*p</span>
            <span class="s1">f1 = fun(x1)</span>
            <span class="s1">f2 = fun(x2)</span>
            <span class="s1">df = f2 - f1</span>
            <span class="s2">return </span><span class="s1">df / dx</span>

    <span class="s2">elif </span><span class="s1">method == </span><span class="s3">'cs'</span><span class="s1">:</span>
        <span class="s2">def </span><span class="s1">matvec(p):</span>
            <span class="s2">if </span><span class="s1">np.array_equal(p</span><span class="s2">, </span><span class="s1">np.zeros_like(p)):</span>
                <span class="s2">return </span><span class="s1">np.zeros(m)</span>
            <span class="s1">dx = h / norm(p)</span>
            <span class="s1">x = x0 + dx*p*</span><span class="s4">1.j</span>
            <span class="s1">f1 = fun(x)</span>
            <span class="s1">df = f1.imag</span>
            <span class="s2">return </span><span class="s1">df / dx</span>

    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">RuntimeError(</span><span class="s3">&quot;Never be here.&quot;</span><span class="s1">)</span>

    <span class="s2">return </span><span class="s1">LinearOperator((m</span><span class="s2">, </span><span class="s1">n)</span><span class="s2">, </span><span class="s1">matvec)</span>


<span class="s2">def </span><span class="s1">_dense_difference(fun</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">f0</span><span class="s2">, </span><span class="s1">h</span><span class="s2">, </span><span class="s1">use_one_sided</span><span class="s2">, </span><span class="s1">method):</span>
    <span class="s1">m = f0.size</span>
    <span class="s1">n = x0.size</span>
    <span class="s1">J_transposed = np.empty((n</span><span class="s2">, </span><span class="s1">m))</span>
    <span class="s1">h_vecs = np.diag(h)</span>

    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(h.size):</span>
        <span class="s2">if </span><span class="s1">method == </span><span class="s3">'2-point'</span><span class="s1">:</span>
            <span class="s1">x = x0 + h_vecs[i]</span>
            <span class="s1">dx = x[i] - x0[i]  </span><span class="s5"># Recompute dx as exactly representable number.</span>
            <span class="s1">df = fun(x) - f0</span>
        <span class="s2">elif </span><span class="s1">method == </span><span class="s3">'3-point' </span><span class="s2">and </span><span class="s1">use_one_sided[i]:</span>
            <span class="s1">x1 = x0 + h_vecs[i]</span>
            <span class="s1">x2 = x0 + </span><span class="s4">2 </span><span class="s1">* h_vecs[i]</span>
            <span class="s1">dx = x2[i] - x0[i]</span>
            <span class="s1">f1 = fun(x1)</span>
            <span class="s1">f2 = fun(x2)</span>
            <span class="s1">df = -</span><span class="s4">3.0 </span><span class="s1">* f0 + </span><span class="s4">4 </span><span class="s1">* f1 - f2</span>
        <span class="s2">elif </span><span class="s1">method == </span><span class="s3">'3-point' </span><span class="s2">and not </span><span class="s1">use_one_sided[i]:</span>
            <span class="s1">x1 = x0 - h_vecs[i]</span>
            <span class="s1">x2 = x0 + h_vecs[i]</span>
            <span class="s1">dx = x2[i] - x1[i]</span>
            <span class="s1">f1 = fun(x1)</span>
            <span class="s1">f2 = fun(x2)</span>
            <span class="s1">df = f2 - f1</span>
        <span class="s2">elif </span><span class="s1">method == </span><span class="s3">'cs'</span><span class="s1">:</span>
            <span class="s1">f1 = fun(x0 + h_vecs[i]*</span><span class="s4">1.j</span><span class="s1">)</span>
            <span class="s1">df = f1.imag</span>
            <span class="s1">dx = h_vecs[i</span><span class="s2">, </span><span class="s1">i]</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">RuntimeError(</span><span class="s3">&quot;Never be here.&quot;</span><span class="s1">)</span>

        <span class="s1">J_transposed[i] = df / dx</span>

    <span class="s2">if </span><span class="s1">m == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">J_transposed = np.ravel(J_transposed)</span>

    <span class="s2">return </span><span class="s1">J_transposed.T</span>


<span class="s2">def </span><span class="s1">_sparse_difference(fun</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">f0</span><span class="s2">, </span><span class="s1">h</span><span class="s2">, </span><span class="s1">use_one_sided</span><span class="s2">,</span>
                       <span class="s1">structure</span><span class="s2">, </span><span class="s1">groups</span><span class="s2">, </span><span class="s1">method):</span>
    <span class="s1">m = f0.size</span>
    <span class="s1">n = x0.size</span>
    <span class="s1">row_indices = []</span>
    <span class="s1">col_indices = []</span>
    <span class="s1">fractions = []</span>

    <span class="s1">n_groups = np.max(groups) + </span><span class="s4">1</span>
    <span class="s2">for </span><span class="s1">group </span><span class="s2">in </span><span class="s1">range(n_groups):</span>
        <span class="s5"># Perturb variables which are in the same group simultaneously.</span>
        <span class="s1">e = np.equal(group</span><span class="s2">, </span><span class="s1">groups)</span>
        <span class="s1">h_vec = h * e</span>
        <span class="s2">if </span><span class="s1">method == </span><span class="s3">'2-point'</span><span class="s1">:</span>
            <span class="s1">x = x0 + h_vec</span>
            <span class="s1">dx = x - x0</span>
            <span class="s1">df = fun(x) - f0</span>
            <span class="s5"># The result is  written to columns which correspond to perturbed</span>
            <span class="s5"># variables.</span>
            <span class="s1">cols</span><span class="s2">, </span><span class="s1">= np.nonzero(e)</span>
            <span class="s5"># Find all non-zero elements in selected columns of Jacobian.</span>
            <span class="s1">i</span><span class="s2">, </span><span class="s1">j</span><span class="s2">, </span><span class="s1">_ = find(structure[:</span><span class="s2">, </span><span class="s1">cols])</span>
            <span class="s5"># Restore column indices in the full array.</span>
            <span class="s1">j = cols[j]</span>
        <span class="s2">elif </span><span class="s1">method == </span><span class="s3">'3-point'</span><span class="s1">:</span>
            <span class="s5"># Here we do conceptually the same but separate one-sided</span>
            <span class="s5"># and two-sided schemes.</span>
            <span class="s1">x1 = x0.copy()</span>
            <span class="s1">x2 = x0.copy()</span>

            <span class="s1">mask_1 = use_one_sided &amp; e</span>
            <span class="s1">x1[mask_1] += h_vec[mask_1]</span>
            <span class="s1">x2[mask_1] += </span><span class="s4">2 </span><span class="s1">* h_vec[mask_1]</span>

            <span class="s1">mask_2 = ~use_one_sided &amp; e</span>
            <span class="s1">x1[mask_2] -= h_vec[mask_2]</span>
            <span class="s1">x2[mask_2] += h_vec[mask_2]</span>

            <span class="s1">dx = np.zeros(n)</span>
            <span class="s1">dx[mask_1] = x2[mask_1] - x0[mask_1]</span>
            <span class="s1">dx[mask_2] = x2[mask_2] - x1[mask_2]</span>

            <span class="s1">f1 = fun(x1)</span>
            <span class="s1">f2 = fun(x2)</span>

            <span class="s1">cols</span><span class="s2">, </span><span class="s1">= np.nonzero(e)</span>
            <span class="s1">i</span><span class="s2">, </span><span class="s1">j</span><span class="s2">, </span><span class="s1">_ = find(structure[:</span><span class="s2">, </span><span class="s1">cols])</span>
            <span class="s1">j = cols[j]</span>

            <span class="s1">mask = use_one_sided[j]</span>
            <span class="s1">df = np.empty(m)</span>

            <span class="s1">rows = i[mask]</span>
            <span class="s1">df[rows] = -</span><span class="s4">3 </span><span class="s1">* f0[rows] + </span><span class="s4">4 </span><span class="s1">* f1[rows] - f2[rows]</span>

            <span class="s1">rows = i[~mask]</span>
            <span class="s1">df[rows] = f2[rows] - f1[rows]</span>
        <span class="s2">elif </span><span class="s1">method == </span><span class="s3">'cs'</span><span class="s1">:</span>
            <span class="s1">f1 = fun(x0 + h_vec*</span><span class="s4">1.j</span><span class="s1">)</span>
            <span class="s1">df = f1.imag</span>
            <span class="s1">dx = h_vec</span>
            <span class="s1">cols</span><span class="s2">, </span><span class="s1">= np.nonzero(e)</span>
            <span class="s1">i</span><span class="s2">, </span><span class="s1">j</span><span class="s2">, </span><span class="s1">_ = find(structure[:</span><span class="s2">, </span><span class="s1">cols])</span>
            <span class="s1">j = cols[j]</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Never be here.&quot;</span><span class="s1">)</span>

        <span class="s5"># All that's left is to compute the fraction. We store i, j and</span>
        <span class="s5"># fractions as separate arrays and later construct coo_matrix.</span>
        <span class="s1">row_indices.append(i)</span>
        <span class="s1">col_indices.append(j)</span>
        <span class="s1">fractions.append(df[i] / dx[j])</span>

    <span class="s1">row_indices = np.hstack(row_indices)</span>
    <span class="s1">col_indices = np.hstack(col_indices)</span>
    <span class="s1">fractions = np.hstack(fractions)</span>
    <span class="s1">J = coo_matrix((fractions</span><span class="s2">, </span><span class="s1">(row_indices</span><span class="s2">, </span><span class="s1">col_indices))</span><span class="s2">, </span><span class="s1">shape=(m</span><span class="s2">, </span><span class="s1">n))</span>
    <span class="s2">return </span><span class="s1">csr_matrix(J)</span>


<span class="s2">def </span><span class="s1">check_derivative(fun</span><span class="s2">, </span><span class="s1">jac</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">bounds=(-np.inf</span><span class="s2">, </span><span class="s1">np.inf)</span><span class="s2">, </span><span class="s1">args=()</span><span class="s2">,</span>
                     <span class="s1">kwargs={}):</span>
    <span class="s0">&quot;&quot;&quot;Check correctness of a function computing derivatives (Jacobian or 
    gradient) by comparison with a finite difference approximation. 
 
    Parameters 
    ---------- 
    fun : callable 
        Function of which to estimate the derivatives. The argument x 
        passed to this function is ndarray of shape (n,) (never a scalar 
        even if n=1). It must return 1-D array_like of shape (m,) or a scalar. 
    jac : callable 
        Function which computes Jacobian matrix of `fun`. It must work with 
        argument x the same way as `fun`. The return value must be array_like 
        or sparse matrix with an appropriate shape. 
    x0 : array_like of shape (n,) or float 
        Point at which to estimate the derivatives. Float will be converted 
        to 1-D array. 
    bounds : 2-tuple of array_like, optional 
        Lower and upper bounds on independent variables. Defaults to no bounds. 
        Each bound must match the size of `x0` or be a scalar, in the latter 
        case the bound will be the same for all variables. Use it to limit the 
        range of function evaluation. 
    args, kwargs : tuple and dict, optional 
        Additional arguments passed to `fun` and `jac`. Both empty by default. 
        The calling signature is ``fun(x, *args, **kwargs)`` and the same 
        for `jac`. 
 
    Returns 
    ------- 
    accuracy : float 
        The maximum among all relative errors for elements with absolute values 
        higher than 1 and absolute errors for elements with absolute values 
        less or equal than 1. If `accuracy` is on the order of 1e-6 or lower, 
        then it is likely that your `jac` implementation is correct. 
 
    See Also 
    -------- 
    approx_derivative : Compute finite difference approximation of derivative. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.optimize._numdiff import check_derivative 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; def f(x, c1, c2): 
    ...     return np.array([x[0] * np.sin(c1 * x[1]), 
    ...                      x[0] * np.cos(c2 * x[1])]) 
    ... 
    &gt;&gt;&gt; def jac(x, c1, c2): 
    ...     return np.array([ 
    ...         [np.sin(c1 * x[1]),  c1 * x[0] * np.cos(c1 * x[1])], 
    ...         [np.cos(c2 * x[1]), -c2 * x[0] * np.sin(c2 * x[1])] 
    ...     ]) 
    ... 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; x0 = np.array([1.0, 0.5 * np.pi]) 
    &gt;&gt;&gt; check_derivative(f, jac, x0, args=(1, 2)) 
    2.4492935982947064e-16 
    &quot;&quot;&quot;</span>
    <span class="s1">J_to_test = jac(x0</span><span class="s2">, </span><span class="s1">*args</span><span class="s2">, </span><span class="s1">**kwargs)</span>
    <span class="s2">if </span><span class="s1">issparse(J_to_test):</span>
        <span class="s1">J_diff = approx_derivative(fun</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">bounds=bounds</span><span class="s2">, </span><span class="s1">sparsity=J_to_test</span><span class="s2">,</span>
                                   <span class="s1">args=args</span><span class="s2">, </span><span class="s1">kwargs=kwargs)</span>
        <span class="s1">J_to_test = csr_matrix(J_to_test)</span>
        <span class="s1">abs_err = J_to_test - J_diff</span>
        <span class="s1">i</span><span class="s2">, </span><span class="s1">j</span><span class="s2">, </span><span class="s1">abs_err_data = find(abs_err)</span>
        <span class="s1">J_diff_data = np.asarray(J_diff[i</span><span class="s2">, </span><span class="s1">j]).ravel()</span>
        <span class="s2">return </span><span class="s1">np.max(np.abs(abs_err_data) /</span>
                      <span class="s1">np.maximum(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">np.abs(J_diff_data)))</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">J_diff = approx_derivative(fun</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">bounds=bounds</span><span class="s2">,</span>
                                   <span class="s1">args=args</span><span class="s2">, </span><span class="s1">kwargs=kwargs)</span>
        <span class="s1">abs_err = np.abs(J_to_test - J_diff)</span>
        <span class="s2">return </span><span class="s1">np.max(abs_err / np.maximum(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">np.abs(J_diff)))</span>
</pre>
</body>
</html>