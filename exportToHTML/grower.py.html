<html>
<head>
<title>grower.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
grower.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
This module contains the TreeGrower class. 
 
TreeGrower builds a regression tree fitting a Newton-Raphson step, based on 
the gradients and hessians of the training data. 
&quot;&quot;&quot;</span>
<span class="s2"># Author: Nicolas Hug</span>

<span class="s3">import </span><span class="s1">numbers</span>
<span class="s3">from </span><span class="s1">heapq </span><span class="s3">import </span><span class="s1">heappop</span><span class="s3">, </span><span class="s1">heappush</span>
<span class="s3">from </span><span class="s1">timeit </span><span class="s3">import </span><span class="s1">default_timer </span><span class="s3">as </span><span class="s1">time</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">sklearn.utils._openmp_helpers </span><span class="s3">import </span><span class="s1">_openmp_effective_n_threads</span>

<span class="s3">from </span><span class="s1">._bitset </span><span class="s3">import </span><span class="s1">set_raw_bitset_from_binned_bitset</span>
<span class="s3">from </span><span class="s1">.common </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">PREDICTOR_RECORD_DTYPE</span><span class="s3">,</span>
    <span class="s1">X_BITSET_INNER_DTYPE</span><span class="s3">,</span>
    <span class="s1">Y_DTYPE</span><span class="s3">,</span>
    <span class="s1">MonotonicConstraint</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">.histogram </span><span class="s3">import </span><span class="s1">HistogramBuilder</span>
<span class="s3">from </span><span class="s1">.predictor </span><span class="s3">import </span><span class="s1">TreePredictor</span>
<span class="s3">from </span><span class="s1">.splitting </span><span class="s3">import </span><span class="s1">Splitter</span>
<span class="s3">from </span><span class="s1">.utils </span><span class="s3">import </span><span class="s1">sum_parallel</span>

<span class="s1">EPS = np.finfo(Y_DTYPE).eps  </span><span class="s2"># to avoid zero division errors</span>


<span class="s3">class </span><span class="s1">TreeNode:</span>
    <span class="s0">&quot;&quot;&quot;Tree Node class used in TreeGrower. 
 
    This isn't used for prediction purposes, only for training (see 
    TreePredictor). 
 
    Parameters 
    ---------- 
    depth : int 
        The depth of the node, i.e. its distance from the root. 
    sample_indices : ndarray of shape (n_samples_at_node,), dtype=np.uint 
        The indices of the samples at the node. 
    sum_gradients : float 
        The sum of the gradients of the samples at the node. 
    sum_hessians : float 
        The sum of the hessians of the samples at the node. 
 
    Attributes 
    ---------- 
    depth : int 
        The depth of the node, i.e. its distance from the root. 
    sample_indices : ndarray of shape (n_samples_at_node,), dtype=np.uint 
        The indices of the samples at the node. 
    sum_gradients : float 
        The sum of the gradients of the samples at the node. 
    sum_hessians : float 
        The sum of the hessians of the samples at the node. 
    split_info : SplitInfo or None 
        The result of the split evaluation. 
    is_leaf : bool 
        True if node is a leaf 
    left_child : TreeNode or None 
        The left child of the node. None for leaves. 
    right_child : TreeNode or None 
        The right child of the node. None for leaves. 
    value : float or None 
        The value of the leaf, as computed in finalize_leaf(). None for 
        non-leaf nodes. 
    partition_start : int 
        start position of the node's sample_indices in splitter.partition. 
    partition_stop : int 
        stop position of the node's sample_indices in splitter.partition. 
    allowed_features : None or ndarray, dtype=int 
        Indices of features allowed to split for children. 
    interaction_cst_indices : None or list of ints 
        Indices of the interaction sets that have to be applied on splits of 
        child nodes. The fewer sets the stronger the constraint as fewer sets 
        contain fewer features. 
    children_lower_bound : float 
    children_upper_bound : float 
    &quot;&quot;&quot;</span>

    <span class="s1">split_info = </span><span class="s3">None</span>
    <span class="s1">left_child = </span><span class="s3">None</span>
    <span class="s1">right_child = </span><span class="s3">None</span>
    <span class="s1">histograms = </span><span class="s3">None</span>

    <span class="s2"># start and stop indices of the node in the splitter.partition</span>
    <span class="s2"># array. Concretely,</span>
    <span class="s2"># self.sample_indices = view(self.splitter.partition[start:stop])</span>
    <span class="s2"># Please see the comments about splitter.partition and</span>
    <span class="s2"># splitter.split_indices for more info about this design.</span>
    <span class="s2"># These 2 attributes are only used in _update_raw_prediction, because we</span>
    <span class="s2"># need to iterate over the leaves and I don't know how to efficiently</span>
    <span class="s2"># store the sample_indices views because they're all of different sizes.</span>
    <span class="s1">partition_start = </span><span class="s4">0</span>
    <span class="s1">partition_stop = </span><span class="s4">0</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">depth</span><span class="s3">, </span><span class="s1">sample_indices</span><span class="s3">, </span><span class="s1">sum_gradients</span><span class="s3">, </span><span class="s1">sum_hessians</span><span class="s3">, </span><span class="s1">value=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">self.depth = depth</span>
        <span class="s1">self.sample_indices = sample_indices</span>
        <span class="s1">self.n_samples = sample_indices.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">self.sum_gradients = sum_gradients</span>
        <span class="s1">self.sum_hessians = sum_hessians</span>
        <span class="s1">self.value = value</span>
        <span class="s1">self.is_leaf = </span><span class="s3">False</span>
        <span class="s1">self.allowed_features = </span><span class="s3">None</span>
        <span class="s1">self.interaction_cst_indices = </span><span class="s3">None</span>
        <span class="s1">self.set_children_bounds(float(</span><span class="s5">&quot;-inf&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">float(</span><span class="s5">&quot;+inf&quot;</span><span class="s1">))</span>

    <span class="s3">def </span><span class="s1">set_children_bounds(self</span><span class="s3">, </span><span class="s1">lower</span><span class="s3">, </span><span class="s1">upper):</span>
        <span class="s0">&quot;&quot;&quot;Set children values bounds to respect monotonic constraints.&quot;&quot;&quot;</span>

        <span class="s2"># These are bounds for the node's *children* values, not the node's</span>
        <span class="s2"># value. The bounds are used in the splitter when considering potential</span>
        <span class="s2"># left and right child.</span>
        <span class="s1">self.children_lower_bound = lower</span>
        <span class="s1">self.children_upper_bound = upper</span>

    <span class="s3">def </span><span class="s1">__lt__(self</span><span class="s3">, </span><span class="s1">other_node):</span>
        <span class="s0">&quot;&quot;&quot;Comparison for priority queue. 
 
        Nodes with high gain are higher priority than nodes with low gain. 
 
        heapq.heappush only need the '&lt;' operator. 
        heapq.heappop take the smallest item first (smaller is higher 
        priority). 
 
        Parameters 
        ---------- 
        other_node : TreeNode 
            The node to compare with. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.split_info.gain &gt; other_node.split_info.gain</span>


<span class="s3">class </span><span class="s1">TreeGrower:</span>
    <span class="s0">&quot;&quot;&quot;Tree grower class used to build a tree. 
 
    The tree is fitted to predict the values of a Newton-Raphson step. The 
    splits are considered in a best-first fashion, and the quality of a 
    split is defined in splitting._split_gain. 
 
    Parameters 
    ---------- 
    X_binned : ndarray of shape (n_samples, n_features), dtype=np.uint8 
        The binned input samples. Must be Fortran-aligned. 
    gradients : ndarray of shape (n_samples,) 
        The gradients of each training sample. Those are the gradients of the 
        loss w.r.t the predictions, evaluated at iteration ``i - 1``. 
    hessians : ndarray of shape (n_samples,) 
        The hessians of each training sample. Those are the hessians of the 
        loss w.r.t the predictions, evaluated at iteration ``i - 1``. 
    max_leaf_nodes : int, default=None 
        The maximum number of leaves for each tree. If None, there is no 
        maximum limit. 
    max_depth : int, default=None 
        The maximum depth of each tree. The depth of a tree is the number of 
        edges to go from the root to the deepest leaf. 
        Depth isn't constrained by default. 
    min_samples_leaf : int, default=20 
        The minimum number of samples per leaf. 
    min_gain_to_split : float, default=0. 
        The minimum gain needed to split a node. Splits with lower gain will 
        be ignored. 
    n_bins : int, default=256 
        The total number of bins, including the bin for missing values. Used 
        to define the shape of the histograms. 
    n_bins_non_missing : ndarray, dtype=np.uint32, default=None 
        For each feature, gives the number of bins actually used for 
        non-missing values. For features with a lot of unique values, this 
        is equal to ``n_bins - 1``. If it's an int, all features are 
        considered to have the same number of bins. If None, all features 
        are considered to have ``n_bins - 1`` bins. 
    has_missing_values : bool or ndarray, dtype=bool, default=False 
        Whether each feature contains missing values (in the training data). 
        If it's a bool, the same value is used for all features. 
    is_categorical : ndarray of bool of shape (n_features,), default=None 
        Indicates categorical features. 
    monotonic_cst : array-like of int of shape (n_features,), dtype=int, default=None 
        Indicates the monotonic constraint to enforce on each feature. 
          - 1: monotonic increase 
          - 0: no constraint 
          - -1: monotonic decrease 
 
        Read more in the :ref:`User Guide &lt;monotonic_cst_gbdt&gt;`. 
    interaction_cst : list of sets of integers, default=None 
        List of interaction constraints. 
    l2_regularization : float, default=0. 
        The L2 regularization parameter. 
    min_hessian_to_split : float, default=1e-3 
        The minimum sum of hessians needed in each node. Splits that result in 
        at least one child having a sum of hessians less than 
        ``min_hessian_to_split`` are discarded. 
    shrinkage : float, default=1. 
        The shrinkage parameter to apply to the leaves values, also known as 
        learning rate. 
    n_threads : int, default=None 
        Number of OpenMP threads to use. `_openmp_effective_n_threads` is called 
        to determine the effective number of threads use, which takes cgroups CPU 
        quotes into account. See the docstring of `_openmp_effective_n_threads` 
        for details. 
 
    Attributes 
    ---------- 
    histogram_builder : HistogramBuilder 
    splitter : Splitter 
    root : TreeNode 
    finalized_leaves : list of TreeNode 
    splittable_nodes : list of TreeNode 
    missing_values_bin_idx : int 
        Equals n_bins - 1 
    n_categorical_splits : int 
    n_features : int 
    n_nodes : int 
    total_find_split_time : float 
        Time spent finding the best splits 
    total_compute_hist_time : float 
        Time spent computing histograms 
    total_apply_split_time : float 
        Time spent splitting nodes 
    with_monotonic_cst : bool 
        Whether there are monotonic constraints that apply. False iff monotonic_cst is 
        None. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">X_binned</span><span class="s3">,</span>
        <span class="s1">gradients</span><span class="s3">,</span>
        <span class="s1">hessians</span><span class="s3">,</span>
        <span class="s1">max_leaf_nodes=</span><span class="s3">None,</span>
        <span class="s1">max_depth=</span><span class="s3">None,</span>
        <span class="s1">min_samples_leaf=</span><span class="s4">20</span><span class="s3">,</span>
        <span class="s1">min_gain_to_split=</span><span class="s4">0.0</span><span class="s3">,</span>
        <span class="s1">n_bins=</span><span class="s4">256</span><span class="s3">,</span>
        <span class="s1">n_bins_non_missing=</span><span class="s3">None,</span>
        <span class="s1">has_missing_values=</span><span class="s3">False,</span>
        <span class="s1">is_categorical=</span><span class="s3">None,</span>
        <span class="s1">monotonic_cst=</span><span class="s3">None,</span>
        <span class="s1">interaction_cst=</span><span class="s3">None,</span>
        <span class="s1">l2_regularization=</span><span class="s4">0.0</span><span class="s3">,</span>
        <span class="s1">min_hessian_to_split=</span><span class="s4">1e-3</span><span class="s3">,</span>
        <span class="s1">shrinkage=</span><span class="s4">1.0</span><span class="s3">,</span>
        <span class="s1">n_threads=</span><span class="s3">None,</span>
    <span class="s1">):</span>
        <span class="s1">self._validate_parameters(</span>
            <span class="s1">X_binned</span><span class="s3">,</span>
            <span class="s1">min_gain_to_split</span><span class="s3">,</span>
            <span class="s1">min_hessian_to_split</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">n_threads = _openmp_effective_n_threads(n_threads)</span>

        <span class="s3">if </span><span class="s1">n_bins_non_missing </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">n_bins_non_missing = n_bins - </span><span class="s4">1</span>

        <span class="s3">if </span><span class="s1">isinstance(n_bins_non_missing</span><span class="s3">, </span><span class="s1">numbers.Integral):</span>
            <span class="s1">n_bins_non_missing = np.array(</span>
                <span class="s1">[n_bins_non_missing] * X_binned.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.uint32</span>
            <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">n_bins_non_missing = np.asarray(n_bins_non_missing</span><span class="s3">, </span><span class="s1">dtype=np.uint32)</span>

        <span class="s3">if </span><span class="s1">isinstance(has_missing_values</span><span class="s3">, </span><span class="s1">bool):</span>
            <span class="s1">has_missing_values = [has_missing_values] * X_binned.shape[</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">has_missing_values = np.asarray(has_missing_values</span><span class="s3">, </span><span class="s1">dtype=np.uint8)</span>

        <span class="s2"># `monotonic_cst` validation is done in _validate_monotonic_cst</span>
        <span class="s2"># at the estimator level and therefore the following should not be</span>
        <span class="s2"># needed when using the public API.</span>
        <span class="s3">if </span><span class="s1">monotonic_cst </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">monotonic_cst = np.full(</span>
                <span class="s1">shape=X_binned.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">fill_value=MonotonicConstraint.NO_CST</span><span class="s3">,</span>
                <span class="s1">dtype=np.int8</span><span class="s3">,</span>
            <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">monotonic_cst = np.asarray(monotonic_cst</span><span class="s3">, </span><span class="s1">dtype=np.int8)</span>
        <span class="s1">self.with_monotonic_cst = np.any(monotonic_cst != MonotonicConstraint.NO_CST)</span>

        <span class="s3">if </span><span class="s1">is_categorical </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">is_categorical = np.zeros(shape=X_binned.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=np.uint8)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">is_categorical = np.asarray(is_categorical</span><span class="s3">, </span><span class="s1">dtype=np.uint8)</span>

        <span class="s3">if </span><span class="s1">np.any(</span>
            <span class="s1">np.logical_and(</span>
                <span class="s1">is_categorical == </span><span class="s4">1</span><span class="s3">, </span><span class="s1">monotonic_cst != MonotonicConstraint.NO_CST</span>
            <span class="s1">)</span>
        <span class="s1">):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Categorical features cannot have monotonic constraints.&quot;</span><span class="s1">)</span>

        <span class="s1">hessians_are_constant = hessians.shape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">1</span>
        <span class="s1">self.histogram_builder = HistogramBuilder(</span>
            <span class="s1">X_binned</span><span class="s3">, </span><span class="s1">n_bins</span><span class="s3">, </span><span class="s1">gradients</span><span class="s3">, </span><span class="s1">hessians</span><span class="s3">, </span><span class="s1">hessians_are_constant</span><span class="s3">, </span><span class="s1">n_threads</span>
        <span class="s1">)</span>
        <span class="s1">missing_values_bin_idx = n_bins - </span><span class="s4">1</span>
        <span class="s1">self.splitter = Splitter(</span>
            <span class="s1">X_binned</span><span class="s3">,</span>
            <span class="s1">n_bins_non_missing</span><span class="s3">,</span>
            <span class="s1">missing_values_bin_idx</span><span class="s3">,</span>
            <span class="s1">has_missing_values</span><span class="s3">,</span>
            <span class="s1">is_categorical</span><span class="s3">,</span>
            <span class="s1">monotonic_cst</span><span class="s3">,</span>
            <span class="s1">l2_regularization</span><span class="s3">,</span>
            <span class="s1">min_hessian_to_split</span><span class="s3">,</span>
            <span class="s1">min_samples_leaf</span><span class="s3">,</span>
            <span class="s1">min_gain_to_split</span><span class="s3">,</span>
            <span class="s1">hessians_are_constant</span><span class="s3">,</span>
            <span class="s1">n_threads</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">self.n_bins_non_missing = n_bins_non_missing</span>
        <span class="s1">self.missing_values_bin_idx = missing_values_bin_idx</span>
        <span class="s1">self.max_leaf_nodes = max_leaf_nodes</span>
        <span class="s1">self.has_missing_values = has_missing_values</span>
        <span class="s1">self.monotonic_cst = monotonic_cst</span>
        <span class="s1">self.interaction_cst = interaction_cst</span>
        <span class="s1">self.is_categorical = is_categorical</span>
        <span class="s1">self.l2_regularization = l2_regularization</span>
        <span class="s1">self.n_features = X_binned.shape[</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">self.max_depth = max_depth</span>
        <span class="s1">self.min_samples_leaf = min_samples_leaf</span>
        <span class="s1">self.X_binned = X_binned</span>
        <span class="s1">self.min_gain_to_split = min_gain_to_split</span>
        <span class="s1">self.shrinkage = shrinkage</span>
        <span class="s1">self.n_threads = n_threads</span>
        <span class="s1">self.splittable_nodes = []</span>
        <span class="s1">self.finalized_leaves = []</span>
        <span class="s1">self.total_find_split_time = </span><span class="s4">0.0  </span><span class="s2"># time spent finding the best splits</span>
        <span class="s1">self.total_compute_hist_time = </span><span class="s4">0.0  </span><span class="s2"># time spent computing histograms</span>
        <span class="s1">self.total_apply_split_time = </span><span class="s4">0.0  </span><span class="s2"># time spent splitting nodes</span>
        <span class="s1">self.n_categorical_splits = </span><span class="s4">0</span>
        <span class="s1">self._intilialize_root(gradients</span><span class="s3">, </span><span class="s1">hessians</span><span class="s3">, </span><span class="s1">hessians_are_constant)</span>
        <span class="s1">self.n_nodes = </span><span class="s4">1</span>

    <span class="s3">def </span><span class="s1">_validate_parameters(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">X_binned</span><span class="s3">,</span>
        <span class="s1">min_gain_to_split</span><span class="s3">,</span>
        <span class="s1">min_hessian_to_split</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Validate parameters passed to __init__. 
 
        Also validate parameters passed to splitter. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">X_binned.dtype != np.uint8:</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError(</span><span class="s5">&quot;X_binned must be of type uint8.&quot;</span><span class="s1">)</span>
        <span class="s3">if not </span><span class="s1">X_binned.flags.f_contiguous:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s5">&quot;X_binned should be passed as Fortran contiguous &quot;</span>
                <span class="s5">&quot;array for maximum efficiency.&quot;</span>
            <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">min_gain_to_split &lt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s5">&quot;min_gain_to_split={} must be positive.&quot;</span><span class="s1">.format(min_gain_to_split)</span>
            <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">min_hessian_to_split &lt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s5">&quot;min_hessian_to_split={} must be positive.&quot;</span><span class="s1">.format(min_hessian_to_split)</span>
            <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">grow(self):</span>
        <span class="s0">&quot;&quot;&quot;Grow the tree, from root to leaves.&quot;&quot;&quot;</span>
        <span class="s3">while </span><span class="s1">self.splittable_nodes:</span>
            <span class="s1">self.split_next()</span>

        <span class="s1">self._apply_shrinkage()</span>

    <span class="s3">def </span><span class="s1">_apply_shrinkage(self):</span>
        <span class="s0">&quot;&quot;&quot;Multiply leaves values by shrinkage parameter. 
 
        This must be done at the very end of the growing process. If this were 
        done during the growing process e.g. in finalize_leaf(), then a leaf 
        would be shrunk but its sibling would potentially not be (if it's a 
        non-leaf), which would lead to a wrong computation of the 'middle' 
        value needed to enforce the monotonic constraints. 
        &quot;&quot;&quot;</span>
        <span class="s3">for </span><span class="s1">leaf </span><span class="s3">in </span><span class="s1">self.finalized_leaves:</span>
            <span class="s1">leaf.value *= self.shrinkage</span>

    <span class="s3">def </span><span class="s1">_intilialize_root(self</span><span class="s3">, </span><span class="s1">gradients</span><span class="s3">, </span><span class="s1">hessians</span><span class="s3">, </span><span class="s1">hessians_are_constant):</span>
        <span class="s0">&quot;&quot;&quot;Initialize root node and finalize it if needed.&quot;&quot;&quot;</span>
        <span class="s1">n_samples = self.X_binned.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">depth = </span><span class="s4">0</span>
        <span class="s1">sum_gradients = sum_parallel(gradients</span><span class="s3">, </span><span class="s1">self.n_threads)</span>
        <span class="s3">if </span><span class="s1">self.histogram_builder.hessians_are_constant:</span>
            <span class="s1">sum_hessians = hessians[</span><span class="s4">0</span><span class="s1">] * n_samples</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">sum_hessians = sum_parallel(hessians</span><span class="s3">, </span><span class="s1">self.n_threads)</span>
        <span class="s1">self.root = TreeNode(</span>
            <span class="s1">depth=depth</span><span class="s3">,</span>
            <span class="s1">sample_indices=self.splitter.partition</span><span class="s3">,</span>
            <span class="s1">sum_gradients=sum_gradients</span><span class="s3">,</span>
            <span class="s1">sum_hessians=sum_hessians</span><span class="s3">,</span>
            <span class="s1">value=</span><span class="s4">0</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s1">self.root.partition_start = </span><span class="s4">0</span>
        <span class="s1">self.root.partition_stop = n_samples</span>

        <span class="s3">if </span><span class="s1">self.root.n_samples &lt; </span><span class="s4">2 </span><span class="s1">* self.min_samples_leaf:</span>
            <span class="s2"># Do not even bother computing any splitting statistics.</span>
            <span class="s1">self._finalize_leaf(self.root)</span>
            <span class="s3">return</span>
        <span class="s3">if </span><span class="s1">sum_hessians &lt; self.splitter.min_hessian_to_split:</span>
            <span class="s1">self._finalize_leaf(self.root)</span>
            <span class="s3">return</span>

        <span class="s3">if </span><span class="s1">self.interaction_cst </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">self.root.interaction_cst_indices = range(len(self.interaction_cst))</span>
            <span class="s1">allowed_features = set().union(*self.interaction_cst)</span>
            <span class="s1">self.root.allowed_features = np.fromiter(</span>
                <span class="s1">allowed_features</span><span class="s3">, </span><span class="s1">dtype=np.uint32</span><span class="s3">, </span><span class="s1">count=len(allowed_features)</span>
            <span class="s1">)</span>

        <span class="s1">tic = time()</span>
        <span class="s1">self.root.histograms = self.histogram_builder.compute_histograms_brute(</span>
            <span class="s1">self.root.sample_indices</span><span class="s3">, </span><span class="s1">self.root.allowed_features</span>
        <span class="s1">)</span>
        <span class="s1">self.total_compute_hist_time += time() - tic</span>

        <span class="s1">tic = time()</span>
        <span class="s1">self._compute_best_split_and_push(self.root)</span>
        <span class="s1">self.total_find_split_time += time() - tic</span>

    <span class="s3">def </span><span class="s1">_compute_best_split_and_push(self</span><span class="s3">, </span><span class="s1">node):</span>
        <span class="s0">&quot;&quot;&quot;Compute the best possible split (SplitInfo) of a given node. 
 
        Also push it in the heap of splittable nodes if gain isn't zero. 
        The gain of a node is 0 if either all the leaves are pure 
        (best gain = 0), or if no split would satisfy the constraints, 
        (min_hessians_to_split, min_gain_to_split, min_samples_leaf) 
        &quot;&quot;&quot;</span>

        <span class="s1">node.split_info = self.splitter.find_node_split(</span>
            <span class="s1">n_samples=node.n_samples</span><span class="s3">,</span>
            <span class="s1">histograms=node.histograms</span><span class="s3">,</span>
            <span class="s1">sum_gradients=node.sum_gradients</span><span class="s3">,</span>
            <span class="s1">sum_hessians=node.sum_hessians</span><span class="s3">,</span>
            <span class="s1">value=node.value</span><span class="s3">,</span>
            <span class="s1">lower_bound=node.children_lower_bound</span><span class="s3">,</span>
            <span class="s1">upper_bound=node.children_upper_bound</span><span class="s3">,</span>
            <span class="s1">allowed_features=node.allowed_features</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">node.split_info.gain &lt;= </span><span class="s4">0</span><span class="s1">:  </span><span class="s2"># no valid split</span>
            <span class="s1">self._finalize_leaf(node)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">heappush(self.splittable_nodes</span><span class="s3">, </span><span class="s1">node)</span>

    <span class="s3">def </span><span class="s1">split_next(self):</span>
        <span class="s0">&quot;&quot;&quot;Split the node with highest potential gain. 
 
        Returns 
        ------- 
        left : TreeNode 
            The resulting left child. 
        right : TreeNode 
            The resulting right child. 
        &quot;&quot;&quot;</span>
        <span class="s2"># Consider the node with the highest loss reduction (a.k.a. gain)</span>
        <span class="s1">node = heappop(self.splittable_nodes)</span>

        <span class="s1">tic = time()</span>
        <span class="s1">(</span>
            <span class="s1">sample_indices_left</span><span class="s3">,</span>
            <span class="s1">sample_indices_right</span><span class="s3">,</span>
            <span class="s1">right_child_pos</span><span class="s3">,</span>
        <span class="s1">) = self.splitter.split_indices(node.split_info</span><span class="s3">, </span><span class="s1">node.sample_indices)</span>
        <span class="s1">self.total_apply_split_time += time() - tic</span>

        <span class="s1">depth = node.depth + </span><span class="s4">1</span>
        <span class="s1">n_leaf_nodes = len(self.finalized_leaves) + len(self.splittable_nodes)</span>
        <span class="s1">n_leaf_nodes += </span><span class="s4">2</span>

        <span class="s1">left_child_node = TreeNode(</span>
            <span class="s1">depth</span><span class="s3">,</span>
            <span class="s1">sample_indices_left</span><span class="s3">,</span>
            <span class="s1">node.split_info.sum_gradient_left</span><span class="s3">,</span>
            <span class="s1">node.split_info.sum_hessian_left</span><span class="s3">,</span>
            <span class="s1">value=node.split_info.value_left</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">right_child_node = TreeNode(</span>
            <span class="s1">depth</span><span class="s3">,</span>
            <span class="s1">sample_indices_right</span><span class="s3">,</span>
            <span class="s1">node.split_info.sum_gradient_right</span><span class="s3">,</span>
            <span class="s1">node.split_info.sum_hessian_right</span><span class="s3">,</span>
            <span class="s1">value=node.split_info.value_right</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s1">node.right_child = right_child_node</span>
        <span class="s1">node.left_child = left_child_node</span>

        <span class="s2"># set start and stop indices</span>
        <span class="s1">left_child_node.partition_start = node.partition_start</span>
        <span class="s1">left_child_node.partition_stop = node.partition_start + right_child_pos</span>
        <span class="s1">right_child_node.partition_start = left_child_node.partition_stop</span>
        <span class="s1">right_child_node.partition_stop = node.partition_stop</span>

        <span class="s2"># set interaction constraints (the indices of the constraints sets)</span>
        <span class="s3">if </span><span class="s1">self.interaction_cst </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s2"># Calculate allowed_features and interaction_cst_indices only once. Child</span>
            <span class="s2"># nodes inherit them before they get split.</span>
            <span class="s1">(</span>
                <span class="s1">left_child_node.allowed_features</span><span class="s3">,</span>
                <span class="s1">left_child_node.interaction_cst_indices</span><span class="s3">,</span>
            <span class="s1">) = self._compute_interactions(node)</span>
            <span class="s1">right_child_node.interaction_cst_indices = (</span>
                <span class="s1">left_child_node.interaction_cst_indices</span>
            <span class="s1">)</span>
            <span class="s1">right_child_node.allowed_features = left_child_node.allowed_features</span>

        <span class="s3">if not </span><span class="s1">self.has_missing_values[node.split_info.feature_idx]:</span>
            <span class="s2"># If no missing values are encountered at fit time, then samples</span>
            <span class="s2"># with missing values during predict() will go to whichever child</span>
            <span class="s2"># has the most samples.</span>
            <span class="s1">node.split_info.missing_go_to_left = (</span>
                <span class="s1">left_child_node.n_samples &gt; right_child_node.n_samples</span>
            <span class="s1">)</span>

        <span class="s1">self.n_nodes += </span><span class="s4">2</span>
        <span class="s1">self.n_categorical_splits += node.split_info.is_categorical</span>

        <span class="s3">if </span><span class="s1">self.max_leaf_nodes </span><span class="s3">is not None and </span><span class="s1">n_leaf_nodes == self.max_leaf_nodes:</span>
            <span class="s1">self._finalize_leaf(left_child_node)</span>
            <span class="s1">self._finalize_leaf(right_child_node)</span>
            <span class="s1">self._finalize_splittable_nodes()</span>
            <span class="s3">return </span><span class="s1">left_child_node</span><span class="s3">, </span><span class="s1">right_child_node</span>

        <span class="s3">if </span><span class="s1">self.max_depth </span><span class="s3">is not None and </span><span class="s1">depth == self.max_depth:</span>
            <span class="s1">self._finalize_leaf(left_child_node)</span>
            <span class="s1">self._finalize_leaf(right_child_node)</span>
            <span class="s3">return </span><span class="s1">left_child_node</span><span class="s3">, </span><span class="s1">right_child_node</span>

        <span class="s3">if </span><span class="s1">left_child_node.n_samples &lt; self.min_samples_leaf * </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s1">self._finalize_leaf(left_child_node)</span>
        <span class="s3">if </span><span class="s1">right_child_node.n_samples &lt; self.min_samples_leaf * </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s1">self._finalize_leaf(right_child_node)</span>

        <span class="s3">if </span><span class="s1">self.with_monotonic_cst:</span>
            <span class="s2"># Set value bounds for respecting monotonic constraints</span>
            <span class="s2"># See test_nodes_values() for details</span>
            <span class="s3">if </span><span class="s1">(</span>
                <span class="s1">self.monotonic_cst[node.split_info.feature_idx]</span>
                <span class="s1">== MonotonicConstraint.NO_CST</span>
            <span class="s1">):</span>
                <span class="s1">lower_left = lower_right = node.children_lower_bound</span>
                <span class="s1">upper_left = upper_right = node.children_upper_bound</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">mid = (left_child_node.value + right_child_node.value) / </span><span class="s4">2</span>
                <span class="s3">if </span><span class="s1">(</span>
                    <span class="s1">self.monotonic_cst[node.split_info.feature_idx]</span>
                    <span class="s1">== MonotonicConstraint.POS</span>
                <span class="s1">):</span>
                    <span class="s1">lower_left</span><span class="s3">, </span><span class="s1">upper_left = node.children_lower_bound</span><span class="s3">, </span><span class="s1">mid</span>
                    <span class="s1">lower_right</span><span class="s3">, </span><span class="s1">upper_right = mid</span><span class="s3">, </span><span class="s1">node.children_upper_bound</span>
                <span class="s3">else</span><span class="s1">:  </span><span class="s2"># NEG</span>
                    <span class="s1">lower_left</span><span class="s3">, </span><span class="s1">upper_left = mid</span><span class="s3">, </span><span class="s1">node.children_upper_bound</span>
                    <span class="s1">lower_right</span><span class="s3">, </span><span class="s1">upper_right = node.children_lower_bound</span><span class="s3">, </span><span class="s1">mid</span>
            <span class="s1">left_child_node.set_children_bounds(lower_left</span><span class="s3">, </span><span class="s1">upper_left)</span>
            <span class="s1">right_child_node.set_children_bounds(lower_right</span><span class="s3">, </span><span class="s1">upper_right)</span>

        <span class="s2"># Compute histograms of children, and compute their best possible split</span>
        <span class="s2"># (if needed)</span>
        <span class="s1">should_split_left = </span><span class="s3">not </span><span class="s1">left_child_node.is_leaf</span>
        <span class="s1">should_split_right = </span><span class="s3">not </span><span class="s1">right_child_node.is_leaf</span>
        <span class="s3">if </span><span class="s1">should_split_left </span><span class="s3">or </span><span class="s1">should_split_right:</span>
            <span class="s2"># We will compute the histograms of both nodes even if one of them</span>
            <span class="s2"># is a leaf, since computing the second histogram is very cheap</span>
            <span class="s2"># (using histogram subtraction).</span>
            <span class="s1">n_samples_left = left_child_node.sample_indices.shape[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">n_samples_right = right_child_node.sample_indices.shape[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s3">if </span><span class="s1">n_samples_left &lt; n_samples_right:</span>
                <span class="s1">smallest_child = left_child_node</span>
                <span class="s1">largest_child = right_child_node</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">smallest_child = right_child_node</span>
                <span class="s1">largest_child = left_child_node</span>

            <span class="s2"># We use the brute O(n_samples) method on the child that has the</span>
            <span class="s2"># smallest number of samples, and the subtraction trick O(n_bins)</span>
            <span class="s2"># on the other one.</span>
            <span class="s2"># Note that both left and right child have the same allowed_features.</span>
            <span class="s1">tic = time()</span>
            <span class="s1">smallest_child.histograms = self.histogram_builder.compute_histograms_brute(</span>
                <span class="s1">smallest_child.sample_indices</span><span class="s3">, </span><span class="s1">smallest_child.allowed_features</span>
            <span class="s1">)</span>
            <span class="s1">largest_child.histograms = (</span>
                <span class="s1">self.histogram_builder.compute_histograms_subtraction(</span>
                    <span class="s1">node.histograms</span><span class="s3">,</span>
                    <span class="s1">smallest_child.histograms</span><span class="s3">,</span>
                    <span class="s1">smallest_child.allowed_features</span><span class="s3">,</span>
                <span class="s1">)</span>
            <span class="s1">)</span>
            <span class="s1">self.total_compute_hist_time += time() - tic</span>

            <span class="s1">tic = time()</span>
            <span class="s3">if </span><span class="s1">should_split_left:</span>
                <span class="s1">self._compute_best_split_and_push(left_child_node)</span>
            <span class="s3">if </span><span class="s1">should_split_right:</span>
                <span class="s1">self._compute_best_split_and_push(right_child_node)</span>
            <span class="s1">self.total_find_split_time += time() - tic</span>

            <span class="s2"># Release memory used by histograms as they are no longer needed</span>
            <span class="s2"># for leaf nodes since they won't be split.</span>
            <span class="s3">for </span><span class="s1">child </span><span class="s3">in </span><span class="s1">(left_child_node</span><span class="s3">, </span><span class="s1">right_child_node):</span>
                <span class="s3">if </span><span class="s1">child.is_leaf:</span>
                    <span class="s3">del </span><span class="s1">child.histograms</span>

        <span class="s2"># Release memory used by histograms as they are no longer needed for</span>
        <span class="s2"># internal nodes once children histograms have been computed.</span>
        <span class="s3">del </span><span class="s1">node.histograms</span>

        <span class="s3">return </span><span class="s1">left_child_node</span><span class="s3">, </span><span class="s1">right_child_node</span>

    <span class="s3">def </span><span class="s1">_compute_interactions(self</span><span class="s3">, </span><span class="s1">node):</span>
        <span class="s0">r&quot;&quot;&quot;Compute features allowed by interactions to be inherited by child nodes. 
 
        Example: Assume constraints [{0, 1}, {1, 2}]. 
           1      &lt;- Both constraint groups could be applied from now on 
          / \ 
         1   2    &lt;- Left split still fulfills both constraint groups. 
        / \ / \      Right split at feature 2 has only group {1, 2} from now on. 
 
        LightGBM uses the same logic for overlapping groups. See 
        https://github.com/microsoft/LightGBM/issues/4481 for details. 
 
        Parameters: 
        ---------- 
        node : TreeNode 
            A node that might have children. Based on its feature_idx, the interaction 
            constraints for possible child nodes are computed. 
 
        Returns 
        ------- 
        allowed_features : ndarray, dtype=uint32 
            Indices of features allowed to split for children. 
        interaction_cst_indices : list of ints 
            Indices of the interaction sets that have to be applied on splits of 
            child nodes. The fewer sets the stronger the constraint as fewer sets 
            contain fewer features. 
        &quot;&quot;&quot;</span>
        <span class="s2"># Note:</span>
        <span class="s2">#  - Case of no interactions is already captured before function call.</span>
        <span class="s2">#  - This is for nodes that are already split and have a</span>
        <span class="s2">#    node.split_info.feature_idx.</span>
        <span class="s1">allowed_features = set()</span>
        <span class="s1">interaction_cst_indices = []</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">node.interaction_cst_indices:</span>
            <span class="s3">if </span><span class="s1">node.split_info.feature_idx </span><span class="s3">in </span><span class="s1">self.interaction_cst[i]:</span>
                <span class="s1">interaction_cst_indices.append(i)</span>
                <span class="s1">allowed_features.update(self.interaction_cst[i])</span>
        <span class="s3">return </span><span class="s1">(</span>
            <span class="s1">np.fromiter(allowed_features</span><span class="s3">, </span><span class="s1">dtype=np.uint32</span><span class="s3">, </span><span class="s1">count=len(allowed_features))</span><span class="s3">,</span>
            <span class="s1">interaction_cst_indices</span><span class="s3">,</span>
        <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_finalize_leaf(self</span><span class="s3">, </span><span class="s1">node):</span>
        <span class="s0">&quot;&quot;&quot;Make node a leaf of the tree being grown.&quot;&quot;&quot;</span>

        <span class="s1">node.is_leaf = </span><span class="s3">True</span>
        <span class="s1">self.finalized_leaves.append(node)</span>

    <span class="s3">def </span><span class="s1">_finalize_splittable_nodes(self):</span>
        <span class="s0">&quot;&quot;&quot;Transform all splittable nodes into leaves. 
 
        Used when some constraint is met e.g. maximum number of leaves or 
        maximum depth.&quot;&quot;&quot;</span>
        <span class="s3">while </span><span class="s1">len(self.splittable_nodes) &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">node = self.splittable_nodes.pop()</span>
            <span class="s1">self._finalize_leaf(node)</span>

    <span class="s3">def </span><span class="s1">make_predictor(self</span><span class="s3">, </span><span class="s1">binning_thresholds):</span>
        <span class="s0">&quot;&quot;&quot;Make a TreePredictor object out of the current tree. 
 
        Parameters 
        ---------- 
        binning_thresholds : array-like of floats 
            Corresponds to the bin_thresholds_ attribute of the BinMapper. 
            For each feature, this stores: 
 
            - the bin frontiers for continuous features 
            - the unique raw category values for categorical features 
 
        Returns 
        ------- 
        A TreePredictor object. 
        &quot;&quot;&quot;</span>
        <span class="s1">predictor_nodes = np.zeros(self.n_nodes</span><span class="s3">, </span><span class="s1">dtype=PREDICTOR_RECORD_DTYPE)</span>
        <span class="s1">binned_left_cat_bitsets = np.zeros(</span>
            <span class="s1">(self.n_categorical_splits</span><span class="s3">, </span><span class="s4">8</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=X_BITSET_INNER_DTYPE</span>
        <span class="s1">)</span>
        <span class="s1">raw_left_cat_bitsets = np.zeros(</span>
            <span class="s1">(self.n_categorical_splits</span><span class="s3">, </span><span class="s4">8</span><span class="s1">)</span><span class="s3">, </span><span class="s1">dtype=X_BITSET_INNER_DTYPE</span>
        <span class="s1">)</span>
        <span class="s1">_fill_predictor_arrays(</span>
            <span class="s1">predictor_nodes</span><span class="s3">,</span>
            <span class="s1">binned_left_cat_bitsets</span><span class="s3">,</span>
            <span class="s1">raw_left_cat_bitsets</span><span class="s3">,</span>
            <span class="s1">self.root</span><span class="s3">,</span>
            <span class="s1">binning_thresholds</span><span class="s3">,</span>
            <span class="s1">self.n_bins_non_missing</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">TreePredictor(</span>
            <span class="s1">predictor_nodes</span><span class="s3">, </span><span class="s1">binned_left_cat_bitsets</span><span class="s3">, </span><span class="s1">raw_left_cat_bitsets</span>
        <span class="s1">)</span>


<span class="s3">def </span><span class="s1">_fill_predictor_arrays(</span>
    <span class="s1">predictor_nodes</span><span class="s3">,</span>
    <span class="s1">binned_left_cat_bitsets</span><span class="s3">,</span>
    <span class="s1">raw_left_cat_bitsets</span><span class="s3">,</span>
    <span class="s1">grower_node</span><span class="s3">,</span>
    <span class="s1">binning_thresholds</span><span class="s3">,</span>
    <span class="s1">n_bins_non_missing</span><span class="s3">,</span>
    <span class="s1">next_free_node_idx=</span><span class="s4">0</span><span class="s3">,</span>
    <span class="s1">next_free_bitset_idx=</span><span class="s4">0</span><span class="s3">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Helper used in make_predictor to set the TreePredictor fields.&quot;&quot;&quot;</span>
    <span class="s1">node = predictor_nodes[next_free_node_idx]</span>
    <span class="s1">node[</span><span class="s5">&quot;count&quot;</span><span class="s1">] = grower_node.n_samples</span>
    <span class="s1">node[</span><span class="s5">&quot;depth&quot;</span><span class="s1">] = grower_node.depth</span>
    <span class="s3">if </span><span class="s1">grower_node.split_info </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">node[</span><span class="s5">&quot;gain&quot;</span><span class="s1">] = grower_node.split_info.gain</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">node[</span><span class="s5">&quot;gain&quot;</span><span class="s1">] = -</span><span class="s4">1</span>

    <span class="s1">node[</span><span class="s5">&quot;value&quot;</span><span class="s1">] = grower_node.value</span>

    <span class="s3">if </span><span class="s1">grower_node.is_leaf:</span>
        <span class="s2"># Leaf node</span>
        <span class="s1">node[</span><span class="s5">&quot;is_leaf&quot;</span><span class="s1">] = </span><span class="s3">True</span>
        <span class="s3">return </span><span class="s1">next_free_node_idx + </span><span class="s4">1</span><span class="s3">, </span><span class="s1">next_free_bitset_idx</span>

    <span class="s1">split_info = grower_node.split_info</span>
    <span class="s1">feature_idx</span><span class="s3">, </span><span class="s1">bin_idx = split_info.feature_idx</span><span class="s3">, </span><span class="s1">split_info.bin_idx</span>
    <span class="s1">node[</span><span class="s5">&quot;feature_idx&quot;</span><span class="s1">] = feature_idx</span>
    <span class="s1">node[</span><span class="s5">&quot;bin_threshold&quot;</span><span class="s1">] = bin_idx</span>
    <span class="s1">node[</span><span class="s5">&quot;missing_go_to_left&quot;</span><span class="s1">] = split_info.missing_go_to_left</span>
    <span class="s1">node[</span><span class="s5">&quot;is_categorical&quot;</span><span class="s1">] = split_info.is_categorical</span>

    <span class="s3">if </span><span class="s1">split_info.bin_idx == n_bins_non_missing[feature_idx] - </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s2"># Split is on the last non-missing bin: it's a &quot;split on nans&quot;.</span>
        <span class="s2"># All nans go to the right, the rest go to the left.</span>
        <span class="s2"># Note: for categorical splits, bin_idx is 0 and we rely on the bitset</span>
        <span class="s1">node[</span><span class="s5">&quot;num_threshold&quot;</span><span class="s1">] = np.inf</span>
    <span class="s3">elif </span><span class="s1">split_info.is_categorical:</span>
        <span class="s1">categories = binning_thresholds[feature_idx]</span>
        <span class="s1">node[</span><span class="s5">&quot;bitset_idx&quot;</span><span class="s1">] = next_free_bitset_idx</span>
        <span class="s1">binned_left_cat_bitsets[next_free_bitset_idx] = split_info.left_cat_bitset</span>
        <span class="s1">set_raw_bitset_from_binned_bitset(</span>
            <span class="s1">raw_left_cat_bitsets[next_free_bitset_idx]</span><span class="s3">,</span>
            <span class="s1">split_info.left_cat_bitset</span><span class="s3">,</span>
            <span class="s1">categories</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">next_free_bitset_idx += </span><span class="s4">1</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">node[</span><span class="s5">&quot;num_threshold&quot;</span><span class="s1">] = binning_thresholds[feature_idx][bin_idx]</span>

    <span class="s1">next_free_node_idx += </span><span class="s4">1</span>

    <span class="s1">node[</span><span class="s5">&quot;left&quot;</span><span class="s1">] = next_free_node_idx</span>
    <span class="s1">next_free_node_idx</span><span class="s3">, </span><span class="s1">next_free_bitset_idx = _fill_predictor_arrays(</span>
        <span class="s1">predictor_nodes</span><span class="s3">,</span>
        <span class="s1">binned_left_cat_bitsets</span><span class="s3">,</span>
        <span class="s1">raw_left_cat_bitsets</span><span class="s3">,</span>
        <span class="s1">grower_node.left_child</span><span class="s3">,</span>
        <span class="s1">binning_thresholds=binning_thresholds</span><span class="s3">,</span>
        <span class="s1">n_bins_non_missing=n_bins_non_missing</span><span class="s3">,</span>
        <span class="s1">next_free_node_idx=next_free_node_idx</span><span class="s3">,</span>
        <span class="s1">next_free_bitset_idx=next_free_bitset_idx</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s1">node[</span><span class="s5">&quot;right&quot;</span><span class="s1">] = next_free_node_idx</span>
    <span class="s3">return </span><span class="s1">_fill_predictor_arrays(</span>
        <span class="s1">predictor_nodes</span><span class="s3">,</span>
        <span class="s1">binned_left_cat_bitsets</span><span class="s3">,</span>
        <span class="s1">raw_left_cat_bitsets</span><span class="s3">,</span>
        <span class="s1">grower_node.right_child</span><span class="s3">,</span>
        <span class="s1">binning_thresholds=binning_thresholds</span><span class="s3">,</span>
        <span class="s1">n_bins_non_missing=n_bins_non_missing</span><span class="s3">,</span>
        <span class="s1">next_free_node_idx=next_free_node_idx</span><span class="s3">,</span>
        <span class="s1">next_free_bitset_idx=next_free_bitset_idx</span><span class="s3">,</span>
    <span class="s1">)</span>
</pre>
</body>
</html>