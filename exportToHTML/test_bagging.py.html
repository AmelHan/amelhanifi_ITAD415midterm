<html>
<head>
<title>test_bagging.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_bagging.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Testing for the bagging ensemble module (sklearn.ensemble.bagging). 
&quot;&quot;&quot;</span>

<span class="s2"># Author: Gilles Louppe</span>
<span class="s2"># License: BSD 3 clause</span>
<span class="s3">from </span><span class="s1">itertools </span><span class="s3">import </span><span class="s1">cycle</span><span class="s3">, </span><span class="s1">product</span>

<span class="s3">import </span><span class="s1">joblib</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">pytest</span>
<span class="s3">from </span><span class="s1">scipy.sparse </span><span class="s3">import </span><span class="s1">csc_matrix</span><span class="s3">, </span><span class="s1">csr_matrix</span>

<span class="s3">from </span><span class="s1">sklearn.base </span><span class="s3">import </span><span class="s1">BaseEstimator</span>
<span class="s3">from </span><span class="s1">sklearn.datasets </span><span class="s3">import </span><span class="s1">load_diabetes</span><span class="s3">, </span><span class="s1">load_iris</span><span class="s3">, </span><span class="s1">make_hastie_10_2</span>
<span class="s3">from </span><span class="s1">sklearn.dummy </span><span class="s3">import </span><span class="s1">DummyClassifier</span><span class="s3">, </span><span class="s1">DummyRegressor</span>
<span class="s3">from </span><span class="s1">sklearn.ensemble </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">BaggingClassifier</span><span class="s3">,</span>
    <span class="s1">BaggingRegressor</span><span class="s3">,</span>
    <span class="s1">HistGradientBoostingClassifier</span><span class="s3">,</span>
    <span class="s1">HistGradientBoostingRegressor</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">sklearn.feature_selection </span><span class="s3">import </span><span class="s1">SelectKBest</span>
<span class="s3">from </span><span class="s1">sklearn.linear_model </span><span class="s3">import </span><span class="s1">LogisticRegression</span><span class="s3">, </span><span class="s1">Perceptron</span>
<span class="s3">from </span><span class="s1">sklearn.model_selection </span><span class="s3">import </span><span class="s1">GridSearchCV</span><span class="s3">, </span><span class="s1">ParameterGrid</span><span class="s3">, </span><span class="s1">train_test_split</span>
<span class="s3">from </span><span class="s1">sklearn.neighbors </span><span class="s3">import </span><span class="s1">KNeighborsClassifier</span><span class="s3">, </span><span class="s1">KNeighborsRegressor</span>
<span class="s3">from </span><span class="s1">sklearn.pipeline </span><span class="s3">import </span><span class="s1">make_pipeline</span>
<span class="s3">from </span><span class="s1">sklearn.preprocessing </span><span class="s3">import </span><span class="s1">FunctionTransformer</span><span class="s3">, </span><span class="s1">scale</span>
<span class="s3">from </span><span class="s1">sklearn.random_projection </span><span class="s3">import </span><span class="s1">SparseRandomProjection</span>
<span class="s3">from </span><span class="s1">sklearn.svm </span><span class="s3">import </span><span class="s1">SVC</span><span class="s3">, </span><span class="s1">SVR</span>
<span class="s3">from </span><span class="s1">sklearn.tree </span><span class="s3">import </span><span class="s1">DecisionTreeClassifier</span><span class="s3">, </span><span class="s1">DecisionTreeRegressor</span>
<span class="s3">from </span><span class="s1">sklearn.utils </span><span class="s3">import </span><span class="s1">check_random_state</span>
<span class="s3">from </span><span class="s1">sklearn.utils._testing </span><span class="s3">import </span><span class="s1">assert_array_almost_equal</span><span class="s3">, </span><span class="s1">assert_array_equal</span>

<span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>

<span class="s2"># also load the iris dataset</span>
<span class="s2"># and randomly permute it</span>
<span class="s1">iris = load_iris()</span>
<span class="s1">perm = rng.permutation(iris.target.size)</span>
<span class="s1">iris.data = iris.data[perm]</span>
<span class="s1">iris.target = iris.target[perm]</span>

<span class="s2"># also load the diabetes dataset</span>
<span class="s2"># and randomly permute it</span>
<span class="s1">diabetes = load_diabetes()</span>
<span class="s1">perm = rng.permutation(diabetes.target.size)</span>
<span class="s1">diabetes.data = diabetes.data[perm]</span>
<span class="s1">diabetes.target = diabetes.target[perm]</span>


<span class="s3">def </span><span class="s1">test_classification():</span>
    <span class="s2"># Check classification for various parameter settings.</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">iris.data</span><span class="s3">, </span><span class="s1">iris.target</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">grid = ParameterGrid(</span>
        <span class="s1">{</span>
            <span class="s5">&quot;max_samples&quot;</span><span class="s1">: [</span><span class="s4">0.5</span><span class="s3">, </span><span class="s4">1.0</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s5">&quot;max_features&quot;</span><span class="s1">: [</span><span class="s4">1</span><span class="s3">, </span><span class="s4">4</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s5">&quot;bootstrap&quot;</span><span class="s1">: [</span><span class="s3">True, False</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s5">&quot;bootstrap_features&quot;</span><span class="s1">: [</span><span class="s3">True, False</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">}</span>
    <span class="s1">)</span>
    <span class="s1">estimators = [</span>
        <span class="s3">None,</span>
        <span class="s1">DummyClassifier()</span><span class="s3">,</span>
        <span class="s1">Perceptron(max_iter=</span><span class="s4">20</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">DecisionTreeClassifier(max_depth=</span><span class="s4">2</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">KNeighborsClassifier()</span><span class="s3">,</span>
        <span class="s1">SVC()</span><span class="s3">,</span>
    <span class="s1">]</span>
    <span class="s2"># Try different parameter settings with different base classifiers without</span>
    <span class="s2"># doing the full cartesian product to keep the test durations low.</span>
    <span class="s3">for </span><span class="s1">params</span><span class="s3">, </span><span class="s1">estimator </span><span class="s3">in </span><span class="s1">zip(grid</span><span class="s3">, </span><span class="s1">cycle(estimators)):</span>
        <span class="s1">BaggingClassifier(</span>
            <span class="s1">estimator=estimator</span><span class="s3">,</span>
            <span class="s1">random_state=rng</span><span class="s3">,</span>
            <span class="s1">n_estimators=</span><span class="s4">2</span><span class="s3">,</span>
            <span class="s1">**params</span><span class="s3">,</span>
        <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train).predict(X_test)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;sparse_format, params, method&quot;</span><span class="s3">,</span>
    <span class="s1">product(</span>
        <span class="s1">[csc_matrix</span><span class="s3">, </span><span class="s1">csr_matrix]</span><span class="s3">,</span>
        <span class="s1">[</span>
            <span class="s1">{</span>
                <span class="s5">&quot;max_samples&quot;</span><span class="s1">: </span><span class="s4">0.5</span><span class="s3">,</span>
                <span class="s5">&quot;max_features&quot;</span><span class="s1">: </span><span class="s4">2</span><span class="s3">,</span>
                <span class="s5">&quot;bootstrap&quot;</span><span class="s1">: </span><span class="s3">True,</span>
                <span class="s5">&quot;bootstrap_features&quot;</span><span class="s1">: </span><span class="s3">True,</span>
            <span class="s1">}</span><span class="s3">,</span>
            <span class="s1">{</span>
                <span class="s5">&quot;max_samples&quot;</span><span class="s1">: </span><span class="s4">1.0</span><span class="s3">,</span>
                <span class="s5">&quot;max_features&quot;</span><span class="s1">: </span><span class="s4">4</span><span class="s3">,</span>
                <span class="s5">&quot;bootstrap&quot;</span><span class="s1">: </span><span class="s3">True,</span>
                <span class="s5">&quot;bootstrap_features&quot;</span><span class="s1">: </span><span class="s3">True,</span>
            <span class="s1">}</span><span class="s3">,</span>
            <span class="s1">{</span><span class="s5">&quot;max_features&quot;</span><span class="s1">: </span><span class="s4">2</span><span class="s3">, </span><span class="s5">&quot;bootstrap&quot;</span><span class="s1">: </span><span class="s3">False, </span><span class="s5">&quot;bootstrap_features&quot;</span><span class="s1">: </span><span class="s3">True</span><span class="s1">}</span><span class="s3">,</span>
            <span class="s1">{</span><span class="s5">&quot;max_samples&quot;</span><span class="s1">: </span><span class="s4">0.5</span><span class="s3">, </span><span class="s5">&quot;bootstrap&quot;</span><span class="s1">: </span><span class="s3">True, </span><span class="s5">&quot;bootstrap_features&quot;</span><span class="s1">: </span><span class="s3">False</span><span class="s1">}</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s5">&quot;predict&quot;</span><span class="s3">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s3">, </span><span class="s5">&quot;predict_log_proba&quot;</span><span class="s3">, </span><span class="s5">&quot;decision_function&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">)</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">test_sparse_classification(sparse_format</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">method):</span>
    <span class="s2"># Check classification for various parameter settings on sparse input.</span>

    <span class="s3">class </span><span class="s1">CustomSVC(SVC):</span>
        <span class="s0">&quot;&quot;&quot;SVC variant that records the nature of the training set&quot;&quot;&quot;</span>

        <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
            <span class="s1">super().fit(X</span><span class="s3">, </span><span class="s1">y)</span>
            <span class="s1">self.data_type_ = type(X)</span>
            <span class="s3">return </span><span class="s1">self</span>

    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">scale(iris.data)</span><span class="s3">, </span><span class="s1">iris.target</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>

    <span class="s1">X_train_sparse = sparse_format(X_train)</span>
    <span class="s1">X_test_sparse = sparse_format(X_test)</span>
    <span class="s2"># Trained on sparse format</span>
    <span class="s1">sparse_classifier = BaggingClassifier(</span>
        <span class="s1">estimator=CustomSVC(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s3">, </span><span class="s1">decision_function_shape=</span><span class="s5">&quot;ovr&quot;</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s4">1</span><span class="s3">,</span>
        <span class="s1">**params</span><span class="s3">,</span>
    <span class="s1">).fit(X_train_sparse</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s1">sparse_results = getattr(sparse_classifier</span><span class="s3">, </span><span class="s1">method)(X_test_sparse)</span>

    <span class="s2"># Trained on dense format</span>
    <span class="s1">dense_classifier = BaggingClassifier(</span>
        <span class="s1">estimator=CustomSVC(kernel=</span><span class="s5">&quot;linear&quot;</span><span class="s3">, </span><span class="s1">decision_function_shape=</span><span class="s5">&quot;ovr&quot;</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s4">1</span><span class="s3">,</span>
        <span class="s1">**params</span><span class="s3">,</span>
    <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s1">dense_results = getattr(dense_classifier</span><span class="s3">, </span><span class="s1">method)(X_test)</span>
    <span class="s1">assert_array_almost_equal(sparse_results</span><span class="s3">, </span><span class="s1">dense_results)</span>

    <span class="s1">sparse_type = type(X_train_sparse)</span>
    <span class="s1">types = [i.data_type_ </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">sparse_classifier.estimators_]</span>

    <span class="s3">assert </span><span class="s1">all([t == sparse_type </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">types])</span>


<span class="s3">def </span><span class="s1">test_regression():</span>
    <span class="s2"># Check regression for various parameter settings.</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">diabetes.data[:</span><span class="s4">50</span><span class="s1">]</span><span class="s3">, </span><span class="s1">diabetes.target[:</span><span class="s4">50</span><span class="s1">]</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">grid = ParameterGrid(</span>
        <span class="s1">{</span>
            <span class="s5">&quot;max_samples&quot;</span><span class="s1">: [</span><span class="s4">0.5</span><span class="s3">, </span><span class="s4">1.0</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s5">&quot;max_features&quot;</span><span class="s1">: [</span><span class="s4">0.5</span><span class="s3">, </span><span class="s4">1.0</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s5">&quot;bootstrap&quot;</span><span class="s1">: [</span><span class="s3">True, False</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s5">&quot;bootstrap_features&quot;</span><span class="s1">: [</span><span class="s3">True, False</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">}</span>
    <span class="s1">)</span>

    <span class="s3">for </span><span class="s1">estimator </span><span class="s3">in </span><span class="s1">[</span>
        <span class="s3">None,</span>
        <span class="s1">DummyRegressor()</span><span class="s3">,</span>
        <span class="s1">DecisionTreeRegressor()</span><span class="s3">,</span>
        <span class="s1">KNeighborsRegressor()</span><span class="s3">,</span>
        <span class="s1">SVR()</span><span class="s3">,</span>
    <span class="s1">]:</span>
        <span class="s3">for </span><span class="s1">params </span><span class="s3">in </span><span class="s1">grid:</span>
            <span class="s1">BaggingRegressor(estimator=estimator</span><span class="s3">, </span><span class="s1">random_state=rng</span><span class="s3">, </span><span class="s1">**params).fit(</span>
                <span class="s1">X_train</span><span class="s3">, </span><span class="s1">y_train</span>
            <span class="s1">).predict(X_test)</span>


<span class="s3">def </span><span class="s1">test_sparse_regression():</span>
    <span class="s2"># Check regression for various parameter settings on sparse input.</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">diabetes.data[:</span><span class="s4">50</span><span class="s1">]</span><span class="s3">, </span><span class="s1">diabetes.target[:</span><span class="s4">50</span><span class="s1">]</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>

    <span class="s3">class </span><span class="s1">CustomSVR(SVR):</span>
        <span class="s0">&quot;&quot;&quot;SVC variant that records the nature of the training set&quot;&quot;&quot;</span>

        <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
            <span class="s1">super().fit(X</span><span class="s3">, </span><span class="s1">y)</span>
            <span class="s1">self.data_type_ = type(X)</span>
            <span class="s3">return </span><span class="s1">self</span>

    <span class="s1">parameter_sets = [</span>
        <span class="s1">{</span>
            <span class="s5">&quot;max_samples&quot;</span><span class="s1">: </span><span class="s4">0.5</span><span class="s3">,</span>
            <span class="s5">&quot;max_features&quot;</span><span class="s1">: </span><span class="s4">2</span><span class="s3">,</span>
            <span class="s5">&quot;bootstrap&quot;</span><span class="s1">: </span><span class="s3">True,</span>
            <span class="s5">&quot;bootstrap_features&quot;</span><span class="s1">: </span><span class="s3">True,</span>
        <span class="s1">}</span><span class="s3">,</span>
        <span class="s1">{</span>
            <span class="s5">&quot;max_samples&quot;</span><span class="s1">: </span><span class="s4">1.0</span><span class="s3">,</span>
            <span class="s5">&quot;max_features&quot;</span><span class="s1">: </span><span class="s4">4</span><span class="s3">,</span>
            <span class="s5">&quot;bootstrap&quot;</span><span class="s1">: </span><span class="s3">True,</span>
            <span class="s5">&quot;bootstrap_features&quot;</span><span class="s1">: </span><span class="s3">True,</span>
        <span class="s1">}</span><span class="s3">,</span>
        <span class="s1">{</span><span class="s5">&quot;max_features&quot;</span><span class="s1">: </span><span class="s4">2</span><span class="s3">, </span><span class="s5">&quot;bootstrap&quot;</span><span class="s1">: </span><span class="s3">False, </span><span class="s5">&quot;bootstrap_features&quot;</span><span class="s1">: </span><span class="s3">True</span><span class="s1">}</span><span class="s3">,</span>
        <span class="s1">{</span><span class="s5">&quot;max_samples&quot;</span><span class="s1">: </span><span class="s4">0.5</span><span class="s3">, </span><span class="s5">&quot;bootstrap&quot;</span><span class="s1">: </span><span class="s3">True, </span><span class="s5">&quot;bootstrap_features&quot;</span><span class="s1">: </span><span class="s3">False</span><span class="s1">}</span><span class="s3">,</span>
    <span class="s1">]</span>

    <span class="s3">for </span><span class="s1">sparse_format </span><span class="s3">in </span><span class="s1">[csc_matrix</span><span class="s3">, </span><span class="s1">csr_matrix]:</span>
        <span class="s1">X_train_sparse = sparse_format(X_train)</span>
        <span class="s1">X_test_sparse = sparse_format(X_test)</span>
        <span class="s3">for </span><span class="s1">params </span><span class="s3">in </span><span class="s1">parameter_sets:</span>
            <span class="s2"># Trained on sparse format</span>
            <span class="s1">sparse_classifier = BaggingRegressor(</span>
                <span class="s1">estimator=CustomSVR()</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">**params</span>
            <span class="s1">).fit(X_train_sparse</span><span class="s3">, </span><span class="s1">y_train)</span>
            <span class="s1">sparse_results = sparse_classifier.predict(X_test_sparse)</span>

            <span class="s2"># Trained on dense format</span>
            <span class="s1">dense_results = (</span>
                <span class="s1">BaggingRegressor(estimator=CustomSVR()</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">**params)</span>
                <span class="s1">.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
                <span class="s1">.predict(X_test)</span>
            <span class="s1">)</span>

            <span class="s1">sparse_type = type(X_train_sparse)</span>
            <span class="s1">types = [i.data_type_ </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">sparse_classifier.estimators_]</span>

            <span class="s1">assert_array_almost_equal(sparse_results</span><span class="s3">, </span><span class="s1">dense_results)</span>
            <span class="s3">assert </span><span class="s1">all([t == sparse_type </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">types])</span>
            <span class="s1">assert_array_almost_equal(sparse_results</span><span class="s3">, </span><span class="s1">dense_results)</span>


<span class="s3">class </span><span class="s1">DummySizeEstimator(BaseEstimator):</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s1">self.training_size_ = X.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">self.training_hash_ = joblib.hash(X)</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s3">return </span><span class="s1">np.ones(X.shape[</span><span class="s4">0</span><span class="s1">])</span>


<span class="s3">def </span><span class="s1">test_bootstrap_samples():</span>
    <span class="s2"># Test that bootstrapping samples generate non-perfect base estimators.</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">diabetes.data</span><span class="s3">, </span><span class="s1">diabetes.target</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>

    <span class="s1">estimator = DecisionTreeRegressor().fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s2"># without bootstrap, all trees are perfect on the training set</span>
    <span class="s1">ensemble = BaggingRegressor(</span>
        <span class="s1">estimator=DecisionTreeRegressor()</span><span class="s3">,</span>
        <span class="s1">max_samples=</span><span class="s4">1.0</span><span class="s3">,</span>
        <span class="s1">bootstrap=</span><span class="s3">False,</span>
        <span class="s1">random_state=rng</span><span class="s3">,</span>
    <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s3">assert </span><span class="s1">estimator.score(X_train</span><span class="s3">, </span><span class="s1">y_train) == ensemble.score(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s2"># with bootstrap, trees are no longer perfect on the training set</span>
    <span class="s1">ensemble = BaggingRegressor(</span>
        <span class="s1">estimator=DecisionTreeRegressor()</span><span class="s3">,</span>
        <span class="s1">max_samples=</span><span class="s4">1.0</span><span class="s3">,</span>
        <span class="s1">bootstrap=</span><span class="s3">True,</span>
        <span class="s1">random_state=rng</span><span class="s3">,</span>
    <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s3">assert </span><span class="s1">estimator.score(X_train</span><span class="s3">, </span><span class="s1">y_train) &gt; ensemble.score(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s2"># check that each sampling correspond to a complete bootstrap resample.</span>
    <span class="s2"># the size of each bootstrap should be the same as the input data but</span>
    <span class="s2"># the data should be different (checked using the hash of the data).</span>
    <span class="s1">ensemble = BaggingRegressor(estimator=DummySizeEstimator()</span><span class="s3">, </span><span class="s1">bootstrap=</span><span class="s3">True</span><span class="s1">).fit(</span>
        <span class="s1">X_train</span><span class="s3">, </span><span class="s1">y_train</span>
    <span class="s1">)</span>
    <span class="s1">training_hash = []</span>
    <span class="s3">for </span><span class="s1">estimator </span><span class="s3">in </span><span class="s1">ensemble.estimators_:</span>
        <span class="s3">assert </span><span class="s1">estimator.training_size_ == X_train.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">training_hash.append(estimator.training_hash_)</span>
    <span class="s3">assert </span><span class="s1">len(set(training_hash)) == len(training_hash)</span>


<span class="s3">def </span><span class="s1">test_bootstrap_features():</span>
    <span class="s2"># Test that bootstrapping features may generate duplicate features.</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">diabetes.data</span><span class="s3">, </span><span class="s1">diabetes.target</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>

    <span class="s1">ensemble = BaggingRegressor(</span>
        <span class="s1">estimator=DecisionTreeRegressor()</span><span class="s3">,</span>
        <span class="s1">max_features=</span><span class="s4">1.0</span><span class="s3">,</span>
        <span class="s1">bootstrap_features=</span><span class="s3">False,</span>
        <span class="s1">random_state=rng</span><span class="s3">,</span>
    <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s3">for </span><span class="s1">features </span><span class="s3">in </span><span class="s1">ensemble.estimators_features_:</span>
        <span class="s3">assert </span><span class="s1">diabetes.data.shape[</span><span class="s4">1</span><span class="s1">] == np.unique(features).shape[</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s1">ensemble = BaggingRegressor(</span>
        <span class="s1">estimator=DecisionTreeRegressor()</span><span class="s3">,</span>
        <span class="s1">max_features=</span><span class="s4">1.0</span><span class="s3">,</span>
        <span class="s1">bootstrap_features=</span><span class="s3">True,</span>
        <span class="s1">random_state=rng</span><span class="s3">,</span>
    <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s3">for </span><span class="s1">features </span><span class="s3">in </span><span class="s1">ensemble.estimators_features_:</span>
        <span class="s3">assert </span><span class="s1">diabetes.data.shape[</span><span class="s4">1</span><span class="s1">] &gt; np.unique(features).shape[</span><span class="s4">0</span><span class="s1">]</span>


<span class="s3">def </span><span class="s1">test_probability():</span>
    <span class="s2"># Predict probabilities.</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">iris.data</span><span class="s3">, </span><span class="s1">iris.target</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>

    <span class="s3">with </span><span class="s1">np.errstate(divide=</span><span class="s5">&quot;ignore&quot;</span><span class="s3">, </span><span class="s1">invalid=</span><span class="s5">&quot;ignore&quot;</span><span class="s1">):</span>
        <span class="s2"># Normal case</span>
        <span class="s1">ensemble = BaggingClassifier(</span>
            <span class="s1">estimator=DecisionTreeClassifier()</span><span class="s3">, </span><span class="s1">random_state=rng</span>
        <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">np.sum(ensemble.predict_proba(X_test)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.ones(len(X_test))</span>
        <span class="s1">)</span>

        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">ensemble.predict_proba(X_test)</span><span class="s3">, </span><span class="s1">np.exp(ensemble.predict_log_proba(X_test))</span>
        <span class="s1">)</span>

        <span class="s2"># Degenerate case, where some classes are missing</span>
        <span class="s1">ensemble = BaggingClassifier(</span>
            <span class="s1">estimator=LogisticRegression()</span><span class="s3">, </span><span class="s1">random_state=rng</span><span class="s3">, </span><span class="s1">max_samples=</span><span class="s4">5</span>
        <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">np.sum(ensemble.predict_proba(X_test)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.ones(len(X_test))</span>
        <span class="s1">)</span>

        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">ensemble.predict_proba(X_test)</span><span class="s3">, </span><span class="s1">np.exp(ensemble.predict_log_proba(X_test))</span>
        <span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_oob_score_classification():</span>
    <span class="s2"># Check that oob prediction is a good estimation of the generalization</span>
    <span class="s2"># error.</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">iris.data</span><span class="s3">, </span><span class="s1">iris.target</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>

    <span class="s3">for </span><span class="s1">estimator </span><span class="s3">in </span><span class="s1">[DecisionTreeClassifier()</span><span class="s3">, </span><span class="s1">SVC()]:</span>
        <span class="s1">clf = BaggingClassifier(</span>
            <span class="s1">estimator=estimator</span><span class="s3">,</span>
            <span class="s1">n_estimators=</span><span class="s4">100</span><span class="s3">,</span>
            <span class="s1">bootstrap=</span><span class="s3">True,</span>
            <span class="s1">oob_score=</span><span class="s3">True,</span>
            <span class="s1">random_state=rng</span><span class="s3">,</span>
        <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

        <span class="s1">test_score = clf.score(X_test</span><span class="s3">, </span><span class="s1">y_test)</span>

        <span class="s3">assert </span><span class="s1">abs(test_score - clf.oob_score_) &lt; </span><span class="s4">0.1</span>

        <span class="s2"># Test with few estimators</span>
        <span class="s1">warn_msg = (</span>
            <span class="s5">&quot;Some inputs do not have OOB scores. This probably means too few &quot;</span>
            <span class="s5">&quot;estimators were used to compute any reliable oob estimates.&quot;</span>
        <span class="s1">)</span>
        <span class="s3">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s3">, </span><span class="s1">match=warn_msg):</span>
            <span class="s1">clf = BaggingClassifier(</span>
                <span class="s1">estimator=estimator</span><span class="s3">,</span>
                <span class="s1">n_estimators=</span><span class="s4">1</span><span class="s3">,</span>
                <span class="s1">bootstrap=</span><span class="s3">True,</span>
                <span class="s1">oob_score=</span><span class="s3">True,</span>
                <span class="s1">random_state=rng</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">clf.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>


<span class="s3">def </span><span class="s1">test_oob_score_regression():</span>
    <span class="s2"># Check that oob prediction is a good estimation of the generalization</span>
    <span class="s2"># error.</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">diabetes.data</span><span class="s3">, </span><span class="s1">diabetes.target</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>

    <span class="s1">clf = BaggingRegressor(</span>
        <span class="s1">estimator=DecisionTreeRegressor()</span><span class="s3">,</span>
        <span class="s1">n_estimators=</span><span class="s4">50</span><span class="s3">,</span>
        <span class="s1">bootstrap=</span><span class="s3">True,</span>
        <span class="s1">oob_score=</span><span class="s3">True,</span>
        <span class="s1">random_state=rng</span><span class="s3">,</span>
    <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s1">test_score = clf.score(X_test</span><span class="s3">, </span><span class="s1">y_test)</span>

    <span class="s3">assert </span><span class="s1">abs(test_score - clf.oob_score_) &lt; </span><span class="s4">0.1</span>

    <span class="s2"># Test with few estimators</span>
    <span class="s1">warn_msg = (</span>
        <span class="s5">&quot;Some inputs do not have OOB scores. This probably means too few &quot;</span>
        <span class="s5">&quot;estimators were used to compute any reliable oob estimates.&quot;</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s3">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">regr = BaggingRegressor(</span>
            <span class="s1">estimator=DecisionTreeRegressor()</span><span class="s3">,</span>
            <span class="s1">n_estimators=</span><span class="s4">1</span><span class="s3">,</span>
            <span class="s1">bootstrap=</span><span class="s3">True,</span>
            <span class="s1">oob_score=</span><span class="s3">True,</span>
            <span class="s1">random_state=rng</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">regr.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>


<span class="s3">def </span><span class="s1">test_single_estimator():</span>
    <span class="s2"># Check singleton ensembles.</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">diabetes.data</span><span class="s3">, </span><span class="s1">diabetes.target</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>

    <span class="s1">clf1 = BaggingRegressor(</span>
        <span class="s1">estimator=KNeighborsRegressor()</span><span class="s3">,</span>
        <span class="s1">n_estimators=</span><span class="s4">1</span><span class="s3">,</span>
        <span class="s1">bootstrap=</span><span class="s3">False,</span>
        <span class="s1">bootstrap_features=</span><span class="s3">False,</span>
        <span class="s1">random_state=rng</span><span class="s3">,</span>
    <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s1">clf2 = KNeighborsRegressor().fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s1">assert_array_almost_equal(clf1.predict(X_test)</span><span class="s3">, </span><span class="s1">clf2.predict(X_test))</span>


<span class="s3">def </span><span class="s1">test_error():</span>
    <span class="s2"># Test support of decision_function</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = iris.data</span><span class="s3">, </span><span class="s1">iris.target</span>
    <span class="s1">base = DecisionTreeClassifier()</span>
    <span class="s3">assert not </span><span class="s1">hasattr(BaggingClassifier(base).fit(X</span><span class="s3">, </span><span class="s1">y)</span><span class="s3">, </span><span class="s5">&quot;decision_function&quot;</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_parallel_classification():</span>
    <span class="s2"># Check parallel classification.</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">iris.data</span><span class="s3">, </span><span class="s1">iris.target</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">)</span>

    <span class="s1">ensemble = BaggingClassifier(</span>
        <span class="s1">DecisionTreeClassifier()</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s2"># predict_proba</span>
    <span class="s1">y1 = ensemble.predict_proba(X_test)</span>
    <span class="s1">ensemble.set_params(n_jobs=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">y2 = ensemble.predict_proba(X_test)</span>
    <span class="s1">assert_array_almost_equal(y1</span><span class="s3">, </span><span class="s1">y2)</span>

    <span class="s1">ensemble = BaggingClassifier(</span>
        <span class="s1">DecisionTreeClassifier()</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s1">y3 = ensemble.predict_proba(X_test)</span>
    <span class="s1">assert_array_almost_equal(y1</span><span class="s3">, </span><span class="s1">y3)</span>

    <span class="s2"># decision_function</span>
    <span class="s1">ensemble = BaggingClassifier(</span>
        <span class="s1">SVC(decision_function_shape=</span><span class="s5">&quot;ovr&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s1">decisions1 = ensemble.decision_function(X_test)</span>
    <span class="s1">ensemble.set_params(n_jobs=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">decisions2 = ensemble.decision_function(X_test)</span>
    <span class="s1">assert_array_almost_equal(decisions1</span><span class="s3">, </span><span class="s1">decisions2)</span>

    <span class="s1">ensemble = BaggingClassifier(</span>
        <span class="s1">SVC(decision_function_shape=</span><span class="s5">&quot;ovr&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s1">decisions3 = ensemble.decision_function(X_test)</span>
    <span class="s1">assert_array_almost_equal(decisions1</span><span class="s3">, </span><span class="s1">decisions3)</span>


<span class="s3">def </span><span class="s1">test_parallel_regression():</span>
    <span class="s2"># Check parallel regression.</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">diabetes.data</span><span class="s3">, </span><span class="s1">diabetes.target</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>

    <span class="s1">ensemble = BaggingRegressor(DecisionTreeRegressor()</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(</span>
        <span class="s1">X_train</span><span class="s3">, </span><span class="s1">y_train</span>
    <span class="s1">)</span>

    <span class="s1">ensemble.set_params(n_jobs=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">y1 = ensemble.predict(X_test)</span>
    <span class="s1">ensemble.set_params(n_jobs=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">y2 = ensemble.predict(X_test)</span>
    <span class="s1">assert_array_almost_equal(y1</span><span class="s3">, </span><span class="s1">y2)</span>

    <span class="s1">ensemble = BaggingRegressor(DecisionTreeRegressor()</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(</span>
        <span class="s1">X_train</span><span class="s3">, </span><span class="s1">y_train</span>
    <span class="s1">)</span>

    <span class="s1">y3 = ensemble.predict(X_test)</span>
    <span class="s1">assert_array_almost_equal(y1</span><span class="s3">, </span><span class="s1">y3)</span>


<span class="s3">def </span><span class="s1">test_gridsearch():</span>
    <span class="s2"># Check that bagging ensembles can be grid-searched.</span>
    <span class="s2"># Transform iris into a binary classification task</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = iris.data</span><span class="s3">, </span><span class="s1">iris.target</span>
    <span class="s1">y[y == </span><span class="s4">2</span><span class="s1">] = </span><span class="s4">1</span>

    <span class="s2"># Grid search with scoring based on decision_function</span>
    <span class="s1">parameters = {</span><span class="s5">&quot;n_estimators&quot;</span><span class="s1">: (</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span><span class="s3">, </span><span class="s5">&quot;estimator__C&quot;</span><span class="s1">: (</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)}</span>

    <span class="s1">GridSearchCV(BaggingClassifier(SVC())</span><span class="s3">, </span><span class="s1">parameters</span><span class="s3">, </span><span class="s1">scoring=</span><span class="s5">&quot;roc_auc&quot;</span><span class="s1">).fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">test_estimator():</span>
    <span class="s2"># Check estimator and its default values.</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s2"># Classification</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">iris.data</span><span class="s3">, </span><span class="s1">iris.target</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>

    <span class="s1">ensemble = BaggingClassifier(</span><span class="s3">None, </span><span class="s1">n_jobs=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s3">assert </span><span class="s1">isinstance(ensemble.estimator_</span><span class="s3">, </span><span class="s1">DecisionTreeClassifier)</span>

    <span class="s1">ensemble = BaggingClassifier(</span>
        <span class="s1">DecisionTreeClassifier()</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span>
    <span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s3">assert </span><span class="s1">isinstance(ensemble.estimator_</span><span class="s3">, </span><span class="s1">DecisionTreeClassifier)</span>

    <span class="s1">ensemble = BaggingClassifier(Perceptron()</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(</span>
        <span class="s1">X_train</span><span class="s3">, </span><span class="s1">y_train</span>
    <span class="s1">)</span>

    <span class="s3">assert </span><span class="s1">isinstance(ensemble.estimator_</span><span class="s3">, </span><span class="s1">Perceptron)</span>

    <span class="s2"># Regression</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">diabetes.data</span><span class="s3">, </span><span class="s1">diabetes.target</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>

    <span class="s1">ensemble = BaggingRegressor(</span><span class="s3">None, </span><span class="s1">n_jobs=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s3">assert </span><span class="s1">isinstance(ensemble.estimator_</span><span class="s3">, </span><span class="s1">DecisionTreeRegressor)</span>

    <span class="s1">ensemble = BaggingRegressor(DecisionTreeRegressor()</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(</span>
        <span class="s1">X_train</span><span class="s3">, </span><span class="s1">y_train</span>
    <span class="s1">)</span>

    <span class="s3">assert </span><span class="s1">isinstance(ensemble.estimator_</span><span class="s3">, </span><span class="s1">DecisionTreeRegressor)</span>

    <span class="s1">ensemble = BaggingRegressor(SVR()</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">).fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s3">assert </span><span class="s1">isinstance(ensemble.estimator_</span><span class="s3">, </span><span class="s1">SVR)</span>


<span class="s3">def </span><span class="s1">test_bagging_with_pipeline():</span>
    <span class="s1">estimator = BaggingClassifier(</span>
        <span class="s1">make_pipeline(SelectKBest(k=</span><span class="s4">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">DecisionTreeClassifier())</span><span class="s3">, </span><span class="s1">max_features=</span><span class="s4">2</span>
    <span class="s1">)</span>
    <span class="s1">estimator.fit(iris.data</span><span class="s3">, </span><span class="s1">iris.target)</span>
    <span class="s3">assert </span><span class="s1">isinstance(estimator[</span><span class="s4">0</span><span class="s1">].steps[-</span><span class="s4">1</span><span class="s1">][</span><span class="s4">1</span><span class="s1">].random_state</span><span class="s3">, </span><span class="s1">int)</span>


<span class="s3">class </span><span class="s1">DummyZeroEstimator(BaseEstimator):</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s1">self.classes_ = np.unique(y)</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s3">return </span><span class="s1">self.classes_[np.zeros(X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">dtype=int)]</span>


<span class="s3">def </span><span class="s1">test_bagging_sample_weight_unsupported_but_passed():</span>
    <span class="s1">estimator = BaggingClassifier(DummyZeroEstimator())</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">estimator.fit(iris.data</span><span class="s3">, </span><span class="s1">iris.target).predict(iris.data)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">estimator.fit(</span>
            <span class="s1">iris.data</span><span class="s3">,</span>
            <span class="s1">iris.target</span><span class="s3">,</span>
            <span class="s1">sample_weight=rng.randint(</span><span class="s4">10</span><span class="s3">, </span><span class="s1">size=(iris.data.shape[</span><span class="s4">0</span><span class="s1">]))</span><span class="s3">,</span>
        <span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_warm_start(random_state=</span><span class="s4">42</span><span class="s1">):</span>
    <span class="s2"># Test if fitting incrementally with warm start gives a forest of the</span>
    <span class="s2"># right size and the same results as a normal fit.</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = make_hastie_10_2(n_samples=</span><span class="s4">20</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>

    <span class="s1">clf_ws = </span><span class="s3">None</span>
    <span class="s3">for </span><span class="s1">n_estimators </span><span class="s3">in </span><span class="s1">[</span><span class="s4">5</span><span class="s3">, </span><span class="s4">10</span><span class="s1">]:</span>
        <span class="s3">if </span><span class="s1">clf_ws </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">clf_ws = BaggingClassifier(</span>
                <span class="s1">n_estimators=n_estimators</span><span class="s3">, </span><span class="s1">random_state=random_state</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True</span>
            <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">clf_ws.set_params(n_estimators=n_estimators)</span>
        <span class="s1">clf_ws.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s3">assert </span><span class="s1">len(clf_ws) == n_estimators</span>

    <span class="s1">clf_no_ws = BaggingClassifier(</span>
        <span class="s1">n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">random_state=random_state</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">False</span>
    <span class="s1">)</span>
    <span class="s1">clf_no_ws.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s3">assert </span><span class="s1">set([tree.random_state </span><span class="s3">for </span><span class="s1">tree </span><span class="s3">in </span><span class="s1">clf_ws]) == set(</span>
        <span class="s1">[tree.random_state </span><span class="s3">for </span><span class="s1">tree </span><span class="s3">in </span><span class="s1">clf_no_ws]</span>
    <span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_warm_start_smaller_n_estimators():</span>
    <span class="s2"># Test if warm start'ed second fit with smaller n_estimators raises error.</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = make_hastie_10_2(n_samples=</span><span class="s4">20</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf = BaggingClassifier(n_estimators=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">clf.set_params(n_estimators=</span><span class="s4">4</span><span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">test_warm_start_equal_n_estimators():</span>
    <span class="s2"># Test that nothing happens when fitting without increasing n_estimators</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = make_hastie_10_2(n_samples=</span><span class="s4">20</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">43</span><span class="s1">)</span>

    <span class="s1">clf = BaggingClassifier(n_estimators=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True, </span><span class="s1">random_state=</span><span class="s4">83</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>

    <span class="s1">y_pred = clf.predict(X_test)</span>
    <span class="s2"># modify X to nonsense values, this should not change anything</span>
    <span class="s1">X_train += </span><span class="s4">1.0</span>

    <span class="s1">warn_msg = </span><span class="s5">&quot;Warm-start fitting without increasing n_estimators does not&quot;</span>
    <span class="s3">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s3">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">clf.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s1">assert_array_equal(y_pred</span><span class="s3">, </span><span class="s1">clf.predict(X_test))</span>


<span class="s3">def </span><span class="s1">test_warm_start_equivalence():</span>
    <span class="s2"># warm started classifier with 5+5 estimators should be equivalent to</span>
    <span class="s2"># one classifier with 10 estimators</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = make_hastie_10_2(n_samples=</span><span class="s4">20</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">43</span><span class="s1">)</span>

    <span class="s1">clf_ws = BaggingClassifier(n_estimators=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True, </span><span class="s1">random_state=</span><span class="s4">3141</span><span class="s1">)</span>
    <span class="s1">clf_ws.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s1">clf_ws.set_params(n_estimators=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">clf_ws.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s1">y1 = clf_ws.predict(X_test)</span>

    <span class="s1">clf = BaggingClassifier(n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">False, </span><span class="s1">random_state=</span><span class="s4">3141</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s1">y2 = clf.predict(X_test)</span>

    <span class="s1">assert_array_almost_equal(y1</span><span class="s3">, </span><span class="s1">y2)</span>


<span class="s3">def </span><span class="s1">test_warm_start_with_oob_score_fails():</span>
    <span class="s2"># Check using oob_score and warm_start simultaneously fails</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = make_hastie_10_2(n_samples=</span><span class="s4">20</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">clf = BaggingClassifier(n_estimators=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True, </span><span class="s1">oob_score=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">test_oob_score_removed_on_warm_start():</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = make_hastie_10_2(n_samples=</span><span class="s4">100</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>

    <span class="s1">clf = BaggingClassifier(n_estimators=</span><span class="s4">5</span><span class="s3">, </span><span class="s1">oob_score=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">clf.set_params(warm_start=</span><span class="s3">True, </span><span class="s1">oob_score=</span><span class="s3">False, </span><span class="s1">n_estimators=</span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s3">with </span><span class="s1">pytest.raises(AttributeError):</span>
        <span class="s1">getattr(clf</span><span class="s3">, </span><span class="s5">&quot;oob_score_&quot;</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_oob_score_consistency():</span>
    <span class="s2"># Make sure OOB scores are identical when random_state, estimator, and</span>
    <span class="s2"># training data are fixed and fitting is done twice</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = make_hastie_10_2(n_samples=</span><span class="s4">200</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">bagging = BaggingClassifier(</span>
        <span class="s1">KNeighborsClassifier()</span><span class="s3">,</span>
        <span class="s1">max_samples=</span><span class="s4">0.5</span><span class="s3">,</span>
        <span class="s1">max_features=</span><span class="s4">0.5</span><span class="s3">,</span>
        <span class="s1">oob_score=</span><span class="s3">True,</span>
        <span class="s1">random_state=</span><span class="s4">1</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">bagging.fit(X</span><span class="s3">, </span><span class="s1">y).oob_score_ == bagging.fit(X</span><span class="s3">, </span><span class="s1">y).oob_score_</span>


<span class="s3">def </span><span class="s1">test_estimators_samples():</span>
    <span class="s2"># Check that format of estimators_samples_ is correct and that results</span>
    <span class="s2"># generated at fit time can be identically reproduced at a later time</span>
    <span class="s2"># using data saved in object attributes.</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = make_hastie_10_2(n_samples=</span><span class="s4">200</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">bagging = BaggingClassifier(</span>
        <span class="s1">LogisticRegression()</span><span class="s3">,</span>
        <span class="s1">max_samples=</span><span class="s4">0.5</span><span class="s3">,</span>
        <span class="s1">max_features=</span><span class="s4">0.5</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s4">1</span><span class="s3">,</span>
        <span class="s1">bootstrap=</span><span class="s3">False,</span>
    <span class="s1">)</span>
    <span class="s1">bagging.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s2"># Get relevant attributes</span>
    <span class="s1">estimators_samples = bagging.estimators_samples_</span>
    <span class="s1">estimators_features = bagging.estimators_features_</span>
    <span class="s1">estimators = bagging.estimators_</span>

    <span class="s2"># Test for correct formatting</span>
    <span class="s3">assert </span><span class="s1">len(estimators_samples) == len(estimators)</span>
    <span class="s3">assert </span><span class="s1">len(estimators_samples[</span><span class="s4">0</span><span class="s1">]) == len(X) // </span><span class="s4">2</span>
    <span class="s3">assert </span><span class="s1">estimators_samples[</span><span class="s4">0</span><span class="s1">].dtype.kind == </span><span class="s5">&quot;i&quot;</span>

    <span class="s2"># Re-fit single estimator to test for consistent sampling</span>
    <span class="s1">estimator_index = </span><span class="s4">0</span>
    <span class="s1">estimator_samples = estimators_samples[estimator_index]</span>
    <span class="s1">estimator_features = estimators_features[estimator_index]</span>
    <span class="s1">estimator = estimators[estimator_index]</span>

    <span class="s1">X_train = (X[estimator_samples])[:</span><span class="s3">, </span><span class="s1">estimator_features]</span>
    <span class="s1">y_train = y[estimator_samples]</span>

    <span class="s1">orig_coefs = estimator.coef_</span>
    <span class="s1">estimator.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s1">new_coefs = estimator.coef_</span>

    <span class="s1">assert_array_almost_equal(orig_coefs</span><span class="s3">, </span><span class="s1">new_coefs)</span>


<span class="s3">def </span><span class="s1">test_estimators_samples_deterministic():</span>
    <span class="s2"># This test is a regression test to check that with a random step</span>
    <span class="s2"># (e.g. SparseRandomProjection) and a given random state, the results</span>
    <span class="s2"># generated at fit time can be identically reproduced at a later time using</span>
    <span class="s2"># data saved in object attributes. Check issue #9524 for full discussion.</span>

    <span class="s1">iris = load_iris()</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = iris.data</span><span class="s3">, </span><span class="s1">iris.target</span>

    <span class="s1">base_pipeline = make_pipeline(</span>
        <span class="s1">SparseRandomProjection(n_components=</span><span class="s4">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">LogisticRegression()</span>
    <span class="s1">)</span>
    <span class="s1">clf = BaggingClassifier(estimator=base_pipeline</span><span class="s3">, </span><span class="s1">max_samples=</span><span class="s4">0.5</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">pipeline_estimator_coef = clf.estimators_[</span><span class="s4">0</span><span class="s1">].steps[-</span><span class="s4">1</span><span class="s1">][</span><span class="s4">1</span><span class="s1">].coef_.copy()</span>

    <span class="s1">estimator = clf.estimators_[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">estimator_sample = clf.estimators_samples_[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">estimator_feature = clf.estimators_features_[</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s1">X_train = (X[estimator_sample])[:</span><span class="s3">, </span><span class="s1">estimator_feature]</span>
    <span class="s1">y_train = y[estimator_sample]</span>

    <span class="s1">estimator.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s1">assert_array_equal(estimator.steps[-</span><span class="s4">1</span><span class="s1">][</span><span class="s4">1</span><span class="s1">].coef_</span><span class="s3">, </span><span class="s1">pipeline_estimator_coef)</span>


<span class="s3">def </span><span class="s1">test_max_samples_consistency():</span>
    <span class="s2"># Make sure validated max_samples and original max_samples are identical</span>
    <span class="s2"># when valid integer max_samples supplied by user</span>
    <span class="s1">max_samples = </span><span class="s4">100</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = make_hastie_10_2(n_samples=</span><span class="s4">2 </span><span class="s1">* max_samples</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">bagging = BaggingClassifier(</span>
        <span class="s1">KNeighborsClassifier()</span><span class="s3">,</span>
        <span class="s1">max_samples=max_samples</span><span class="s3">,</span>
        <span class="s1">max_features=</span><span class="s4">0.5</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s4">1</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s1">bagging.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s3">assert </span><span class="s1">bagging._max_samples == max_samples</span>


<span class="s3">def </span><span class="s1">test_set_oob_score_label_encoding():</span>
    <span class="s2"># Make sure the oob_score doesn't change when the labels change</span>
    <span class="s2"># See: https://github.com/scikit-learn/scikit-learn/issues/8933</span>
    <span class="s1">random_state = </span><span class="s4">5</span>
    <span class="s1">X = [[-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s1">]] * </span><span class="s4">5</span>
    <span class="s1">Y1 = [</span><span class="s5">&quot;A&quot;</span><span class="s3">, </span><span class="s5">&quot;B&quot;</span><span class="s3">, </span><span class="s5">&quot;C&quot;</span><span class="s1">] * </span><span class="s4">5</span>
    <span class="s1">Y2 = [-</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">] * </span><span class="s4">5</span>
    <span class="s1">Y3 = [</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">] * </span><span class="s4">5</span>
    <span class="s1">x1 = (</span>
        <span class="s1">BaggingClassifier(oob_score=</span><span class="s3">True, </span><span class="s1">random_state=random_state)</span>
        <span class="s1">.fit(X</span><span class="s3">, </span><span class="s1">Y1)</span>
        <span class="s1">.oob_score_</span>
    <span class="s1">)</span>
    <span class="s1">x2 = (</span>
        <span class="s1">BaggingClassifier(oob_score=</span><span class="s3">True, </span><span class="s1">random_state=random_state)</span>
        <span class="s1">.fit(X</span><span class="s3">, </span><span class="s1">Y2)</span>
        <span class="s1">.oob_score_</span>
    <span class="s1">)</span>
    <span class="s1">x3 = (</span>
        <span class="s1">BaggingClassifier(oob_score=</span><span class="s3">True, </span><span class="s1">random_state=random_state)</span>
        <span class="s1">.fit(X</span><span class="s3">, </span><span class="s1">Y3)</span>
        <span class="s1">.oob_score_</span>
    <span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">[x1</span><span class="s3">, </span><span class="s1">x2] == [x3</span><span class="s3">, </span><span class="s1">x3]</span>


<span class="s3">def </span><span class="s1">replace(X):</span>
    <span class="s1">X = X.astype(</span><span class="s5">&quot;float&quot;</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s1">X[~np.isfinite(X)] = </span><span class="s4">0</span>
    <span class="s3">return </span><span class="s1">X</span>


<span class="s3">def </span><span class="s1">test_bagging_regressor_with_missing_inputs():</span>
    <span class="s2"># Check that BaggingRegressor can accept X with missing/infinite data</span>
    <span class="s1">X = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, </span><span class="s4">5</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s3">, None, </span><span class="s4">6</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s1">np.nan</span><span class="s3">, </span><span class="s4">6</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s1">np.inf</span><span class="s3">, </span><span class="s4">6</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-np.inf</span><span class="s3">, </span><span class="s4">6</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">y_values = [</span>
        <span class="s1">np.array([</span><span class="s4">2</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, </span><span class="s4">3</span><span class="s1">])</span><span class="s3">,</span>
        <span class="s1">np.array(</span>
            <span class="s1">[</span>
                <span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">9</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">[</span><span class="s4">3</span><span class="s3">, </span><span class="s4">6</span><span class="s3">, </span><span class="s4">8</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">[</span><span class="s4">3</span><span class="s3">, </span><span class="s4">6</span><span class="s3">, </span><span class="s4">8</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">[</span><span class="s4">3</span><span class="s3">, </span><span class="s4">6</span><span class="s3">, </span><span class="s4">8</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">[</span><span class="s4">3</span><span class="s3">, </span><span class="s4">6</span><span class="s3">, </span><span class="s4">8</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">]</span>
        <span class="s1">)</span><span class="s3">,</span>
    <span class="s1">]</span>
    <span class="s3">for </span><span class="s1">y </span><span class="s3">in </span><span class="s1">y_values:</span>
        <span class="s1">regressor = DecisionTreeRegressor()</span>
        <span class="s1">pipeline = make_pipeline(FunctionTransformer(replace)</span><span class="s3">, </span><span class="s1">regressor)</span>
        <span class="s1">pipeline.fit(X</span><span class="s3">, </span><span class="s1">y).predict(X)</span>
        <span class="s1">bagging_regressor = BaggingRegressor(pipeline)</span>
        <span class="s1">y_hat = bagging_regressor.fit(X</span><span class="s3">, </span><span class="s1">y).predict(X)</span>
        <span class="s3">assert </span><span class="s1">y.shape == y_hat.shape</span>

        <span class="s2"># Verify that exceptions can be raised by wrapper regressor</span>
        <span class="s1">regressor = DecisionTreeRegressor()</span>
        <span class="s1">pipeline = make_pipeline(regressor)</span>
        <span class="s3">with </span><span class="s1">pytest.raises(ValueError):</span>
            <span class="s1">pipeline.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s1">bagging_regressor = BaggingRegressor(pipeline)</span>
        <span class="s3">with </span><span class="s1">pytest.raises(ValueError):</span>
            <span class="s1">bagging_regressor.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">test_bagging_classifier_with_missing_inputs():</span>
    <span class="s2"># Check that BaggingClassifier can accept X with missing/infinite data</span>
    <span class="s1">X = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">3</span><span class="s3">, </span><span class="s4">5</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s3">, None, </span><span class="s4">6</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s1">np.nan</span><span class="s3">, </span><span class="s4">6</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s1">np.inf</span><span class="s3">, </span><span class="s4">6</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-np.inf</span><span class="s3">, </span><span class="s4">6</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">y = np.array([</span><span class="s4">3</span><span class="s3">, </span><span class="s4">6</span><span class="s3">, </span><span class="s4">6</span><span class="s3">, </span><span class="s4">6</span><span class="s3">, </span><span class="s4">6</span><span class="s1">])</span>
    <span class="s1">classifier = DecisionTreeClassifier()</span>
    <span class="s1">pipeline = make_pipeline(FunctionTransformer(replace)</span><span class="s3">, </span><span class="s1">classifier)</span>
    <span class="s1">pipeline.fit(X</span><span class="s3">, </span><span class="s1">y).predict(X)</span>
    <span class="s1">bagging_classifier = BaggingClassifier(pipeline)</span>
    <span class="s1">bagging_classifier.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">y_hat = bagging_classifier.predict(X)</span>
    <span class="s3">assert </span><span class="s1">y.shape == y_hat.shape</span>
    <span class="s1">bagging_classifier.predict_log_proba(X)</span>
    <span class="s1">bagging_classifier.predict_proba(X)</span>

    <span class="s2"># Verify that exceptions can be raised by wrapper classifier</span>
    <span class="s1">classifier = DecisionTreeClassifier()</span>
    <span class="s1">pipeline = make_pipeline(classifier)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">pipeline.fit(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">bagging_classifier = BaggingClassifier(pipeline)</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">bagging_classifier.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">test_bagging_small_max_features():</span>
    <span class="s2"># Check that Bagging estimator can accept low fractional max_features</span>

    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">3</span><span class="s3">, </span><span class="s4">4</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])</span>

    <span class="s1">bagging = BaggingClassifier(LogisticRegression()</span><span class="s3">, </span><span class="s1">max_features=</span><span class="s4">0.3</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">bagging.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s3">def </span><span class="s1">test_bagging_get_estimators_indices():</span>
    <span class="s2"># Check that Bagging estimator can generate sample indices properly</span>
    <span class="s2"># Non-regression test for:</span>
    <span class="s2"># https://github.com/scikit-learn/scikit-learn/issues/16436</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(</span><span class="s4">13</span><span class="s3">, </span><span class="s4">4</span><span class="s1">)</span>
    <span class="s1">y = np.arange(</span><span class="s4">13</span><span class="s1">)</span>

    <span class="s3">class </span><span class="s1">MyEstimator(DecisionTreeRegressor):</span>
        <span class="s0">&quot;&quot;&quot;An estimator which stores y indices information at fit.&quot;&quot;&quot;</span>

        <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
            <span class="s1">self._sample_indices = y</span>

    <span class="s1">clf = BaggingRegressor(estimator=MyEstimator()</span><span class="s3">, </span><span class="s1">n_estimators=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">assert_array_equal(clf.estimators_[</span><span class="s4">0</span><span class="s1">]._sample_indices</span><span class="s3">, </span><span class="s1">clf.estimators_samples_[</span><span class="s4">0</span><span class="s1">])</span>


<span class="s2"># TODO(1.4): remove in 1.4</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;Bagging, Estimator&quot;</span><span class="s3">,</span>
    <span class="s1">[</span>
        <span class="s1">(BaggingClassifier</span><span class="s3">, </span><span class="s1">DecisionTreeClassifier)</span><span class="s3">,</span>
        <span class="s1">(BaggingRegressor</span><span class="s3">, </span><span class="s1">DecisionTreeRegressor)</span><span class="s3">,</span>
    <span class="s1">]</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">test_base_estimator_argument_deprecated(Bagging</span><span class="s3">, </span><span class="s1">Estimator):</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">3</span><span class="s3">, </span><span class="s4">4</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">model = Bagging(base_estimator=Estimator()</span><span class="s3">, </span><span class="s1">n_estimators=</span><span class="s4">10</span><span class="s1">)</span>

    <span class="s1">warn_msg = (</span>
        <span class="s5">&quot;`base_estimator` was renamed to `estimator` in version 1.2 and &quot;</span>
        <span class="s5">&quot;will be removed in 1.4.&quot;</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s3">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">model.fit(X</span><span class="s3">, </span><span class="s1">y)</span>


<span class="s2"># TODO(1.4): remove in 1.4</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;Bagging&quot;</span><span class="s3">,</span>
    <span class="s1">[BaggingClassifier</span><span class="s3">, </span><span class="s1">BaggingClassifier]</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">test_base_estimator_property_deprecated(Bagging):</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">3</span><span class="s3">, </span><span class="s4">4</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">model = Bagging()</span>
    <span class="s1">model.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">warn_msg = (</span>
        <span class="s5">&quot;Attribute `base_estimator_` was deprecated in version 1.2 and &quot;</span>
        <span class="s5">&quot;will be removed in 1.4. Use `estimator_` instead.&quot;</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s3">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">model.base_estimator_</span>


<span class="s2"># TODO(1.4): remove</span>
<span class="s3">def </span><span class="s1">test_deprecated_base_estimator_has_decision_function():</span>
    <span class="s0">&quot;&quot;&quot;Check that `BaggingClassifier` delegate to classifier with 
    `decision_function`.&quot;&quot;&quot;</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y = iris.data</span><span class="s3">, </span><span class="s1">iris.target</span>
    <span class="s1">clf = BaggingClassifier(base_estimator=SVC())</span>
    <span class="s3">assert </span><span class="s1">hasattr(clf</span><span class="s3">, </span><span class="s5">&quot;decision_function&quot;</span><span class="s1">)</span>
    <span class="s1">warn_msg = (</span>
        <span class="s5">&quot;`base_estimator` was renamed to `estimator` in version 1.2 and &quot;</span>
        <span class="s5">&quot;will be removed in 1.4.&quot;</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s3">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">y_decision = clf.fit(X</span><span class="s3">, </span><span class="s1">y).decision_function(X)</span>
    <span class="s3">assert </span><span class="s1">y_decision.shape == (</span><span class="s4">150</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;bagging, expected_allow_nan&quot;</span><span class="s3">,</span>
    <span class="s1">[</span>
        <span class="s1">(BaggingClassifier(HistGradientBoostingClassifier(max_iter=</span><span class="s4">1</span><span class="s1">))</span><span class="s3">, True</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">(BaggingRegressor(HistGradientBoostingRegressor(max_iter=</span><span class="s4">1</span><span class="s1">))</span><span class="s3">, True</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">(BaggingClassifier(LogisticRegression())</span><span class="s3">, False</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">(BaggingRegressor(SVR())</span><span class="s3">, False</span><span class="s1">)</span><span class="s3">,</span>
    <span class="s1">]</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">test_bagging_allow_nan_tag(bagging</span><span class="s3">, </span><span class="s1">expected_allow_nan):</span>
    <span class="s0">&quot;&quot;&quot;Check that bagging inherits allow_nan tag.&quot;&quot;&quot;</span>
    <span class="s3">assert </span><span class="s1">bagging._get_tags()[</span><span class="s5">&quot;allow_nan&quot;</span><span class="s1">] == expected_allow_nan</span>
</pre>
</body>
</html>