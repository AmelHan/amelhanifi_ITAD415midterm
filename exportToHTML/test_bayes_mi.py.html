<html>
<head>
<title>test_bayes_mi.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6897bb;}
.s3 { color: #808080;}
.s4 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_bayes_mi.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pandas </span><span class="s0">as </span><span class="s1">pd</span>
<span class="s0">import </span><span class="s1">statsmodels.api </span><span class="s0">as </span><span class="s1">sm</span>
<span class="s0">from </span><span class="s1">statsmodels.imputation.bayes_mi </span><span class="s0">import </span><span class="s1">BayesGaussMI</span><span class="s0">, </span><span class="s1">MI</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">assert_allclose</span><span class="s0">, </span><span class="s1">assert_equal</span>


<span class="s0">def </span><span class="s1">test_pat():</span>

    <span class="s1">x = np.asarray([[</span><span class="s2">1</span><span class="s0">, </span><span class="s1">np.nan</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[np.nan</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s1">np.nan]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">3</span><span class="s0">, </span><span class="s1">np.nan</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">,</span>
                    <span class="s1">[np.nan</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s1">np.nan]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">3</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]])</span>
    <span class="s1">bm = BayesGaussMI(x)</span>
    <span class="s1">assert_allclose(bm.patterns[</span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.r_[</span><span class="s2">0</span><span class="s0">, </span><span class="s2">2</span><span class="s1">])</span>
    <span class="s1">assert_allclose(bm.patterns[</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.r_[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">3</span><span class="s1">])</span>


<span class="s0">def </span><span class="s1">test_2x2():</span>

    <span class="s3"># Generate correlated data with mean and variance</span>
    <span class="s1">np.random.seed(</span><span class="s2">3434</span><span class="s1">)</span>
    <span class="s1">x = np.random.normal(size=(</span><span class="s2">1000</span><span class="s0">, </span><span class="s2">2</span><span class="s1">))</span>
    <span class="s1">r = </span><span class="s2">0.5</span>
    <span class="s1">x[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] = r*x[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] + np.sqrt(</span><span class="s2">1</span><span class="s1">-r**</span><span class="s2">2</span><span class="s1">)*x[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span>
    <span class="s1">x[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] *= </span><span class="s2">2</span>
    <span class="s1">x[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] *= </span><span class="s2">3</span>
    <span class="s1">x[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] += </span><span class="s2">1</span>
    <span class="s1">x[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] -= </span><span class="s2">2</span>

    <span class="s3"># Introduce some missing values</span>
    <span class="s1">u = np.random.normal(size=x.shape[</span><span class="s2">0</span><span class="s1">])</span>
    <span class="s1">x[u &gt; </span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] = np.nan</span>
    <span class="s1">u = np.random.normal(size=x.shape[</span><span class="s2">0</span><span class="s1">])</span>
    <span class="s1">x[u &gt; </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] = np.nan</span>

    <span class="s1">bm = BayesGaussMI(x)</span>

    <span class="s3"># Burn-in</span>
    <span class="s0">for </span><span class="s1">k </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">500</span><span class="s1">):</span>
        <span class="s1">bm.update()</span>

    <span class="s3"># Estimate the posterior mean</span>
    <span class="s1">mean = </span><span class="s2">0</span>
    <span class="s1">cov = </span><span class="s2">0</span>
    <span class="s1">dmean = </span><span class="s2">0</span>
    <span class="s1">dcov = </span><span class="s2">0</span>
    <span class="s0">for </span><span class="s1">k </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">500</span><span class="s1">):</span>
        <span class="s1">bm.update()</span>
        <span class="s1">mean += bm.mean</span>
        <span class="s1">cov += bm.cov</span>
        <span class="s1">dmean += bm.data.mean(</span><span class="s2">0</span><span class="s1">)</span>
        <span class="s1">dcov += np.cov(bm.data.T)</span>
    <span class="s1">mean /= </span><span class="s2">500</span>
    <span class="s1">cov /= </span><span class="s2">500</span>
    <span class="s1">dmean /= </span><span class="s2">500</span>
    <span class="s1">dcov /= </span><span class="s2">500</span>

    <span class="s1">assert_allclose(mean</span><span class="s0">, </span><span class="s1">np.r_[</span><span class="s2">1</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s2">0.1</span><span class="s1">)</span>
    <span class="s1">assert_allclose(dmean</span><span class="s0">, </span><span class="s1">np.r_[</span><span class="s2">1</span><span class="s0">, </span><span class="s1">-</span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s2">0.1</span><span class="s1">)</span>
    <span class="s1">assert_allclose(cov</span><span class="s0">, </span><span class="s1">np.asarray([[</span><span class="s2">4</span><span class="s0">, </span><span class="s2">6</span><span class="s1">*r]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">6</span><span class="s1">*r</span><span class="s0">, </span><span class="s2">9</span><span class="s1">]])</span><span class="s0">, </span><span class="s2">0.1</span><span class="s1">)</span>
    <span class="s1">assert_allclose(dcov</span><span class="s0">, </span><span class="s1">np.asarray([[</span><span class="s2">4</span><span class="s0">, </span><span class="s2">6</span><span class="s1">*r]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">6</span><span class="s1">*r</span><span class="s0">, </span><span class="s2">9</span><span class="s1">]])</span><span class="s0">, </span><span class="s2">0.1</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_MI():</span>

    <span class="s1">np.random.seed(</span><span class="s2">414</span><span class="s1">)</span>
    <span class="s1">x = np.random.normal(size=(</span><span class="s2">200</span><span class="s0">, </span><span class="s2">4</span><span class="s1">))</span>
    <span class="s1">x[[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s2">9</span><span class="s1">]</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] = np.nan</span>
    <span class="s1">x[[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">4</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] = np.nan</span>
    <span class="s1">x[[</span><span class="s2">2</span><span class="s0">, </span><span class="s2">11</span><span class="s0">, </span><span class="s2">21</span><span class="s1">]</span><span class="s0">, </span><span class="s2">2</span><span class="s1">] = np.nan</span>
    <span class="s1">x[[</span><span class="s2">11</span><span class="s0">, </span><span class="s2">22</span><span class="s0">, </span><span class="s2">99</span><span class="s1">]</span><span class="s0">, </span><span class="s2">3</span><span class="s1">] = np.nan</span>

    <span class="s0">def </span><span class="s1">model_args_fn(x):</span>
        <span class="s3"># Return endog, exog</span>
        <span class="s3"># Regress x0 on x1 and x2</span>
        <span class="s0">if </span><span class="s1">type(x) </span><span class="s0">is </span><span class="s1">np.ndarray:</span>
            <span class="s0">return </span><span class="s1">(x[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">x[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">:])</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">(x.iloc[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">].values</span><span class="s0">, </span><span class="s1">x.iloc[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">:].values)</span>

    <span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">(</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">):</span>
        <span class="s1">np.random.seed(</span><span class="s2">2342</span><span class="s1">)</span>
        <span class="s1">imp = BayesGaussMI(x.copy())</span>
        <span class="s1">mi = MI(imp</span><span class="s0">, </span><span class="s1">sm.OLS</span><span class="s0">, </span><span class="s1">model_args_fn</span><span class="s0">, </span><span class="s1">burn=</span><span class="s2">0</span><span class="s1">)</span>
        <span class="s1">r = mi.fit()</span>
        <span class="s1">r.summary()  </span><span class="s3"># smoke test</span>
        <span class="s3"># TODO: why does the test tolerance need to be so slack?</span>
        <span class="s3"># There is unexpected variation across versions</span>
        <span class="s1">assert_allclose(r.params</span><span class="s0">, </span><span class="s1">np.r_[</span>
            <span class="s1">-</span><span class="s2">0.05347919</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.02479701</span><span class="s0">, </span><span class="s2">0.10075517</span><span class="s1">]</span><span class="s0">, </span><span class="s2">0.25</span><span class="s0">, </span><span class="s2">0</span><span class="s1">)</span>

        <span class="s1">c = np.asarray([[</span><span class="s2">0.00418232</span><span class="s0">, </span><span class="s2">0.00029746</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.00035057</span><span class="s1">]</span><span class="s0">,</span>
                        <span class="s1">[</span><span class="s2">0.00029746</span><span class="s0">, </span><span class="s2">0.00407264</span><span class="s0">, </span><span class="s2">0.00019496</span><span class="s1">]</span><span class="s0">,</span>
                        <span class="s1">[-</span><span class="s2">0.00035057</span><span class="s0">, </span><span class="s2">0.00019496</span><span class="s0">, </span><span class="s2">0.00509413</span><span class="s1">]])</span>
        <span class="s1">assert_allclose(r.cov_params()</span><span class="s0">, </span><span class="s1">c</span><span class="s0">, </span><span class="s2">0.3</span><span class="s0">, </span><span class="s2">0</span><span class="s1">)</span>

        <span class="s3"># Test with ndarray and pandas input</span>
        <span class="s1">x = pd.DataFrame(x)</span>


<span class="s0">def </span><span class="s1">test_MI_stat():</span>
    <span class="s3"># Test for MI where we know statistically what should happen. The</span>
    <span class="s3"># analysis model is x0 ~ x1 with standard error 1/sqrt(n) for the</span>
    <span class="s3"># slope parameter.  The nominal n is 1000, but half of the cases</span>
    <span class="s3"># have missing x1.  Then we introduce x2 that is either</span>
    <span class="s3"># independent of x1, or almost perfectly correlated with x1.  In</span>
    <span class="s3"># the first case the SE is 1/sqrt(500), in the second case the SE</span>
    <span class="s3"># is 1/sqrt(1000).</span>

    <span class="s1">np.random.seed(</span><span class="s2">414</span><span class="s1">)</span>
    <span class="s1">z = np.random.normal(size=(</span><span class="s2">1000</span><span class="s0">, </span><span class="s2">3</span><span class="s1">))</span>
    <span class="s1">z[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] += </span><span class="s2">0.5</span><span class="s1">*z[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span>

    <span class="s3"># Control the degree to which x2 proxies for x1</span>
    <span class="s1">exp = [</span><span class="s2">1</span><span class="s1">/np.sqrt(</span><span class="s2">500</span><span class="s1">)</span><span class="s0">, </span><span class="s2">1</span><span class="s1">/np.sqrt(</span><span class="s2">1000</span><span class="s1">)]</span>
    <span class="s1">fmi = [</span><span class="s2">0.5</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span>
    <span class="s0">for </span><span class="s1">j</span><span class="s0">, </span><span class="s1">r </span><span class="s0">in </span><span class="s1">enumerate((</span><span class="s2">0</span><span class="s0">, </span><span class="s2">0.9999</span><span class="s1">)):</span>

        <span class="s1">x = z.copy()</span>
        <span class="s1">x[:</span><span class="s0">, </span><span class="s2">2</span><span class="s1">] = r*x[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] + np.sqrt(</span><span class="s2">1 </span><span class="s1">- r**</span><span class="s2">2</span><span class="s1">)*x[:</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span>
        <span class="s1">x[</span><span class="s2">0</span><span class="s1">:</span><span class="s2">500</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] = np.nan</span>

        <span class="s0">def </span><span class="s1">model_args(x):</span>
            <span class="s3"># Return endog, exog</span>
            <span class="s3"># Regress x1 on x2</span>
            <span class="s0">return </span><span class="s1">(x[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">x[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">])</span>

        <span class="s1">np.random.seed(</span><span class="s2">2342</span><span class="s1">)</span>
        <span class="s1">imp = BayesGaussMI(x.copy())</span>
        <span class="s1">mi = MI(imp</span><span class="s0">, </span><span class="s1">sm.OLS</span><span class="s0">, </span><span class="s1">model_args</span><span class="s0">, </span><span class="s1">nrep=</span><span class="s2">100</span><span class="s0">, </span><span class="s1">skip=</span><span class="s2">10</span><span class="s1">)</span>
        <span class="s1">r = mi.fit()</span>

        <span class="s3"># Check the SE</span>
        <span class="s1">d = np.abs(r.bse[</span><span class="s2">0</span><span class="s1">] - exp[j]) / exp[j]</span>
        <span class="s0">assert </span><span class="s1">d &lt; </span><span class="s2">0.03</span>

        <span class="s3"># Check the FMI</span>
        <span class="s1">d = np.abs(r.fmi[</span><span class="s2">0</span><span class="s1">] - fmi[j])</span>
        <span class="s0">assert </span><span class="s1">d &lt; </span><span class="s2">0.05</span>


<span class="s0">def </span><span class="s1">test_mi_formula():</span>

    <span class="s1">np.random.seed(</span><span class="s2">414</span><span class="s1">)</span>
    <span class="s1">x = np.random.normal(size=(</span><span class="s2">200</span><span class="s0">, </span><span class="s2">4</span><span class="s1">))</span>
    <span class="s1">x[[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s2">9</span><span class="s1">]</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] = np.nan</span>
    <span class="s1">x[[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">4</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] = np.nan</span>
    <span class="s1">x[[</span><span class="s2">2</span><span class="s0">, </span><span class="s2">11</span><span class="s0">, </span><span class="s2">21</span><span class="s1">]</span><span class="s0">, </span><span class="s2">2</span><span class="s1">] = np.nan</span>
    <span class="s1">x[[</span><span class="s2">11</span><span class="s0">, </span><span class="s2">22</span><span class="s0">, </span><span class="s2">99</span><span class="s1">]</span><span class="s0">, </span><span class="s2">3</span><span class="s1">] = np.nan</span>
    <span class="s1">df = pd.DataFrame({</span><span class="s4">&quot;y&quot;</span><span class="s1">: x[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s4">&quot;x1&quot;</span><span class="s1">: x[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span><span class="s0">,</span>
                       <span class="s4">&quot;x2&quot;</span><span class="s1">: x[:</span><span class="s0">, </span><span class="s2">2</span><span class="s1">]</span><span class="s0">, </span><span class="s4">&quot;x3&quot;</span><span class="s1">: x[:</span><span class="s0">, </span><span class="s2">3</span><span class="s1">]})</span>
    <span class="s1">fml = </span><span class="s4">&quot;y ~ 0 + x1 + x2 + x3&quot;</span>

    <span class="s0">def </span><span class="s1">model_kwds_fn(x):</span>
        <span class="s0">return </span><span class="s1">{</span><span class="s4">&quot;data&quot;</span><span class="s1">: x}</span>

    <span class="s1">np.random.seed(</span><span class="s2">2342</span><span class="s1">)</span>
    <span class="s1">imp = BayesGaussMI(df.copy())</span>
    <span class="s1">mi = MI(imp</span><span class="s0">, </span><span class="s1">sm.OLS</span><span class="s0">, </span><span class="s1">formula=fml</span><span class="s0">, </span><span class="s1">burn=</span><span class="s2">0</span><span class="s0">,</span>
            <span class="s1">model_kwds_fn=model_kwds_fn)</span>

    <span class="s1">results_cb = </span><span class="s0">lambda </span><span class="s1">x: x</span>

    <span class="s1">r = mi.fit(results_cb=results_cb)</span>
    <span class="s1">r.summary()  </span><span class="s3"># smoke test</span>
    <span class="s3"># TODO: why does the test tolerance need to be so slack?</span>
    <span class="s3"># There is unexpected variation across versions</span>
    <span class="s1">assert_allclose(r.params</span><span class="s0">, </span><span class="s1">np.r_[</span>
            <span class="s1">-</span><span class="s2">0.05347919</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.02479701</span><span class="s0">, </span><span class="s2">0.10075517</span><span class="s1">]</span><span class="s0">, </span><span class="s2">0.25</span><span class="s0">, </span><span class="s2">0</span><span class="s1">)</span>

    <span class="s1">c = np.asarray([[</span><span class="s2">0.00418232</span><span class="s0">, </span><span class="s2">0.00029746</span><span class="s0">, </span><span class="s1">-</span><span class="s2">0.00035057</span><span class="s1">]</span><span class="s0">,</span>
                    <span class="s1">[</span><span class="s2">0.00029746</span><span class="s0">, </span><span class="s2">0.00407264</span><span class="s0">, </span><span class="s2">0.00019496</span><span class="s1">]</span><span class="s0">,</span>
                    <span class="s1">[-</span><span class="s2">0.00035057</span><span class="s0">, </span><span class="s2">0.00019496</span><span class="s0">, </span><span class="s2">0.00509413</span><span class="s1">]])</span>
    <span class="s1">assert_allclose(r.cov_params()</span><span class="s0">, </span><span class="s1">c</span><span class="s0">, </span><span class="s2">0.3</span><span class="s0">, </span><span class="s2">0</span><span class="s1">)</span>

    <span class="s1">assert_equal(len(r.results)</span><span class="s0">, </span><span class="s2">20</span><span class="s1">)</span>
</pre>
</body>
</html>